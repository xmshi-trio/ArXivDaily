{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. (arXiv:2309.15129v1 [cs.AI])","link":"http://arxiv.org/abs/2309.15129","description":"<p>Recently an influx of studies claim emergent cognitive abilities in large\nlanguage models (LLMs). Yet, most rely on anecdotes, overlook contamination of\ntraining sets, or lack systematic Evaluation involving multiple tasks, control\nconditions, multiple iterations, and statistical robustness tests. Here we make\ntwo major contributions. First, we propose CogEval, a cognitive\nscience-inspired protocol for the systematic evaluation of cognitive capacities\nin Large Language Models. The CogEval protocol can be followed for the\nevaluation of various abilities. Second, here we follow CogEval to\nsystematically evaluate cognitive maps and planning ability across eight LLMs\n(OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard,\nCohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base\nour task prompts on human experiments, which offer both established construct\nvalidity for evaluating planning, and are absent from LLM training sets. We\nfind that, while LLMs show apparent competence in a few planning tasks with\nsimpler structures, systematic evaluation reveals striking failure modes in\nplanning tasks, including hallucinations of invalid trajectories and getting\ntrapped in loops. These findings do not support the idea of emergent\nout-of-the-box planning ability in LLMs. This could be because LLMs do not\nunderstand the latent relational structures underlying planning problems, known\nas cognitive maps, and fail at unrolling goal-directed trajectories based on\nthe underlying structure. Implications for application and future directions\nare discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_H/0/1/0/all/0/1\">Hosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_F/0/1/0/all/0/1\">Felipe Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Hiteshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ness_R/0/1/0/all/0/1\">Robert Osazuwa Ness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1\">Jonathan Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation. (arXiv:2309.15176v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15176","description":"<p>Stance detection is the process of inferring a person's position or\nstandpoint on a specific issue to deduce prevailing perceptions toward topics\nof general or controversial interest, such as health policies during the\nCOVID-19 pandemic. Existing models for stance detection are trained to perform\nwell for a single domain (e.g., COVID-19) and a specific target topic (e.g.,\nmasking protocols), but are generally ineffectual in other domains or targets\ndue to distributional shifts in the data. However, constructing\nhigh-performing, domain-specific stance detection models requires an extensive\ncorpus of labeled data relevant to the targeted domain, yet such datasets are\nnot readily available. This poses a challenge as the process of annotating data\nis costly and time-consuming. To address these challenges, we introduce a novel\nstance detection model coined domain-adaptive Cross-target STANCE detection via\nContrastive learning and Counterfactual generation (STANCE-C3) that uses\ncounterfactual data augmentation to enhance domain-adaptive training by\nenriching the target domain dataset during the training process and requiring\nsignificantly less information from the new domain. We also propose a modified\nself-supervised contrastive learning as a component of STANCE-C3 to prevent\noverfitting for the existing domain and target and enable cross-target stance\ndetection. Through experiments on various datasets, we show that STANCE-C3\nshows performance improvement over existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nayoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosallanezhad_D/0/1/0/all/0/1\">David Mosallanezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancenido_M/0/1/0/all/0/1\">Michelle V. Mancenido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAGAS: Automated Evaluation of Retrieval Augmented Generation. (arXiv:2309.15217v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15217","description":"<p>We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With RAGAs, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Es_S/0/1/0/all/0/1\">Shahul Es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_J/0/1/0/all/0/1\">Jithin James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15223","description":"<p>We propose a neural language modeling system based on low-rank adaptation\n(LoRA) for speech recognition output rescoring. Although pretrained language\nmodels (LMs) like BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pretraining stage and\nadapting the pretrained models to specific domains limit their practical use in\nrescoring. Here we present a method based on low-rank decomposition to train a\nrescoring BERT model and adapt it to new domains using only a fraction (0.08%)\nof the pretrained parameters. These inserted matrices are optimized through a\ndiscriminative training objective along with a correlation-based regularization\nloss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is\nevaluated on LibriSpeech and internal datasets with decreased training times by\nfactors between 5.4 and 3.6.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolehmainen_J/0/1/0/all/0/1\">Jari Kolehmainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivakumar_P/0/1/0/all/0/1\">Prashanth G. Shivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Sungho Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Roger Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourav_A/0/1/0/all/0/1\">Aditya Gourav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi-Chieh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastow_A/0/1/0/all/0/1\">Ariya Rastow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15238","description":"<p>Learning Using Privileged Information is a particular type of knowledge\ndistillation where the teacher model benefits from an additional data\nrepresentation during training, called privileged information, improving the\nstudent model, which does not see the extra representation. However, privileged\ninformation is rarely available in practice. To this end, we propose a text\nclassification framework that harnesses text-to-image diffusion models to\ngenerate artificial privileged information. The generated images and the\noriginal text samples are further used to train multimodal teacher models based\non state-of-the-art transformer-based architectures. Finally, the knowledge\nfrom multimodal teachers is distilled into a text-based (unimodal) student.\nHence, by employing a generative model to produce synthetic data as privileged\ninformation, we guide the training of the student model. Our framework, called\nLearning Using Generated Privileged Information (LUGPI), yields noticeable\nperformance gains on four text classification data sets, demonstrating its\npotential in text classification without any additional cost during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menadil_R/0/1/0/all/0/1\">Rafael-Edy Menadil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"joint prediction and denoising for large-scale multilingual self-supervised learning. (arXiv:2309.15317v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15317","description":"<p>Multilingual self-supervised learning (SSL) has often lagged behind\nstate-of-the-art (SOTA) methods due to the expenses and complexity required to\nhandle many languages. This further harms the reproducibility of SSL, which is\nalready limited to few research groups due to its resource usage. We show that\nmore powerful techniques can actually lead to more efficient pre-training,\nopening SSL to more research groups. We propose WavLabLM, which extends WavLM's\njoint prediction and denoising to 40k hours of data across 136 languages. To\nbuild WavLabLM, we devise a novel multi-stage pre-training method, designed to\naddress the language imbalance of multilingual data. WavLabLM achieves\ncomparable performance to XLS-R on ML-SUPERB with less than 10% of the training\ndata, making SSL realizable with academic compute. We show that further\nefficiency can be achieved with a vanilla HuBERT Base model, which can maintain\n94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited\ntrials. We open-source all code and models in ESPnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wangyou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Chat: Executable and Verifiable Text-Editing with LLMs. (arXiv:2309.15337v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15337","description":"<p>Conversational interfaces powered by Large Language Models (LLMs) have\nrecently become a popular way to obtain feedback during document editing.\nHowever, standard chat-based conversational interfaces do not support\ntransparency and verifiability of the editing changes that they suggest. To\ngive the author more agency when editing with an LLM, we present InkSync, an\nediting interface that suggests executable edits directly within the document\nbeing edited. Because LLMs are known to introduce factual errors, Inksync also\nsupports a 3-stage approach to mitigate this risk: Warn authors when a\nsuggested edit introduces new information, help authors Verify the new\ninformation's accuracy through external search, and allow an auditor to perform\nan a-posteriori verification by Auditing the document via a trace of all\nauto-generated content. Two usability studies confirm the effectiveness of\nInkSync's components when compared to standard LLM-based chat interfaces,\nleading to more accurate, more efficient editing, and improved user experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15402","description":"<p>Chain-of-thought reasoning, a cognitive process fundamental to human\nintelligence, has garnered significant attention in the realm of artificial\nintelligence and natural language processing. However, there still remains a\nlack of a comprehensive survey for this arena. To this end, we take the first\nstep and present a thorough survey of this research field carefully and widely.\nWe use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail,\nwe systematically organize the current research according to the taxonomies of\nmethods, including XoT construction, XoT structure variants, and enhanced XoT.\nAdditionally, we describe XoT with frontier applications, covering planning,\ntool use, and distillation. Furthermore, we address challenges and discuss some\nfuture directions, including faithfulness, multi-modal, and theory. We hope\nthis survey serves as a valuable resource for researchers seeking to innovate\nwithin the domain of chain-of-thought reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weihua Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15427","description":"<p>Large Language Models (LLMs) have shown remarkable generalization capability\nwith exceptional performance in various language modeling tasks. However, they\nstill exhibit inherent limitations in precisely capturing and returning\ngrounded knowledge. While existing work has explored utilizing knowledge graphs\nto enhance language modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing to their large number\nof parameters and high computational cost. In addition, how to leverage the\npre-trained LLMs and avoid training a customized model from scratch remains an\nopen question. In this work, we propose Graph Neural Prompting (GNP), a novel\nplug-and-play method to assist pre-trained LLMs in learning beneficial\nknowledge from KGs. GNP encompasses various designs, including a standard graph\nneural network encoder, a cross-modality pooling module, a domain projector,\nand a self-supervised link prediction objective. Extensive experiments on\nmultiple datasets demonstrate the superiority of GNP on both commonsense and\nbiomedical reasoning tasks across different LLM sizes and settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yijun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Huan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Panpan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatCounselor: A Large Language Models for Mental Health Support. (arXiv:2309.15461v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15461","description":"<p>This paper presents ChatCounselor, a large language model (LLM) solution\ndesigned to provide mental health support. Unlike generic chatbots,\nChatCounselor is distinguished by its foundation in real conversations between\nconsulting clients and professional psychologists, enabling it to possess\nspecialized knowledge and counseling skills in the field of psychology. The\ntraining dataset, Psych8k, was constructed from 260 in-depth interviews, each\nspanning an hour. To assess the quality of counseling responses, the counseling\nBench was devised. Leveraging GPT-4 and meticulously crafted prompts based on\nseven metrics of psychological counseling assessment, the model underwent\nevaluation using a set of real-world counseling questions. Impressively,\nChatCounselor surpasses existing open-source models in the counseling Bench and\napproaches the performance level of ChatGPT, showcasing the remarkable\nenhancement in model capability attained through high-quality domain-specific\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">June M. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Donghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">He Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tianhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zeyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiamin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis. (arXiv:2309.15476v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15476","description":"<p>Conversational aspect-based sentiment quadruple analysis (DiaASQ) aims to\nextract the quadruple of target-aspect-opinion-sentiment within a dialogue. In\nDiaASQ, a quadruple's elements often cross multiple utterances. This situation\ncomplicates the extraction process, emphasizing the need for an adequate\nunderstanding of conversational context and interactions. However, existing\nwork independently encodes each utterance, thereby struggling to capture\nlong-range conversational context and overlooking the deep inter-utterance\ndependencies. In this work, we propose a novel Dynamic Multi-scale Context\nAggregation network (DMCA) to address the challenges. Specifically, we first\nutilize dialogue structure to generate multi-scale utterance windows for\ncapturing rich contextual information. After that, we design a Dynamic\nHierarchical Aggregation module (DHA) to integrate progressive cues between\nthem. In addition, we form a multi-stage loss strategy to improve model\nperformance and generalization ability. Extensive experimental results show\nthat the DMCA model outperforms baselines significantly and achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Siyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zisen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingbang Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning. (arXiv:2309.15494v1 [cs.CV])","link":"http://arxiv.org/abs/2309.15494","description":"<p>Multimodal transfer learning aims to transform pretrained representations of\ndiverse modalities into a common domain space for effective multimodal fusion.\nHowever, conventional systems are typically built on the assumption that all\nmodalities exist, and the lack of modalities always leads to poor inference\nperformance. Furthermore, extracting pretrained embeddings for all modalities\nis computationally inefficient for inference. In this work, to achieve high\nefficiency-performance multimodal transfer learning, we propose VideoAdviser, a\nvideo knowledge distillation method to transfer multimodal knowledge of\nvideo-enhanced prompts from a multimodal fundamental model (teacher) to a\nspecific modal fundamental model (student). With an intuition that the best\nlearning performance comes with professional advisers and smart students, we\nuse a CLIP-based teacher model to provide expressive multimodal knowledge\nsupervision signals to a RoBERTa-based student model via optimizing a\nstep-distillation objective loss -- first step: the teacher distills multimodal\nknowledge of video-enhanced prompts from classification logits to a regression\nlogit -- second step: the multimodal knowledge is distilled from the regression\nlogit of the teacher to the student. We evaluate our method in two challenging\nmultimodal tasks: video-level sentiment analysis (MOSI and MOSEI datasets) and\naudio-visual retrieval (VEGAS dataset). The student (requiring only the text\nmodality as input) achieves an MAE score improvement of up to 12.3% for MOSI\nand MOSEI. Our method further enhances the state-of-the-art method by 3.4% mAP\nscore for VEGAS without additional computations for inference. These results\nsuggest the strengths of our method for achieving high efficiency-performance\nmultimodal transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Donghuo Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_S/0/1/0/all/0/1\">Shinya Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurihara_S/0/1/0/all/0/1\">Satoshi Kurihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v1 [cs.SD])","link":"http://arxiv.org/abs/2309.15512","description":"<p>Text-to-speech (TTS) methods have shown promising results in voice cloning,\nbut they require a large number of labeled text-speech pairs.\nMinimally-supervised speech synthesis decouples TTS by combining two types of\ndiscrete speech representations(semantic \\&amp; acoustic) and using two\nsequence-to-sequence tasks to enable training with minimal supervision.\nHowever, existing methods suffer from information redundancy and dimension\nexplosion in semantic representation, and high-frequency waveform distortion in\ndiscrete acoustic representation. Autoregressive frameworks exhibit typical\ninstability and uncontrollability issues. And non-autoregressive frameworks\nsuffer from prosodic averaging caused by duration prediction models. To address\nthese issues, we propose a minimally-supervised high-fidelity speech synthesis\nmethod, where all modules are constructed based on the diffusion models. The\nnon-autoregressive framework enhances controllability, and the duration\ndiffusion model enables diversified prosodic expression. Contrastive\nToken-Acoustic Pretraining (CTAP) is used as an intermediate semantic\nrepresentation to solve the problems of information redundancy and dimension\nexplosion in existing semantic coding methods. Mel-spectrogram is used as the\nacoustic representation. Both semantic and acoustic representations are\npredicted by continuous variable regression tasks to solve the problem of\nhigh-frequency fine-grained waveform distortion. Experimental results show that\nour proposed method outperforms the baseline method. We provide audio samples\non our website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yixin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15516","description":"<p>Various works have been extensively studied in the research of text-to-image\ngeneration. Although existing models perform well in text-to-image generation,\nthere are significant challenges when directly employing them to generate\nimages in dialogs. In this paper, we first highlight a new problem:\ndialog-to-image generation, that is, given the dialog context, the model should\ngenerate a realistic image which is consistent with the specified conversation\nas response. To tackle the problem, we propose an efficient approach for\ndialog-to-image generation without any intermediate translation, which\nmaximizes the extraction of the semantic information contained in the dialog.\nConsidering the characteristics of dialog structure, we put segment token\nbefore each sentence in a turn of a dialog to differentiate different speakers.\nThen, we fine-tune pre-trained text-to-image models to enable them to generate\nimages conditioning on processed dialog context. After fine-tuning, our\napproach can consistently improve the performance of various models across\nmultiple metrics. Experimental results on public benchmark demonstrate the\neffectiveness and practicability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaowen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xingyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023. (arXiv:2309.15554v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15554","description":"<p>This paper describes the FBK's participation in the Simultaneous Translation\nand Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our\nsubmission focused on the use of direct architectures to perform both tasks:\nfor the simultaneous one, we leveraged the knowledge already acquired by\noffline-trained models and directly applied a policy to obtain the real-time\ninference; for the subtitling one, we adapted the direct ST model to produce\nwell-formed subtitles and exploited the same architecture to produce timestamps\nneeded for the subtitle synchronization with audiovisual content. Our\nEnglish-German SimulST system shows a reduced computational-aware latency\ncompared to the one achieved by the top-ranked systems in the 2021 and 2022\nrounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling\nsystem outperforms the only existing solution based on a direct system by 3.7\nand 1.7 SubER in English-German and English-Spanish respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])","link":"http://arxiv.org/abs/2309.15564","description":"<p>In recent years, advances in the large-scale pretraining of language and\ntext-to-image models have revolutionized the field of machine learning. Yet,\nintegrating these two modalities into a single, robust model capable of\ngenerating seamless multimodal outputs remains a significant challenge. To\naddress this gap, we present the Joint Autoregressive Mixture (JAM) framework,\na modular approach that systematically fuses existing text and image generation\nmodels. We also introduce a specialized, data-efficient instruction-tuning\nstrategy, tailored for mixed-modal generation tasks. Our final instruct-tuned\nmodel demonstrates unparalleled performance in generating high-quality\nmultimodal outputs and represents the first model explicitly designed for this\npurpose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aiello_E/0/1/0/all/0/1\">Emanuele Aiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lili Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Multi-Label Aspect Category Detection Utilizing Prototypical Network with Sentence-Level Weighting and Label Augmentation. (arXiv:2309.15588v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15588","description":"<p>Multi-label aspect category detection is intended to detect multiple aspect\ncategories occurring in a given sentence. Since aspect category detection often\nsuffers from limited datasets and data sparsity, the prototypical network with\nattention mechanisms has been applied for few-shot aspect category detection.\nNevertheless, most of the prototypical networks used so far calculate the\nprototypes by taking the mean value of all the instances in the support set.\nThis seems to ignore the variations between instances in multi-label aspect\ncategory detection. Also, several related works utilize label text information\nto enhance the attention mechanism. However, the label text information is\noften short and limited, and not specific enough to discern categories. In this\npaper, we first introduce support set attention along with the augmented label\ninformation to mitigate the noise at word-level for each support set instance.\nMoreover, we use a sentence-level attention mechanism that gives different\nweights to each instance in the support set in order to compute prototypes by\nweighted averaging. Finally, the calculated prototypes are further used in\nconjunction with query instances to compute query attention and thereby\neliminate noises from the query set. Experimental results on the Yelp dataset\nshow that our proposed method is useful and outperforms all baselines in four\ndifferent scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaihara_M/0/1/0/all/0/1\">Mizuho Iwaihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15609","description":"<p>This paper presents an end-to-end solution for the creation of fully\nautomated conference meeting transcripts and their machine translations into\nvarious languages. This tool has been developed at the World Intellectual\nProperty Organization (WIPO) using in-house developed speech-to-text (S2T) and\nmachine translation (MT) components. Beyond describing data collection and\nfine-tuning, resulting in a highly customized and robust system, this paper\ndescribes the architecture and evolution of the technical components as well as\nhighlights the business impact and benefits from the user side. We also point\nout particular challenges in the evolution and adoption of the system and how\nthe new approach created a new product and replaced existing established\nworkflows in conference management documentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dewan_A/0/1/0/all/0/1\">Akshat Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziemski_M/0/1/0/all/0/1\">Michal Ziemski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meylan_H/0/1/0/all/0/1\">Henri Meylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Concina_L/0/1/0/all/0/1\">Lorenzo Concina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouliquen_B/0/1/0/all/0/1\">Bruno Pouliquen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15630","description":"<p>Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15649","description":"<p>We explore the ability of large language models (LLMs) to act as ASR\npost-processors that perform rescoring and error correction. Our focus is on\ninstruction prompting to let LLMs perform these task without fine-tuning, for\nwhich we evaluate different prompting schemes, both zero- and few-shot\nin-context learning, and a novel task-activating prompting (TAP) method that\ncombines instruction and demonstration. Using a pre-trained first-pass system\nand rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that\nrescoring only by in-context learning with frozen LLMs achieves results that\nare competitive with rescoring by domain-tuned LMs. By combining prompting\ntechniques with fine-tuning we achieve error rates below the N-best oracle\nlevel, showcasing the generalization power of the LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi-Chieh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis. (arXiv:2309.15656v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15656","description":"<p>Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, the linguistic\ncharacteristics of those dialogues are notably different from those observed in\ncorpora of spontaneous interactions. This difference is particularly marked for\ncommunicative feedback and grounding phenomena such as backchannels,\nacknowledgments, or clarification requests. Such signals are known to\nconstitute a key part of the conversation flow and are used by the dialogue\nparticipants to provide feedback to one another on their perception of the\nongoing interaction. This paper presents a quantitative analysis of such\ncommunicative feedback phenomena in both subtitles and spontaneous\nconversations. Based on dialogue data in English, French, German, Hungarian,\nItalian, Japanese, Norwegian and Chinese, we extract both lexical statistics\nand classification outputs obtained with a neural dialogue act tagger. Two main\nfindings of this empirical study are that (1) conversational feedback is\nmarkedly less frequent in subtitles than in spontaneous dialogues and (2)\nsubtitles contain a higher proportion of negative feedback. Furthermore, we\nshow that dialogue responses generated by large language models also follow the\nsame underlying trends and include comparatively few occurrences of\ncommunicative feedback, except when those models are explicitly fine-tuned on\nspontaneous dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilan_I/0/1/0/all/0/1\">Ildik&#xf3; Pil&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevot_L/0/1/0/all/0/1\">Laurent Pr&#xe9;vot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschmeier_H/0/1/0/all/0/1\">Hendrik Buschmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])","link":"http://arxiv.org/abs/2309.15670","description":"<p>In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banshal_S/0/1/0/all/0/1\">Sumit Kumar Banshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sajal Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shammi_S/0/1/0/all/0/1\">Shumaiya Akter Shammi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_N/0/1/0/all/0/1\">Narayan Ranjan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech collage: code-switched audio generation by collaging monolingual corpora. (arXiv:2309.15674v1 [cs.SD])","link":"http://arxiv.org/abs/2309.15674","description":"<p>Designing effective automatic speech recognition (ASR) systems for\nCode-Switching (CS) often depends on the availability of the transcribed CS\nresources. To address data scarcity, this paper introduces Speech Collage, a\nmethod that synthesizes CS data from monolingual corpora by splicing audio\nsegments. We further improve the smoothness quality of audio generation using\nan overlap-add approach. We investigate the impact of generated data on speech\nrecognition in two scenarios: using in-domain CS text and a zero-shot approach\nwith synthesized CS text. Empirical results highlight up to 34.4% and 16.2%\nrelative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and\nzero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation\nbolsters the model's code-switching inclination and reduces its monolingual\nbias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeinali_D/0/1/0/all/0/1\">Dorsa Zeinali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ond&#x159;ej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization. (arXiv:2309.15686v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15686","description":"<p>Incorporating longer context has been shown to benefit machine translation,\nbut the inclusion of context in end-to-end speech translation (E2E-ST) remains\nunder-studied. To bridge this gap, we introduce target language context in\nE2E-ST, enhancing coherence and overcoming memory constraints of extended audio\nsegments. Additionally, we propose context dropout to ensure robustness to the\nabsence of context, and further improve performance by adding speaker\ninformation. Our proposed contextual E2E-ST outperforms the isolated\nutterance-based E2E-ST approach. Lastly, we demonstrate that in conversational\nspeech, contextual information primarily contributes to capturing context\nstyle, as well as resolving anaphora and named entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15701","description":"<p>Advancements in deep neural networks have allowed automatic speech\nrecognition (ASR) systems to attain human parity on several publicly available\nclean speech datasets. However, even state-of-the-art ASR systems experience\nperformance degradation when confronted with adverse conditions, as a\nwell-trained acoustic model is sensitive to variations in the speech domain,\ne.g., background noise. Intuitively, humans address this issue by relying on\ntheir linguistic knowledge: the meaning of ambiguous spoken terms is usually\ninferred from contextual cues thereby reducing the dependency on the auditory\nsystem. Inspired by this observation, we introduce the first open-source\nbenchmark to utilize external large language models (LLMs) for ASR error\ncorrection, where N-best decoding hypotheses provide informative elements for\ntrue transcription prediction. This approach is a paradigm shift from the\ntraditional language model rescoring strategy that can only select one\ncandidate hypothesis as the output transcription. The proposed benchmark\ncontains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs\nof N-best hypotheses and corresponding accurate transcriptions across prevalent\nspeech domains. Given this dataset, we examine three types of error correction\ntechniques based on LLMs with varying amounts of labeled\nhypotheses-transcription pairs, which gains a significant word error rate (WER)\nreduction. Experimental evidence demonstrates the proposed technique achieves a\nbreakthrough by surpassing the upper bound of traditional re-ranking based\nmethods. More surprisingly, LLM with reasonable prompt and its generative\ncapability can even correct those tokens that are missing in N-best list. We\nmake our results publicly accessible for reproducible pipelines with released\npre-trained models, thus providing a new evaluation paradigm for ASR error\ncorrection with LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Macro Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15714","description":"<p>With the recent explosion of large language models (LLMs), such as Generative\nPretrained Transformers (GPT), the need to understand the ability of humans and\nmachines to comprehend semantic language meaning has entered a new phase. This\nrequires interdisciplinary research that bridges the fields of cognitive\nscience and natural language processing (NLP). This pilot study aims to provide\ninsights into individuals' neural states during a semantic relation\nreading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and\nelectroencephalographic (EEG) data to study how the brain processes words with\nvarying degrees of relevance to a keyword during reading. We also use a feature\nengineering approach to improve the fixation-related EEG data classification\nwhile participants read words with high versus low relevance to the keyword.\nThe best validation accuracy in this word-level classification is over 60\\%\nacross 12 subjects. Words of high relevance to the inference keyword had\nsignificantly more eye fixations per word: 1.0584 compared to 0.6576 when\nexcluding no-fixation words, and 1.5126 compared to 1.4026 when including them.\nThis study represents the first attempt to classify brain states at a word\nlevel using LLM knowledge. It provides valuable insights into human cognitive\nabilities and the realm of Artificial General Intelligence (AGI), and offers\nguidance for developing potential reading-assisted technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahata_S/0/1/0/all/0/1\">Sujal Nahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tasnia Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shih-kuen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cauwenberghs_G/0/1/0/all/0/1\">Gert Cauwenberghs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_T/0/1/0/all/0/1\">Tzyy-Ping Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization. (arXiv:2309.15739v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15739","description":"<p>With the advancement of telemedicine, both researchers and medical\npractitioners are working hand-in-hand to develop various techniques to\nautomate various medical operations, such as diagnosis report generation. In\nthis paper, we first present a multi-modal clinical conversation summary\ngeneration task that takes a clinician-patient interaction (both textual and\nvisual information) and generates a succinct synopsis of the conversation. We\npropose a knowledge-infused, multi-modal, multi-tasking medical domain\nidentification and clinical conversation summary generation\n(MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and\nvisual features and unify the fused feature vector using a gated mechanism.\nFurthermore, we developed a multi-modal, multi-intent clinical conversation\nsummarization corpus annotated with intent, symptom, and summary. The extensive\nset of experiments, both quantitatively and qualitatively, led to the following\nfindings: (a) critical significance of visuals, (b) more precise and medical\nentity preserving summary with additional knowledge infusion, and (c) a\ncorrelation between medical department identification and clinical synopsis\ngeneration. Furthermore, the dataset and source code are available at\nhttps://github.com/NLP-RL/MM-CliConSummation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Abhisek Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Anisha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1\">Minakshi Dhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question answering using deep learning in low resource Indian language Marathi. (arXiv:2309.15779v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15779","description":"<p>Precise answers are extracted from a text for a given input question in a\nquestion answering system. Marathi question answering system is created in\nrecent studies by using ontology, rule base and machine learning based\napproaches. Recently transformer models and transfer learning approaches are\nused to solve question answering challenges. In this paper we investigate\ndifferent transformer models for creating a reading comprehension-based Marathi\nquestion answering system. We have experimented on different pretrained Marathi\nlanguage multilingual and monolingual models like Multilingual Representations\nfor Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder\nRepresentations from Transformers (IndicBERT) and fine-tuned it on a Marathi\nreading comprehension-based data set. We got the best accuracy in a MuRIL\nmultilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning\nthe model on the Marathi dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_D/0/1/0/all/0/1\">Dhiraj Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govilkar_S/0/1/0/all/0/1\">Sharvari Govilkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Sagar Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Routing with Benchmark Datasets. (arXiv:2309.15789v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15789","description":"<p>There is a rapidly growing number of open-source Large Language Models (LLMs)\nand benchmark datasets to compare them. While some models dominate these\nbenchmarks, no single model typically achieves the best accuracy in all tasks\nand use cases. In this work, we address the challenge of selecting the best LLM\nout of a collection of models for new tasks. We propose a new formulation for\nthe problem, in which benchmark datasets are repurposed to learn a \"router\"\nmodel for this LLM selection, and we show that this problem can be reduced to a\ncollection of binary classification tasks. We demonstrate the utility and\nlimitations of learning model routers from various benchmark datasets, where we\nconsistently improve performance upon using any single model for all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shnitzer_T/0/1/0/all/0/1\">Tal Shnitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_A/0/1/0/all/0/1\">Anthony Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">M&#xed;rian Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soule_K/0/1/0/all/0/1\">Kate Soule</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuekai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_N/0/1/0/all/0/1\">Neil Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition. (arXiv:2309.15796v1 [eess.AS])","link":"http://arxiv.org/abs/2309.15796","description":"<p>Training automatic speech recognition (ASR) systems requires large amounts of\nwell-curated paired data. However, human annotators usually perform\n\"non-verbatim\" transcription, which can result in poorly trained models. In\nthis paper, we propose Omni-temporal Classification (OTC), a novel training\ncriterion that explicitly incorporates label uncertainties originating from\nsuch weak supervision. This allows the model to effectively learn speech-text\nalignments while accommodating errors present in the training transcripts. OTC\nextends the conventional CTC objective for imperfect transcripts by leveraging\nweighted finite state transducers. Through experiments conducted on the\nLibriSpeech and LibriVox datasets, we demonstrate that training ASR models with\nOTC avoids performance degradation even with transcripts containing up to 70%\nerrors, a scenario where CTC models fail completely. Our implementation is\navailable at https://github.com/k2-fsa/icefall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_D/0/1/0/all/0/1\">Dongji Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hainan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_D/0/1/0/all/0/1\">Desh Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perera_L/0/1/0/all/0/1\">Leibny Paola Garcia Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study. (arXiv:2309.15800v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15800","description":"<p>Speech signals, typically sampled at rates in the tens of thousands per\nsecond, contain redundancies, evoking inefficiencies in sequence modeling.\nHigh-dimensional speech features such as spectrograms are often used as the\ninput for the subsequent model. However, they can still be redundant. Recent\ninvestigations proposed the use of discrete speech units derived from\nself-supervised learning representations, which significantly compresses the\nsize of speech data. Applying various methods, such as de-duplication and\nsubword modeling, can further compress the speech sequence length. Hence,\ntraining time is significantly reduced while retaining notable performance. In\nthis study, we undertake a comprehensive and systematic exploration into the\napplication of discrete units within end-to-end speech processing models.\nExperiments on 12 automatic speech recognition, 3 speech translation, and 1\nspoken language understanding corpora demonstrate that discrete units achieve\nreasonably good results in almost all the settings. We intend to release our\nconfigurations and trained models to foster future research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jeeweon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yichen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuya Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maekaku_T/0/1/0/all/0/1\">Takashi Maekaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saijo_K/0/1/0/all/0/1\">Kohei Saijo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsiu-Hsuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15806","description":"<p>Large Language Models (LLMs) present an intriguing avenue for exploration in\nthe field of formal theorem proving. Nevertheless, their full potential,\nparticularly concerning the mitigation of hallucinations and refinement through\nprover error messages, remains an area that has yet to be thoroughly\ninvestigated. To enhance the effectiveness of LLMs in the field, we introduce\nthe Lyra, a new framework that employs two distinct correction mechanisms: Tool\nCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction in\nthe post-processing of formal proofs, we leverage prior knowledge to utilize\npredefined prover tools (e.g., Sledgehammer) for guiding the replacement of\nincorrect tools. Tool Correction significantly contributes to mitigating\nhallucinations, thereby improving the overall accuracy of the proof. In\naddition, we introduce Conjecture Correction, an error feedback mechanism\ndesigned to interact with prover to refine formal proof conjectures with prover\nerror messages. Compared to the previous refinement framework, the proposed\nConjecture Correction refines generation with instruction but does not collect\npaired (generation, error &amp; refinement) prompts. Our method has achieved\nstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)\nand test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. We\nbelieve Tool Correction (post-process for hallucination mitigation) and\nConjecture Correction (subgoal adjustment from interaction with environment)\ncould provide a promising avenue for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1\">Huajian Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])","link":"http://arxiv.org/abs/2309.15817","description":"<p>Recent advances in Language Model (LM) agents and tool use, exemplified by\napplications like ChatGPT Plugins, enable a rich set of capabilities but also\namplify potential risks - such as leaking private data or causing financial\nlosses. Identifying these risks is labor-intensive, necessitating implementing\nthe tools, manually setting up the environment for each test scenario, and\nfinding risky cases. As tools and agents become more complex, the high cost of\ntesting these agents will make it increasingly difficult to find high-stakes,\nlong-tailed risks. To address these challenges, we introduce ToolEmu: a\nframework that uses an LM to emulate tool execution and enables the testing of\nLM agents against a diverse range of tools and scenarios, without manual\ninstantiation. Alongside the emulator, we develop an LM-based automatic safety\nevaluator that examines agent failures and quantifies associated risks. We test\nboth the tool emulator and evaluator through human evaluation and find that\n68.8% of failures identified with ToolEmu would be valid real-world agent\nfailures. Using our curated initial benchmark consisting of 36 high-stakes\ntools and 144 test cases, we provide a quantitative risk analysis of current LM\nagents and identify numerous failures with potentially severe outcomes.\nNotably, even the safest LM agent exhibits such failures 23.9% of the time\naccording to our evaluator, underscoring the need to develop safer LM agents\nfor real-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yangjun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Honghua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitis_S/0/1/0/all/0/1\">Silviu Pitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongchao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing. (arXiv:2309.15826v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15826","description":"<p>Recent works in end-to-end speech-to-text translation (ST) have proposed\nmulti-tasking methods with soft parameter sharing which leverage machine\ntranslation (MT) data via secondary encoders that map text inputs to an\neventual cross-modal representation. In this work, we instead propose a ST/MT\nmulti-tasking framework with hard parameter sharing in which all model\nparameters are shared cross-modally. Our method reduces the speech-text\nmodality gap via a pre-processing stage which converts speech and text inputs\ninto two discrete token sequences of similar length -- this allows models to\nindiscriminately process both modalities simply using a joint vocabulary. With\nexperiments on MuST-C, we demonstrate that our multi-tasking framework improves\nattentional encoder-decoder, Connectionist Temporal Classification (CTC),\ntransducer, and joint CTC/attention models by an average of +0.5 BLEU without\nany external MT data. Further, we show that this framework incorporates\nexternal MT data, yielding +0.8 BLEU, and also improves transfer learning from\npre-trained textual models, yielding +1.8 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuya Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How We Define Harm Impacts Data Annotations: Explaining How Annotators Distinguish Hateful, Offensive, and Toxic Comments. (arXiv:2309.15827v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15827","description":"<p>Computational social science research has made advances in machine learning\nand natural language processing that support content moderators in detecting\nharmful content. These advances often rely on training datasets annotated by\ncrowdworkers for harmful content. In designing instructions for annotation\ntasks to generate training data for these algorithms, researchers often treat\nthe harm concepts that we train algorithms to detect - 'hateful', 'offensive',\n'toxic', 'racist', 'sexist', etc. - as interchangeable. In this work, we\nstudied whether the way that researchers define 'harm' affects annotation\noutcomes. Using Venn diagrams, information gain comparisons, and content\nanalyses, we reveal that annotators do not use the concepts 'hateful',\n'offensive', and 'toxic' interchangeably. We identify that features of harm\ndefinitions and annotators' individual characteristics explain much of how\nannotators use these terms differently. Our results offer empirical evidence\ndiscouraging the common practice of using harm concepts interchangeably in\ncontent moderation research. Instead, researchers should make specific choices\nabout which harm concepts to analyze based on their research goals. Recognizing\nthat researchers are often resource constrained, we also encourage researchers\nto provide information to bound their findings when their concepts of interest\ndiffer from concepts that off-the-shelf harmful content detection algorithms\nidentify. Finally, we encourage algorithm providers to ensure their instruments\ncan adapt to contextually-specific content detection goals (e.g., soliciting\ninstrument users' feedback).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopke_Gonzalez_A/0/1/0/all/0/1\">Angela Sch&#xf6;pke-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sagar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnick_P/0/1/0/all/0/1\">Paul J. Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15840","description":"<p>Large language models (LLMs) can \"lie\", which we define as outputting false\nstatements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n\"lie\", for example, when instructed to output misinformation. Here, we develop\na simple lie detector that requires neither access to the LLM's activations\n(black-box) nor ground-truth knowledge of the fact in question. The detector\nworks by asking a predefined set of unrelated follow-up questions after a\nsuspected lie, and feeding the LLM's yes/no answers into a logistic regression\nclassifier. Despite its simplicity, this lie detector is highly accurate and\nsurprisingly general. When trained on examples from a single setting --\nprompting GPT-3.5 to lie about factual questions -- the detector generalises\nout-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\nsales. These results indicate that LLMs have distinctive lie-related\nbehavioural patterns, consistent across architectures and contexts, which could\nenable general-purpose lie detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pacchiardi_L/0/1/0/all/0/1\">Lorenzo Pacchiardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alex J. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscovitz_I/0/1/0/all/0/1\">Ilan Moscovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexa Y. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disinformation Detection: An Evolving Challenge in the Age of LLMs. (arXiv:2309.15847v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15847","description":"<p>The advent of generative Large Language Models (LLMs) such as ChatGPT has\ncatalyzed transformative advancements across multiple domains. However,\nalongside these advancements, they have also introduced potential threats. One\ncritical concern is the misuse of LLMs by disinformation spreaders, leveraging\nthese models to generate highly persuasive yet misleading content that\nchallenges the disinformation detection system. This work aims to address this\nissue by answering three research questions: (1) To what extent can the current\ndisinformation detection technique reliably detect LLM-generated\ndisinformation? (2) If traditional techniques prove less effective, can LLMs\nthemself be exploited to serve as a robust defense against advanced\ndisinformation? and, (3) Should both these strategies falter, what novel\napproaches can be proposed to counter this burgeoning threat effectively? A\nholistic exploration for the formation and detection of disinformation is\nconducted to foster this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bohan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nirmal_A/0/1/0/all/0/1\">Ayushi Nirmal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned Reinforcement Learning. (arXiv:2209.12758v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.12758","description":"<p>Teaching an agent to perform new tasks using natural language can easily be\nhindered by ambiguities in interpretation. When a teacher provides an\ninstruction to a learner about an object by referring to its features, the\nlearner can misunderstand the teacher's intentions, for instance if the\ninstruction ambiguously refer to features of the object, a phenomenon called\nreferential ambiguity. We study how two concepts derived from cognitive\nsciences can help resolve those referential ambiguities: pedagogy (selecting\nthe right instructions) and pragmatism (learning the preferences of the other\nagents using inductive reasoning). We apply those ideas to a teacher/learner\nsetup with two artificial agents on a simulated robotic task (block-stacking).\nWe show that these concepts improve sample efficiency for training the learner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1\">Hugo Caselles-Dupr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_M/0/1/0/all/0/1\">Mohamed Chetouani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Late Audio-Visual Fusion for In-The-Wild Speaker Diarization. (arXiv:2211.01299v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.01299","description":"<p>Speaker diarization is well studied for constrained audios but little\nexplored for challenging in-the-wild videos, which have more speakers, shorter\nutterances, and inconsistent on-screen speakers. We address this gap by\nproposing an audio-visual diarization model which combines audio-only and\nvisual-centric sub-systems via late fusion. For audio, we show that an\nattractor-based end-to-end system (EEND-EDA) performs remarkably well when\ntrained with our proposed recipe of a simulated proxy dataset, and propose an\nimproved version, EEND-EDA++, that uses attention in decoding and a speaker\nrecognition loss during training to better handle the larger number of\nspeakers. The visual-centric sub-system leverages facial attributes and\nlip-audio synchrony for identity and speech activity estimation of on-screen\nspeakers. Both sub-systems surpass the state of the art (SOTA) by a large\nmargin, with the fused audio-visual system achieving a new SOTA on the AVA-AVD\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zexu Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wichern_G/0/1/0/all/0/1\">Gordon Wichern</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Germain_F/0/1/0/all/0/1\">Fran&#xe7;ois G. Germain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subramanian_A/0/1/0/all/0/1\">Aswin Subramanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Neo for commonsense reasoning -- a theoretical and practical lens. (arXiv:2211.15593v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15593","description":"<p>Recent work has demonstrated substantial gains in pre-training large-language\nmodels (LLMs) followed by supervised fine-tuning on the downstream task. In\nthis paper, we evaluate the performance of the GPT-neo model using $6$\ncommonsense reasoning benchmark tasks. We aim to examine the performance of\nsmaller models using the GPT-neo models against several larger model baselines\nsuch as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the\nappropriate set of hyperparameters, our model achieves competitive accuracy on\nseveral tasks. We also investigate and substantiate our results using\nattention-head visualization to better understand the model performance.\nFinally, we conduct various robustness tests using various methods to gauge the\nmodel performance under numerous settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_R/0/1/0/all/0/1\">Rohan Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_V/0/1/0/all/0/1\">Vivek Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P%2E_N/0/1/0/all/0/1\">Narendra C.P.</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning System for Domain-specific Speech Recognition. (arXiv:2303.10510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10510","description":"<p>As human-machine voice interfaces provide easy access to increasingly\nintelligent machines, many state-of-the-art automatic speech recognition (ASR)\nsystems are proposed. However, commercial ASR systems usually have poor\nperformance on domain-specific speech especially under low-resource settings.\nThe author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to\ndevelop benefit-specific ASR systems. The domain-specific data are collected\nusing proposed semi-supervised learning annotation with little human\nintervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60\nacoustic model with an external KenLM, which surpasses the Google and AWS ASR\nsystems on benefit-specific speech. The viability of using error prone ASR\ntranscriptions as part of spoken language understanding (SLU) is also\ninvestigated. Results of a benefit-specific natural language understanding\n(NLU) task show that the domain-specific fine-tuned ASR system can outperform\nthe commercial ASR systems even when its transcriptions have higher word error\nrate (WER), and the results between fine-tuned ASR and human transcriptions are\nsimilar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yanan Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Used to Estimate the Latent Positions of Politicians. (arXiv:2303.12057v4 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2303.12057","description":"<p>Existing approaches to estimating politicians' latent positions along\nspecific dimensions often fail when relevant data is limited. We leverage the\nembedded knowledge in generative large language models (LLMs) to address this\nchallenge and measure lawmakers' positions along specific political or policy\ndimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare\nlawmakers and then scale the resulting graph using the Bradley-Terry model. We\nestimate novel measures of U.S. senators' positions on liberal-conservative\nideology, gun control, and abortion. Our liberal-conservative scale, used to\nvalidate LLM-driven scaling, strongly correlates with existing measures and\noffsets interpretive gaps, suggesting LLMs synthesize relevant data from\ninternet and digitized media rather than memorizing existing measures. Our gun\ncontrol and abortion measures -- the first of their kind -- differ from the\nliberal-conservative scale in face-valid ways and predict interest group\nratings and legislator votes better than ideology alone. Our findings suggest\nLLMs hold promise for solving complex social science measurement problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messing_S/0/1/0/all/0/1\">Solomon Messing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers. (arXiv:2306.06531v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2306.06531","description":"<p>For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkin_J/0/1/0/all/0/1\">Jacob Arkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_C/0/1/0/all/0/1\">Charles Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1\">Nicholas Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuchu Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13840","description":"<p>Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alycia Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundar_S/0/1/0/all/0/1\">Sudharsan Sundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. (arXiv:2307.07683v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2307.07683","description":"<p>Synthetic-voice cloning technologies have seen significant advances in recent\nyears, giving rise to a range of potential harms. From small- and large-scale\nfinancial fraud to disinformation campaigns, the need for reliable methods to\ndifferentiate real and synthesized voices is imperative. We describe three\ntechniques for differentiating a real from a cloned voice designed to\nimpersonate a specific person. These three approaches differ in their feature\nextraction stage with low-dimensional perceptual features offering high\ninterpretability but lower accuracy, to generic spectral features, and\nend-to-end learned features offering less interpretability but higher accuracy.\nWe show the efficacy of these approaches when trained on a single speaker's\nvoice and when trained on multiple voices. The learned features consistently\nyield an equal error rate between 0% and 4%, and are reasonably robust to\nadversarial laundering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barrington_S/0/1/0/all/0/1\">Sarah Barrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_R/0/1/0/all/0/1\">Romit Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koorma_G/0/1/0/all/0/1\">Gautham Koorma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1\">Hany Farid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.15593","description":"<p>We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1\">Rohith Kuditipudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Beyond Identification: Multi-bit Watermark for Large Language Models. (arXiv:2308.00221v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00221","description":"<p>We propose a method to tackle misuses of large language models beyond the\nidentification of machine-generated text. While existing methods focus on\ndetection, some malicious misuses demand tracing the adversary user for\ncounteracting them. To address this, we propose Multi-bit Watermark via\nPosition Allocation, embedding traceable multi-bit information during language\nmodel generation. Leveraging the benefits of zero-bit watermarking, our method\nenables robust extraction of the watermark without any model access, embedding\nand extraction of long messages ($\\geq$ 32-bit) without finetuning, and\nmaintaining text quality, while allowing zero-bit detection all at the same\ntime. Moreover, our watermark is relatively robust under strong attacks like\ninterleaving human texts and paraphrasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.12877","description":"<p>This paper outlines the performance evaluation of a system for adverse drug\nevent normalization, developed by the Data Science for Digital Health (DS4DH)\ngroup for the Social Media Mining for Health Applications (SMM4H) 2023 shared\ntask 5. Shared task 5 targeted the normalization of adverse drug event mentions\nin Twitter to standard concepts of the Medical Dictionary for Regulatory\nActivities terminology. Our system hinges on a two-stage approach: BERT\nfine-tuning for entity recognition, followed by zero-shot normalization using\nsentence transformers and reciprocal-rank fusion. The approach yielded a\nprecision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed\nthe median performance in shared task 5 by 10% and demonstrated the highest\nperformance among all participants. These results substantiate the\neffectiveness of our approach and its potential application for adverse drug\nevent normalization in the realm of social media text mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Anthony Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1\">Hossein Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_D/0/1/0/all/0/1\">David Vicente Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodoro_D/0/1/0/all/0/1\">Douglas Teodoro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Speech Representation From Contrastive Token-Acoustic Pretraining. (arXiv:2309.00424v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.00424","description":"<p>For fine-grained generation and recognition tasks such as\nminimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic\nspeech recognition (ASR), the intermediate representations extracted from\nspeech should serve as a \"bridge\" between text and acoustic information,\ncontaining information from both modalities. The semantic content is\nemphasized, while the paralinguistic information such as speaker identity and\nacoustic details should be de-emphasized. However, existing methods for\nextracting fine-grained intermediate representations from speech suffer from\nissues of excessive redundancy and dimension explosion. Contrastive learning is\na good method for modeling intermediate representations from two modalities.\nHowever, existing contrastive learning methods in the audio field focus on\nextracting global descriptive information for downstream audio classification\ntasks, making them unsuitable for TTS, VC, and ASR tasks. To address these\nissues, we propose a method named \"Contrastive Token-Acoustic Pretraining\n(CTAP)\", which uses two encoders to bring phoneme and speech into a joint\nmultimodal space, learning how to connect phoneme and speech at the frame\nlevel. The CTAP model is trained on 210k speech and phoneme pairs, achieving\nminimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a\npromising solution for fine-grained generation and recognition downstream tasks\nin speech processing. We provide a website with audio samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yixin Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.02427","description":"<p>Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07311","description":"<p>Most interpretability research in NLP focuses on understanding the behavior\nand features of a fully trained model. However, certain insights into model\nbehavior may only be accessible by observing the trajectory of the training\nprocess. In this paper, we present a case study of syntax acquisition in masked\nlanguage models (MLMs). Our findings demonstrate how analyzing the evolution of\ninterpretable artifacts throughout training deepens our understanding of\nemergent behavior. In particular, we study Syntactic Attention Structure (SAS),\na naturally emerging property of MLMs wherein specific Transformer heads tend\nto focus on specific syntactic relations. We identify a brief window in\ntraining when models abruptly acquire SAS and find that this window is\nconcurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent\nacquisition of linguistic capabilities. We then examine the causal role of SAS\nby introducing a regularizer to manipulate SAS during training, and demonstrate\nthat SAS is necessary for the development of grammatical capabilities. We\nfurther find that SAS competes with other beneficial traits and capabilities\nduring training, and that briefly suppressing SAS can improve model quality.\nThese findings reveal a real-world example of the relationship between\ndisadvantageous simplicity bias and interpretable breakthrough training\ndynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1\">Ravid Shwartz-Ziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leavitt_M/0/1/0/all/0/1\">Matthew L. Leavitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. (arXiv:2309.10677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10677","description":"<p>Data contamination in model evaluation is getting increasingly prevalent as\nthe massive training corpora of large language models often unintentionally\ninclude benchmark samples. Therefore, contamination analysis has became an\ninevitable part of reliable model evaluation. However, existing method of\ncontamination analysis requires the access of the entire training data which is\noften confidential for recent models. This prevent the community to rigorously\naudit these models and conduct accurate assessment of their capability. In this\npaper, we propose a novel method to quantify contamination without the access\nof the full training set, that measure the extent of contamination with\nperplexity. Our analysis provides evidence of significant memorisation of\nrecent foundation models in popular reading comprehension, summarisation\nbenchmarks, while multiple choice appears less contaminated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10966","description":"<p>Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that MAP decoding is not optimal, because model probabilities\ndo not always align with human preferences. Stronger decoding methods,\nincluding Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\ndecoding, have since been proposed to mitigate the model-perplexity-vs-quality\nmismatch. While these decoding methods achieve state-of-the-art performance,\nthey are prohibitively expensive to compute. In this work, we propose MBR\nfinetuning and QE finetuning which distill the quality gains from these\ndecoding methods at training time, while using an efficient decoding algorithm\nat inference time. Using the canonical NLG task of Neural Machine Translation\n(NMT), we show that even with self-training, these finetuning methods\nsignificantly outperform the base model. Moreover, when using an external LLM\nas a teacher model, these finetuning methods outperform finetuning on\nhuman-generated references. These findings suggest new ways to leverage\nmonolingual data to achieve improvements in model quality that are on par with,\nor even exceed, improvements from human-curated data, while maintaining maximum\nefficiency during decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1\">Mara Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11166","description":"<p>The swift advancement in the scales and capabilities of Large Language Models\n(LLMs) positions them as promising tools for a variety of downstream tasks. In\naddition to the pursuit of better performance and the avoidance of violent\nfeedback on a certain prompt, to ensure the responsibility of the LLM, much\nattention is drawn to the robustness of LLMs. However, existing evaluation\nmethods mostly rely on traditional question answering datasets with predefined\nsupervised labels, which do not align with the superior generation capabilities\nof contemporary LLMs. To address this issue, we propose a novel rational\nevaluation approach that leverages pre-trained reward models as diagnostic\ntools to evaluate the longer conversation generated from more challenging open\nquestions by LLMs, which we refer to as the Reward Model for Reasonable\nRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensive\ngrasp of language models in terms of their proficiency in understanding\nquestions, a capability not entirely encompassed by individual words or\nletters, which may exhibit oversimplification and inherent biases. Our\nextensive empirical experiments demonstrate that TREvaL provides an innovative\nmethod for evaluating the robustness of an LLM. Furthermore, our results\ndemonstrate that LLMs frequently exhibit vulnerability to word-level\nperturbations that are commonplace in daily language usage. Notably, we are\nsurprised to discover that robustness tends to decrease as fine-tuning (SFT and\nRLHF) is conducted. The code of TREval is available in\nhttps://github.com/Harry-mic/TREvaL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guozheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_N/0/1/0/all/0/1\">Ning Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Suwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yongzhe Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features. (arXiv:2309.11791v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2309.11791","description":"<p>Wikipedia articles are hierarchically organized through categories and lists,\nproviding one of the most comprehensive and universal taxonomy, but its open\ncreation is causing redundancies and inconsistencies. Assigning DBPedia classes\nto Wikipedia categories and lists can alleviate the problem, realizing a large\nknowledge graph which is essential for categorizing digital contents through\nentity linking and typing. However, the existing approach of CaLiGraph is\nproducing incomplete and non-fine grained mappings. In this paper, we tackle\nthe problem as ontology alignment, where structural information of knowledge\ngraphs and lexical and semantic features of ontology class names are utilized\nto discover confident mappings, which are in turn utilized for finetuing\npretrained language models in a distant supervision fashion. Our method SLHCat\nconsists of two main parts: 1) Automatically generating training data by\nleveraging knowledge graph structure, semantic similarities, and named entity\ntyping. 2) Finetuning and prompt-tuning of the pre-trained language model BERT\nare carried out over the training data, to capture semantic and syntactic\nproperties of class names. Our model SLHCat is evaluated over a benchmark\ndataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping\npairs. SLHCat is outperforming the baseline model by a large margin of 25% in\naccuracy, offering a practical solution for large-scale ontology mapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiaxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaihara_M/0/1/0/all/0/1\">Mizuho Iwaihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning. (arXiv:2309.13701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13701","description":"<p>From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_H/0/1/0/all/0/1\">Hosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Hiteshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betthauser_L/0/1/0/all/0/1\">Leo Betthauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1\">Felipe Vieira Frujeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.14398","description":"<p>Motivational Interviewing (MI) is an approach to therapy that emphasizes\ncollaboration and encourages behavioral change. To evaluate the quality of an\nMI conversation, client utterances can be classified using the MISC code as\neither change talk, sustain talk, or follow/neutral talk. The proportion of\nchange talk in a MI conversation is positively correlated with therapy\noutcomes, making accurate classification of client utterances essential. In\nthis paper, we present a classifier that accurately distinguishes between the\nthree MISC classes (change talk, sustain talk, and follow/neutral talk)\nleveraging multimodal features such as text, prosody, facial expressivity, and\nbody expressivity. To train our model, we perform annotations on the publicly\navailable AnnoMI dataset to collect multimodal information, including text,\naudio, facial expressivity, and body expressivity. Furthermore, we identify the\nmost important modalities in the decision-making process, providing valuable\ninsights into the interplay of different modalities during a MI conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galland_L/0/1/0/all/0/1\">Lucie Galland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelachaud_C/0/1/0/all/0/1\">Catherine Pelachaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pecune_F/0/1/0/all/0/1\">Florian Pecune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2309.06726","description":"<p>Keyphrase generation is a task of identifying a set of phrases that best\nrepre-sent the main topics or themes of a given text. Keyphrases are dividend\nint pre-sent and absent keyphrases. Recent approaches utilizing\nsequence-to-sequence models show effectiveness on absent keyphrase generation.\nHowever, the per-formance is still limited due to the hardness of finding\nabsent keyphrases. In this paper, we propose Keyphrase-Focused BART, which\nexploits the differ-ences between present and absent keyphrase generations, and\nperforms fine-tuning of two separate BART models for present and absent\nkeyphrases. We further show effective approaches of shuffling keyphrases and\ncandidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART\nachieved new state-of-the-art score on F1@5 in two out of five keyphrase\ngen-eration benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaihara_M/0/1/0/all/0/1\">Mizuho Iwaihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2309.14717","description":"<p>Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuhui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhensu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}