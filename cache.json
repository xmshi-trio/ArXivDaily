{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Alzheimer's Diagnosis and Generation-Based Chatbot Using Hierarchical Attention and Transformer. (arXiv:2211.07703v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07703","description":"<p>In this paper, we propose a natural language processing architecture that can\nhandle tasks that previously required two models as one model. With a single\nmodel, we analyze the language patterns and conversational context of\nAlzheimer's patients and derive answers from two results: patient\nclassification and chatbot. If the patient's language characteristics are\nidentified by chatbots in daily life, doctors can plan more precise diagnosis\nand treatment for early diagnosis. The proposed model is used to develop\nchatbots that replace questionnaires that required experts. There are two\nnatural language processing tasks performed by the model. The first is a\n'natural language classification' that indicates with probability whether the\npatient has an illness, and the second is to generate the next 'answer' of the\nchatbot to the patient's answer. In the first half, a context vector, which is\na characteristic of patient utterance, is extracted through a self-attention\nneural network. This context vector and chatbot (expert, moderator) questions\nare entered together into the encoder to obtain a matrix containing the\ncharacteristics of the interaction between the questioner and the patient. The\nvectorized matrix becomes a probability value for classification of patients.\nEnter the matrix into the decoder with the next answer from the chatbot (the\nmoderator) to generate the next utterance. As a result of learning this\nstructure with DmentiaBank's cookie theft description corpus, it was confirmed\nthat the value of the loss function of the encoder and decoder was\nsignificantly reduced and converged. This shows that capturing the speech\nlanguage pattern of Alzheimer's disease patients can contribute to early\ndiagnosis and longitudinal studies of the disease in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeong_P/0/1/0/all/0/1\">Park Jun Yeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_S/0/1/0/all/0/1\">Shin Su Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwan_C/0/1/0/all/0/1\">Choi Chang Hwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jae_L/0/1/0/all/0/1\">Lee Jung Jae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_il_C/0/1/0/all/0/1\">Choi Sang-il</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Approach to Classifying Construction Cost Documents into the International Construction Measurement Standard. (arXiv:2211.07705v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07705","description":"<p>We introduce the first automated models for classifying natural language\ndescriptions provided in cost documents called \"Bills of Quantities\" (BoQs)\npopular in the infrastructure construction industry, into the International\nConstruction Measurement Standard (ICMS). The models we deployed and\nsystematically evaluated for multi-class text classification are learnt from a\ndataset of more than 50 thousand descriptions of items retrieved from 24 large\ninfrastructure construction projects across the United Kingdom. We describe our\napproach to language representation and subsequent modelling to examine the\nstrength of contextual semantics and temporal dependency of language used in\nconstruction project documentation. To do that we evaluate two experimental\npipelines to inferring ICMS codes from text, on the basis of two different\nlanguage representation models and a range of state-of-the-art sequence-based\nclassification methods, including recurrent and convolutional neural network\narchitectures. The findings indicate a highly effective and accurate ICMS\nautomation model is within reach, with reported accuracy results above 90% F1\nscore on average, on 32 ICMS categories. Furthermore, due to the specific\nnature of language use in the BoQs text; short, largely descriptive and\ntechnical, we find that simpler models compare favourably to achieving higher\naccuracy results. Our analysis suggest that information is more likely embedded\nin local key features in the descriptive text, which explains why a simpler\ngeneric temporal convolutional network (TCN) exhibits comparable memory to\nrecurrent architectures with the same capacity, and subsequently outperforms\nthese at this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deza_J/0/1/0/all/0/1\">J. Ignacio Deza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihshaish_H/0/1/0/all/0/1\">Hisham Ihshaish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdjoubi_L/0/1/0/all/0/1\">Lamine Mahdjoubi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incongruity Detection between Bangla News Headline and Body Content through Graph Neural Network. (arXiv:2211.07709v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07709","description":"<p>Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palash_M/0/1/0/all/0/1\">Md Aminul Haque Palash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1\">Kawsarul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahjahan_R/0/1/0/all/0/1\">Ryan Mohammad Bin Shahjahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English. (arXiv:2211.07710v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07710","description":"<p>Automation of on-call customer support relies heavily on accurate and\nefficient speech-to-intent (S2I) systems. Building such systems using\nmulti-component pipelines can pose various challenges because they require\nlarge annotated datasets, have higher latency, and have complex deployment.\nThese pipelines are also prone to compounding errors. To overcome these\nchallenges, we discuss an end-to-end (E2E) S2I model for customer support\nvoicebot task in a bilingual setting. We show how we can solve E2E intent\nclassification by leveraging a pre-trained automatic speech recognition (ASR)\nmodel with slight modification and fine-tuning on small annotated datasets.\nExperimental results show that our best E2E model outperforms a conventional\npipeline by a relative ~27% on the F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Abhinav Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anupam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garera_N/0/1/0/all/0/1\">Nikesh Garera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel Transformer For Multimodal Emotion Recognition. (arXiv:2211.07711v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07711","description":"<p>Multimodal emotion recognition has attracted much attention recently. Fusing\nmultiple modalities effectively with limited labeled data is a challenging\ntask. Considering the success of pre-trained model and fine-grained nature of\nemotion expression, it is reasonable to take these two aspects into\nconsideration. Unlike previous methods that mainly focus on one aspect, we\nintroduce a novel multi-granularity framework, which combines fine-grained\nrepresentation with pre-trained utterance-level representation. Inspired by\nTransformer TTS, we propose a multilevel transformer model to perform\nfine-grained multimodal emotion recognition. Specifically, we explore different\nmethods to incorporate phoneme-level embedding with word-level embedding. To\nperform multi-granularity learning, we simply combine multilevel transformer\nmodel with Albert. Extensive experimental results show that both our multilevel\ntransformer model and multi-granularity model outperform previous\nstate-of-the-art approaches on IEMOCAP dataset with text transcripts and speech\nsignal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meimei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Feng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloning Ideology and Style using Deep Learning. (arXiv:2211.07712v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07712","description":"<p>Text generation tasks have gotten the attention of researchers in the last\nfew years because of their applications on a large scale.In the past, many\nresearchers focused on task-based text generations.Our research focuses on text\ngeneration based on the ideology and style of a specific author, and text\ngeneration on a topic that was not written by the same author in the past.Our\ntrained model requires an input prompt containing initial few words of text to\nproduce a few paragraphs of text based on the ideology and style of the author\non which the model is trained.Our methodology to accomplish this task is based\non Bi-LSTM.The Bi-LSTM model is used to make predictions at the character\nlevel, during the training corpus of a specific author is used along with the\nground truth corpus.A pre-trained model is used to identify the sentences of\nground truth having contradiction with the author's corpus to make our language\nmodel inclined.During training, we have achieved a perplexity score of 2.23 at\nthe character level. The experiments show a perplexity score of around 3 over\nthe test dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beg_D/0/1/0/all/0/1\">Dr. Omer Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Nasir Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_W/0/1/0/all/0/1\">Waleed Anjum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Long Is Enough? Exploring the Optimal Intervals of Long-Range Clinical Note Language Modeling. (arXiv:2211.07713v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07713","description":"<p>Large pre-trained language models (LMs) have been widely adopted in\nbiomedical and clinical domains, introducing many powerful LMs such as bio-lm\nand BioELECTRA. However, the applicability of these methods to real clinical\nuse cases is hindered, due to the limitation of pre-trained LMs in processing\nlong textual data with thousands of words, which is a common length for a\nclinical note. In this work, we explore long-range adaptation from such LMs\nwith Longformer, allowing the LMs to capture longer clinical notes context. We\nconduct experiments on three n2c2 challenges datasets and a longitudinal\nclinical dataset from Hong Kong Hospital Authority electronic health record\n(EHR) system to show the effectiveness and generalizability of this concept,\nachieving 10\\% F1-score improvement. Based on our experiments, we conclude that\ncapturing a longer clinical note interval is beneficial to the model\nperformance, but there are different cut-off intervals to achieve the optimal\nperformance for different target variables. Our code is available at\nhttps://github.com/HLTCHKUST/long-biomedical-model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Huan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">MingQian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ip_Y/0/1/0/all/0/1\">Yuk-Yu Nancy Ip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Attention Weights as Explanations from an Information Theoretic Perspective. (arXiv:2211.07714v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07714","description":"<p>Attention mechanisms have recently demonstrated impressive performance on a\nrange of NLP tasks, and attention scores are often used as a proxy for model\nexplainability. However, there is a debate on whether attention weights can, in\nfact, be used to identify the most important inputs to a model. We approach\nthis question from an information theoretic perspective by measuring the mutual\ninformation between the model output and the hidden states. From extensive\nexperiments, we draw the following conclusions: (i) Additive and Deep attention\nmechanisms are likely to be better at preserving the information between the\nhidden states and the model output (compared to Scaled Dot-product); (ii)\nablation studies indicate that Additive attention can actively learn to explain\nthe importance of its input hidden representations; (iii) when attention values\nare nearly the same, the rank order of attention values is not consistent with\nthe rank order of the mutual information(iv) Using Gumbel-Softmax with a\ntemperature lower than one, tends to produce a more skewed attention score\ndistribution compared to softmax and hence is a better choice for explainable\ndesign; (v) some building blocks are better at preserving the correlation\nbetween the ordered list of mutual information and attention weights order (for\ne.g., the combination of BiLSTM encoder and Additive attention). Our findings\nindicate that attention mechanisms do have the potential to function as a\nshortcut to model explanations when they are carefully combined with other\nmodel elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bingyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbalakshmi_K/0/1/0/all/0/1\">K.P. Subbalakshmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast DistilBERT on CPUs. (arXiv:2211.07715v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07715","description":"<p>Transformer-based language models have become the standard approach to\nsolving natural language processing tasks. However, industry adoption usually\nrequires the maximum throughput to comply with certain latency constraints that\nprevents Transformer models from being used in production. To address this gap,\nmodel compression techniques such as quantization and pruning may be used to\nimprove inference efficiency. However, these compression techniques require\nspecialized software to apply and deploy at scale. In this work, we propose a\nnew pipeline for creating and running Fast Transformer models on CPUs,\nutilizing hardware-aware pruning, knowledge distillation, quantization, and our\nown Transformer inference runtime engine with optimized kernels for sparse and\nquantized operators. We demonstrate the efficiency of our pipeline by creating\na Fast DistilBERT model showing minimal accuracy loss on the question-answering\nSQuADv1.1 benchmark, and throughput results under typical production\nconstraints and environments. Our results outperform existing state-of-the-art\nNeural Magic's DeepSparse runtime performance by up to 50% and up to 4.1x\nperformance speedup over ONNX Runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafrir_O/0/1/0/all/0/1\">Ofir Zafrir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Hengyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xinyu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hanwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudoukh_G/0/1/0/all/0/1\">Guy Boudoukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Text Matching for Automated Auditing using Sentence Transformers. (arXiv:2211.07716v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07716","description":"<p>Natural language processing methods have several applications in automated\nauditing, including document or passage classification, information retrieval,\nand question answering. However, training such models requires a large amount\nof annotated data which is scarce in industrial settings. At the same time,\ntechniques like zero-shot and unsupervised learning allow for application of\nmodels pre-trained using general domain data to unseen domains.\n</p>\n<p>In this work, we study the efficiency of unsupervised text matching using\nSentence-Bert, a transformer-based model, by applying it to the semantic\nsimilarity of financial passages. Experimental results show that this model is\nrobust to documents from in- and out-of-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biesner_D/0/1/0/all/0/1\">David Biesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1\">Rajkumar Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilmaghani_T/0/1/0/all/0/1\">Tim Dilmaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliem_B/0/1/0/all/0/1\">Bernd Kliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loitz_R/0/1/0/all/0/1\">R&#xfc;diger Loitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07717","description":"<p>We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) model, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity,\ndata distributions, and clinical depression detection related settings to draw\na complete picture of the impact of these features across our created datasets.\nFinally, we show that, in general, only semantic oriented representation models\nperform well. However, clinical features may enhance overall performance\nprovided that the training and testing distribution is similar, and there is\nmore data in a user's timeline. Further, we show that the predictive capability\nof depression scores increase significantly while used in a more sensitive\nclinical depression detection settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v1 [cs.LG])","link":"http://arxiv.org/abs/2211.07730","description":"<p>Zero-shot transfer learning for document understanding is a crucial yet\nunder-investigated scenario to help reduce the high cost involved in annotating\ndocument entities. We present a novel query-based framework, QueryForm, that\nextracts entity values from form-like documents in a zero-shot fashion.\nQueryForm contains a dual prompting mechanism that composes both the document\nschema and a specific entity type into a query, which is used to prompt a\nTransformer model to perform a single entity extraction task. Furthermore, we\npropose to leverage large-scale query-entity pairs generated from form-like\nwebpages with weak HTML annotations to pre-train QueryForm. By unifying\npre-training and fine-tuning into the same query-based framework, QueryForm\nenables models to learn from structured documents containing various entities\nand layouts, leading to better generalization to target document types without\nthe need for target-specific training data. QueryForm sets new state-of-the-art\naverage F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%)\nzero-shot benchmark, with a smaller model size and no additional image input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1\">Jennifer Dy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07733","description":"<p>Pre-trained multilingual language models (PMLMs) are commonly used when\ndealing with data from multiple languages and cross-lingual transfer. However,\nPMLMs are trained on varying amounts of data for each language. In practice\nthis means their performance is often much better on English than many other\nlanguages. We explore to what extent this also applies to moral norms. Do the\nmodels capture moral norms from English and impose them on other languages? Do\nthe models exhibit random and thus potentially harmful beliefs in certain\nlanguages? Both these issues could negatively impact cross-lingual transfer and\npotentially lead to harmful outcomes. In this paper, we (1) apply the\nMoralDirection framework to multilingual models, comparing results in German,\nCzech, Arabic, Mandarin Chinese, and English, (2) analyse model behaviour on\nfiltered parallel subtitles corpora, and (3) apply the models to a Moral\nFoundations Questionnaire, comparing with human responses from different\ncountries. Our experiments demonstrate that, indeed, PMLMs encode differing\nmoral biases, but these do not necessarily correspond to cultural differences\nor commonalities in human opinions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure. (arXiv:2211.07743v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07743","description":"<p>Generative models have demonstrated impressive results on Aspect-based\nSentiment Analysis (ABSA) tasks, particularly for the emerging task of\nextracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these\nmodels struggle with implicit sentiment expressions, which are commonly\nobserved in opinionated content such as online reviews. In this work, we\nintroduce GEN-SCL-NAT, which consists of two techniques for improved structured\ngeneration for ACOS quadruple extraction. First, we propose GEN-SCL, a\nsupervised contrastive learning objective that aids quadruple prediction by\nencouraging the model to produce input representations that are discriminable\nacross key input attributes, such as sentiment polarity and the existence of\nimplicit opinions and aspects. Second, we introduce GEN-NAT, a new structured\ngeneration format that better adapts autoregressive encoder-decoder models to\nextract quadruples in a generative fashion. Experimental results show that\nGEN-SCL-NAT achieves top performance across three ACOS datasets, averaging\n1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.\nAdditionally, we see significant gains on implicit aspect and opinion splits\nthat have been shown as challenging for existing ACOS approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peper_J/0/1/0/all/0/1\">Joseph J. Peper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations. (arXiv:2211.07769v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07769","description":"<p>Children's speech recognition is a vital, yet largely overlooked domain when\nbuilding inclusive speech technologies. The major challenge impeding progress\nin this domain is the lack of adequate child speech corpora; however, recent\nadvances in self-supervised learning have created a new opportunity for\novercoming this problem of data scarcity. In this paper, we leverage\nself-supervised adult speech representations and use three well-known child\nspeech corpora to build models for children's speech recognition. We assess the\nperformance of fine-tuning on both native and non-native children's speech,\nexamine the effect of cross-domain child corpora, and investigate the minimum\namount of child speech required to fine-tune a model which outperforms a\nstate-of-the-art adult model. We also analyze speech recognition performance\nacross children's ages. Our results demonstrate that fine-tuning with\ncross-domain child corpora leads to relative improvements of up to 46.08% and\n45.53% for native and non-native child speech respectively, and absolute\nimprovements of 14.70% and 31.10%. We also show that with as little as 5 hours\nof transcribed children's speech, it is possible to fine-tune a children's\nspeech recognition system that outperforms a state-of-the-art adult model\nfine-tuned on 960 hours of adult speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Renee Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahin_M/0/1/0/all/0/1\">Mostafa Shahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1\">Beena Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07828","description":"<p>Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced\nimpressive gains over purely parametric LMs, by leveraging large-scale\nneighborhood retrieval over external memory datastores. However, there has been\nlittle investigation into adapting such models for new domains. This work\nattempts to fill that gap and suggests the following approaches for adapting\n$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding\nneighborhood retrieval over an additional adaptation datastore, and 3) adapting\nthe weights (scores) of retrieved neighbors using a learned Rescorer module. We\nstudy each adaptation strategy separately, as well as the combined performance\nimprovement through ablation experiments and an extensive set of evaluations\nrun over seven adaptation domains. Our combined adaptation approach\nconsistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)\nbaselines that construct datastores from the adaptation data. On average, we\nsee perplexity improvements of 17.1\\% and 16\\% for these respective baselines,\nacross domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polovets_G/0/1/0/all/0/1\">George Polovets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1\">Monica Sunkara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07830","description":"<p>Although pretrained language models (PLMs) can be prompted to perform a wide\nrange of language tasks, it remains an open question how much this ability\ncomes from generalizable linguistic representations versus more surface-level\nlexical patterns. To test this, we present a structured prompting approach that\ncan be used to prompt for linguistic structure prediction tasks, allowing us to\nperform zero- and few-shot sequence tagging with autoregressive PLMs. We\nevaluate this approach on part-of-speech tagging, named entity recognition, and\nsentence chunking and demonstrate strong few-shot performance in all cases. We\nalso find that, though the surface forms of the tags provide some signal,\nstructured prompting can retrieve linguistic structure even with arbitrary\nlabels, indicating that PLMs contain this knowledge in a general manner robust\nto label choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation. (arXiv:2211.07842v1 [cs.LG])","link":"http://arxiv.org/abs/2211.07842","description":"<p>Despite the increase in popularity of language models for code generation, it\nis still unknown how training on bimodal coding forums affects a model's code\ngeneration performance and reliability. We, therefore, collect a dataset of\nover 2.2M StackOverflow questions with answers for finetuning. These fine-tuned\nmodels have average $pass@k$ improvements of 54.64% and 85.35% on the HumanEval\n(Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021)\ntasks, respectively. This regime further decreases the number of generated\nprograms with both syntax and runtime errors. However, we find that at higher\ntemperatures, there are significant decreases to the model's ability to\ngenerate runnable programs despite higher $pass@k$ scores, underscoring the\nneed for better methods of incorporating such data that mitigate these side\neffects. The code can be found\nhttps://github.com/gabeorlanski/bimodalcode-generation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1\">Gabriel Orlanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seonhye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healy_M/0/1/0/all/0/1\">Michael Healy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Spelling Check with Nearest Neighbors. (arXiv:2211.07843v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07843","description":"<p>Chinese Spelling Check (CSC) aims to detect and correct error tokens in\nChinese contexts, which has a wide range of applications. In this paper, we\nintroduce InfoKNN-CSC, extending the standard CSC model by linearly\ninterpolating it with a k-nearest neighbors (kNN) model. Moreover, the\nphonetic, graphic, and contextual information (info) of tokens and contexts are\nelaborately incorporated into the design of the query and key of kNN, according\nto the characteristics of the task. After retrieval, in order to match the\ncandidates more accurately, we also perform reranking methods based on the\noverlap of the n-gram values and inputs. Experiments on the SIGHAN benchmarks\ndemonstrate that the proposed model achieves state-of-the-art performance with\nsubstantial improvements over existing work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xunjian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship of the language distance to English ability of a country. (arXiv:2211.07855v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07855","description":"<p>Language difference is one of the factors that hinder the acquisition of\nsecond language skills. In this article, we introduce a novel solution that\nleverages the strength of deep neural networks to measure the semantic\ndissimilarity between languages based on their word distributions in the\nembedding space of the multilingual pre-trained language model (e.g.,BERT).\nThen, we empirically examine the effectiveness of the proposed semantic\nlanguage distance (SLD) in explaining the consistent variation in English\nability of countries, which is proxied by their performance in the\nInternet-Based Test of English as Foreign Language (TOEFL iBT). The\nexperimental results show that the language distance demonstrates negative\ninfluence on a country's average English ability. Interestingly, the effect is\nmore significant on speaking and writing subskills, which pertain to the\nproductive aspects of language learning. Besides, we provide specific\nrecommendations for future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xinxin_C/0/1/0/all/0/1\">Cao Xinxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaolan_L/0/1/0/all/0/1\">Lei Xiaolan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Murtadha Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey for Efficient Open Domain Question Answering. (arXiv:2211.07886v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07886","description":"<p>Open domain question answering (ODQA) is a longstanding task aimed at\nanswering factual questions from a large knowledge corpus without any explicit\nevidence in natural language processing (NLP). Recent works have predominantly\nfocused on improving the answering accuracy and achieved promising progress.\nHowever, higher accuracy often comes with more memory consumption and inference\nlatency, which might not necessarily be efficient enough for direct deployment\nin the real world. Thus, a trade-off between accuracy, memory consumption and\nprocessing speed is pursued. In this paper, we provide a survey of recent\nadvances in the efficiency of ODQA models. We walk through the ODQA models and\nconclude the core techniques on efficiency. Quantitative analysis on memory\ncost, processing speed, accuracy and overall comparison are given. We hope that\nthis work would keep interested scholars informed of the advances and open\nchallenges in ODQA efficiency research, and thus contribute to the further\ndevelopment of ODQA efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shangsi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qingqing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Phrase-based Sequence-to-Sequence Learning. (arXiv:2211.07906v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07906","description":"<p>We describe a neural transducer that maintains the flexibility of standard\nsequence-to-sequence (seq2seq) models while incorporating hierarchical phrases\nas a source of inductive bias during training and as explicit constraints\nduring inference. Our approach trains two models: a discriminative parser based\non a bracketing transduction grammar whose derivation tree hierarchically\naligns source and target phrases, and a neural seq2seq model that learns to\ntranslate the aligned phrases one-by-one. We use the same seq2seq model to\ntranslate at all phrase scales, which results in two inference modes: one mode\nin which the parser is discarded and only the seq2seq component is used at the\nsequence-level, and another in which the parser is combined with the seq2seq\nmodel. Decoding in the latter mode is done with the cube-pruned CKY algorithm,\nwhich is more involved but can make use of new translation rules during\ninference. We formalize our model as a source-conditioned synchronous grammar\nand develop an efficient variational inference algorithm for training. When\napplied on top of both randomly initialized and pretrained seq2seq models, we\nfind that both inference modes performs well compared to baselines on small\nscale machine translation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Open-Ended Stressor Responses to Predict Depressive Symptoms across Demographics. (arXiv:2211.07932v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07932","description":"<p>Stressors are related to depression, but this relationship is complex. We\ninvestigate the relationship between open-ended text responses about stressors\nand depressive symptoms across gender and racial/ethnic groups. First, we use\ntopic models and other NLP tools to find thematic and vocabulary differences\nwhen reporting stressors across demographic groups. We train language models\nusing self-reported stressors to predict depressive symptoms, finding a\nrelationship between stressors and depression. Finally, we find that\ndifferences in stressors translate to downstream performance differences across\ndemographic groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_C/0/1/0/all/0/1\">Carlos Aguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs. (arXiv:2211.07950v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07950","description":"<p>Can we teach natural language understanding models to track their beliefs\nthrough intermediate points in text? We propose a representation learning\nframework called breakpoint modeling that allows for learning of this type.\nGiven any text encoder and data marked with intermediate states (breakpoints)\nalong with corresponding textual queries viewed as true/false propositions\n(i.e., the candidate beliefs of a model, consisting of information changing\nthrough time) our approach trains models in an efficient and end-to-end fashion\nto build intermediate representations that facilitate teaching and direct\nquerying of beliefs at arbitrary points alongside solving other end tasks. To\nshow the benefit of our approach, we experiment with a diverse set of NLU tasks\nincluding relational reasoning on CLUTRR and narrative understanding on bAbI.\nUsing novel belief prediction tasks for both tasks, we show the benefit of our\nmain breakpoint transformer, based on T5, over conventional representation\nlearning approaches in terms of processing efficiency, prediction accuracy and\nprediction consistency, all with minimal to no effect on corresponding QA end\ntasks. To show the feasibility of incorporating our belief tracker into more\ncomplex reasoning pipelines, we also obtain SOTA performance on the\nthree-tiered reasoning challenge for the TRIP benchmark (around 23-32% absolute\nimprovement on Tasks 2-3).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1\">Ronen Tamari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_O/0/1/0/all/0/1\">Oren Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview on Controllable Text Generation via Variational Auto-Encoders. (arXiv:2211.07954v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07954","description":"<p>Recent advances in neural-based generative modeling have reignited the hopes\nof having computer systems capable of conversing with humans and able to\nunderstand natural language. The employment of deep neural architectures has\nbeen largely explored in a multitude of context and tasks to fulfill various\nuser needs. On one hand, producing textual content that meets specific\nrequirements is of priority for a model to seamlessly conduct conversations\nwith different groups of people. On the other hand, latent variable models\n(LVM) such as variational auto-encoders (VAEs) as one of the most popular\ngenres of generative models are designed to characterize the distributional\npattern of textual data. Thus they are inherently capable of learning the\nintegral textual features that are worth exploring for controllable pursuits.\n</p>\n<p>\\noindent This overview gives an introduction to existing generation schemes,\nproblems associated with text variational auto-encoders, and a review of\nseveral applications about the controllable generation that are instantiations\nof these general formulations,\\footnote{A detailed paper list is available at\n\\url{https://github.com/ImKeTT/CTG-latentAEs}} as well as related datasets,\nmetrics and discussions for future researches. Hopefully, this overview will\nprovide an overview of living questions, popular methodologies and raw thoughts\nfor controllable language generation under the scope of variational\nauto-encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of discourse and conversation impairments in patients with dementia. (arXiv:2211.07971v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07971","description":"<p>Neurodegeneration characterizes patients with different dementia subtypes\n(e.g., patients with Alzheimer's Disease, Primary Progressive Aphasia, and\nParkinson's Disease), leading to progressive decline in cognitive, linguistic,\nand social functioning. Speech and language impairments are early symptoms in\npatients with focal forms of neurodegenerative conditions, coupled with\ndeficits in cognitive, social, and behavioral domains. This paper reviews the\nfindings on language and communication deficits and identifies the effects of\ndementia on the production and perception of discourse. It discusses findings\nconcerning (i) language function, cognitive representation, and impairment ,\n(ii) communicative competence, emotions, empathy, and theory-of-mind, and (iii)\nspeech-in-interaction. It argues that clinical discourse analysis can provide a\ncomprehensive assessment of language and communication skills in patients,\nwhich complements the existing neurolinguistic evaluation for (differential)\ndiagnosis, prognosis, and treatment efficacy evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Themistocleous_C/0/1/0/all/0/1\">Charalambos Themistocleous</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit. (arXiv:2211.07980v1 [cs.CL])","link":"http://arxiv.org/abs/2211.07980","description":"<p>Sanskrit is a classical language with about 30 million extant manuscripts fit\nfor digitisation, available in written, printed or scannedimage forms. However,\nit is still considered to be a low-resource language when it comes to available\ndigital resources. In this work, we release a post-OCR text correction dataset\ncontaining around 218,000 sentences, with 1.5 million words, from 30 different\nbooks. Texts in Sanskrit are known to be diverse in terms of their linguistic\nand stylistic usage since Sanskrit was the 'lingua franca' for discourse in the\nIndian subcontinent for about 3 millennia. Keeping this in mind, we release a\nmulti-domain dataset, from areas as diverse as astronomy, medicine and\nmathematics, with some of them as old as 18 centuries. Further, we release\nmultiple strong baselines as benchmarks for the task, based on pre-trained\nSeq2Seq language models. We find that our best-performing model, consisting of\nbyte level tokenization in conjunction with phonetic encoding (Byt5+SLP1),\nyields a 23% point increase over the OCR output in terms of word and character\nerror rates. Moreover, we perform extensive experiments in evaluating these\nmodels on their performance and analyse common causes of mispredictions both at\nthe graphemic and lexical levels. Our code and dataset is publicly available at\nhttps://github.com/ayushbits/pe-ocr-sanskrit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers. (arXiv:2211.08025v1 [cs.LG])","link":"http://arxiv.org/abs/2211.08025","description":"<p>Federated Learning (FL) is an emerging paradigm that enables distributed\nusers to collaboratively and iteratively train machine learning models without\nsharing their private data. Motivated by the effectiveness and robustness of\nself-attention-based architectures, researchers are turning to using\npre-trained Transformers (i.e., foundation models) instead of traditional\nconvolutional neural networks in FL to leverage their excellent transfer\nlearning capabilities. Despite recent progress, how pre-trained Transformer\nmodels play a role in FL remains obscure, that is, how to efficiently fine-tune\nthese pre-trained models in FL and how FL users could benefit from this new\nparadigm. In this paper, we explore this issue and demonstrate that the\nfine-tuned Transformers achieve extraordinary performance on FL, and that the\nlightweight fine-tuning method facilitates a fast convergence rate and low\ncommunication costs. Concretely, we conduct a rigorous empirical study of three\ntuning methods (i.e., modifying the input, adding extra modules, and adjusting\nthe backbone) using two types of pre-trained models (i.e., vision-language\nmodels and vision models) for FL. Our experiments show that 1) Fine-tuning the\nbias term of the backbone performs best when relying on a strong pre-trained\nmodel; 2) The vision-language model (e.g., CLIP) outperforms the pure vision\nmodel (e.g., ViT) and is more robust to the few-shot settings; 3) Compared to\npure local training, FL with pre-trained models has a higher accuracy because\nit alleviates the problem of over-fitting. We will release our code and\nencourage further exploration of pre-trained Transformers and FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches. (arXiv:2211.08029v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08029","description":"<p>Emotion recognition is one of the machine learning applications which can be\ndone using text, speech, or image data gathered from social media spaces.\nDetecting emotion can help us in different fields, including opinion mining.\nWith the spread of social media, different platforms like Twitter have become\ndata sources, and the language used in these platforms is informal, making the\nemotion detection task difficult. EmoPars and ArmanEmo are two new\nhuman-labeled emotion datasets for the Persian language. These datasets,\nespecially EmoPars, are suffering from inequality between several samples\nbetween two classes. In this paper, we evaluate EmoPars and compare them with\nArmanEmo. Throughout this analysis, we use data augmentation techniques, data\nre-sampling, and class-weights with Transformer-based Pretrained Language\nModels(PLMs) to handle the imbalance problem of these datasets. Moreover,\nfeature selection is used to enhance the models' performance by emphasizing the\ntext's specific features. In addition, we provide a new policy for selecting\ndata from EmoPars, which selects the high-confidence samples; as a result, the\nmodel does not see samples that do not have specific emotion during training.\nOur model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and\nEmoPars, respectively, which are new state-of-the-art results in these\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabri_N/0/1/0/all/0/1\">Nazanin Sabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual and Multimodal Topic Modelling with Pretrained Embeddings. (arXiv:2211.08057v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08057","description":"<p>This paper presents M3L-Contrast -- a novel multimodal multilingual (M3L)\nneural topic model for comparable data that maps texts from multiple languages\nand images into a shared topic space. Our model is trained jointly on texts and\nimages and takes advantage of pretrained document and image embeddings to\nabstract the complexities between different languages and modalities. As a\nmultilingual topic model, it produces aligned language-specific topics and as\nmultimodal model, it infers textual representations of semantic concepts in\nimages. We demonstrate that our model is competitive with a zero-shot topic\nmodel in predicting topic distributions for comparable multilingual data and\nsignificantly outperforms a zero-shot model in predicting topic distributions\nfor comparable texts and images. We also show that our model performs almost as\nwell on unaligned embeddings as it does on aligned embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zosa_E/0/1/0/all/0/1\">Elaine Zosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1\">Lidia Pivovarova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08073","description":"<p>Pre-trained language models (PLMs) improve the model generalization by\nleveraging massive data as the training corpus in the pre-training phase.\nHowever, currently, the out-of-distribution (OOD) generalization becomes a\ngenerally ill-posed problem, even for the large-scale PLMs in natural language\nunderstanding tasks, which prevents the deployment of NLP methods in the real\nworld. To facilitate the research in this direction, this paper makes the first\nattempt to establish a unified benchmark named GLUE-X, highlighting the\nimportance of OOD robustness and providing insights on how to measure the\nrobustness of a model and how to improve it. To this end, we collect 13\npublicly available datasets as OOD test data, and conduct evaluations on 8\nclassic NLP tasks over \\emph{18} popularly used models. Our findings confirm\nthat the OOD accuracy in NLP tasks needs to be paid more attention to since the\nsignificant performance decay compared to ID accuracy has been found in all\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuibai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded VQA by Lattice-based Retrieval. (arXiv:2211.08086v1 [cs.CV])","link":"http://arxiv.org/abs/2211.08086","description":"<p>Visual Grounding (VG) in Visual Question Answering (VQA) systems describes\nhow well a system manages to tie a question and its answer to relevant image\nregions. Systems with strong VG are considered intuitively interpretable and\nsuggest an improved scene understanding. While VQA accuracy performances have\nseen impressive gains over the past few years, explicit improvements to VG\nperformance and evaluation thereof have often taken a back seat on the road to\noverall accuracy improvements. A cause of this originates in the predominant\nchoice of learning paradigm for VQA systems, which consists of training a\ndiscriminative classifier over a predetermined set of answer options.\n</p>\n<p>In this work, we break with the dominant VQA modeling paradigm of\nclassification and investigate VQA from the standpoint of an information\nretrieval task. As such, the developed system directly ties VG into its core\nsearch procedure. Our system operates over a weighted, directed, acyclic graph,\na.k.a. \"lattice\", which is derived from the scene graph of a given image in\nconjunction with region-referring expressions extracted from the question.\n</p>\n<p>We give a detailed analysis of our approach and discuss its distinctive\nproperties and limitations. Our approach achieves the strongest VG performance\namong examined systems and exhibits exceptional generalization capabilities in\na number of scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">Daniel Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putze_F/0/1/0/all/0/1\">Felix Putze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_T/0/1/0/all/0/1\">Tanja Schultz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08099","description":"<p>Generative modeling has been the dominant approach for large-scale\npretraining and zero-shot generalization. In this work, we challenge this\nconvention by showing that discriminative approaches perform substantially\nbetter than generative ones on a large number of NLP tasks. Technically, we\ntrain a single discriminator to predict whether a text sample comes from the\ntrue data distribution, similar to GANs. Since many NLP tasks can be formulated\nas selecting from a few options, we use this discriminator to predict the\noption with the highest probability. This simple formulation achieves\nstate-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by\n16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning\nsetting, our approach also achieves new state-of-the-art results on a wide\nrange of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,\nour approach requires minimal prompting efforts, which largely improves\nrobustness and is essential for real-world applications. Furthermore, we also\njointly train a generalized UD in combination with generative tasks, which\nmaintains its advantage on discriminative tasks and simultaneously works on\ngenerative tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haike Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Pronunciation Assessment with Multi-Aspect Attention. (arXiv:2211.08102v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08102","description":"<p>Automatic pronunciation assessment is a major component of a\ncomputer-assisted pronunciation training system. To provide in-depth feedback,\nscoring pronunciation at various levels of granularity such as phoneme, word,\nand utterance, with diverse aspects such as accuracy, fluency, and\ncompleteness, is essential. However, existing multi-aspect multi-granularity\nmethods simultaneously predict all aspects at all granularity levels;\ntherefore, they have difficulty in capturing the linguistic hierarchy of\nphoneme, word, and utterance. This limitation further leads to neglecting\nintimate cross-aspect relations at the same linguistic unit. In this paper, we\npropose a Hierarchical Pronunciation Assessment with Multi-aspect Attention\n(HiPAMA) model, which hierarchically represents the granularity levels to\ndirectly capture their linguistic structures and introduces multi-aspect\nattention that reflects associations across aspects at the same level to create\nmore connotative representations. By obtaining relational information from both\nthe granularity- and aspect-side, HiPAMA can take full advantage of multi-task\nlearning. Remarkable improvements in the experimental results on the\nspeachocean762 datasets demonstrate the robustness of HiPAMA, particularly in\nthe difficult-to-assess aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_H/0/1/0/all/0/1\">Heejin Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition. (arXiv:2211.08104v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08104","description":"<p>We present DualNER, a simple and effective framework to make full use of both\nannotated source language corpus and unlabeled target language text for\nzero-shot cross-lingual named entity recognition (NER). In particular, we\ncombine two complementary learning paradigms of NER, i.e., sequence labeling\nand span prediction, into a unified multi-task framework. After obtaining a\nsufficient NER model trained on the source data, we further train it on the\ntarget data in a {\\it dual-teaching} manner, in which the pseudo-labels for one\ntask are constructed from the prediction of the other task. Moreover, based on\nthe span prediction, an entity-aware regularization is proposed to enhance the\nintrinsic cross-lingual alignment between the same entities in different\nlanguages. Experiments and analysis demonstrate the effectiveness of our\nDualNER. Code is available at https://github.com/lemon0830/dualNER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yufan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Active Learning Pipeline for Legal Text Classification. (arXiv:2211.08112v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08112","description":"<p>Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamooler_S/0/1/0/all/0/1\">Sepideh Mamooler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massonnet_S/0/1/0/all/0/1\">St&#xe9;phane Massonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08142","description":"<p>Mathematical notation makes up a large portion of STEM literature, yet,\nfinding semantic representations for formulae remains a challenging problem.\nBecause mathematical notation is precise and its meaning changes significantly\nwith small character shifts, the methods that work for natural text do not\nnecessarily work well for mathematical expressions. In this work, we describe\nan approach for representing mathematical expressions in a continuous vector\nspace. We use the encoder of a sequence-to-sequence architecture, trained on\nvisually different but mathematically equivalent expressions, to generate\nvector representations (embeddings). We compare this approach with an\nautoencoder and show that the former is better at capturing mathematical\nsemantics. Finally, to expedite future projects, we publish a corpus of\nequivalent transcendental and algebraic expression pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_N/0/1/0/all/0/1\">Neeraj Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kani_N/0/1/0/all/0/1\">Nickvash Kani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSynGEC: Incorporating Constituent-based Syntax for Grammatical Error Correction with a Tailored GEC-Oriented Parser. (arXiv:2211.08158v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08158","description":"<p>Recently, Zhang et al. (2022) propose a syntax-aware grammatical error\ncorrection (GEC) approach, named SynGEC, showing that incorporating tailored\ndependency-based syntax of the input sentence is quite beneficial to GEC. This\nwork considers another mainstream syntax formalism, i.e., constituent-based\nsyntax. By drawing on the successful experience of SynGEC, we first propose an\nextended constituent-based syntax scheme to accommodate errors in ungrammatical\nsentences. Then, we automatically obtain constituency trees of ungrammatical\nsentences to train a GEC-oriented constituency parser by using parallel GEC\ndata as a pivot. For syntax encoding, we employ the graph convolutional network\n(GCN). Experimental results show that our method, named CSynGEC, yields\nsubstantial improvements over strong baselines. Moreover, we investigate the\nintegration of constituent-based and dependency-based syntax for GEC in two\nways: 1) intra-model combination, which means using separate GCNs to encode\nboth kinds of syntax for decoding in a single model; 2)inter-model combination,\nwhich means gathering and selecting edits predicted by different models to\nachieve final corrections. We find that the former method improves recall over\nusing one standalone syntax formalism while the latter improves precision, and\nboth lead to better F0.5 values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Type Information Utilized Event Detection via Multi-Channel GNNs in Electrical Power Systems. (arXiv:2211.08168v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08168","description":"<p>Event detection in power systems aims to identify triggers and event types,\nwhich helps relevant personnel respond to emergencies promptly and facilitates\nthe optimization of power supply strategies. However, the limited length of\nshort electrical record texts causes severe information sparsity, and numerous\ndomain-specific terminologies of power systems makes it difficult to transfer\nknowledge from language models pre-trained on general-domain texts. Traditional\nevent detection approaches primarily focus on the general domain and ignore\nthese two problems in the power system domain. To address the above issues, we\npropose a Multi-Channel graph neural network utilizing Type information for\nEvent Detection in power systems, named MC-TED, leveraging a semantic channel\nand a topological channel to enrich information interaction from short texts.\nConcretely, the semantic channel refines textual representations with semantic\nsimilarity, building the semantic information interaction among potential\nevent-related words. The topological channel generates a relation-type-aware\ngraph modeling word dependencies, and a word-type-aware graph integrating\npart-of-speech tags. To further reduce errors worsened by professional\nterminologies in type analysis, a type learning mechanism is designed for\nupdating the representations of both the word type and relation type in the\ntopological channel. In this way, the information sparsity and professional\nterm occurrence problems can be alleviated by enabling interaction between\ntopological and semantic information. Furthermore, to address the lack of\nlabeled data in power systems, we built a Chinese event detection dataset based\non electrical Power Event texts, named PoE. In experiments, our model achieves\ncompelling results not only on the PoE dataset, but on general-domain event\ndetection datasets including ACE 2005 and MAVEN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Cheng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingyun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Shan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Question Answering over Knowledge Bases. (arXiv:2211.08170v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08170","description":"<p>Question answering over knowledge bases (KBQA) has become a popular approach\nto help users extract information from knowledge bases. Although several\nsystems exist, choosing one suitable for a particular application scenario is\ndifficult. In this article, we provide a comparative study of six\nrepresentative KBQA systems on eight benchmark datasets. In that, we study\nvarious question types, properties, languages, and domains to provide insights\non where existing systems struggle. On top of that, we propose an advanced\nmapping algorithm to aid existing models in achieving superior results.\nMoreover, we also develop a multilingual corpus COVID-KGQA, which encourages\nCOVID-19 research and multilingualism for the diversity of future AI. Finally,\nwe discuss the key findings and their implications as well as performance\nguidelines and some future improvements. Our source code is available at\n\\url{https://github.com/tamlhp/kbqa}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hao Phu Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1\">Khang Nguyen Duc Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jun Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tam Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use. (arXiv:2211.08192v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08192","description":"<p>Large transformer-based language models, e.g. BERT and GPT-3, outperform\nprevious architectures on most natural language processing tasks. Such language\nmodels are first pre-trained on gigantic corpora of text and later used as\nbase-model for finetuning on a particular task. Since the pre-training step is\nusually not repeated, base models are not up-to-date with the latest\ninformation. In this paper, we update RobBERT, a RoBERTa-based state-of-the-art\nDutch language model, which was trained in 2019. First, the tokenizer of\nRobBERT is updated to include new high-frequent tokens present in the latest\nDutch OSCAR corpus, e.g. corona-related words. Then we further pre-train the\nRobBERT model using this dataset. To evaluate if our new model is a plug-in\nreplacement for RobBERT, we introduce two additional criteria based on concept\ndrift of existing tokens and alignment for novel tokens.We found that for\ncertain language tasks this update results in a significant performance\nincrease. These results highlight the benefit of continually updating a\nlanguage model to account for evolving language use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delobelle_P/0/1/0/all/0/1\">Pieter Delobelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winters_T/0/1/0/all/0/1\">Thomas Winters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1\">Bettina Berendt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dependence on Frequency of Word Embedding Similarity Measures. (arXiv:2211.08203v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08203","description":"<p>Recent research has shown that static word embeddings can encode word\nfrequency information. However, little has been studied about this phenomenon\nand its effects on downstream tasks. In the present work, we systematically\nstudy the association between frequency and semantic similarity in several\nstatic word embeddings. We find that Skip-gram, GloVe and FastText embeddings\ntend to produce higher semantic similarity between high-frequency words than\nbetween other frequency combinations. We show that the association between\nfrequency and similarity also appears when words are randomly shuffled. This\nproves that the patterns found are not due to real semantic associations\npresent in the texts, but are an artifact produced by the word embeddings.\nFinally, we provide an example of how word frequency can strongly impact the\nmeasurement of gender bias with embedding-based metrics. In particular, we\ncarry out a controlled experiment that shows that biases can even change sign\nor reverse their order by manipulating word frequencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentini_F/0/1/0/all/0/1\">Francisco Valentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1\">Diego Fernandez Slezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altszyler_E/0/1/0/all/0/1\">Edgar Altszyler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications. (arXiv:2211.08228v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08228","description":"<p>Open Information Extraction (OpenIE) has been used in the pipelines of\nvarious NLP tasks. Unfortunately, there is no clear consensus on which models\nto use in which tasks. Muddying things further is the lack of comparisons that\ntake differing training sets into account. In this paper, we present an\napplication-focused empirical survey of neural OpenIE models, training sets,\nand benchmarks in an effort to help users choose the most suitable OpenIE\nsystems for their applications. We find that the different assumptions made by\ndifferent models and datasets have a statistically significant effect on\nperformance, making it important to choose the most appropriate model for one's\napplications. We demonstrate the applicability of our recommendations on a\ndownstream Complex QA application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_K/0/1/0/all/0/1\">Kevin Pei</a> (Grainger College of Engineering, University of Illinois at Urbana-Champaign), <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a> (IBM Research), <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a> (Grainger College of Engineering, University of Illinois at Urbana-Champaign), <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">Chengxiang Zhai</a> (Grainger College of Engineering, University of Illinois at Urbana-Champaign), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a> (Apple Knowledge Platform)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v1 [cs.SD])","link":"http://arxiv.org/abs/2211.08233","description":"<p>Speech emotion recognition (SER) plays a vital role in improving the\ninteractions between humans and machines by inferring human emotion and\naffective states from speech signals. Whereas recent works primarily focus on\nmining spatiotemporal information from hand-crafted features, we explore how to\nmodel the temporal patterns of speech emotions from dynamic temporal scales.\nTowards that goal, we introduce a novel temporal emotional modeling approach\nfor SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),\nwhich learns multi-scale contextual affective representations from various time\nscales. Specifically, TIM-Net first employs temporal-aware blocks to learn\ntemporal affective representation, then integrates complementary information\nfrom the past and the future to enrich contextual representations, and finally,\nfuses multiple time scale features for better adaptation to the emotional\nvariation. Extensive experimental results on six benchmark SER datasets\ndemonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%\nimprovements of the average UAR and WAR over the second-best on each corpus.\nRemarkably, TIM-Net outperforms the latest domain-adaptation method on the\ncross-corpus SER tasks, demonstrating strong generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiaxin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xincheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yujie Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search. (arXiv:2211.08237v1 [cs.SD])","link":"http://arxiv.org/abs/2211.08237","description":"<p>Speech emotion recognition (SER) classifies audio into emotion categories\nsuch as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion\nRecognition (SER) is a common application for popular languages, it continues\nto be a problem for low-resourced languages, i.e., languages with no pretrained\nspeech-to-text recognition models. This paper firstly proposes a\nlanguage-specific model that extract emotional information from multiple\npre-trained speech models, and then designs a multi-domain model that\nsimultaneously performs SER for various languages. Our multidomain model\nemploys a multi-gating mechanism to generate unique weighted feature\ncombination for each language, and also searches for specific neural network\nstructure for each language through a neural architecture search module. In\naddition, we introduce a contrastive auxiliary loss to build more separable\nrepresentations for audio data. Our experiments show that our model raises the\nstate-of-the-art accuracy by 3% for German and 14.3% for French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">HaiFeng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">XinRui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">KeHao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08238","description":"<p>Given the fact description text of a legal case, legal judgment prediction\n(LJP) aims to predict the case's charge, law article and penalty term. A core\nproblem of LJP is how to distinguish confusing legal cases, where only subtle\ntext differences exist. Previous studies fail to distinguish different\nclassification errors with a standard cross-entropy classification loss, and\nignore the numbers in the fact description for predicting the term of penalty.\nTo tackle these issues, in this work, first, we propose a moco-based supervised\ncontrastive learning to learn distinguishable representations, and explore the\nbest strategy to construct positive example pairs to benefit all three subtasks\nof LJP simultaneously. Second, in order to exploit the numbers in legal cases\nfor predicting the penalty terms of certain cases, we further enhance the\nrepresentation of the fact description with extracted crime amounts which are\nencoded by a pre-trained numeracy model. Extensive experiments on public\nbenchmarks show that the proposed method achieves new state-of-the-art results,\nespecially on confusing legal cases. Ablation studies also demonstrate the\neffectiveness of each component.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Leilei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baokui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08264","description":"<p>The availability of large, high-quality datasets has been one of the main\ndrivers of recent progress in question answering (QA). Such annotated datasets\nhowever are difficult and costly to collect, and rarely exist in languages\nother than English, rendering QA technology inaccessible to underrepresented\nlanguages. An alternative to building large monolingual training datasets is to\nleverage pre-trained language models (PLMs) under a few-shot learning setting.\nOur approach, QAmeleon, uses a PLM to automatically generate multilingual data\nupon which QA models are trained, thus avoiding costly annotation. Prompt\ntuning the PLM for data synthesis with only five examples per language delivers\naccuracy superior to translation-based baselines, bridges nearly 60% of the gap\nbetween an English-only baseline and a fully supervised upper bound trained on\nalmost 50,000 hand labeled examples, and always leads to substantial\nimprovements compared to fine-tuning a QA model directly on labeled examples in\nlow resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show\nthat few-shot prompt tuning for data synthesis scales across languages and is a\nviable alternative to large-scale annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Priyanka Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1\">Fantine Huot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An FNet based Auto Encoder for Long Sequence News Story Generation. (arXiv:2211.08295v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08295","description":"<p>In this paper, we design an auto encoder based off of Google's FNet\nArchitecture in order to generate text from a subset of news stories contained\nin Google's C4 dataset. We discuss previous attempts and methods to generate\ntext from autoencoders and non LLM Models. FNET poses multiple advantages to\nBERT based encoders in the realm of efficiency which train 80% faster on GPUs\nand 70% faster on TPUs. We then compare outputs of how this autencoder perfroms\non different epochs. Finally, we analyze what outputs the encoder produces with\ndifferent seed text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1\">Paul K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahto_R/0/1/0/all/0/1\">Rakeshkumar Mahto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARTNR: Pick and place Ambiguity Resolving by Trustworthy iNteractive leaRning. (arXiv:2211.08304v1 [cs.RO])","link":"http://arxiv.org/abs/2211.08304","description":"<p>Several recent works show impressive results in mapping language-based human\ncommands and image scene observations to direct robot executable policies\n(e.g., pick and place poses). However, these approaches do not consider the\nuncertainty of the trained policy and simply always execute actions suggested\nby the current policy as the most probable ones. This makes them vulnerable to\ndomain shift and inefficient in the number of required demonstrations. We\nextend previous works and present the PARTNR algorithm that can detect\nambiguities in the trained policy by analyzing multiple modalities in the pick\nand place poses using topological analysis. PARTNR employs an adaptive,\nsensitivity-based, gating function that decides if additional user\ndemonstrations are required. User demonstrations are aggregated to the dataset\nand used for subsequent training. In this way, the policy can adapt promptly to\ndomain shift and it can minimize the number of required demonstrations for a\nwell-trained policy. The adaptive threshold enables to achieve the\nuser-acceptable level of ambiguity to execute the policy autonomously and in\nturn, increase the trustworthiness of our system. We demonstrate the\nperformance of PARTNR in a table-top pick and place task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luijkx_J/0/1/0/all/0/1\">Jelle Luijkx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajanovic_Z/0/1/0/all/0/1\">Zlatan Ajanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1\">Laura Ferranti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FolkScope: Intention Knowledge Graph Construction for Discovering E-commerce Commonsense. (arXiv:2211.08316v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08316","description":"<p>As stated by Oren Etzioni, ``commonsense is the dark matter of artificial\nintelligence''. In e-commerce, understanding users' needs or intentions\nrequires substantial commonsense knowledge, e.g., ``A user bought an iPhone and\na compatible case because the user wanted the phone to be protected''. In this\npaper, we present FolkScope, an intention knowledge graph construction\nframework, to reveal the structure of humans' minds about purchasing items on\ne-commerce platforms such as Amazon. As commonsense knowledge is usually\nineffable and not expressed explicitly, it is challenging to perform any kind\nof information extraction. Thus, we propose a new approach that leverages the\ngeneration power of large-scale language models and human-in-the-loop\nannotations to semi-automatically construct the knowledge graph. We annotate a\nlarge amount of assertions for both plausibility and typicality of an intention\nthat can explain a purchasing or co-purchasing behavior, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we populate the\nannotated information to all automatically generated ones, and further\nstructurize the assertions using pattern mining and conceptualization to form\nmore condensed and abstractive knowledge. We evaluate our knowledge graph using\nboth intrinsic quality measures and a downstream application, i.e.,\nrecommendation. The comprehensive study shows that our knowledge graph can well\nmodel e-commerce commonsense knowledge and can have many potential\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08358","description":"<p>Few-shot classification in NLP has recently made great strides due to the\navailability of large foundation models that, through priming and prompting,\nare highly effective few-shot learners. However, this approach has high\nvariance across different sets of few shots and across different finetuning\nruns. For example, we find that validation accuracy on RTE can vary by as much\nas 27 points. In this context, we make two contributions for more effective\nfew-shot learning. First, we propose novel ensembling methods and show that\nthey substantially reduce variance. Second, since performance depends a lot on\nthe set of few shots selected, active learning is promising for few-shot\nclassification. Based on our stable ensembling method, we build on existing\nwork on active learning and introduce a new criterion: inter-prompt uncertainty\nsampling with diversity. We present the first active learning based approach to\nselect training examples for prompt-based learning and show that it outperforms\nprior work on active learning. Finally, we show that our combined method, MEAL\n(Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 absolute points\non five different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying text using machine learning models and determining conversation drift. (arXiv:2211.08365v1 [cs.LG])","link":"http://arxiv.org/abs/2211.08365","description":"<p>Text classification helps analyse texts for semantic meaning and relevance,\nby mapping the words against this hierarchy. An analysis of various types of\ntexts is invaluable to understanding both their semantic meaning, as well as\ntheir relevance. Text classification is a method of categorising documents. It\ncombines computer text classification and natural language processing to\nanalyse text in aggregate. This method provides a descriptive categorization of\nthe text, with features like content type, object field, lexical\ncharacteristics, and style traits. In this research, the authors aim to use\nnatural language feature extraction methods in machine learning which are then\nused to train some of the basic machine learning models like Naive Bayes,\nLogistic Regression, and Support Vector Machine. These models are used to\ndetect when a teacher must get involved in a discussion when the lines go\noff-topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_C/0/1/0/all/0/1\">Chaitanya Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vandit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_A/0/1/0/all/0/1\">Ashish Khanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08369","description":"<p>A popular approach to unveiling the black box of neural NLP models is to\nleverage saliency methods, which assign scalar importance scores to each input\ncomponent. A common practice for evaluating whether an interpretability method\nis \\textit{faithful} and \\textit{plausible} has been to use\nevaluation-by-agreement -- multiple methods agreeing on an explanation\nincreases its credibility. However, recent work has found that even saliency\nmethods have weak rank correlations and advocated for the use of alternative\ndiagnostic methods. In our work, we demonstrate that rank correlation is not a\ngood fit for evaluating agreement and argue that Pearson-$r$ is a better suited\nalternative. We show that regularization techniques that increase faithfulness\nof attention explanations also increase agreement between saliency methods.\nThrough connecting our findings to instance categories based on training\ndynamics we show that, surprisingly, easy-to-learn instances exhibit low\nagreement in saliency method explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutek_M/0/1/0/all/0/1\">Martin Tutek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatics in Grounded Language Learning: Phenomena, Tasks, and Modeling Approaches. (arXiv:2211.08371v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08371","description":"<p>People rely heavily on context to enrich meaning beyond what is literally\nsaid, enabling concise but effective communication. To interact successfully\nand naturally with people, user-facing artificial intelligence systems will\nrequire similar skills in pragmatics: relying on various types of context --\nfrom shared linguistic goals and conventions, to the visual and embodied world\n-- to use language effectively.\n</p>\n<p>We survey existing grounded settings and pragmatic modeling approaches and\nanalyze how the task goals, environmental contexts, and communicative\naffordances in each work enrich linguistic meaning. We present recommendations\nfor future grounded task design to naturally elicit pragmatic phenomena, and\nsuggest directions that focus on a broader range of communicative contexts and\naffordances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Roma Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Language Models with Knowledge Graph Reasoning for Question Answering. (arXiv:2211.08380v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08380","description":"<p>Answering open-domain questions requires world knowledge about in-context\nentities. As pre-trained Language Models (LMs) lack the power to store all\nrequired knowledge, external knowledge sources, such as knowledge graphs, are\noften used to augment LMs. In this work, we propose knOwledge REasOning\nempowered Language Model (OREO-LM), which consists of a novel Knowledge\nInteraction Layer that can be flexibly plugged into existing Transformer-based\nLMs to interact with a differentiable Knowledge Graph Reasoning module\ncollaboratively. In this way, LM guides KG to walk towards the desired answer,\nwhile the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and\nT5, we show significant performance gain, achieving state-of-art results in the\nClosed-Book setting. The performance enhancement is mainly from the KG\nreasoning's capacity to infer missing relational facts. In addition, OREO-LM\nprovides reasoning paths as rationales to interpret the model's decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness. (arXiv:2211.08386v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08386","description":"<p>In this thesis, we investigated the relevance, faithfulness, and succinctness\naspects of Long Form Question Answering (LFQA). LFQA aims to generate an\nin-depth, paragraph-length answer for a given question, to help bridge the gap\nbetween real scenarios and the existing open-domain QA models which can only\nextract short-span answers. LFQA is quite challenging and under-explored. Few\nworks have been done to build an effective LFQA system. It is even more\nchallenging to generate a good-quality long-form answer relevant to the query\nand faithful to facts, since a considerable amount of redundant, complementary,\nor contradictory information will be contained in the retrieved documents.\nMoreover, no prior work has been investigated to generate succinct answers. We\nare among the first to research the LFQA task. We pioneered the research\ndirection to improve the answer quality in terms of 1) query-relevance, 2)\nanswer faithfulness, and 3) answer succinctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation. (arXiv:2211.08387v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08387","description":"<p>Lexically constrained text generation is one of the constrained text\ngeneration tasks, which aims to generate text that covers all the given\nconstraint lexicons. While the existing approaches tackle this problem using a\nlexically constrained beam search algorithm or dedicated model using\nnon-autoregressive decoding, there is a trade-off between the generated text\nquality and the hard constraint satisfaction. We introduce AutoTemplate, a\nsimple yet effective lexically constrained text generation framework divided\ninto template generation and lexicalization tasks. The template generation is\nto generate the text with the placeholders, and lexicalization replaces them\ninto the constraint lexicons to perform lexically constrained text generation.\nWe conducted the experiments on two tasks: keywords-to-sentence generations and\nentity-guided summarization. Experimental results show that the AutoTemplate\noutperforms the competitive baselines on both tasks while satisfying the hard\nlexical constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing Semantics into Speech Encoders. (arXiv:2211.08402v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08402","description":"<p>Recent studies find existing self-supervised speech encoders contain\nprimarily acoustic rather than semantic information. As a result, pipelined\nsupervised automatic speech recognition (ASR) to large language model (LLM)\nsystems achieve state-of-the-art results on semantic spoken language tasks by\nutilizing rich semantic representations from the LLM. These systems come at the\ncost of labeled audio transcriptions, which is expensive and time-consuming to\nobtain. We propose a task-agnostic unsupervised way of incorporating semantic\ninformation from LLMs into self-supervised speech encoders without labeled\naudio transcriptions. By introducing semantics, we improve existing speech\nencoder spoken language understanding performance by over 10\\% on intent\nclassification, with modest gains in named entity resolution and slot filling,\nand spoken question answering FF1 score by over 2\\%. Our unsupervised approach\nachieves similar performance as supervised methods trained on over 100 hours of\nlabeled audio transcripts, demonstrating the feasibility of unsupervised\nsemantic augmentations to existing speech encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Derek Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_L/0/1/0/all/0/1\">Liang-Hsuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08411","description":"<p>The internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, there is a huge variability in the number of times\na given piece of information appears on the web. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in their pre-training datasets. In particular, we show that a\nlanguage model's ability to answer a fact-based question relates to how many\ndocuments associated with that question were seen during pre-training. We\nidentify these relevant documents by entity linking pre-training datasets and\ncounting documents that contain the same entities as a given question-answer\npair. Our results demonstrate strong correlational and causal relationships\nbetween accuracy and relevant document count for numerous question answering\ndatasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes\n(e.g., 176B parameters). Moreover, we find that while larger models are better\nat learning long-tail knowledge, we estimate that today's models must be scaled\nby many orders of magnitude to reach competitive QA performance on questions\nwith little support in the pre-training data. Finally, we show that\nretrieval-augmentation can reduce the dependence on relevant document count,\npresenting a promising approach for capturing the long-tail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1\">Nikhil Kandpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Haikang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Factual Consistency of Large Language Models Through Summarization. (arXiv:2211.08412v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08412","description":"<p>While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascarenhas_A/0/1/0/all/0/1\">Anisha Mascarenhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_S/0/1/0/all/0/1\">Sarah Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best-First Beam Search. (arXiv:2007.03909v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.03909","description":"<p>Decoding for many NLP tasks requires an effective heuristic algorithm for\napproximating exact search since the problem of searching the full output space\nis often intractable, or impractical in many settings. The default algorithm\nfor this job is beam search -- a pruned version of breadth-first search. Quite\nsurprisingly, beam search often returns better results than exact inference due\nto beneficial search bias for NLP tasks. In this work, we show that the\nstandard implementation of beam search can be made up to 10x faster in\npractice. Our method assumes that the scoring function is monotonic in the\nsequence length, which allows us to safely prune hypotheses that cannot be in\nthe final set of hypotheses early on. We devise effective monotonic\napproximations to popular nonmonontic scoring functions, including length\nnormalization and mutual information decoding. Lastly, we propose a\nmemory-reduced variant of Best-First Beam Search, which has a similar\nbeneficial search bias in terms of downstream performance, but runs in a\nfraction of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Guarantees for De-identifying Text Transformations. (arXiv:2008.03101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.03101","description":"<p>Machine Learning approaches to Natural Language Processing tasks benefit from\na comprehensive collection of real-life user data. At the same time, there is a\nclear need for protecting the privacy of the users whose data is collected and\nprocessed. For text collections, such as, e.g., transcripts of voice\ninteractions or patient records, replacing sensitive parts with benign\nalternatives can provide de-identification. However, how much privacy is\nactually guaranteed by such text transformations, and are the resulting texts\nstill useful for machine learning? In this paper, we derive formal privacy\nguarantees for general text transformation-based de-identification methods on\nthe basis of Differential Privacy. We also measure the effect that different\nways of masking private information in dialog transcripts have on a subsequent\nmachine learning task. To this end, we formulate different masking strategies\nand compare their privacy-utility trade-offs. In particular, we compare a\nsimple redact approach with more sophisticated word-by-word replacement using\ndeep learning models on multiple natural language understanding tasks like\nnamed entity recognition, intent detection, and dialog act classification. We\nfind that only word-by-word replacement is robust against performance drops in\nvarious tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davody_A/0/1/0/all/0/1\">Ali Davody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing. (arXiv:2103.02227v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02227","description":"<p>Data augmentation has attracted a lot of research attention in the deep\nlearning era for its ability in alleviating data sparseness. The lack of\nlabeled data for unseen evaluation databases is exactly the major challenge for\ncross-domain text-to-SQL parsing. Previous works either require human\nintervention to guarantee the quality of generated data, or fail to handle\ncomplex SQL queries. This paper presents a simple yet effective data\naugmentation framework. First, given a database, we automatically produce a\nlarge number of SQL queries based on an abstract syntax tree grammar. For\nbetter distribution matching, we require that at least 80% of SQL patterns in\nthe training data are covered by generated queries. Second, we propose a\nhierarchical SQL-to-question generation model to obtain high-quality natural\nlanguage questions, which is the major contribution of this work. Finally, we\ndesign a simple sampling strategy that can greatly improve training efficiency\ngiven large amounts of generated data. Experiments on three cross-domain\ndatasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that\nour proposed data augmentation framework can consistently improve performance\nover strong baselines, and the hierarchical generation component is the key for\nthe improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning Event Extraction: Approaches and Applications. (arXiv:2107.02126v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02126","description":"<p>Event extraction (EE) is a crucial research task for promptly apprehending\nevent information from massive textual data. With the rapid development of deep\nlearning, EE based on deep learning technology has become a research hotspot.\nNumerous methods, datasets, and evaluation metrics have been proposed in the\nliterature, raising the need for a comprehensive and updated survey. This\narticle fills the research gap by reviewing the state-of-the-art approaches,\nespecially focusing on the general domain EE based on deep learning models. We\nintroduce a new literature classification of current general domain EE research\naccording to the task definition. Afterward, we summarize the paradigm and\nmodels of EE approaches, and then discuss each of them in detail. As an\nimportant aspect, we summarize the benchmarks that support tests of predictions\nand evaluation metrics. A comprehensive comparison among different approaches\nis also provided in this survey. Finally, we conclude by summarizing future\nresearch directions facing the research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1\">Amin Beheshti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07410","description":"<p>Given the recent proliferation of false claims online, there has been a lot\nof manual fact-checking effort. As this is very time-consuming, human\nfact-checkers can benefit from tools that can support them and make them more\nefficient. Here, we focus on building a system that could provide such support.\nGiven an input document, it aims to detect all sentences that contain a claim\nthat can be verified by some previously fact-checked claims (from a given\ndatabase). The output is a re-ranked list of the document sentences, so that\nthose that can be verified are ranked as high as possible, together with\ncorresponding evidence. Unlike previous work, which has looked into claim\nretrieval, here we take a document-level perspective. We create a new manually\nannotated dataset for this task, and we propose suitable evaluation measures.\nWe further experiment with a learning-to-rank approach, achieving sizable\nperformance gains over several strong baselines. Our analysis demonstrates the\nimportance of modeling text similarity and stance, while also taking into\naccount the veracity of the retrieved previously fact-checked claims. We\nbelieve that this research would be of interest to fact-checkers, journalists,\nmedia, and regulatory authorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_N/0/1/0/all/0/1\">Nikola Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Aisha Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.07031","description":"<p>An interactive instruction following task has been proposed as a benchmark\nfor learning to map natural language instructions and first-person vision into\nsequences of actions to interact with objects in 3D environments. We found that\nan existing end-to-end neural model for this task tends to fail to interact\nwith objects of unseen attributes and follow various instructions. We assume\nthat this problem is caused by the high sensitivity of neural feature\nextraction to small changes in vision and language inputs. To mitigate this\nproblem, we propose a neuro-symbolic approach that utilizes high-level symbolic\nfeatures, which are robust to small changes in raw inputs, as intermediate\nrepresentations. We verify the effectiveness of our model with the subtask\nevaluation on the ALFRED benchmark. Our experiments show that our approach\nsignificantly outperforms the end-to-end neural model by 9, 46, and 74 points\nin the success rate on the ToggleObject, PickupObject, and SliceObject subtasks\nin unseen environments respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Language Modeling. (arXiv:2110.08413v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08413","description":"<p>Large pretrained language models are critical components of modern NLP\npipelines. Yet, they suffer from spurious correlations, poor out-of-domain\ngeneralization, and biases. Inspired by recent progress in causal machine\nlearning, in particular the invariant risk minimization (IRM) paradigm, we\npropose invariant language modeling, a framework for learning invariant\nrepresentations that generalize better across multiple environments. In\nparticular, we adapt a game-theoretic formulation of IRM (IRM-games) to\nlanguage models, where the invariance emerges from a specific training schedule\nin which all the environments compete to optimize their own\nenvironment-specific loss by updating subsets of the model in a round-robin\nfashion. We focus on controlled experiments to precisely demonstrate the\nability of our method to (i) remove structured noise, (ii) ignore specific\nspurious correlations without affecting global performance, and (iii) achieve\nbetter out-of-domain generalization. These benefits come with a negligible\ncomputational overhead compared to standard training, do not require changing\nthe local loss, and can be applied to any language model. We believe this\nframework is promising to help mitigate spurious correlations and biases in\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghotra_S/0/1/0/all/0/1\">Sarvjeet Singh Ghotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vidhan Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carignan_D/0/1/0/all/0/1\">Dean Carignan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1\">Emre Kiciman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05955","description":"<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI improves performance on eight out-of-domain test sets we\nconsider, including by 11% on HANS and 9% on Adversarial NLI, compared to\ntraining on the 4x larger MultiNLI. Moreover, it continues to be more effective\nthan MultiNLI augmented with other NLI datasets. Our results demonstrate the\npromise of leveraging natural language generation techniques and re-imagining\nthe role of humans in the dataset creation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11176","description":"<p>Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\nare weak in recognizing coherence, and thus are not reliable in a way to spot\nthe discourse-level improvements of those text generation systems. In this\nwork, we introduce DiscoScore, a parametrized discourse metric, which uses BERT\nto model discourse coherence from different perspectives, driven by Centering\ntheory. Our experiments encompass 16 non-discourse and discourse metrics,\nincluding DiscoScore and popular coherence models, evaluated on summarization\nand document-level machine translation (MT). We find that (i) the majority of\nBERT-based metrics correlate much worse with human rated coherence than early\ndiscourse metrics, invented a decade ago; (ii) the recent state-of-the-art\nBARTScore is weak when operated at system level -- which is particularly\nproblematic as systems are typically compared in this manner. DiscoScore, in\ncontrast, achieves strong system-level correlation with human ratings, not only\nin coherence but also in factual consistency and other aspects, and surpasses\nBARTScore by over 10 correlation points on average. Further, aiming to\nunderstand DiscoScore, we provide justifications to the importance of discourse\ncoherence for evaluation metrics, and explain the superiority of one variant\nover another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1\">Michael Strube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10232","description":"<p>In this paper, we present DuReader_retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader_retrieval contains more than 90K queries and\nover 8M unique passages from a commercial search engine. To alleviate the\nshortcomings of other datasets and ensure the quality of our benchmark, we (1)\nreduce the false negatives in development and test sets by manually annotating\nresults pooled from multiple retrievers, and (2) remove the training queries\nthat are semantically similar to the development and testing queries.\nAdditionally, we provide two out-of-domain testing sets for cross-domain\nevaluation, as well as a set of human translated queries for for cross-lingual\nretrieval evaluation. The experiments demonstrate that DuReader_retrieval is\nchallenging and a number of problems remain unsolved, such as the salient\nphrase mismatch and the syntactic mismatch between queries and paragraphs.\nThese experiments also show that dense retrievers do not generalize well across\ndomains, and cross-lingual retrieval is essentially challenging.\nDuReader_retrieval is publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yifu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dual Encoder Architectures for Question Answering. (arXiv:2204.07120v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07120","description":"<p>Dual encoders have been used for question-answering (QA) and information\nretrieval (IR) tasks with good results. Previous research focuses on two major\ntypes of dual encoders, Siamese Dual Encoder (SDE), with parameters shared\nacross two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly\nparameterized encoders. In this work, we explore different ways in which the\ndual encoder can be structured, and evaluate how these differences can affect\ntheir efficacy in terms of QA retrieval tasks. By evaluating on MS MARCO, open\ndomain NQ and the MultiReQA benchmarks, we show that SDE performs significantly\nbetter than ADE. We further propose three different improved versions of ADEs\nby sharing or freezing parts of the architectures between two encoder towers.\nWe find that sharing parameters in projection layers would enable ADEs to\nperform competitively with or outperform SDEs. We further explore and explain\nwhy parameter sharing in projection layer significantly improves the efficacy\nof the dual encoders, by directly probing the embedding spaces of the two\nencoder towers with t-SNE algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1\">Daniel M. Bikel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonseca_E/0/1/0/all/0/1\">Enrique Alfonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitouni_I/0/1/0/all/0/1\">Imed Zitouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10989","description":"<p>Dialogue meaning representation formulates natural language utterance\nsemantics in their conversational context in an explicit and machine-readable\nform. Previous work typically follows the intent-slot framework, which is easy\nfor annotation yet limited in scalability for complex linguistic expressions. A\nline of works alleviates the representation issue by introducing hierarchical\nstructures but challenging to express complex compositional semantics, such as\nnegation and coreference. We propose Dialogue Meaning Representation (DMR), a\npliable and easily extendable representation for task-oriented dialogue. Our\nrepresentation contains a set of nodes and edges to represent rich\ncompositional semantics. Moreover, we propose an inheritance hierarchy\nmechanism focusing on domain extensibility. Additionally, we annotated\nDMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with\nDMR. We propose two evaluation tasks to evaluate different dialogue models and\na novel coreference resolution model GNNCoref for the graph-based coreference\nresolution task. Experiments show that DMR can be parsed well with pre-trained\nSeq2Seq models, and GNNCoref outperforms the baseline models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangkun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankGen: Improving Text Generation with Large Ranking Models. (arXiv:2205.09726v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09726","description":"<p>Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues we present RankGen, a 1.2B parameter encoder\nmodel for English that scores model generations given a prefix. RankGen can be\nflexibly incorporated as a scoring function in beam search and used to decode\nfrom any pretrained language model. We train RankGen using large-scale\ncontrastive learning to map a prefix close to the ground-truth sequence that\nfollows it and far away from two types of negatives: (1) random sequences from\nthe same document as the prefix, and (2) sequences generated from a large\nlanguage model conditioned on the prefix. Experiments across four different\nlanguage models (345M-11B parameters) and two domains show that RankGen\nsignificantly outperforms decoding algorithms like nucleus, top-k, and typical\nsampling, as well as contrastive decoding and search, on both automatic metrics\n(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English\nwriters (74.5% human preference over nucleus sampling). Analysis reveals that\nRankGen outputs are more relevant to the prefix and improve continuity and\ncoherence compared to baselines. We release our model checkpoints, code, and\nhuman preference data with explanations to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yapei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fine-grained Interpretability Evaluation Benchmark for Neural NLP. (arXiv:2205.11097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11097","description":"<p>While there is increasing concern about the interpretability of neural\nmodels, the evaluation of interpretability remains an open problem, due to the\nlack of proper evaluation datasets and metrics. In this paper, we present a\nnovel benchmark to evaluate the interpretability of both neural models and\nsaliency methods. This benchmark covers three representative NLP tasks:\nsentiment analysis, textual similarity and reading comprehension, each provided\nwith both English and Chinese annotated data. In order to precisely evaluate\nthe interpretability, we provide token-level rationales that are carefully\nannotated to be sufficient, compact and comprehensive. We also design a new\nmetric, i.e., the consistency between the rationales before and after\nperturbations, to uniformly evaluate the interpretability on different types of\ntasks. Based on this benchmark, we conduct experiments on three typical models\nwith three saliency methods, and unveil their strengths and weakness in terms\nof interpretability. We will release this benchmark\nhttps://www.luge.ai/#/luge/task/taskDetail?taskId=15 and hope it can facilitate\nthe research in building trustworthy systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaozong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11697","description":"<p>Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.29\\% without using a language model and 4.05\\% with an external language\nmodel on the testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liuwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jie Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. (arXiv:2210.03797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03797","description":"<p>Recent progress in language model pre-training has led to important\nimprovements in Named Entity Recognition (NER). Nonetheless, this progress has\nbeen mainly tested in well-formatted documents such as news, Wikipedia, or\nscientific articles. In social media the landscape is different, in which it\nadds another layer of complexity due to its noisy and dynamic nature. In this\npaper, we focus on NER in Twitter, one of the largest social media platforms,\nand construct a new NER dataset, TweetNER7, which contains seven entity types\nannotated over 11,382 tweets from September 2019 to August 2021. The dataset\nwas constructed by carefully distributing the tweets over time and taking\nrepresentative trends as a basis. Along with the dataset, we provide a set of\nlanguage model baselines and perform an analysis on the language model\nperformance on the task, especially analyzing the impact of different time\nperiods. In particular, we focus on three important temporal aspects in our\nanalysis: short-term degradation of NER models over time, strategies to\nfine-tune a language model over different periods, and self-labeling as an\nalternative to lack of recently-labeled data. TweetNER7 is released publicly\n(https://huggingface.co/datasets/tner/tweetner7) along with the models\nfine-tuned on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vitor Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05098","description":"<p>The ability to extract high-quality translation dictionaries from monolingual\nword embedding spaces depends critically on the geometric similarity of the\nspaces -- their degree of \"isomorphism.\" We address the root-cause of faulty\ncross-lingual mapping: that word embedding training resulted in the underlying\nspaces being non-isomorphic. We incorporate global measures of isomorphism\ndirectly into the Skip-gram loss function, successfully increasing the relative\nisomorphism of trained word embedding spaces and improving their ability to be\nmapped to a shared cross-lingual space. The result is improved bilingual\nlexicon induction in general data conditions, under domain mismatch, and with\ntraining algorithm dissimilarities. We release IsoVec at\nhttps://github.com/kellymarchisio/isovec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Neha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13952","description":"<p>We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md. Mahabub Faisal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornec_O/0/1/0/all/0/1\">Owen Cornec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Massimiliano Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering. (arXiv:2210.14353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14353","description":"<p>We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large language models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this work, we propose a\nframework to evaluate the robustness of linguistic representations using\nprobing tasks. We leverage recent advances in extracting emergent linguistic\nconstructs from LLMs and apply syntax-preserving perturbations to test the\nstability of these constructs in order to better understand the representations\nlearned by LLMs. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures. We provide evidence that\ncontext-free representation (e.g., GloVe) are in some cases competitive with\ncontext-dependent representations from modern LLMs (e.g., BERT), yet equally\nbrittle to syntax-preserving manipulations. Emergent syntactic representations\nin neural networks are brittle, thus our work poses the attention on the risk\nof comparing such structures to those that are object of a long lasting debate\nin linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatkowska_M/0/1/0/all/0/1\">Marta Kiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token. (arXiv:2211.04898v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04898","description":"<p>The pre-training of masked language models (MLMs) consumes massive\ncomputation to achieve good results on downstream NLP tasks, resulting in a\nlarge carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as\nplaceholders and gather the contextualized information from unmasked tokens to\nrestore the corrupted information. It raises the question of whether we can\nappend [MASK]s at a later layer, to reduce the sequence length for earlier\nlayers and make the pre-training more efficient. We show: (1) [MASK]s can\nindeed be appended at a later layer, being disentangled from the word\nembedding; (2) The gathering of contextualized information from unmasked tokens\ncan be conducted with a few layers. By further increasing the masking rate from\n15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with\nonly 78% and 68% of the original computational budget without any degradation\non the GLUE benchmark. When pre-training with the original budget, our method\noutperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1\">Sanjika Hewavitharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.06774","description":"<p>When trained on large-scale datasets, image captioning models can understand\nthe content of images from a general domain but often fail to generate\naccurate, detailed captions. To improve performance, pretraining-and-finetuning\nhas been a key strategy for image captioning. However, we find that large-scale\nbidirectional training between image and text enables zero-shot image\ncaptioning. In this paper, we introduce Bidirectional Image Text Training in\nlargER Scale, BITTERS, an efficient training and inference framework for\nzero-shot image captioning. We also propose a new evaluation benchmark which\ncomprises of high quality datasets and an extensive set of metrics to properly\nevaluate zero-shot captioning accuracy and societal bias. We additionally\nprovide an efficient finetuning approach for keyword extraction. We show that\ncareful selection of large-scale training set and model architecture is the key\nto achieving zero-shot image captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsden_M/0/1/0/all/0/1\">Mark Marsden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1\">Pyunghwan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_A/0/1/0/all/0/1\">Alessandra Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What would Harry say? Building Dialogue Agents for Characters in a Story. (arXiv:2211.06869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06869","description":"<p>We present HPD: Harry Potter Dialogue Dataset to facilitate the study of\nbuilding dialogue agents for characters in a story. It differs from existing\ndialogue datasets in two aspects: 1) HPD provides rich background information\nabout the novel Harry Potter, including scene, character attributes, and\ncharacter relations; 2) All these background information will change as the\nstory goes on. In other words, each dialogue session in HPD correlates to a\ndifferent background, and the storyline determines how the background changes.\nWe evaluate some baselines (e.g., GPT-2, BOB) on both automatic and human\nmetrics to determine how well they can generate Harry Potter-like responses.\nExperimental results indicate that although the generated responses are fluent\nand relevant to the dialogue history, they are remained to sound out of\ncharacter for Harry, indicating there is a large headroom for future studies.\nOur dataset is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Adversarial Training with Robust Early-Bird Tickets. (arXiv:2211.07263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07263","description":"<p>Adversarial training is one of the most powerful methods to improve the\nrobustness of pre-trained language models (PLMs). However, this approach is\ntypically more expensive than traditional fine-tuning because of the necessity\nto generate adversarial examples via gradient descent. Delving into the\noptimization process of adversarial training, we find that robust connectivity\npatterns emerge in the early training phase (typically $0.15\\sim0.3$ epochs),\nfar before parameters converge. Inspired by this finding, we dig out robust\nearly-bird tickets (i.e., subnetworks) to develop an efficient adversarial\ntraining method: (1) searching for robust tickets with structured sparsity in\nthe early stage; (2) fine-tuning robust tickets in the remaining time. To\nextract the robust tickets as early as possible, we design a ticket convergence\nmetric to automatically terminate the searching process. Experiments show that\nthe proposed efficient adversarial training method can achieve up to $7\\times\n\\sim 13 \\times$ training speedups while maintaining comparable or even better\nrobustness compared to the most competitive state-of-the-art adversarial\ntraining methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhiheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing and Adversarial: Improve ASR with Speaker Labels. (arXiv:2211.06369v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2211.06369","description":"<p>ASR can be improved by multi-task learning (MTL) with domain enhancing or\ndomain adversarial training, which are two opposite objectives with the aim to\nincrease/decrease domain variance towards domain-aware/agnostic ASR,\nrespectively. In this work, we study how to best apply these two opposite\nobjectives with speaker labels to improve conformer-based ASR. We also propose\na novel adaptive gradient reversal layer for stable and effective adversarial\ntraining without tuning effort. Detailed analysis and experimental verification\nare conducted to show the optimal positions in the ASR neural network (NN) to\napply speaker enhancing and adversarial training. We also explore their\ncombination for further improvement, achieving the same performance as\ni-vectors plus adversarial training. Our best speaker-based MTL achieves 7\\%\nrelative improvement on the Switchboard Hub5'00 set. We also investigate the\neffect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Haotian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}