{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts. (arXiv:2211.11772v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11772","description":"<p>Classroom discourse is a core medium of instruction -- analyzing it can\nprovide a window into teaching and learning as well as driving the development\nof new tools for improving instruction. We introduce the largest dataset of\nmathematics classroom transcripts available to researchers, and demonstrate how\nthis data can help improve instruction. The dataset consists of 1,660 45-60\nminute long 4th and 5th grade elementary mathematics observations collected by\nthe National Center for Teacher Effectiveness (NCTE) between 2010-2013. The\nanonymized transcripts represent data from 317 teachers across 4 school\ndistricts that serve largely historically marginalized students. The\ntranscripts come with rich metadata, including turn-level annotations for\ndialogic discourse moves, classroom observation scores, demographic\ninformation, survey responses and student test scores. We demonstrate that our\nnatural language processing model, trained on our turn-level annotations, can\nlearn to identify dialogic discourse moves and these moves are correlated with\nbetter classroom observation scores and learning outcomes. This dataset opens\nup several possibilities for researchers, educators and policymakers to learn\nabout and improve K-12 instruction. The data and its terms of use can be\naccessed here: https://github.com/ddemszky/classroom-transcript-analysis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1\">Dorottya Demszky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_H/0/1/0/all/0/1\">Heather Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can You Label Less by Using Out-of-Domain Data? Active & Transfer Learning with Few-shot Instructions. (arXiv:2211.11798v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11798","description":"<p>Labeling social-media data for custom dimensions of toxicity and social bias\nis challenging and labor-intensive. Existing transfer and active learning\napproaches meant to reduce annotation effort require fine-tuning, which suffers\nfrom over-fitting to noise and can cause domain shift with small sample sizes.\nIn this work, we propose a novel Active Transfer Few-shot Instructions (ATF)\napproach which requires no fine-tuning. ATF leverages the internal linguistic\nknowledge of pre-trained language models (PLMs) to facilitate the transfer of\ninformation from existing pre-labeled datasets (source-domain task) with\nminimum labeling effort on unlabeled target data (target-domain task). Our\nstrategy can yield positive transfer achieving a mean AUC gain of 10.5%\ncompared to no transfer with a large 22b parameter PLM. We further show that\nannotation of just a few target-domain samples via active learning can be\nbeneficial for transfer, but the impact diminishes with more annotation effort\n(26% drop in gain between 100 and 2000 annotated examples). Finally, we find\nthat not all transfer scenarios yield a positive gain, which seems related to\nthe PLMs initial performance on the target-domain task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1\">Rafal Kocielnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangaslahti_S/0/1/0/all/0/1\">Sara Kangaslahti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hari_M/0/1/0/all/0/1\">Meena Hari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_R/0/1/0/all/0/1\">R. Michael Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised extraction, labelling and clustering of segments from clinical notes. (arXiv:2211.11799v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11799","description":"<p>This work is motivated by the scarcity of tools for accurate, unsupervised\ninformation extraction from unstructured clinical notes in computationally\nunderrepresented languages, such as Czech. We introduce a stepping stone to a\nbroad array of downstream tasks such as summarisation or integration of\nindividual patient records, extraction of structured information for national\ncancer registry reporting or building of semi-structured semantic patient\nrepresentations for computing patient embeddings. More specifically, we present\na method for unsupervised extraction of semantically-labelled textual segments\nfrom clinical notes and test it out on a dataset of Czech breast cancer\npatients, provided by Masaryk Memorial Cancer Institute (the largest Czech\nhospital specialising in oncology). Our goal was to extract, classify (i.e.\nlabel) and cluster segments of the free-text notes that correspond to specific\nclinical features (e.g., family background, comorbidities or toxicities). The\npresented results demonstrate the practical relevance of the proposed approach\nfor building more sophisticated extraction and analytical pipelines deployed on\nCzech clinical notes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelina_P/0/1/0/all/0/1\">Petr Zelina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halamkova_J/0/1/0/all/0/1\">Jana Hal&#xe1;mkov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novacek_V/0/1/0/all/0/1\">V&#xed;t Nov&#xe1;&#x10d;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference. (arXiv:2211.11875v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11875","description":"<p>While large pre-trained language models are powerful, their predictions often\nlack logical consistency across test inputs. For example, a state-of-the-art\nMacaw question-answering (QA) model answers 'Yes' to 'Is a sparrow a bird?' and\n'Does a bird have feet?' but answers 'No' to 'Does a sparrow have feet?'. To\naddress this failure mode, we propose a framework, Consistency Correction\nthrough Relation Detection, or ConCoRD, for boosting the consistency and\naccuracy of pre-trained NLP models using pre-trained natural language inference\n(NLI) models without fine-tuning or re-training. Given a batch of test inputs,\nConCoRD samples several candidate outputs for each input and instantiates a\nfactor graph that accounts for both the model's belief about the likelihood of\neach answer choice in isolation and the NLI model's beliefs about pair-wise\nanswer choice compatibility. We show that a weighted MaxSAT solver can\nefficiently compute high-quality answer choices under this factor graph,\nimproving over the raw model's predictions. Our experiments demonstrate that\nConCoRD consistently boosts accuracy and consistency of off-the-shelf\nclosed-book QA and VQA models using off-the-shelf NLI models, notably\nincreasing accuracy of LXMERT on ConVQA by 5% absolute. See\nhttps://ericmitchell.ai/emnlp-2022-concord/ for code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Joseph J. Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_W/0/1/0/all/0/1\">William S. Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ananth Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Patrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEMPERA: Test-Time Prompting via Reinforcement Learning. (arXiv:2211.11890v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11890","description":"<p>Careful prompt design is critical to the use of large language models in\nzero-shot or few-shot learning. As a consequence, there is a growing interest\nin automated methods to design optimal prompts. In this work, we propose\nTest-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to\nprior prompt generation methods, TEMPERA can efficiently leverage prior\nknowledge, is adaptive to different queries and provides an interpretable\nprompt for every query. To achieve this, we design a novel action space that\nallows flexible editing of the initial prompts covering a wide set of\ncommonly-used components like instructions, few-shot exemplars, and\nverbalizers. The proposed method achieves significant gains compared with\nrecent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a\nvariety of tasks including sentiment analysis, topic classification, natural\nlanguage inference, and reading comprehension. Our method achieves 5.33x on\naverage improvement in sample efficiency when compared to the traditional\nfine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Knowledge Dependency of Questions. (arXiv:2211.11902v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11902","description":"<p>The automatic generation of Multiple Choice Questions (MCQ) has the potential\nto reduce the time educators spend on student assessment significantly.\nHowever, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,\nand METEOR, focus on the n-gram based similarity of the generated MCQ to the\ngold sample in the dataset and disregard their educational value. They fail to\nevaluate the MCQ's ability to assess the student's knowledge of the\ncorresponding target fact. To tackle this issue, we propose a novel automatic\nevaluation metric, coined Knowledge Dependent Answerability (KDA), which\nmeasures the MCQ's answerability given knowledge of the target fact.\nSpecifically, we first show how to measure KDA based on student responses from\na human survey. Then, we propose two automatic evaluation metrics, KDA_disc and\nKDA_cont, that approximate KDA by leveraging pre-trained language models to\nimitate students' problem-solving behavior. Through our human studies, we show\nthat KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)\nusability in an actual classroom setting, labeled by experts. Furthermore, when\ncombined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown\nto have a strong predictive power for various expert-labeled MCQ quality\nmeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeongdon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yoonseok Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Myeongho Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Juneyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsam Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seungtaek Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best-$k$ Search Algorithm for Neural Text Generation. (arXiv:2211.11924v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11924","description":"<p>Modern natural language generation paradigms require a good decoding strategy\nto obtain quality sequences out of the model. Beam search yields high-quality\nbut low diversity outputs; stochastic approaches suffer from high variance and\nsometimes low quality, but the outputs tend to be more natural and creative. In\nthis work, we propose a deterministic search algorithm balancing both quality\nand diversity. We first investigate the vanilla best-first search (BFS)\nalgorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we\ngreedily expand the top $k$ nodes, instead of only the first node, to boost\nefficiency and diversity. Upweighting recently discovered nodes accompanied by\nheap pruning ensures the completeness of the search procedure. Experiments on\nfour NLG tasks, including question generation, commonsense generation, text\nsummarization, and translation, show that best-$k$ search yields more diverse\nand natural outputs compared to strong baselines, while our approach maintains\nhigh text quality. The proposed algorithm is parameter-free, lightweight,\nefficient, and easy to use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Backdoor Attack and Defense in Natural Language Processing. (arXiv:2211.11958v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11958","description":"<p>Deep learning is becoming increasingly popular in real-life applications,\nespecially in natural language processing (NLP). Users often choose training\noutsourcing or adopt third-party data and models due to data and computation\nresources being limited. In such a situation, training data and models are\nexposed to the public. As a result, attackers can manipulate the training\nprocess to inject some triggers into the model, which is called backdoor\nattack. Backdoor attack is quite stealthy and difficult to be detected because\nit has little inferior influence on the model's performance for the clean\nsamples. To get a precise grasp and understanding of this problem, in this\npaper, we conduct a comprehensive review of backdoor attacks and defenses in\nthe field of NLP. Besides, we summarize benchmark datasets and point out the\nopen issues to design credible systems to defend against backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xuan Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhaoyang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangmao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems. (arXiv:2211.11982v1 [cs.CL])","link":"http://arxiv.org/abs/2211.11982","description":"<p>We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for\ncommercial text-based task-oriented dialog (TOD) systems. BotSIM consists of\nthree major components: 1) a Generator that can infer semantic-level dialog\nacts and entities from bot definitions and generate user queries via\nmodel-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to\nsimulate conversations with the dialog agents; 3) a Remediator to analyze the\nsimulated conversations, visualize the bot health reports and provide\nactionable remediation suggestions for bot troubleshooting and improvement. We\ndemonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and\nmulti-intent dialog generation via case studies on two commercial bot\nplatforms. BotSIM's \"generation-simulation-remediation\" paradigm accelerates\nthe end-to-end bot evaluation and iteration process by: 1) reducing manual test\ncases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU\nand end-to-end performance via extensive dialog simulation; 3) improving the\nbot troubleshooting process with actionable suggestions. A demo of our system\ncan be found at https://tinyurl.com/mryu74cd and a demo video at\nhttps://youtu.be/qLi5iSoly30.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangsen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Au_J/0/1/0/all/0/1\">Jimmy Au</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English. (arXiv:2211.12000v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12000","description":"<p>We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -\nEnglish Speech Translation Corpus. This corpus is an extension of the ArzEn\nspeech corpus, which was collected through informal interviews with bilingual\nspeakers. In this work, we collect translations in both directions, monolingual\nEgyptian Arabic and monolingual English, forming a three-way speech translation\ncorpus. We make the translation guidelines and corpus publicly available. We\nalso report results for baseline systems for machine translation and speech\ntranslation tasks. We believe this is a valuable resource that can motivate and\nfacilitate further research studying the code-switching phenomenon from a\nlinguistic perspective and can be used to train and evaluate NLP systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12054","description":"<p>Large-scale commonsense knowledge bases empower a broad range of AI\napplications, where the automatic extraction of commonsense knowledge (CKE) is\na fundamental and challenging problem. CKE from text is known for suffering\nfrom the inherent sparsity and reporting bias of commonsense in text. Visual\nperception, on the other hand, contains rich commonsense knowledge about\nreal-world entities, e.g., (person, can_hold, bottle), which can serve as\npromising sources for acquiring grounded commonsense knowledge. In this work,\nwe present CLEVER, which formulates CKE as a distantly supervised\nmulti-instance learning problem, where models learn to summarize commonsense\nrelations from a bag of images about an entity pair without any human\nannotation on image instances. To address the problem, CLEVER leverages\nvision-language pre-training models for deep understanding of each image in the\nbag, and selects informative instances from the bag to summarize commonsense\nentity relations via a novel contrastive attention mechanism. Comprehensive\nexperimental results in held-out and human evaluation show that CLEVER can\nextract commonsense knowledge in promising quality, outperforming pre-trained\nlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted\ncommonsense scores show strong correlation with human judgment with a 0.78\nSpearman coefficient. Moreover, the extracted commonsense can also be grounded\ninto images with reasonable interpretability. The data and codes can be\nobtained at https://github.com/thunlp/CLEVER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models. (arXiv:2211.12092v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12092","description":"<p>The simplest way to obtain continuous interpolation between two points in\nhigh dimensional space is to draw a line between them. While previous works\nfocused on the general connectivity between model parameters, we explored\nlinear interpolation for parameters of pre-trained models after fine-tuning.\nSurprisingly, we could perform linear interpolation without a performance drop\nin intermediate points for fine-tuned models. For controllable text generation,\nsuch interpolation could be seen as moving a model towards or against the\ndesired text attribute (e.g., positive sentiment), which could be used as\ngrounds for further methods for controllable text generation without inference\nspeed overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rofin_M/0/1/0/all/0/1\">Mark Rofin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk. (arXiv:2211.12118v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12118","description":"<p>One of the challenges of developing a summarization model arises from the\ndifficulty in measuring the factual inconsistency of the generated text. In\nthis study, we reinterpret the decoder overconfidence-regularizing objective\nsuggested in (Miao et al., 2021) as a hallucination risk measurement to better\nestimate the quality of generated summaries. We propose a reference-free\nmetric, HaRiM+, which only requires an off-the-shelf summarization model to\ncompute the hallucination risk based on token likelihoods. Deploying it\nrequires no additional training of models or ad-hoc modules, which usually need\nalignment to human judgments. For summary-quality estimation, HaRiM+ records\nstate-of-the-art correlation to human judgment on three summary-quality\nannotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits\nthe use of summarization models, facilitates the progress of both automated\nevaluation and generation of summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Seonil Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jeong-in Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Dataset for Biomedical Keyphrase Generation. (arXiv:2211.12124v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12124","description":"<p>Keyphrase generation is the task consisting in generating a set of words or\nphrases that highlight the main topics of a document. There are few datasets\nfor keyphrase generation in the biomedical domain and they do not meet the\nexpectations in terms of size for training generative models. In this paper, we\nintroduce kp-biomed, the first large-scale biomedical keyphrase generation\ndataset with more than 5M documents collected from PubMed abstracts. We train\nand release several generative models and conduct a series of experiments\nshowing that using large scale datasets improves significantly the performances\nfor present and absent keyphrase generation. The dataset is available under\nCC-BY-NC v4.0 license at https://huggingface.co/ datasets/taln-ls2n/kpbiomed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houbre_M/0/1/0/all/0/1\">Mael Houbre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daille_B/0/1/0/all/0/1\">Beatrice Daille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12130","description":"<p>Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenxuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference Resolution through a seq2seq Transition-Based System. (arXiv:2211.12142v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12142","description":"<p>Most recent coreference resolution systems use search algorithms over\npossible spans to identify mentions and resolve coreference. We instead present\na coreference resolution system that uses a text-to-text (seq2seq) paradigm to\npredict mentions and links jointly. We implement the coreference system as a\ntransition system and use multilingual T5 as an underlying language model. We\nobtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score\nfor English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))\nusing only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than\nprevious work) and 74.3 F1-score for Chinese (+5.3). In addition we use the\nSemEval-2010 data sets for experiments in the zero-shot setting, a few-shot\nsetting, and supervised setting using all available training data. We get\nsubstantially higher zero-shot F1-scores for 3 out of 4 languages than previous\napproaches and significantly exceed previous supervised state-of-the-art\nresults for all five tested languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022. (arXiv:2211.12154v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12154","description":"<p>The Event Causality Identification Shared Task of CASE 2022 involved two\nsubtasks working on the Causal News Corpus. Subtask 1 required participants to\npredict if a sentence contains a causal relation or not. This is a supervised\nbinary classification task. Subtask 2 required participants to identify the\nCause, Effect and Signal spans per causal sentence. This could be seen as a\nsupervised sequence labeling task. For both subtasks, participants uploaded\ntheir predictions for a held-out test set, and ranking was done based on binary\nF1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes\nthe work of the 17 teams that submitted their results to our competition and 12\nsystem description papers that were received. The best F1 scores achieved for\nSubtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing\napproaches involved pre-trained language models fine-tuned to the targeted\ntask. We further discuss these approaches and analyze errors across\nparticipants' systems in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fiona Anting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1\">Ali H&#xfc;rriyeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uca_O/0/1/0/all/0/1\">Onur Uca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liza_F/0/1/0/all/0/1\">Farhana Ferdousi Liza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oostdijk_N/0/1/0/all/0/1\">Nelleke Oostdijk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PESE: Event Structure Extraction using Pointer Network based Encoder-Decoder Architecture. (arXiv:2211.12157v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12157","description":"<p>The task of event extraction (EE) aims to find the events and event-related\nargument information from the text and represent them in a structured format.\nMost previous works try to solve the problem by separately identifying multiple\nsubstructures and aggregating them to get the complete event structure. The\nproblem with the methods is that it fails to identify all the interdependencies\namong the event participants (event-triggers, arguments, and roles). In this\npaper, we represent each event record in a unique tuple format that contains\ntrigger phrase, trigger type, argument phrase, and corresponding role\ninformation. Our proposed pointer network-based encoder-decoder model generates\nan event tuple in each time step by exploiting the interactions among event\nparticipants and presenting a truly end-to-end solution to the EE task. We\nevaluate our model on the ACE2005 dataset, and experimental results demonstrate\nthe effectiveness of our model by achieving competitive performance compared to\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuila_A/0/1/0/all/0/1\">Alapan Kuila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeshan Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type. (arXiv:2211.12164v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12164","description":"<p>Machine generation of Arithmetic Word Problems (AWPs) is challenging as they\nexpress quantities and mathematical relationships and need to be consistent.\nML-solvers require a large annotated training set of consistent problems with\nlanguage variations. Exploiting domain-knowledge is needed for consistency\nchecking whereas LSTM-based approaches are good for producing text with\nlanguage variations. Combining these we propose a system, OLGA, to generate\nconsistent word problems of TC (Transfer-Case) type, involving object transfers\namong agents. Though we provide a dataset of consistent 2-agent TC-problems for\ntraining, only about 36% of the outputs of an LSTM-based generator are found\nconsistent. We use an extension of TC-Ontology, proposed by us previously, to\ndetermine the consistency of problems. Among the remaining 64%, about 40% have\nminor errors which we repair using the same ontology. To check consistency and\nfor the repair process, we construct an instance-specific representation (ABox)\nof an auto-generated problem. We use a sentence classifier and BERT models for\nthis task. The training set for these LMs is problem-texts where sentence-parts\nare annotated with ontology class-names. As three-agent problems are longer,\nthe percentage of consistent problems generated by an LSTM-based approach drops\nfurther. Hence, we propose an ontology-based method that extends consistent\n2-agent problems into consistent 3-agent problems. Overall, our approach\ngenerates a large number of consistent TC-type AWPs involving 2 or 3 agents. As\nABox has all the information of a problem, any annotations can also be\ngenerated. Adopting the proposed approach to generate other types of AWPs is\ninteresting future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suresh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">P Sreenivasa Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptTTS: Controllable Text-to-Speech with Text Descriptions. (arXiv:2211.12171v1 [eess.AS])","link":"http://arxiv.org/abs/2211.12171","description":"<p>Using a text description as prompt to guide the generation of text or images\n(e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and\nimage generation, in this work, we explore the possibility of utilizing text\ndescriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS)\nsystem (dubbed as PromptTTS) that takes a prompt with both style and content\ndescriptions as input to synthesize the corresponding speech. Specifically,\nPromptTTS consists of a style encoder and a content encoder to extract the\ncorresponding representations from the prompt, and a speech decoder to\nsynthesize speech according to the extracted style and content representations.\nCompared with previous works in controllable TTS that require users to have\nacoustic knowledge to understand style factors such as prosody and pitch,\nPromptTTS is more user-friendly since text descriptions are a more natural way\nto express speech style (e.g., ''A lady whispers to her friend slowly''). Given\nthat there is no TTS dataset with prompts, to benchmark the task of PromptTTS,\nwe construct and release a dataset containing prompts with style and content\ninformation and the corresponding speech. Experiments show that PromptTTS can\ngenerate speech with precise style control and high speech quality. Audio\nsamples and our dataset are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1\">Zhifang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yihan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding. (arXiv:2211.12220v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12220","description":"<p>Multi-Intent Spoken Language Understanding (SLU), a novel and more complex\nscenario of SLU, is attracting increasing attention. Unlike traditional SLU,\neach intent in this scenario has its specific scope. Semantic information\noutside the scope even hinders the prediction, which tremendously increases the\ndifficulty of intent detection. More seriously, guiding slot filling with these\ninaccurate intent labels suffers error propagation problems, resulting in\nunsatisfied overall performance. To solve these challenges, in this paper, we\npropose a novel Scope-Sensitive Result Attention Network (SSRAN) based on\nTransformer, which contains a Scope Recognizer (SR) and a Result Attention\nNetwork (RAN). Scope Recognizer assignments scope information to each token,\nreducing the distraction of out-of-scope tokens. Result Attention Network\neffectively utilizes the bidirectional interaction between results of slot\nfilling and intent detection, mitigating the error propagation problem.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (5.4\\% and 2.1\\% on Overall accuracy) over the\nstate-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lizhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenmian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Weijia Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-dependent Contrastive Learning with Cluster Sampling for Inductive Relation Prediction. (arXiv:2211.12266v1 [cs.LG])","link":"http://arxiv.org/abs/2211.12266","description":"<p>Relation prediction is a task designed for knowledge graph completion which\naims to predict missing relationships between entities. Recent subgraph-based\nmodels for inductive relation prediction have received increasing attention,\nwhich can predict relation for unseen entities based on the extracted subgraph\nsurrounding the candidate triplet. However, they are not completely inductive\nbecause of their disability of predicting unseen relations. Moreover, they fail\nto pay sufficient attention to the role of relation as they only depend on the\nmodel to learn parameterized relation embedding, which leads to inaccurate\nprediction on long-tail relations. In this paper, we introduce\nRelation-dependent Contrastive Learning (ReCoLe) for inductive relation\nprediction, which adapts contrastive learning with a novel sampling method\nbased on clustering algorithm to enhance the role of relation and improve the\ngeneralization ability to unseen relations. Instead of directly learning\nembedding for relations, ReCoLe allocates a pre-trained GNN-based encoder to\neach relation to strengthen the influence of relation. The GNN-based encoder is\noptimized by contrastive learning, which ensures satisfactory performance on\nlong-tail relations. In addition, the cluster sampling method equips ReCoLe\nwith the ability to handle both unseen relations and entities. Experimental\nresults suggest that ReCoLe outperforms state-of-the-art methods on commonly\nused inductive datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1\">Sijie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v1 [cs.LG])","link":"http://arxiv.org/abs/2211.12316","description":"<p>Despite the widespread success of Transformers on NLP tasks, recent works\nhave found that they struggle to model several formal languages when compared\nto recurrent models. This raises the question of why Transformers perform well\nin practice and whether they have any properties that enable them to generalize\nbetter than recurrent models. In this work, we conduct an extensive empirical\nstudy on Boolean functions to demonstrate the following: (i) Random\nTransformers are relatively more biased towards functions of low sensitivity.\n(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize\nlearning functions of low sensitivity, with Transformers ultimately converging\nto functions of lower sensitivity. (iii) On sparse Boolean functions which have\nlow sensitivity, we find that Transformers generalize near perfectly even in\nthe presence of noisy labels whereas LSTMs overfit and achieve poor\ngeneralization accuracy. Overall, our results provide strong quantifiable\nevidence that suggests differences in the inductive biases of Transformers and\nrecurrent models which may help explain Transformer's effective generalization\nperformance despite relatively limited expressiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattamishra_S/0/1/0/all/0/1\">Satwik Bhattamishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Arkil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1\">Varun Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GDPR Compliant Collection of Therapist-Patient-Dialogues. (arXiv:2211.12360v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12360","description":"<p>According to the Global Burden of Disease list provided by the World Health\nOrganization (WHO), mental disorders are among the most debilitating\ndisorders.To improve the diagnosis and the therapy effectiveness in recent\nyears, researchers have tried to identify individual biomarkers. Gathering\nneurobiological data however, is costly and time-consuming. Another potential\nsource of information, which is already part of the clinical routine, are\ntherapist-patient dialogues. While there are some pioneering works\ninvestigating the role of language as predictors for various therapeutic\nparameters, for example patient-therapist alliance, there are no large-scale\nstudies. A major obstacle to conduct these studies is the availability of\nsizeable datasets, which are needed to train machine learning models. While\nthese conversations are part of the daily routine of clinicians, gathering them\nis usually hindered by various ethical (purpose of data usage), legal (data\nprivacy) and technical (data formatting) limitations. Some of these limitations\nare particular to the domain of therapy dialogues, like the increased\ndifficulty in anonymisation, or the transcription of the recordings. In this\npaper, we elaborate on the challenges we faced in starting our collection of\ntherapist-patient dialogues in a psychiatry clinic under the General Data\nPrivacy Regulation of the European Union with the goal to use the data for\nNatural Language Processing (NLP) research. We give an overview of each step in\nour procedure and point out the potential pitfalls to motivate further research\nin this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_T/0/1/0/all/0/1\">Tobias Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warikoo_N/0/1/0/all/0/1\">Neha Warikoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimm_O/0/1/0/all/0/1\">Oliver Grimm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_A/0/1/0/all/0/1\">Andreas Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Method for Determining the Similarity of Text Documents for the Kazakh language, Taking Into Account Synonyms: Extension to TF-IDF. (arXiv:2211.12364v1 [cs.IR])","link":"http://arxiv.org/abs/2211.12364","description":"<p>The task of determining the similarity of text documents has received\nconsiderable attention in many areas such as Information Retrieval, Text\nMining, Natural Language Processing (NLP) and Computational Linguistics.\nTransferring data to numeric vectors is a complex task where algorithms such as\ntokenization, stopword filtering, stemming, and weighting of terms are used.\nThe term frequency - inverse document frequency (TF-IDF) is the most widely\nused term weighting method to facilitate the search for relevant documents. To\nimprove the weighting of terms, a large number of TF-IDF extensions are made.\nIn this paper, another extension of the TF-IDF method is proposed where\nsynonyms are taken into account. The effectiveness of the method is confirmed\nby experiments on functions such as Cosine, Dice and Jaccard to measure the\nsimilarity of text documents for the Kazakh language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakiyev_B/0/1/0/all/0/1\">Bakhyt Bakiyev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Emotion-Aware Multi-Task Approach to Fake News and Rumour Detection using Transfer Learning. (arXiv:2211.12374v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12374","description":"<p>Social networking sites, blogs, and online articles are instant sources of\nnews for internet users globally. However, in the absence of strict regulations\nmandating the genuineness of every text on social media, it is probable that\nsome of these texts are fake news or rumours. Their deceptive nature and\nability to propagate instantly can have an adverse effect on society. This\nnecessitates the need for more effective detection of fake news and rumours on\nthe web. In this work, we annotate four fake news detection and rumour\ndetection datasets with their emotion class labels using transfer learning. We\nshow the correlation between the legitimacy of a text with its intrinsic\nemotion for fake news and rumour detection, and prove that even within the same\nemotion class, fake and real news are often represented differently, which can\nbe used for improved feature extraction. Based on this, we propose a multi-task\nframework for fake news and rumour detection, predicting both the emotion and\nlegitimacy of the text. We train a variety of deep learning models in\nsingle-task and multi-task settings for a more comprehensive comparison. We\nfurther analyze the performance of our multi-task approach for fake news\ndetection in cross-domain settings to verify its efficacy for better\ngeneralization across datasets, and to verify that emotions act as a\ndomain-independent feature. Experimental results verify that our multi-task\nmodels consistently outperform their single-task counterparts in terms of\naccuracy, precision, recall, and F1 score, both for in-domain and cross-domain\nsettings. We also qualitatively analyze the difference in performance in\nsingle-task and multi-task learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Minni Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12402","description":"<p>Vision language pre-training aims to learn alignments between vision and\nlanguage from a large amount of data. We proposed multi-grained vision language\npre-training, a unified approach which can learn vision language alignments in\nmultiple granularity. This paper advances the proposed method by unifying image\nand video encoding in one model and scaling up the model with large-scale data.\nWe present X$^2$-VLM, a pre-trained VLM with a modular architecture for both\nimage-text tasks and video-text tasks. Experiment results show that X$^2$-VLM\nperforms the best on base and large scale for both image-text and video-text\ntasks, making a good trade-off between performance and model scale. Moreover,\nwe show that the modular design of X$^2$-VLM results in high transferability\nfor X$^2$-VLM to be utilized in any language or domain. For example, by simply\nreplacing the text encoder with XLM-R, X$^2$-VLM outperforms state-of-the-art\nmultilingual multi-modal pre-trained models without any multilingual\npre-training. The code and pre-trained models will be available at\ngithub.com/zengyan-97/X2-VLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Narrative Information and the Distillation of Stories. (arXiv:2211.12423v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12423","description":"<p>The act of telling stories is a fundamental part of what it means to be\nhuman. This work introduces the concept of narrative information, which we\ndefine to be the overlap in information space between a story and the items\nthat compose the story. Using contrastive learning methods, we show how modern\nartificial neural networks can be leveraged to distill stories and extract a\nrepresentation of the narrative information. We then demonstrate how\nevolutionary algorithms can leverage this to extract a set of narrative\ntemplates and how these templates -- in tandem with a novel curve-fitting\nalgorithm we introduce -- can reorder music albums to automatically induce\nstories in them. In the process of doing so, we give strong statistical\nevidence that these narrative information templates are present in existing\nalbums. While we experiment only with music albums here, the premises of our\nwork extend to any form of (largely) independent media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrmann_V/0/1/0/all/0/1\">Vincent Herrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friggstad_Z/0/1/0/all/0/1\">Zachary Friggstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTuning: Toward Adapting Large Language Models without Back-propagation. (arXiv:2211.12485v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12485","description":"<p>Fine-tuning large language models for different tasks can be costly and\ninefficient, and even methods that reduce the number of tuned parameters still\nrequire full gradient-based optimization. We propose HyperTuning, a novel\napproach to model adaptation that uses a hypermodel to generate task-specific\nparameters for a fixed downstream model. We demonstrate a simple setup for\nhypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or\nLoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5\nin two stages: first, hyperpretraining with a modified conditional language\nmodeling objective that trains a hypermodel to generate parameters; second,\nmulti-task fine-tuning (MTF) on a large number of diverse language tasks. We\nevaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and\nshow that it can effectively generate parameters for unseen tasks. Moreover, we\nshow that using hypermodel-generated parameters as initializations for further\nparameter-efficient fine-tuning improves performance. HyperTuning can thus be a\nflexible and efficient way to leverage large language models for diverse\ndownstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified MRC Framework for Named Entity Recognition. (arXiv:1910.11476v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1910.11476","description":"<p>The task of named entity recognition (NER) is normally divided into nested\nNER and flat NER depending on whether named entities are nested or not. Models\nare usually separately developed for the two tasks, since sequence labeling\nmodels, the most widely used backbone for flat NER, are only able to assign a\nsingle label to a particular token, which is unsuitable for nested NER where a\ntoken may be assigned several labels.\n</p>\n<p>In this paper, we propose a unified framework that is capable of handling\nboth flat and nested NER tasks. Instead of treating the task of NER as a\nsequence labeling problem, we propose to formulate it as a machine reading\ncomprehension (MRC) task. For example, extracting entities with the\n\\textsc{per} label is formalized as extracting answer spans to the question\n\"{\\it which person is mentioned in the text?}\". This formulation naturally\ntackles the entity overlapping issue in nested NER: the extraction of two\noverlapping entities for different categories requires answering two\nindependent questions. Additionally, since the query encodes informative prior\nknowledge, this strategy facilitates the process of entity extraction, leading\nto better performances for not only nested NER, but flat NER.\n</p>\n<p>We conduct experiments on both {\\em nested} and {\\em flat} NER datasets.\nExperimental results demonstrate the effectiveness of the proposed formulation.\nWe are able to achieve vast amount of performance boost over current SOTA\nmodels on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively\non ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets,\ni.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English\nOntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingrong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qinghong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Nearest Neighbor Machine Translation. (arXiv:2105.14528v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14528","description":"<p>Though nearest neighbor Machine Translation ($k$NN-MT)\n\\citep{khandelwal2020nearest} has proved to introduce significant performance\nboosts over standard neural MT systems, it is prohibitively slow since it uses\nthe entire reference corpus as the datastore for the nearest neighbor search.\nThis means each step for each beam in the beam search has to search over the\nentire reference corpus. $k$NN-MT is thus two-orders slower than vanilla MT\nmodels, making it hard to be applied to real-world applications, especially\nonline services. In this work, we propose Fast $k$NN-MT to address this issue.\nFast $k$NN-MT constructs a significantly smaller datastore for the nearest\nneighbor search: for each word in a source sentence, Fast $k$NN-MT first\nselects its nearest token-level neighbors, which is limited to tokens that are\nthe same as the query token. Then at each decoding step, in contrast to using\nthe entire corpus as the datastore, the search space is limited to target\ntokens corresponding to the previously selected reference source tokens. This\nstrategy avoids search through the whole datastore for nearest neighbors and\ndrastically improves decoding efficiency. Without loss of performance, Fast\n$k$NN-MT is two-orders faster than $k$NN-MT, and is only two times slower than\nthe standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT\nsystems in real-world MT applications. The code is available at\n\\url{https://github.com/ShannonAI/fast-knn-nmt}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiayu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Evaluation of Cross-document Coreference Resolution Models Using Datasets with Diverse Annotation Schemes. (arXiv:2109.05250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05250","description":"<p>Established cross-document coreference resolution (CDCR) datasets contain\nevent-centric coreference chains of events and entities with identity\nrelations. These datasets establish strict definitions of the coreference\nrelations across related tests but typically ignore anaphora with more vague\ncontext-dependent loose coreference relations. In this paper, we qualitatively\nand quantitatively compare the annotation schemes of ECB+, a CDCR dataset with\nidentity coreference relations, and NewsWCL50, a CDCR dataset with a mix of\nloose context-dependent and strict coreference relations. We propose a phrasing\ndiversity metric (PD) that encounters for the diversity of full phrases unlike\nthe previously proposed metrics and allows to evaluate lexical diversity of the\nCDCR datasets in a higher precision. The analysis shows that coreference chains\nof NewsWCL50 are more lexically diverse than those of ECB+ but annotating of\nNewsWCL50 leads to the lower inter-coder reliability. We discuss the different\ntasks that both CDCR datasets create for the CDCR models, i.e., lexical\ndisambiguation and lexical diversity. Finally, to ensure generalizability of\nthe CDCR models, we propose a direction for CDCR evaluation that combines CDCR\ndatasets with multiple annotation schemes that focus of various properties of\nthe coreference chains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Intrinsic Exploration with Language Abstractions. (arXiv:2202.08938v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.08938","description":"<p>Reinforcement learning (RL) agents are particularly hard to train when\nrewards are sparse. One common solution is to use intrinsic rewards to\nencourage agents to explore their environment. However, recent intrinsic\nexploration methods often use state-based novelty measures which reward\nlow-level exploration and may not scale to domains requiring more abstract\nskills. Instead, we explore natural language as a general medium for\nhighlighting relevant abstractions in an environment. Unlike previous work, we\nevaluate whether language can improve over existing exploration methods by\ndirectly extending (and comparing to) competitive intrinsic exploration\nbaselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These\nlanguage-based variants outperform their non-linguistic forms by 47-85% across\n13 challenging tasks from the MiniGrid and MiniHack environment suites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1\">Edward Grefenstette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models. (arXiv:2205.11169v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11169","description":"<p>Vision-language pre-training (VLP) has shown impressive performance on a wide\nrange of cross-modal tasks, where VLP models without reliance on object\ndetectors are becoming the mainstream due to their superior computation\nefficiency and competitive performance. However, the removal of object\ndetectors also deprives the capability of VLP models in explicit object\nmodeling, which is essential to various position-sensitive vision-language (VL)\ntasks, such as referring expression comprehension and visual commonsense\nreasoning. To address the challenge, we introduce PEVL that enhances the\npre-training and prompt tuning of VLP models with explicit object position\nmodeling. Specifically, PEVL reformulates discretized object positions and\nlanguage in a unified language modeling framework, which facilitates explicit\nVL alignment during pre-training, and also enables flexible prompt tuning for\nvarious downstream tasks. We show that PEVL enables state-of-the-art\nperformance of detector-free VLP models on position-sensitive tasks such as\nreferring expression comprehension and phrase grounding, and also improves the\nperformance on position-insensitive tasks with grounded inputs. We make the\ndata and code for this paper publicly available at\nhttps://github.com/thunlp/PEVL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts. (arXiv:2205.12701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12701","description":"<p>Recent works suggest that transformer models are capable of multi-tasking on\ndiverse NLP tasks and adapting to new tasks efficiently. However, the potential\nof these multi-task models may be limited as they use the same set of\nparameters for all tasks. In contrast, humans tackle tasks in a more flexible\nway, by making proper presumptions on what skills and knowledge are relevant\nand executing only the necessary computations. Inspired by this, we propose to\nuse task-level mixture-of-expert models, which has a collection of transformer\nlayers (i.e., experts) and a router component that chooses from these experts\ndynamically and flexibly. We find that these models help improve the average\nperformance gain (ARG) metric by 2.6% when adapting to unseen tasks in the\nfew-shot setting and by 5.6% in the zero-shot generalization setting. Further,\nwe show that the learned routing decisions partly rediscover human\ncategorization of NLP tasks -- certain experts are strongly associated with\nextractive tasks, some with classification tasks, and some with tasks requiring\nworld knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Juan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08853","description":"<p>Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuncong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haoyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Andrew Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabelBERT: Massively Multilingual Transformers Meet a Massively Multilingual Lexical Resource. (arXiv:2208.01018v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01018","description":"<p>While pretrained language models (PLMs) primarily serve as general purpose\ntext encoders that can be fine-tuned for a wide variety of downstream tasks,\nrecent work has shown that they can also be rewired to produce high-quality\nword representations (i.e., static word embeddings) and yield good performance\nin type-level lexical tasks. While existing work primarily focused on lexical\nspecialization of PLMs in monolingual and bilingual settings, in this work we\nexpose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to\nmultilingual lexical knowledge at scale, leveraging BabelNet as the readily\navailable rich source of multilingual and cross-lingual type-level lexical\nknowledge. Concretely, we leverage BabelNet's multilingual synsets to create\nsynonym pairs across $50$ languages and then subject the MMTs (mBERT and XLM-R)\nto a lexical specialization procedure guided by a contrastive objective. We\nshow that such massively multilingual lexical specialization brings massive\ngains in two standard cross-lingual lexical tasks, bilingual lexicon induction\nand cross-lingual word similarity, as well as in cross-lingual sentence\nretrieval. Crucially, we observe gains for languages unseen in specialization,\nindicating that the multilingual lexical specialization enables generalization\nto languages with no lexical constraints. In a series of subsequent controlled\nexperiments, we demonstrate that the pretraining quality of word\nrepresentations in the MMT for languages involved in specialization has a much\nlarger effect on performance than the linguistic diversity of the set of\nconstraints. Encouragingly, this suggests that lexical tasks involving\nlow-resource languages benefit the most from lexical knowledge of resource-rich\nlanguages, generally much more available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Green_T/0/1/0/all/0/1\">Tommaso Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention. (arXiv:2209.03126v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2209.03126","description":"<p>There is increasing interest in the use of multimodal data in various web\napplications, such as digital advertising and e-commerce. Typical methods for\nextracting important information from multimodal data rely on a mid-fusion\narchitecture that combines the feature representations from multiple encoders.\nHowever, as the number of modalities increases, several potential problems with\nthe mid-fusion model structure arise, such as an increase in the dimensionality\nof the concatenated multimodal features and missing modalities. To address\nthese problems, we propose a new concept that considers multimodal inputs as a\nset of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our\nset-aware concept consists of three components that capture the relationships\namong multiple modalities: (a) a BERT-based encoder to handle the inter- and\nintra-order of elements in the sequences, (b) intra-modality residual attention\n(IntraMRA) to capture the importance of the elements in a modality, and (c)\ninter-modality residual attention (InterMRA) to enhance the importance of\nelements with modality-level granularity further. Our concept exhibits\nperformance that is comparable to or better than the previous set-aware models.\nFurthermore, we demonstrate that the visualization of the learned InterMRA and\nIntraMRA weights can provide an interpretation of the prediction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwazaki_Y/0/1/0/all/0/1\">Yuki Iwazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togashi_R/0/1/0/all/0/1\">Riku Togashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvNext Based Neural Network for Audio Anti-Spoofing. (arXiv:2209.06434v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2209.06434","description":"<p>Automatic speaker verification (ASV) has been widely used in the real life\nfor identity authentication. However, with the rapid development of speech\nconversion, speech synthesis algorithms, ASV systems are vulnerable for spoof\nattacks. In recent years, there have many works about synthetic speech\ndetection, researchers had proposed a number of anti-spoofing methods based on\nhand-crafted features to improve the detection accuracy and robustness of ASV\nsystems. However, using hand-crafted features rather than raw waveform would\nlose certain information for anti-spoofing, which will reduce the detection\nperformance of the system. Inspired by the promising performance of ConvNeXt in\nimage classification tasks, we revise the ConvNeXt network architecture\naccordingly for spoof attacks detection task and propose a light weight\nend-to-end anti-spoofing model. By integrating the revised architecture with\nthe channel attention block and using the focal loss function, the proposed\nmodel can focus on the most informative sub-bands of speech representations to\nimprove the anti-spoofing performance and the difficult samples that are hard\nfor models to classify. Experiments show that our proposed best single system\ncould achieve an equal error rate of 0.75% and min-tDCF of 0.0212 for the\nASVSpoof2019 LA evaluation dataset, which outperform the state-of-the-art\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiaowei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinghui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wing W.Y. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selection Induced Collider Bias: A Gender Pronoun Uncertainty Case Study. (arXiv:2210.00131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00131","description":"<p>In this paper, we cast the problem of task underspecification in causal\nterms, and develop a method for empirical measurement of spurious associations\nbetween gender and gender-neutral entities for unmodified large language\nmodels, detecting previously unreported spurious correlations. We then describe\na lightweight method to exploit the resulting spurious associations for\nprediction task uncertainty classification, achieving over 90% accuracy on a\nWinogender Schemas challenge set. Finally, we generalize our approach to\naddress a wider range of prediction tasks and provide open-source demos for\neach method described here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McMilin_E/0/1/0/all/0/1\">Emily McMilin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13952","description":"<p>We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornec_O/0/1/0/all/0/1\">Owen Cornec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Massimiliano Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues. (arXiv:2211.11049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11049","description":"<p>Conversations emerge as the primary media for exchanging ideas and\nconceptions. From the listener's perspective, identifying various affective\nqualities, such as sarcasm, humour, and emotions, is paramount for\ncomprehending the true connotation of the emitted utterance. However, one of\nthe major hurdles faced in learning these affect dimensions is the presence of\nfigurative language, viz. irony, metaphor, or sarcasm. We hypothesize that any\ndetection system constituting the exhaustive and explicit presentation of the\nemitted utterance would improve the overall comprehension of the dialogue. To\nthis end, we explore the task of Sarcasm Explanation in Dialogues, which aims\nto unfold the hidden irony behind sarcastic utterances. We propose MOSES, a\ndeep neural network, which takes a multimodal (sarcastic) dialogue instance as\nan input and generates a natural language sentence as its explanation.\nSubsequently, we leverage the generated explanation for various natural\nlanguage understanding tasks in a conversational dialogue setup, such as\nsarcasm detection, humour identification, and emotion recognition. Our\nevaluation shows that MOSES outperforms the state-of-the-art system for SED by\nan average of ~2% on different evaluation metrics, such as ROUGE, BLEU, and\nMETEOR. Further, we observe that leveraging the generated explanation advances\nthree downstream tasks for affect classification - an average improvement of\n~14% F1-score in the sarcasm detection task and ~2% in the humour\nidentification and emotion recognition task. We also perform extensive analyses\nto assess the quality of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi. (arXiv:2211.11187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11187","description":"<p>Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ananya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajale_A/0/1/0/all/0/1\">Aditi Kajale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_J/0/1/0/all/0/1\">Janhavi Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deode_S/0/1/0/all/0/1\">Samruddhi Deode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.11419","description":"<p>This paper presents an in-depth study on a Sequentially Sampled Chunk\nConformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer\nfirst demonstrates the significant performance gains from using the\nsequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the\nConformer encoder by allowing efficient cross-chunk interactions while keeping\nlinear complexities. Furthermore, it explores taking advantage of chunked\nconvolution to make use of the chunk-wise future context and integrates with\ncasual convolution in the convolution layers to further reduce CER. We verify\nthe proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results\nshow that a state-of-the-art performance for streaming E2E ASR is achieved with\nCER 5.33% without LM rescoring. And, owing to its linear complexity, the\nSSC-Conformer can train with large batch sizes and infer more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training. (arXiv:2211.11446v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11446","description":"<p>Video-language pre-training is crucial for learning powerful multi-modal\nrepresentation. However, it typically requires a massive amount of computation.\nIn this paper, we develop SMAUG, an efficient pre-training framework for\nvideo-language models. The foundation component in SMAUG is masked\nautoencoders. Different from prior works which only mask textual inputs, our\nmasking strategy considers both visual and textual modalities, providing a\nbetter cross-modal alignment and saving more pre-training costs. On top of\nthat, we introduce a space-time token sparsification module, which leverages\ncontext information to further select only \"important\" spatial regions and\ntemporal frames for pre-training. Coupling all these designs allows our method\nto enjoy both competitive performances on text-to-video retrieval and video\nquestion answering tasks, and much less pre-training costs by 1.9X or more. For\nexample, our SMAUG only needs about 50 NVIDIA A6000 GPU hours for pre-training\nto attain competitive performances on these two video-language tasks across six\npopular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Vision-Language Prompt Tuning. (arXiv:2211.11720v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11720","description":"<p>Prompt Tuning, conditioning on task-specific learned prompt vectors, has\nemerged as a data-efficient and parameter-efficient method for adapting large\npretrained vision-language models to multiple downstream tasks. However,\nexisting approaches usually consider learning prompt vectors for each task\nindependently from scratch, thereby failing to exploit the rich shareable\nknowledge across different vision-language tasks. In this paper, we propose\nmultitask vision-language prompt tuning (MVLPT), which incorporates cross-task\nknowledge into prompt tuning for vision-language models. Specifically, (i) we\ndemonstrate the effectiveness of learning a single transferable prompt from\nmultiple source tasks to initialize the prompt for each target task; (ii) we\nshow many target tasks can benefit each other from sharing prompt vectors and\nthus can be jointly learned via multitask prompt tuning. We benchmark the\nproposed MVLPT using three representative prompt tuning methods, namely text\nprompt tuning, visual prompt tuning, and the unified vision-language prompt\ntuning. Results in 20 vision tasks demonstrate that the proposed approach\noutperforms all single-task baseline prompt tuning methods, setting the new\nstate-of-the-art on the few-shot ELEVATER benchmarks and cross-task\ngeneralization benchmarks. To understand where the cross-task knowledge is most\neffective, we also conduct a large-scale study on task transferability with 20\nvision tasks in 400 combinations for each prompt tuning method. It shows that\nthe most performant MVLPT for each prompt tuning method prefers different task\ncombinations and many tasks can benefit each other, depending on their visual\nsimilarity and label similarity. Code is available at\nhttps://github.com/sIncerass/MVLPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convexifying Transformers: Improving optimization and understanding of transformer networks. (arXiv:2211.11052v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2211.11052","description":"<p>Understanding the fundamental mechanism behind the success of transformer\nnetworks is still an open problem in the deep learning literature. Although\ntheir remarkable performance has been mostly attributed to the self-attention\nmechanism, the literature still lacks a solid analysis of these networks and\ninterpretation of the functions learned by them. To this end, we study the\ntraining problem of attention/transformer networks and introduce a novel convex\nanalytic approach to improve the understanding and optimization of these\nnetworks. Particularly, we first introduce a convex alternative to the\nself-attention mechanism and reformulate the regularized training problem of\ntransformer networks with our alternative convex attention. Then, we cast the\nreformulation as a convex optimization problem that is interpretable and easier\nto optimize. Moreover, as a byproduct of our convex analysis, we reveal an\nimplicit regularization mechanism, which promotes sparsity across tokens.\nTherefore, we not only improve the optimization of attention/transformer\nnetworks but also provide a solid theoretical understanding of the functions\nlearned by them. We also demonstrate the effectiveness of our theory through\nseveral numerical experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}