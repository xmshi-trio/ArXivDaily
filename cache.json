{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Semantic-aware Digital Twin for Metaverse: A Comprehensive Review. (arXiv:2305.18304v1 [cs.CY])","link":"http://arxiv.org/abs/2305.18304","description":"<p>To facilitate the deployment of digital twins in Metaverse, the paradigm with\nsemantic awareness has been proposed as a means for enabling accurate and\ntask-oriented information extraction with inherent intelligence. However, this\nframework requires all devices in the Metaverse environment to be directly\nlinked with the semantic model to enable faithful interpretation of messages.\nIn contrast, this article introduces the digital twin framework, considering a\nsmart industrial application, which enables semantic communication in\nconjugation with the Metaverse enabling technologies. The fundamentals of this\nframework are demonstrated on an industrial shopfloor management use case with\na digital twin so as to improve its performance through semantic communication.\nAn overview of semantic communication, Metaverse, and digital twins is\npresented. Integration of these technologies with the basic architecture as\nwell as the impact on future industrial applications is presented. In a\nnutshell, this article showcases how semantic awareness can be an effective\ncandidate in the implementation of digital twins for Metaverse applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jagatheesaperumal_S/0/1/0/all/0/1\">Senthil Kumar Jagatheesaperumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chongwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shikh_Bahaei_M/0/1/0/all/0/1\">Mohammad Shikh-Bahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18315","description":"<p>A basic task for most Legal Artificial Intelligence (Legal AI) applications\nis Named Entity Recognition (NER). However, texts produced in the context of\nlegal practice make references to entities that are not trivially recognized by\nthe currently available NERs. There is a lack of categorization of legislation,\njurisprudence, evidence, penalties, the roles of people in a legal process\n(judge, lawyer, victim, defendant, witness), types of locations (crime\nlocation, defendant's address), etc. In this sense, there is still a need for a\nrobust golden collection, annotated with fine-grained entities of the legal\ndomain, and which covers various documents of a legal process, such as\npetitions, inquiries, complaints, decisions and sentences. In this article, we\ndescribe the development of the Golden Collection of the Brazilian Judiciary\n(CDJUR-BR) contemplating a set of fine-grained named entities that have been\nannotated by experts in legal documents. The creation of CDJUR-BR followed its\nown methodology that aimed to attribute a character of comprehensiveness and\nrobustness. Together with the CDJUR-BR repository we provided a NER based on\nthe BERT model and trained with the CDJUR-BR, whose results indicated the\nprevalence of the CDJUR-BR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mauricio_A/0/1/0/all/0/1\">Antonio Mauricio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_V/0/1/0/all/0/1\">Vladia Pinheiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furtado_V/0/1/0/all/0/1\">Vasco Furtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1\">Jo&#xe3;o Ara&#xfa;jo Monteiro Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bomfim_F/0/1/0/all/0/1\">Francisco das Chagas Juc&#xe1; Bomfim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Andr&#xe9; C&#xe2;mara Ferreira da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_R/0/1/0/all/0/1\">Raquel Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragao_N/0/1/0/all/0/1\">Nilsiton Arag&#xe3;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18319","description":"<p>Timely feedback is an important part of teaching and learning. Here we\ndescribe how a readily available neural network transformer (machine-learning)\nmodel (BERT) can be used to give feedback on the structure of the response to\nan abstracting exercise where students are asked to summarise the contents of a\npublished article after finding it from a publication database. The dataset\ncontained 207 submissions from two consecutive years of the course, summarising\na total of 21 different papers from the primary literature. The model was\npre-trained using an available dataset (approx. 15,000 samples) and then\nfine-tuned on 80% of the submitted dataset. This fine tuning was seen to be\nimportant. The sentences in the student submissions are characterised into\nthree classes - background, technique and observation - which allows a\ncomparison of how each submission is structured. Comparing the structure of the\nstudents' abstract a large collection of those from the PubMed database shows\nthat students in this exercise concentrate more on the background to the paper\nand less on the techniques and results than the abstracts to papers themselves.\nThe results allowed feedback for each submitted assignment to be automatically\ngenerated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morris_O/0/1/0/all/0/1\">Oscar Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_R/0/1/0/all/0/1\">Russell Morris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students. (arXiv:2305.18320v1 [cs.CY])","link":"http://arxiv.org/abs/2305.18320","description":"<p>Large language models are becoming increasingly integrated into our lives.\nHence, it is important to understand the biases present in their outputs in\norder to avoid perpetuating harmful stereotypes, which originate in our own\nflawed ways of thinking. This challenge requires developing new benchmarks and\nmethods for quantifying affective and semantic bias, keeping in mind that LLMs\nact as psycho-social mirrors that reflect the views and tendencies that are\nprevalent in society. One such tendency that has harmful negative effects is\nthe global phenomenon of anxiety toward math and STEM subjects. Here, we\ninvestigate perceptions of math and STEM fields provided by cutting-edge\nlanguage models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach\nfrom network science and cognitive psychology. Specifically, we use behavioral\nforma mentis networks (BFMNs) to understand how these LLMs frame math and STEM\ndisciplines in relation to other concepts. We use data obtained by probing the\nthree LLMs in a language generation task that has previously been applied to\nhumans. Our findings indicate that LLMs have an overall negative perception of\nmath and STEM fields, with math being perceived most negatively. We observe\nsignificant differences across the three LLMs. We observe that newer versions\n(i.e. GPT-4) produce richer, more complex perceptions as well as less negative\nperceptions compared to older versions and N=159 high-school students. These\nfindings suggest that advances in the architecture of LLMs may lead to\nincreasingly less biased models that could even perhaps someday aid in reducing\nharmful stereotypes in society rather than perpetuating them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abramski_K/0/1/0/all/0/1\">Katherine Abramski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_S/0/1/0/all/0/1\">Salvatore Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_L/0/1/0/all/0/1\">Luigi Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetti_G/0/1/0/all/0/1\">Giulio Rossetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18322","description":"<p>A number of datasets for Relation Extraction (RE) have been created to aide\ndownstream tasks such as information retrieval, semantic search, question\nanswering and textual entailment. However, these datasets fail to capture\nfinancial-domain specific challenges since most of these datasets are compiled\nusing general knowledge sources such as Wikipedia, web-based text and news\narticles, hindering real-life progress and adoption within the financial world.\nTo address this limitation, we propose REFinD, the first large-scale annotated\ndataset of relations, with $\\sim$29K instances and 22 relations amongst 8 types\nof entity pairs, generated entirely over financial documents. We also provide\nan empirical evaluation with various state-of-the-art models as benchmarks for\nthe RE task and highlight the challenges posed by our dataset. We observed that\nvarious state-of-the-art deep learning models struggle with numeric inference,\nrelational and directional ambiguity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1\">Simerjot Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_J/0/1/0/all/0/1\">Joy Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddagangappa_S/0/1/0/all/0/1\">Suchetha Siddagangappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguda_T/0/1/0/all/0/1\">Toyin Aguda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18323","description":"<p>Augmented Language Models (ALMs) blend the reasoning capabilities of Large\nLanguage Models (LLMs) with tools that allow for knowledge retrieval and action\nexecution. Existing ALM systems trigger LLM thought processes while pulling\nobservations from these tools in an interleaved fashion. Specifically, an LLM\nreasons to call an external tool, gets halted to fetch the tool's response, and\nthen decides the next action based on all preceding response tokens. Such a\nparadigm, though straightforward and easy to implement, often leads to huge\ncomputation complexity from redundant prompts and repeated execution. This\nstudy addresses such challenges for the first time, proposing a modular\nparadigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning\nprocess from external observations, thus significantly reducing token\nconsumption. Comprehensive evaluations across six public NLP benchmarks and a\ncurated dataset reveal consistent performance enhancements with our proposed\nmethodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy\nimprovement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO\ndemonstrates robustness under tool-failure scenarios. Beyond prompt efficiency,\ndecoupling parametric modules from non-parametric tool calls enables\ninstruction fine-tuning to offload LLMs into smaller language models, thus\nsubstantially reducing model parameters. Our illustrative work offloads\nreasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant\npotential for truly efficient and scalable ALM systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Bowen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18324","description":"<p>A common way to use large pre-trained language models for downstream tasks is\nto fine tune them using additional layers. This may not work well if downstream\ndomain is a specialized domain whereas the large language model has been\npre-trained on a generic corpus. In this paper, we discuss the use of regular\nexpression patterns employed as features for domain knowledge during the\nprocess of fine tuning, in addition to domain specific text. Our experiments on\nreal scenario production data show that this method of fine tuning improves the\ndownstream text classification tasks as compared to fine tuning only on domain\nspecific text. We also show that the use of attention network for fine tuning\nimproves results compared to simple linear layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_V/0/1/0/all/0/1\">Vanessa Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murtaza_S/0/1/0/all/0/1\">Syed Shariyar Murtaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yifan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])","link":"http://arxiv.org/abs/2305.18330","description":"<p>Automatic evaluation of hashtag recommendation models is a fundamental task\nin many online social network systems. In the traditional evaluation method,\nthe recommended hashtags from an algorithm are firstly compared with the ground\ntruth hashtags for exact correspondences. The number of exact matches is then\nused to calculate the hit rate, hit ratio, precision, recall, or F1-score. This\nway of evaluating hashtag similarities is inadequate as it ignores the semantic\ncorrelation between the recommended and ground truth hashtags. To tackle this\nproblem, we propose a novel semantic evaluation framework for hashtag\nrecommendation, called #REval. This framework includes an internal module\nreferred to as BERTag, which automatically learns the hashtag embeddings. We\ninvestigate on how the #REval framework performs under different word embedding\nmethods and different numbers of synonyms and hashtags in the recommendation\nusing our proposed #REval-hit-ratio measure. Our experiments of the proposed\nframework on three large datasets show that #REval gave more meaningful hashtag\nsynonyms for hashtag recommendation evaluation. Our analysis also highlights\nthe sensitivity of the framework to the word embedding technique, with #REval\nbased on BERTag more superior over #REval based on FastText and Word2Vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsini_A/0/1/0/all/0/1\">Areej Alsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Du Q. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Amitava Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])","link":"http://arxiv.org/abs/2305.18339","description":"<p>With the widespread use of large artificial intelligence (AI) models such as\nChatGPT, AI-generated content (AIGC) has garnered increasing attention and is\nleading a paradigm shift in content creation and knowledge representation. AIGC\nuses generative large AI algorithms to assist or replace humans in creating\nmassive, high-quality, and human-like content at a faster pace and lower cost,\nbased on user-provided prompts. Despite the recent significant progress in\nAIGC, security, privacy, ethical, and legal challenges still need to be\naddressed. This paper presents an in-depth survey of working principles,\nsecurity and privacy threats, state-of-the-art solutions, and future challenges\nof the AIGC paradigm. Specifically, we first explore the enabling technologies,\ngeneral architecture of AIGC, and discuss its working modes and key\ncharacteristics. Then, we investigate the taxonomy of security and privacy\nthreats to AIGC and highlight the ethical and societal implications of GPT and\nAIGC technologies. Furthermore, we review the state-of-the-art AIGC\nwatermarking approaches for regulatable AIGC paradigms regarding the AIGC model\nand its produced content. Finally, we identify future challenges and open\nresearch directions related to AIGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuntao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yanghe Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Miao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhou Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1\">Tom H. Luan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis. (arXiv:2305.18340v1 [cs.CY])","link":"http://arxiv.org/abs/2305.18340","description":"<p>The exponential growth in user acquisition and popularity of ChatGPT, an\nartificial intelligence(AI) powered chatbot, was accompanied by widespread\nmainstream media coverage. This article presents a quantitative data analysis\nof the early trends and sentiments revealed by conducting text mining and NLP\nmethods onto a corpus of 10,902 mainstream news headlines related to the\nsubject of ChatGPT and artificial intelligence, from the launch of ChatGPT in\nNovember 2022 to March 2023. The findings revealed in sentiment analysis,\nChatGPT and artificial intelligence, were perceived more positively than\nnegatively in the mainstream media. In regards to word frequency results, over\nsixty-five percent of the top frequency words were focused on Big Tech issues\nand actors while topics such as jobs, diversity, ethics, copyright, gender and\nwomen were poorly represented or completely absent and only accounted for six\npercent of the total corpus. This article is a critical analysis into the power\nstructures and collusions between Big Tech and Big Media in their matrix of\ndomination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karanouh_M/0/1/0/all/0/1\">Maya Karanouh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18342","description":"<p>Generative neural models hold great promise in enhancing programming\neducation by synthesizing new content for students. We seek to design neural\nmodels that can automatically generate programming tasks for a given\nspecification in the context of visual programming domains. Despite the recent\nsuccesses of large generative models like GPT-4, our initial results show that\nthese models are ineffective in synthesizing visual programming tasks and\nstruggle with logical and spatial reasoning. We propose a novel neuro-symbolic\ntechnique, NeurTaskSyn, that can synthesize programming tasks for a\nspecification given in the form of desired programming concepts exercised by\nits solution code and constraints on the visual task. NeurTaskSyn has two\ncomponents: the first component is trained via imitation learning procedure to\ngenerate possible solution codes, and the second component is trained via\nreinforcement learning procedure to guide an underlying symbolic execution\nengine that generates visual tasks for these codes. We demonstrate the\neffectiveness of NeurTaskSyn through an extensive empirical evaluation and a\nqualitative study on reference tasks taken from the Hour of Code: Classic Maze\nchallenge by Code-dot-org and the Intro to Programming with Karel course by\nCodeHS-dot-com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1\">Victor-Alexandru P&#x103;durean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzannetos_G/0/1/0/all/0/1\">Georgios Tzannetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18350","description":"<p>We present a new task setting for attribute mining on e-commerce products,\nserving as a practical solution to extract open-world attributes without\nextensive human intervention. Our supervision comes from a high-quality seed\nattribute set bootstrapped from existing resources, and we aim to expand the\nattribute vocabulary of existing seed types, and also to discover any new\nattribute types automatically. A new dataset is created to support our setting,\nand our approach Amacer is proposed specifically to tackle the limited\nsupervision. Especially, given that no direct supervision is available for\nthose unseen new attributes, our novel formulation exploits self-supervised\nheuristic and unsupervised latent attributes, which attains implicit semantic\nsignals as additional supervision by leveraging product context. Experiments\nsuggest that our approach surpasses various baselines by 12 F1, expanding\nattributes of existing types significantly by up to 12 times, and discovering\nvalues from 39% new types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18354","description":"<p>Can a Large Language Model (LLM) solve simple abstract reasoning problems? We\nexplore this broad question through a systematic analysis of GPT on the\nAbstraction and Reasoning Corpus (ARC), a representative benchmark of abstract\nreasoning ability from limited examples in which solutions require some \"core\nknowledge\" of concepts such as objects, goal states, counting, and basic\ngeometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when\nusing textual encodings for their two-dimensional input-output grids. Our\nfailure analysis reveals that GPT-4's capacity to identify objects and reason\nabout them is significantly influenced by the sequential nature of the text\nthat represents an object within a text encoding of a task. To test this\nhypothesis, we design a new benchmark, the 1D-ARC, which consists of\none-dimensional (array-like) tasks that are more conducive to GPT-based\nreasoning, and where it indeed performs better than on the (2D) ARC. To\nalleviate this issue, we propose an object-based representation that is\nobtained through an external tool, resulting in nearly doubling the performance\non solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the\nstate-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language\ndomains such as the 1D-ARC or a simple ARC subset, our study reveals that the\nuse of object-based representations can significantly improve its reasoning\nability. Visualizations, GPT logs, and data are available at\nhttps://khalil-research.github.io/LLM4ARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yudong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1\">Pashootan Vaezipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_E/0/1/0/all/0/1\">Elias B. Khalil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18357","description":"<p>In this paper, we design novel interactive deep learning methods to improve\nsemantic interactions in visual analytics applications. The ability of semantic\ninteraction to infer analysts' precise intents during sensemaking is dependent\non the quality of the underlying data representation. We propose the\n$\\text{DeepSI}_{\\text{finetune}}$ framework that integrates deep learning into\nthe human-in-the-loop interactive sensemaking pipeline, with two important\nproperties. First, deep learning extracts meaningful representations from raw\ndata, which improves semantic interaction inference. Second, semantic\ninteractions are exploited to fine-tune the deep learning representations,\nwhich then further improves semantic interaction inference. This feedback loop\nbetween human interaction and deep learning enables efficient learning of user-\nand task-specific representations. To evaluate the advantage of embedding the\ndeep learning within the semantic interaction loop, we compare\n$\\text{DeepSI}_{\\text{finetune}}$ against a state-of-the-art but more basic use\nof deep learning as only a feature extractor pre-processed outside of the\ninteractive loop. Results of two complementary studies, a human-centered\nqualitative case study and an algorithm-centered simulation-based quantitative\nexperiment, show that $\\text{DeepSI}_{\\text{finetune}}$ more accurately\ncaptures users' complex mental models with fewer interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yali Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+North_C/0/1/0/all/0/1\">Chris North</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18365","description":"<p>Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been rapidly applied in various kinds of\nareas such as science, finance and software engineering. However, the\ncapability of LLMs to advance the field of chemistry remains unclear. In this\npaper,we establish a comprehensive benchmark containing 8 practical chemistry\ntasks, including 1) name prediction, 2) property prediction, 3) yield\nprediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants\nfrom products), 6)text-based molecule design, 7) molecule captioning, and 8)\nreagent selection. Our analysis draws on widely recognized datasets including\nBBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the\ncapacities of LLMs within the context of practical chemistry. Three GPT models\n(GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in\nzero-shot and few-shot in-context learning settings with carefully selected\ndemonstration examples and specially crafted prompts. The key results of our\ninvestigation are 1) GPT-4 outperforms the other two models among the three\nevaluated; 2) GPT models exhibit less competitive performance in tasks\ndemanding precise understanding of molecular SMILES representation, such as\nreaction prediction and retrosynthesis;3) GPT models demonstrate strong\ncapabilities in text-related explanation tasks such as molecule captioning; and\n4) GPT models exhibit comparable or better performance to classical machine\nlearning models when applied to chemical problems that can be transformed into\nclassification or ranking tasks, such as property prediction, and yield\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Taicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kehan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+nan_B/0/1/0/all/0/1\">Bozhao nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiest_O/0/1/0/all/0/1\">Olaf Wiest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models. (arXiv:2305.18373v1 [cs.CV])","link":"http://arxiv.org/abs/2305.18373","description":"<p>Image ad understanding is a crucial task with wide real-world applications.\nAlthough highly challenging with the involvement of diverse atypical scenes,\nreal-world entities, and reasoning over scene-texts, how to interpret image ads\nis relatively under-explored, especially in the era of foundational\nvision-language models (VLMs) featuring impressive generalizability and\nadaptability. In this paper, we perform the first empirical study of image ad\nunderstanding through the lens of pre-trained VLMs. We benchmark and reveal\npractical challenges in adapting these VLMs to image ad understanding. We\npropose a simple feature adaptation strategy to effectively fuse multimodal\ninformation for image ads and further empower it with knowledge of real-world\nentities. We hope our study draws more attention to image ad understanding\nwhich is broadly relevant to the advertising industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1\">Pradyumna Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_G/0/1/0/all/0/1\">Garima Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sugato Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18390","description":"<p>This work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhiyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18391","description":"<p>Memes are a popular form of communicating trends and ideas in social media\nand on the internet in general, combining the modalities of images and text.\nThey can express humor and sarcasm but can also have offensive content.\nAnalyzing and classifying memes automatically is challenging since their\ninterpretation relies on the understanding of visual elements, language, and\nbackground knowledge. Thus, it is important to meaningfully represent these\nsources and the interaction between them in order to classify a meme as a\nwhole. In this work, we propose to use scene graphs, that express images in\nterms of objects and their visual relations, and knowledge graphs as structured\nrepresentations for meme classification with a Transformer-based architecture.\nWe compare our approach with ImgBERT, a multimodal model that uses only learned\n(instead of structured) representations of the meme, and observe consistent\nimprovements. We further provide a dataset with human graph annotations that we\ncompare to automatically generated graphs and entity linking. Analysis shows\nthat automatic methods link more entities than human annotators and that\nautomatically generated graphs are better suited for hatefulness classification\nin memes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kougia_V/0/1/0/all/0/1\">Vasiliki Kougia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzel_S/0/1/0/all/0/1\">Simon Fetzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchmair_T/0/1/0/all/0/1\">Thomas Kirchmair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baharlou_S/0/1/0/all/0/1\">Sina Moayed Baharlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18395","description":"<p>Large Language Models (LLMs) have shown promising performance in\nknowledge-intensive reasoning tasks that require a compound understanding of\nknowledge. However, deployment of the LLMs in real-world applications can be\nchallenging due to their high computational requirements and concerns on data\nprivacy. Previous studies have focused on building task-specific small language\nmodels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,\nthese approaches are ill-suited for knowledge-intensive reasoning tasks due to\nthe limited capacity of small LMs in memorizing the knowledge required.\nMotivated by our theoretical analysis on memorization, we propose\nKnowledge-Augmented Reasoning Distillation (KARD), a novel method that\nfine-tunes small LMs to generate rationales with augmented knowledge retrieved\nfrom an external knowledge base. Moreover, we further propose a neural reranker\nto obtain documents relevant to rationale generation. We empirically show that\nKARD significantly improves the performance of small T5 and Flan-T5 models on\nthe challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE and\nStrategyQA. Notably, our method makes the 250M models achieve superior\nperformance against the fine-tuned 3B models, having 12 times larger\nparameters, on both MedQA-USMLE and StrategyQA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18396","description":"<p>Prior works have attempted to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs the private\ndata for inference. However, these frameworks impose significant overhead when\nthe private inputs are forward propagated through the original LLMs. In this\npaper, we show that substituting the computation- and communication-heavy\noperators in the transformer architecture with privacy-computing friendly\napproximations can greatly reduce the private inference costs with minor impact\non model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our\nprivacy-computing friendly model inference pipeline achieves a $5\\times$\nacceleration in computation and an 80\\% reduction in communication overhead,\nwhile retaining nearly identical accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuanqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuotao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18404","description":"<p>As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bhawesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charlie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1\">David Bellamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18410","description":"<p>The need for more usable and explainable machine learning models in\nhealthcare increases the importance of developing and utilizing causal\ndiscovery algorithms, which aim to discover causal relations by analyzing\nobservational data. Explainable approaches aid clinicians and biologists in\npredicting the prognosis of diseases and suggesting proper treatments. However,\nvery little research has been conducted at the crossroads between causal\ndiscovery, genomics, and breast cancer, and we aim to bridge this gap.\nMoreover, evaluation of causal discovery methods on real data is in general\nnotoriously difficult because ground-truth causal relations are usually\nunknown, and accordingly, in this paper, we also propose to address the\nevaluation problem with large language models. In particular, we exploit\nsuitable causal discovery algorithms to investigate how various perturbations\nin the genome can affect the survival of patients diagnosed with breast cancer.\nWe used three main causal discovery algorithms: PC, Greedy Equivalence Search\n(GES), and a Generalized Precision Matrix-based one. We experiment with a\nsubset of The Cancer Genome Atlas, which contains information about mutations,\ncopy number variations, protein levels, and gene expressions for 705 breast\ncancer patients. Our findings reveal important factors related to the vital\nstatus of patients using causal discovery algorithms. However, the reliability\nof these results remains a concern in the medical domain. Accordingly, as\nanother contribution of the work, the results are validated through language\nmodels trained on biomedical literature, such as BlueBERT and other large\nlanguage models trained on medical corpora. Our results profess proper\nutilization of causal discovery algorithms and language models for revealing\nreliable causal relations for clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Mugariya Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardan_S/0/1/0/all/0/1\">Shahad Hardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhumbhayeva_A/0/1/0/all/0/1\">Aigerim Zhumbhayeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yujia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18419","description":"<p>We propose a method of segmenting long-form speech by separating semantically\ncomplete sentences within the utterance. This prevents the ASR decoder from\nneedlessly processing faraway context while also preventing it from missing\nrelevant context within the current sentence. Semantically complete sentence\nboundaries are typically demarcated by punctuation in written text; but\nunfortunately, spoken real-world utterances rarely contain punctuation. We\naddress this limitation by distilling punctuation knowledge from a\nbidirectional teacher language model (LM) trained on written, punctuated text.\nWe compare our segmenter, which is distilled from the LM teacher, against a\nsegmenter distilled from a acoustic-pause-based teacher used in other works, on\na streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%\nrelative WER gain along with a 60 ms median end-of-segment latency reduction on\na YouTube captioning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])","link":"http://arxiv.org/abs/2305.18449","description":"<p>We tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabuada_P/0/1/0/all/0/1\">Paulo Tabuada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18462","description":"<p>Membership Inference attacks (MIAs) aim to predict whether a data sample was\npresent in the training data of a machine learning model or not, and are widely\nused for assessing the privacy risks of language models. Most existing attacks\nrely on the observation that models tend to assign higher probabilities to\ntheir training samples than non-training points. However, simple thresholding\nof the model score in isolation tends to lead to high false-positive rates as\nit does not account for the intrinsic complexity of a sample. Recent work has\ndemonstrated that reference-based attacks which compare model scores to those\nobtained from a reference model trained on similar data can substantially\nimprove the performance of MIAs. However, in order to train reference models,\nattacks of this kind make the strong and arguably unrealistic assumption that\nan adversary has access to samples closely resembling the original training\ndata. Therefore, we investigate their performance in more realistic scenarios\nand find that they are highly fragile in relation to the data distribution used\nto train reference models. To investigate whether this fragility provides a\nlayer of safety, we propose and evaluate neighbourhood attacks, which compare\nmodel scores for a given sample to scores of synthetically generated neighbour\ntexts and therefore eliminate the need for access to the training data\ndistribution. We show that, in addition to being competitive with\nreference-based attacks that have perfect knowledge about the training data\ndistribution, our attack clearly outperforms existing reference-free attacks as\nwell as reference-based attacks with imperfect knowledge, which demonstrates\nthe need for a reevaluation of the threat model of adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18466","description":"<p>Many recent efforts aim to augment language models with relevant information\nretrieved from a database at test time. We avoid the need for prompt\nengineering by directly fine-tuning the model on data retrieved at test time\nusing its standard training setup. For this purpose, we build a large-scale\ndistributed nearest neighbor index based on text embeddings of the Pile\ndataset. Given a query to a language model, our system retrieves the neighbors\nof the query and fine-tunes the model on the text data corresponding to those\nneighbors. Surprisingly, retrieving and training on as few as 20 neighbors,\neach for only one gradient iteration, drastically improves performance across\nmore than twenty language modeling tasks in the Pile benchmark. For example,\ntest-time training significantly narrows the performance gap between a small\nGPT2 model and a GPTNeo model, more than ten times larger, that was\nspecifically trained to convergence on the Pile. Sufficient index quality and\nsize, however, are important. Our work establishes a valuable first baseline\nfor implementing test-time training in the context of large language models,\nopening the door to numerous promising research avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18486","description":"<p>The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Amran Hossen Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])","link":"http://arxiv.org/abs/2305.18498","description":"<p>The advents of Large Language Models (LLMs) have shown promise in augmenting\nprogramming using natural interactions. However, while LLMs are proficient in\ncompiling common usage patterns into a programming language, e.g., Python, it\nremains a challenge how to edit and debug an LLM-generated program. We\nintroduce ANPL, a programming system that allows users to decompose\nuser-specific tasks. In an ANPL program, a user can directly manipulate sketch,\nwhich specifies the data flow of the generated program. The user annotates the\nmodules, or hole with natural language descriptions offloading the expensive\ntask of generating functionalities to the LLM. Given an ANPL program, the ANPL\ncompiler generates a cohesive Python program that implements the\nfunctionalities in hole, while respecting the dataflows specified in sketch. We\ndeploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique\ntasks that are challenging for state-of-the-art AI systems, showing it\noutperforms baseline programming systems that (a) without the ability to\ndecompose tasks interactively and (b) without the guarantee that the modules\ncan be correctly composed together. We obtain a dataset consisting of 300/400\nARC tasks that were successfully decomposed and grounded in Python, providing\nvaluable insights into how humans decompose programmatic tasks. See the dataset\nat https://iprc-dip.github.io/DARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Ziyuan Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pengwei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuanbo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yewen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])","link":"http://arxiv.org/abs/2305.18500","description":"<p>Vision and text have been fully explored in contemporary video-text\nfoundational models, while other modalities such as audio and subtitles in\nvideos have not received sufficient attention. In this paper, we resort to\nestablish connections between multi-modality video tracks, including Vision,\nAudio, and Subtitle, and Text by exploring an automatically generated\nlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,\nwe first collect 27 million open-domain video clips and separately train a\nvision and an audio captioner to generate vision and audio captions. Then, we\nemploy an off-the-shelf Large Language Model (LLM) to integrate the generated\ncaptions, together with subtitles and instructional prompts into omni-modality\ncaptions. Based on the proposed VAST-27M dataset, we train an omni-modality\nvideo-text foundational model named VAST, which can perceive and process\nvision, audio, and subtitle modalities from video, and better support various\ntasks including vision-text, audio-text, and multi-modal video-text tasks\n(retrieval, captioning and QA). Extensive experiments have been conducted to\ndemonstrate the effectiveness of our proposed VAST-27M corpus and VAST\nfoundation model. VAST achieves 22 new state-of-the-art results on various\ncross-modality benchmarks. Code, model and dataset will be released at\nhttps://github.com/TXH-mercury/VAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Handong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingzhen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18503","description":"<p>Textual adversarial attacks can discover models' weaknesses by adding\nsemantic-preserved but misleading perturbations to the inputs. The long-lasting\nadversarial attack-and-defense arms race in Natural Language Processing (NLP)\nis algorithm-centric, providing valuable techniques for automatic robustness\nevaluation. However, the existing practice of robustness evaluation may exhibit\nissues of incomprehensive evaluation, impractical evaluation protocol, and\ninvalid adversarial samples. In this paper, we aim to set up a unified\nautomatic robustness evaluation framework, shifting towards model-centric\nevaluation to further exploit the advantages of adversarial attacks. To address\nthe above challenges, we first determine robustness evaluation dimensions based\non model capabilities and specify the reasonable algorithm to generate\nadversarial samples for each dimension. Then we establish the evaluation\nprotocol, including evaluation settings and metrics, under realistic demands.\nFinally, we use the perturbation degree of adversarial samples to control the\nsample validity. We implement a toolkit RobTest that realizes our automatic\nrobustness evaluation framework. In our experiments, we conduct a robustness\nevaluation of RoBERTa models to demonstrate the effectiveness of our evaluation\nframework, and further show the rationality of each component in the framework.\nThe code will be made public at \\url{https://github.com/thunlp/RobTest}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanlu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18507","description":"<p>Large language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muhan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18513","description":"<p>Transformer-based models, such as BERT and ViT, have achieved\nstate-of-the-art results across different natural language processing (NLP) and\ncomputer vision (CV) tasks. However, these models are extremely memory\nintensive during their fine-tuning process, making them difficult to deploy on\nGPUs with limited memory resources. To address this issue, we introduce a new\ntool called SlimFit that reduces the memory requirements of these models by\ndynamically analyzing their training dynamics and freezing less-contributory\nlayers during fine-tuning. The layers to freeze are chosen using a runtime\ninter-layer scheduling algorithm. SlimFit adopts quantization and pruning for\nparticular layers to balance the load of dynamic activations and to minimize\nthe memory footprint of static activations, where static activations refer to\nthose that cannot be discarded regardless of freezing. This allows SlimFit to\nfreeze up to 95% of layers and reduce the overall on-device GPU memory usage of\ntransformer-based models such as ViT and BERT by an average of 2.2x, across\ndifferent NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10,\nCIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For\nsuch NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory\nusage with an accuracy degradation of only up to 0.4%. As a result, while\nfine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128\nrequires 3 and 2 32GB GPUs respectively, SlimFit enables their fine-tuning on a\nsingle 32GB GPU without any significant accuracy degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardakani_A/0/1/0/all/0/1\">Arash Ardakani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_A/0/1/0/all/0/1\">Altan Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shangyin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovici_D/0/1/0/all/0/1\">Doru Thom Popovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1\">Alvin Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iancu_C/0/1/0/all/0/1\">Costin Iancu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1\">Koushik Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forgotten Knowledge: Examining the Citational Amnesia in NLP. (arXiv:2305.18554v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18554","description":"<p>Citing papers is the primary method through which modern scientific writing\ndiscusses and builds on past work. Collectively, citing a diverse set of papers\n(in time and area of study) is an indicator of how widely the community is\nreading. Yet, there is little work looking at broad temporal patterns of\ncitation. This work systematically and empirically examines: How far back in\ntime do we tend to go to cite papers? How has that changed over time, and what\nfactors correlate with this citational attention/amnesia? We chose NLP as our\ndomain of interest and analyzed approximately 71.5K papers to show and quantify\nseveral key trends in citation. Notably, around 62% of cited papers are from\nthe immediate five years prior to publication, whereas only about 17% are more\nthan ten years old. Furthermore, we show that the median age and age diversity\nof cited papers were steadily increasing from 1990 to 2014, but since then, the\ntrend has reversed, and current NLP papers have an all-time low temporal\ncitation diversity. Finally, we show that unlike the 1990s, the highly cited\npapers in the last decade were also papers with the least citation diversity,\nlikely contributing to the intense (and arguably harmful) recency focus. Code,\ndata, and a demo are available on the project homepage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Janvijay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rungta_M/0/1/0/all/0/1\">Mukund Rungta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model. (arXiv:2305.18565v1 [cs.CV])","link":"http://arxiv.org/abs/2305.18565","description":"<p>We present the training recipe and results of scaling up PaLI-X, a\nmultilingual vision and language model, both in terms of size of the components\nand the breadth of its training task mixture. Our model achieves new levels of\nperformance on a wide-range of varied and complex tasks, including multiple\nimage-based captioning and question-answering tasks, image-based document\nunderstanding and few-shot (in-context) learning, as well as object detection,\nvideo question answering, and video captioning. PaLI-X advances the\nstate-of-the-art on most vision-and-language benchmarks considered (25+ of\nthem). Finally, we observe emerging capabilities, such as complex counting and\nmultilingual object detection, tasks that are not explicitly in the training\nmix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padlewski_P/0/1/0/all/0/1\">Piotr Padlewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1\">Carlos Riquelme Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1\">Sebastian Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salz_D/0/1/0/all/0/1\">Daniel Salz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1\">Michael Tschannen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_C/0/1/0/all/0/1\">Ceslee Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietrzyk_P/0/1/0/all/0/1\">Paulina Pietrzyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1\">Marvin Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavetic_F/0/1/0/all/0/1\">Filip Pavetic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amelot_J/0/1/0/all/0/1\">Julien Amelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Peter Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_K/0/1/0/all/0/1\">Keran Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyedhosseini_M/0/1/0/all/0/1\">Mojtaba Seyedhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18569","description":"<p>Understanding and addressing unfairness in LLMs are crucial for responsible\nAI deployment. However, there is a limited availability of quantitative\nanalyses and in-depth studies regarding fairness evaluations in LLMs,\nespecially when applying LLMs to high-stakes fields. This work aims to fill\nthis gap by providing a systematic evaluation of the effectiveness and fairness\nof LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's\nperformance in high-takes fields including education, criminology, finance and\nhealthcare. To make thorough evaluation, we consider both group fairness and\nindividual fairness and we also observe the disparities in ChatGPT's outputs\nunder a set of biased or unbiased prompts. This work contributes to a deeper\nunderstanding of LLMs' fairness performance, facilitates bias mitigation and\nfosters the development of responsible artificial intelligence systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding. (arXiv:2305.18576v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18576","description":"<p>ICD coding is designed to assign the disease codes to electronic health\nrecords (EHRs) upon discharge, which is crucial for billing and clinical\nstatistics. In an attempt to improve the effectiveness and efficiency of manual\ncoding, many methods have been proposed to automatically predict ICD codes from\nclinical notes. However, most previous works ignore the decisive information\ncontained in structured medical data in EHRs, which is hard to be captured from\nthe noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal\nAttention Network (TreeMAN) to fuse tabular features and textual features into\nmultimodal representations by enhancing the text representations with\ntree-based features via the attention mechanism. Tree-based features are\nconstructed according to decision trees learned from structured multimodal\nmedical data, which capture the decisive information about ICD coding. We can\napply the same multi-label classifier from previous text models to the\nmultimodal representations to predict ICD codes. Experiments on two MIMIC\ndatasets show that our method outperforms prior state-of-the-art ICD coding\napproaches. The code is available at https://github.com/liu-zichen/TreeMAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yanlong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fen Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18582","description":"<p>Current LLMs have demonstrated remarkable capabilities in addressing users'\nrequests for various types of information. However, these models are limited by\nthe most recent data available in their pretraining corpora, rendering them\nincapable of providing up-to-date information. Retraining LLMs from scratch is\ncost-prohibitive, and the effectiveness of continual fine-tuning on new corpora\nhas not been thoroughly examined. Additionally, current update procedures\ntypically demand significant human input to prepare the information into more\nstructured format, such as knowledge triples, conversational data or responses\nwith human feedback. In this study, we conduct a comprehensive examination of a\nnovel self information update task in LLMs, which only requires the provision\nof informative text corpora. For instance, we can use the latest news articles\nto update the LLMs' existing knowledge. We define the self information update\ntask and assess the continual fine-tuning approach for this purpose. We observe\nthat the naive method of continual fine-tuning can be problematic due to LLMs'\nexposure bias, which prioritizes existing information over new information we\naim to integrate and leads to incorrect reasoning chains that ultimately\ndiminish the efficacy of information updates. Based on our analysis, we propose\nan effective method to mitigate exposure bias by incorporating the selection of\nrelevant facts into training losses. Furthermore, we develop a dataset to\nevaluate information updates, derived from news articles published after March\n2023. Experimental results demonstrate that our proposed approach significantly\nincreases the factual consistency score (0 to 1) by 0.16 while having minimal\nimpact on performance for instructions not directly related to the new\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pengfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18585","description":"<p>The advent of social media has given rise to numerous ethical challenges,\nwith hate speech among the most significant concerns. Researchers are\nattempting to tackle this problem by leveraging hate-speech detection and\nemploying language models to automatically moderate content and promote civil\ndiscourse. Unfortunately, recent studies have revealed that hate-speech\ndetection systems can be misled by adversarial attacks, raising concerns about\ntheir resilience. While previous research has separately addressed the\nrobustness of these models under adversarial attacks and their\ninterpretability, there has been no comprehensive study exploring their\nintersection. The novelty of our work lies in combining these two critical\naspects, leveraging interpretability to identify potential vulnerabilities and\nenabling the design of targeted adversarial attacks. We present a comprehensive\nand comparative analysis of adversarial robustness exhibited by various\nhate-speech detection models. Our study evaluates the resilience of these\nmodels against adversarial attacks using explainability techniques. To gain\ninsights into the models' decision-making processes, we employ the Local\nInterpretable Model-agnostic Explanations (LIME) framework. Based on the\nexplainability results obtained by LIME, we devise and execute targeted attacks\non the text by leveraging the TextAttack tool. Our findings enhance the\nunderstanding of the vulnerabilities and strengths exhibited by\nstate-of-the-art hate-speech detection models. This work underscores the\nimportance of incorporating explainability in the development and evaluation of\nsuch models to enhance their resilience against adversarial attacks.\nUltimately, this work paves the way for creating more robust and reliable\nhate-speech detection systems, fostering safer online environments and\npromoting ethical discourse on social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumbam_P/0/1/0/all/0/1\">Pranath Reddy Kumbam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Sohaib Uddin Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamminedi_P/0/1/0/all/0/1\">Prashanth Thamminedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harish_S/0/1/0/all/0/1\">Suhas Harish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_I/0/1/0/all/0/1\">Ian Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorr_B/0/1/0/all/0/1\">Bonnie J. Dorr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Accurate Low Latency ASR for Streaming Voice Search. (arXiv:2305.18596v1 [cs.SD])","link":"http://arxiv.org/abs/2305.18596","description":"<p>Automatic Speech Recognition (ASR) plays a crucial role in voice-based\napplications. For applications requiring real-time feedback like Voice Search,\nstreaming capability becomes vital. While LSTM/RNN and CTC based ASR systems\nare commonly employed for low-latency streaming applications, they often\nexhibit lower accuracy compared to state-of-the-art models due to a lack of\nfuture audio frames. In this work, we focus on developing accurate LSTM,\nattention, and CTC based streaming ASR models for large-scale Hinglish (a blend\nof Hindi and English) Voice Search. We investigate various modifications in\nvanilla LSTM training which enhance the system's accuracy while preserving its\nstreaming capabilities. We also address the critical requirement of\nend-of-speech (EOS) detection in streaming applications. We present a simple\ntraining and inference strategy for end-to-end CTC models that enables joint\nASR and EOS detection. The evaluation of our model on Flipkart's Voice Search,\nwhich handles substantial traffic of approximately 6 million queries per day,\ndemonstrates significant performance gains over the vanilla LSTM-CTC model. Our\nmodel achieves a word error rate (WER) of 3.69% without EOS and 4.78% with EOS\nwhile also reducing the search latency by approximately ~1300 ms (equivalent to\n46.64% reduction) when compared to an independent voice activity detection\n(VAD) model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Abhinav Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garera_N/0/1/0/all/0/1\">Nikesh Garera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces. (arXiv:2305.18598v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18598","description":"<p>We study semantic construal in grammatical constructions using large language\nmodels. First, we project contextual word embeddings into three interpretable\nsemantic spaces, each defined by a different set of psycholinguistic feature\nnorms. We validate these interpretable spaces and then use them to\nautomatically derive semantic characterizations of lexical items in two\ngrammatical constructions: nouns in subject or object position within the same\nsentence, and the AANN construction (e.g., `a beautiful three days'). We show\nthat a word in subject position is interpreted as more agentive than the very\nsame word in object position, and that the nouns in the AANN construction are\ninterpreted as more measurement-like than when in the canonical alternation.\nOur method can probe the distributional meaning of syntactic constructions at a\ntemplatic level, abstracted away from specific lexemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronis_G/0/1/0/all/0/1\">Gabriella Chronis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1\">Katrin Erk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18599","description":"<p>The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for fake news\ndetection. However, state-of-the-art approaches are usually trained on datasets\nof smaller size or with a limited set of specific topics. As a consequence,\nthese models lack generalization capabilities and are not applicable to\nreal-world data. In this paper, we propose three models that adopt and\nfine-tune state-of-the-art multimodal transformers for multimodal fake news\ndetection. We conduct an in-depth analysis by manipulating the input data aimed\nto explore models performance in realistic use cases on social media. Our study\nacross multiple models demonstrates that these systems suffer significant\nperformance drops against manipulated data. To reduce the bias and improve\nmodel generalization, we suggest training data augmentation to conduct more\nmeaningful experiments for fake news detection on social media. The proposed\ndata augmentation techniques enable models to generalize better and yield\nimproved state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebi_S/0/1/0/all/0/1\">Sahar Tahmasebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From `Snippet-lects' to Doculects and Dialects: Leveraging Neural Representations of Speech for Placing Audio Signals in a Language Landscape. (arXiv:2305.18602v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18602","description":"<p>XLSR-53 a multilingual model of speech, builds a vector representation from\naudio, which allows for a range of computational treatments. The experiments\nreported here use this neural representation to estimate the degree of\ncloseness between audio files, ultimately aiming to extract relevant linguistic\nproperties. We use max-pooling to aggregate the neural representations from a\n\"snippet-lect\" (the speech in a 5-second audio snippet) to a \"doculect\" (the\nspeech in a given resource), then to dialects and languages. We use data from\ncorpora of 11 dialects belonging to 5 less-studied languages. Similarity\nmeasurements between the 11 corpora bring out greatest closeness between those\nthat are known to be dialects of the same language. The findings suggest that\n(i) dialect/language can emerge among the various parameters characterizing\naudio files and (ii) estimates of overall phonetic/phonological closeness can\nbe obtained for a little-resourced or fully unknown language. The findings help\nshed light on the type of information captured by neural representations of\nspeech and how it can be extracted from these representations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillaume_S/0/1/0/all/0/1\">S&#xe9;verine Guillaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaud_A/0/1/0/all/0/1\">Alexis Michaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18618","description":"<p>A comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plevris_V/0/1/0/all/0/1\">Vagelis Plevris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papazafeiropoulos_G/0/1/0/all/0/1\">George Papazafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Alejandro Jim&#xe9;nez Rios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18619","description":"<p>Despite a growing interest in diffusion-based language models, existing work\nhas not shown that these models can attain nontrivial likelihoods on standard\nlanguage modeling benchmarks. In this work, we take the first steps towards\nclosing the likelihood gap between autoregressive and diffusion-based language\nmodels, with the goal of building and releasing a diffusion model which\noutperforms a small but widely-known autoregressive model. We pursue this goal\nthrough algorithmic improvements, scaling laws, and increased compute. On the\nalgorithmic front, we introduce several methodological improvements for the\nmaximum-likelihood training of diffusion language models. We then study scaling\nlaws for our diffusion models and find compute-optimal training regimes which\ndiffer substantially from autoregressive models. Using our methods and scaling\nanalysis, we train and release Plaid 1B, a large diffusion language model which\noutperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent\nsamples in unconditional and zero-shot control settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1\">Ishaan Gulrajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18620","description":"<p>We introduce CONA, a novel context-aware instruction paradigm for effective\nknowledge dissemination using generative pre-trained transformer (GPT) models.\nCONA is a flexible framework designed to leverage the capabilities of Large\nLanguage Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,\nWisdom) hierarchy to automatically instruct and optimise presentation content,\nanticipate potential audience inquiries, and provide context-aware answers that\nadaptive to the knowledge level of the audience group. The unique aspect of the\nCONA paradigm lies in its combination of an independent advisory mechanism and\na recursive feedback loop rooted on the DIKW hierarchy. This synergy\nsignificantly enhances context-aware contents, ensuring they are accessible and\neasily comprehended by the audience. This paradigm is an early pioneer to\nexplore new methods for knowledge dissemination and communication in the LLM\nera, offering effective support for everyday knowledge sharing scenarios. We\nconduct experiments on a range of audience roles, along with materials from\nvarious disciplines using GPT4. Both quantitative and qualitative results\ndemonstrated that the proposed CONA paradigm achieved remarkable performance\ncompared to the outputs guided by conventional prompt engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xinghui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alfred: A System for Prompted Weak Supervision. (arXiv:2305.18623v1 [cs.LG])","link":"http://arxiv.org/abs/2305.18623","description":"<p>Alfred is the first system for programmatic weak supervision (PWS) that\ncreates training data for machine learning by prompting. In contrast to typical\nPWS systems where weak supervision sources are programs coded by experts,\nAlfred enables users to encode their subject matter expertise via natural\nlanguage prompts for language and vision-language models. Alfred provides a\nsimple Python interface for the key steps of this emerging paradigm, with a\nhigh-throughput backend for large-scale data labeling. Users can quickly\ncreate, evaluate, and refine their prompt-based weak supervision sources; map\nthe results to weak labels; and resolve their disagreements with a label model.\nAlfred enables a seamless local development experience backed by models served\nfrom self-managed computing clusters. It automatically optimizes the execution\nof prompts with optimized batching mechanisms. We find that this optimization\nimproves query throughput by 2.9x versus a naive approach. We present two\nexample use cases demonstrating Alfred on YouTube comment spam detection and\npet breeds classification. Alfred is open source, available at\nhttps://github.com/BatsResearch/alfred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peilin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18624","description":"<p>Contrastive learning has become a popular solution for few-shot Name Entity\nRecognization (NER). The conventional configuration strives to reduce the\ndistance between tokens with the same labels and increase the distance between\ntokens with different labels. The effect of this setup may, however, in the\nmedical domain, there are a lot of entities annotated as OUTSIDE (O), and they\nare undesirably pushed apart to other entities that are not labeled as OUTSIDE\n(O) by the current contrastive learning method end up with a noisy prototype\nfor the semantic representation of the label, though there are many OUTSIDE (O)\nlabeled entities are relevant to the labeled entities. To address this\nchallenge, we propose a novel method named Weighted Prototypical Contrastive\nLearning for Medical Few Shot Named Entity Recognization (W-PROCER). Our\napproach primarily revolves around constructing the prototype-based contractive\nloss and weighting network. These components play a crucial role in assisting\nthe model in differentiating the negative samples from OUTSIDE (O) tokens and\nenhancing the discrimination ability of contrastive learning. Experimental\nresults show that our proposed W-PROCER framework significantly outperforms the\nstrong baselines on the three medical benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1\">Jeremy Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huixue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Huaiyuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model. (arXiv:2305.18638v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18638","description":"<p>In this study, we developed an automated short answer grading (ASAG) model\nthat provided both analytic scores and final holistic scores. Short answer\nitems typically consist of multiple sub-questions, and providing an analytic\nscore and the text span relevant to each sub-question can increase the\ninterpretability of the automated scores. Furthermore, they can be used to\ngenerate actionable feedback for students. Despite these advantages, most\nstudies have focused on predicting only holistic scores due to the difficulty\nin constructing dataset with manual annotations. To address this difficulty, we\nused large language model (LLM)-based one-shot prompting and a text similarity\nscoring model with domain adaptation using small manually annotated dataset.\nThe accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a\nsubset of the publicly available ASAG dataset. The model achieved a substantial\nimprovement over the majority baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Su-Youn Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs. (arXiv:2305.18641v1 [cs.CL])","link":"http://arxiv.org/abs/2305.18641","description":"<p>Building cross-model intelligence that can understand charts and communicate\nthe salient information hidden behind them is an appealing challenge in the\nvision and language(V+L) community. The capability to uncover the underlined\ntable data of chart figures is a critical key to automatic chart understanding.\nWe introduce ChartT5, a V+L model that learns how to interpret table\ninformation from chart images via cross-modal pre-training on plot table pairs.\nSpecifically, we propose two novel pre-training objectives: Masked Header\nPrediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with\ndifferent skills to interpret the table information. We have conducted\nextensive experiments on chart question answering and chart summarization to\nverify the effectiveness of the proposed pre-training strategies. In\nparticular, on the ChartQA benchmark, our ChartT5 outperforms the\nstate-of-the-art non-pretraining methods by over 8% performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Inspiring Content on Social Media. (arXiv:2109.02734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02734","description":"<p>Inspiration moves a person to see new possibilities and transforms the way\nthey perceive their own potential. Inspiration has received little attention in\npsychology, and has not been researched before in the NLP community. To the\nbest of our knowledge, this work is the first to study inspiration through\nmachine learning methods. We aim to automatically detect inspiring content from\nsocial media data. To this end, we analyze social media posts to tease out what\nmakes a post inspiring and what topics are inspiring. We release a dataset of\n5,800 inspiring and 5,800 non-inspiring English-language public post unique ids\ncollected from a dump of Reddit public posts made available by a third party\nand use linguistic heuristics to automatically detect which social media\nEnglish-language posts are inspiring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02732","description":"<p>Conversational recommender systems (CRS) aim to recommend suitable items to\nusers through natural language conversations. For developing effective CRSs, a\nmajor technical issue is how to accurately infer user preference from very\nlimited conversation context. To address issue, a promising solution is to\nincorporate external data for enriching the context information. However, prior\nstudies mainly focus on designing fusion models tailored for some specific type\nof external data, which is not general to model and utilize multi-type external\ndata.\n</p>\n<p>To effectively leverage multi-type external data, we propose a novel\ncoarse-to-fine contrastive learning framework to improve data semantic fusion\nfor CRS. In our approach, we first extract and represent multi-grained semantic\nunits from different data signals, and then align the associated multi-type\nsemantic units in a coarse-to-fine way. To implement this framework, we design\nboth coarse-grained and fine-grained procedures for modeling user preference,\nwhere the former focuses on more general, coarse-grained semantic fusion and\nthe latter focuses on more specific, fine-grained semantic fusion. Such an\napproach can be extended to incorporate more kinds of external data. Extensive\nexperiments on two public CRS datasets have demonstrated the effectiveness of\nour approach in both recommendation and conversation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">He Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13151","description":"<p>We design and evaluate a Bayesian optimization framework for resource\nefficient pre-training of Transformer-based language models (TLMs). TLM\npre-training requires high computational resources and introduces many\nunresolved design choices, such as selecting its pre-training hyperparameters.\nWe propose a multi-armed bandit framework for the sequential selection of TLM\npre-training hyperparameters, aimed at optimizing language model performance,\nin a resource efficient manner. We design a Thompson sampling algorithm, with a\nsurrogate Gaussian process reward model of the Masked Language Model (MLM)\npre-training objective, for its sequential minimization. Instead of MLM\npre-training with fixed masking probabilities, the proposed Gaussian\nprocess-based Thompson sampling (GP-TS) accelerates pre-training by\nsequentially selecting masking hyperparameters that improve performance. We\nempirically demonstrate how GP-TS pre-trains language models efficiently, i.e.,\nit achieves lower MLM loss in fewer epochs, across a variety of settings. In\naddition, GP-TS pre-trained TLMs attain competitive downstream performance,\nwhile avoiding expensive hyperparameter grid search. GP-TS provides an\ninteractive framework for efficient and optimized TLM pre-training that, by\ncircumventing costly hyperparameter selection, enables substantial\ncomputational savings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urteaga_I/0/1/0/all/0/1\">I&#xf1;igo Urteaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draidia_M/0/1/0/all/0/1\">Moulay-Za&#xef;dane Dra&#xef;dia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1\">Tomer Lancewicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity Enhanced Table-to-Text Generation via Type Control. (arXiv:2205.10938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10938","description":"<p>Generating natural language statements to convey logical inferences from\ntabular data (i.e., Logical NLG) is a process with one input and a variety of\nvalid outputs. This characteristic underscores the need for a method to produce\na diverse set of valid outputs, presenting different perspectives of the input\ndata. We propose a simple yet effective diversity-enhancing scheme that builds\nupon an inherent property of the statements, their logic-types, by using a\ntype-controlled table-to-text generation model. We demonstrate, through\nextensive automatic and human evaluations over the two publicly available\nLogical NLG datasets, that our proposed method both facilitates the ability to\neffectively control the generated statement type, and produces results superior\nto the strongest baselines in terms of quality and factuality-diversity\ntrade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1\">Yotam Perlitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheinwald_D/0/1/0/all/0/1\">Dafna Sheinwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization. (arXiv:2209.03549v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03549","description":"<p>The problems of unfaithful summaries have been widely discussed under the\ncontext of abstractive summarization. Though extractive summarization is less\nprone to the common unfaithfulness issues of abstractive summaries, does that\nmean extractive is equal to faithful? Turns out that the answer is no. In this\nwork, we define a typology with five types of broad unfaithfulness problems\n(including and beyond not-entailment) that can appear in extractive summaries,\nincluding incorrect coreference, incomplete coreference, incorrect discourse,\nincomplete discourse, as well as other misleading information. We ask humans to\nlabel these problems out of 1600 English summaries produced by 16 diverse\nextractive systems. We find that 30% of the summaries have at least one of the\nfive issues. To automatically detect these problems, we find that 5 existing\nfaithfulness evaluation metrics for summarization have poor correlations with\nhuman judgment. To remedy this, we propose a new metric, ExtEval, that is\ndesigned for detecting unfaithful extractive summaries and is shown to have the\nbest performance. We hope our work can increase the awareness of unfaithfulness\nproblems in extractive summarization and help future work to evaluate and\nresolve these issues. Our data and code are publicly available at\nhttps://github.com/ZhangShiyue/extractive_is_not_faithful\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">David Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical computation and compositionality. (arXiv:2210.00392v3 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2210.00392","description":"<p>Developments in quantum computing and, more in general, non-standard\ncomputing systems, represent a clear indication that the very notion of what a\nphysical computing device is and does should be recast in a rigorous and sound\nframework. Physical computing has opened a whole stream of new research aimed\nto understand and control how information is processed by several types of\nphysical devices. Therefore, classical definitions and entire frameworks need\nto be adapted in order to fit a broader notion of what physical computing\nsystems really are. Recent studies have proposed a formalism that can be used\nto carve out a more proper notion of physical computing. In this paper we\npresent a framework which capture such results in a very natural way via some\nbasic constructions in Category Theory. Furthermore, we show that, within our\nframework, the compositional nature of physical computing systems is naturally\nformalized, and that it can be organized in coherent structures by the means of\ntheir relational nature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Dehghani_N/0/1/0/all/0/1\">Nima Dehghani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Caterina_G/0/1/0/all/0/1\">Gianluca Caterina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07222","description":"<p>Saliency maps can explain a neural model's predictions by identifying\nimportant input features. They are difficult to interpret for laypeople,\nespecially for instances with many features. In order to make them more\naccessible, we formalize the underexplored task of translating saliency maps\ninto natural language and compare methods that address two key challenges of\nthis approach -- what and how to verbalize. In both automatic and human\nevaluation setups, using token-level attributions from text classification\ntasks, we compare two novel methods (search-based and instruction-based\nverbalizations) against conventional feature importance representations\n(heatmap visualizations and extractive rationales), measuring simulatability,\nfaithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to\ngenerate saliency map verbalizations yields plausible explanations which\ninclude associations, abstractive summarization and commonsense reasoning,\nachieving by far the highest human ratings, but they are not faithfully\ncapturing numeric information and are inconsistent in their interpretation of\nthe task. In comparison, our search-based, model-free verbalization approach\nefficiently completes templated verbalizations, is faithful by design, but\nfalls short in helpfulness and simulatability. Our results suggest that\nsaliency map verbalization makes feature attribution explanations more\ncomprehensible and less cognitively challenging to humans than conventional\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasert_M/0/1/0/all/0/1\">Maximilian Dustin Nasert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_C/0/1/0/all/0/1\">Christopher Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers. (arXiv:2211.02809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02809","description":"<p>Automatic speech recognition (ASR) and speech translation (ST) can both use\nneural transducers as the model structure. It is thus possible to use a single\ntransducer model to perform both tasks. In real-world applications, such joint\nASR and ST models may need to be streaming and do not require source language\nidentification (i.e. language-agnostic). In this paper, we propose LAMASSU, a\nstreaming language-agnostic multilingual speech recognition and translation\nmodel using neural transducers. Based on the transducer model structure, we\npropose four methods, a unified joint and prediction network for multilingual\noutput, a clustered multilingual encoder, target language identification for\nencoder, and connectionist temporal classification regularization. Experimental\nresults show that LAMASSU not only drastically reduces the model size but also\nreaches the performances of monolingual ASR and bilingual ST models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using contradictions improves question answering systems. (arXiv:2211.05598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05598","description":"<p>This work examines the use of contradiction in natural language inference\n(NLI) for question answering (QA). Typically, NLI systems help answer questions\nby determining if a potential answer is \\emph{entailed} (supported) by some\nbackground context. But is it useful to also determine if an answer contradicts\nthe context? We test this in two settings, multiple choice and extractive QA,\nand find that systems that incorporate contradiction can do slightly better\nthan entailment-only systems on certain datasets. However, the best\nperformances come from using contradiction, entailment, and QA model confidence\nscores together. This has implications for the deployment of QA systems in\ndomains such as medicine and science where safety is an issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fortier_Dubois_E/0/1/0/all/0/1\">&#xc9;tienne Fortier-Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"World Knowledge in Multiple Choice Reading Comprehension. (arXiv:2211.07040v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07040","description":"<p>Recently it has been shown that without any access to the contextual passage,\nmultiple choice reading comprehension (MCRC) systems are able to answer\nquestions significantly better than random on average. These systems use their\naccumulated \"world knowledge\" to directly answer questions, rather than using\ninformation from the passage. This paper examines the possibility of exploiting\nthis observation as a tool for test designers to ensure that the use of \"world\nknowledge\" is acceptable for a particular set of questions. We propose\ninformation-theory based metrics that enable the level of \"world knowledge\"\nexploited by systems to be assessed. Two metrics are described: the expected\nnumber of options, which measures whether a passage-free system can identify\nthe answer a question using world knowledge; and the contextual mutual\ninformation, which measures the importance of context for a given question. We\ndemonstrate that questions with low expected number of options, and hence\nanswerable by the shortcut system, are often similarly answerable by humans\nwithout context. This highlights that the general knowledge 'shortcuts' could\nbe equally used by exam candidates, and that our proposed metrics may be\nhelpful for future test designers to monitor the quality of questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14228","description":"<p>In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelghani_R/0/1/0/all/0/1\">Rania Abdelghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yen-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_P/0/1/0/all/0/1\">Pauline Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06094","description":"<p>Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n</p>\n<p>Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks while abstracting language model internals and providing\nhigh-level semantics.\n</p>\n<p>To enable LMP, we implement LMQL(short for Language Model Query Language),\nwhich leverages the constraints and control flow from an LMP prompt to generate\nan efficient inference procedure that minimizes the number of expensive calls\nto the underlying language model.\n</p>\n<p>We show that LMQL can capture a wide range of state-of-the-art prompting\nmethods in an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (26-85% cost savings).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beurer_Kellner_L/0/1/0/all/0/1\">Luca Beurer-Kellner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08011","description":"<p>Dialect differences caused by regional, social, and economic factors cause\nperformance discrepancies for many groups of language technology users.\nInclusive and equitable language technology must critically be dialect\ninvariant, meaning that performance remains constant over dialectal shifts.\nCurrent systems often fall short of this ideal since they are designed and\ntested on a single dialect: Standard American English (SAE). We introduce a\nsuite of resources for evaluating and achieving English dialect invariance. The\nresource is called Multi-VALUE, a controllable rule-based translation system\nspanning 50 English dialects and 189 unique linguistic features. Multi-VALUE\nmaps SAE to synthetic forms of each dialect. First, we use this system to\nstress tests question answering, machine translation, and semantic parsing.\nStress tests reveal significant performance disparities for leading models on\nnon-standard dialects. Second, we use this system as a data augmentation\ntechnique to improve the dialect robustness of existing systems. Finally, we\npartner with native speakers of Chicano and Indian English to release new\ngold-standard variants of the popular CoQA task. To execute the transformation\ncode, run model checkpoints, and download both synthetic and gold-standard\ndialectal benchmark datasets, see <a href=\"http://value-nlp.org.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09739","description":"<p>Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heineman_D/0/1/0/all/0/1\">David Heineman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Embedder, Any Task: Instruction-Finetuned Text Embeddings. (arXiv:2212.09741v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09741","description":"<p>We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hongjin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Trajectories of Language Models Across Scales. (arXiv:2212.09803v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09803","description":"<p>Scaling up language models has led to unprecedented performance gains, but\nlittle is understood about how the training dynamics change as models get\nlarger. How do language models of different sizes learn during pre-training?\nWhy do larger language models demonstrate more desirable behaviors? In this\npaper, we analyze the intermediate training checkpoints of differently sized\nOPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token\nprediction, sequence-level generation, and downstream tasks. We find that 1) at\na given perplexity and independent of model sizes, a similar subset of training\ntokens see the most significant reduction in loss, with the rest stagnating or\nshowing double-descent behavior; 2) early in training, all models learn to\nreduce the perplexity of grammatical sequences that contain hallucinations,\nwith small models halting at this suboptimal distribution and larger ones\neventually learning to assign these sequences lower probabilities; 3)\nperplexity is a strong predictor of in-context learning performance on 74\nmultiple-choice tasks from BIG-Bench, and this holds independent of the model\nsize. Together, these results show that perplexity is more predictive of model\nbehaviors than model size or training computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. (arXiv:2212.10511v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10511","description":"<p>Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\nlimitations of relying solely on their parameters to encode a wealth of world\nknowledge. This paper aims to understand LMs' strengths and limitations in\nmemorizing factual knowledge, by conducting large-scale knowledge probing\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\npopular factual knowledge, and that scaling fails to appreciably improve\nmemorization of factual knowledge in the long tail. We then show that\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\nwhile unassisted LMs remain competitive in questions about high-popularity\nentities. Based on those findings, we devise a simple, yet effective, method\nfor powerful and efficient retrieval-augmented LMs, which retrieves\nnon-parametric memories only when necessary. Experimental results show that\nthis significantly improves models' performance while reducing the inference\ncosts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1\">Alex Mallen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically-informed Hierarchical Event Modeling. (arXiv:2212.10547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10547","description":"<p>Prior work has shown that coupling sequential latent variable models with\nsemantic ontological knowledge can improve the representational capabilities of\nevent modeling approaches. In this work, we present a novel, doubly\nhierarchical, semi-supervised event modeling framework that provides structural\nhierarchy while also accounting for ontological hierarchy. Our approach\nconsists of multiple layers of structured latent variables, where each\nsuccessive layer compresses and abstracts the previous layers. We guide this\ncompression through the injection of structured ontological knowledge that is\ndefined at the type level of events: importantly, our model allows for partial\ninjection of semantic knowledge and it does not depend on observing instances\nat any particular level of the semantic ontology. Across two different datasets\nand four different evaluation metrics, we demonstrate that our approach is able\nto out-perform the previous state-of-the-art approaches by up to 8.5%,\ndemonstrating the benefits of structured and semantic hierarchical knowledge\nfor event modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dipta_S/0/1/0/all/0/1\">Shubhashis Roy Dipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_M/0/1/0/all/0/1\">Mehdi Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10758","description":"<p>Due to their crucial role in all NLP, several benchmarks have been proposed\nto evaluate pretrained language models. In spite of these efforts, no public\nbenchmark of diverse nature currently exists for evaluation of Arabic. This\nmakes it challenging to measure progress for both Arabic and multilingual\nlanguage models. This challenge is compounded by the fact that any benchmark\ntargeting Arabic needs to take into account the fact that Arabic is not a\nsingle language but rather a collection of languages and varieties. In this\nwork, we introduce ORCA, a publicly available benchmark for Arabic language\nunderstanding evaluation. ORCA is carefully constructed to cover diverse Arabic\nvarieties and a wide range of challenging Arabic understanding tasks exploiting\n60 different datasets across seven NLU task clusters. To measure current\nprogress in Arabic NLU, we use ORCA to offer a comprehensive comparison between\n18 multilingual and Arabic language models. We also provide a public\nleaderboard with a unified single-number evaluation metric (ORCA score) to\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders. (arXiv:2212.10818v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2212.10818","description":"<p>The network architecture of end-to-end (E2E) automatic speech recognition\n(ASR) can be classified into several models, including connectionist temporal\nclassification (CTC), recurrent neural network transducer (RNN-T), attention\nmechanism, and non-autoregressive mask-predict models. Since each of these\nnetwork architectures has pros and cons, a typical use case is to switch these\nseparate models depending on the application requirement, resulting in the\nincreased overhead of maintaining all models. Several methods for integrating\ntwo of these complementary models to mitigate the overhead issue have been\nproposed; however, if we integrate more models, we will further benefit from\nthese complementary models and realize broader applications with a single\nsystem. This paper proposes four-decoder joint modeling (4D) of CTC, attention,\nRNN-T, and mask-predict, which has the following three advantages: 1) The four\ndecoders are jointly trained so that they can be easily switched depending on\nthe application scenarios. 2) Joint training may bring model regularization and\nimprove the model robustness thanks to their complementary properties. 3) Novel\none-pass joint decoding methods using CTC, attention, and RNN-T further\nimproves the performance. The experimental results showed that the proposed\nmodel consistently reduced the WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudo_Y/0/1/0/all/0/1\">Yui Sudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeel_M/0/1/0/all/0/1\">Muhammad Shakeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10823","description":"<p>Relation extraction (RE), which has relied on structurally annotated corpora\nfor model training, has been particularly challenging in low-resource scenarios\nand domains. Recent literature has tackled low-resource RE by self-supervised\nlearning, where the solution involves pretraining the entity pair embedding by\nRE-based objective and finetuning on labeled data by classification-based\nobjective. However, a critical challenge to this approach is the gap in\nobjectives, which prevents the RE model from fully utilizing the knowledge in\npretrained representations. In this paper, we aim at bridging the gap and\npropose to pretrain and finetune the RE model using consistent objectives of\ncontrastive learning. Since in this kind of representation learning paradigm,\none relation may easily form multiple clusters in the representation space, we\nfurther propose a multi-center contrastive loss that allows one relation to\nform multiple clusters to better align with pretraining. Experiments on two\ndocument-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness\nof our method. Particularly, when using 1% end-task training data, our method\noutperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10915","description":"<p>Dialogue state tracking (DST) is an important step in dialogue management to\nkeep track of users' beliefs. Existing works fine-tune all language model (LM)\nparameters to tackle the DST task, which requires significant data and\ncomputing resources for training and hosting. The cost grows exponentially in\nthe real-world deployment where dozens of fine-tuned LM are used for different\ndomains and tasks. To reduce parameter size and better utilize cross-task\nshared information, we propose to use soft prompt token embeddings to learn\ntask properties. Without tuning LM parameters, our method drastically reduces\nthe number of parameters needed to less than 0.5% of prior works while achieves\nbetter low-resource DST performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Jiun-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arpit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11716","description":"<p>The gap between speech and text modalities is a major challenge in\nspeech-to-text translation (ST). Different methods have been proposed to reduce\nthis gap, but most of them require architectural changes in ST training. In\nthis work, we propose to mitigate this issue at the pre-training stage,\nrequiring no change in the ST model. First, we show that the connectionist\ntemporal classification (CTC) loss can reduce the modality gap by design. We\nprovide a quantitative comparison with the more common cross-entropy loss,\nshowing that pre-training with CTC consistently achieves better final ST\naccuracy. Nevertheless, CTC is only a partial solution and thus, in our second\ncontribution, we propose a novel pre-training method combining CTC and optimal\ntransport to further reduce this gap. Our method pre-trains a Siamese-like\nmodel composed of two encoders, one for acoustic inputs and the other for\ntextual inputs, such that they produce representations that are close to each\nother in the Wasserstein space. Extensive experiments on the standard CoVoST-2\nand MuST-C datasets show that our pre-training method applied to the vanilla\nencoder-decoder Transformer achieves state-of-the-art performance under the\nno-external-data setting, and performs on par with recent strong multi-task\nlearning systems trained with external data. Finally, our method can also be\napplied on top of these multi-task systems, leading to further improvements for\nthese models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phuong-Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecouteux_B/0/1/0/all/0/1\">Benjamin Lecouteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09852","description":"<p>Out-of-distribution (OOD) detection is a rapidly growing field due to new\nrobustness and security requirements driven by an increased number of AI-based\nsystems. Existing OOD textual detectors often rely on an anomaly score (e.g.,\nMahalanobis distance) computed on the embedding output of the last layer of the\nencoder. In this work, we observe that OOD detection performance varies greatly\ndepending on the task and layer output. More importantly, we show that the\nusual choice (the last layer) is rarely the best one for OOD detection and that\nfar better results could be achieved if the best layer were picked. To leverage\nthis observation, we propose a data-driven, unsupervised method to combine\nlayer-wise anomaly scores. In addition, we extend classical textual OOD\nbenchmarks by including classification tasks with a greater number of classes\n(up to 77), which reflects more realistic settings. On this augmented\nbenchmark, we show that the proposed post-aggregation methods achieve robust\nand consistent results while removing manual feature selection altogether.\nTheir performance achieves near oracle's best layer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darrin_M/0/1/0/all/0/1\">Maxime Darrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_E/0/1/0/all/0/1\">Eduardo Dadalto C&#xe2;mara Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie CK Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.08127","description":"<p>CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharf_J/0/1/0/all/0/1\">Jacob Sharf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error Correction. (arXiv:2303.14342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14342","description":"<p>GPT-3 and GPT-4 models are powerful, achieving high performance on a variety\nof Natural Language Processing tasks. However, there is a relative lack of\ndetailed published analysis of their performance on the task of grammatical\nerror correction (GEC). To address this, we perform experiments testing the\ncapabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model\n(gpt-4-0314) on major GEC benchmarks. We compare the performance of different\nprompts in both zero-shot and few-shot settings, analyzing intriguing or\nproblematic outputs encountered with different prompt formats. We report the\nperformance of our best prompt on the BEA-2019 and JFLEG datasets, finding that\nthe GPT models can perform well in a sentence-level revision setting, with\nGPT-4 achieving a new high score on the JFLEG benchmark. Through human\nevaluation experiments, we compare the GPT models' corrections to source, human\nreference, and baseline GEC system sentences and observe differences in editing\nstrategies and how they are scored by human raters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coyne_S/0/1/0/all/0/1\">Steven Coyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvan_Sosa_D/0/1/0/all/0/1\">Diana Galvan-Sosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zock_M/0/1/0/all/0/1\">Michael Zock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16992","description":"<p>Recent work has compared neural network representations via similarity-based\nanalyses to improve model interpretation. The quality of a similarity measure\nis typically evaluated by its success in assigning a high score to\nrepresentations that are expected to be matched. However, existing similarity\nmeasures perform mediocrely on standard benchmarks. In this work, we develop a\nnew similarity measure, dubbed ContraSim, based on contrastive learning. In\ncontrast to common closed-form similarity measures, ContraSim learns a\nparameterized measure by using both similar and dissimilar examples. We perform\nan extensive experimental evaluation of our method, with both language and\nvision models, on the standard layer prediction benchmark and two new\nbenchmarks that we introduce: the multilingual benchmark and the image-caption\nbenchmark. In all cases, ContraSim achieves much higher accuracy than previous\nsimilarity measures, even when presented with challenging examples. Finally,\nContraSim is more suitable for the analysis of neural networks, revealing new\ninsights not captured by previous measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahamim_A/0/1/0/all/0/1\">Adir Rahamim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03682","description":"<p>Coreference Resolution is a well studied problem in NLP. While widely studied\nfor English and other resource-rich languages, research on coreference\nresolution in Bengali largely remains unexplored due to the absence of relevant\ndatasets. Bengali, being a low-resource language, exhibits greater\nmorphological richness compared to English. In this article, we introduce a new\ndataset, BenCoref, comprising coreference annotations for Bengali texts\ngathered from four distinct domains. This relatively small dataset contains\n5200 mention annotations forming 502 mention clusters within 48,569 tokens. We\ndescribe the process of creating this dataset and report performance of\nmultiple models trained using BenCoref. We anticipate that our work sheds some\nlight on the variations in coreference phenomena across multiple domains in\nBengali and encourages the development of additional resources for Bengali.\nFurthermore, we found poor crosslingual performance at zero-shot setting from\nEnglish, highlighting the need for more language-specific resources for this\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohan_S/0/1/0/all/0/1\">Shadman Rohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Mojammel Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Mohammad Mamun Or Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2304.06795","description":"<p>This paper introduces a novel Token-and-Duration Transducer (TDT)\narchitecture for sequence-to-sequence tasks. TDT extends conventional\nRNN-Transducer architectures by jointly predicting both a token and its\nduration, i.e. the number of input frames covered by the emitted token. This is\nachieved by using a joint network with two outputs which are independently\nnormalized to generate distributions over tokens and durations. During\ninference, TDT models can skip input frames guided by the predicted duration\noutput, which makes them significantly faster than conventional Transducers\nwhich process the encoder output frame by frame. TDT models achieve both better\naccuracy and significantly faster inference than conventional Transducers on\ndifferent sequence transduction tasks. TDT models for Speech Recognition\nachieve better accuracy and up to 2.82X faster inference than conventional\nTransducers. TDT models for Speech Translation achieve an absolute gain of over\n1 BLEU on the MUST-C test compared with conventional Transducers, and its\ninference is 2.27X faster. In Speech Intent Classification and Slot Filling\ntasks, TDT models improve the intent accuracy by up to over 1% (absolute) over\nconventional Transducers, while running up to 1.28X faster. Our implementation\nof the TDT model will be open-sourced with the NeMo\n(https://github.com/NVIDIA/NeMo) toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hainan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. (arXiv:2304.09116v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2304.09116","description":"<p>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild\ndatasets is important to capture the diversity in human speech such as speaker\nidentities, prosodies, and styles (e.g., singing). Current large TTS systems\nusually quantize speech into discrete tokens and use language models to\ngenerate these tokens one by one, which suffer from unstable prosody, word\nskipping/repeating issue, and poor voice quality. In this paper, we develop\nNaturalSpeech 2, a TTS system that leverages a neural audio codec with residual\nvector quantizers to get the quantized latent vectors and uses a diffusion\nmodel to generate these latent vectors conditioned on text input. To enhance\nthe zero-shot capability that is important to achieve diverse speech synthesis,\nwe design a speech prompting mechanism to facilitate in-context learning in the\ndiffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to\nlarge-scale datasets with 44K hours of speech and singing data and evaluate its\nvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS\nsystems by a large margin in terms of prosody/timbre similarity, robustness,\nand voice quality in a zero-shot setting, and performs novel zero-shot singing\nsynthesis with only a speech prompt. Audio samples are available at\nhttps://speechresearch.github.io/naturalspeech2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_K/0/1/0/all/0/1\">Kai Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ju_Z/0/1/0/all/0/1\">Zeqian Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00217","description":"<p>In a recent paper published in the Journal of Language Evolution, Kauhanen,\nEinhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the\nresults presented in one of my papers (Koplenig, Royal Society Open Science, 6,\n181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show\nthrough a series of statistical analyses that large numbers of L2 (second\nlanguage) speakers do not seem to affect the (grammatical or statistical)\ncomplexity of a language. To this end, I focus on the way in which the\nEthnologue assesses language status: a language is characterised as vehicular\nif, in addition to being used by L1 (first language) speakers, it should also\nhave a significant number of L2 users. KEW criticise both the use of\nvehicularity as a (binary) indicator of whether a language has a significant\nnumber of L2 users and the idea of imputing a zero proportion of L2 speakers to\nnon-vehicular languages whenever a direct estimate of that proportion is\nunavailable. While I recognise the importance of post-publication commentary on\npublished research, I show in this rejoinder that both points of criticism are\nexplicitly mentioned and analysed in my paper. In addition, I also comment on\nother points raised by KEW and demonstrate that both alternative analyses\noffered by KEW do not stand up to closer scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00586","description":"<p>Pre-trained language models can be surprisingly adept at tasks they were not\nexplicitly trained on, but how they implement these capabilities is poorly\nunderstood. In this paper, we investigate the basic mathematical abilities\noften acquired by pre-trained language models. Concretely, we use mechanistic\ninterpretability techniques to explain the (limited) mathematical abilities of\nGPT-2 small. As a case study, we examine its ability to take in sentences such\nas \"The war lasted from the year 1732 to the year 17\", and predict valid\ntwo-digit end years (years &gt; 32). We first identify a circuit, a small subset\nof GPT-2 small's computational graph that computes this task's output. Then, we\nexplain the role of each circuit component, showing that GPT-2 small's final\nmulti-layer perceptrons boost the probability of end years greater than the\nstart year. Finally, we find related tasks that activate our circuit. Our\nresults suggest that GPT-2 small computes greater-than using a complex but\ngeneral mechanism that activates across diverse contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanna_M/0/1/0/all/0/1\">Michael Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1\">Ollie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1\">Alexandre Variengien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02637","description":"<p>As pointed out by several scholars, current research on hate speech (HS)\nrecognition is characterized by unsystematic data creation strategies and\ndiverging annotation schemata. Subsequently, supervised-learning models tend to\ngeneralize poorly to datasets they were not trained on, and the performance of\nthe models trained on datasets labeled using different HS taxonomies cannot be\ncompared. To ease this problem, we propose applying extremely weak supervision\nthat only relies on the class name rather than on class samples from the\nannotated data. We demonstrate the effectiveness of a state-of-the-art\nweakly-supervised text classification model in various in-dataset and\ncross-dataset settings. Furthermore, we conduct an in-depth quantitative and\nqualitative analysis of the source of poor generalizability of HS\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_V/0/1/0/all/0/1\">Vishakha Laxman Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Alexander Shvets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03111","description":"<p>Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Ge Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rongyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_N/0/1/0/all/0/1\">Nan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin C.C. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Reynold Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.03602","description":"<p>Vision-and-Language Navigation (VLN) is a realistic but challenging task that\nrequires an agent to locate the target region using verbal and visual cues.\nWhile significant advancements have been achieved recently, there are still two\nbroad limitations: (1) The explicit information mining for significant guiding\nsemantics concealed in both vision and language is still under-explored; (2)\nThe previously structured map method provides the average historical appearance\nof visited nodes, while it ignores distinctive contributions of various images\nand potent information retention in the reasoning process. This work proposes a\ndual semantic-aware recurrent global-adaptive network (DSRG) to address the\nabove problems. First, DSRG proposes an instruction-guidance linguistic module\n(IGL) and an appearance-semantics visual module (ASV) for boosting vision and\nlanguage semantic learning respectively. For the memory mechanism, a global\nadaptive aggregation module (GAA) is devised for explicit panoramic observation\nfusion, and a recurrent memory fusion module (RMF) is introduced to supply\nimplicit temporal hidden states. Extensive experimental results on the R2R and\nREVERIE datasets demonstrate that our method achieves better performance than\nexisting methods. Code is available at https://github.com/CrystalSixone/DSRG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liuyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zongtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiagui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_R/0/1/0/all/0/1\">Ronghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts. (arXiv:2305.04928v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04928","description":"<p>Supervised named entity recognition (NER) in the biomedical domain depends on\nlarge sets of annotated texts with the given named entities. The creation of\nsuch datasets can be time-consuming and expensive, while extraction of new\nentities requires additional annotation tasks and retraining the model. To\naddress these challenges, this paper proposes a method for zero- and few-shot\nNER in the biomedical domain. The method is based on transforming the task of\nmulti-class token classification into binary token classification and\npre-training on a large amount of datasets and biomedical entities, which allow\nthe model to learn semantic relations between the given and potentially novel\nnamed entity labels. We have achieved average F1 scores of 35.44% for zero-shot\nNER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot\nNER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based\nmodel. The results demonstrate the effectiveness of the proposed method for\nrecognizing new biomedical entities with no or limited number of examples,\noutperforming previous transformer-based methods, and being comparable to\nGPT3-based models using models with over 1000 times fewer parameters. We make\nmodels and developed code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosprdic_M/0/1/0/all/0/1\">Milo&#x161; Ko&#x161;prdi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prodanovic_N/0/1/0/all/0/1\">Nikola Prodanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljajic_A/0/1/0/all/0/1\">Adela Ljaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1\">Bojana Ba&#x161;aragin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milo&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.05402","description":"<p>The categorization of massive e-Commerce data is a crucial, well-studied\ntask, which is prevalent in industrial settings. In this work, we aim to\nimprove an existing product categorization model that is already in use by a\nmajor web company, serving multiple applications. At its core, the product\ncategorization model is a text classification model that takes a product title\nas an input and outputs the most suitable category out of thousands of\navailable candidates. Upon a closer inspection, we found inconsistencies in the\nlabeling of similar items. For example, minor modifications of the product\ntitle pertaining to colors or measurements majorly impacted the model's output.\nThis phenomenon can negatively affect downstream recommendation or search\napplications, leading to a sub-optimal user experience.\n</p>\n<p>To address this issue, we propose a new framework for consistent text\ncategorization. Our goal is to improve the model's consistency while\nmaintaining its production-level performance. We use a semi-supervised approach\nfor data augmentation and presents two different methods for utilizing\nunlabeled samples. One method relies directly on existing catalogs, while the\nother uses a generative model. We compare the pros and cons of each approach\nand present our experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_G/0/1/0/all/0/1\">Guy Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daye_S/0/1/0/all/0/1\">Stav Yanovsky Daye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avigdor_Elgrabli_N/0/1/0/all/0/1\">Noa Avigdor-Elgrabli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_A/0/1/0/all/0/1\">Ariel Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07393","description":"<p>Dialogue systems for non-English languages have long been under-explored. In\nthis paper, we take the first step to investigate few-shot cross-lingual\ntransfer learning (FS-XLT) and multitask learning (MTL) in the context of\nopen-domain dialogue generation for non-English languages with limited data. We\nobserved catastrophic forgetting in both FS-XLT and MTL for all 6 languages in\nour preliminary experiments. To mitigate the issue, we propose a simple yet\neffective prompt learning approach that can preserve the multilinguality of\nmultilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging\nthe gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and\nour hand-crafted prompts. Experimental results on all 6 languages in terms of\nboth automatic and human evaluations demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07490","description":"<p>In recent years, large language models (LLMs) have made significant progress\nin natural language processing (NLP), with models like ChatGPT and GPT-4\nachieving impressive capabilities in various linguistic tasks. However,\ntraining models on such a large scale is challenging, and finding datasets that\nmatch the model's scale is often difficult. Fine-tuning and training models\nwith fewer parameters using novel methods have emerged as promising approaches\nto overcome these challenges. One such model is MiniGPT-4, which achieves\ncomparable vision-language understanding to GPT-4 by leveraging novel\npre-training models and innovative training strategies. However, the model\nstill faces some challenges in image understanding, particularly in artistic\npictures. A novel multimodal model called ArtGPT-4 has been proposed to address\nthese limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100\ndevice in just 2 hours, using only about 200 GB of data. The model can depict\nimages with an artistic flair and generate visual code, including aesthetically\npleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks\nfor evaluating the performance of vision-language models. In the subsequent\nevaluation methods, ArtGPT-4 scored more than 1 point higher than the current\n\\textbf{state-of-the-art} model and was only 0.25 points lower than artists on\na 6-point scale. Our code and pre-trained model are available at\n\\url{https://huggingface.co/Tyrannosaurus/ArtGPT-4}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuanzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.08455","description":"<p>We call on the Document AI (DocAI) community to reevaluate current\nmethodologies and embrace the challenge of creating more practically-oriented\nbenchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to\nremediate the halted research progress in understanding visually-rich documents\n(VRDs). We present a new dataset with novelties related to types of questions,\nanswers, and document layouts based on multi-industry, multi-domain, and\nmulti-page VRDs of various origins, and dates. Moreover, we are pushing the\nboundaries of current methods by creating multi-task and multi-domain\nevaluation setups that more accurately simulate real-world situations where\npowerful generalization and adaptation under low-resource settings are desired.\nDUDE aims to set a new standard as a more practical, long-standing benchmark\nfor the community, and we hope that it will lead to future extensions and\ncontributions that address real-world challenges. Finally, our work illustrates\nthe importance of finding more efficient ways to model language, images, and\nlayout in DocAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1\">Jordy Van Landeghem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Rub&#xe9;n Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joziak_P/0/1/0/all/0/1\">Pawe&#x142; J&#xf3;ziak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Micka&#xeb;l Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackaert_B/0/1/0/all/0/1\">Bertrand Ackaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew Blaschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_S/0/1/0/all/0/1\">Sien Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanislawek_T/0/1/0/all/0/1\">Tomasz Stanis&#x142;awek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09941","description":"<p>Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization by TGNB persons\ncontributes to and persists within Open Language Generation (OLG). By first\nunderstanding their marginalization stressors, we evaluate (1) misgendering and\n(2) harmful responses to gender disclosure. To do this, we introduce the TANGO\ndataset, comprising of template-based text curated from real-world text within\na TGNB-oriented community. We discover a dominance of binary gender norms\nwithin the models; LLMs least misgendered subjects in generated text when\ntriggered by prompts whose subjects used binary pronouns. Meanwhile,\nmisgendering was most prevalent when triggering generation with singular they\nand neopronouns. When prompted with gender disclosures, LLM text contained\nstigmatizing language and scored most toxic when triggered by TGNB gender\ndisclosure. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggers_Z/0/1/0/all/0/1\">Zachary Jaggers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10036","description":"<p>Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wenjun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingwei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11541","description":"<p>Large Language Model (LLM) has gained popularity and achieved remarkable\nresults in open-domain tasks, but its performance in real industrial\ndomain-specific scenarios is average since there is no specific knowledge in\nit. This issue has attracted widespread attention, but there are few relevant\nbenchmarks available. In this paper, we provide a benchmark Question Answering\n(QA) dataset named MSQA, which is about Microsoft products and IT technical\nproblems encountered by customers. This dataset contains industry\ncloud-specific QA knowledge, which is not available for general LLM, so it is\nwell suited for evaluating methods aimed at improving domain-specific\ncapabilities of LLM. In addition, we propose a new model interaction paradigm\nthat can empower LLM to achieve better performance on domain-specific tasks\nwhere it is not proficient. Extensive experiments demonstrate that the approach\nfollowing our model fusion framework outperforms the commonly used LLM with\nretrieval methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Mohit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Forward Tuning Boosts In-context Learning in Language Models. (arXiv:2305.13016v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13016","description":"<p>Large language models (LLMs) have exhibited an emergent in-context learning\n(ICL) ability. However, the ICL models that can solve ordinary cases are hardly\nextended to solve more complex tasks by processing the demonstration examples\nonce. This single-turn ICL is incoordinate with the decision making process of\nhumans by learning from analogy. In this paper, we propose an effective and\nefficient two-stage framework to boost ICL in LLMs by exploiting a dual form\nbetween Transformer attention and gradient descent-based optimization.\nConcretely, we divide the ICL process into \"Deep-Thinking\" and inference\nstages. The \"Deep-Thinking\" stage performs iterative forward optimization of\ndemonstrations, which is expected to boost the reasoning abilities of LLMs at\ntest time by \"thinking\" demonstrations multiple times. It produces accumulated\nmeta-gradients by manipulating the Key-Value matrices in the self-attention\nmodules of the Transformer. Then, the inference stage only takes the test query\nas input without concatenating demonstrations and applies the learned\nmeta-gradients through attention for output prediction. In this way,\ndemonstrations are not required during the inference stage since they are\nalready learned and stored in the definitive meta-gradients. LLMs can be\neffectively and efficiently adapted to downstream tasks. Extensive experiments\non ten classification and multiple-choice datasets show that our method\nachieves substantially better performance than standard ICL in terms of both\naccuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.13108","description":"<p>Automatic speech recognition systems based on deep learning are mainly\ntrained under empirical risk minimization (ERM). Since ERM utilizes the\naveraged performance on the data samples regardless of a group such as healthy\nor dysarthric speakers, ASR systems are unaware of the performance disparities\nacross the groups. This results in biased ASR systems whose performance\ndifferences among groups are severe. In this study, we aim to improve the ASR\nsystem in terms of group robustness for dysarthric speakers. To achieve our\ngoal, we present a novel approach, sample reweighting with sample affinity test\n(Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given\ndata sample and then mitigates the bias by debiasing helpfulness-based sample\nreweighting. Experimental results demonstrate that Re-SAT contributes to\nimproved ASR performance on dysarthric speech without performance degradation\non healthy speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_E/0/1/0/all/0/1\">Eungbeom Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chae_Y/0/1/0/all/0/1\">Yunkee Chae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_J/0/1/0/all/0/1\">Jaeheon Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyogu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Fragile is Relation Extraction under Entity Replacements?. (arXiv:2305.13551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13551","description":"<p>Relation extraction (RE) aims to extract the relations between entity names\nfrom the textual context. In principle, textual context determines the\nground-truth relation and the RE models should be able to correctly identify\nthe relations reflected by the textual context. However, existing work has\nfound that the RE models memorize the entity name patterns to make RE\npredictions while ignoring the textual context. This motivates us to raise the\nquestion: ``are RE models robust to the entity replacements?'' In this work, we\noperate the random and type-constrained entity replacements over the RE\ninstances in TACRED and evaluate the state-of-the-art RE models under the\nentity replacements. We observe the 30\\% - 50\\% F1 score drops on the\nstate-of-the-art RE models under entity replacements. These results suggest\nthat we need more efforts to develop effective RE models robust to entity\nreplacements. We release the source code at\nhttps://github.com/wangywUST/RobustRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1\">Manjuan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.13716","description":"<p>The recently proposed serialized output training (SOT) simplifies\nmulti-talker automatic speech recognition (ASR) by generating speaker\ntranscriptions separated by a special token. However, frequent speaker changes\ncan make speaker change prediction difficult. To address this, we propose\nboundary-aware serialized output training (BA-SOT), which explicitly\nincorporates boundary knowledge into the decoder via a speaker change detection\ntask and boundary constraint loss. We also introduce a two-stage connectionist\ntemporal classification (CTC) strategy that incorporates token-level SOT CTC to\nrestore temporal context information. Besides typical character error rate\n(CER), we introduce utterance-dependent character error rate (UD-CER) to\nfurther measure the precision of speaker change prediction. Compared to\noriginal SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a\npre-trained ASR model for BA-SOT model initialization further reduces\nCER/UD-CER by 8.4%/19.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14571","description":"<p>Current state-of-the-art models for natural language understanding require a\npreprocessing step to convert raw text into discrete tokens. This process known\nas tokenization relies on a pre-built vocabulary of words or sub-word\nmorphemes. This fixed vocabulary limits the model's robustness to spelling\nerrors and its capacity to adapt to new domains. In this work, we introduce a\nnovel open-vocabulary language model that adopts a hierarchical two-level\napproach: one at the word level and another at the sequence level. Concretely,\nwe design an intra-word module that uses a shallow Transformer architecture to\nlearn word representations from their characters, and a deep inter-word\nTransformer module that contextualizes each word representation by attending to\nthe entire word sequence. Our model thus directly operates on character\nsequences with explicit awareness of word boundaries, but without biased\nsub-word or word-level vocabulary. Experiments on various downstream tasks show\nthat our method outperforms strong baselines. We also demonstrate that our\nhierarchical model is robust to textual corruption and domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luisier_F/0/1/0/all/0/1\">Florian Luisier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16264","description":"<p>The current trend of scaling language models involves increasing both\nparameter count and training dataset size. Extrapolating this trend suggests\nthat training dataset size may soon be limited by the amount of text data\navailable on the internet. Motivated by this limit, we investigate scaling\nlanguage models in data-constrained regimes. Specifically, we run a large set\nof experiments varying the extent of data repetition and compute budget,\nranging up to 900 billion training tokens and 9 billion parameter models. We\nfind that with constrained data for a fixed compute budget, training with up to\n4 epochs of repeated data yields negligible changes to loss compared to having\nunique data. However, with more repetition, the value of adding compute\neventually decays to zero. We propose and empirically validate a scaling law\nfor compute optimality that accounts for the decreasing value of repeated\ntokens and excess parameters. Finally, we experiment with approaches mitigating\ndata scarcity, including augmenting the training dataset with code data or\nremoving commonly used filters. Models and datasets from our 400 training runs\nare freely available at https://github.com/huggingface/datablations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1\">Nouamane Tazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16756","description":"<p>Accurate and rapid situation analysis during humanitarian crises is critical\nto delivering humanitarian aid efficiently and is fundamental to humanitarian\nimperatives and the Leave No One Behind (LNOB) principle. This data analysis\ncan highly benefit from language processing systems, e.g., by classifying the\ntext data according to a humanitarian ontology. However, approaching this by\nsimply fine-tuning a generic large language model (LLM) involves considerable\npractical and ethical issues, particularly the lack of effectiveness on\ndata-sparse and complex subdomains, and the encoding of societal biases and\nunwanted associations. In this work, we aim to provide an effective and\nethically-aware system for humanitarian data analysis. We approach this by (1)\nintroducing a novel architecture adjusted to the humanitarian analysis\nframework, (2) creating and releasing a novel humanitarian-specific LLM called\nHumBert, and (3) proposing a systematic way to measure and mitigate biases. Our\nexperiments' results show the better performance of our approach on zero-shot\nand full-training settings in comparison with strong baseline models, while\nalso revealing the existence of biases in the resulting LLMs. Utilizing a\ntargeted counterfactual data augmentation approach, we significantly reduce\nthese biases without compromising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamagnone_N/0/1/0/all/0/1\">Nicol&#xf2; Tamagnone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekih_S/0/1/0/all/0/1\">Selim Fekih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contla_X/0/1/0/all/0/1\">Ximena Contla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orozco_N/0/1/0/all/0/1\">Nayid Orozco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16806","description":"<p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_H/0/1/0/all/0/1\">Hany Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16809","description":"<p>When caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arun Balajiee Lekshmi Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Ligia E. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_M/0/1/0/all/0/1\">Martha Michelle Soto Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tri Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_C/0/1/0/all/0/1\">Chris Blais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restrepo_M/0/1/0/all/0/1\">M. Adelaida Restrepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenberg_A/0/1/0/all/0/1\">Art Glenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation. (arXiv:2305.16938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16938","description":"<p>Few-shot fine-tuning and in-context learning are two alternative strategies\nfor task adaptation of pre-trained language models. Recently, in-context\nlearning has gained popularity over fine-tuning due to its simplicity and\nimproved out-of-domain generalization, and because extensive evidence shows\nthat fine-tuned models pick up on spurious correlations. Unfortunately,\nprevious comparisons of the two approaches were done using models of different\nsizes. This raises the question of whether the observed weaker out-of-domain\ngeneralization of fine-tuned models is an inherent property of fine-tuning or a\nlimitation of the experimental setup. In this paper, we compare the\ngeneralization of few-shot fine-tuning and in-context learning to challenge\ndatasets, while controlling for the models used, the number of examples, and\nthe number of parameters, ranging from 125M to 30B. Our results show that\nfine-tuned language models can in fact generalize well out-of-domain. We find\nthat both approaches generalize similarly; they exhibit large variation and\ndepend on properties such as model size and the number of examples,\nhighlighting that robust task adaptation remains a challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information. (arXiv:2305.16967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16967","description":"<p>The long-standing one-to-many issue of the open-domain dialogues poses\nsignificant challenges for automatic evaluation methods, i.e., there may be\nmultiple suitable responses which differ in semantics for a given\nconversational context. To tackle this challenge, we propose a novel\nlearning-based automatic evaluation metric (CMN), which can robustly evaluate\nopen-domain dialogues by augmenting Conditional Variational Autoencoders\n(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual\nInformation (MI) to model the semantic similarity of text in the latent space.\nExperimental results on two open-domain dialogue datasets demonstrate the\nsuperiority of our method compared with a wide range of baselines, especially\nin handling responses which are distant to the golden reference responses in\nsemantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bohao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaohui Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. (arXiv:2305.17116v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17116","description":"<p>Large language models (LLMs) have made significant advancements in natural\nlanguage processing (NLP). Broad corpora capture diverse patterns but can\nintroduce irrelevance, while focused corpora enhance reliability by reducing\nmisleading information. Training LLMs on focused corpora poses computational\nchallenges. An alternative approach is to use a retrieval-augmentation (RetA)\nmethod tested in a specific domain.\n</p>\n<p>To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a\ncustom RetA model were compared using 19 questions on diffuse large B-cell\nlymphoma (DLBCL) disease. Eight independent reviewers assessed responses based\non accuracy, relevance, and readability (rated 1-3).\n</p>\n<p>The RetA model performed best in accuracy (12/19 3-point scores, total=47)\nand relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4\nreceived the highest readability scores (17/19, 55), followed by GPT-3 (15/19,\n53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34),\nrelevance (32), and readability (38).\n</p>\n<p>Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared\nto the RetA model and Prometheus. Hallucinations were mostly associated with\nnon-existent references or fabricated efficacy data.\n</p>\n<p>These findings suggest that RetA models, supplemented with domain-specific\ncorpora, may outperform general-purpose LLMs in accuracy and relevance within\nspecific domains. However, this evaluation was limited to specific questions\nand metrics and may not capture challenges in semantic search and other NLP\ntasks. Further research will explore different LLM architectures, RetA\nmethodologies, and evaluation methods to assess strengths and limitations more\ncomprehensively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soong_D/0/1/0/all/0/1\">David Soong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Sriram Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_H/0/1/0/all/0/1\">Han Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Jan-Samuel Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_A/0/1/0/all/0/1\">Ana Caroline Costa S&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Christina Y Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karagoz_K/0/1/0/all/0/1\">Kubra Karagoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Meijian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamadeh_H/0/1/0/all/0/1\">Hisham Hamadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgs_B/0/1/0/all/0/1\">Brandon W Higgs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection. (arXiv:2305.17480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17480","description":"<p>Hyperbole and metaphor are common in day-to-day communication (e.g., \"I am in\ndeep trouble\": how does trouble have depth?), which makes their detection\nimportant, especially in a conversational AI setting. Existing approaches to\nautomatically detect metaphor and hyperbole have studied these language\nphenomena independently, but their relationship has hardly, if ever, been\nexplored computationally. In this paper, we propose a multi-task deep learning\nframework to detect hyperbole and metaphor simultaneously. We hypothesize that\nmetaphors help in hyperbole detection, and vice-versa. To test this hypothesis,\nwe annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels.\nSimultaneously, we annotate two metaphor datasets- TroFi and LCC- with\nhyperbole labels. Experiments using these datasets give an improvement of the\nstate of the art of hyperbole detection by 12%. Additionally, our multi-task\nlearning (MTL) approach shows an improvement of up to 17% over single-task\nlearning (STL) for both hyperbole and metaphor detection, supporting our\nhypothesis. To the best of our knowledge, ours is the first demonstration of\ncomputational leveraging of linguistic intimacy between metaphor and hyperbole,\nleading to showing the superiority of MTL over STL for hyperbole and metaphor\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badathala_N/0/1/0/all/0/1\">Naveen Badathala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalarani_A/0/1/0/all/0/1\">Abisek Rajakumar Kalarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siledar_T/0/1/0/all/0/1\">Tejpalsingh Siledar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Children: Improving Image-Caption Pretraining via Curriculum. (arXiv:2305.17540v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.17540","description":"<p>Image-caption pretraining has been quite successfully used for downstream\nvision tasks like zero-shot image classification and object detection. However,\nimage-caption pretraining is still a hard problem -- it requires multiple\nconcepts (nouns) from captions to be aligned to several objects in images. To\ntackle this problem, we go to the roots -- the best learner, children. We take\ninspiration from cognitive science studies dealing with children's language\nlearning to propose a curriculum learning framework. The learning begins with\neasy-to-align image caption pairs containing one concept per caption. The\ndifficulty is progressively increased with each new phase by adding one more\nconcept per caption. Correspondingly, the knowledge acquired in each learning\nphase is utilized in subsequent phases to effectively constrain the learning\nproblem to aligning one new concept-object pair in each phase. We show that\nthis learning strategy improves over vanilla image-caption training in various\nsettings -- pretraining from scratch, using a pretrained image or/and\npretrained text encoder, low data regime etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1\">Hammad A. Ayyubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1\">Rahul Lokesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1\">Alireza Zareian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application. (arXiv:2305.17701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17701","description":"<p>Large language models (LLMs) learn not only natural text generation abilities\nbut also social biases against different demographic groups from real-world\ndata. This poses a critical risk when deploying LLM-based applications.\nExisting research and resources are not readily applicable in South Korea due\nto the differences in language and culture, both of which significantly affect\nthe biases and targeted demographic groups. This limitation requires localized\nsocial bias datasets to ensure the safe and effective deployment of LLMs. To\nthis end, we present KO SB I, a new social bias dataset of 34k pairs of\ncontexts and sentences in Korean covering 72 demographic groups in 15\ncategories. We find that through filtering-based moderation, social biases in\ngenerated content can be reduced by 16.47%p on average for HyperCLOVA (30B and\n82B), and GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seokhee Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Large Language Models Know What They Don't Know?. (arXiv:2305.18153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18153","description":"<p>Large language models (LLMs) have a wealth of knowledge that allows them to\nexcel in various Natural Language Processing (NLP) tasks. Current research\nfocuses on enhancing their performance within their existing knowledge. Despite\ntheir vast knowledge, LLMs are still limited by the amount of information they\ncan accommodate and comprehend. Therefore, the ability to understand their own\nlimitations on the unknows, referred to as self-knowledge, is of paramount\nimportance. This study aims to evaluate LLMs' self-knowledge by assessing their\nability to identify unanswerable or unknowable questions. We introduce an\nautomated methodology to detect uncertainty in the responses of these models,\nproviding a novel measure of their self-knowledge. We further introduce a\nunique dataset, SelfAware, consisting of unanswerable questions from five\ndiverse categories and their answerable counterparts. Our extensive analysis,\ninvolving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an\nintrinsic capacity for self-knowledge within these models. Moreover, we\ndemonstrate that in-context learning and instruction tuning can further enhance\nthis self-knowledge. Despite this promising insight, our findings also\nhighlight a considerable gap between the capabilities of these models and human\nproficiency in recognizing the limits of their knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiushi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2305.09782","description":"<p>Visual question answering (VQA) usesimage processing algorithms to process\nthe image and natural language processing methods to understand and answer the\nquestion. VQA is helpful to a visually impaired person, can be used for the\nsecurity surveillance system and online chatbots that learn from the web. It\nuses NLP methods to learn the semantic of the question and to derive the\ntextual features. Computer vision techniques are used for generating image\nrepresentation in such a way that they can identify the objects about which\nquestion is asked. The Attention model tries to mimic the human behavior of\ngiving attention to a different region of an image according to our\nunderstanding of its context. This paper critically examines and reviews\nmethods of VQA algorithm such as generation of semantics of text,\nidentification of objects and answer classification techniques that use the\nco-attention approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahir_P/0/1/0/all/0/1\">Param Ahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diwanji_H/0/1/0/all/0/1\">Hiteishi M. Diwanji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2305.15937","description":"<p>We propose a visually grounded speech model that acquires new words and their\nvisual depictions from just a few word-image example pairs. Given a set of test\nimages and a spoken query, we ask the model which image depicts the query word.\nPrevious work has simplified this problem by either using an artificial setting\nwith digit word-image pairs or by using a large number of examples per class.\nWe propose an approach that can work on natural word-image pairs but with less\nexamples, i.e. fewer shots. Our approach involves using the given word-image\nexample pairs to mine new unsupervised word-image training pairs from large\ncollections of unlabelled speech and images. Additionally, we use a\nword-to-image attention mechanism to determine word-image similarity. With this\nnew model, we achieve better performance with fewer shots than any existing\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1\">Leanne Nortje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_B/0/1/0/all/0/1\">Benjamin van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}