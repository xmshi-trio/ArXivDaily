{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])","link":"http://arxiv.org/abs/2309.11506","description":"<p>Enterprises often own large collections of structured data in the form of\nlarge databases or an enterprise data lake. Such data collections come with\nlimited metadata and strict access policies that could limit access to the data\ncontents and, therefore, limit the application of classic retrieval and\nanalysis solutions. As a result, there is a need for solutions that can\neffectively utilize the available metadata. In this paper, we study the problem\nof matching table metadata to a business glossary containing data labels and\ndescriptions. The resulting matching enables the use of an available or curated\nbusiness glossary for retrieval and analysis without or before requesting\naccess to the data contents. One solution to this problem is to use\nmanually-defined rules or similarity measures on column names and glossary\ndescriptions (or their vector embeddings) to find the closest match. However,\nsuch approaches need to be tuned through manual labeling and cannot handle many\nbusiness glossaries that contain a combination of simple as well as complex and\nlong descriptions. In this work, we leverage the power of large language models\n(LLMs) to design generic matching methods that do not require manual tuning and\ncan identify complex relations between column names and glossaries. We propose\nmethods that utilize LLMs in two ways: a) by generating additional context for\ncolumn names that can aid with matching b) by using LLMs to directly infer if\nthere is a relation between column names and glossary descriptions. Our\npreliminary experimental results show the effectiveness of our proposed\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lobo_E/0/1/0/all/0/1\">Elita Lobo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanzadeh_O/0/1/0/all/0/1\">Oktie Hassanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1\">Dharmashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samulowitz_H/0/1/0/all/0/1\">Horst Samulowitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11508","description":"<p>Grading of exams is an important, labor intensive, subjective, repetitive and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and because of the substantial influx of data brought\nabout by digitalization. However, entrusting AI models with decision-making\nroles raises ethical considerations, mainly stemming from potential biases and\nissues related to generating false information. Thus, in this manuscript we\nprovide an evaluation of a large language model for the purpose of autograding,\nwhile also highlighting how LLMs can support educators in validating their\ngrading procedures. Our evaluation is targeted towards automatic short textual\nanswers grading (ASAG), spanning various languages and examinations from two\ndistinct courses. Our findings suggest that while \"out-of-the-box\" LLMs provide\na valuable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Johannes Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schenk_B/0/1/0/all/0/1\">Bernd Schenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1\">Christina Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_M/0/1/0/all/0/1\">Michaelis Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])","link":"http://arxiv.org/abs/2309.11564","description":"<p>Hierarchical reinforcement learning has been a compelling approach for\nachieving goal directed behavior over long sequences of actions. However, it\nhas been challenging to implement in realistic or open-ended environments. A\nmain challenge has been to find the right space of sub-goals over which to\ninstantiate a hierarchy. We present a novel approach where we use data from\nhumans solving these tasks to softly supervise the goal space for a set of long\nrange tasks in a 3D embodied environment. In particular, we use unconstrained\nnatural language to parameterize this space. This has two advantages: first, it\nis easy to generate this data from naive human participants; second, it is\nflexible enough to represent a vast range of sub-goals in human-relevant tasks.\nOur approach outperforms agents that clone expert behavior on these tasks, as\nwell as HRL from scratch without this supervised sub-goal space. Our work\npresents a novel approach to combining human expert supervision with the\nbenefits and flexibility of reinforcement learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Arun Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_K/0/1/0/all/0/1\">Kavya Kopparapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SignBank+: Multilingual Sign Language Translation Dataset. (arXiv:2309.11566v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11566","description":"<p>This work advances the field of sign language machine translation by focusing\non dataset quality and simplification of the translation system. We introduce\nSignBank+, a clean version of the SignBank dataset, optimized for machine\ntranslation. Contrary to previous works that employ complex factorization\ntechniques for translation, we advocate for a simplified text-to-text\ntranslation approach. Our evaluation shows that models trained on SignBank+\nsurpass those on the original dataset, establishing a new benchmark and\nproviding an open resource for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zifan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])","link":"http://arxiv.org/abs/2309.11568","description":"<p>We introduce the Bittensor Language Model, called \"BTLM-3B-8K\", a new\nstate-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was\ntrained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and\n8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models\nby 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B\nparameter models. Additionally, BTLM-3B-8K provides excellent long context\nperformance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192\ncontext length. We trained the model on a cleaned and deduplicated SlimPajama\ndataset; aggressively tuned the \\textmu P hyperparameters and schedule; used\nALiBi position embeddings; and adopted the SwiGLU nonlinearity.\n</p>\n<p>On Hugging Face, the most popular models have 7B parameters, indicating that\nusers prefer the quality-size ratio of 7B models. Compacting the 7B parameter\nmodel to one with 3B parameters, with little performance impact, is an\nimportant milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision\nand takes 2.5x less inference compute than 7B models, helping to open up access\nto a powerful language model on mobile and edge devices. BTLM-3B-8K is\navailable under an Apache 2.0 license on Hugging Face:\nhttps://huggingface.co/cerebras/btlm-3b-8k-base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1\">Nolan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soboleva_D/0/1/0/all/0/1\">Daria Soboleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khateeb_F/0/1/0/all/0/1\">Faisal Al-Khateeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathria_R/0/1/0/all/0/1\">Ribhu Pathria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khachane_H/0/1/0/all/0/1\">Hemant Khachane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shaheer Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhiming/0/1/0/all/0/1\">Zhiming</a> (Charles) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_R/0/1/0/all/0/1\">Robert Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeves_J/0/1/0/all/0/1\">Jacob Robert Steeves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilieva_N/0/1/0/all/0/1\">Natalia Vassilieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Marvin Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets. (arXiv:2309.11576v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11576","description":"<p>A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization. (arXiv:2309.11582v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11582","description":"<p>Previous attempts to incorporate a mention detection step into end-to-end\nneural coreference resolution for English have been hampered by the lack of\nsingleton mention span data as well as other entity information. This paper\npresents a coreference model that learns singletons as well as features such as\nentity type and information status via a multi-task learning-based approach.\nThis approach achieves new state-of-the-art scores on the OntoGUM benchmark\n(+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3\npoints on average), likely due to greater generalizability for mention\ndetection and utilization of more data from singletons when compared to only\ncoreferent mention pair matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Sameer Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechAlign: a Framework for Speech Translation Alignment Evaluation. (arXiv:2309.11585v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11585","description":"<p>Speech-to-Speech and Speech-to-Text translation are currently dynamic areas\nof research. To contribute to these fields, we present SpeechAlign, a framework\nto evaluate the underexplored field of source-target alignment in speech\nmodels. Our framework has two core components. First, to tackle the absence of\nsuitable evaluation datasets, we introduce the Speech Gold Alignment dataset,\nbuilt upon a English-German text translation gold alignment dataset. Secondly,\nwe introduce two novel metrics, Speech Alignment Error Rate (SAER) and\nTime-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment\nquality in speech models. By publishing SpeechAlign we provide an accessible\nevaluation framework for model assessment, and we employ it to benchmark\nopen-source Speech Translation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1\">Belen Alastruey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sant_A/0/1/0/all/0/1\">Aleix Sant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate speech detection in algerian dialect using deep learning. (arXiv:2309.11611v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11611","description":"<p>With the proliferation of hate speech on social networks under different\nformats, such as abusive language, cyberbullying, and violence, etc., people\nhave experienced a significant increase in violence, putting them in\nuncomfortable situations and threats. Plenty of efforts have been dedicated in\nthe last few years to overcome this phenomenon to detect hate speech in\ndifferent structured languages like English, French, Arabic, and others.\nHowever, a reduced number of works deal with Arabic dialects like Tunisian,\nEgyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in\nthis work a complete approach for detecting hate speech on online Algerian\nmessages. Many deep learning architectures have been evaluated on the corpus we\ncreated from some Algerian social networks (Facebook, YouTube, and Twitter).\nThis corpus contains more than 13.5K documents in Algerian dialect written in\nArabic, labeled as hateful or non-hateful. Promising results are obtained,\nwhich show the efficiency of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanasri_D/0/1/0/all/0/1\">Dihia Lanasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olano_J/0/1/0/all/0/1\">Juan Olano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klioui_S/0/1/0/all/0/1\">Sifal Klioui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sin Liang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekkai_L/0/1/0/all/0/1\">Lamia Sekkai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11668","description":"<p>Resolving semantic ambiguity has long been recognised as a central challenge\nin the field of machine translation. Recent work on benchmarking translation\nperformance on ambiguous sentences has exposed the limitations of conventional\nNeural Machine Translation (NMT) systems, which fail to capture many of these\ncases. Large language models (LLMs) have emerged as a promising alternative,\ndemonstrating comparable performance to traditional NMT models while\nintroducing new paradigms for controlling the target outputs. In this paper, we\nstudy the capabilities of LLMs to translate ambiguous sentences containing\npolysemous words and rare word senses. We also propose two ways to improve the\nhandling of such ambiguity through in-context learning and fine-tuning on\ncarefully curated ambiguous datasets. Experiments show that our methods can\nmatch or outperform state-of-the-art systems such as DeepL and NLLB in four out\nof five language directions. Our research provides valuable insights into\neffectively adapting LLMs for disambiguation during machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Vivek Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11669","description":"<p>Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used\nto train forward and reverse neural models that generate text from KG and vice\nversa. However models trained on datasets where KG and text pairs are not\nequivalent can suffer from more hallucination and poorer recall. In this paper,\nwe verify this empirically by generating datasets with different levels of\nnoise and find that noisier datasets do indeed lead to more hallucination. We\nargue that the ability of forward and reverse models trained on a dataset to\ncyclically regenerate source KG or text is a proxy for the equivalence between\nthe KG and the text in the dataset. Using cyclic evaluation we find that\nmanually created WebNLG is much better than automatically created TeKGen and\nT-REx. Guided by these observations, we construct a new, improved dataset\ncalled LAGRANGE using heuristics meant to improve equivalence between KG and\ntext and show the impact of each of the heuristics on cyclic evaluation. We\nalso construct two synthetic datasets using large language models (LLMs), and\nobserve that these are conducive to models that perform significantly well on\ncyclic generation of text, but less so on cyclic generation of KGs, probably\nbecause of a lack of a consistent underlying ontology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_A/0/1/0/all/0/1\">Ali Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theo Rekatsinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pound_J/0/1/0/all/0/1\">Jeff Pound</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_N/0/1/0/all/0/1\">Natalie Schluter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1\">Ihab Ilyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1\">Navdeep Jaitly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11674","description":"<p>Generative Large Language Models (LLMs) have achieved remarkable advancements\nin various NLP tasks. However, these advances have not been reflected in the\ntranslation task, especially those with moderate model sizes (i.e., 7B or 13B\nparameters), which still lag behind conventional supervised encoder-decoder\ntranslation models. Previous studies have attempted to improve the translation\ncapabilities of these moderate LLMs, but their gains have been limited. In this\nstudy, we propose a novel fine-tuning approach for LLMs that is specifically\ndesigned for the translation task, eliminating the need for the abundant\nparallel data that traditional translation models usually depend on. Our\napproach consists of two fine-tuning stages: initial fine-tuning on monolingual\ndata followed by subsequent fine-tuning on a small set of high-quality parallel\ndata. We introduce the LLM developed through this strategy as Advanced Language\nModel-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our\nresults show that the model can achieve an average improvement of more than 12\nBLEU and 12 COMET over its zero-shot performance across 10 translation\ndirections from the WMT'21 (2 directions) and WMT'22 (8 directions) test\ndatasets. The performance is significantly better than all prior work and even\nsuperior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or\n13B parameters. This method establishes the foundation for a novel training\nparadigm in machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11688","description":"<p>While large language models (LLMs) have demonstrated impressive performance\nin question-answering tasks, their performance is limited when the questions\nrequire knowledge that is not included in the model's training data and can\nonly be acquired through direct observation or interaction with the real world.\nExisting methods decompose reasoning tasks through the use of modules invoked\nsequentially, limiting their ability to answer deep reasoning tasks. We\nintroduce a method, Recursion based extensible LLM (REBEL), which handles\nopen-world, deep reasoning tasks by employing automated reasoning techniques\nlike dynamic planning and forward-chaining strategies. REBEL allows LLMs to\nreason via recursive problem decomposition and utilization of external tools.\nThe tools that REBEL uses are specified only by natural language description.\nWe further demonstrate REBEL capabilities on a set of problems that require a\ndeeply nested use of external tools in a compositional and conversational\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1\">Abhigya Sodani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moos_L/0/1/0/all/0/1\">Lauren Moos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirman_M/0/1/0/all/0/1\">Matthew Mirman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised News Discourse Profiling with Contrastive Learning. (arXiv:2309.11692v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11692","description":"<p>News Discourse Profiling seeks to scrutinize the event-related role of each\nsentence in a news article and has been proven useful across various downstream\napplications. Specifically, within the context of a given news discourse, each\nsentence is assigned to a pre-defined category contingent upon its depiction of\nthe news event structure. However, existing approaches suffer from an\ninadequacy of available human-annotated data, due to the laborious and\ntime-intensive nature of generating discourse-level annotations. In this paper,\nwe present a novel approach, denoted as Intra-document Contrastive Learning\nwith Distillation (ICLD), for addressing the news discourse profiling task,\ncapitalizing on its unique structural characteristics. Notably, we are the\nfirst to apply a semi-supervised methodology within this task paradigm, and\nevaluation demonstrates the effectiveness of the presented approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruihong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11696","description":"<p>Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable\nproficiency in comprehending and generating natural language. However, their\nunpersonalized generation paradigm may result in suboptimal user-specific\noutcomes. Typically, users converse differently based on their knowledge and\npreferences. This necessitates the task of enhancing user-oriented LLM which\nremains unexplored. While one can fully train an LLM for this objective, the\nresource consumption is unaffordable. Prior research has explored memory-based\nmethods to store and retrieve knowledge to enhance generation without\nretraining for new queries. However, we contend that a mere memory module is\ninadequate to comprehend a user's preference, and fully training an LLM can be\nexcessively costly. In this study, we propose a novel computational bionic\nmemory mechanism, equipped with a parameter-efficient fine-tuning schema, to\npersonalize LLMs. Our extensive experimental results demonstrate the\neffectiveness and superiority of the proposed approach. To encourage further\nresearch into this area, we are releasing a new conversation dataset generated\nentirely by LLM based on an open-source medical corpus, as well as our\nimplementation code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fubang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11710","description":"<p>Referenceless metrics (e.g., CLIPScore) use pretrained vision--language\nmodels to assess image descriptions directly without costly ground-truth\nreference texts. Such methods can facilitate rapid progress, but only if they\ntruly align with human preference judgments. In this paper, we introduce\nContextRef, a benchmark for assessing referenceless metrics for such alignment.\nContextRef has two components: human ratings along a variety of established\nquality dimensions, and ten diverse robustness checks designed to uncover\nfundamental weaknesses. A crucial aspect of ContextRef is that images and\ndescriptions are presented in context, reflecting prior work showing that\ncontext is important for description quality. Using ContextRef, we assess a\nvariety of pretrained models, scoring functions, and techniques for\nincorporating context. None of the methods is successful with ContextRef, but\nwe show that careful fine-tuning yields substantial improvements. ContextRef\nremains a challenging benchmark though, in large part due to the challenge of\ncontext dependence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features. (arXiv:2309.11791v1 [cs.DL])","link":"http://arxiv.org/abs/2309.11791","description":"<p>Wikipedia articles are hierarchically organized through categories and lists,\nproviding one of the most comprehensive and universal taxonomy, but its open\ncreation is causing redundancies and inconsistencies. Assigning DBPedia classes\nto Wikipedia categories and lists can alleviate the problem, realizing a large\nknowledge graph which is essential for categorizing digital contents through\nentity linking and typing. However, the existing approach of CaLiGraph is\nproducing incomplete and non-fine grained mappings. In this paper, we tackle\nthe problem as ontology alignment, where structural information of knowledge\ngraphs and lexical and semantic features of ontology class names are utilized\nto discover confident mappings, which are in turn utilized for finetuing\npretrained language models in a distant supervision fashion. Our method SLHCat\nconsists of two main parts: 1) Automatically generating training data by\nleveraging knowledge graph structure, semantic similarities, and named entity\ntyping. 2) Finetuning and prompt-tuning of the pre-trained language model BERT\nare carried out over the training data, to capture semantic and syntactic\nproperties of class names. Our model SLHCat is evaluated over a benchmark\ndataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping\npairs. SLHCat is outperforming the baseline model by a large margin of 25% in\naccuracy, offering a practical solution for large-scale ontology mapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiaxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaihara_M/0/1/0/all/0/1\">Mizuho Iwaihara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Embedding with Neural Probabilistic Prior. (arXiv:2309.11824v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11824","description":"<p>To improve word representation learning, we propose a probabilistic prior\nwhich can be seamlessly integrated with word embedding models. Different from\nprevious methods, word embedding is taken as a probabilistic generative model,\nand it enables us to impose a prior regularizing word representation learning.\nThe proposed prior not only enhances the representation of embedding vectors\nbut also improves the model's robustness and stability. The structure of the\nproposed prior is simple and effective, and it can be easily implemented and\nflexibly plugged in most existing word embedding models. Extensive experiments\nshow the proposed method improves word representation on various tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shaogang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Chinese Prompt Attack Dataset for LLMs with Evil Content. (arXiv:2309.11830v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11830","description":"<p>Large Language Models (LLMs) present significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset to evaluate the abilities of\ndefending prompt attack. In this paper, we introduce a Chinese Prompt Attack\nDataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate\nunexpected outputs with several carefully designed prompt attack approaches and\nwidely concerned attacking contents. Different from previous datasets involving\nsafety estimation, We construct the prompts considering three dimensions:\ncontents, attacking methods and goals, thus the responses can be easily\nevaluated and analysed. We run several well-known Chinese LLMs on our dataset,\nand the results show that our prompts are significantly harmful to LLMs, with\naround 70% attack success rate. We will release CPAD to encourage further\nstudies on prompt attack and defense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fubang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1\">Lizhi Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11838","description":"<p>In this paper, we investigate the use of large language models (LLMs) like\nChatGPT for document-grounded response generation in the context of\ninformation-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus\nof task-oriented dialogues in four social service domains previously used in\nthe DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded\nin multiple documents providing relevant information. We generate dialogue\ncompletion responses by prompting a ChatGPT model, using two methods:\nChat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT\nmodel pretraining while LlamaIndex also extracts relevant information from\ndocuments. Observing that document-grounded response generation via LLMs cannot\nbe adequately assessed by automatic evaluation metrics as they are\nsignificantly more verbose, we perform a human evaluation where annotators rate\nthe output of the shared task winning system, the two Chat-GPT variants\noutputs, and human responses. While both ChatGPT variants are more likely to\ninclude information not present in the relevant segments, possibly including a\npresence of hallucinations, they are rated higher than both the shared task\nwinning system and human responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braunschweiler_N/0/1/0/all/0/1\">Norbert Braunschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keizer_S/0/1/0/all/0/1\">Simon Keizer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1\">Svetlana Stoyanchev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis. (arXiv:2309.11849v1 [cs.SD])","link":"http://arxiv.org/abs/2309.11849","description":"<p>This paper explores predicting suitable prosodic features for fine-grained\nemotion analysis from the discourse-level text. To obtain fine-grained\nemotional prosodic features as predictive values for our model, we extract a\nphoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style\nEmbedding as prosodic speech features from the speech with the help of a style\ntransfer model. We propose a Discourse-level Multi-scale text Prosodic Model\n(D-MPM) that exploits multi-scale text to predict these two prosodic features.\nThe proposed model can be used to analyze better emotional prosodic features\nand thus guide the speech synthesis model to synthesize more expressive speech.\nTo quantitatively evaluate the proposed model, we contribute a new and\nlarge-scale Discourse-level Chinese Audiobook (DCA) dataset with more than\n13,000 utterances annotated sequences to evaluate the proposed model.\nExperimental results on the DCA dataset show that the multi-scale text\ninformation effectively helps to predict prosodic features, and the\ndiscourse-level text improves both the overall coherence and the user\nexperience. More interestingly, although we aim at the synthesis effect of the\nstyle transfer model, the synthesized speech by the proposed text prosodic\nanalysis model is even better than the style transfer from the original speech\nin some user evaluation indicators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xianhao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Sanitization of Large Language Models. (arXiv:2309.11852v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11852","description":"<p>We explore a knowledge sanitization approach to mitigate the privacy concerns\nassociated with large language models (LLMs). LLMs trained on a large corpus of\nWeb data can memorize and potentially reveal sensitive or confidential\ninformation, raising critical security concerns. Our technique fine-tunes these\nmodels, prompting them to generate harmless responses such as ``I don't know''\nwhen queried about specific information. Experimental results in a closed-book\nquestion-answering task show that our straightforward method not only minimizes\nparticular knowledge leakage but also preserves the overall performance of LLM.\nThese two advantages strengthen the defense against extraction attacks and\nreduces the emission of harmful content such as hallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishibashi_Y/0/1/0/all/0/1\">Yoichi Ishibashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11853","description":"<p>Relation triple extraction (RTE) is an essential task in information\nextraction and knowledge graph construction. Despite recent advancements,\nexisting methods still exhibit certain limitations. They just employ\ngeneralized pre-trained models and do not consider the specificity of RTE\ntasks. Moreover, existing tagging-based approaches typically decompose the RTE\ntask into two subtasks, initially identifying subjects and subsequently\nidentifying objects and relations. They solely focus on extracting relational\ntriples from subject to object, neglecting that once the extraction of a\nsubject fails, it fails in extracting all triples associated with that subject.\nTo address these issues, we propose BitCoin, an innovative Bidirectional\ntagging and supervised Contrastive learning based joint relational triple\nextraction framework. Specifically, we design a supervised contrastive learning\nmethod that considers multiple positives per anchor rather than restricting it\nto just one positive. Furthermore, a penalty term is introduced to prevent\nexcessive similarity between the subject and object. Our framework implements\ntaggers in two directions, enabling triples extraction from subject to object\nand object to subject. Experimental results show that BitCoin achieves\nstate-of-the-art results on the benchmark datasets and significantly improves\nthe F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luyao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongbao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Sen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System. (arXiv:2309.11869v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11869","description":"<p>While language is a complex adaptive system, most work on syntactic variation\nobserves a few individual constructions in isolation from the rest of the\ngrammar. This means that the grammar, a network which connects thousands of\nstructures at different levels of abstraction, is reduced to a few disconnected\nvariables. This paper quantifies the impact of such reductions by\nsystematically modelling dialectal variation across 49 local populations of\nEnglish speakers in 16 countries. We perform dialect classification with both\nan entire grammar as well as with isolated nodes within the grammar in order to\ncharacterize the syntactic differences between these dialects. The results\nshow, first, that many individual nodes within the grammar are subject to\nvariation but, in isolation, none perform as well as the grammar as a whole.\nThis indicates that an important part of syntactic variation consists of\ninteractions between different parts of the grammar. Second, the results show\nthat the similarity between dialects depends heavily on the sub-set of the\ngrammar being observed: for example, New Zealand English could be more similar\nto Australian English in phrasal verbs but at the same time more similar to UK\nEnglish in dative phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit. (arXiv:2309.11888v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11888","description":"<p>This work visits the topic of jointly parsing constituency and dependency\ntrees, i.e., to produce compatible constituency and dependency trees\nsimultaneously for input sentences, which is attractive considering that the\ntwo types of trees are complementary in representing syntax. Compared with\nprevious works, we make progress in four aspects: (1) adopting a much more\nefficient decoding algorithm, (2) exploring joint modeling at the training\nphase, instead of only at the inference phase, (3) proposing high-order scoring\ncomponents for constituent-dependency interaction, (4) gaining more insights\nvia in-depth experiments and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yanggang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xinyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])","link":"http://arxiv.org/abs/2309.11895","description":"<p>Audio classification plays a crucial role in speech and sound processing\ntasks with a wide range of applications. There still remains a challenge of\nstriking the right balance between fitting the model to the training data\n(avoiding overfitting) and enabling it to generalise well to a new domain.\nLeveraging the transferability of contrastive learning, we introduce Audio\nContrastive-based Fine-tuning (AudioConFit), an efficient approach\ncharacterised by robust generalisability. Empirical experiments on a variety of\naudio classification tasks demonstrate the effectiveness and robustness of our\napproach, which achieves state-of-the-art results in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qibin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenghao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11896","description":"<p>Although pre-trained large language models (PLMs) have achieved\nstate-of-the-art on many NLP tasks, they lack understanding of subtle\nexpressions of implicit hate speech. Such nuanced and implicit hate is often\nmisclassified as non-hate. Various attempts have been made to enhance the\ndetection of (implicit) hate content by augmenting external context or\nenforcing label separation via distance-based metrics. We combine these two\napproaches and introduce FiADD, a novel Focused Inferential Adaptive Density\nDiscrimination framework. FiADD enhances the PLM finetuning pipeline by\nbringing the surface form of an implicit hate speech closer to its implied form\nwhile increasing the inter-cluster distance among various class labels. We test\nFiADD on three implicit hate datasets and observe significant improvement in\nthe two-way and three-way hate classification tasks. We further experiment on\nthe generalizability of FiADD on three other tasks, namely detecting sarcasm,\nirony, and stance, in which surface and implied forms differ, and observe\nsimilar performance improvement. We analyze the generated latent space to\nunderstand its evolution under FiADD, which corroborates the advantage of\nemploying FiADD for implicit hate speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1\">Sarah Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajpai_A/0/1/0/all/0/1\">Ashutosh Bajpai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11911","description":"<p>The development of emotion recognition in dialogue (ERC) has been\nconsistently hindered by the complexity of pipeline designs, leading to ERC\nmodels that often overfit to specific datasets and dialogue patterns. In this\nstudy, we propose a novel approach, namely\n</p>\n<p>InstructERC, to reformulates the ERC task from a discriminative framework to\na generative framework based on Large Language Models (LLMs) . InstructERC has\ntwo significant contributions: Firstly, InstructERC introduces a simple yet\neffective retrieval template module, which helps the model explicitly integrate\nmulti-granularity dialogue supervision information by concatenating the\nhistorical dialog content, label statement, and emotional domain demonstrations\nwith high semantic similarity. Furthermore, we introduce two additional emotion\nalignment tasks, namely speaker identification and emotion prediction tasks, to\nimplicitly model the dialogue role relationships and future emotional\ntendencies in conversations. Our LLM-based plug-and-play plugin framework\nsignificantly outperforms all previous models and achieves comprehensive SOTA\non three commonly used ERC datasets. Extensive analysis of parameter-efficient\nand data-scaling experiments provide empirical guidance for applying\nInstructERC in practical scenarios. Our code will be released after blind\nreview.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shanglin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task. (arXiv:2309.11925v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11925","description":"<p>We present the joint contribution of Unbabel and Instituto Superior T\\'ecnico\nto the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated\non all tasks: sentence- and word-level quality prediction (task 1) and\nfine-grained error span detection (task 2). For all tasks, we build on the\nCOMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked\nfirst for all tasks, reaching state-of-the-art performance for quality\nestimation at word-, span- and sentence-level granularity. Compared to the\nprevious state-of-the-art COMETKIWI-22, we show large improvements in\ncorrelation with human judgements (up to 10 Spearman points). Moreover, we\nsurpass the second-best multilingual submission to the shared-task with up to\n3.8 absolute points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno M. Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pombal_J/0/1/0/all/0/1\">Jos&#xe9; Pombal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stigt_D/0/1/0/all/0/1\">Daan van Stigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_J/0/1/0/all/0/1\">Jos&#xe9; G.C. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F.T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])","link":"http://arxiv.org/abs/2309.11979","description":"<p>With the rapid development of big data and computing devices, low-latency\nautomatic trading platforms based on real-time information acquisition have\nbecome the main components of the stock trading market, so the topic of\nquantitative trading has received widespread attention. And for non-strongly\nefficient trading markets, human emotions and expectations always dominate\nmarket trends and trading decisions. Therefore, this paper starts from the\ntheory of emotion, taking East Money as an example, crawling user comment\ntitles data from its corresponding stock bar and performing data cleaning.\nSubsequently, a natural language processing model BERT was constructed, and the\nBERT model was fine-tuned using existing annotated data sets. The experimental\nresults show that the fine-tuned model has different degrees of performance\nimprovement compared to the original model and the baseline model.\nSubsequently, based on the above model, the user comment data crawled is\nlabeled with emotional polarity, and the obtained label information is combined\nwith the Alpha191 model to participate in regression, and significant\nregression results are obtained. Subsequently, the regression model is used to\npredict the average price change for the next five days, and use it as a signal\nto guide automatic trading. The experimental results show that the\nincorporation of emotional factors increased the return rate by 73.8\\% compared\nto the baseline during the trading period, and by 32.41\\% compared to the\noriginal alpha191 model. Finally, we discuss the advantages and disadvantages\nof incorporating emotional factors into quantitative trading, and give possible\ndirections for further research in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Lou_J/0/1/0/all/0/1\">Jiashu Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11981","description":"<p>In the burgeoning field of artificial intelligence (AI), the unprecedented\nprogress of large language models (LLMs) in natural language processing (NLP)\noffers an opportunity to revisit the entire approach of traditional metrics of\nmachine intelligence, both in form and content. As the realm of machine\ncognitive evaluation has already reached Imitation, the next step is an\nefficient Language Acquisition and Understanding. Our paper proposes a paradigm\nshift from the established Turing Test towards an all-embracing framework that\nhinges on language acquisition, taking inspiration from the recent advancements\nin LLMs. The present contribution is deeply tributary of the excellent work\nfrom various disciplines, point out the need to keep interdisciplinary bridges\nopen, and delineates a more robust and sustainable approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Patricio Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1\">Pedro Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1\">Lisa Barraza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])","link":"http://arxiv.org/abs/2309.11998","description":"<p>Studying how people interact with large language models (LLMs) in real-world\nscenarios is increasingly important due to their widespread use in various\napplications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset\ncontaining one million real-world conversations with 25 state-of-the-art LLMs.\nThis dataset is collected from 210K unique IP addresses in the wild on our\nVicuna demo and Chatbot Arena website. We offer an overview of the dataset's\ncontent, including its curation process, basic statistics, and topic\ndistribution, highlighting its diversity, originality, and scale. We\ndemonstrate its versatility through four use cases: developing content\nmoderation models that perform similarly to GPT-4, building a safety benchmark,\ntraining instruction-following models that perform similarly to Vicuna, and\ncreating challenging benchmark questions. We believe that this dataset will\nserve as a valuable resource for understanding and advancing LLM capabilities.\nThe dataset is publicly available at\n\\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Ying Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianle Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yonghao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric. P Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation. (arXiv:2309.12030v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12030","description":"<p>In response to the limitations of manual online ad production, significant\nresearch has been conducted in the field of automatic ad text generation (ATG).\nHowever, comparing different methods has been challenging because of the lack\nof benchmarks encompassing the entire field and the absence of well-defined\nproblem sets with clear model inputs and outputs. To address these challenges,\nthis paper aims to advance the field of ATG by introducing a redesigned task\nand constructing a benchmark. Specifically, we defined ATG as a\ncross-application task encompassing various aspects of the Internet\nadvertising. As part of our contribution, we propose a first benchmark dataset,\nCA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed\nfor ATG to be able to leverage multi-modal information and conduct an\nindustry-wise evaluation. Furthermore, we demonstrate the usefulness of our\nproposed benchmark through evaluation experiments using multiple baseline\nmodels, which vary in terms of the type of pre-trained language model used and\nthe incorporation of multi-modal information. We also discuss the current state\nof the task and the future challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mita_M/0/1/0/all/0/1\">Masato Mita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_S/0/1/0/all/0/1\">Soichiro Murakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_A/0/1/0/all/0/1\">Akihiko Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peinan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12053","description":"<p>This paper explores the imperative need and methodology for developing a\nlocalized Large Language Model (LLM) tailored for Arabic, a language with\nunique cultural characteristics that are not adequately addressed by current\nmainstream models like ChatGPT. Key concerns additionally arise when\nconsidering cultural sensitivity and local values. To this end, the paper\noutlines a packaged solution, including further pre-training with Arabic texts,\nsupervised fine-tuning (SFT) using native Arabic instructions and GPT-4\nresponses in Arabic, and reinforcement learning with AI feedback (RLAIF) using\na reward model that is sensitive to local culture and values. The objective is\nto train culturally aware and value-aligned Arabic LLMs that can serve the\ndiverse application-specific needs of Arabic-speaking communities.\n</p>\n<p>Extensive evaluations demonstrated that the resulting LLM called\n`\\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including\ninstruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval),\nknowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the\nnewly-proposed Arabic cultural \\&amp; value alignment benchmark. Notably, AceGPT\noutperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with\nGPT-4, despite the benchmark's limited scale. % Natural Language Understanding\n(NLU) benchmark (i.e., ALUE)\n</p>\n<p>Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xuening Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dingjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharthi_A/0/1/0/all/0/1\">Abdulmohsen Alharthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziche Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12056","description":"<p>This paper presents BELT, a novel model and learning framework for the\npivotal topic of brain-to-language translation research. The translation from\nnoninvasive brain signals into readable natural language has the potential to\npromote the application scenario as well as the development of brain-computer\ninterfaces (BCI) as a whole. The critical problem in brain signal decoding or\nbrain-to-language translation is the acquisition of semantically appropriate\nand discriminative EEG representation from a dataset of limited scale and\nquality. The proposed BELT method is a generic and efficient framework that\nbootstraps EEG representation learning using off-the-shelf large-scale\npretrained language models (LMs). With a large LM's capacity for understanding\nsemantic information and zero-shot generalization, BELT utilizes large LMs\ntrained on Internet-scale datasets to bring significant improvements to the\nunderstanding of EEG signals.\n</p>\n<p>In particular, the BELT model is composed of a deep conformer encoder and a\nvector quantization encoder. Semantical EEG representation is achieved by a\ncontrastive learning step that provides natural language supervision. We\nachieve state-of-the-art results on two featuring brain decoding tasks\nincluding the brain-to-language translation and zero-shot sentiment\nclassification. Specifically, our model surpasses the baseline model on both\ntasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%\nprecision on the main evaluation metrics for translation and zero-shot\nsentiment classification respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinzhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yiqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Teng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam. (arXiv:2309.12071v1 [cs.AI])","link":"http://arxiv.org/abs/2309.12071","description":"<p>Although Large Language Models (LLMs) represent a revolution in the way we\ninteract with computers, allowing the construction of complex questions and the\nability to reason over a sequence of statements, their use is restricted due to\nthe need for dedicated hardware for execution. In this study, we evaluate the\nperformance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a\nquantization process and run on home hardware. The models considered were\nAlpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we\ndeveloped a database containing 1,006 questions from the ENEM (Brazilian\nNational Secondary School Exam). Our analysis revealed that the best performing\nmodels achieved an accuracy of approximately 46% for the original texts of the\nPortuguese questions and 49% on their English translations. In addition, we\nevaluated the computational efficiency of the models by measuring the time\nrequired for execution. On average, the 7 and 13 billion LLMs took\napproximately 20 and 50 seconds, respectively, to process the queries on a\nmachine equipped with an AMD Ryzen 5 3600x processor\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Matheus L. O. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campelo_C/0/1/0/all/0/1\">Cl&#xe1;udio E. C. Campelo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12075","description":"<p>Prompt Tuning is emerging as a scalable and cost-effective method to\nfine-tune Pretrained Language Models (PLMs). This study benchmarks the\nperformance and computational efficiency of Prompt Tuning and baseline methods\non a multi-label text classification task. This is applied to the use case of\nclassifying companies into an investment firm's proprietary industry taxonomy,\nsupporting their thematic investment strategy. Text-to-text classification with\nPLMs is frequently reported to outperform classification with a classification\nhead, but has several limitations when applied to a multi-label classification\nproblem where each label consists of multiple tokens: (a) Generated labels may\nnot match any label in the industry taxonomy; (b) During fine-tuning, multiple\nlabels must be provided in an arbitrary order; (c) The model provides a binary\ndecision for each label, rather than an appropriate confidence score.\nLimitation (a) is addressed by applying constrained decoding using Trie Search,\nwhich slightly improves classification performance. All limitations (a), (b),\nand (c) are addressed by replacing the PLM's language head with a\nclassification head. This improves performance significantly, while also\nreducing computational costs during inference. The results indicate the\ncontinuing need to adapt state-of-the-art methods to domain-specific tasks,\neven in the era of PLMs with strong generalization abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchner_V/0/1/0/all/0/1\">Valentin Leonhard Buchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalo_J/0/1/0/all/0/1\">Jan-Christoph Kalo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. (arXiv:2309.12102v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12102","description":"<p>We describe SemEval-2022 Task 7, a shared task on rating the plausibility of\nclarifications in instructional texts. The dataset for this task consists of\nmanually clarified how-to guides for which we generated alternative\nclarifications and collected human plausibility judgements. The task of\nparticipating systems was to automatically determine the plausibility of a\nclarification in the respective context. In total, 21 participants took part in\nthis task, with the best system achieving an accuracy of 68.9%. This report\nsummarizes the results and findings from 8 teams and their system descriptions.\nFinally, we show in an additional evaluation that predictions by the top\nparticipating team make it possible to identify contexts with multiple\nplausible clarifications with an accuracy of 75.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1\">Michael Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthonio_T/0/1/0/all/0/1\">Talita Anthonio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1\">Anna Sauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Analysis of Vagueness in Revisions of Instructional Texts. (arXiv:2309.12107v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12107","description":"<p>WikiHow is an open-domain repository of instructional articles for a variety\nof tasks, which can be revised by users. In this paper, we extract pairwise\nversions of an instruction before and after a revision was made. Starting from\na noisy dataset of revision histories, we specifically extract and analyze\nedits that involve cases of vagueness in instructions. We further investigate\nthe ability of a neural model to distinguish between two versions of an\ninstruction in our data by adopting a pairwise ranking task from previous work\nand showing improvements over existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1\">Alok Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1\">Michael Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12109","description":"<p>In this era of large language models (LLMs), the traditional training of\nmodels has become increasingly unimaginable for regular users and institutions.\nThe exploration of efficient fine-tuning for high-resource languages on these\nmodels is an undeniable trend that is gradually gaining popularity. However,\nthere has been very little exploration for various low-resource languages, such\nas Tibetan. Research in Tibetan NLP is inherently scarce and limited. While\nthere is currently no existing large language model for Tibetan due to its\nlow-resource nature, that day will undoubtedly arrive. Therefore, research on\nefficient fine-tuning for low-resource language models like Tibetan is highly\nnecessary. Our research can serve as a reference to fill this crucial gap.\nEfficient fine-tuning strategies for pre-trained language models (PLMs) in\nTibetan have seen minimal exploration. We conducted three types of efficient\nfine-tuning experiments on the publicly available TNCC-title dataset:\n\"prompt-tuning,\" \"Adapter lightweight fine-tuning,\" and \"prompt-tuning +\nAdapter fine-tuning.\" The experimental results demonstrate significant\nimprovements using these methods, providing valuable insights for advancing\nTibetan language applications in the context of pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mingjun_Z/0/1/0/all/0/1\">Zhou Mingjun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuoma_D/0/1/0/all/0/1\">Daiqing Zhuoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuo_Q/0/1/0/all/0/1\">Qun Nuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tashi_N/0/1/0/all/0/1\">Nyima Tashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How-to Guides for Specific Audiences: A Corpus and Initial Findings. (arXiv:2309.12117v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12117","description":"<p>Instructional texts for specific target groups should ideally take into\naccount the prior knowledge and needs of the readers in order to guide them\nefficiently to their desired goals. However, targeting specific groups also\ncarries the risk of reflecting disparate social norms and subtle stereotypes.\nIn this paper, we investigate the extent to which how-to guides from one\nparticular platform, wikiHow, differ in practice depending on the intended\naudience. We conduct two case studies in which we examine qualitative features\nof texts written for specific audiences. In a generalization study, we\ninvestigate which differences can also be systematically demonstrated using\ncomputational methods. The results of our studies show that guides from\nwikiHow, like other text genres, are subject to subtle biases. We aim to raise\nawareness of these inequalities as a first step to addressing them in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fanton_N/0/1/0/all/0/1\">Nicola Fanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falenska_A/0/1/0/all/0/1\">Agnieszka Falenska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1\">Michael Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12137","description":"<p>While resources for English language are fairly sufficient to understand\ncontent on social media, similar resources in Arabic are still immature. The\nmain reason that the resources in Arabic are insufficient is that Arabic has\nmany dialects in addition to the standard version (MSA). Arabs do not use MSA\nin their daily communications; rather, they use dialectal versions.\nUnfortunately, social users transfer this phenomenon into their use of social\nmedia platforms, which in turn has raised an urgent need for building suitable\nAI models for language-dependent applications. Existing machine translation\n(MT) systems designed for MSA fail to work well with Arabic dialects. In light\nof this, it is necessary to adapt to the informal nature of communication on\nsocial networks by developing MT systems that can effectively handle the\nvarious dialects of Arabic. Unlike for MSA that shows advanced progress in MT\nsystems, little effort has been exerted to utilize Arabic dialects for MT\nsystems. While few attempts have been made to build translation datasets for\ndialectal Arabic, they are domain dependent and are not OSN cultural-language\nfriendly. In this work, we attempt to alleviate these limitations by proposing\nan online social network-based multidialect Arabic dataset that is crafted by\ncontextually translating English tweets into four Arabic dialects: Gulf,\nYemeni, Iraqi, and Levantine. To perform the translation, we followed our\nproposed guideline framework for content translation, which could be\nuniversally applicable for translation between foreign languages and local\ndialects. We validated the authenticity of our proposed dataset by developing\nneural MT models for four Arabic dialects. Our results have shown a superior\nperformance of our NMT models trained using our dataset. We believe that our\ndataset can reliably serve as an Arabic multidialectal translation dataset for\ninformal MT tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1\">Fatimah Alzamzami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1\">Abdulmotaleb El Saddik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12161","description":"<p>High-quality conversational datasets are integral to the successful\ndevelopment of Intelligent Tutoring Systems (ITS) that employ a Large Language\nModel (LLM) backend. These datasets, when used to fine-tune the LLM backend,\nsignificantly enhance the quality of interactions between students and ITS. A\ncommon strategy for developing these datasets involves generating synthetic\nstudent-teacher dialogues using advanced GPT-4 models. However, challenges\narise when these dialogues demand complex calculations, common in subjects like\nphysics. Despite its advanced capabilities, GPT-4's performance falls short in\nreliably handling even simple multiplication tasks, marking a significant\nlimitation in its utility for these subjects. To address these challenges, this\npaper introduces an innovative stateful prompt design. Our approach generates a\nmock conversation between a student and a tutorbot, both roles simulated by\nGPT-4. Each student response triggers a soliloquy (an inner monologue) in the\nGPT-tutorbot, which assesses whether its response would necessitate\ncalculations. If so, it proceeds to script the required code in Python and then\nuses the resulting output to construct its response to the student. Our\napproach notably enhances the quality of synthetic conversation datasets,\nespecially for subjects that are calculation-intensive. Our findings show that\nour Higgs model -- a LLaMA finetuned with datasets generated through our novel\nstateful prompt design -- proficiently utilizes Python for computations.\nConsequently, finetuning with our datasets enriched with code soliloquies\nenhances not just the accuracy but also the computational reliability of Higgs'\nresponses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sonkar_S/0/1/0/all/0/1\">Shashank Sonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">MyCo Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Naiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallick_D/0/1/0/all/0/1\">Debshila Basu Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches. (arXiv:2309.12224v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12224","description":"<p>The increase in the availability of online videos has transformed the way we\naccess information and knowledge. A growing number of individuals now prefer\ninstructional videos as they offer a series of step-by-step procedures to\naccomplish particular tasks. The instructional videos from the medical domain\nmay provide the best possible visual answers to first aid, medical emergency,\nand medical education questions. Toward this, this paper is focused on\nanswering health-related questions asked by the public by providing visual\nanswers from medical videos. The scarcity of large-scale datasets in the\nmedical domain is a key challenge that hinders the development of applications\nthat can help the public with their health-related questions. To address this\nissue, we first proposed a pipelined approach to create two large-scale\ndatasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal\nand multimodal approaches that can effectively provide visual answers from\nmedical videos to natural language questions. We conducted a comprehensive\nanalysis of the results, focusing on the impact of the created datasets on\nmodel training and the significance of visual features in enhancing the\nperformance of the monomodal and multi-modal approaches. Our findings suggest\nthat these datasets have the potential to enhance the performance of medical\nvisual answer localization tasks and provide a promising future direction to\nfurther enhance the performance by using pre-trained language-vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attal_K/0/1/0/all/0/1\">Kush Attal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition. (arXiv:2309.12234v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12234","description":"<p>In this study, we present synchronous bilingual Connectionist Temporal\nClassification (CTC), an innovative framework that leverages dual CTC to bridge\nthe gaps of both modality and language in the speech translation (ST) task.\nUtilizing transcript and translation as concurrent objectives for CTC, our\nmodel bridges the gap between audio and text as well as between source and\ntarget languages. Building upon the recent advances in CTC application, we\ndevelop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art\nperformances on the MuST-C ST benchmarks under resource-constrained scenarios.\nIntriguingly, our method also yields significant improvements in speech\nrecognition performance, revealing the effect of cross-lingual learning on\ntranscription and demonstrating its broad applicability. The source code is\navailable at https://github.com/xuchennlp/S2T.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_E/0/1/0/all/0/1\">Erfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_D/0/1/0/all/0/1\">Dapeng Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12244","description":"<p>Children typically learn to identify and express emotions through sharing\ntheir stories and feelings with others, particularly their family. However, it\nis challenging for parents or siblings to have emotional communication with\nchildren since children are still developing their communication skills. We\npresent ChaCha, a chatbot that encourages and guides children to share personal\nevents and associated emotions. ChaCha combines a state machine and large\nlanguage models (LLMs) to keep the dialogue on track while carrying on\nfree-form conversations. Through an exploratory study with 20 children (aged\n8-12), we examine how ChaCha prompts children to share personal events and\nguides them to describe associated emotions. Participants perceived ChaCha as a\nclose friend and shared their stories on various topics, such as family trips\nand personal achievements. Based on the quantitative and qualitative findings,\nwe discuss opportunities for leveraging LLMs to design child-friendly chatbots\nto support children in sharing their emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1\">Woosuk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chanmo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12247","description":"<p>Detecting fake news requires both a delicate sense of diverse clues and a\nprofound understanding of the real-world background, which remains challenging\nfor detectors based on small language models (SLMs) due to their knowledge and\ncapability limitations. Recent advances in large language models (LLMs) have\nshown remarkable performance in various tasks, but whether and how LLMs could\nhelp with fake news detection remains underexplored. In this paper, we\ninvestigate the potential of LLMs in fake news detection. First, we conduct an\nempirical study and find that a sophisticated LLM such as GPT 3.5 could\ngenerally expose fake news and provide desirable multi-perspective rationales\nbut still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis\nattributes such a gap to the LLM's inability to select and integrate rationales\nproperly to conclude. Based on these findings, we propose that current LLMs may\nnot substitute fine-tuned SLMs in fake news detection but can be a good advisor\nfor SLMs by providing multi-perspective instructive rationales. To instantiate\nthis proposal, we design an adaptive rationale guidance network for fake news\ndetection (ARG), in which SLMs selectively acquire insights on news analysis\nfrom the LLMs' rationales. We further derive a rationale-free version of ARG by\ndistillation, namely ARG-D, which services cost-sensitive scenarios without\ninquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and\nARG-D outperform three types of baseline methods, including SLM-based,\nLLM-based, and combinations of small and large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Beizhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuhui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12250","description":"<p>Evaluation of QA systems is very challenging and expensive, with the most\nreliable approach being human annotations of correctness of answers for\nquestions. Recent works (AVA, BEM) have shown that transformer LM encoder based\nsimilarity metrics transfer well for QA evaluation, but they are limited by the\nusage of a single correct reference answer. We propose a new evaluation metric:\nSQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference\nanswers (combining multiple correct and incorrect references) for sentence-form\nQA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and\ngenerative (GenQA) QA systems, across multiple academic and industrial\ndatasets, and show that it outperforms previous baselines and obtains the\nhighest correlation with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabburo_M/0/1/0/all/0/1\">Matteo Gabburo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedziorski_R/0/1/0/all/0/1\">Rik Koncel Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12263","description":"<p>Prompt Tuning is a popular parameter-efficient finetuning method for\npre-trained large language models (PLMs). Recently, based on experiments with\nRoBERTa, it has been suggested that Prompt Tuning activates specific neurons in\nthe transformer's feed-forward networks, that are highly predictive and\nselective for the given task. In this paper, we study the robustness of Prompt\nTuning in relation to these \"skill neurons\", using RoBERTa and T5. We show that\nprompts tuned for a specific task are transferable to tasks of the same type\nbut are not very robust to adversarial data, with higher robustness for T5 than\nRoBERTa. At the same time, we replicate the existence of skill neurons in\nRoBERTa and further show that skill neurons also seem to exist in T5.\nInterestingly, the skill neurons of T5 determined on non-adversarial data are\nalso among the most predictive neurons on the adversarial data, which is not\nthe case for RoBERTa. We conclude that higher adversarial robustness may be\nrelated to a model's ability to activate the relevant skill neurons on\nadversarial data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ackermann_L/0/1/0/all/0/1\">Leon Ackermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1\">Xenia Ohmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12269","description":"<p>We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.\nIt consists of over 250 000 court cases from the UK. Most cases are from the\n21st century, but the corpus includes cases as old as the 16th century. This\npaper presents the first release of the corpus, containing the raw text and\nmeta-data. Together with the corpus, we provide annotations on case outcomes\nfor 638 cases, done by legal experts. Using our annotated data, we have trained\nand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to\nprovide benchmarks. We include an extensive legal and ethical discussion to\naddress the potentially sensitive nature of this material. As a consequence,\nthe corpus will only be released for research purposes under certain\nrestrictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostling_A/0/1/0/all/0/1\">Andreas &#xd6;stling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargeant_H/0/1/0/all/0/1\">Holli Sargeant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Huiyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1\">Ludwig Bull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terenin_A/0/1/0/all/0/1\">Alexander Terenin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_L/0/1/0/all/0/1\">Leif Jonsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_M/0/1/0/all/0/1\">M&#xe5;ns Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steffek_F/0/1/0/all/0/1\">Felix Steffek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12273","description":"<p>Rapid and accurate identification of Venous thromboembolism (VTE), a severe\ncardiovascular condition including deep vein thrombosis (DVT) and pulmonary\nembolism (PE), is important for effective treatment. Leveraging Natural\nLanguage Processing (NLP) on radiology reports, automated methods have shown\npromising advancements in identifying VTE events from retrospective data\ncohorts or aiding clinical experts in identifying VTE events from radiology\nreports. However, effectively training Deep Learning (DL) and the NLP models is\nchallenging due to limited labeled medical text data, the complexity and\nheterogeneity of radiology reports, and data imbalance. This study proposes\nnovel method combinations of DL methods, along with data augmentation, adaptive\npre-trained NLP model selection, and a clinical expert NLP rule-based\nclassifier, to improve the accuracy of VTE identification in unstructured\n(free-text) radiology reports. Our experimental results demonstrate the model's\nefficacy, achieving an impressive 97\\% accuracy and 97\\% F1 score in predicting\nDVT, and an outstanding 98.3\\% accuracy and 98.4\\% F1 score in predicting PE.\nThese findings emphasize the model's robustness and its potential to\nsignificantly contribute to VTE research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jamie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yusen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayssen_H/0/1/0/all/0/1\">Hilary Hayssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englum_B/0/1/0/all/0/1\">Brain Englum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankaria_A/0/1/0/all/0/1\">Aman Kankaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayorga_Carlin_M/0/1/0/all/0/1\">Minerva Mayorga-Carlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Shalini Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkin_J/0/1/0/all/0/1\">John Sorkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_B/0/1/0/all/0/1\">Brajesh Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1\">Yelena Yesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12276","description":"<p>We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1\">Fernanda De La Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cathy Mengying Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banburski_Fahey_A/0/1/0/all/0/1\">Andrzej Banburski-Fahey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Judith Amores Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1\">Jaron Lanier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition. (arXiv:2309.12278v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12278","description":"<p>Large language models (LLMs) have demonstrated dominating performance in many\nNLP tasks, especially on generative tasks. However, they often fall short in\nsome information extraction tasks, particularly those requiring domain-specific\nknowledge, such as Biomedical Named Entity Recognition (NER). In this paper,\ninspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER\nstep-by-step: break down the NER task into entity span extraction and entity\ntype determination. Additionally, for entity type determination, we inject\nentity knowledge to address the problem that LLM's lack of domain knowledge\nwhen predicting entity category. Experimental results show a significant\nimprovement in our two-step BioNER approach compared to previous few-shot LLM\nbaseline. Additionally, the incorporation of external knowledge significantly\nenhances entity category determination performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shanfeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12284","description":"<p>Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (\\eg, LLaMA-2) are still far away\nfrom satisfactory for solving mathematical problem due to the complex reasoning\nprocedures. To bridge this gap, we propose \\emph{MetaMath}, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (\\ie, GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves\n$66.4\\%$ on GSM8K and $19.4\\%$ on MATH, exceeding the state-of-the-art models\nof the same size by $11.5\\%$ and $8.7\\%$. Particularly, {MetaMath-70B} achieves\nan accuracy of $82.3\\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We\nrelease the {MetaMathQA} dataset, the {MetaMath} models with different model\nsizes and the training code for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weisen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jincheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James T. Kwok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\". (arXiv:2309.12288v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12288","description":"<p>We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Olaf Scholz\nwas the ninth Chancellor of Germany\", it will not automatically be able to\nanswer the question, \"Who was the ninth Chancellor of Germany?\". Moreover, the\nlikelihood of the correct answer (\"Olaf Scholz\") will not be higher than for a\nrandom name. Thus, models exhibit a basic failure of logical deduction and do\nnot generalize a prevalent pattern in their training set (i.e. if \"A is B''\noccurs, \"B is A\" is more likely to occur). We provide evidence for the Reversal\nCurse by finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of 'Abyssal Melodies'\" and showing that they fail to\ncorrectly answer \"Who composed 'Abyssal Melodies?'\". The Reversal Curse is\nrobust across model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter. This shows a failure of logical deduction that we hypothesize is caused\nby the Reversal Curse. Code is available at\nhttps://github.com/lukasberglund/reversal_curse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berglund_L/0/1/0/all/0/1\">Lukas Berglund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meg Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1\">Max Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1\">Mikita Balesni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12294","description":"<p>Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language generation. However, their output quality can be inconsistent,\nposing challenges for generating natural language from logical forms (LFs).\nThis task requires the generated outputs to embody the exact semantics of LFs,\nwithout missing any LF semantics or creating any hallucinations. In this work,\nwe tackle this issue by proposing a novel generate-and-rerank approach. Our\napproach involves initially generating a set of candidate outputs by prompting\nan LLM and subsequently reranking them using a task-specific reranker model. In\naddition, we curate a manually collected dataset to evaluate the alignment\nbetween different ranking metrics and human judgements. The chosen ranking\nmetrics are utilized to enhance the training and evaluation of the reranker\nmodel. By conducting extensive experiments on three diverse datasets, we\ndemonstrate that the candidates selected by our reranker outperform those\nselected by baseline methods in terms of semantic consistency and fluency, as\nmeasured by three comprehensive metrics. Our findings provide strong evidence\nfor the effectiveness of our approach in improving the quality of generated\noutputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haroutunian_L/0/1/0/all/0/1\">Levon Haroutunian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galescu_L/0/1/0/all/0/1\">Lucian Galescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_P/0/1/0/all/0/1\">Philip Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumuluri_R/0/1/0/all/0/1\">Raj Tumuluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])","link":"http://arxiv.org/abs/2309.12307","description":"<p>We present LongLoRA, an efficient fine-tuning approach that extends the\ncontext sizes of pre-trained large language models (LLMs), with limited\ncomputation cost. Typically, training LLMs with long context sizes is\ncomputationally expensive, requiring extensive training hours and GPU\nresources. For example, training on the context length of 8192 needs 16x\ncomputational costs in self-attention layers as that of 2048. In this paper, we\nspeed up the context extension of LLMs in two aspects. On the one hand,\nalthough dense global attention is needed during inference, fine-tuning the\nmodel can be effectively and efficiently done by sparse local attention. The\nproposed shift short attention effectively enables context extension, leading\nto non-trivial computation saving with similar performance to fine-tuning with\nvanilla attention. Particularly, it can be implemented with only two lines of\ncode in training, while being optional in inference. On the other hand, we\nrevisit the parameter-efficient fine-tuning regime for context expansion.\nNotably, we find that LoRA for context extension works well under the premise\nof trainable embedding and normalization. LongLoRA demonstrates strong\nempirical results on various tasks on LLaMA2 models from 7B/13B to 70B.\nLongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a\nsingle 8x A100 machine. LongLoRA extends models' context while retaining their\noriginal architectures, and is compatible with most existing techniques, like\nFlashAttention-2. In addition, to make LongLoRA practical, we collect a\ndataset, LongQA, for supervised fine-tuning. It contains more than 3k long\ncontext question-answer pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengju Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])","link":"http://arxiv.org/abs/2309.12309","description":"<p>Interpersonal conflict is an uncomfortable but unavoidable fact of life.\nNavigating conflict successfully is a skill -- one that can be learned through\ndeliberate practice -- but few have access to effective training or feedback.\nTo expand this access, we introduce Rehearsal, a system that allows users to\nrehearse conflicts with a believable simulated interlocutor, explore\ncounterfactual \"what if?\" scenarios to identify alternative conversational\npaths, and learn through feedback on how and when to apply specific conflict\nstrategies. Users can utilize Rehearsal to practice handling a variety of\npredefined conflict scenarios, from office disputes to relationship issues, or\nthey can choose to create their own. To enable Rehearsal, we develop IRP\nprompting, a method of conditioning output of a large language model on the\ninfluential Interest-Rights-Power (IRP) theory from conflict resolution.\nRehearsal uses IRP to generate utterances grounded in conflict resolution\ntheory, guiding users towards counterfactual conflict resolution strategies\nthat help de-escalate difficult conversations. In a between-subjects\nevaluation, 40 participants engaged in an actual conflict with a confederate\nafter training. Compared to a control group with lecture material covering the\nsame IRP theory, participants with simulated training from Rehearsal\nsignificantly improved their performance in the unaided conflict: they reduced\ntheir use of escalating competitive strategies by an average of 67%, while\ndoubling their use of cooperative strategies. Overall, Rehearsal highlights the\npotential effectiveness of language models as tools for learning and practicing\ninterpersonal skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_V/0/1/0/all/0/1\">Valentino Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelfand_M/0/1/0/all/0/1\">Michele J. Gelfand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael S. Bernstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])","link":"http://arxiv.org/abs/2309.12311","description":"<p>3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuweiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengyi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1\">Nikhil Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_M/0/1/0/all/0/1\">Madhavan Iyengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages. (arXiv:2201.12911v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12911","description":"<p>Grammatical cues are sometimes redundant with word meanings in natural\nlanguage. For instance, English word order rules constrain the word order of a\nsentence like \"The dog chewed the bone\" even though the status of \"dog\" as\nsubject and \"bone\" as object can be inferred from world knowledge and\nplausibility. Quantifying how often this redundancy occurs, and how the level\nof redundancy varies across typologically diverse languages, can shed light on\nthe function and evolution of grammar. To that end, we performed a behavioral\nexperiment in English and Russian and a cross-linguistic computational analysis\nmeasuring the redundancy of grammatical cues in transitive clauses extracted\nfrom corpus text. English and Russian speakers (n=484) were presented with\nsubjects, verbs, and objects (in random order and with morphological markings\nremoved) extracted from naturally occurring sentences and were asked to\nidentify which noun is the subject of the action. Accuracy was high in both\nlanguages (~89% in English, ~87% in Russian). Next, we trained a neural network\nmachine classifier on a similar task: predicting which nominal in a\nsubject-verb-object triad is the subject. Across 30 languages from eight\nlanguage families, performance was consistently high: a median accuracy of 87%,\ncomparable to the accuracy observed in the human experiments. The conclusion is\nthat grammatical cues such as word order are necessary to convey subjecthood\nand objecthood in a minority of naturally occurring transitive clauses;\nnevertheless, they can (a) provide an important source of redundancy and (b)\nare crucial for conveying intended meaning that cannot be inferred from the\nwords alone, including descriptions of human interactions, where roles are\noften reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing\nnon-prototypical meanings (e.g., \"The bone chewed the dog.\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diachek_E/0/1/0/all/0/1\">Evgeniia Diachek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1\">Edward Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1\">Richard Futrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05232","description":"<p>Aspect-based sentiment analysis (ABSA) is a natural language processing\nproblem that requires analyzing user-generated reviews to determine: a) The\ntarget entity being reviewed, b) The high-level aspect to which it belongs, and\nc) The sentiment expressed toward the targets and the aspects. Numerous yet\nscattered corpora for ABSA make it difficult for researchers to identify\ncorpora best suited for a specific ABSA subtask quickly. This study aims to\npresent a database of corpora that can be used to train and assess autonomous\nABSA systems. Additionally, we provide an overview of the major corpora for\nABSA and its subtasks and highlight several features that researchers should\nconsider when selecting a corpus. Finally, we discuss the advantages and\ndisadvantages of current collection approaches and make recommendations for\nfuture corpora creation. This survey examines 65 publicly available ABSA\ndatasets covering over 25 domains, including 45 English and 20 other languages\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13379","description":"<p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)\nperformance on a gamut of complex reasoning tasks, the generated reasoning\nchain does not necessarily reflect how the model arrives at the answer (aka.\nfaithfulness). We propose Faithful CoT, a reasoning framework involving two\nstages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning\nchain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM\nand a deterministic solver respectively. This guarantees that the reasoning\nchain provides a faithful explanation of the final answer. Aside from\ninterpretability, Faithful CoT also improves empirical performance: it\noutperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a\nrelative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning,\n5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference.\nFurthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot\nperformance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong\nsynergy between faithfulness and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1\">Adam Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Delip Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2302.04456","description":"<p>In recent years, the burgeoning interest in diffusion models has led to\nsignificant advances in image and speech generation. Nevertheless, the direct\nsynthesis of music waveforms from unrestricted textual prompts remains a\nrelatively underexplored domain. In response to this lacuna, this paper\nintroduces a pioneering contribution in the form of a text-to-waveform music\ngeneration model, underpinned by the utilization of diffusion models. Our\nmethodology hinges on the innovative incorporation of free-form textual prompts\nas conditional factors to guide the waveform generation process within the\ndiffusion model framework. Addressing the challenge of limited text-music\nparallel data, we undertake the creation of a dataset by harnessing web\nresources, a task facilitated by weak supervision techniques. Furthermore, a\nrigorous empirical inquiry is undertaken to contrast the efficacy of two\ndistinct prompt formats for text conditioning, namely, music tags and\nunconstrained textual descriptions. The outcomes of this comparative analysis\naffirm the superior performance of our proposed model in terms of enhancing\ntext-music relevance. Finally, our work culminates in a demonstrative\nexhibition of the excellent capabilities of our model in text-to-music\ngeneration. We further demonstrate that our generated music in the waveform\ndomain outperforms previous works by a large margin in terms of diversity,\nquality, and text-music relevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.06908","description":"<p>Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Incumbent/Opposition Status and Ideological Similitude on Emotions in Political Manifestos. (arXiv:2305.08383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08383","description":"<p>The study involved the analysis of emotion-associated language in the UK\nConservative and Labour party general election manifestos between 2000 to 2019.\nWhile previous research have shown a general correlation between ideological\npositioning and overlap of public policies, there are still conflicting results\nin matters of sentiments in such manifestos. Using new data, we present how\nvalence level can be swayed by party status within government with incumbent\nparties presenting a higher frequency in positive emotion-associated words\nwhile negative emotion-associated words are more prevalent in opposition\nparties. We also demonstrate that parties with ideological similitude use\npositive language prominently further adding to the literature on the\nrelationship between sentiments and party status.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishi_T/0/1/0/all/0/1\">Takumi Nishi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science. (arXiv:2305.14310v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14310","description":"<p>Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Ben P. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_W/0/1/0/all/0/1\">William Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_A/0/1/0/all/0/1\">Ambrose Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can large language models generate salient negative statements?. (arXiv:2305.16755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16755","description":"<p>We examine the ability of large language models (LLMs) to generate salient\n(interesting) negative statements about real-world entities; an emerging\nresearch topic of the last few years. We probe the LLMs using zero- and k-shot\nunconstrained probes, and compare with traditional methods for negation\ngeneration, i.e., pattern-based textual extractions and knowledge-graph-based\ninferences, as well as crowdsourced gold statements. We measure the correctness\nand salience of the generated lists about subjects from different domains. Our\nevaluation shows that guided probes do in fact improve the quality of generated\nnegatives, compared to the zero-shot variant. Nevertheless, using both prompts,\nLLMs still struggle with the notion of factuality of negatives, frequently\ngenerating many ambiguous statements, or statements with negative keywords but\na positive meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.14743","description":"<p>Whisper is one of the recent state-of-the-art multilingual speech recognition\nand translation models, however, it is not designed for real time\ntranscription. In this paper, we build on top of Whisper and create\nWhisper-Streaming, an implementation of real-time speech transcription and\ntranslation of Whisper-like models. Whisper-Streaming uses local agreement\npolicy with self-adaptive latency to enable streaming transcription. We show\nthat Whisper-Streaming achieves high quality and 3.3 seconds latency on\nunsegmented long-form speech transcription test set, and we demonstrate its\nrobustness and practical usability as a component in live transcription service\nat a multilingual conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.09440","description":"<p>With easier access to powerful compute resources, there is a growing trend in\nthe field of AI for software development to develop larger and larger language\nmodels (LLMs) to address a variety of programming tasks. Even LLMs applied to\ntasks from the high-performance computing (HPC) domain are huge in size (e.g.,\nbillions of parameters) and demand expensive compute resources for training. We\nfound this design choice confusing - why do we need large LLMs trained on\nnatural languages and programming languages unrelated to HPC for HPC-specific\ntasks? In this line of work, we aim to question design choices made by existing\nLLMs by developing smaller LLMs for specific domains - we call them\ndomain-specific LLMs. Specifically, we start off with HPC as a domain and\npropose a novel tokenizer named Tokompiler, designed specifically for\npreprocessing code in HPC and compilation-centric tasks. Tokompiler leverages\nknowledge of language primitives to generate language-oriented tokens,\nproviding a context-aware understanding of code structure while avoiding human\nsemantics attributed to code structures completely. We applied Tokompiler to\npre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortran\ncode corpus mined from GitHub. We evaluate the performance of these models\nagainst the conventional LLMs. Results demonstrate that Tokompiler\nsignificantly enhances code completion accuracy and semantic understanding\ncompared to traditional tokenizers in normalized-perplexity tests, down to ~1\nperplexity score. This research opens avenues for further advancements in\ndomain-specific LLMs, catering to the unique demands of HPC and compilation\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadosh_T/0/1/0/all/0/1\">Tal Kadosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1\">Niranjan Hasabnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy A. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nadav Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krien_N/0/1/0/all/0/1\">Neva Krien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasay_A/0/1/0/all/0/1\">Abdul Wasay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1\">Nesreen Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1\">Ted Willke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_G/0/1/0/all/0/1\">Guy Tamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1\">Timothy Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10792","description":"<p>This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Runyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10855","description":"<p>With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengzuo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wuhe Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weidong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11531","description":"<p>Our project aims at helping and supporting stakeholders in refugee status\nadjudications, such as lawyers, judges, governing bodies, and claimants, in\norder to make better decisions through data-driven intelligence and increase\nthe understanding and transparency of the refugee application process for all\ninvolved parties. This PhD project has two primary objectives: (1) to retrieve\npast cases, and (2) to analyze legal decision-making processes on a dataset of\nCanadian cases. In this paper, we present the current state of our work, which\nincludes a completed experiment on part (1) and ongoing efforts related to part\n(2). We believe that NLP-based solutions are well-suited to address these\nchallenges, and we investigate the feasibility of automating all steps\ninvolved. In addition, we introduce a novel benchmark for future NLP research\nin refugee law. Our methodology aims to be inclusive to all end-users and\nstakeholders, with expected benefits including reduced time-to-decision, fairer\nand more transparent outcomes, and improved decision quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barale_C/0/1/0/all/0/1\">Claire Barale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.03992","description":"<p>Large language models (LLMs) are increasingly being used for generating text\nin a variety of use cases, including journalistic news articles. Given the\npotential malicious nature in which these LLMs can be used to generate\ndisinformation at scale, it is important to build effective detectors for such\nAI-generated text. Given the surge in development of new LLMs, acquiring\nlabeled training data for supervised detectors is a bottleneck. However, there\nmight be plenty of unlabeled text data available, without information on which\ngenerator it came from. In this work we tackle this data problem, in detecting\nAI-generated news text, and frame the problem as an unsupervised domain\nadaptation task. Here the domains are the different text generators, i.e. LLMs,\nand we assume we have access to only the labeled source data and unlabeled\ntarget data. We develop a Contrastive Domain Adaptation framework, called\nConDA, that blends standard domain adaptation techniques with the\nrepresentation power of contrastive learning to learn domain invariant\nrepresentations that are effective for the final unsupervised detection task.\nOur experiments demonstrate the effectiveness of our framework, resulting in\naverage performance gains of 31.7% from the best performing baselines, and\nwithin 0.8% margin of a fully supervised detector. All our code and data is\navailable at https://github.com/AmritaBh/ConDA-gen-text-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1\">Raha Moraffah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPPF: A contextual and post-processing-free model for automatic speech recognition. (arXiv:2309.07413v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07413","description":"<p>ASR systems have become increasingly widespread in recent years. However,\ntheir textual outputs often require post-processing tasks before they can be\npractically utilized. To address this issue, we draw inspiration from the\nmultifaceted capabilities of LLMs and Whisper, and focus on integrating\nmultiple ASR text processing tasks related to speech recognition into the ASR\nmodel. This integration not only shortens the multi-stage pipeline, but also\nprevents the propagation of cascading errors, resulting in direct generation of\npost-processed text. In this study, we focus on ASR-related processing tasks,\nincluding Contextual ASR and multiple ASR post processing tasks. To achieve\nthis objective, we introduce the CPPF model, which offers a versatile and\nhighly effective alternative to ASR processing. CPPF seamlessly integrates\nthese tasks without any significant loss in recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hongyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L1-aware Multilingual Mispronunciation Detection Framework. (arXiv:2309.07719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07719","description":"<p>The phonological discrepancies between a speaker's native (L1) and the\nnon-native language (L2) serves as a major factor for mispronunciation. This\npaper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched\nwith L1-aware speech representation. An end-to-end speech encoder is trained on\nthe input signal and its corresponding reference phoneme sequence. First, an\nattention mechanism is deployed to align the input audio with the reference\nphoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an\nauxiliary model, pretrained in a multi-task setup identifying L1 and L2\nlanguage, and are infused with the primary network. Finally, the L1-MultiMDD is\nthen optimized for a unified multilingual phoneme recognition task using\nconnectionist temporal classification (CTC) loss for the target languages:\nEnglish, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of\nthe proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and\nAraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistent\ngains in PER, and false rejection rate (FRR) across all target languages\nconfirm our approach's robustness, efficacy, and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheir_Y/0/1/0/all/0/1\">Yassine El Kheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.10916","description":"<p>Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n</p>\n<p>In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1\">Shakila Mahjabin Tonni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11049","description":"<p>Question answering on tabular data (a.k.a TableQA), which aims at generating\nanswers to questions grounded on a provided table, has gained significant\nattention recently. Prior work primarily produces concise factual responses\nthrough information extraction from individual or limited table cells, lacking\nthe ability to reason across diverse table cells. Yet, the realm of free-form\nTableQA, which demands intricate strategies for selecting relevant table cells\nand the sophisticated integration and inference of discrete data fragments,\nremains mostly unexplored. To this end, this paper proposes a generalized\nthree-stage approach: Table-to- Graph conversion and cell localizing, external\nknowledge retrieval, and the fusion of table and text (called TAG-QA), to\naddress the challenge of inferring long free-form answers in generative\nTableQA. In particular, TAG-QA (1) locates relevant table cells using a graph\nneural network to gather intersecting cells between relevant rows and columns,\n(2) leverages external knowledge from Wikipedia, and (3) generates answers by\nintegrating both tabular data and natural linguistic information. Experiments\nshowcase the superior capabilities of TAG-QA in generating sentences that are\nboth faithful and coherent, particularly when compared to several\nstate-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based\nbaseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score,\nrespectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16%\nand 12% on BLEU-4 and PARENT F-score, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11052","description":"<p>The proliferation of fake news has become a significant concern in recent\ntimes due to its potential to spread misinformation and manipulate public\nopinion. This paper presents a comprehensive study on detecting fake news in\nBrazilian Portuguese, focusing on journalistic-type news. We propose a machine\nlearning-based approach that leverages natural language processing techniques,\nincluding TF-IDF and Word2Vec, to extract features from textual data. We\nevaluate the performance of various classification algorithms, such as logistic\nregression, support vector machine, random forest, AdaBoost, and LightGBM, on a\ndataset containing both true and fake news articles. The proposed approach\nachieves high accuracy and F1-Score, demonstrating its effectiveness in\nidentifying fake news. Additionally, we developed a user-friendly web platform,\nfakenewsbr.com, to facilitate the verification of news articles' veracity. Our\nplatform provides real-time analysis, allowing users to assess the likelihood\nof fake news articles. Through empirical analysis and comparative studies, we\ndemonstrate the potential of our approach to contribute to the fight against\nthe spread of fake news and promote more informed media consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giordani_L/0/1/0/all/0/1\">Luiz Giordani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daru_G/0/1/0/all/0/1\">Gilsiley Dar&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1\">Rhenan Queiroz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzinaro_V/0/1/0/all/0/1\">Vitor Buzinaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiva_D/0/1/0/all/0/1\">Davi Keglevich Neiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_D/0/1/0/all/0/1\">Daniel Camilo Fuentes Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_M/0/1/0/all/0/1\">Marcos Jardel Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1\">Oilson Alberto Gonzatto Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louzada_F/0/1/0/all/0/1\">Francisco Louzada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11206","description":"<p>Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1\">Anhuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11436","description":"<p>Autonomous user interface (UI) agents aim to facilitate task automation by\ninteracting with the user interface without manual intervention. Recent studies\nhave investigated eliciting the capabilities of large language models (LLMs)\nfor effective engagement in diverse environments. To align with the\ninput-output requirement of LLMs, existing approaches are developed under a\nsandbox setting where they rely on external tools and application-specific APIs\nto parse the environment into textual elements and interpret the predicted\nactions. Consequently, those approaches often grapple with inference\ninefficiency and error propagation risks. To mitigate the challenges, we\nintroduce Auto-UI, a multimodal solution that directly interacts with the\ninterface, bypassing the need for environment parsing or reliance on\napplication-dependent APIs. Moreover, we propose a chain-of-action technique --\nleveraging a series of intermediate previous action histories and future action\nplans -- to help the agent decide what action to execute. We evaluate our\napproach on a new device-control benchmark AITW with 30K unique instructions,\nspanning multi-step tasks such as application operation, web searching, and web\nshopping. Experimental results show that Auto-UI achieves state-of-the-art\nperformance with an action type prediction accuracy of 90% and an overall\naction success rate of 74%. Code is publicly available at\nhttps://github.com/cooelf/Auto-UI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.11489","description":"<p>Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation of dense reward functions\nbased on large language models (LLMs). Given a goal described in natural\nlanguage, Text2Reward generates dense reward functions as an executable program\ngrounded in a compact representation of the environment. Unlike inverse RL and\nrecent work that uses LLMs to write sparse reward codes, Text2Reward produces\ninterpretable, free-form dense reward codes that cover a wide range of tasks,\nutilize existing packages, and allow iterative refinement with human feedback.\nWe evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,\nMetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17\nmanipulation tasks, policies trained with generated reward codes achieve\nsimilar or better task success rates and convergence speed than expert-written\nreward codes. For locomotion tasks, our method learns six novel locomotion\nbehaviors with a success rate exceeding 94%. Furthermore, we show that the\npolicies trained in the simulator with our method can be deployed in the real\nworld. Finally, Text2Reward further improves the policies by refining their\nreward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}