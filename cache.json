{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01209","description":"<p>Unsupervised Relation Extraction (RE) aims to identify relations between\nentities in text, without having access to labeled data during training. This\nsetting is particularly relevant for domain specific RE where no annotated\ndataset is available and for open-domain RE where the types of relations are a\npriori unknown. Although recent approaches achieve promising results, they\nheavily depend on hyperparameters whose tuning would most often require labeled\ndata. To mitigate the reliance on hyperparameters, we propose PromptORE, a\n''Prompt-based Open Relation Extraction'' model. We adapt the novel\nprompt-tuning paradigm to work in an unsupervised setting, and use it to embed\nsentences expressing a relation. We then cluster these embeddings to discover\ncandidate relations, and we experiment different strategies to automatically\nestimate an adequate number of clusters. To the best of our knowledge,\nPromptORE is the first unsupervised RE model that does not need hyperparameter\ntuning. Results on three general and specific domain datasets show that\nPromptORE consistently outperforms state-of-the-art models with a relative gain\nof more than 40% in B 3 , V-measure and ARI. Qualitative analysis also\nindicates PromptORE's ability to identify semantically coherent clusters that\nare very close to true relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Genest_P/0/1/0/all/0/1\">Pierre-Yves Genest</a> (Alteca, DRIM), <a href=\"http://arxiv.org/find/cs/1/au:+Portier_P/0/1/0/all/0/1\">Pierre-Edouard Portier</a> (DRIM), <a href=\"http://arxiv.org/find/cs/1/au:+Egyed_Zsigmond_E/0/1/0/all/0/1\">El&#xf6;d Egyed-Zsigmond</a> (DRIM), <a href=\"http://arxiv.org/find/cs/1/au:+Goix_L/0/1/0/all/0/1\">Laurent-Walter Goix</a> (Alteca)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01228","description":"<p>Pre-trained language models for code (PLMCs) have gained attention in recent\nresearch. These models are pre-trained on large-scale datasets using\nmulti-modal objectives. However, fine-tuning them requires extensive\nsupervision and is limited by the size of the dataset provided. We aim to\nimprove this issue by proposing a simple data augmentation framework. Our\nframework utilizes knowledge gained during the pre-training and fine-tuning\nstage to generate pseudo data, which is then used as training data for the next\nstep. We incorporate this framework into the state-of-the-art language models,\nsuch as CodeT5, CodeBERT, and UnixCoder. The results show that our framework\nsignificantly improves PLMCs' performance in code-related sequence generation\ntasks, such as code summarization and code generation in the CodeXGLUE\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1\">Hung Quoc To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tien N. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01233","description":"<p>Language modeling have shown impressive progress in generating compelling\ntext with good accuracy and high semantic coherence. An interesting research\ndirection is to augment these powerful models for specific applications using\ncontextual information. In this work, we explore multi-modal language modeling\nfor healthcare applications. We are interested in outcome prediction and\npatient triage in hospital emergency department based on text information in\nchief complaints and vital signs recorded at triage. We adapt Perceiver - a\nmodality-agnostic transformer-based model that has shown promising results in\nseveral applications. Since vital-sign modality is represented in tabular\nformat, we modified Perceiver position encoding to ensure permutation\ninvariance. We evaluated the multi-modal language model for the task of\ndiagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the\nexperimental analysis, we show that mutli-modality improves the prediction\nperformance compared with models trained solely on text or vital signs. We\nidentified disease categories for which multi-modality leads to performance\nimprovement and show that for these categories, vital signs have added\npredictive power. By analyzing the cross-attention layer, we show how\nmulti-modality contributes to model predictions. This work gives interesting\ninsights on the development of multi-modal language models for healthcare\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boughorbel_S/0/1/0/all/0/1\">Sabri Boughorbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarray_F/0/1/0/all/0/1\">Fethi Jarray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homaid_A/0/1/0/all/0/1\">Abdulaziz Al Homaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niaz_R/0/1/0/all/0/1\">Rashid Niaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafei_K/0/1/0/all/0/1\">Khalid Alyafei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01238","description":"<p>This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labonne_M/0/1/0/all/0/1\">Maxime Labonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Sean Moran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01240","description":"<p>Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_J/0/1/0/all/0/1\">Jaya Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velupillai_S/0/1/0/all/0/1\">Sumithra Velupillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_R/0/1/0/all/0/1\">Robert Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Angus Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01241","description":"<p>The increase in abusive content on online social media platforms is impacting\nthe social life of online users. Use of offensive and hate speech has been\nmaking so-cial media toxic. Homophobia and transphobia constitute offensive\ncomments against LGBT+ community. It becomes imperative to detect and handle\nthese comments, to timely flag or issue a warning to users indulging in such\nbehaviour. However, automated detection of such content is a challenging task,\nmore so in Dravidian languages which are identified as low resource languages.\nMotivated by this, the paper attempts to explore applicability of different\ndeep learning mod-els for classification of the social media comments in\nMalayalam and Tamil lan-guages as homophobic, transphobic and\nnon-anti-LGBT+content. The popularly used deep learning models- Convolutional\nNeural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and\ntransformer-based learning models (Multilingual BERT and IndicBERT) are applied\nto the classification problem. Results obtained show that IndicBERT outperforms\nthe other imple-mented models, with obtained weighted average F1-score of 0.86\nand 0.77 for Malayalam and Tamil, respectively. Therefore, the present work\nconfirms higher performance of IndicBERT on the given task in selected\nDravidian languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Deepawali Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vedika Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vivek Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs. (arXiv:2304.01242v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01242","description":"<p>Clinical evidence encompasses the associations and impacts between patients,\ninterventions (such as drugs or physiotherapy), problems, and outcomes. The\ngoal of recommending clinical evidence is to provide medical practitioners with\nrelevant information to support their decision-making processes and to generate\nnew evidence. Our specific task focuses on recommending evidence based on\nclinical problems. However, the direct connections between certain clinical\nproblems and related evidence are often sparse, creating a challenge of link\nsparsity. Additionally, to recommend appropriate evidence, it is essential to\njointly exploit both topological relationships among evidence and textual\ninformation describing them. To address these challenges, we define two\nknowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to\nrepresent the topological and linguistic relations among evidential elements,\nrespectively. We also introduce a multi-channel heterogeneous learning model\nand a fusional attention mechanism to handle the co-reference-text\nheterogeneity in evidence recommendation. Our experiments demonstrate that our\nmodel outperforms state-of-the-art methods on open data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Maolin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01246","description":"<p>Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI\nheatwave due to its human-like conversations with detailed and articulate\nanswers across many domains of knowledge. While LLMs are being quickly applied\nto many AI application domains, we are interested in the following question:\nCan safety analysis for safety-critical systems make use of LLMs? To answer, we\nconduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic\nEmergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent\ntechniques for hazard analysis, is known to have limitations such as high\ncomplexity and subjectivity, which this paper aims to explore the use of\nChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA\nare investigated by considering its interaction with human experts: one-off\nsimplex interaction, recurring simplex interaction, and recurring duplex\ninteraction. Comparative results reveal that: (i) using ChatGPT without human\nexperts' intervention can be inadequate due to reliability and accuracy issues\nof LLMs; (ii) more interactions between ChatGPT and human experts may yield\nbetter results; and (iii) using ChatGPT in STPA with extra care can outperform\nhuman safety experts alone, as demonstrated by reusing an existing comparison\nmethod with baselines. In addition to making the first attempt to apply LLMs in\nsafety analysis, this paper also identifies key challenges (e.g.,\ntrustworthiness concern of LLMs, the need of standardisation) for future\nresearch in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for Translation with Semi-Supervised Pseudo-Parallel Document Generation. (arXiv:2304.01282v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01282","description":"<p>Multilingual pre-training significantly improves many multilingual NLP tasks,\nincluding machine translation. Most existing methods are based on some variants\nof masked language modeling and text-denoising objectives on monolingual data.\nMultilingual pre-training on monolingual data ignores the availability of\nparallel data in many language pairs. Also, some other works integrate the\navailable human-generated parallel translation data in their pre-training. This\nkind of parallel data is definitely helpful, but it is limited even in\nhigh-resource language pairs. This paper introduces a novel semi-supervised\nmethod, SPDG, that generates high-quality pseudo-parallel data for multilingual\npre-training. First, a denoising model is pre-trained on monolingual data to\nreorder, add, remove, and substitute words, enhancing the pre-training\ndocuments' quality. Then, we generate different pseudo-translations for each\npre-training document using dictionaries for word-by-word translation and\napplying the pre-trained denoising model. The resulting pseudo-parallel data is\nthen used to pre-train our multilingual sequence-to-sequence model, PEACH. Our\nexperiments show that PEACH outperforms existing approaches used in training\nmT5 and mBART on various translation tasks, including supervised, zero- and\nfew-shot scenarios. Moreover, PEACH's ability to transfer knowledge between\nsimilar languages makes it particularly useful for low-resource languages. Our\nresults demonstrate that with high-quality dictionaries for generating accurate\npseudo-parallel, PEACH can be valuable for low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1\">Alireza Salemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_S/0/1/0/all/0/1\">Sara Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakery_A/0/1/0/all/0/1\">Azadeh Shakery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01295","description":"<p>Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD, a parallel and large-scale\nmultilingual conversation dataset that we created by translating the\nEnglish-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into\n105 other languages. XSGD contains approximately 330k utterances per language.\nTo facilitate aligned cross-lingual representations, we develop an efficient\nprompt-tuning-based method for learning alignment prompts. We also investigate\ntwo different classifiers: NLI-based and vanilla classifiers, and test\ncross-lingual capability enabled by the aligned prompts. We evaluate our\nmodel's cross-lingual generalization capabilities on two conversation tasks:\nslot-filling and intent classification. Our results demonstrate the strong and\nefficient modeling ability of NLI-based classifiers and the large cross-lingual\ntransfer improvements achieved by our aligned prompts, particularly in few-shot\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki. (arXiv:2304.01319v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01319","description":"<p>One of the major challenges that under-represented and endangered language\ncommunities face in language technology is the lack or paucity of language\ndata. This is also the case of the Southern varieties of the Kurdish and Laki\nlanguages for which very limited resources are available with insubstantial\nprogress in tools. To tackle this, we provide a few approaches that rely on the\ncontent of local news websites, a local radio station that broadcasts content\nin Southern Kurdish and fieldwork for Laki. In this paper, we describe some of\nthe challenges of such under-represented languages, particularly in writing and\nstandardization, and also, in retrieving sources of data and retro-digitizing\nhandwritten content to create a corpus for Southern Kurdish and Laki. In\naddition, we study the task of language identification in light of the other\nvariants of Kurdish and Zaza-Gorani languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azin_Z/0/1/0/all/0/1\">Zahra Azin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belelli_S/0/1/0/all/0/1\">Sara Belelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALI: A Language Identification Benchmark for Perso-Arabic Scripts. (arXiv:2304.01322v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01322","description":"<p>The Perso-Arabic scripts are a family of scripts that are widely adopted and\nused by various linguistic communities around the globe. Identifying various\nlanguages using such scripts is crucial to language technologies and\nchallenging in low-resource setups. As such, this paper sheds light on the\nchallenges of detecting languages using Perso-Arabic scripts, especially in\nbilingual communities where ``unconventional'' writing is practiced. To address\nthis, we use a set of supervised techniques to classify sentences into their\nlanguages. Building on these, we also propose a hierarchical model that targets\nclusters of languages that are more often confused by the classifiers. Our\nexperiment results indicate the effectiveness of our solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Milind Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grand Challenge On Detecting Cheapfakes. (arXiv:2304.01328v1 [cs.CV])","link":"http://arxiv.org/abs/2304.01328","description":"<p>Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1\">Duc-Tien Dang-Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sohail Ahmed Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1\">Cise Midoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1\">Minh-Son Dao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Document Similarity Algorithms. (arXiv:2304.01330v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01330","description":"<p>Document similarity is an important part of Natural Language Processing and\nis most commonly used for plagiarism-detection and text summarization. Thus,\nfinding the overall most effective document similarity algorithm could have a\nmajor positive impact on the field of Natural Language Processing. This report\nsets out to examine the numerous document similarity algorithms, and determine\nwhich ones are the most useful. It addresses the most effective document\nsimilarity algorithm by categorizing them into 3 types of document similarity\nalgorithms: statistical algorithms, neural networks, and corpus/knowledge-based\nalgorithms. The most effective algorithms in each category are also compared in\nour work using a series of benchmark datasets and evaluations that test every\npossible area that each algorithm could be used in.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gahman_N/0/1/0/all/0/1\">Nicholas Gahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_V/0/1/0/all/0/1\">Vinayak Elangovan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks. (arXiv:2304.01331v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01331","description":"<p>Event data, or structured records of ``who did what to whom'' that are\nautomatically extracted from text, is an important source of data for scholars\nof international politics. The high cost of developing new event datasets,\nespecially using automated systems that rely on hand-built dictionaries, means\nthat most researchers draw on large, pre-existing datasets such as ICEWS rather\nthan developing tailor-made event datasets optimized for their specific\nresearch question. This paper describes a ``bag of tricks'' for efficient,\ncustom event data production, drawing on recent advances in natural language\nprocessing (NLP) that allow researchers to rapidly produce customized event\ndatasets. The paper introduces techniques for training an event category\nclassifier with active learning, identifying actors and the recipients of\nactions in text using large language models and standard machine learning\nclassifiers and pretrained ``question-answering'' models from NLP, and\nresolving mentions of actors to their Wikipedia article to categorize them. We\ndescribe how these techniques produced the new POLECAT global event dataset\nthat is intended to replace ICEWS, along with examples of how scholars can\nquickly produce smaller, custom event datasets. We publish example code and\nmodels to implement our new techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schrodt_P/0/1/0/all/0/1\">Philip A. Schrodt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beger_A/0/1/0/all/0/1\">Andreas Beger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagozzi_B/0/1/0/all/0/1\">Benjamin E. Bagozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarborough_G/0/1/0/all/0/1\">Grace I. Scarborough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies. (arXiv:2304.01344v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01344","description":"<p>End-to-end relation extraction (E2ERE) is an important task in information\nextraction, more so for biomedicine as scientific literature continues to grow\nexponentially. E2ERE typically involves identifying entities (or named entity\nrecognition (NER)) and associated relations, while most RE tasks simply assume\nthat the entities are provided upfront and end up performing relation\nclassification. E2ERE is inherently more difficult than RE alone given the\npotential snowball effect of errors from NER leading to more errors in RE. A\ncomplex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI,\n2017) that identifies relations between chemical compounds and genes/proteins\nin scientific literature. ChemProt is included in all recent biomedical natural\nlanguage processing benchmarks including BLUE, BLURB, and BigBio. However, its\ntreatment in these benchmarks and in other separate efforts is typically not\nend-to-end, with few exceptions. In this effort, we employ a span-based\npipeline approach to produce a new state-of-the-art E2ERE performance on the\nChemProt dataset, resulting in $&gt; 4\\%$ improvement in F1-score over the prior\nbest effort. Our results indicate that a straightforward fine-grained\ntokenization scheme helps span-based approaches excel in E2ERE, especially with\nregards to handling complex named entities. Our error analysis also identifies\na few key failure modes in E2ERE for ChemProt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1\">Xuguang Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01352","description":"<p>We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avetisyan_K/0/1/0/all/0/1\">Karen Avetisyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malajyan_A/0/1/0/all/0/1\">Arthur Malajyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghukasyan_T/0/1/0/all/0/1\">Tsolak Ghukasyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01373","description":"<p>How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\nhttps://github.com/EleutherAI/pythia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1\">Quentin Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1\">Herbie Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_K/0/1/0/all/0/1\">Kyle O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallahan_E/0/1/0/all/0/1\">Eric Hallahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prashanth_U/0/1/0/all/0/1\">USVSN Sai Prashanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skowron_A/0/1/0/all/0/1\">Aviya Skowron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01412","description":"<p>We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation\nturns between agents working at Statistics Canada and online users looking for\npublished data tables. The conversations stem from genuine intents, are held in\nEnglish or French, and lead to agents retrieving one of over 5000 complex data\ntables. Based on this dataset, we propose two tasks: (1) automatic retrieval of\nrelevant tables based on a on-going conversation, and (2) automatic generation\nof appropriate agent responses at each turn. We investigate the difficulty of\neach task by establishing strong baselines. Our experiments on a temporal data\nsplit reveal that all models struggle to generalize to future conversations, as\nwe observe a significant drop in performance across both tasks when we move\nfrom the validation to the test set. In addition, we find that response\ngeneration models struggle to decide when to return a table. Considering that\nthe tasks pose significant challenges to existing models, we encourage the\ncommunity to develop models for our task, which can be directly used to help\nknowledge workers find relevant tables for live chat users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xing Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thematic context vector association based on event uncertainty for Twitter. (arXiv:2304.01423v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01423","description":"<p>Keyword extraction is a crucial process in text mining. The extraction of\nkeywords with respective contextual events in Twitter data is a big challenge.\nThe challenging issues are mainly because of the informality in the language\nused. The use of misspelled words, acronyms, and ambiguous terms causes\ninformality. The extraction of keywords with informal language in current\nsystems is pattern based or event based. In this paper, contextual keywords are\nextracted using thematic events with the help of data association. The thematic\ncontext for events is identified using the uncertainty principle in the\nproposed system. The thematic contexts are weighed with the help of vectors\ncalled thematic context vectors which signifies the event as certain or\nuncertain. The system is tested on the Twitter COVID-19 dataset and proves to\nbe effective. The system extracts event-specific thematic context vectors from\nthe test dataset and ranks them. The extracted thematic context vectors are\nused for the clustering of contextual thematic vectors which improves the\nsilhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF.\nThe thematic context vector can be used in other applications like\nCyberbullying, sarcasm detection, figurative language detection, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khatavkar_V/0/1/0/all/0/1\">Vaibhav Khatavkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mane_S/0/1/0/all/0/1\">Swapnil Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Parag Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarity based Sarcasm Detection using Semigraph. (arXiv:2304.01424v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01424","description":"<p>Sarcasm is an advanced linguistic expression often found on various online\nplatforms. Sarcasm detection is challenging in natural language processing\ntasks that affect sentiment analysis. This article presents the inventive\nmethod of the semigraph, including semigraph construction and sarcasm detection\nprocesses. A variation of the semigraph is suggested in the pattern-relatedness\nof the text document. The proposed method is to obtain the sarcastic and\nnon-sarcastic polarity scores of a document using a semigraph. The sarcastic\npolarity score represents the possibility that a document will become\nsarcastic. Sarcasm is detected based on the polarity scoring model. The\nperformance of the proposed model enhances the existing prior art approach to\nsarcasm detection. In the Amazon product review, the model achieved the\naccuracy, recall, and f-measure of 0.87, 0.79, and 0.83, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mane_S/0/1/0/all/0/1\">Swapnil Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatavkar_V/0/1/0/all/0/1\">Vaibhav Khatavkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Vector Grounding Problem. (arXiv:2304.01481v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01481","description":"<p>The remarkable performance of large language models (LLMs) on complex\nlinguistic tasks has sparked a lively debate on the nature of their\ncapabilities. Unlike humans, these models learn language exclusively from\ntextual data, without direct interaction with the real world. Nevertheless,\nthey can generate seemingly meaningful text about a wide range of topics. This\nimpressive accomplishment has rekindled interest in the classical 'Symbol\nGrounding Problem,' which questioned whether the internal representations and\noutputs of classical symbolic AI systems could possess intrinsic meaning.\nUnlike these systems, modern LLMs are artificial neural networks that compute\nover vectors rather than symbols. However, an analogous problem arises for such\nsystems, which we dub the Vector Grounding Problem. This paper has two primary\nobjectives. First, we differentiate various ways in which internal\nrepresentations can be grounded in biological or artificial systems,\nidentifying five distinct notions discussed in the literature: referential,\nsensorimotor, relational, communicative, and epistemic grounding.\nUnfortunately, these notions of grounding are often conflated. We clarify the\ndifferences between them, and argue that referential grounding is the one that\nlies at the heart of the Vector Grounding Problem. Second, drawing on theories\nof representational content in philosophy and cognitive science, we propose\nthat certain LLMs, particularly those fine-tuned with Reinforcement Learning\nfrom Human Feedback (RLHF), possess the necessary features to overcome the\nVector Grounding Problem, as they stand in the requisite causal-historical\nrelations to the world that underpin intrinsic meaning. We also argue that,\nperhaps unexpectedly, multimodality and embodiment are neither necessary nor\nsufficient conditions for referential grounding in artificial systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mollo_D/0/1/0/all/0/1\">Dimitri Coelho Mollo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milliere_R/0/1/0/all/0/1\">Rapha&#xeb;l Milli&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01483","description":"<p>Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have\nrecently attracted increasing interest, research enthusiasm, and business\ndemand. However, their massive computation resources and huge memory footprint\nare inevitable challenges. To tackle this issue, we propose BCT, a framework of\nblockwise compression for transformers without retraining, to lower deployment\nthresholds. BCT achieves more fine-grained compression of the whole\ntransformer, including embedding, matrix multiplication, GELU, Softmax, layer\nnormalization, and all the intermediate results. As a case, we compress an\nefficient model with BCT and evaluate it on several General Language\nUnderstanding Evaluation (GLUE) datasets. The results show that BCT can achieve\na less than 0.90% accuracy drop in most tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Gaochen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])","link":"http://arxiv.org/abs/2304.01487","description":"<p>ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&amp;A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_A/0/1/0/all/0/1\">Alessandro Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_K/0/1/0/all/0/1\">Kavita Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fereidooni_H/0/1/0/all/0/1\">Hossein Fereidooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_A/0/1/0/all/0/1\">Ahmad-Reza Sadeghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01492","description":"<p>The truth is significantly hampered by massive rumors that spread along with\nbreaking news or popular topics. Since there is sufficient corpus gathered from\nthe same domain for model training, existing rumor detection algorithms show\npromising performance on yesterday's news. However, due to a lack of training\ndata and prior expert knowledge, they are poor at spotting rumors concerning\nunforeseen events, especially those propagated in different languages (i.e.,\nlow-resource regimes). In this paper, we propose a unified contrastive transfer\nframework to detect rumors by adapting the features learned from well-resourced\nrumor data to that of the low-resourced. More specifically, we first represent\nrumor circulated on social media as an undirected topology, and then train a\nMulti-scale Graph Convolutional Network via a unified contrastive paradigm. Our\nmodel explicitly breaks the barriers of the domain and/or language issues, via\nlanguage alignment and a novel domain-adaptive contrastive learning mechanism.\nTo enhance the representation learning from a small set of target events, we\nreveal that rumor-indicative signal is closely correlated with the uniformity\nof the distribution of these events. We design a target-wise contrastive\ntraining mechanism with three data augmentation strategies, capable of unifying\nthe representations by distinguishing target events. Extensive experiments\nconducted on four low-resource datasets collected from real-world microblog\nplatforms demonstrate that our framework achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models. (arXiv:2304.01515v1 [cs.LG])","link":"http://arxiv.org/abs/2304.01515","description":"<p>Token-based masked generative models are gaining popularity for their fast\ninference time with parallel decoding. While recent token-based approaches\nachieve competitive performance to diffusion-based models, their generation\nperformance is still suboptimal as they sample multiple tokens simultaneously\nwithout considering the dependence among them. We empirically investigate this\nproblem and propose a learnable sampling model, Text-Conditioned Token\nSelection (TCTS), to select optimal tokens via localized supervision with text\ninformation. TCTS improves not only the image quality but also the semantic\nalignment of the generated images with the given texts. To further improve the\nimage quality, we introduce a cohesive sampling strategy, Frequency Adaptive\nSampling (FAS), to each group of tokens divided according to the self-attention\nmaps. We validate the efficacy of TCTS combined with FAS with various\ngenerative tasks, demonstrating that it significantly outperforms the baselines\nin image-text alignment and image quality. Our text-conditioned sampling\nframework further reduces the original inference time by more than 50% without\nmodifying the original generative model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewoong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sangwon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaehyeong Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01563","description":"<p>The multi-modal entity alignment (MMEA) aims to find all equivalent entity\npairs between multi-modal knowledge graphs (MMKGs). Rich attributes and\nneighboring entities are valuable for the alignment task, but existing works\nignore contextual gap problems that the aligned entities have different numbers\nof attributes on specific modality when learning entity representations. In\nthis paper, we propose a novel attribute-consistent knowledge graph\nrepresentation learning framework for MMEA (ACK-MMEA) to compensate the\ncontextual gaps through incorporating consistent alignment knowledge.\nAttribute-consistent KGs (ACKGs) are first constructed via multi-modal\nattribute uniformization with merge and generate operators so that each entity\nhas one and only one uniform feature in each modality. The ACKGs are then fed\ninto a relation-aware graph neural network with random dropouts, to obtain\naggregated relation representations and robust entity representations. In order\nto evaluate the ACK-MMEA facilitated for entity alignment, we specially design\na joint alignment loss for both entity and attribute evaluation. Extensive\nexperiments conducted on two benchmark datasets show that our approach achieves\nexcellent performance compared to its competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yangyifei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Cheng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01597","description":"<p>Masked language modeling (MLM) plays a key role in pretraining large language\nmodels. But the MLM objective is often dominated by high-frequency words that\nare sub-optimal for learning factual knowledge. In this work, we propose an\napproach for influencing MLM pretraining in a way that can improve language\nmodel performance on a variety of knowledge-intensive tasks. We force the\nlanguage model to prioritize informative words in a fully unsupervised way.\nExperiments demonstrate that the proposed approach can significantly improve\nthe performance of pretrained language models on tasks such as factual recall,\nquestion answering, sentiment analysis, and natural language inference in a\nclosed-book setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadeq_N/0/1/0/all/0/1\">Nafis Sadeq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamba_P/0/1/0/all/0/1\">Prarit Lamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDeR: A Dataset for Exploring Dependency Relations Between Events. (arXiv:2304.01612v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01612","description":"<p>Relation extraction is a central task in natural language processing (NLP)\nand information retrieval (IR) research. We argue that an important type of\nrelation not explored in NLP or IR research to date is that of an event being\nan argument - required or optional - of another event. We introduce the\nhuman-annotated Event Dependency Relation dataset (EDeR) which provides this\ndependency relation. The annotation is done on a sample of documents from the\nOntoNotes dataset, which has the added benefit that it integrates with\nexisting, orthogonal, annotations of this dataset. We investigate baseline\napproaches for predicting the event dependency relation, the best of which\nachieves an accuracy of 82.61 for binary argument/non-argument classification.\nWe show that recognizing this relation leads to more accurate event extraction\n(semantic role labelling) and can improve downstream tasks that depend on this,\nsuch as co-reference resolution. Furthermore, we demonstrate that predicting\nthe three-way classification into the required argument, optional argument or\nnon-argument is a more challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslum_P/0/1/0/all/0/1\">Patrik Haslum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01621","description":"<p>Cross-lingual science journalism generates popular science stories of\nscientific articles different from the source language for a non-expert\naudience. Hence, a cross-lingual popular summary must contain the salient\ncontent of the input document, and the content should be coherent,\ncomprehensible, and in a local language for the targeted audience. We improve\nthese aspects of cross-lingual summary generation by joint training of two\nhigh-level NLP tasks, simplification and cross-lingual summarization. The\nformer task reduces linguistic complexity, and the latter focuses on\ncross-lingual abstractive summarization. We propose a novel multi-task\narchitecture - SimCSum consisting of one shared encoder and two parallel\ndecoders jointly learning simplification and cross-lingual summarization. We\nempirically investigate the performance of SimCSum by comparing it with several\nstrong baselines over several evaluation metrics and by human evaluation.\nOverall, SimCSum demonstrates statistically significant improvements over the\nstate-of-the-art on two non-synthetic cross-lingual scientific datasets.\nFurthermore, we conduct an in-depth investigation into the linguistic\nproperties of generated summaries and an error analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fatima_M/0/1/0/all/0/1\">Mehwish Fatima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolber_T/0/1/0/all/0/1\">Tim Kolber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Katja Markert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1\">Michael Strube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01622","description":"<p>Similar Case Matching (SCM) is designed to determine whether two cases are\nsimilar. The task has an essential role in the legal system, helping legal\nprofessionals to find relevant cases quickly and thus deal with them more\nefficiently. Existing research has focused on improving the model's performance\nbut not on its interpretability. Therefore, this paper proposes a pipeline\nframework for interpretable SCM, which consists of four modules: a judicial\nfeature sentence identification module, a case matching module, a feature\nsentence alignment module, and a conflict disambiguation module. Unlike\nexisting SCM methods, our framework will identify feature sentences in a case\nthat contain essential information, perform similar case matching based on the\nextracted feature sentence results, and align the feature sentences in the two\ncases to provide evidence for the similarity of the cases. SCM results may\nconflict with feature sentence alignment results, and our framework further\ndisambiguates against this inconsistency. The experimental results show the\neffectiveness of our framework, and our work provides a new benchmark for\ninterpretable SCM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haonan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiajun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01638","description":"<p>Because of the inevitable cost and complexity of transformer and pre-trained\nmodels, efficiency concerns are raised for long text classification. Meanwhile,\nin the highly sensitive domains, e.g., healthcare and legal long-text mining,\npotential model distrust, yet underrated and underexplored, may hatch vital\napprehension. Existing methods generally segment the long text, encode each\npiece with the pre-trained model, and use attention or RNNs to obtain long text\nrepresentation for classification. In this work, we propose a simple but\neffective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace\nattention/RNNs in the above framework. Unlike prior efforts, SWIPE can\neffectively learn the label of the entire text with supervised training, while\nperceive the labels of the segments and estimate their contributions to the\nlong-text labeling in an unsupervised manner. As a general classifier, SWIPE\ncan endorse different encoders, and it outperforms SOTA models in terms of\nclassification accuracy and model efficiency. It is noteworthy that SWIPE\nachieves superior interpretability to transparentize long text classification\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Image Captioning with Discriminative Finetuning. (arXiv:2304.01662v1 [cs.CV])","link":"http://arxiv.org/abs/2304.01662","description":"<p>Neural captioners are typically trained to mimic human-generated references\nwithout optimizing for any specific communication goal, leading to problems\nsuch as the generation of vague captions. In this paper, we show that\nfine-tuning an out-of-the-box neural captioner with a self-supervised\ndiscriminative communication objective helps to recover a plain, visually\ndescriptive language that is more informative about image contents. Given a\ntarget image, the system must learn to produce a description that enables an\nout-of-the-box text-conditioned image retriever to identify such image among a\nset of candidates. We experiment with the popular ClipCap captioner, also\nreplicating the main results with BLIP. In terms of similarity to ground-truth\nhuman descriptions, the captions emerging from discriminative finetuning lag\nslightly behind those generated by the non-finetuned model, when the latter is\ntrained and tested on the same caption dataset. However, when the model is used\nwithout further tuning to generate captions for out-of-domain datasets, our\ndiscriminatively-finetuned captioner generates descriptions that resemble human\nreferences more than those produced by the same captioner without finetuning.\nWe further show that, on the Conceptual Captions dataset, discriminatively\nfinetuned captions are more helpful than either vanilla ClipCap captions or\nground-truth captions for human annotators tasked with an image discrimination\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevilacqua_M/0/1/0/all/0/1\">Michele Bevilacqua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gualdoni_E/0/1/0/all/0/1\">Eleonora Gualdoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotonirina_N/0/1/0/all/0/1\">Nathanael Carraz Rakotonirina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franzon_F/0/1/0/all/0/1\">Francesca Franzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01665","description":"<p>Language models have achieved impressive results in natural language\nprocessing tasks, but their ability to perform symbolic operations and\narithmetic operations, remains limited, which attribute to their learn the\nrules implicitly from data. We explore how to incorporate compiled neural\nnetworks (CoNNs) which weight is specially designed, into the architecture of\nlanguage models to enable the language model trained by gradient to obtain\nfully rule comprehension ability. The incorporation of compiled neural networks\noffers a promising direction for improving the performance of language models\non compound tasks, particularly in areas that require a deeper comprehension of\nabstract rules beyond recognizing patterns in training data. Our method, which\ncall \"Neural Comprehension\", helps language models achieve absolute accuracy in\nsymbolic operations, thereby enhancing their ability for rule reasoning,\nsymbolic reasoning, and arithmetic reasoning. Our code is publicly available\nat: \\url{https://github.com/WENGSYX/Neural-Comprehension}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01666","description":"<p>Semantic Shift Detection (SSD) is the task of identifying, interpreting, and\nassessing the possible change over time in the meanings of a target word.\nTraditionally, SSD has been addressed by linguists and social scientists\nthrough manual and time-consuming activities. In the recent years,\ncomputational approaches based on Natural Language Processing and word\nembeddings gained increasing attention to automate SSD as much as possible. In\nparticular, over the past three years, significant advancements have been made\nalmost exclusively based on word contextualised embedding models, which can\nhandle the multiple usages/meanings of the words and better capture the related\nsemantic shifts. In this paper, we survey the approaches based on\ncontextualised embeddings for SSD (i.e., CSSDetection) and we propose a\nclassification framework characterised by meaning representation,\ntime-awareness, and learning modality dimensions. The framework is exploited i)\nto review the measures for shift assessment, ii) to compare the approaches on\nperformance, and iii) to discuss the current issues in terms of scalability,\ninterpretability, and robustness. Open challenges and future research\ndirections about CSSDetection are finally outlined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montanelli_S/0/1/0/all/0/1\">Stefano Montanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Periti_F/0/1/0/all/0/1\">Francesco Periti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01680","description":"<p>This paper investigates how Transformer language models (LMs) fine-tuned for\nacceptability classification capture linguistic features. Our approach uses the\nbest practices of topological data analysis (TDA) in NLP: we construct directed\nattention graphs from attention matrices, derive topological features from\nthem, and feed them to linear classifiers. We introduce two novel features,\nchordality, and the matching number, and show that TDA-based classifiers\noutperform fine-tuning baselines. We experiment with two datasets, CoLA and\nRuCoLA in English and Russian, typologically different languages. On top of\nthat, we propose several black-box introspection techniques aimed at detecting\nchanges in the attention mode of the LMs during fine-tuning, defining the LM's\nprediction confidences, and associating individual heads with fine-grained\ngrammar phenomena. Our results contribute to understanding the behavior of\nmonolingual LMs in the acceptability classification task, provide insights into\nthe functional roles of attention heads, and highlight the advantages of\nTDA-based approaches for analyzing LMs. We release the code and the\nexperimental results for further uptake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Proskurina_I/0/1/0/all/0/1\">Irina Proskurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumour Detection and Analysis on Twitter. (arXiv:2304.01712v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01712","description":"<p>In recent years people have become increasingly reliant on social media to\nread news and get information, and some social media users post unsubstantiated\ninformation to gain attention. Such information is known as rumours. Nowadays,\nrumour detection is receiving a growing amount of attention because of the\npandemic of the New Coronavirus, which has led to a large number of rumours\nbeing spread. In this paper, a Natural Language Processing (NLP) system is\nbuilt to predict rumours. The best model is applied to the COVID-19 tweets to\nconduct exploratory data analysis. The contribution of this study is twofold:\n(1) to compare rumours and facts using state-of-the-art natural language\nprocessing models in two dimensions: language structure and propagation route.\n(2) An analysis of how rumours differ from facts in terms of their lexical use\nand the emotions they imply. This study shows that linguistic structure is a\nbetter feature to distinguish rumours from facts compared to the propagation\npath. In addition, rumour tweets contain more vocabulary related to politics\nand negative emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaohou Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation. (arXiv:2304.01746v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01746","description":"<p>ChatGPT, a large-scale language model based on the advanced GPT-3.5\narchitecture, has shown remarkable potential in various Natural Language\nProcessing (NLP) tasks. However, there is currently a dearth of comprehensive\nstudy exploring its potential in the area of Grammatical Error Correction\n(GEC). To showcase its capabilities in GEC, we design zero-shot\nchain-of-thought (CoT) and few-shot CoT settings using in-context learning for\nChatGPT. Our evaluation involves assessing ChatGPT's performance on five\nofficial test sets in three different languages, along with three\ndocument-level GEC test sets in English. Our experimental results and human\nevaluations demonstrate that ChatGPT has excellent error detection capabilities\nand can freely correct errors to make the corrected sentences very fluent,\npossibly due to its over-correction tendencies and not adhering to the\nprinciple of minimal edits. Additionally, its performance in non-English and\nlow-resource settings highlights its potential in multilingual GEC tasks.\nHowever, further analysis of various types of errors at the document-level has\nshown that ChatGPT cannot effectively correct agreement, coreference, tense\nerrors across sentences, and cross-sentence boundary errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_K/0/1/0/all/0/1\">Kaixin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])","link":"http://arxiv.org/abs/2304.01752","description":"<p>Vision-Language (V-L) models trained with contrastive learning to align the\nvisual and language modalities have been shown to be strong few-shot learners.\nSoft prompt learning is the method of choice for few-shot downstream adaption\naiming to bridge the modality gap caused by the distribution shift induced by\nthe new domain. While parameter-efficient, prompt learning still requires\naccess to the model weights and can be computationally infeasible for large\nmodels with billions of parameters. To address these shortcomings, in this\nwork, we describe a black-box method for V-L few-shot adaptation that (a)\noperates on pre-computed image and text features and hence works without access\nto the model's weights, (b) it is orders of magnitude faster at training time,\n(c) it is amenable to both supervised and unsupervised training, and (d) it can\nbe even used to align image and text features computed from uni-modal models.\nTo achieve this, we propose Linear Feature Alignment (LFA), a simple linear\napproach for V-L re-alignment in the target domain. LFA is initialized from a\nclosed-form solution to a least-squares problem and then it is iteratively\nupdated by minimizing a re-ranking loss. Despite its simplicity, our approach\ncan even surpass soft-prompt learning methods as shown by extensive experiments\non 11 image and 2 video datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouali_Y/0/1/0/all/0/1\">Yassine Ouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01774","description":"<p>Human-in-the-loop topic modelling incorporates users' knowledge into the\nmodelling process, enabling them to refine the model iteratively. Recent\nresearch has demonstrated the value of user feedback, but there are still\nissues to consider, such as the difficulty in tracking changes, comparing\ndifferent models and the lack of evaluation based on real-world examples of\nuse. We developed a novel, interactive human-in-the-loop topic modeling system\nwith a user-friendly interface that enables users compare and record every step\nthey take, and a novel topic words suggestion feature to help users provide\nfeedback that is faithful to the ground truth. Our system also supports not\nonly what traditional topic models can do, i.e., learning the topics from the\nwhole corpus, but also targeted topic modelling, i.e., learning topics for\nspecific aspects of the corpus. In this article, we provide an overview of the\nsystem and present the results of a series of user studies designed to assess\nthe value of the system in progressively more realistic applications of topic\nmodelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alqazlan_L/0/1/0/all/0/1\">Lama Alqazlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Du Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01852","description":"<p>This paper presents a comprehensive survey of ChatGPT and GPT-4,\nstate-of-the-art large language models (LLM) from the GPT series, and their\nprospective applications across diverse domains. Indeed, key innovations such\nas large-scale pre-training that captures knowledge across the entire world\nwide web, instruction fine-tuning and Reinforcement Learning from Human\nFeedback (RLHF) have played significant roles in enhancing LLMs' adaptability\nand performance. We performed an in-depth analysis of 194 relevant papers on\narXiv, encompassing trend analysis, word cloud representation, and distribution\nanalysis across various application domains. The findings reveal a significant\nand increasing interest in ChatGPT/GPT-4 research, predominantly centered on\ndirect natural language processing applications, while also demonstrating\nconsiderable potential in areas ranging from education and history to\nmathematics, medicine, and physics. This study endeavors to furnish insights\ninto ChatGPT's capabilities, potential implications, ethical concerns, and\noffer direction for future advancements in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianle Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiaming Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Antong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_N/0/1/0/all/0/1\">Ning Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dingang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01890","description":"<p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants. (arXiv:2304.01894v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01894","description":"<p>In this work, we develop language models for the Sanskrit language, namely\nBidirectional Encoder Representations from Transformers (BERT) and its\nvariants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using\nDevanagari Sanskrit text corpus. Then we extracted the features for the given\ntext from these models. We applied the dimensional reduction and clustering\ntechniques on the features to generate an extractive summary for a given\nSanskrit document. Along with the extractive text summarization techniques, we\nhave also created and released a Sanskrit Devanagari text corpus publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_K/0/1/0/all/0/1\">Kartik Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lonka_S/0/1/0/all/0/1\">Sampath Lonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_J/0/1/0/all/0/1\">Jammi Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1\">Mahabala Rao M G</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01904","description":"<p>Language models (LMs) have recently shown remarkable performance on reasoning\ntasks by explicitly generating intermediate inferences, e.g., chain-of-thought\nprompting. However, these intermediate inference steps may be inappropriate\ndeductions from the initial context and lead to incorrect final predictions.\nHere we introduce REFINER, a framework for finetuning LMs to explicitly\ngenerate intermediate reasoning steps while interacting with a critic model\nthat provides automated feedback on the reasoning. Specifically, the critic\nprovides structured feedback that the reasoning LM uses to iteratively improve\nits intermediate arguments. Empirical evaluations of REFINER on three diverse\nreasoning tasks show significant improvements over baseline LMs of comparable\nscale. Furthermore, when using GPT3.5 as the reasoner, the trained critic\nsignificantly improves reasoning without finetuning the reasoner. Finally, our\ncritic model is trained without expensive human-in-the-loop data but can be\nsubstituted with humans at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjit Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ismayilzada_M/0/1/0/all/0/1\">Mete Ismayilzada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])","link":"http://arxiv.org/abs/2304.01905","description":"<p>We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1\">Saumya Y. Sahai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniyappa_T/0/1/0/all/0/1\">Thejaswi Muniyappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree M. Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandridis_A/0/1/0/all/0/1\">Anastasios Alexandridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGowan_R/0/1/0/all/0/1\">Ross McGowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources and Few-shot Learners for In-context Learning in Slavic Languages. (arXiv:2304.01922v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01922","description":"<p>Despite the rapid recent progress in creating accurate and compact in-context\nlearners, most recent work focuses on in-context learning (ICL) for tasks in\nEnglish. However, the ability to interact with users of languages outside\nEnglish presents a great potential for broadening the applicability of language\ntechnologies to non-English speakers.\n</p>\n<p>In this work, we collect the infrastructure necessary for training and\nevaluation of ICL in a selection of Slavic languages: Czech, Polish, and\nRussian. We link a diverse set of datasets and cast these into a unified\ninstructional format through a set of transformations and newly-crafted\ntemplates written purely in target languages. Using the newly-curated dataset,\nwe evaluate a set of the most recent in-context learners and compare their\nresults to the supervised baselines. Finally, we train, evaluate and publish a\nset of in-context learning models that we train on the collected resources and\ncompare their performance to previous work.\n</p>\n<p>We find that ICL models tuned in English are also able to learn some tasks\nfrom non-English contexts, but multilingual instruction fine-tuning\nconsistently improves the ICL ability. We also find that the massive multitask\ntraining can be outperformed by single-task training in the target language,\nuncovering the potential for specializing in-context learners to the\nlanguage(s) of their application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramacki_P/0/1/0/all/0/1\">Piotr Gramacki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01933","description":"<p>The success of large language models (LLMs), like GPT-3 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by fine-tuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such\nas Series adapter, Parallel adapter, and LoRA. The framework is designed to be\nresearch-friendly, efficient, modular, and extendable, allowing the integration\nof new adapters and the evaluation of them with new and larger-scale LLMs.\nFurthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we\nconduct experiments on six math reasoning datasets. The results demonstrate\nthat using adapter-based PEFT in smaller-scale LLMs (7B) with few extra\ntrainable parameters yields comparable, and in some cases superior, performance\nto that of powerful LLMs (175B) in zero-shot inference on simple math reasoning\ndatasets. Overall, we provide a promising framework for fine-tuning large LLMs\non downstream tasks. We believe the proposed LLMs-Adapters will advance\nadapter-based PEFT research, facilitate the deployment of research pipelines,\nand enable practical applications to real-world systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])","link":"http://arxiv.org/abs/2304.01938","description":"<p>We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Holmes_J/0/1/0/all/0/1\">Jason Holmes</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1\">Lian Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ding_Y/0/1/0/all/0/1\">Yuzhen Ding</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sio_T/0/1/0/all/0/1\">Terence T. Sio</a>, <a href=\"http://arxiv.org/find/physics/1/au:+McGee_L/0/1/0/all/0/1\">Lisa A. McGee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ashman_J/0/1/0/all/0/1\">Jonathan B. Ashman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shen_J/0/1/0/all/0/1\">Jiajian Shen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])","link":"http://arxiv.org/abs/2304.01961","description":"<p>This paper presents the AToMiC (Authoring Tools for Multimedia Content)\ndataset, designed to advance research in image/text cross-modal retrieval.\nWhile vision-language pretrained transformers have led to significant\nimprovements in retrieval effectiveness, existing research has relied on\nimage-caption datasets that feature only simplistic image-text relationships\nand underspecified user models of retrieval tasks. To address the gap between\nthese oversimplified settings and real-world applications for multimedia\ncontent creation, we introduce a new approach for building retrieval test\ncollections. We leverage hierarchical structures and diverse domains of texts,\nstyles, and types of images, as well as large-scale image-document associations\nembedded in Wikipedia. We formulate two tasks based on a realistic user model\nand validate our dataset through retrieval experiments using baseline models.\nAToMiC offers a testbed for scalable, diverse, and reproducible multimedia\nretrieval research. Finally, the dataset provides the basis for a dedicated\ntrack at the 2023 Text Retrieval Conference (TREC), and is publicly available\nat https://github.com/TREC-AToMiC/AToMiC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jheng-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redi_M/0/1/0/all/0/1\">Miriam Redi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01969","description":"<p>Text classification typically requires a substantial amount of\nhuman-annotated data to serve as supervision, which is costly to obtain in\ndynamic emerging domains. Certain methods seek to address this problem by\nsolely relying on the surface text of class names to serve as extremely weak\nsupervision. However, existing methods fail to account for single-class\ndocuments discussing multiple topics. Both topic diversity and vague sentences\nmay introduce noise into the document's underlying representation and\nconsequently the precision of the predicted class. Furthermore, current work\nfocuses on text granularities (documents, sentences, or words) independently,\nwhich limits the degree of coarse- or fine-grained context that we can jointly\nextract from all three to identify significant subtext for classification. In\norder to address this problem, we propose MEGClass, an extremely\nweakly-supervised text classification method to exploit Mutually-Enhancing Text\nGranularities. Specifically, MEGClass constructs class-oriented sentence and\nclass representations based on keywords for performing a sentence-level\nconfidence-weighted label ensemble in order to estimate a document's initial\nclass distribution. This serves as the target distribution for a multi-head\nattention network with a class-weighted contrastive loss. This network learns\ncontextualized sentence representations and weights to form document\nrepresentations that reflect its original document and sentence-level topic\ndiversity. Retaining this heterogeneity allows MEGClass to select the most\nclass-indicative documents to serve as iterative feedback for enhancing the\nclass representations. Finally, these top documents are used to fine-tune a\npre-trained text classifier. As demonstrated through extensive experiments on\nsix benchmark datasets, MEGClass outperforms other weakly and extremely weakly\nsupervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kargupta_P/0/1/0/all/0/1\">Priyanka Kargupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komarlu_T/0/1/0/all/0/1\">Tanay Komarlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Susik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue-Contextualized Re-ranking for Medical History-Taking. (arXiv:2304.01974v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01974","description":"<p>AI-driven medical history-taking is an important component in symptom\nchecking, automated patient intake, triage, and other AI virtual care\napplications. As history-taking is extremely varied, machine learning models\nrequire a significant amount of data to train. To overcome this challenge,\nexisting systems are developed using indirect data or expert knowledge. This\nleads to a training-inference gap as models are trained on different kinds of\ndata than what they observe at inference time. In this work, we present a\ntwo-stage re-ranking approach that helps close the training-inference gap by\nre-ranking the first-stage question candidates using a dialogue-contextualized\nmodel. For this, we propose a new model, global re-ranker, which cross-encodes\nthe dialogue with all questions simultaneously, and compare it with several\nexisting neural baselines. We test both transformer and S4-based language model\nbackbones. We find that relative to the expert system, the best performance is\nachieved by our proposed global re-ranker with a transformer backbone,\nresulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%\nhigher mean average precision (mAP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmianski_I/0/1/0/all/0/1\">Ilya Valmianski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. (arXiv:2304.01982v1 [cs.CL])","link":"http://arxiv.org/abs/2304.01982","description":"<p>Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]\nallow token-level interactions between queries and documents, and hence achieve\nstate of the art on many information retrieval benchmarks. However, their\nnon-linear scoring function cannot be scaled to millions of documents,\nnecessitating a three-stage process for inference: retrieving initial\ncandidates via token retrieval, accessing all token vectors, and scoring the\ninitial candidate documents. The non-linear scoring function is applied over\nall token vectors of each candidate document, making the inference process\ncomplicated and slow. In this paper, we aim to simplify the multi-vector\nretrieval by rethinking the role of token retrieval. We present XTR,\nConteXtualized Token Retriever, which introduces a simple, yet novel, objective\nfunction that encourages the model to retrieve the most important document\ntokens first. The improvement to token retrieval allows XTR to rank candidates\nonly using the retrieved tokens rather than all tokens in the document, and\nenables a newly designed scoring stage that is two-to-three orders of magnitude\ncheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the\nstate-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis\nconfirms our decision to revisit the token retrieval stage, as XTR demonstrates\nmuch better recall of the token retrieval stage compared to ColBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duddu_S/0/1/0/all/0/1\">Sai Meher Karthik Duddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.15949","description":"<p>Transformer networks have revolutionized NLP representation learning since\nthey were introduced. Though a great effort has been made to explain the\nrepresentation in transformers, it is widely recognized that our understanding\nis not sufficient. One important reason is that there lack enough visualization\ntools for detailed analysis. In this paper, we propose to use dictionary\nlearning to open up these \"black boxes\" as linear superpositions of transformer\nfactors. Through visualization, we demonstrate the hierarchical semantic\nstructures captured by the transformer factors, e.g., word-level polysemy\ndisambiguation, sentence-level pattern formation, and long-range dependency.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.\nWe hope this visualization tool can bring further knowledge and a better\nunderstanding of how transformer networks work. The code is available at\nhttps://github.com/zeyuyun1/TransformerVis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_Z/0/1/0/all/0/1\">Zeyu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1\">Bruno A Olshausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00396","description":"<p>Social media has become extremely influential when it comes to policy making\nin modern societies, especially in the western world, where platforms such as\nTwitter allow users to follow politicians, thus making citizens more involved\nin political discussion. In the same vein, politicians use Twitter to express\ntheir opinions, debate among others on current topics and promote their\npolitical agendas aiming to influence voter behaviour. In this paper, we\nattempt to analyse tweets of politicians from three European countries and\nexplore the virality of their tweets. Previous studies have shown that tweets\nconveying negative sentiment are likely to be retweeted more frequently. By\nutilising state-of-the-art pre-trained language models, we performed sentiment\nanalysis on hundreds of thousands of tweets collected from members of\nparliament in Greece, Spain and the United Kingdom, including devolved\nadministrations. We achieved this by systematically exploring and analysing the\ndifferences between influential and less popular tweets. Our analysis indicates\nthat politicians' negatively charged tweets spread more widely, especially in\nmore recent times, and highlights interesting differences between political\nparties as well as between politicians and the general population.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05711","description":"<p>Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yidan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1\">Qin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation. (arXiv:2205.12649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12649","description":"<p>Data sparsity is a main problem hindering the development of code-switching\n(CS) NLP systems. In this paper, we investigate data augmentation techniques\nfor synthesizing dialectal Arabic-English CS text. We perform lexical\nreplacements using word-aligned parallel corpora where CS points are either\nrandomly chosen or learnt using a sequence-to-sequence model. We compare these\napproaches against dictionary-based replacements. We assess the quality of the\ngenerated sentences through human evaluation and evaluate the effectiveness of\ndata augmentation on machine translation (MT), automatic speech recognition\n(ASR), and speech translation (ST) tasks. Results show that using a predictive\nmodel results in more natural CS sentences compared to the random approach, as\nreported in human judgements. In the downstream tasks, despite the random\napproach generating more data, both approaches perform equally (outperforming\ndictionary-based replacements). Overall, data augmentation achieves 34%\nimprovement in perplexity, 5.2% relative improvement on WER for ASR task,\n+4.0-5.1 BLEU points on MT task, and +2.1-2.2 BLEU points on ST over a baseline\ntrained on available data without augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07316","description":"<p>Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02821","description":"<p>We propose a two-stage approach for training a single NMT model to translate\nunseen languages both to and from English. For the first stage, we initialize\nan encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform\nmultilingual fine-tuning on parallel data in 40 languages to English. We find\nthis model can generalize to zero-shot translations on unseen languages. For\nthe second stage, we leverage this generalization ability to generate synthetic\nparallel data from monolingual datasets, then bidirectionally train with\nsuccessive rounds of back-translation.\n</p>\n<p>Our approach, which we EcXTra (English-centric Crosslingual (X) Transfer), is\nconceptually simple, only using a standard cross-entropy objective throughout.\nIt is also data-driven, sequentially leveraging auxiliary parallel data and\nmonolingual data. We evaluate unsupervised NMT results for 7 low-resource\nlanguages, and find that each round of back-translation training further\nrefines bidirectional performance. Our final single EcXTra-trained model\nachieves competitive translation performance in all translation directions,\nnotably establishing a new state-of-the-art for English-to-Kazakh (22.9 &gt; 10.4\nBLEU). Our code is available at https://github.com/manestay/EcXTra .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2209.05135","description":"<p>Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15088","description":"<p>Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lilian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01515","description":"<p>Predicting personality traits based on online posts has emerged as an\nimportant task in many fields such as social network analysis. One of the\nchallenges of this task is assembling information from various posts into an\noverall profile for each user. While many previous solutions simply concatenate\nthe posts into a long document and then encode the document by sequential or\nhierarchical models, they introduce unwarranted orders for the posts, which may\nmislead the models. In this paper, we propose a dynamic deep graph\nconvolutional network (D-DGCN) to overcome the above limitation. Specifically,\nwe design a learn-to-connect approach that adopts a dynamic multi-hop structure\ninstead of a deterministic structure, and combine it with a DGCN module to\nautomatically learn the connections between posts. The modules of post encoder,\nlearn-to-connect, and DGCN are jointly trained in an end-to-end manner.\nExperimental results on the Kaggle and Pandora datasets show the superior\nperformance of D-DGCN to state-of-the-art baselines. Our code is available at\nhttps://github.com/djz233/D-DGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinghao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04054","description":"<p>Given a piece of text, a video clip and a reference audio, the movie dubbing\n(also known as visual voice clone V2C) task aims to generate speeches that\nmatch the speaker's emotion presented in the video using the desired speaker\nvoice as reference. V2C is more challenging than conventional text-to-speech\ntasks as it additionally requires the generated speech to exactly match the\nvarying emotions and speaking speed presented in the video. Unlike previous\nworks, we propose a novel movie dubbing architecture to tackle these problems\nvia hierarchical prosody modelling, which bridges the visual information to\ncorresponding speech prosody from three aspects: lip, face, and scene.\nSpecifically, we align lip movement to the speech duration, and convey facial\nexpression to speech energy and pitch via attention mechanism based on valence\nand arousal representations inspired by recent psychology findings. Moreover,\nwe design an emotion booster to capture the atmosphere from global video\nscenes. All these embeddings together are used to generate mel-spectrogram and\nthen convert to speech waves via existing vocoder. Extensive experimental\nresults on the Chem and V2C benchmark datasets demonstrate the favorable\nperformance of the proposed method. The source code and trained models will be\nreleased to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gaoxiang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical multimodal transformers for Multi-Page DocVQA. (arXiv:2212.05935v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.05935","description":"<p>Document Visual Question Answering (DocVQA) refers to the task of answering\nquestions from document images. Existing work on DocVQA only considers\nsingle-page documents. However, in real scenarios documents are mostly composed\nof multiple pages that should be processed altogether. In this work we extend\nDocVQA to the multi-page scenario. For that, we first create a new dataset,\nMP-DocVQA, where questions are posed over multi-page documents instead of\nsingle pages. Second, we propose a new hierarchical method, Hi-VT5, based on\nthe T5 architecture, that overcomes the limitations of current methods to\nprocess long multi-page documents. The proposed method is based on a\nhierarchical transformer architecture where the encoder summarizes the most\nrelevant information of every page and then, the decoder takes this summarized\ninformation to generate the final answer. Through extensive experimentation, we\ndemonstrate that our method is able, in a single stage, to answer the questions\nand provide the page that contains the relevant information to find the answer,\nwhich can be used as a kind of explainability measure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Rub&#xe8;n Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02427","description":"<p>We explore the possibility of meta-learning for the language-independent\nunsupervised tokenization problem for English, Russian, and Chinese. We\nimplement the meta-learning approach for automatic determination of\nhyper-parameters of the unsupervised tokenization model proposed in earlier\nworks, relying on various human-independent fitness functions such as\nnormalised anti-entropy, compression factor and cross-split F1 score, as well\nas additive and multiplicative composite combinations of the three metrics,\ntesting them against the conventional F1 tokenization score. We find a fairly\ngood correlation between the latter and the additive combination of the former\nthree metrics for English and Russian. In case of Chinese, we find a\nsignificant correlation between the F 1 score and the compression factor. Our\nresults suggest the possibility of robust unsupervised tokenization of\nlow-resource and dead languages and allow us to think about human languages in\nterms of the evolution of efficient symbolic communication codes with different\nstructural optimisation schemes that have evolved in different human cultures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolonin_A/0/1/0/all/0/1\">Anton Kolonin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12582","description":"<p>The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Sanchit Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Josh Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_P/0/1/0/all/0/1\">Pete Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick Von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaumond_J/0/1/0/all/0/1\">Julien Chaumond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noyan_M/0/1/0/all/0/1\">Merve Noyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanseviero_O/0/1/0/all/0/1\">Omar Sanseviero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2303.14524","description":"<p>Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunfan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Youlin Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17612","description":"<p>In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings improves distillation and model initialization to deliver higher\naccuracy on a broad range of transfer tasks. In generating oBERTa, we explore\nhow the highly optimized RoBERTa differs from the BERT for pruning during\npre-training and finetuning. We find it less amenable to compression during\nfine-tuning. We explore the use of oBERTa on seven representative NLP tasks and\nfind that the improved compression techniques allow a pruned oBERTa model to\nmatch the performance of BERTbase and exceed the performance of Prune OFA Large\non the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,\nrespectively faster in inference. We release our code, training regimes, and\nassociated model for broad usage to encourage usage and experimentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2303.17710","description":"<p>The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ya-Fang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v2 [physics.pop-ph] UPDATED)","link":"http://arxiv.org/abs/2303.17853","description":"<p>In what will likely be a litany of generative-model-themed arXiv submissions\ncelebrating April the 1st, we evaluate the capacity of state-of-the-art\ntransformer models to create a paper detailing the detection of a Pulsar Wind\nNebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)\nArray. We do this to evaluate the ability of such models to interpret\nastronomical observations and sources based on language information alone, and\nto assess potential means by which fraudulently generated scientific papers\ncould be identified during peer review (given that reliable generative model\nwatermarking has yet to be deployed for these tools). We conclude that our jobs\nas astronomers are safe for the time being. From this point on, prompts given\nto ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT\nis shown in black, whereas analysis by the (human) authors is in blue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Spencer_S/0/1/0/all/0/1\">Samuel T. Spencer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Joshi_V/0/1/0/all/0/1\">Vikas Joshi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mitchell_A/0/1/0/all/0/1\">Alison M.W. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01196","description":"<p>Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize. An online\ndemo is also available at\nhttps://huggingface.co/spaces/project-baize/baize-lora-7B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2304.01046","description":"<p>Throughout schooling, students are tested on reading comprehension and\nlogical reasoning. Students have developed various strategies for completing\nsuch exams, some of which are generally thought to outperform others. One such\nstrategy involves emphasizing relative accuracy over absolute accuracy and can\ntheoretically produce the correct answer without full knowledge of the\ninformation required to solve the question. This paper examines the\neffectiveness of applying such a strategy to train transfer learning models to\nsolve reading comprehension and logical reasoning questions. The models were\nevaluated on the ReClor dataset, a challenging reading comprehension and\nlogical reasoning benchmark. While previous studies targeted logical reasoning\nskills, we focus on a general training method and model architecture. We\npropose the polytuplet loss function, an extension of the triplet loss\nfunction, to ensure prioritization of learning the relative correctness of\nanswer choices over learning the true accuracy of each choice. Our results\nindicate that models employing polytuplet loss outperform existing baseline\nmodels. Although polytuplet loss is a promising alternative to other\ncontrastive loss functions, further research is required to quantify the\nbenefits it may present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jeffrey Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1\">Ivan Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}