{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Joint Coreference Resolution for Zeros and non-Zeros in Arabic. (arXiv:2210.12169v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12169","description":"<p>Most existing proposals about anaphoric zero pronoun (AZP) resolution regard\nfull mention coreference and AZP resolution as two independent tasks, even\nthough the two tasks are clearly related. The main issues that need tackling to\ndevelop a joint model for zero and non-zero mentions are the difference between\nthe two types of arguments (zero pronouns, being null, provide no nominal\ninformation) and the lack of annotated datasets of a suitable size in which\nboth types of arguments are annotated for languages other than Chinese and\nJapanese. In this paper, we introduce two architectures for jointly resolving\nAZPs and non-AZPs, and evaluate them on Arabic, a language for which, as far as\nwe know, there has been no prior work on joint resolution. Doing this also\nrequired creating a new version of the Arabic subset of the standard\ncoreference resolution dataset used for the CoNLL-2012 shared task (Pradhan et\nal.,2012) in which both zeros and non-zeros are included in a single dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aloraini_A/0/1/0/all/0/1\">Abdulrahman Aloraini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Sameer Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Differences in the Representation of People using Contextualized Semantic Axes. (arXiv:2210.12170v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12170","description":"<p>A common paradigm for identifying semantic differences across social and\ntemporal contexts is the use of static word embeddings and their distances. In\nparticular, past work has compared embeddings against \"semantic axes\" that\nrepresent two opposing concepts. We extend this paradigm to BERT embeddings,\nand construct contextualized axes that mitigate the pitfall where antonyms have\nneighboring representations. We validate and demonstrate these axes on two\npeople-centric datasets: occupations from Wikipedia, and multi-platform\ndiscussions in extremist, men's communities over fourteen years. In both\nstudies, contextualized semantic axes can characterize differences among\ninstances of the same word type. In the latter study, we show that references\nto women and the contexts around them have become more detestable over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucy_L/0/1/0/all/0/1\">Li Lucy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadimeti_D/0/1/0/all/0/1\">Divya Tadimeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1\">David Bamman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards. (arXiv:2210.12186v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12186","description":"<p>Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. These systems have achieved\npromising performance as measured by widely used NLG metrics such as BLEU and\nCIDEr. However, the current systems face important limitations. First, they\npresent an increased complexity in architecture that offers only marginal\nimprovements on NLG metrics. Secondly, these systems that achieve high\nperformance on these metrics are not always factually complete or consistent\ndue to both inadequate training and evaluation. Recent studies have shown the\nsystems can be substantially improved by using new methods encouraging 1) the\ngeneration of domain entities consistent with the reference and 2) describing\nthese entities in inferentially consistent ways. So far, these methods rely on\nweakly-supervised approaches (rule-based) and named entity recognition systems\nthat are not specific to the chest X-ray domain. To overcome this limitation,\nwe propose a new method, the RadGraph reward, to further improve the factual\ncompleteness and correctness of generated radiology reports. More precisely, we\nleverage the RadGraph dataset containing annotated chest X-ray reports with\nentities and relations between entities. On two open radiology report datasets,\nour system substantially improves the scores up to 14.2% and 25.3% on metrics\nevaluating the factual correctness and completeness of reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bluethgen_C/0/1/0/all/0/1\">Christian Bluethgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_E/0/1/0/all/0/1\">Emily Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almusa_O/0/1/0/all/0/1\">Omar Almusa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities. (arXiv:2210.12187v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12187","description":"<p>Humans exhibit garden path effects: When reading sentences that are\ntemporarily structurally ambiguous, they slow down when the structure is\ndisambiguated in favor of the less preferred alternative. Surprisal theory\n(Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes\nthat these slowdowns are due to the unpredictability of each of the words that\noccur in these sentences. Challenging this hypothesis, van Schijndel &amp; Linzen\n(2021) find that estimates of the cost of word predictability derived from\nlanguage models severely underestimate the magnitude of human garden path\neffects. In this work, we consider whether this underestimation is due to the\nfact that humans weight syntactic factors in their predictions more highly than\nlanguage models do. We propose a method for estimating syntactic predictability\nfrom a language model, allowing us to weigh the cost of lexical and syntactic\npredictability independently. We find that treating syntactic predictability\nindependently from lexical predictability indeed results in larger estimates of\ngarden path. At the same time, even when syntactic predictability is\nindependently weighted, surprisal still greatly underestimate the magnitude of\nhuman garden path effects. Our results support the hypothesis that\npredictability is not the only factor responsible for the processing cost\nassociated with garden path sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arehalli_S/0/1/0/all/0/1\">Suhas Arehalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillon_B/0/1/0/all/0/1\">Brian Dillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. (arXiv:2210.12197v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12197","description":"<p>Analogy-making gives rise to reasoning, abstraction, flexible categorization\nand counterfactual inference -- abilities lacking in even the best AI systems\ntoday. Much research has suggested that analogies are key to non-brittle\nsystems that can adapt to new domains. Despite their importance, analogies\nreceived little attention in the NLP community, with most research focusing on\nsimple word analogies. Work that tackled more complex analogies relied heavily\non manually constructed, hard-to-scale input representations. In this work, we\nexplore a more realistic, challenging setup: our input is a pair of natural\nlanguage procedural texts, describing a situation or a process (e.g., how the\nheart works/how a pump works). Our goal is to automatically extract entities\nand their relations from the text and find a mapping between the different\ndomains based on relational similarity (e.g., blood is mapped to water). We\ndevelop an interpretable, scalable algorithm and demonstrate that it identifies\nthe correct mappings 87% of the time for procedural texts and 94% for stories\nfrom cognitive-psychology literature. We show it can extract analogies from a\nlarge dataset of procedural texts, achieving 79% precision (analogy prevalence\nin data: 3%). Lastly, we demonstrate that our algorithm is robust to\nparaphrasing the input texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_O/0/1/0/all/0/1\">Oren Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing with Noise: Unpicking the Warp and Weft of Embeddings. (arXiv:2210.12206v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12206","description":"<p>Improving our understanding of how information is encoded in vector space can\nyield valuable interpretability insights. Alongside vector dimensions, we argue\nthat it is possible for the vector norm to also carry linguistic information.\nWe develop a method to test this: an extension of the probing framework which\nallows for relative intrinsic interpretations of probing results. It relies on\nintroducing noise that ablates information encoded in embeddings, grounded in\nrandom baselines and confidence intervals. We apply the method to\nwell-established probing tasks and find evidence that confirms the existence of\nseparate information containers in English GloVe and BERT embeddings. Our\ncorrelation analysis aligns with the experimental findings that different\nencoders use the norm to encode different kinds of information: GloVe stores\nsyntactic and sentence length information in the vector norm, while BERT uses\nit to encode contextual incongruity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klubicka_F/0/1/0/all/0/1\">Filip Klubi&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation. (arXiv:2210.12213v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12213","description":"<p>Named geographic entities (geo-entities for short) are the building blocks of\nmany geographic datasets. Characterizing geo-entities is integral to various\napplication domains, such as geo-intelligence and map comprehension, while a\nkey challenge is to capture the spatial-varying context of an entity. We\nhypothesize that we shall know the characteristics of a geo-entity by its\nsurrounding entities, similar to knowing word meanings by their linguistic\ncontext. Accordingly, we propose a novel spatial language model, SpaBERT, which\nprovides a general-purpose geo-entity representation based on neighboring\nentities in geospatial data. SpaBERT extends BERT to capture linearized spatial\ncontext, while incorporating a spatial coordinate embedding mechanism to\npreserve spatial relations of entities in the 2-dimensional space. SpaBERT is\npretrained with masked language modeling and masked entity prediction tasks to\nlearn spatial dependencies. We apply SpaBERT to two downstream tasks:\ngeo-entity typing and geo-entity linking. Compared with the existing language\nmodels that do not use spatial context, SpaBERT shows significant performance\nimprovement on both tasks. We also analyze the entity representation from\nSpaBERT in various settings and the effect of spatial coordinate embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yao-Yi Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Bilingual Neural Transducer with Synthetic Code-switching Text Generation. (arXiv:2210.12214v1 [cs.SD])","link":"http://arxiv.org/abs/2210.12214","description":"<p>Code-switching describes the practice of using more than one language in the\nsame sentence. In this study, we investigate how to optimize a neural\ntransducer based bilingual automatic speech recognition (ASR) model for\ncode-switching speech. Focusing on the scenario where the ASR model is trained\nwithout supervised code-switching data, we found that semi-supervised training\nand synthetic code-switched data can improve the bilingual ASR system on\ncode-switching speech. We analyze how each of the neural transducer's encoders\ncontributes towards code-switching performance by measuring encoder-specific\nrecall values, and evaluate our English/Mandarin system on the ASCEND data set.\nOur final system achieves 25% mixed error rate (MER) on the ASCEND\nEnglish/Mandarin code-switching test set -- reducing the MER by 2.1% absolute\ncompared to the previous literature -- while maintaining good accuracy on the\nmonolingual test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nathalie Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liuhui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1\">Thiago Fraga da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radzihovsky_M/0/1/0/all/0/1\">Matthew Radzihovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1\">Henry Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermott_E/0/1/0/all/0/1\">Erik McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_D/0/1/0/all/0/1\">Dogan Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swietojanski_P/0/1/0/all/0/1\">Pawel Swietojanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1\">Lyan Verwimp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyman_S/0/1/0/all/0/1\">Sibel Oyman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvizo_T/0/1/0/all/0/1\">Tresi Arvizo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silovsky_H/0/1/0/all/0/1\">Honza Silovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_A/0/1/0/all/0/1\">Arnab Ghoshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_M/0/1/0/all/0/1\">Mathieu Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambati_B/0/1/0/all/0/1\">Bharat Ram Ambati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohamed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gui at MixMT 2022 : English-Hinglish: An MT approach for translation of code mixed data. (arXiv:2210.12215v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12215","description":"<p>Code-mixed machine translation has become an important task in multilingual\ncommunities and extending the task of machine translation to code mixed data\nhas become a common task for these languages. In the shared tasks of WMT 2022,\nwe try to tackle the same for both English + Hindi to Hinglish and Hinglish to\nEnglish. The first task dealt with both Roman and Devanagari script as we had\nmonolingual data in both English and Hindi whereas the second task only had\ndata in Roman script. To our knowledge, we achieved one of the top ROUGE-L and\nWER scores for the first task of Monolingual to Code-Mixed machine translation.\nIn this paper, we discuss the use of mBART with some special pre-processing and\npost-processing (transliteration from Devanagari to Roman) for the first task\nin detail and the experiments that we performed for the second task of\ntranslating code-mixed Hinglish to monolingual English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gahoi_A/0/1/0/all/0/1\">Akshat Gahoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duneja_J/0/1/0/all/0/1\">Jayant Duneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_A/0/1/0/all/0/1\">Anshul Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangale_S/0/1/0/all/0/1\">Shivam Mangale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Saransh Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_T/0/1/0/all/0/1\">Tanvi Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dipti Misra Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning. (arXiv:2210.12217v1 [cs.AI])","link":"http://arxiv.org/abs/2210.12217","description":"<p>Our goal is a question-answering (QA) system that can show how its answers\nare implied by its own internal beliefs via a systematic chain of reasoning.\nSuch a capability would allow better understanding of why a model produced the\nanswer it did. Our approach is to recursively combine a trained\nbackward-chaining model, capable of generating a set of premises entailing an\nanswer hypothesis, with a verifier that checks that the model itself believes\nthose premises (and the entailment itself) through self-querying. To our\nknowledge, this is the first system to generate multistep chains that are both\nfaithful (the answer follows from the reasoning) and truthful (the chain\nreflects the system's own internal beliefs). In evaluation using two different\ndatasets, users judge that a majority (70%+) of generated chains clearly show\nhow an answer follows from a set of facts - substantially better than a\nhigh-performance baseline - while preserving answer accuracy. By materializing\nmodel beliefs that systematically support an answer, new opportunities arise\nfor understanding the model's system of belief, and diagnosing and correcting\nits misunderstandings when an answer is wrong.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Multilingual and Zero-Shot Multispeaker TTS. (arXiv:2210.12223v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12223","description":"<p>While neural methods for text-to-speech (TTS) have shown great advances in\nmodeling multiple speakers, even in zero-shot settings, the amount of data\nneeded for those approaches is generally not feasible for the vast majority of\nthe world's over 6,000 spoken languages. In this work, we bring together the\ntasks of zero-shot voice cloning and multilingual low-resource TTS. Using the\nlanguage agnostic meta learning (LAML) procedure and modifications to a TTS\nencoder, we show that it is possible for a system to learn speaking a new\nlanguage using just 5 minutes of training data while retaining the ability to\ninfer the voice of even unseen speakers in the newly learned language. We show\nthe success of our proposed approach in terms of intelligibility, naturalness\nand similarity to target speaker using objective metrics as well as human\nstudies and provide our code and trained models open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDUKG: a Heterogeneous Sustainable K-12 Educational Knowledge Graph. (arXiv:2210.12228v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12228","description":"<p>Web and artificial intelligence technologies, especially semantic web and\nknowledge graph (KG), have recently raised significant attention in educational\nscenarios. Nevertheless, subject-specific KGs for K-12 education still lack\nsufficiency and sustainability from knowledge and data perspectives. To tackle\nthese issues, we propose EDUKG, a heterogeneous sustainable K-12 Educational\nKnowledge Graph. We first design an interdisciplinary and fine-grained ontology\nfor uniformly modeling knowledge and resource in K-12 education, where we\ndefine 635 classes, 445 object properties, and 1314 datatype properties in\ntotal. Guided by this ontology, we propose a flexible methodology for\ninteractively extracting factual knowledge from textbooks. Furthermore, we\nestablish a general mechanism based on our proposed generalized entity linking\nsystem for EDUKG's sustainable maintenance, which can dynamically index\nnumerous heterogeneous resources and data with knowledge topics in EDUKG. We\nfurther evaluate EDUKG to illustrate its sufficiency, richness, and\nvariability. We publish EDUKG with more than 252 million entities and 3.86\nbillion triplets. Our code and data repository is now available at\nhttps://github.com/THU-KEG/EDUKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bowen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiuding Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xingyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Plain Language Adaptation of Biomedical Abstracts. (arXiv:2210.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12242","description":"<p>Though exponentially growing health-related literature has been made\navailable to a broad audience online, the language of scientific articles can\nbe difficult for the general public to understand. Therefore, adapting this\nexpert-level language into plain language versions is necessary for the public\nto reliably comprehend the vast health-related literature. Deep Learning\nalgorithms for automatic adaptation are a possible solution; however, gold\nstandard datasets are needed for proper evaluation. Proposed datasets thus far\nconsist of either pairs of comparable professional- and general public-facing\ndocuments or pairs of semantically similar sentences mined from such documents.\nThis leads to a trade-off between imperfect alignments and small test sets. To\naddress this issue, we created the Plain Language Adaptation of Biomedical\nAbstracts dataset. This dataset is the first manually adapted dataset that is\nboth document- and sentence-aligned. The dataset contains 750 adapted\nabstracts, totaling 7643 sentence pairs. Along with describing the dataset, we\nbenchmark automatic adaptation on the dataset with state-of-the-art Deep\nLearning approaches, setting baselines for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attal_K/0/1/0/all/0/1\">Kush Attal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondov_B/0/1/0/all/0/1\">Brian Ondov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Tabular Reasoning with Pattern Exploiting Training. (arXiv:2210.12259v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12259","description":"<p>Recent methods based on pre-trained language models have exhibited superior\nperformance over tabular tasks (e.g., tabular NLI), despite showing inherent\nproblems such as not using the right evidence and inconsistent predictions\nacross inputs while reasoning over the tabular data. In this work, we utilize\nPattern-Exploiting Training (PET) (i.e., strategic MLM) on pre-trained language\nmodels to strengthen these tabular reasoning models' pre-existing knowledge and\nreasoning abilities. Our upgraded model exhibits a superior understanding of\nknowledge facts and tabular reasoning compared to current baselines.\nAdditionally, we demonstrate that such models are more effective for underlying\ndownstream tasks of tabular inference on InfoTabs. Furthermore, we show our\nmodel's robustness against adversarial sets generated through various character\nand word level perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shankarampeta_A/0/1/0/all/0/1\">Abhilash Reddy Shankarampeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination. (arXiv:2210.12261v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12261","description":"<p>Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Calibration of Massively Multilingual Language Models. (arXiv:2210.12265v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12265","description":"<p>Massively Multilingual Language Models (MMLMs) have recently gained\npopularity due to their surprising effectiveness in cross-lingual transfer.\nWhile there has been much work in evaluating these models for their performance\non a variety of tasks and languages, little attention has been paid on how well\ncalibrated these models are with respect to the confidence in their\npredictions. We first investigate the calibration of MMLMs in the zero-shot\nsetting and observe a clear case of miscalibration in low-resource languages or\nthose which are typologically diverse from English. Next, we empirically show\nthat calibration methods like temperature scaling and label smoothing do\nreasonably well towards improving calibration in the zero-shot scenario. We\nalso find that few-shot examples in the language can further help reduce the\ncalibration errors, often substantially. Overall, our work contributes towards\nbuilding more reliable multilingual models by highlighting the issue of their\nmiscalibration, understanding what language and model specific factors\ninfluence it, and pointing out the strategies to improve the same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12273","description":"<p>Since its original appearance in 1991, the Perso-Arabic script representation\nin Unicode has grown from 169 to over 440 atomic isolated characters spread\nover several code pages representing standard letters, various diacritics and\npunctuation for the original Arabic and numerous other regional orthographic\ntraditions. This paper documents the challenges that Perso-Arabic presents\nbeyond the best-documented languages, such as Arabic and Persian, building on\nearlier work by the expert community. We particularly focus on the situation in\nnatural language processing (NLP), which is affected by multiple, often\nneglected, issues such as the use of visually ambiguous yet canonically\nnonequivalent letters and the mixing of letters from different orthographies.\nAmong the contributing conflating factors are the lack of input methods, the\ninstability of modern orthographies, insufficient literacy, and loss or lack of\northographic tradition. We evaluate the effects of script normalization on\neight languages from diverse language families in the Perso-Arabic script\ndiaspora on machine translation and statistical language modeling tasks. Our\nresults indicate statistically significant improvements in performance in most\nconditions for all the languages considered when normalization is applied. We\nargue that better understanding and representation of Perso-Arabic script\nvariation within regional orthographic traditions, where those are present, is\ncrucial for further progress of modern computational NLP techniques especially\nfor languages with a paucity of resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doctor_R/0/1/0/all/0/1\">Raiomond Doctor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johny_C/0/1/0/all/0/1\">Cibu Johny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Editing as Imitation Game. (arXiv:2210.12276v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12276","description":"<p>Text editing, such as grammatical error correction, arises naturally from\nimperfect textual data. Recent works frame text editing as a multi-round\nsequence tagging task, where operations -- such as insertion and substitution\n-- are represented as a sequence of tags. While achieving good results, this\nencoding is limited in flexibility as all actions are bound to token-level\ntags. In this work, we reformulate text editing as an imitation game using\nbehavioral cloning. Specifically, we convert conventional sequence-to-sequence\ndata into state-to-action demonstrations, where the action space can be as\nflexible as needed. Instead of generating the actions one at a time, we\nintroduce a dual decoders structure to parallel the decoding while retaining\nthe dependencies between action tokens, coupled with trajectory augmentation to\nalleviate the distribution shift that imitation learning often suffers. In\nexperiments on a suite of Arithmetic Equation benchmarks, our model\nconsistently outperforms the autoregressive baselines in terms of performance,\nefficiency, and robustness. We hope our findings will shed light on future\nstudies in reinforcement learning applying sequence-level action generation to\nnatural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yewen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Large Language Models Learn beyond Language?. (arXiv:2210.12302v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12302","description":"<p>Large language models (LMs) have rapidly become a mainstay in Natural\nLanguage Processing. These models are known to acquire rich linguistic\nknowledge from training on large amounts of text. In this paper, we investigate\nif pre-training on text also confers these models with helpful `inductive\nbiases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic\ntasks involving quantitative computations, recognizing regular expressions and\nreasoning over strings. We find that pretrained models significantly outperform\ncomparable non-pretrained neural models. This remains true also in experiments\nwith training non-pretrained models with fewer parameters to account for model\nregularization effects. We further explore the effect of text domain on LMs by\npretraining models from text from different domains and provenances. Our\nexperiments surprisingly reveal that the positive effects of pre-training\npersist even when pretraining on multi-lingual text or computer code, and even\nfor text generated from synthetic languages. Our findings suggest a hitherto\nunexplored deep connection between pre-training and inductive learning\nabilities of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Grammar Inducer from Massive Uncurated Instructional Videos. (arXiv:2210.12309v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12309","description":"<p>Video-aided grammar induction aims to leverage video information for finding\nmore accurate syntactic grammars for accompanying text. While previous work\nfocuses on building systems for inducing grammars on text that are well-aligned\nwith video content, we investigate the scenario, in which text and video are\nonly in loose correspondence. Such data can be found in abundance online, and\nthe weak correspondence is similar to the indeterminacy problem studied in\nlanguage acquisition. Furthermore, we build a new model that can better learn\nvideo-span correlation without manually designed features adopted by previous\nwork. Experiments show that our model trained only on large-scale YouTube data\nwith no text-video alignment reports strong and robust performances across\nthree unseen datasets, despite domain shift and noisy label issues. Furthermore\nour model yields higher F1 scores than the previous state-of-the-art systems\ntrained on in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark Study of Contrastive Learning for Arabic Social Meaning. (arXiv:2210.12314v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12314","description":"<p>Contrastive learning (CL) brought significant progress to various NLP tasks.\nDespite this progress, CL has not been applied to Arabic NLP to date. Nor is it\nclear how much benefits it could bring to particular classes of tasks such as\nthose involved in Arabic social meaning (e.g., sentiment analysis, dialect\nidentification, hate speech detection). In this work, we present a\ncomprehensive benchmark study of state-of-the-art supervised CL methods on a\nwide array of Arabic social meaning tasks. Through extensive empirical\nanalyses, we show that CL methods outperform vanilla finetuning on most tasks\nwe consider. We also show that CL can be data efficient and quantify this\nefficiency. Overall, our work allows us to demonstrate the promise of CL\nmethods, including in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khondaker_M/0/1/0/all/0/1\">Md Tawkat Islam Khondaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection. (arXiv:2210.12321v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12321","description":"<p>Neural networks have long been at the center of a debate around the cognitive\nmechanism by which humans process inflectional morphology. This debate has\ngravitated into NLP by way of the question: Are neural networks a feasible\naccount for human behavior in morphological inflection? We address that\nquestion by measuring the correlation between human judgments and neural\nnetwork probabilities for unknown word inflections. We test a larger range of\narchitectures than previously studied on two important tasks for the cognitive\nprocessing debate: English past tense, and German number inflection. We find\nevidence that the Transformer may be a better account of human behavior than\nLSTMs on these datasets, and that LSTM features known to increase inflection\naccuracy do not always result in more human-like behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiemerslage_A/0/1/0/all/0/1\">Adam Wiemerslage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Based Conditioned Variational Autoencoder for Dialogue Generation. (arXiv:2210.12326v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12326","description":"<p>In human dialogue, a single query may elicit numerous appropriate responses.\nThe Transformer-based dialogue model produces frequently occurring sentences in\nthe corpus since it is a one-to-one mapping function. CVAE is a technique for\nreducing generic replies. In this paper, we create a new dialogue model\n(CVAE-T) based on the Transformer with CVAE structure. We use a pre-trained MLM\nmodel to rewrite some key n-grams in responses to obtain a series of negative\nexamples, and introduce a regularization term during training to explicitly\nguide the latent variable in learning the semantic differences between each\npair of positive and negative examples. Experiments suggest that the method we\ndesign is capable of producing more informative replies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huihui Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R$^2$F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference. (arXiv:2210.12328v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12328","description":"<p>Document-level natural language inference (DOCNLI) is a new challenging task\nin natural language processing, aiming at judging the entailment relationship\nbetween a pair of hypothesis and premise documents. Current datasets and\nbaselines largely follow sentence-level settings, but fail to address the\nissues raised by longer documents. In this paper, we establish a general\nsolution, named Retrieval, Reading and Fusion (R2F) framework, and a new\nsetting, by analyzing the main challenges of DOCNLI: interpretability,\nlong-range dependency, and cross-sentence inference. The basic idea of the\nframework is to simplify document-level task into a set of sentence-level\ntasks, and improve both performance and interpretability with the power of\nevidence. For each hypothesis sentence, the framework retrieves evidence\nsentences from the premise, and reads to estimate its credibility. Then the\nsentence-level results are fused to judge the relationship between the\ndocuments. For the setting, we contribute complementary evidence and entailment\nlabel annotation on hypothesis sentences, for interpretability study. Our\nexperimental results show that R2F framework can obtain state-of-the-art\nperformance and is robust for diverse evidence retrieval methods. Moreover, it\ncan give more interpretable prediction results. Our model and code are released\nat https://github.com/phoenixsecularbird/R2F.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback. (arXiv:2210.12329v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12329","description":"<p>Recently, dataset-generation-based zero-shot learning has shown promising\nresults by training a task-specific model with a dataset synthesized from large\npre-trained language models (PLMs). The final task-specific model often\nachieves compatible or even better performance than PLMs under the zero-shot\nsetting, with orders of magnitude fewer parameters. However, synthetic datasets\nhave their drawbacks. They have long been suffering from low-quality issues\n(e.g., low informativeness and redundancy). This explains why the massive\nsynthetic data does not lead to better performance -- a scenario we would\nexpect in the human-labeled data. To improve the quality of dataset synthesis,\nwe propose a progressive zero-shot dataset generation framework, ProGen, which\nleverages the feedback from the task-specific model to guide the generation of\nnew training data via in-context examples. Extensive experiments on five text\nclassification datasets demonstrate the effectiveness of the proposed approach.\nWe also show ProGen achieves on-par or superior performance with only 1\\%\nsynthetic dataset size compared to baseline methods without in-context\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salience Allocation as Guidance for Abstractive Summarization. (arXiv:2210.12330v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12330","description":"<p>Abstractive summarization models typically learn to capture the salient\ninformation from scratch implicitly. Recent literature adds extractive\nsummaries as guidance for abstractive summarization models to provide hints of\nsalient content and achieves better performance. However, extractive summaries\nas guidance could be over strict, leading to information loss or noisy signals.\nFurthermore, it cannot easily adapt to documents with various abstractiveness.\nAs the number and allocation of salience content pieces vary, it is hard to\nfind a fixed threshold deciding which content should be included in the\nguidance. In this paper, we propose a novel summarization approach with a\nflexible and reliable salience guidance, namely SEASON (SaliencE Allocation as\nGuidance for Abstractive SummarizatiON). SEASON utilizes the allocation of\nsalience expectation to guide abstractive summarization and adapts well to\narticles in different abstractiveness. Automatic and human evaluations on two\nbenchmark datasets show that the proposed method is effective and reliable.\nEmpirical results on more than one million news articles demonstrate a natural\nfifteen-fifty salience split for news article sentences, providing a useful\ninsight for composing news articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided contrastive self-supervised pre-training for automatic speech recognition. (arXiv:2210.12335v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12335","description":"<p>Contrastive Predictive Coding (CPC) is a representation learning method that\nmaximizes the mutual information between intermediate latent representations\nand the output of a given model. It can be used to effectively initialize the\nencoder of an Automatic Speech Recognition (ASR) model. We present a novel\nmodification of CPC called Guided Contrastive Predictive Coding (GCPC). Our\nproposed method maximizes the mutual information between representations from a\nprior-knowledge model and the output of the model being pre-trained, allowing\nprior knowledge injection during pre-training. We validate our method on 3 ASR\ntasks: German, French and English. Our method outperforms CPC pre-training on\nall three datasets, reducing the Word Error Rate (WER) by 4.44%, 6.55% and\n15.43% relative on the German, French and English (Librispeech) tasks\nrespectively, compared to training from scratch, while CPC pre-training only\nbrings 2.96%, 1.01% and 14.39% relative WER reduction respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1\">Aparna Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhati_S/0/1/0/all/0/1\">Saurabhchand Bhati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maas_R/0/1/0/all/0/1\">Roland Maas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge. (arXiv:2210.12338v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12338","description":"<p>We propose a novel open-domain question answering (ODQA) framework for\nanswering single/multi-hop questions across heterogeneous knowledge sources.\nThe key novelty of our method is the introduction of the intermediary modules\ninto the current retriever-reader pipeline. Unlike previous methods that solely\nrely on the retriever for gathering all evidence in isolation, our intermediary\nperforms a chain of reasoning over the retrieved set. Specifically, our method\nlinks the retrieved evidence with its related global context into graphs and\norganizes them into a candidate list of evidence chains. Built upon pretrained\nlanguage models, our system achieves competitive performance on two ODQA\ndatasets, OTT-QA and NQ, against tables and passages from Wikipedia. In\nparticular, our model substantially outperforms the previous state-of-the-art\non OTT-QA with an exact match score of 47.3 (45 % relative gain).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training. (arXiv:2210.12339v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12339","description":"<p>Conventional autoregressive left-to-right (L2R) sequence generation faces two\nissues during decoding: limited to unidirectional target sequence modeling, and\nconstrained on strong local dependencies. To address the aforementioned\nproblem, we propose P$^3$LM, a probabilistically permuted prophet language\nmodel, which strengthens the modeling of bidirectional information and long\ntoken dependencies for sequence generation. Specifically, P$^3$LM learns to\ngenerate tokens in permuted order upon an order-aware transformer decoder, as\nwell as to generate the corresponding future $N$ tokens with a multi-stream\nattention mechanism. Extensive experiments are conducted on the GLGE benchmark,\nwhich includes four datasets for summarization, two for question generation,\none for conversational question answering, and one for dialog response\ngeneration, where P$^3$LM achieves state-of-the-art results compared with\nstrong publicly available generative pre-training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_J/0/1/0/all/0/1\">Jiangyong Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Arabic Language and Speech Tutor. (arXiv:2210.12346v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12346","description":"<p>In the past decade, we have observed a growing interest in using technologies\nsuch as artificial intelligence (AI), machine learning, and chatbots to provide\nassistance to language learners, especially in second language learning. By\nusing AI and natural language processing (NLP) and chatbots, we can create an\nintelligent self-learning environment that goes beyond multiple-choice\nquestions and/or fill in the blank exercises. In addition, NLP allows for\nlearning to be adaptive in that it offers more than an indication that an error\nhas occurred. It also provides a description of the error, uses linguistic\nanalysis to isolate the source of the error, and then suggests additional\ndrills to achieve optimal individualized learning outcomes. In this paper, we\npresent our approach for developing an Artificial Intelligence-based Arabic\nLanguage and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.\nThe AI-ALST system is an intelligent tutor that provides analysis and\nassessment of students learning the Moroccan dialect at University of Arizona\n(UA). The AI-ALST provides a self-learned environment to practice each lesson\nfor pronunciation training. In this paper, we present our initial experimental\nevaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum\ncoefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),\nattention mechanism, and a cost-based strategy for dealing with class-imbalance\nlearning. We evaluated our tutor on the word pronunciation of lesson 1 of the\nMoroccan Arabic dialect class. The experimental results show that the AI-ALST\ncan effectively and successfully detect pronunciation errors and evaluate its\nperformance by using F_1-score, accuracy, precision, and recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Sicong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharir_S/0/1/0/all/0/1\">Saleem Alharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariri_S/0/1/0/all/0/1\">Salim Hariri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satam_P/0/1/0/all/0/1\">Pratik Satam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiri_S/0/1/0/all/0/1\">Sonia Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbarki_A/0/1/0/all/0/1\">Abdessamad Mbarki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12353","description":"<p>While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Transport-based Policy for Simultaneous Translation. (arXiv:2210.12357v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12357","description":"<p>Simultaneous translation (ST) outputs translation while receiving the source\ninputs, and hence requires a policy to determine whether to translate a target\ntoken or wait for the next source token. The major challenge of ST is that each\ntarget token can only be translated based on the current received source\ntokens, where the received source information will directly affect the\ntranslation quality. So naturally, how much source information is received for\nthe translation of the current target token is supposed to be the pivotal\nevidence for the ST policy to decide between translating and waiting. In this\npaper, we treat the translation as information transport from source to target\nand accordingly propose an Information-Transport-based Simultaneous Translation\n(ITST). ITST quantifies the transported information weight from each source\ntoken to the current target token, and then decides whether to translate the\ntarget token according to its accumulated received information. Experiments on\nboth text-to-text ST and speech-to-text ST (a.k.a., streaming speech\ntranslation) tasks show that ITST outperforms strong baselines and achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models. (arXiv:2210.12360v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12360","description":"<p>Pre-trained multilingual language models show significant performance gains\nfor zero-shot cross-lingual model transfer on a wide range of natural language\nunderstanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,\npre-trained models are only fine-tuned on English data and tested on a variety\nof target languages. In this paper, we do cross-lingual evaluation on various\nNLU tasks (sentence classification, sequence labeling, question answering)\nusing prompt-tuning and compare it with fine-tuning. The results show that\nprompt tuning achieves much better cross-lingual transfer than fine-tuning\nacross datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt tuning can have better\ncross-lingual transferability of representations on downstream tasks with\nbetter aligned decision boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EnDex: Evaluation of Dialogue Engagingness at Scale. (arXiv:2210.12362v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12362","description":"<p>We propose EnDex, the first human-reaction based model to evaluate dialogue\nengagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED)\ncurated using a novel distant-supervision framework. Engagingness is a key\nmeasure that captures high-level quality of AI dialogue systems and closely\nreflects actual user experience. However, data shortage, plus the abstract and\nextensive definition of engagingness makes it challenging to develop an\nautomatic metric. Our work departs from mainstream approaches that use\nsynthetic negative examples to train binary classifiers, and instead, proposes\na solution using distant-supervision from human-reaction feedback. To support\nthe soundness of our EnDex metric, we offer a theoretical foundation for\nengagement, an extensive ablation study, and empirical evidence of high\ncorrelation on five engagingness related datasets. We will release code,\noff-the-shelf EnDex model, and a large-scale dataset upon paper publication to\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harel_Canada_F/0/1/0/all/0/1\">Fabrice Harel-Canada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_N/0/1/0/all/0/1\">Nischal Reddy Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction. (arXiv:2210.12364v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12364","description":"<p>Grammatical Error Correction (GEC) has been broadly applied in automatic\ncorrection and proofreading system recently. However, it is still immature in\nChinese GEC due to limited high-quality data from native speakers in terms of\ncategory and scale. In this paper, we present FCGEC, a fine-grained corpus to\ndetect, identify and correct the grammatical errors. FCGEC is a human-annotated\ncorpus with multiple references, consisting of 41,340 sentences collected\nmainly from multi-choice questions in public school Chinese examinations.\nFurthermore, we propose a Switch-Tagger-Generator (STG) baseline model to\ncorrect the grammatical errors in low-resource settings. Compared to other GEC\nbenchmark models, experimental results illustrate that STG outperforms them on\nour FCGEC. However, there exists a significant gap between benchmark models and\nhumans that encourages future models to bridge it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lvxiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianwang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiawei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiayu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Ming Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation. (arXiv:2210.12365v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12365","description":"<p>While counterfactual data augmentation offers a promising step towards robust\ngeneralization in natural language processing, producing a set of\ncounterfactuals that offer valuable inductive bias for models remains a\nchallenge. Most existing approaches for producing counterfactuals, manual or\nautomated, rely on small perturbations via minimal edits, resulting in\nsimplistic changes. We introduce NeuroCounterfactuals, designed as loose\ncounterfactuals, allowing for larger edits which result in naturalistic\ngenerations containing linguistic diversity, while still bearing similarity to\nthe original document. Our novel generative approach bridges the benefits of\nconstrained decoding, with those of language model adaptation for sentiment\nsteering. Training data augmentation with our generations results in both\nin-domain and out-of-domain improvements for sentiment classification,\noutperforming even manually curated counterfactuals, under select settings. We\nfurther present detailed analyses to show the advantages of\nNeuroCounterfactuals over approaches involving simple, minimal edits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_G/0/1/0/all/0/1\">Gadi Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation. (arXiv:2210.12367v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12367","description":"<p>Though model robustness has been extensively studied in language\nunderstanding, the robustness of Seq2Seq generation remains understudied. In\nthis paper, we conduct the first quantitative analysis on the robustness of\npre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq\nmodel (BART) is still vulnerable, which leads to significant degeneration in\nfaithfulness and informativeness for text generation tasks. This motivated us\nto further propose a novel adversarial augmentation framework, namely AdvSeq,\nfor generally improving faithfulness and informativeness of Seq2Seq models via\nenhancing their robustness. AdvSeq automatically constructs two types of\nadversarial augmentations during training, including implicit adversarial\nsamples by perturbing word representations and explicit adversarial samples by\nword swapping, both of which effectively improve Seq2Seq robustness. Extensive\nexperiments on three popular text generation tasks demonstrate that AdvSeq\nsignificantly improves both the faithfulness and informativeness of Seq2Seq\ngeneration under both automatic and human evaluation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples. (arXiv:2210.12374v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12374","description":"<p>Reasoning over tabular data requires both table structure understanding and a\nbroad set of table reasoning skills. Current models with table-specific\narchitectures and pre-training methods perform well on understanding table\nstructures, but they still struggle with tasks that require various table\nreasoning skills. In this work, we develop ReasTAP to show that high-level\ntable reasoning skills can be injected into models during pre-training without\na complex table-specific architecture design. We define 7 table reasoning\nskills, such as numerical operation, temporal comparison, and conjunction. Each\nreasoning skill is associated with one example generator, which synthesizes\nquestions over semi-structured tables according to the sampled templates. We\nmodel the table pre-training task as a sequence generation task and pre-train\nReasTAP to generate precise answers to the synthetic examples. ReasTAP is\nevaluated on four benchmarks covering three downstream tasks including: 1)\nWikiSQL and WTQ for Table Question Answering; 2) TabFact for Table Fact\nVerification; and 3) LogicNLG for Faithful Table-to-Text Generation.\nExperimental results demonstrate that ReasTAP achieves new state-of-the-art\nperformance on all benchmarks and delivers a significant improvement on\nlow-resource setting. Our code is publicly available at\nhttps://github.com/Yale-LILY/ReasTAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhenting Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling. (arXiv:2210.12378v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12378","description":"<p>Abstractive summarization models often generate inconsistent summaries\ncontaining factual errors or hallucinated content. Recent works focus on\ncorrecting factual errors in generated summaries via post-editing. Such\ncorrection models are trained using adversarial non-factual summaries\nconstructed using heuristic rules for injecting errors. However, generating\nnon-factual summaries using heuristics often does not generalize well to actual\nmodel errors. In this work, we propose to generate hard, representative\nsynthetic examples of non-factual summaries through infilling language models.\nWith this data, we train a more robust fact-correction model to post-edit the\nsummaries to improve factual consistency. Through quantitative and qualitative\nexperiments on two popular summarization datasets -- CNN/DM and XSum -- we show\nthat our approach vastly outperforms prior methods in correcting erroneous\nsummaries. Our model -- FactEdit -- improves factuality scores by over ~11\npoints on CNN/DM and over ~31 points on XSum on average across multiple\nsummarization models, producing more factual summaries while maintaining\ncompetitive summarization quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stance Detection and Open Research Avenues. (arXiv:2210.12383v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12383","description":"<p>This tutorial aims to cover the state-of-the-art on stance detection and\naddress open research avenues for interested researchers and practitioners.\nStance detection is a recent research topic where the stance towards a given\ntarget or target set is determined based on the given content and there are\nsignificant application opportunities of stance detection in various domains.\nThe tutorial comprises two parts where the first part outlines the fundamental\nconcepts, problems, approaches, and resources of stance detection, while the\nsecond part covers open research avenues and application areas of stance\ndetection. The tutorial will be a useful guide for researchers and\npractitioners of stance detection, social media analysis, information\nretrieval, and natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucuk_D/0/1/0/all/0/1\">Dilek K&#xfc;&#xe7;&#xfc;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_F/0/1/0/all/0/1\">Fazli Can</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition. (arXiv:2210.12391v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12391","description":"<p>African languages are spoken by over a billion people, but are\nunderrepresented in NLP research and development. The challenges impeding\nprogress include the limited availability of annotated datasets, as well as a\nlack of understanding of the settings where current methods are effective. In\nthis paper, we make progress towards solutions for these challenges, focusing\non the task of named entity recognition (NER). We create the largest\nhuman-annotated NER dataset for 20 African languages, and we study the behavior\nof state-of-the-art cross-lingual transfer methods in an Africa-centric\nsetting, demonstrating that the choice of source language significantly affects\nperformance. We show that choosing the best transfer language improves\nzero-shot F1 scores by an average of 14 points across 20 languages compared to\nusing English. Our results highlight the need for benchmark datasets and models\nthat cover typologically-diverse African languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1\">Michael Beukman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen H. Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabende_P/0/1/0/all/0/1\">Peter Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dione_C/0/1/0/all/0/1\">Cheikh M. Bamba Dione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukula_A/0/1/0/all/0/1\">Andiswa Bukula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabuya_R/0/1/0/all/0/1\">Rooweither Mabuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sibanda_B/0/1/0/all/0/1\">Blessing Sibanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalipe_G/0/1/0/all/0/1\">Godson Kalipe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbaye_D/0/1/0/all/0/1\">Derguene Mbaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Amelia Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabore_F/0/1/0/all/0/1\">Fatoumata Kabore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gitau_C/0/1/0/all/0/1\">Catherine Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkoh_Buabeng_E/0/1/0/all/0/1\">Edwin Munkoh-Buabeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koagne_V/0/1/0/all/0/1\">Victoire M. Koagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapo_A/0/1/0/all/0/1\">Allahsera Auguste Tapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macucwa_T/0/1/0/all/0/1\">Tebogo Macucwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mboning_E/0/1/0/all/0/1\">Elvis Mboning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakatumba_Nabende_J/0/1/0/all/0/1\">Joyce Nakatumba-Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokono_N/0/1/0/all/0/1\">Neo L. Mokono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Q. Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngoli_T/0/1/0/all/0/1\">Tatiana Moteu Ngoli</a>, et al. (1 additional author not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation. (arXiv:2210.12396v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12396","description":"<p>Adversarial Examples Detection (AED) is a crucial defense technique against\nadversarial attacks and has drawn increasing attention from the Natural\nLanguage Processing (NLP) community. Despite the surge of new AED methods, our\nstudies show that existing methods heavily rely on a shortcut to achieve good\nperformance. In other words, current search-based adversarial attacks in NLP\nstop once model predictions change, and thus most adversarial examples\ngenerated by those attacks are located near model decision boundaries. To\nsurpass this shortcut and fairly evaluate AED methods, we propose to test AED\nmethods with \\textbf{F}ar \\textbf{B}oundary (\\textbf{FB}) adversarial examples.\nExisting methods show worse than random guess performance under this scenario.\nTo overcome this limitation, we propose a new technique, \\textbf{ADDMU},\n\\textbf{a}dversary \\textbf{d}etection with \\textbf{d}ata and \\textbf{m}odel\n\\textbf{u}ncertainty, which combines two types of uncertainty estimation for\nboth regular and FB adversarial example detection. Our new method outperforms\nprevious methods by 3.6 and 6.0 \\emph{AUC} points under each scenario. Finally,\nour analysis shows that the two types of uncertainty provided by \\textbf{ADDMU}\ncan be leveraged to characterize adversarial examples and identify the ones\nthat contribute most to model's robustness in adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaASSIST: Robust Dialogue State Tracking with Meta Learning. (arXiv:2210.12397v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12397","description":"<p>Existing dialogue datasets contain lots of noise in their state annotations.\nSuch noise can hurt model training and ultimately lead to poor generalization\nperformance. A general framework named ASSIST has recently been proposed to\ntrain robust dialogue state tracking (DST) models. It introduces an auxiliary\nmodel to generate pseudo labels for the noisy training set. These pseudo labels\nare combined with vanilla labels by a common fixed weighting parameter to train\nthe primary DST model. Notwithstanding the improvements of ASSIST on DST,\ntuning the weighting parameter is challenging. Moreover, a single parameter\nshared by all slots and all instances may be suboptimal. To overcome these\nlimitations, we propose a meta learning-based framework MetaASSIST to\nadaptively learn the weighting parameter. Specifically, we propose three\nschemes with varying degrees of flexibility, ranging from slot-wise to both\nslot-wise and instance-wise, to convert the weighting parameter into learnable\nfunctions. These functions are trained in a meta-learning manner by taking the\nvalidation set as meta data. Experimental results demonstrate that all three\nschemes can achieve competitive performance. Most impressively, we achieve a\nstate-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_S/0/1/0/all/0/1\">Samuel Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Varifocal Question Generation for Fact-checking. (arXiv:2210.12400v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12400","description":"<p>Fact-checking requires retrieving evidence related to a claim under\ninvestigation. The task can be formulated as question generation based on a\nclaim, followed by question answering. However, recent question generation\napproaches assume that the answer is known and typically contained in a passage\ngiven as input, whereas such passages are what is being sought when verifying a\nclaim. In this paper, we present {\\it Varifocal}, a method that generates\nquestions based on different focal points within a given claim, i.e.\\ different\nspans of the claim and its metadata, such as its source and date. Our method\noutperforms previous work on a fact-checking question generation dataset on a\nwide range of automatic evaluation metrics. These results are corroborated by\nour manual evaluation, which indicates that our method generates more relevant\nand informative questions. We further demonstrate the potential of focal points\nin generating sets of clarification questions for product descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhangdie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text. (arXiv:2210.12401v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12401","description":"<p>Scientific action graphs extraction from materials synthesis procedures is\nimportant for reproducible research, machine automation, and material\nprediction. But the lack of annotated data has hindered progress in this field.\nWe demonstrate an effort to annotate Polycrystalline Materials Synthesis\nProcedures (PcMSP) from 305 open access scientific articles for the\nconstruction of synthesis action graphs. This is a new dataset for material\nscience information extraction that simultaneously contains the synthesis\nsentences extracted from the experimental paragraphs, as well as the entity\nmentions and intra-sentence relations. A two-step human annotation and\ninter-annotator agreement study guarantee the high quality of the PcMSP corpus.\nWe introduce four natural language processing tasks: sentence classification,\nnamed entity recognition, relation classification, and joint extraction of\nentities and relations. Comprehensive experiments validate the effectiveness of\nseveral state-of-the-art models for these challenges while leaving large space\nfor improvement. We also perform the error analysis and point out some unique\nchallenges that require further investigation. We will release our annotation\nscheme, the corpus, and codes to the research community to alleviate the\nscarcity of labeled data in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_Y/0/1/0/all/0/1\">Ya Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1\">Julia Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Stephen Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models. (arXiv:2210.12403v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12403","description":"<p>A wide range of NLP tasks benefit from the fine-tuning of pretrained language\nmodels (PLMs). However, a number of redundant parameters which contribute less\nto the downstream task are observed in a directly fine-tuned model. We consider\nthe gap between pretraining and downstream tasks hinders the training of these\nredundant parameters, and results in a suboptimal performance of the overall\nmodel. In this paper, we present PATS (Perturbation According To Sensitivity),\na noisy training mechanism which considers each parameter's importance in the\ndownstream task to help fine-tune PLMs. The main idea of PATS is to add bigger\nnoise to parameters with lower sensitivity and vice versa, in order to activate\nmore parameters' contributions to downstream tasks without affecting the\nsensitive ones much. Extensive experiments conducted on different tasks of the\nGLUE benchmark show PATS can consistently empower the fine-tuning of different\nsizes of PLMs, and the parameters in the well-performing models always have\nmore concentrated distributions of sensitivities, which experimentally proves\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yupeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. (arXiv:2210.12409v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12409","description":"<p>Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose DELLA, a Transformer-based recurrent VAE structure. DELLA\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that DELLA could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that DELLA achieves significantly improved diversity\nwhile maintaining satisfactory generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Shared Task on Gender Rewriting. (arXiv:2210.12410v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12410","description":"<p>In this paper, we present the results and findings of the Shared Task on\nGender Rewriting, which was organized as part of the Seventh Arabic Natural\nLanguage Processing Workshop. The task of gender rewriting refers to generating\nalternatives of a given sentence to match different target user gender contexts\n(e.g., female speaker with a male listener, a male speaker with a male\nlistener, etc.). This requires changing the grammatical gender (masculine or\nfeminine) of certain words referring to the users. In this task, we focus on\nArabic, a gender-marking morphologically rich language. A total of five teams\nfrom four countries participated in the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_O/0/1/0/all/0/1\">Ossama Obeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alrowili_S/0/1/0/all/0/1\">Sultan Alrowili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzeer_D/0/1/0/all/0/1\">Daliyah Alzeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshanqiti_K/0/1/0/all/0/1\">Khawlah M. Alshanqiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ElBakry_A/0/1/0/all/0/1\">Ahmed ElBakry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1\">Muhammad ElNokrashy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabr_M/0/1/0/all/0/1\">Mohamed Gabr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issam_A/0/1/0/all/0/1\">Abderrahmane Issam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qaddoumi_A/0/1/0/all/0/1\">Abdelrahim Qaddoumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_Shanker_K/0/1/0/all/0/1\">K. Vijay-Shanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyate_M/0/1/0/all/0/1\">Mahmoud Zyate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and Reliable Language Model. (arXiv:2210.12427v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12427","description":"<p>In knowledge distillation, a student model is trained with supervisions from\nboth knowledge from a teacher and observations drawn from a training data\ndistribution. Knowledge of a teacher is considered a subject that holds\ninter-class relations which send a meaningful supervision to a student; hence,\nmuch effort has been put to find such knowledge to be distilled. In this paper,\nwe explore a question that has been given little attention: \"when to distill\nsuch knowledge.\" The question is answered in our work with the concept of model\ncalibration; we view a teacher model not only as a source of knowledge but also\nas a gauge to detect miscalibration of a student. This simple and yet novel\nview leads to a hard gate knowledge distillation scheme that switches between\nlearning from a teacher model and training data. We verify the gating mechanism\nin the context of natural language generation at both the token-level and the\nsentence-level. Empirical comparisons with strong baselines show that hard gate\nknowledge distillation not only improves model generalization, but also\nsignificantly lowers model calibration error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems. (arXiv:2210.12429v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12429","description":"<p>Dialog systems are often designed or trained to output human-like responses.\nHowever, some responses may be impossible for a machine to truthfully say (e.g.\n\"that movie made me cry\"). Highly anthropomorphic responses might make users\nuncomfortable or implicitly deceive them into thinking they are interacting\nwith a human. We collect human ratings on the feasibility of approximately 900\ntwo-turn dialogs sampled from 9 diverse data sources. Ratings are for two\nhypothetical machine embodiments: a futuristic humanoid robot and a digital\nassistant. We find that for some data-sources commonly used to train dialog\nsystems, 20-30% of utterances are not viewed as possible for a machine. Rating\nis marginally affected by machine embodiment. We explore qualitative and\nquantitative reasons for these ratings. Finally, we build classifiers and\nexplore how modeling configuration might affect output permissibly, and discuss\nimplications for building less falsely anthropomorphic dialog systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1\">David Gros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Unified M-Tree Coding Solver for MathWord Problem. (arXiv:2210.12432v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12432","description":"<p>As one of the challenging NLP tasks, designing math word problem (MWP)\nsolvers has attracted increasing research attention for the past few years. In\nprevious work, models designed by taking into account the properties of the\nbinary tree structure of mathematical expressions at the output side have\nachieved better performance. However, the expressions corresponding to a MWP\nare often diverse (e.g., $n_1+n_2 \\times n_3-n_4$, $n_3\\times n_2-n_4+n_1$,\netc.), and so are the corresponding binary trees, which creates difficulties in\nmodel learning due to the non-deterministic output space. In this paper, we\npropose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies\na tree with any M branches (M-tree) to unify the output structures. To learn\nthe M-tree, we use a mapping to convert the M-tree into the M-tree codes, where\ncodes store the information of the paths from tree root to leaf nodes and the\ninformation of leaf nodes themselves, and then devise a Sequence-to-Code\n(seq2code) model to generate the codes. Experimental results on the widely used\nMAWPS and Math23K datasets have demonstrated that SUMC-Solver not only\noutperforms several state-of-the-art models under similar experimental settings\nbut also performs much better under low-resource conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiangzhou Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xin-Yu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Prompt Tuning for Relation Classification. (arXiv:2210.12435v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12435","description":"<p>Using prompts to explore the knowledge contained within pre-trained language\nmodels for downstream tasks has now become an active topic. Current prompt\ntuning methods mostly convert the downstream tasks to masked language modeling\nproblems by adding cloze-style phrases and mapping all labels to verbalizations\nwith fixed length, which has proven effective for tasks with simple label\nspaces. However, when applied to relation classification exhibiting complex\nlabel spaces, vanilla prompt tuning methods may struggle with label\nverbalizations with arbitrary lengths due to rigid prompt restrictions.\nInspired by the text infilling task for pre-training generative models that can\nflexibly predict missing spans, we propose a novel generative prompt tuning\nmethod to reformulate relation classification as an infilling problem, which\nfrees our approach from limitations of current prompt based approaches and thus\nfully exploits rich semantics of entity and relation types. In addition, we\ndesign entity-guided decoding and discriminative relation scoring to generate\nand align relations effectively and efficiently during inference. Extensive\nexperiments under fully supervised settings and low-resource settings\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shengkun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance. (arXiv:2210.12437v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12437","description":"<p>Summarizing legal decisions requires the expertise of law practitioners,\nwhich is both time- and cost-intensive. This paper presents techniques for\nextractive summarization of legal decisions in a low-resource setting using\nlimited expert annotated data. We test a set of models that locate relevant\ncontent using a sequential model and tackle redundancy by leveraging maximal\nmarginal relevance to compose summaries. We also demonstrate an implicit\napproach to help train our proposed models generate more informative summaries.\nOur multi-task learning model variant leverages rhetorical role identification\nas an auxiliary task to further improve the summarizer. We perform extensive\nexperiments on datasets containing legal decisions from the US Board of\nVeterans' Appeals and conduct quantitative and expert-ranked evaluations of our\nmodels. Our results show that the proposed approaches can achieve ROUGE scores\nvis-\\`a-vis expert extracted summaries that match those achieved by\ninter-annotator comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Abhishek Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shanshan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Temporal Article Grounding. (arXiv:2210.12444v1 [cs.CV])","link":"http://arxiv.org/abs/2210.12444","description":"<p>Given a long untrimmed video and natural language queries, video grounding\n(VG) aims to temporally localize the semantically-aligned video segments.\nAlmost all existing VG work holds two simple but unrealistic assumptions: 1)\nAll query sentences can be grounded in the corresponding video. 2) All query\nsentences for the same video are always at the same semantic scale.\nUnfortunately, both assumptions make today's VG models fail to work in\npractice. For example, in real-world multimodal assets (eg, news articles),\nmost of the sentences in the article can not be grounded in their affiliated\nvideos, and they typically have rich hierarchical relations (ie, at different\nsemantic scales). To this end, we propose a new challenging grounding task:\nWeakly-Supervised temporal Article Grounding (WSAG). Specifically, given an\narticle and a relevant video, WSAG aims to localize all ``groundable''\nsentences to the video, and these sentences are possibly at different semantic\nscales. Accordingly, we collect the first WSAG dataset to facilitate this task:\nYouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow\narticles and plentiful YouTube videos. In addition, we propose a simple but\neffective method DualMIL for WSAG, which consists of a two-level MIL loss and a\nsingle-/cross- sentence constraint loss. These training objectives are\ncarefully designed for these relaxed assumptions. Extensive ablations have\nverified the effectiveness of DualMIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1\">Hammad Ayyubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Generalization for AMR Parsing. (arXiv:2210.12445v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12445","description":"<p>Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph\nfrom textual input. Recently, there has been notable growth in AMR parsing\nperformance. However, most existing work focuses on improving the performance\nin the specific domain, ignoring the potential domain dependence of AMR parsing\nsystems. To address this, we extensively evaluate five representative AMR\nparsers on five domains and analyze challenges to cross-domain AMR parsing. We\nobserve that challenges to cross-domain AMR parsing mainly arise from the\ndistribution shift of words and AMR concepts. Based on our observation, we\ninvestigate two approaches to reduce the domain distribution divergence of text\nand AMR features, respectively. Experimental results on two out-of-domain test\nsets show the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning. (arXiv:2210.12459v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12459","description":"<p>Knowledge-grounded conversation (KGC) shows excellent potential to deliver an\nengaging and informative response. However, existing approaches emphasize\nselecting one golden knowledge given a particular dialogue context, overlooking\nthe one-to-many phenomenon in dialogue. As a result, the existing paradigm\nlimits the diversity of knowledge selection and generation. To this end, we\nestablish a multi-reference KGC dataset and propose a series of metrics to\nsystematically assess the one-to-many efficacy of existing KGC models.\nFurthermore, to extend the hypothesis space of knowledge selection to enhance\nthe mapping relationship between multiple knowledge and multiple responses, we\ndevise a span-based variational model and optimize the model in a wake-sleep\nstyle with an ameliorated evidence lower bound objective to learn the\none-to-many generalization. Both automatic and human evaluations demonstrate\nthe efficacy of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tingchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation. (arXiv:2210.12460v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12460","description":"<p>We study video-grounded dialogue generation, where a response is generated\nbased on the dialogue context and the associated video. The primary challenges\nof this task lie in (1) the difficulty of integrating video data into\npre-trained language models (PLMs) which presents obstacles to exploiting the\npower of large-scale pre-training; and (2) the necessity of taking into account\nthe complementarity of various modalities throughout the reasoning process.\nAlthough having made remarkable progress in video-grounded dialogue generation,\nexisting methods still fall short when it comes to integrating with PLMs in a\nway that allows information from different modalities to complement each other.\nTo alleviate these issues, we first propose extracting pertinent information\nfrom videos and turning it into reasoning paths that are acceptable to PLMs.\nAdditionally, we propose a multi-agent reinforcement learning method to\ncollaboratively perform reasoning on different modalities (i.e., video and\ndialogue context). Empirical experiment results on two public datasets indicate\nthat the proposed model can significantly outperform state-of-the-art models by\nlarge margins on both automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenshuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure. (arXiv:2210.12461v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12461","description":"<p>With the availability of massive general-domain dialogue data, pre-trained\ndialogue generation appears to be super appealing to transfer knowledge from\nthe general domain to downstream applications. In most existing work, such\ntransferable ability is mainly obtained by fitting a large model with hundreds\nof millions of parameters on massive data in an exhaustive way, leading to\ninefficient running and poor interpretability. This paper proposes a novel\ndialogue generation model with a latent structure that is easily transferable\nfrom the general domain to downstream tasks in a lightweight and transparent\nway. Experiments on two benchmarks validate the effectiveness of the proposed\nmodel. Thanks to the transferable latent structure, our model is able to yield\nbetter dialogue responses than four strong baselines in terms of both automatic\nand human evaluations, and our model with about 22% parameters particularly\ndelivers a 5x speedup in running time compared with the strongest baseline.\nMoreover, the proposed model is explainable by interpreting the discrete latent\nvariables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tingchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention. (arXiv:2210.12463v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12463","description":"<p>One of the key challenges of automatic story generation is how to generate a\nlong narrative that can maintain fluency, relevance, and coherence. Despite\nrecent progress, current story generation systems still face the challenge of\nhow to effectively capture contextual and event features, which has a profound\nimpact on a model's generation performance. To address these challenges, we\npresent EtriCA, a novel neural generation model, which improves the relevance\nand coherence of the generated stories through residually mapping context\nfeatures to event sequences with a cross-attention mechanism. Such a feature\ncapturing mechanism allows our model to better exploit the logical relatedness\nbetween events when generating stories. Extensive experiments based on both\nautomatic and human evaluations show that our model significantly outperforms\nstate-of-the-art baselines, demonstrating the effectiveness of our model in\nleveraging context and event features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Henglin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts. (arXiv:2210.12467v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12467","description":"<p>Despite tremendous progress in automatic summarization, state-of-the-art\nmethods are predominantly trained to excel in summarizing short newswire\narticles, or documents with strong layout biases such as scientific articles or\ngovernment reports. Efficient techniques to summarize financial documents,\nincluding facts and figures, have largely been unexplored, majorly due to the\nunavailability of suitable datasets. In this work, we present ECTSum, a new\ndataset with transcripts of earnings calls (ECTs), hosted by publicly traded\ncompanies, as documents, and short experts-written telegram-style bullet point\nsummaries derived from corresponding Reuters articles. ECTs are long\nunstructured documents without any prescribed length limit or format. We\nbenchmark our dataset with state-of-the-art summarizers across various metrics\nevaluating the content quality and factual consistency of the generated\nsummaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to\ngenerate a set of bullet points that precisely capture the important facts\ndiscussed in the calls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rajdeep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohra_A/0/1/0/all/0/1\">Abhinav Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Akash Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Soumya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_M/0/1/0/all/0/1\">Manjunath Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Afreen Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shivani Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1\">Koustuv Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoSense: Commonsense Reasoning with Discourse Connectives. (arXiv:2210.12478v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12478","description":"<p>We present DiscoSense, a benchmark for commonsense reasoning via\nunderstanding a wide variety of discourse connectives. We generate compelling\ndistractors in DiscoSense using Conditional Adversarial Filtering, an extension\nof Adversarial Filtering that employs conditional generation. We show that\nstate-of-the-art pre-trained language models struggle to perform well on\nDiscoSense, which makes this dataset ideal for evaluating next-generation\ncommonsense reasoning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_V/0/1/0/all/0/1\">Vincent Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser. (arXiv:2210.12484v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12484","description":"<p>This work proposes a syntax-enhanced grammatical error correction (GEC)\napproach named SynGEC that effectively incorporates dependency syntactic\ninformation into the encoder part of GEC models. The key challenge for this\nidea is that off-the-shelf parsers are unreliable when processing ungrammatical\nsentences. To confront this challenge, we propose to build a tailored\nGEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First,\nwe design an extended syntax representation scheme that allows us to represent\nboth grammatical errors and syntax in a unified tree structure. Then, we obtain\nparse trees of the source incorrect sentences by projecting trees of the target\ncorrect sentences. Finally, we train GOPar with such projected trees. For GEC,\nwe employ the graph convolution network to encode source-side syntactic\ninformation produced by GOPar, and fuse them with the outputs of the\nTransformer encoder. Experiments on mainstream English and Chinese GEC datasets\nshow that our proposed SynGEC approach consistently and substantially\noutperforms strong baselines and achieves competitive performance. Our code and\ndata are all publicly available at https://github.com/HillZhang1999/SynGEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DANLI: Deliberative Agent for Following Natural Language Instructions. (arXiv:2210.12485v1 [cs.AI])","link":"http://arxiv.org/abs/2210.12485","description":"<p>Recent years have seen an increasing amount of work on embodied AI agents\nthat can perform tasks by following human language instructions. However, most\nof these agents are reactive, meaning that they simply learn and imitate\nbehaviors encountered in the training data. These reactive agents are\ninsufficient for long-horizon complex tasks. To address this limitation, we\npropose a neuro-symbolic deliberative agent that, while following language\ninstructions, proactively applies reasoning and planning based on its neural\nand symbolic representations acquired from past experience (e.g., natural\nlanguage and egocentric vision). We show that our deliberative agent achieves\ngreater than 70% improvement over reactive baselines on the challenging TEACh\nbenchmark. Moreover, the underlying reasoning and planning processes, together\nwith our modular framework, offer impressive transparency and explainability to\nthe behaviors of the agent. This enables an in-depth understanding of the\nagent's capabilities, which shed light on challenges and opportunities for\nfuture embodied agents for instruction following. The code is available at\nhttps://github.com/sled-group/DANLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devraj_N/0/1/0/all/0/1\">Nikhil Devraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yuwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure. (arXiv:2210.12487v1 [cs.AI])","link":"http://arxiv.org/abs/2210.12487","description":"<p>In this paper, we propose a comprehensive benchmark to investigate models'\nlogical reasoning capabilities in complex real-life scenarios. Current\nexplanation datasets often employ synthetic data with simple reasoning\nstructures. Therefore, it cannot express more complex reasoning processes, such\nas the rebuttal to a reasoning step and the degree of certainty of the\nevidence. To this end, we propose a comprehensive logical reasoning explanation\nform. Based on the multi-hop chain of reasoning, the explanation form includes\nthree main components: (1) The condition of rebuttal that the reasoning node\ncan be challenged; (2) Logical formulae that uncover the internal texture of\nreasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The\nfine-grained structure conforms to the real logical reasoning scenario, better\nfitting the human cognitive process but, simultaneously, is more challenging\nfor the current models. We evaluate the current best models' performance on\nthis new explanation form. The experimental results show that generating\nreasoning graphs remains a challenging task for current models, even with the\nhelp of giant pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU. (arXiv:2210.12499v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12499","description":"<p>Curriculum Learning (CL) is a technique of training models via ranking\nexamples in a typically increasing difficulty trend with the aim of\naccelerating convergence and improving generalisability. Current approaches for\nNatural Language Understanding (NLU) tasks use CL to improve in-distribution\ndata performance often via heuristic-oriented or task-agnostic difficulties. In\nthis work, instead, we employ CL for NLU by taking advantage of training\ndynamics as difficulty metrics, i.e., statistics that measure the behavior of\nthe model at hand on specific task-data instances during training and propose\nmodifications of existing CL schedulers based on these statistics. Differently\nfrom existing works, we focus on evaluating models on in-distribution (ID),\nout-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer\ndatasets. We show across several NLU tasks that CL with training dynamics can\nresult in better performance mostly on zero-shot cross-lingual transfer and OOD\nsettings with improvements up by 8.5% in certain cases. Overall, experiments\nindicate that training dynamics can lead to better performing models with\nsmoother training compared to other difficulty metrics while being 20% faster\non average. In addition, through analysis we shed light on the correlations of\ntask-specific versus task-agnostic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christopoulou_F/0/1/0/all/0/1\">Fenia Christopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents. (arXiv:2210.12511v1 [cs.AI])","link":"http://arxiv.org/abs/2210.12511","description":"<p>In the real world, autonomous driving agents navigate in highly dynamic\nenvironments full of unexpected situations where pre-trained models are\nunreliable. In these situations, what is immediately available to vehicles is\noften only human operators. Empowering autonomous driving agents with the\nability to navigate in a continuous and dynamic environment and to communicate\nwith humans through sensorimotor-grounded dialogue becomes critical. To this\nend, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a\nnovel interactive simulation platform that enables the creation of unexpected\nsituations on the fly to support empirical studies on situated communication\nwith autonomous driving agents. Based on this platform, we created the Situated\nDialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of\n8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed\naudio. SDN is developed to evaluate the agent's ability to predict dialogue\nmoves from humans as well as generate its own dialogue moves and physical\nnavigation actions. We further developed a transformer-based baseline model for\nthese SDN tasks. Our empirical results indicate that language guided-navigation\nin a highly dynamic environment is an extremely difficult task for end-to-end\nmodels. These results will provide insight towards future work on robust\nautonomous driving agents. The DOROTHIE platform, SDN benchmark, and code for\nthe baseline model are available at https://github.com/sled-group/DOROTHIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanDerPloeg_B/0/1/0/all/0/1\">Ben VanDerPloeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yidong_H/0/1/0/all/0/1\">Huang Yidong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eui-In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gervits_F/0/1/0/all/0/1\">Felix Gervits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marge_M/0/1/0/all/0/1\">Matthew Marge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring The Landscape of Distributional Robustness for Question Answering Models. (arXiv:2210.12517v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12517","description":"<p>We conduct a large empirical evaluation to investigate the landscape of\ndistributional robustness in question answering. Our investigation spans over\n350 models and 16 question answering datasets, including a diverse set of\narchitectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter\ntuning, in-context learning, etc.). We find that, in many cases, model\nvariations do not affect robustness and in-distribution performance alone\ndetermines out-of-distribution performance. Moreover, our findings indicate\nthat i) zero-shot and in-context learning methods are more robust to\ndistribution shifts than fully fine-tuned models; ii) few-shot prompt\nfine-tuned models exhibit better robustness than few-shot fine-tuned span\nprediction models; iii) parameter-efficient and robustness enhancing training\nmethods provide no significant robustness improvements. In addition, we\npublicly release all evaluations to encourage researchers to further analyze\nrobustness trends for question answering models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1\">Anas Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMPriors: Pre-Trained Language Models as Task-Specific Priors. (arXiv:2210.12530v1 [cs.LG])","link":"http://arxiv.org/abs/2210.12530","description":"<p>Particularly in low-data regimes, an outstanding challenge in machine\nlearning is developing principled techniques for augmenting our models with\nsuitable priors. This is to encourage them to learn in ways that are compatible\nwith our understanding of the world. But in contrast to generic priors such as\nshrinkage or sparsity, we draw inspiration from the recent successes of\nlarge-scale language models (LMs) to construct task-specific priors distilled\nfrom the rich knowledge of LMs. Our method, Language Model Priors (LMPriors),\nincorporates auxiliary natural language metadata about the task -- such as\nvariable names and descriptions -- to encourage downstream model outputs to be\nconsistent with the LM's common-sense reasoning based on the metadata.\nEmpirically, we demonstrate that LMPriors improve model performance in settings\nwhere such natural language descriptions are available, and perform well on\nseveral tasks that benefit from such prior knowledge, such as feature\nselection, causal inference, and safe reinforcement learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kristy Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1\">Chris Cundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjari Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts. (arXiv:2210.12531v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12531","description":"<p>Crises such as the COVID-19 pandemic continuously threaten our world and\nemotionally affect billions of people worldwide in distinct ways. Understanding\nthe triggers leading to people's emotions is of crucial importance. Social\nmedia posts can be a good source of such analysis, yet these texts tend to be\ncharged with multiple emotions, with triggers scattering across multiple\nsentences. This paper takes a novel angle, namely, emotion detection and\ntrigger summarization, aiming to both detect perceived emotions in text, and\nsummarize events and their appraisals that trigger each emotion. To support\nthis goal, we introduce CovidET (Emotions and their Triggers during Covid-19),\na dataset of ~1,900 English Reddit posts related to COVID-19, which contains\nmanual annotations of perceived emotions and abstractive summaries of their\ntriggers described in the post. We develop strong baselines to jointly detect\nemotions and summarize emotion triggers. Our analyses show that CovidET\npresents new challenges in emotion-specific summarization, as well as\nmulti-emotion detection in long social media posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hongli Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosea_T/0/1/0/all/0/1\">Tiberiu Sosea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching. (arXiv:2210.12540v1 [cs.CL])","link":"http://arxiv.org/abs/2210.12540","description":"<p>Accurate alignment between languages is fundamental for improving\ncross-lingual pre-trained language models (XLMs). Motivated by the natural\nphenomenon of code-switching (CS) in multilingual speakers, CS has been used as\nan effective data augmentation method that offers language alignment at word-\nor phrase-level, in contrast to sentence-level via parallel instances. Existing\napproaches either use dictionaries or parallel sentences with word-alignment to\ngenerate CS data by randomly switching words in a sentence. However, such\nmethods can be suboptimal as dictionaries disregard semantics, and syntax might\nbecome invalid after random word switching. In this work, we propose EntityCS,\na method that focuses on Entity-level Code-Switching to capture fine-grained\ncross-lingual semantics without corrupting syntax. We use Wikidata and the\nEnglish Wikipedia to construct an entity-centric CS corpus by switching\nentities to their counterparts in other languages. We further propose\nentity-oriented masking strategies during intermediate model training on the\nEntityCS corpus for improving entity prediction. Evaluation of the trained\nmodels on four entity-centric downstream tasks shows consistent improvements\nover the baseline with a notable increase of 10% in Fact Retrieval. We release\nthe corpus and models to assist research on code-switching and enriching XLMs\nwith external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christopoulou_F/0/1/0/all/0/1\">Fenia Christopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration. (arXiv:2012.15375v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15375","description":"<p>Persuasion dialogue systems reflect the machine's ability to make strategic\nmoves beyond verbal communication, and therefore differentiate themselves from\ntask-oriented or open-domain dialogue systems and have their own unique values.\nHowever, the repetition and inconsistency problems still persist in dialogue\nresponse generation and could substantially impact user experience and impede\nthe persuasion outcome. Besides, although reinforcement learning (RL)\napproaches have achieved big success in strategic tasks such as games, they\nrequire a sophisticated user simulator to provide real-time feedback to the\ndialogue system, which limits the application of RL on persuasion dialogues. To\naddress these issues towards a better persuasion dialogue system, we apply RL\nto refine a language model baseline without user simulators, and distill\nsentence-level information about repetition, inconsistency, and task relevance\nthrough rewards. Moreover, to better accomplish the persuasion task, the model\nlearns from human demonstration to imitate human persuasion behavior and\nselects the most persuasive responses. Experiments show that our model\noutperforms previous state-of-the-art dialogue models on both automatic metrics\nand human evaluation results on a donation persuasion task, and generates more\ndiverse, consistent and persuasive conversations according to the user\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poolingformer: Long Document Modeling with Pooling Attention. (arXiv:2105.04371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04371","description":"<p>In this paper, we introduce a two-level attention schema, Poolingformer, for\nlong document modeling. Its first level uses a smaller sliding window pattern\nto aggregate information from neighbors. Its second level employs a larger\nwindow to increase receptive fields with pooling attention to reduce both\ncomputational cost and memory consumption. We first evaluate Poolingformer on\ntwo long sequence QA tasks: the monolingual NQ and the multilingual TyDi QA.\nExperimental results show that Poolingformer sits atop three official\nleaderboards measured by F1, outperforming previous state-of-the-art models by\n1.9 points (79.8 vs. 77.9) on NQ long answer, 1.9 points (79.5 vs. 77.6) on\nTyDi QA passage answer, and 1.6 points (67.6 vs. 66.0) on TyDi QA minimal\nanswer. We further evaluate Poolingformer on a long sequence summarization\ntask. Experimental results on the arXiv benchmark continue to demonstrate its\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient (Soft) Q-Learning for Text Generation with Limited Good Data. (arXiv:2106.07704v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07704","description":"<p>Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many emerging applications, such as generating\nadversarial attacks or generating prompts to control language models.\nReinforcement learning (RL) on the other hand offers a more flexible solution\nby allowing users to plug in arbitrary task metrics as reward. Yet previous RL\nalgorithms for text generation, such as policy gradient (on-policy RL) and\nQ-learning (off-policy RL), are often notoriously inefficient or unstable to\ntrain due to the large sequence space and the sparse reward received only at\nthe end of sequences. In this paper, we introduce a new RL formulation for text\ngeneration from the soft Q-learning (SQL) perspective. It enables us to draw\nfrom the latest RL advances, such as path consistency learning, to combine the\nbest of on-/off-policy updates, and learn effectively from sparse reward. We\napply the approach to a wide range of novel text generation tasks, including\nlearning from noisy/negative examples, adversarial attacks, and prompt\ngeneration. Experiments show our approach consistently outperforms both\ntask-specialized algorithms and the previous RL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PicTalky: Augmentative and Alternative Communication Software for Language Developmental Disabilities. (arXiv:2109.12941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12941","description":"<p>Augmentative and alternative communication (AAC) is a practical means of\ncommunication for people with language disabilities. In this study, we propose\nPicTalky, which is an AI-based AAC system that helps children with language\ndevelopmental disabilities to improve their communication skills and language\ncomprehension abilities. PicTalky can process both text and pictograms more\naccurately by connecting a series of neural-based NLP modules. Moreover, we\nperform quantitative and qualitative analyses on the essential features of\nPicTalky. It is expected that those suffering from language problems will be\nable to express their intentions or desires more easily and improve their\nquality of life by using this service. We have made the models freely available\nalongside a demonstration of the Web interface. Furthermore, we implemented\nrobotics AAC for the first time by applying PicTalky to the NAO robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kisu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models. (arXiv:2110.08426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08426","description":"<p>Pre-trained encoder-decoder transformer architectures have become\nincreasingly popular recently with the advent of T5 models. T5 has also become\nmore favorable over other architectures like BERT due to the amount of data\nthat it is pre-trained on, increased scale of model parameter sizes and easy\napplicability to a diverse set of tasks due to the generative nature of the\nmodel. While being able to generalize to a wide variety of tasks, it is not\nclear that encoder-decoder architectures are the most efficient for fine-tuning\ntasks that don't require auto-regressive decoding. In this work, we study\nfine-tuning pre-trained encoder-decoder models for tasks such as\nclassification, multi-label classification, and structured prediction. We\npropose \\textbf{EncT5}, a framework for these problems, and illustrate\ninstantiations for these tasks. Our experiment results show that EncT5 has\nadvantages over T5 such as efficiency and usability out performs BERT when\nevaluated on publicly available pre-trained checkpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Terry Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shihang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KPDrop: Improving Absent Keyphrase Generation. (arXiv:2112.01476v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01476","description":"<p>Keyphrase generation is the task of generating phrases (keyphrases) that\nsummarize the main topics of a given document. Keyphrases can be either present\nor absent from the given document. While the extraction of present keyphrases\nhas received much attention in the past, only recently a stronger focus has\nbeen placed on the generation of absent keyphrases. However, generating absent\nkeyphrases is challenging; even the best methods show only a modest degree of\nsuccess. In this paper, we propose a model-agnostic approach called keyphrase\ndropout (or KPDrop) to improve absent keyphrase generation. In this approach,\nwe randomly drop present keyphrases from the document and turn them into\nartificial absent keyphrases during training. We test our approach extensively\nand show that it consistently improves the absent performance of strong\nbaselines in both supervised and resource-constrained semi-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seoyeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_T/0/1/0/all/0/1\">Tuhin Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings. (arXiv:2112.04871v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2112.04871","description":"<p>Learning the embeddings of knowledge graphs (KG) is vital in artificial\nintelligence, and can benefit various downstream applications, such as\nrecommendation and question answering. In recent years, many research efforts\nhave been proposed for knowledge graph embedding (KGE). However, most previous\nKGE methods ignore the semantic similarity between the related entities and\nentity-relation couples in different triples since they separately optimize\neach triple with the scoring function. To address this problem, we propose a\nsimple yet efficient contrastive learning framework for tensor decomposition\nbased (TDB) KGE, which can shorten the semantic distance of the related\nentities and entity-relation couples in different triples and thus improve the\nperformance of KGE. We evaluate our proposed method on three standard KGE\ndatasets: WN18RR, FB15k-237 and YAGO3-10. Our method can yield some new\nstate-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR\ndataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8%\nHits@1 on the YAGO3-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wentao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup. (arXiv:2112.06204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06204","description":"<p>Training a model to provide natural language explanations (NLEs) for its\npredictions usually requires the acquisition of task-specific NLEs, which is\ntime- and resource-consuming. A potential solution is the few-shot\nout-of-domain transfer of NLEs from a parent task with many NLEs to a child\ntask. In this work, we examine the setup in which the child task has few NLEs\nbut abundant labels. We establish four few-shot transfer learning methods that\ncover the possible fine-tuning combinations of the labels and NLEs for the\nparent and child tasks. We transfer explainability from a large natural\nlanguage inference dataset (e-SNLI) separately to two child tasks: (1) hard\ncases of pronoun resolution, where we introduce the small-e-WinoGrande dataset\nof NLEs on top of the WinoGrande dataset, and (2)~commonsense validation\n(ComVE). Our results demonstrate that the parent task helps with NLE generation\nand we establish the best methods for this setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yordanov_Y/0/1/0/all/0/1\">Yordan Yordanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Questions Generate Named Entity Recognition Datasets. (arXiv:2112.08808v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08808","description":"<p>Recent named entity recognition (NER) models often rely on human-annotated\ndatasets, requiring the significant engagement of professional knowledge on the\ntarget domain and entities. This research introduces an ask-to-generate\napproach that automatically generates NER datasets by asking questions in\nsimple natural language to an open-domain question answering system (e.g.,\n\"Which disease?\"). Despite using fewer in-domain resources, our models, solely\ntrained on the generated datasets, largely outperform strong low-resource\nmodels by an average F1 score of 19.5 for six popular NER benchmarks.\nFurthermore, our models provide competitive performance with rich-resource\nmodels that additionally leverage in-domain dictionaries provided by domain\nexperts. In few-shot NER, we outperform the previous best model by an F1 score\nof 5.2 on three benchmarks and achieve new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning with Multilingual Language Models. (arXiv:2112.10668v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10668","description":"<p>Large-scale generative language models such as GPT-3 are competitive few-shot\nlearners. While these models are known to be able to jointly represent many\ndifferent languages, their training data is dominated by English, potentially\nlimiting their cross-lingual generalization. In this work, we train\nmultilingual generative language models on a corpus covering a diverse set of\nlanguages, and study their few- and zero-shot learning capabilities in a wide\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\nthe art in few-shot learning in more than 20 representative languages,\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\noutperforms GPT-3 on 171 out of 182 directions with 32 training examples, while\nsurpassing the official supervised baseline in 45 directions. We conduct an\nin-depth analysis of different multilingual prompting approaches, showing in\nparticular that strong few-shot learning performance across languages can be\nachieved via cross-lingual transfer through both templates and demonstration\nexamples. Finally, we evaluate our models in social value tasks such as hate\nspeech detection in five languages and find it has limitations similar to\ncomparable sized GPT-3 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OHoro_B/0/1/0/all/0/1\">Brian O&#x27;Horo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jeff Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation. (arXiv:2201.02009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02009","description":"<p>While end-to-end neural machine translation (NMT) has achieved impressive\nprogress, noisy input usually leads models to become fragile and unstable.\nGenerating adversarial examples as the augmented data has been proved to be\nuseful to alleviate this problem. Existing methods for adversarial example\ngeneration (AEG) are word-level or character-level, which ignore the ubiquitous\nphrase structure. In this paper, we propose a Phrase-level Adversarial Example\nGeneration (PAEG) framework to enhance the robustness of the translation model.\nOur method further improves the gradient-based word-level AEG method by\nadopting a phrase-level substitution strategy. We verify our method on three\nbenchmarks, including LDC Chinese-English, IWSLT14 German-English, and WMT14\nEnglish-German tasks. Experimental results demonstrate that our approach\nsignificantly improves translation performance and robustness to noise compared\nto previous strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Juncheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unobserved Local Structures Make Compositional Generalization Hard. (arXiv:2201.05899v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05899","description":"<p>While recent work has convincingly showed that sequence-to-sequence models\nstruggle to generalize to new compositions (termed compositional\ngeneralization), little is known on what makes compositional generalization\nhard on a particular test instance. In this work, we investigate what are the\nfactors that make generalization to certain test instances challenging. We\nfirst substantiate that indeed some examples are more difficult than others by\nshowing that different models consistently fail or succeed on the same test\ninstances. Then, we propose a criterion for the difficulty of an example: a\ntest instance is hard if it contains a local structure that was not observed at\ntraining time. We formulate a simple decision rule based on this criterion and\nempirically show it predicts instance-level generalization well across 5\ndifferent semantic parsing datasets, substantially better than alternative\ndecision rules. Last, we show local structures can be leveraged for creating\ndifficult adversarial compositional splits and also to improve compositional\ngeneralization under limited training budgets by strategically selecting\nexamples for the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05955","description":"<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI improves performance on eight out-of-domain test sets we\nconsider, including by 11% on HANS and 9% on Adversarial NLI, compared to\ntraining on the 4x larger MultiNLI. Moreover, it continues to be more effective\nthan MultiNLI augmented with other NLI datasets. Our results demonstrate the\npromise of leveraging natural language generation techniques and re-imagining\nthe role of humans in the dataset creation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homophone, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. Code, data, and\ninstructions to implement MEMPROMPT for a new task at\nhttps://www.memprompt.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training. (arXiv:2201.08081v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08081","description":"<p>Language-based environment manipulation requires agents to manipulate the\nenvironment following natural language instructions, which is challenging due\nto the huge space of the environments. To address this challenge, various\napproaches have been proposed in recent work. Although these approaches work\nwell for their intended environments, they are difficult to generalize across\nenvironments. In this work, we propose LEMON, a general framework for\nlanguage-based environment manipulation tasks. Specifically, we first specify a\ntask-agnostic approach for language-based environment manipulation tasks, which\ncan deal with various environments using the same generative language model.\nThen we propose an execution-guided pre-training strategy to inject prior\nknowledge of environments to the language model with a pure synthetic\npre-training corpus. Experimental results on tasks including Alchemy, Scene,\nTangrams, ProPara and Recipes demonstrate the effectiveness of LEMON: it\nachieves new state-of-the-art results on four of the tasks, and the\nexecution-guided pre-training strategy brings remarkable improvements on all\nexperimental tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack. (arXiv:2201.08193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08193","description":"<p>Existing textual adversarial attacks usually utilize the gradient or\nprediction confidence to generate adversarial examples, making it hard to be\ndeployed in real-world applications. To this end, we consider a rarely\ninvestigated but more rigorous setting, namely hard-label attack, in which the\nattacker can only access the prediction label. In particular, we find we can\nlearn the importance of different words via the change on prediction label\ncaused by word substitutions on the adversarial examples. Based on this\nobservation, we propose a novel adversarial attack, termed Text Hard-label\nattacker (TextHacker). TextHacker randomly perturbs lots of words to craft an\nadversarial example. Then, TextHacker adopts a hybrid local search algorithm\nwith the estimation of word importance from the attack history to minimize the\nadversarial perturbation. Extensive evaluations for text classification and\ntextual entailment show that TextHacker significantly outperforms existing\nhard-label attacks regarding the attack performance as well as adversary\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>As two important textual modalities in electronic health records (EHR), both\nstructured data (clinical codes) and unstructured data (clinical narratives)\nhave recently been increasingly applied to the healthcare domain. Most existing\nEHR-oriented studies, however, either focus on a particular modality or\nintegrate data from different modalities in a straightforward manner, which\nusually treats structured and unstructured data as two independent sources of\ninformation about patient admission and ignore the intrinsic interactions\nbetween them. In fact, the two modalities are documented during the same\nencounter where structured data inform the documentation of unstructured data\nand vice versa. In this paper, we proposed a Medical Multimodal Pre-trained\nLanguage Model, named MedM-PLM, to learn enhanced EHR representations over\nstructured and unstructured data and explore the interaction of two modalities.\nIn MedM-PLM, two Transformer-based neural network components are firstly\nadopted to learn representative characteristics from each modality. A\ncross-modal module is then introduced to model their interactions. We\npre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of\nthe model on three downstream clinical tasks, i.e., medication recommendation,\n30-day readmission prediction and ICD coding. Extensive experiments demonstrate\nthe power of MedM-PLM compared with state-of-the-art methods. Further analyses\nand visualizations show the robustness of our model, which could potentially\nprovide more comprehensive interpretations for clinical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Like Program Executors. (arXiv:2201.11473v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11473","description":"<p>Reasoning over natural language is a long-standing goal for the research\ncommunity. However, studies have shown that existing language models are\ninadequate in reasoning. To address the issue, we present POET, a novel\nreasoning pre-training paradigm. Through pre-training language models with\nprograms and their execution results, POET empowers language models to harvest\nthe reasoning knowledge possessed by program executors via a data-driven\napproach. POET is conceptually simple and can be instantiated by different\nkinds of program executors. In this paper, we showcase two simple instances\nPOET-Math and POET-Logic, in addition to a complex instance, POET-SQL.\nExperimental results on six benchmarks demonstrate that POET can significantly\nboost model performance in natural language reasoning, such as numerical\nreasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on\nreasoning-enhancement pre-training, and we hope our analysis would shed light\non the future research of reasoning like program executors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_X/0/1/0/all/0/1\">Xinyu Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyadi_M/0/1/0/all/0/1\">Morteza Ziyadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multi-Granularity Summarization. (arXiv:2201.12502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12502","description":"<p>Text summarization is a user-preference based task, i.e., for one document,\nusers often have different priorities for summary. As a key aspect of\ncustomization in summarization, granularity is used to measure the semantic\ncoverage between the summary and source document. However, developing systems\nthat can generate summaries with customizable semantic coverage is still an\nunder-explored topic. In this paper, we propose the first unsupervised\nmulti-granularity summarization framework, GranuSum. We take events as the\nbasic semantic units of the source documents and propose to rank these events\nby their salience. We also develop a model to summarize input documents with\ngiven events as anchors and hints. By inputting different numbers of events,\nGranuSum is capable of producing multi-granular summaries in an unsupervised\nmanner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple\nsummaries at different granularities for each document cluster. Experimental\nresults confirm the substantial superiority of GranuSum on multi-granularity\nsummarization over strong baselines. Further, by exploiting the event\ninformation, GranuSum also exhibits state-of-the-art performance under the\nconventional unsupervised abstractive setting. Dataset for this paper can be\nfound at: https://github.com/maszhongming/GranuDUC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yizhu Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04173","description":"<p>Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating and Editing Factual Associations in GPT. (arXiv:2202.05262v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05262","description":"<p>We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel's factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kevin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation. (arXiv:2202.07922v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07922","description":"<p>There is a growing interest in dataset generation recently due to the\nsuperior generative capacity of large pre-trained language models (PLMs). In\nthis paper, we study a flexible and efficient zero-short learning method,\n\\textsc{ZeroGen}. Given a zero-shot task, we first generate a dataset from\nscratch using PLMs in an unsupervised manner. Then, we train a tiny task model\n(e.g., LSTM) under the supervision of the synthesized dataset. This approach\nallows highly efficient inference as the final task model only has orders of\nmagnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being\nannotation-free and efficient, we argue that \\textsc{ZeroGen} can also provide\nuseful insights from the perspective of data-free model-agnostic knowledge\ndistillation, and unreferenced text generation evaluation. Experiments and\nanalysis on different NLP tasks, namely, text classification, question\nanswering, and natural language inference, show the effectiveness of\n\\textsc{ZeroGen}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Parameter-Efficient Tuning: Are We Really There Yet?. (arXiv:2202.07962v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07962","description":"<p>Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the\nnew paradigm for using pretrained language models (PLMs). By tuning just a\nfraction amount of parameters comparing to full model finetuning, PETuning\nmethods claim to have achieved performance on par with or even better than\nfinetuning. In this work, we take a step back and re-examine these PETuning\nmethods by conducting the first comprehensive investigation into the training\nand evaluation of them. We found the problematic validation and testing\npractice in current studies, when accompanied by the instability nature of\nPETuning methods, has led to unreliable conclusions. When being compared under\na truly fair evaluation protocol, PETuning cannot yield consistently\ncompetitive performance while finetuning remains to be the best-performing\nmethod in medium- and high-resource settings. We delve deeper into the cause of\nthe instability and observed that the number of trainable parameters and\ntraining iterations are two main factors: reducing trainable parameters and\nprolonging training iterations may lead to higher stability in PETuning\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shangsong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets. (arXiv:2202.12459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12459","description":"<p>In hate speech detection, developing training and evaluation datasets across\nvarious domains is the critical issue. Whereas, major approaches crawl social\nmedia texts and hire crowd-workers to annotate the data. Following this\nconvention often restricts the scope of pejorative expressions to a single\ndomain lacking generalization. Sometimes domain overlap between training corpus\nand evaluation set overestimate the prediction performance when pretraining\nlanguage models on low-data language. To alleviate these problems in Korean, we\npropose APEACH that asks unspecified users to generate hate speech examples\nfollowed by minimal post-labeling. We find that APEACH can collect useful\ndatasets that are less sensitive to the lexical overlaps between the\npretraining corpus and the evaluation set, thereby properly measuring the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kichang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Mixing of Contextual Information in the Transformer. (arXiv:2203.04212v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04212","description":"<p>The Transformer architecture aggregates input information through the\nself-attention mechanism, but there is no clear understanding of how this\ninformation is mixed across the entire model. Additionally, recent works have\ndemonstrated that attention weights alone are not enough to describe the flow\nof information. In this paper, we consider the whole attention block --\nmulti-head attention, residual connection, and layer normalization -- and\ndefine a metric to measure token-to-token interactions within each layer. Then,\nwe aggregate layer-wise interpretations to provide input attribution scores for\nmodel predictions. Experimentally, we show that our method, ALTI (Aggregation\nof Layer-wise Token-to-token Interactions), provides more faithful explanations\nand increased robustness than gradient-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Resource-Constrained Keyphrase Generation. (arXiv:2203.08118v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08118","description":"<p>State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with limited\nannotated data. To overcome this challenge, we design a data-oriented approach\nthat first identifies salient information using retrieval-based corpus-level\nstatistics, and then learns a task-specific intermediate representation based\non a pre-trained language model using large-scale unlabeled documents. We\nintroduce salient span recovery and salient span prediction as denoising\ntraining objectives that condense the intra-article and inter-article knowledge\nessential for keyphrase generation. Through experiments on multiple keyphrase\ngeneration benchmarks, we show the effectiveness of the proposed approach for\nfacilitating low-resource keyphrase generation and zero-shot domain adaptation.\nOur method especially benefits the generation of absent keyphrases, approaching\nthe performance of models trained with large training sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought. (arXiv:2203.08383v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08383","description":"<p>While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex &amp; multi-step reasoning. Similar to how humans develop a\n\"chain of thought\" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Summary of Long Instructions is Better for Program Synthesis. (arXiv:2203.08597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08597","description":"<p>Despite the success of large pre-trained language models (LMs) such as Codex,\nthey show below-par performance on the larger and more complicated programming\nrelated questions. We show that LMs benefit from the summarized version of\ncomplicated questions. Our findings show that superfluous information often\npresent in problem description such as human characters, background stories,\nand names (which are included to help humans in understanding a task) does not\nhelp models in understanding a task. To this extent, we create a meta-dataset\nfrom the frequently used APPS dataset and the newly created CodeContests\ndataset for the program synthesis task. Our meta-dataset consists of human and\nsynthesized summaries of the long and complicated programming questions.\nExperimental results on Codex show that our proposed approach outperforms\nbaseline by 8.13% on the APPS dataset and 11.88% on the CodeContests dataset on\naverage in terms of strict accuracy. Our analysis shows that summaries\nsignificantly improve performance for introductory (9.86%) and interview\n(11.48%) programming questions. However, it shows improvement by a small margin\n(~ 2%) for competitive programming questions, implying scope for future\nresearch in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09590","description":"<p>World knowledge exists in both structured (tables, knowledge graphs) and\nunstructured forms (texts). Recently, there have been extensive research\nefforts in the integration of structured factual knowledge and unstructured\ntextual knowledge. However, most studies focus on incorporating static factual\nknowledge into pre-trained language models, while there is less work on\nenhancing temporal knowledge graph embedding using textual knowledge. Existing\nintegration approaches can not apply to temporal knowledge graphs (tKGs) since\nthey often assume knowledge embedding is time-invariant. In fact, the entity\nembedding in tKG embedding models usually evolves over time, which poses the\nchallenge of aligning temporally relevant textual information with entities. To\nthis end, we propose Enhanced Temporal Knowledge Embeddings with Contextualized\nLanguage Representations (ECOLA), which uses tKG quadruple as an implicit\nmeasure to temporally align textual data and the time-evolving entity\nrepresentations and uses a novel knowledge-text prediction task to inject\ntextual information into temporal knowledge embedding. ECOLA jointly optimizes\nthe knowledge-text prediction objective and the temporal knowledge embedding\nobjective, and thus, can simultaneously take full advantage of textual and\nstructured knowledge. Since existing datasets do not provide tKGs with aligned\ntextual data, we introduce three new datasets for training and evaluating\nECOLA. Experimental results on the temporal knowledge graph completion task\nshow that ECOLA outperforms state-of-the-art tKG embedding models by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppl_H/0/1/0/all/0/1\">Heinz K&#xf6;ppl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The SAME score: Improved cosine based bias score for word embeddings. (arXiv:2203.14603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14603","description":"<p>Over the last years, word and sentence embeddings have established as text\npreprocessing for all kinds of NLP tasks and improved performances in these\ntasks significantly. Unfortunately, it has also been shown that these\nembeddings inherit various kinds of biases from the training data and thereby\npass on biases present in society to NLP solutions. Many papers attempted to\nquantify bias in word or sentence embeddings to evaluate debiasing methods or\ncompare different embedding models, often with cosine-based scores. However,\nsome works have raised doubts about these scores showing that even though they\nreport low biases, biases persist and can be shown with other tests. In fact,\nthere is a great variety of bias scores or tests proposed in the literature\nwithout any consensus on the optimal solutions. We lack works that study the\nbehavior of bias scores and elaborate their advantages and disadvantages. In\nthis work, we will explore different cosine-based bias scores. We provide a\nbias definition based on the ideas from the literature and derive novel\nrequirements for bias scores. Furthermore, we thoroughly investigate the\nexisting cosine-based scores and their limitations in order to show why these\nscores fail to report biases in some situations. Finally, we propose a new bias\nscore, SAME, to address the shortcomings of existing bias scores and show\nempirically that SAME is better suited to quantify biases in word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Sarah Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Alexander Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenneweg_P/0/1/0/all/0/1\">Philip Kenneweg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldhans_R/0/1/0/all/0/1\">Robert Feldhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1\">Fabian Hinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility Issues for BERT-based Evaluation Metrics. (arXiv:2204.00004v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00004","description":"<p>Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1\">Jonas Belouadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Tokenisation by Alternative Treatment of Spaces. (arXiv:2204.04058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04058","description":"<p>Tokenisation is the first step in almost all NLP tasks, and state-of-the-art\ntransformer-based language models all use subword tokenisation algorithms to\nprocess input text. Existing algorithms have problems, often producing\ntokenisations of limited linguistic validity, and representing equivalent\nstrings differently depending on their position within a word. We hypothesise\nthat these problems hinder the ability of transformer-based models to handle\ncomplex words, and suggest that these problems are a result of allowing tokens\nto include spaces. We thus experiment with an alternative tokenisation approach\nwhere spaces are always treated as individual tokens. Specifically, we apply\nthis modification to the BPE and Unigram algorithms. We find that our modified\nalgorithms lead to improved performance on downstream NLP tasks that involve\nhandling complex words, whilst having no detrimental effect on performance in\ngeneral natural language understanding tasks. Intrinsically, we find our\nmodified algorithms give more morphologically correct tokenisations, in\nparticular when handling prefixes. Given the results of our experiments, we\nadvocate for always treating spaces as individual tokens as an improved\ntokenisation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaining Insights into Unrecognized User Utterances in Task-Oriented Dialog Systems. (arXiv:2204.05158v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05158","description":"<p>The rapidly growing market demand for automatic dialogue agents capable of\ngoal-oriented behavior has caused many tech-industry leaders to invest\nconsiderable efforts into task-oriented dialog systems. The success of these\nsystems is highly dependent on the accuracy of their intent identification --\nthe process of deducing the goal or meaning of the user's request and mapping\nit to one of the known intents for further processing. Gaining insights into\nunrecognized utterances -- user requests the systems fail to attribute to a\nknown intent -- is therefore a key process in continuous improvement of\ngoal-oriented dialog systems.\n</p>\n<p>We present an end-to-end pipeline for processing unrecognized user\nutterances, deployed in a real-world, commercial task-oriented dialog system,\nincluding a specifically-tailored clustering algorithm, a novel approach to\ncluster representative extraction, and cluster naming. We evaluated the\nproposed components, demonstrating their benefits in the analysis of\nunrecognized user requests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetzler_M/0/1/0/all/0/1\">Matan Vetzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boaz_D/0/1/0/all/0/1\">David Boaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vineet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07496","description":"<p>We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Fine-tune Twice: Selective Differential Privacy for Large Language Models. (arXiv:2204.07667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07667","description":"<p>Protecting large language models from privacy leakage is becoming\nincreasingly crucial with their wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical notion with provable privacy\nguarantees for machine learning models, to those models remains challenging due\nto the trade-off between model utility and privacy loss. Utilizing the fact\nthat sensitive information in language data tends to be sparse, Shi et al.\n(2021) formalized a DP notion extension called Selective Differential Privacy\n(SDP) to protect only the sensitive tokens defined by a policy function.\nHowever, their algorithm only works for RNN-based models. In this paper, we\ndevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models. Our method is easy to\nimplement: it first fine-tunes the model with redacted in-domain data, and then\nfine-tunes it again with the original in-domain data using a private training\nmechanism. Furthermore, we study the scenario of imperfect implementation of\npolicy functions that misses sensitive tokens and develop systematic methods to\nhandle it. Experiments show that our method achieves strong utility compared to\nprevious baselines. We also analyze the SDP privacy guarantee empirically with\nthe canary insertion attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shea_R/0/1/0/all/0/1\">Ryan Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. (arXiv:2204.07705v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07705","description":"<p>How well can NLP models generalize to a variety of unseen tasks when provided\nwith task instructions? To address this question, we first introduce\nSuper-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their\nexpert-written instructions. Our collection covers 76 distinct task types,\nincluding but not limited to classification, extraction, infilling, sequence\ntagging, text rewriting, and text composition. This large and diverse\ncollection of tasks enables rigorous benchmarking of cross-task generalization\nunder instructions -- training models to follow instructions on a subset of\ntasks and evaluating them on the remaining unseen ones. Furthermore, we build\nTk-Instruct, a transformer model trained to follow a variety of in-context\ninstructions (plain language task definitions or k-shot examples). Our\nexperiments show that Tk-Instruct outperforms existing instruction-following\nmodels such as InstructGPT by over 9% on our benchmark despite being an order\nof magnitude smaller. We further analyze generalization as a function of\nvarious scaling parameters, such as the number of observed tasks, the number of\ninstances per task, and model sizes. We hope our dataset and model facilitate\nfuture progress towards more general-purpose NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordi_Y/0/1/0/all/0/1\">Yeganeh Kordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Amirreza Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1\">Arjun Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanasekaran_A/0/1/0/all/0/1\">Arut Selvan Dhanasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_E/0/1/0/all/0/1\">Eshaan Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Haizhi Gary Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_I/0/1/0/all/0/1\">Ishan Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1\">Jacob Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1\">Krima Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Maitreya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1\">Mirali Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaza_P/0/1/0/all/0/1\">Phani Rohitha Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Pulkit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karia_R/0/1/0/all/0/1\">Rushang Karia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampat_S/0/1/0/all/0/1\">Shailaja Keyur Sampat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_S/0/1/0/all/0/1\">Savan Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sujan Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_S/0/1/0/all/0/1\">Sumanta Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. (arXiv:2204.08109v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08109","description":"<p>Question answering on knowledge bases (KBQA) poses a unique challenge for\nsemantic parsing research due to two intertwined challenges: large search space\nand ambiguities in schema linking. Conventional ranking-based KBQA models,\nwhich rely on a candidate enumeration step to reduce the search space, struggle\nwith flexibility in predicting complicated queries and have impractical running\ntime. In this paper, we present ArcaneQA, a novel generation-based model that\naddresses both the large search space and the schema linking challenges in a\nunified framework with two mutually boosting ingredients: dynamic program\ninduction for tackling the large search space and dynamic contextualized\nencoding for schema linking. Experimental results on multiple popular KBQA\ndatasets demonstrate the highly competitive performance of ArcaneQA in both\neffectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Position Encoding for Transformers. (arXiv:2204.08142v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08142","description":"<p>Recurrent models have been dominating the field of neural machine translation\n(NMT) for the past few years. Transformers \\citep{vaswani2017attention}, have\nradically changed it by proposing a novel architecture that relies on a\nfeed-forward backbone and self-attention mechanism. Although Transformers are\npowerful, they could fail to properly encode sequential/positional information\ndue to their non-recurrent nature. To solve this problem, position embeddings\nare defined exclusively for each time step to enrich word information. However,\nsuch embeddings are fixed after training regardless of the task and the word\nordering system of the source or target language.\n</p>\n<p>In this paper, we propose a novel architecture with new position embeddings\ndepending on the input text to address this shortcoming by taking the order of\ntarget words into consideration. Instead of using predefined position\nembeddings, our solution generates new embeddings to refine each word's\nposition information. Since we do not dictate the position of source tokens and\nlearn them in an end-to-end fashion, we refer to our method as dynamic position\nencoding (DPE). We evaluated the impact of our model on multiple datasets to\ntranslate from English into German, French, and Italian and observed meaningful\nimprovements in comparison to the original Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Joyce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-LITE: Learning Transferable Visual Models with External Knowledge. (arXiv:2204.09222v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09222","description":"<p>The new generation of state-of-the-art computer vision systems are trained\nfrom natural language supervision, ranging from simple object category names to\ndescriptive captions. This form of supervision ensures high generality and\nusability of the learned visual models, due to the broad concept coverage\nachieved via large-scale data collection process. Alternatively, we argue that\nlearning with external knowledge is a promising way which leverages a much more\nstructured source of supervision and offers sample efficiency. We propose\nK-LITE, a simple strategy to leverage external knowledge for building\ntransferable visual systems: In training, it enriches entities in text with\nWordNet and Wiktionary knowledge, leading to an efficient and scalable approach\nto learning image representations that uses knowledge about the visual\nconcepts. In evaluation, the text is also augmented with external knowledge and\nthen used to reference learned visual concepts (or describe new ones) to enable\nzero-shot and few-shot transfer of the pre-trained models. We study the\nperformance of K-LITE on two important computer vision problems, image\nclassification and object detection, benchmarking on 20 and 13 different\nexisting datasets, respectively. The proposed knowledge-augmented models show\nsignificant improvement in transfer learning performance over existing methods.\nOur code is available at https://github.com/microsoft/klite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10293","description":"<p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link\nprediction (ZSLP) which aims to predict unobserved relations in KGs has\nattracted recent interest from researchers. A common solution is to use textual\nfeatures of relations (e.g., surface name or textual descriptions) as auxiliary\ninformation to bridge the gap between seen and unseen relations. Current\napproaches learn an embedding for each word token in the text. These methods\nlack robustness as they suffer from the out-of-vocabulary (OOV) problem.\nMeanwhile, models built on character n-grams have the capability of generating\nexpressive representations for OOV words. Thus, in this paper, we propose a\nHierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which\nconsiders the dependencies among character n-grams of the relation surface name\nfor ZSLP. Our approach works by first constructing a hierarchical n-gram graph\non the surface name to model the organizational structure of n-grams that leads\nto the surface name. A GramTransformer, based on the Transformer is then\npresented to model the hierarchical n-gram graph to construct the relation\nembedding for ZSLP. Experimental results show the proposed HNZSLP achieved\nstate-of-the-art performance on two ZSLP datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. (arXiv:2204.10757v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10757","description":"<p>The goal of information-seeking dialogue is to respond to seeker queries with\nnatural language utterances that are grounded on knowledge sources. However,\ndialogue systems often produce unsupported utterances, a phenomenon known as\nhallucination. To mitigate this behavior, we adopt a data-centric solution and\ncreate FaithDial, a new benchmark for hallucination-free dialogues, by editing\nhallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe\nthat FaithDial is more faithful than WoW while also maintaining engaging\nconversations. We show that FaithDial can serve as training signal for: i) a\nhallucination critic, which discriminates whether an utterance is faithful or\nnot, and boosts the performance by 12.8 F1 score on the BEGIN benchmark\ncompared to existing datasets for dialogue coherence; ii) high-quality dialogue\ngeneration. We benchmark a series of state-of-the-art models and propose an\nauxiliary contrastive objective that achieves the highest level of faithfulness\nand abstractiveness based on several automated metrics. Further, we find that\nthe benefits of FaithDial generalize to zero-shot transfer on other datasets,\nsuch as CMU-Dog and TopicalChat. Finally, human evaluation reveals that\nresponses generated by models trained on FaithDial are perceived as more\ninterpretable, cooperative, and engaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milton_S/0/1/0/all/0/1\">Sivan Milton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement. (arXiv:2204.13074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13074","description":"<p>Our goal is a teachable reasoning system for question-answering (QA), where a\nuser can interact with faithful answer explanations, and correct its errors so\nthat the system improves over time. Our approach is to augment a QA model with\na dynamic memory of user feedback, containing user-supplied corrections to\nerroneous model beliefs that users identify during interaction. Retrievals from\nmemory are used as additional context for QA, to help avoid previous mistakes\nin similar new situations - a novel application of memory-based continuous\nlearning. With simulated feedback, we find that our system (called TeachMe)\ncontinually improves with time, and without model retraining, requiring\nfeedback on only 25% of training examples to reach within 1% of the upper-bound\n(feedback on all examples). Similarly, in experiments with real users, we\nobserve a similar trend, with performance improving by over 15% on a hidden\ntest set after teaching. This suggests new opportunities for using frozen\nlanguage models in an interactive setting where users can inspect, debug, and\ncorrect the model's beliefs, leading to improved system's performance over\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling. (arXiv:2204.14017v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.14017","description":"<p>Recent advances in federated learning have demonstrated its promising\ncapability to learn on decentralized datasets. However, a considerable amount\nof work has raised concerns due to the potential risks of adversaries\nparticipating in the framework to poison the global model for an adversarial\npurpose. This paper investigates the feasibility of model poisoning for\nbackdoor attacks through rare word embeddings of NLP models. In text\nclassification, less than 1% of adversary clients suffices to manipulate the\nmodel output without any drop in the performance on clean sentences. For a less\ncomplex dataset, a mere 0.1% of adversary clients is enough to poison the\nglobal model effectively. We also propose a technique specialized in the\nfederated learning scheme called Gradient Ensemble, which enhances the backdoor\nperformance in all our experimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering. (arXiv:2205.07257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07257","description":"<p>Machine learning models are prone to overfitting their training (source)\ndomains, which is commonly believed to be the reason why they falter in novel\ntarget domains. Here we examine the contrasting view that multi-source domain\ngeneralization (DG) is first and foremost a problem of mitigating source domain\nunderfitting: models not adequately learning the signal already present in\ntheir multi-domain training data. Experiments on a reading comprehension DG\nbenchmark show that as a model learns its source domains better -- using\nfamiliar methods such as knowledge distillation (KD) from a bigger model -- its\nzero-shot out-of-domain utility improves at an even faster pace. Improved\nsource domain learning also demonstrates superior out-of-domain generalization\nover three popular existing DG approaches that aim to limit overfitting. Our\nimplementation of KD-based domain generalization is available via PrimeQA at:\nhttps://ibm.biz/domain-generalization-with-kd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers. (arXiv:2205.07686v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07686","description":"<p>Context-dependent text-to-SQL is the task of translating multi-turn questions\ninto database-related SQL queries. Existing methods typically focus on making\nfull use of history context or previously predicted SQL for currently SQL\nparsing, while neglecting to explicitly comprehend the schema and\nconversational dependency, such as co-reference, ellipsis and user focus\nchange. In this paper, we propose CQR-SQL, which uses auxiliary Conversational\nQuestion Reformulation (CQR) learning to explicitly exploit schema and decouple\ncontextual dependency for SQL parsing. Specifically, we first present a schema\nenhanced recursive CQR method to produce domain-relevant self-contained\nquestions. Secondly, we train CQR-SQL models to map the semantics of multi-turn\nquestions and auxiliary self-contained questions into the same latent space\nthrough schema grounding consistency task and tree-structured SQL parsing\nconsistency task, which enhances the abilities of SQL parsing by adequately\ncontextual understanding. At the time of writing, our CQR-SQL achieves new\nstate-of-the-art results on two context-dependent text-to-SQL benchmarks SParC\nand CoSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dongling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Linzheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.08232","description":"<p>Recently, deep learning models have made great progress in MWP solving on\nanswer accuracy. However, they are uninterpretable since they mainly rely on\nshallow heuristics to achieve high performance without understanding and\nreasoning the grounded math logic. To address this issue and make a step\ntowards interpretable MWP solving, we first construct a high-quality MWP\ndataset named InterMWP which consists of 11,495 MWPs and annotates\ninterpretable logical formulas based on algebraic knowledge as the grounded\nlinguistic logic of each solution equation. Different from existing MWP\ndatasets, our InterMWP benchmark asks for a solver to not only output the\nsolution expressions but also predict the corresponding logical formulas. We\nfurther propose a novel approach with logical prompt and interpretation\ngeneration, called LogicSolver. For each MWP, our LogicSolver first retrieves\nsome highly-correlated algebraic knowledge and then passes them to the backbone\nmodel as prompts to improve the semantic representations of MWPs. With these\nimproved semantic representations, our LogicSolver generates corresponding\nsolution expressions and interpretable knowledge formulas in accord with the\ngenerated solution expressions, simultaneously. Experimental results show that\nour LogicSolver has stronger logical formula-based interpretability than\nbaselines while achieving higher answer accuracy with the help of logical\nprompts, simultaneously. The source code and dataset is available at\nhttps://github.com/yangzhch6/InterMWP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acceptability Judgements via Examining the Topology of Attention Maps. (arXiv:2205.09630v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09630","description":"<p>The role of the attention mechanism in encoding linguistic knowledge has\nreceived special interest in NLP. However, the ability of the attention heads\nto judge the grammatical acceptability of a sentence has been underexplored.\nThis paper approaches the paradigm of acceptability judgments with topological\ndata analysis (TDA), showing that the geometric properties of the attention\ngraph can be efficiently exploited for two standard practices in linguistics:\nbinary judgments and linguistic minimal pairs. Topological features enhance the\nBERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three\nlanguages (English, Italian, and Swedish). By revealing the topological\ndiscrepancy between attention maps of minimal pairs, we achieve the human-level\nperformance on the BLiMP benchmark, outperforming nine statistical and\nTransformer LM baselines. At the same time, TDA provides the foundation for\nanalyzing the linguistic functions of attention heads and interpreting the\ncorrespondence between the graph features and grammatical phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurina_I/0/1/0/all/0/1\">Irina Proskurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce. (arXiv:2205.10843v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10843","description":"<p>In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for\nwidespread applications such as product search and recommendation. For example,\nwhen users search for ``running'' in e-commerce, they would like to find\nproducts highly related to running, such as ``running shoes'' rather than\n``shoes''. Nevertheless, many existing CSK collections rank statements solely\nby confidence scores, and there is no information about which ones are salient\nfrom a human perspective. In this work, we define the task of supervised\nsalience evaluation, where given a CSK triple, the model is required to learn\nwhether the triple is salient or not. In addition to formulating the new task,\nwe also release a new Benchmark dataset of Salience Evaluation in E-commerce\n(BSEE) and hope to promote related research on commonsense knowledge salience\nevaluation. We conduct experiments in the dataset with several representative\nbaseline models. The experimental results show that salience evaluation is a\nchallenging task where models perform poorly on our evaluation set. We further\npropose a simple but effective approach, PMI-tuning, which shows promise for\nsolving this novel problem. Code is available in\n\\url{https://github.com/OpenBGBenchmark/OpenBG-CSK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yincen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zezhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Geometry of Multilingual Language Model Representations. (arXiv:2205.10964v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10964","description":"<p>We assess how multilingual language models maintain a shared multilingual\nrepresentation space while still encoding language-sensitive information in\neach language. Using XLM-R as a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering, evaluated based on causal\neffects on language modeling performance and direct comparisons between\nsubspaces for 88 languages. The subspace means differ along language-sensitive\naxes that are relatively stable throughout middle layers, and these axes encode\ninformation such as token vocabularies. Shifting representations by language\nmeans is sufficient to induce token predictions in different languages.\nHowever, we also identify stable language-neutral axes that encode information\nsuch as token positions and part-of-speech. We visualize representations\nprojected onto language-sensitive and language-neutral axes, identifying\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\ncurves representing token position information. These results demonstrate that\nmultilingual language models encode information along orthogonal\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\nvariety of features for downstream tasks and cross-lingual transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler A. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding. (arXiv:2205.11024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11024","description":"<p>Prompt Tuning has been largely successful as a parameter-efficient method of\nconditioning large-scale pre-trained language models to perform downstream\ntasks. Thus far, soft prompt tuning learns a fixed set of task-specific\ncontinuous vectors, i.e., soft tokens that remain static across the task\nsamples. A fixed prompt, however, may not generalize well to the diverse kinds\nof inputs the task comprises. In order to address this, we propose\nVector-quantized Input-contextualized Prompts (VIP) as an extension to the soft\nprompt tuning framework. VIP particularly focuses on two aspects -- contextual\nprompts that learns input-specific contextualization of the soft prompt tokens\nthrough a small-scale sentence encoder and quantized prompts that maps the\ncontextualized prompts to a set of learnable codebook vectors through a Vector\nquantization network. On various language understanding tasks like SuperGLUE,\nQA, Relation classification, NER and NLI, VIP outperforms the soft prompt\ntuning (PT) baseline by an average margin of 1.19%. Further, our generalization\nstudies show that VIP learns more robust prompt representations, surpassing PT\nby a margin of 0.6% - 5.3% on Out-of-domain QA and NLI tasks respectively, and\nby 0.75% on Multi-Task setup over 4 tasks spanning across 12 domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Amrita Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?. (arXiv:2205.11277v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11277","description":"<p>Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting\nlarge pre-trained models while only tuning a small number of parameters. They\nhave been shown to be competitive with full model fine-tuning for many\ndownstream tasks. However, prior work indicates that PEFTs may not work as well\nfor machine translation (MT), and there is no comprehensive study showing when\nPEFTs work for MT. We conduct a comprehensive empirical study of PEFTs for MT,\nconsidering (1) various parameter budgets, (2) a diverse set of language-pairs,\nand (3) different pre-trained models. We find that 'adapters', in which small\nfeed-forward networks are added after every layer, are indeed on par with full\nmodel fine-tuning when the parameter budget corresponds to 10% of total model\nparameters. Nevertheless, as the number of tuned parameters decreases, the\nperformance of PEFTs decreases. The magnitude of this decrease depends on the\nlanguage pair, with PEFTs particularly struggling for distantly related\nlanguage-pairs. We find that using PEFTs with a larger pre-trained model\noutperforms full fine-tuning with a smaller model, and for smaller training\ndata sizes, PEFTs outperform full fine-tuning for the same pre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outliers Dimensions that Disrupt Transformers Are Driven by Frequency. (arXiv:2205.11380v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11380","description":"<p>While Transformer-based language models are generally very robust to pruning,\nthere is the recently discovered outlier phenomenon: disabling only 48 out of\n110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We\nreplicate the original evidence for the outlier phenomenon and we link it to\nthe geometry of the embedding space. We find that in both BERT and RoBERTa the\nmagnitude of hidden state coefficients corresponding to outlier dimensions\ncorrelates with the frequency of encoded tokens in pre-training data, and it\nalso contributes to the \"vertical\" self-attention pattern enabling the model to\nfocus on the special tokens. This explains the drop in performance from\ndisabling the outliers, and it suggests that to decrease anisotropicity in\nfuture models we need pre-training schemas that would better take into account\nthe skewed token distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puccetti_G/0/1/0/all/0/1\">Giovanni Puccetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozd_A/0/1/0/all/0/1\">Aleksandr Drozd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DellOrletta_F/0/1/0/all/0/1\">Felice Dell&#x27;Orletta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains. (arXiv:2205.11416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11416","description":"<p>Recent model pruning methods have demonstrated the ability to remove\nredundant parameters without sacrificing model performance. Common methods\nremove redundant parameters according to the parameter sensitivity, a\ngradient-based measure reflecting the contribution of the parameters. In this\npaper, however, we argue that redundant parameters can be trained to make\nbeneficial contributions. We first highlight the large sensitivity\n(contribution) gap among high-sensitivity and low-sensitivity parameters and\nshow that the model generalization performance can be significantly improved\nafter balancing the contribution of all parameters. Our goal is to balance the\nsensitivity of all parameters and encourage all of them to contribute equally.\nWe propose a general task-agnostic method, namely intra-distillation, appended\nto the regular training loss to balance parameter sensitivity. Moreover, we\nalso design a novel adaptive learning method to control the strength of\nintra-distillation loss for faster convergence. Our experiments show the strong\neffectiveness of our methods on machine translation, natural language\nunderstanding, and zero-shot cross-lingual transfer across up to 48 languages,\ne.g., a gain of 3.54 BLEU on average across 8 language pairs from the IWSLT'14\ntranslation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization. (arXiv:2205.11686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11686","description":"<p>Combining the visual modality with pretrained language models has been\nsurprisingly effective for simple descriptive tasks such as image captioning.\nMore general text generation however remains elusive. We take a step back and\nask: How do these models work for more complex generative tasks, i.e.\nconditioning on both text and images? Are multimodal models simply visually\nadapted language models, or do they combine they reason jointly over\nmodalities?\n</p>\n<p>We investigate these questions in the context of self-rationalization\n(jointly generating task labels/answers and free-text explanations) of three\ntasks: (i) visual question answering in VQA-X, (ii) visual commonsense\nreasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show\nthat recent unimodal advances, CLIP image representations and scaling of\nlanguage models, do not consistently improve self-rationalization in multimodal\ntasks. We find that no single model type works universally best across tasks,\ndatasets, and finetuning data sizes. Our findings motivate the need for novel\ngeneral backbones approach that move text generation from images and text\nbeyond image captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Akshita Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models. (arXiv:2205.11758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11758","description":"<p>The emergent cross-lingual transfer seen in multilingual pretrained models\nhas sparked significant interest in studying their behavior. However, because\nthese analyses have focused on fully trained multilingual models, little is\nknown about the dynamics of the multilingual pretraining process. We\ninvestigate when these models acquire their in-language and cross-lingual\nabilities by probing checkpoints taken from throughout XLM-R pretraining, using\na suite of linguistic tasks. Our analysis shows that the model achieves high\nin-language performance early on, with lower-level linguistic skills acquired\nbefore more complex ones. In contrast, the point in pretraining when the model\nlearns to transfer cross-lingually differs across language pairs.\nInterestingly, we also observe that, across many languages and tasks, the final\nmodel layer exhibits significant performance degradation over time, while\nlinguistic knowledge propagates to lower layers of the network. Taken together,\nthese insights highlight the complexity of multilingual pretraining and the\nresulting varied behavior for different languages over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat. (arXiv:2205.11764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11764","description":"<p>In a depression-diagnosis-directed clinical session, doctors initiate a\nconversation with ample emotional support that guides the patients to expose\ntheir symptoms based on clinical diagnosis criteria. Such a dialogue system is\ndistinguished from existing single-purpose human-machine dialog systems, as it\ncombines task-oriented and chit-chats with uniqueness in dialogue topics and\nprocedures. However, due to the social stigma associated with mental illness,\nthe dialogue data related to depression consultation and diagnosis are rarely\ndisclosed. Based on clinical depression diagnostic criteria ICD-11 and DSM-5,\nwe designed a 3-phase procedure to construct D$^4$: a Chinese Dialogue Dataset\nfor Depression-Diagnosis-Oriented Chat, which simulates the dialogue between\ndoctors and patients during the diagnosis of depression, including diagnosis\nresults and symptom summary given by professional psychiatrists for each\nconversation. Upon the newly-constructed dataset, four tasks mirroring the\ndepression diagnosis process are established: response generation, topic\nprediction, dialog summary, and severity classification of depressive episode\nand suicide risk. Multi-scale evaluation results demonstrate that a more\nempathy-driven and diagnostic-accurate consultation dialogue system trained on\nour dataset can be achieved compared to rule-based bots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Binwei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Likai Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lingfeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer. (arXiv:2205.12148v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12148","description":"<p>Massively multilingual models are promising for transfer learning across\ntasks and languages. However, existing methods are unable to fully leverage\ntraining data when it is available in different task-language combinations. To\nexploit such heterogeneous supervision, we propose Hyper-X, a single\nhypernetwork that unifies multi-task and multilingual learning with efficient\nadaptation. This model generates weights for adapter modules conditioned on\nboth tasks and language embeddings. By learning to combine task and\nlanguage-specific knowledge, our model enables zero-shot transfer for unseen\nlanguages and task-language combinations. Our experiments on a diverse set of\nlanguages demonstrate that Hyper-X achieves the best or competitive gain when a\nmixture of multiple resources is available, while being on par with strong\nbaselines in the standard scenario. Hyper-X is also considerably more efficient\nin terms of parameters and resources compared to methods that train separate\nadapters. Finally, Hyper-X consistently produces strong results in few-shot\nscenarios for new languages, showing the versatility of our approach beyond\nzero-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Natural Language Proofs with Verifier-Guided Search. (arXiv:2205.12443v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12443","description":"<p>Reasoning over natural language is a challenging problem in NLP. In this\nwork, we focus on proof generation: Given a hypothesis and a set of supporting\nfacts, the model generates a proof tree indicating how to derive the hypothesis\nfrom supporting facts. Compared to generating the entire proof in one shot,\nstepwise generation can better exploit the compositionality and generalize to\nlonger proofs but has achieved limited success on real-world data. Existing\nstepwise methods struggle to generate proof steps that are both logically valid\nand relevant to the hypothesis. Instead, they tend to hallucinate invalid steps\ngiven the hypothesis. In this paper, we present a novel stepwise method,\nNLProofS (Natural Language Proof Search), which learns to generate relevant\nsteps conditioning on the hypothesis. At the core of our approach, we train an\nindependent verifier to check the validity of the proof steps to prevent\nhallucination. Instead of generating steps greedily, we search for proofs\nmaximizing a global proof score judged by the verifier. NLProofS achieves\nstate-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it\nimproves the correctness of predicted proofs from 27.7% to 33.3% in the\ndistractor setting of EntailmentBank, demonstrating the effectiveness of\nNLProofS in generating challenging human-authored proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional set generation using Seq2seq models. (arXiv:2205.12485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12485","description":"<p>Conditional set generation learns a mapping from an input sequence of tokens\nto a set. Several NLP tasks, such as entity typing and dialogue emotion\ntagging, are instances of set generation. Seq2Seq models, a popular choice for\nset generation, treat a set as a sequence and do not fully leverage its key\nproperties, namely order-invariance and cardinality. We propose a novel\nalgorithm for effectively sampling informative orders over the combinatorial\nspace of label orders. We jointly model the set cardinality and output by\nprepending the set size and taking advantage of the autoregressive\nfactorization used by Seq2Seq models. Our method is a model-independent data\naugmentation approach that endows any Seq2Seq model with the signals of\norder-invariance and cardinality. Training a Seq2Seq model on this augmented\ndata (without any additional annotations) gets an average relative improvement\nof 20% on four benchmark datasets across various models: BART, T5, and GPT-3.\nCode to use SETAUG available at: https://setgen.structgen.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-Examining Calibration: The Case of Question Answering. (arXiv:2205.12507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12507","description":"<p>For users to trust model predictions, they need to understand model outputs,\nparticularly their confidence - calibration aims to adjust (calibrate) models'\nconfidence to match expected accuracy. We argue that the traditional\ncalibration evaluation does not promote effective calibrations: for example, it\ncan encourage always assigning a mediocre confidence score to all predictions,\nwhich does not help users distinguish correct predictions from wrong ones.\nBuilding on those observations, we propose a new calibration metric, MacroCE,\nthat better captures whether the model assigns low confidence to wrong\npredictions and high confidence to correct predictions. Focusing on the\npractical application of open-domain question answering, we examine\nconventional calibration methods applied on the widely-used retriever-reader\npipeline, all of which do not bring significant gains under our new MacroCE\nmetric. Toward better calibration, we propose a new calibration method\n(ConsCal) that uses not just final model predictions but whether multiple model\ncheckpoints make consistent predictions. Altogether, we provide an alternative\nview of calibration along with a new metric, re-evaluation of existing\ncalibration methods on our metric, and proposal of a more effective calibration\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning. (arXiv:2205.12548v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12548","description":"<p>Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by \"enumeration\n(e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Mingkai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cheng-Ping Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Meng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Information-Seeking Conversations from Unlabeled Documents. (arXiv:2205.12609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12609","description":"<p>In this paper, we introduce a novel framework, SIMSEEK, (Simulating\ninformation-Seeking conversation from unlabeled documents), and compare its two\nvariants. In our baseline SIMSEEK-SYM, a questioner generates follow-up\nquestions upon the predetermined answer by an answerer. On the contrary,\nSIMSEEK-ASYM first generates the question and then finds its corresponding\nanswer under the conversational context. Our experiments show that they can\nsynthesize effective training resources for CQA and conversational search\ntasks. As a result, conversations from SIMSEEK-ASYM not only make more\nimprovements in our experiments but also are favorably reviewed in a human\nevaluation. We finally release a large-scale resource of synthetic\nconversations, WIKI-SIMSEEK, containing 2 million CQA pairs built upon\nWikipedia documents. With the dataset, our CQA model achieves state-of-the-art\nperformance on a recent CQA benchmark, QuAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation. (arXiv:2205.12647v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12647","description":"<p>In this paper, we explore the challenging problem of performing a generative\ntask in a target language when labeled data is only available in English, using\nsummarization as a case study. We assume a strict setting with no access to\nparallel data or machine translation and find that common transfer learning\napproaches struggle in this setting, as a generative multilingual model\nfine-tuned purely on English catastrophically forgets how to generate\nnon-English. Given the recent rise of parameter-efficient adaptation\ntechniques, we conduct the first investigation into how one such method, prompt\ntuning (Lester et al., 2021), can overcome catastrophic forgetting to enable\nzero-shot cross-lingual generation. Our experiments show that\nparameter-efficient prompt tuning provides gains over standard fine-tuning when\ntransferring between less-related languages, e.g., from English to Thai.\nHowever, a significant gap still remains between these methods and\nfully-supervised baselines. To improve cross-lingual transfer further, we\nexplore several approaches, including: (1) mixing in unlabeled multilingual\ndata, and (2) explicitly factoring prompts into recombinable language and task\ncomponents. Our approaches can provide further quality gains, suggesting that\nrobust zero-shot cross-lingual generation is within reach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1\">Aditya Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. (arXiv:2205.12685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12685","description":"<p>Despite recent explosion of interests in in-context learning, the underlying\nmechanism and the precise impact of the quality of demonstrations remain\nelusive. Intuitively, ground-truth labels should have as much impact in\nin-context learning (ICL) as supervised learning, but recent work reported that\nthe input-label correspondence is significantly less important than previously\nthought. Intrigued by this counter-intuitive observation, we re-examine the\nimportance of ground-truth labels in in-context learning. With the introduction\nof two novel metrics, namely Label-Correctness Sensitivity and Ground-truth\nLabel Effect Ratio (GLER), we were able to conduct quantifiable analysis on the\nimpact of ground-truth label demonstrations. Through extensive analyses, we\nfind that the correct input-label mappings can have varying impacts on the\ndownstream in-context learning performances, depending on the experimental\nconfiguration. Through additional studies, we identify key components, such as\nthe verbosity of prompt templates and the language model size, as the\ncontrolling factor to achieve more noise-resilient ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Hwiyeol Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models. (arXiv:2205.12694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12694","description":"<p>Model compression by way of parameter pruning, quantization, or distillation\nhas recently gained popularity as an approach for reducing the computational\nrequirements of modern deep neural network models for NLP. Inspired by prior\nworks suggesting a connection between simpler, more generalizable models and\nthose that lie within wider loss basins, we hypothesize that optimizing for\nflat minima should lead to simpler parameterizations and thus more compressible\nmodels. We propose to combine sharpness-aware minimization (SAM) with various\ntask-specific model compression methods, including iterative magnitude pruning\n(IMP), structured pruning with a distillation objective, and post-training\ndynamic quantization. Empirically, we show that optimizing for flatter minima\nconsistently leads to greater compressibility of parameters compared to vanilla\nAdam when fine-tuning BERT models, with little to no loss in accuracy on the\nGLUE text classification and SQuAD question answering benchmarks. Moreover, SAM\nfinds superior winning tickets during IMP that 1) are amenable to vanilla Adam\noptimization, and 2) transfer more effectively across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Na_C/0/1/0/all/0/1\">Clara Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factuality Enhanced Language Models for Open-Ended Text Generation. (arXiv:2206.04624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04624","description":"<p>Pretrained language models (LMs) are susceptible to generate text with\nnonfactual information. In this work, we measure and improve the factual\naccuracy of large-scale LMs for open-ended text generation. We design the\nFactualityPrompts test set and metrics to measure the factuality of LM\ngenerations. Based on that, we study the factual accuracy of LMs with parameter\nsizes ranging from 126M to 530B. Interestingly, we find that larger LMs are\nmore factual than smaller ones, although a previous study suggests that larger\nLMs can be less truthful in terms of misconceptions. In addition, popular\nsampling algorithms (e.g., top-p) in open-ended text generation can harm the\nfactuality due to the ''uniform randomness'' introduced at every sampling step.\nWe propose the factual-nucleus sampling algorithm that dynamically adapts the\nrandomness to improve the factuality of generation while maintaining quality.\nFurthermore, we analyze the inefficiencies of the standard training method in\nlearning correct associations between entities from factual text corpus (e.g.,\nWikipedia). We propose a factuality-enhanced training method that uses\nTopicPrefix for better awareness of facts and sentence completion as the\ntraining objective, which can vastly reduce the factual errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech. (arXiv:2206.12229v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.12229","description":"<p>The cloning of a speaker's voice using an untranscribed reference sample is\none of the great advances of modern neural text-to-speech (TTS) methods.\nApproaches for mimicking the prosody of a transcribed reference audio have also\nbeen proposed recently. In this work, we bring these two tasks together for the\nfirst time through utterance level normalization in conjunction with an\nutterance level speaker embedding. We further introduce a lightweight aligner\nfor extracting fine-grained prosodic features, that can be finetuned on\nindividual samples within seconds. We show that it is possible to clone the\nvoice of a speaker as well as the prosody of a spoken reference independently\nwithout any degradation in quality and high similarity to both original voice\nand prosody, as our objective evaluation and human study show. All of our code\nand trained models are available, alongside static and interactive demos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2207.01063","description":"<p>The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where's the Learning in Representation Learning for Compositional Semantics and the Case of Thematic Fit. (arXiv:2208.04749v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.04749","description":"<p>Observing that for certain NLP tasks, such as semantic role prediction or\nthematic fit estimation, random embeddings perform as well as pretrained\nembeddings, we explore what settings allow for this and examine where most of\nthe learning is encoded: the word embeddings, the semantic role embeddings, or\n``the network''. We find nuanced answers, depending on the task and its\nrelation to the training objective. We examine these representation learning\naspects in multi-task learning, where role prediction and role-filling are\nsupervised tasks, while several thematic fit tasks are outside the models'\ndirect supervision. We observe a non-monotonous relation between some tasks'\nquality score and the training data size. In order to better understand this\nobservation, we analyze these results using easier, per-verb versions of these\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muthupari_M/0/1/0/all/0/1\">Mughilan Muthupari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halder_S/0/1/0/all/0/1\">Samrat Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_A/0/1/0/all/0/1\">Asad Sayeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marton_Y/0/1/0/all/0/1\">Yuval Marton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPOT: Knowledge-Enhanced Language Representations for Information Extraction. (arXiv:2208.09625v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09625","description":"<p>Knowledge-enhanced pre-trained models for language representation have been\nshown to be more effective in knowledge base construction tasks (i.e.,~relation\nextraction) than language models such as BERT. These knowledge-enhanced\nlanguage models incorporate knowledge into pre-training to generate\nrepresentations of entities or relationships. However, existing methods\ntypically represent each entity with a separate embedding. As a result, these\nmethods struggle to represent out-of-vocabulary entities and a large amount of\nparameters, on top of their underlying token models (i.e.,~the transformer),\nmust be used and the number of entities that can be handled is limited in\npractice due to memory constraints. Moreover, existing models still struggle to\nrepresent entities and relationships simultaneously. To address these problems,\nwe propose a new pre-trained model that learns representations of both entities\nand relationships from token spans and span pairs in the text respectively. By\nencoding spans efficiently with span modules, our model can represent both\nentities and their relationships but requires fewer parameters than existing\nmodels. We pre-trained our model with the knowledge graph extracted from\nWikipedia and test it on a broad range of supervised and unsupervised\ninformation extraction tasks. Results show that our model learns better\nrepresentations for both entities and relationships than baselines, while in\nsupervised settings, fine-tuning our model outperforms RoBERTa consistently and\nachieves competitive results on information extraction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1\">Yannis Katsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Tyler Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Ho-Cheol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartko_A/0/1/0/all/0/1\">Andrew Bartko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous QA Learning with Structured Prompts. (arXiv:2208.14602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.14602","description":"<p>QA models with lifelong learning (LL) abilities are important for practical\nQA applications, and architecture-based LL methods are reported to be an\neffective implementation for these models. However, it is non-trivial to extend\nprevious approaches to QA tasks since they either require access to task\nidentities in the testing phase or do not explicitly model samples from unseen\ntasks. In this paper, we propose Diana: a dynamic architecture-based lifelong\nQA model that tries to learn a sequence of QA tasks with a prompt enhanced\nlanguage model. Four types of hierarchically organized prompts are used in\nDiana to capture QA knowledge from different granularities. Specifically, we\ndedicate task-level prompts to capture task-specific knowledge to retain high\nLL performances and maintain instance-level prompts to learn knowledge shared\nacross different input samples to improve the model's generalization\nperformance. Moreover, we dedicate separate prompts to explicitly model unseen\ntasks and introduce a set of prompt key vectors to facilitate knowledge sharing\nbetween tasks. Extensive experiments demonstrate that Diana outperforms\nstate-of-the-art lifelong QA models, especially in handling unseen tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning of Lexical Semantic Families for Argumentative Discourse Units Identification. (arXiv:2209.02495v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02495","description":"<p>Argument mining tasks require an informed range of low to high complexity\nlinguistic phenomena and commonsense knowledge. Previous work has shown that\npre-trained language models are highly effective at encoding syntactic and\nsemantic linguistic phenomena when applied with transfer learning techniques\nand built on different pre-training objectives. It remains an issue of how much\nthe existing pre-trained language models encompass the complexity of argument\nmining tasks. We rely on experimentation to shed light on how language models\nobtained from different lexical semantic families leverage the performance of\nthe identification of argumentative discourse units task. Experimental results\nshow that transfer learning techniques are beneficial to the task and that\ncurrent methods may be insufficient to leverage commonsense knowledge from\ndifferent lexical semantic families.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_J/0/1/0/all/0/1\">Jo&#xe3;o Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_R/0/1/0/all/0/1\">Ruben Branco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_A/0/1/0/all/0/1\">Ant&#xf3;nio Branco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Question Answering: A Semantic Parsing Perspective. (arXiv:2209.04994v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04994","description":"<p>Recent advances in deep learning have greatly propelled the research on\nsemantic parsing. Improvement has since been made in many downstream tasks,\nincluding natural language interface to web APIs, text-to-SQL generation, among\nothers. However, despite the close connection shared with these tasks, research\non question answering over knowledge bases (KBQA) has comparatively been\nprogressing slowly. We identify and attribute this to two unique challenges of\nKBQA, schema-level complexity and fact-level complexity. In this survey, we\nsituate KBQA in the broader literature of semantic parsing and give a\ncomprehensive account of how existing KBQA approaches attempt to address the\nunique challenges. Regardless of the unique challenges, we argue that we can\nstill take much inspiration from the literature of semantic parsing, which has\nbeen overlooked by existing research on KBQA. Based on our discussion, we can\nbetter understand the bottleneck of current KBQA research and shed light on\npromising directions for KBQA to keep up with the literature of semantic\nparsing, particularly in the era of pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1\">Vardaan Pahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U3E: Unsupervised and Erasure-based Evidence Extraction for Machine Reading Comprehension. (arXiv:2210.02621v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02621","description":"<p>More tasks in Machine Reading Comprehension(MRC) require, in addition to\nanswer prediction, the extraction of evidence sentences that support the\nanswer. However, the annotation of supporting evidence sentences is usually\ntime-consuming and labor-intensive. In this paper, to address this issue and\nconsidering that most of the existing extraction methods are semi-supervised,\nwe propose an unsupervised evidence extraction method (U3E). U3E takes the\nchanges after sentence-level feature erasure in the document as input,\nsimulating the decline in problem-solving ability caused by human memory\ndecline. In order to make selections on the basis of fully understanding the\nsemantics of the original text, we also propose metrics to quickly select the\noptimal memory model for this input changes. To compare U3E with typical\nevidence extraction methods and investigate its effectiveness in evidence\nextraction, we conduct experiments on different datasets. Experimental results\nshow that U3E is simple but effective, not only extracting evidence more\naccurately, but also significantly improving model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Suzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shumin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenghao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer. (arXiv:2210.02729v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02729","description":"<p>Developing neural architectures that are capable of logical reasoning has\nbecome increasingly important for a wide range of applications (e.g., natural\nlanguage processing). Towards this grand objective, we propose a symbolic\nreasoning architecture that chains many join operators together to model output\nlogical expressions. In particular, we demonstrate that such an ensemble of\njoin-chains can express a broad subset of ''tree-structured'' first-order\nlogical expressions, named FOET, which is particularly useful for modeling\nnatural languages. To endow it with differentiable learning capability, we\nclosely examine various neural operators for approximating the symbolic\njoin-chains. Interestingly, we find that the widely used multi-head\nself-attention module in transformer can be understood as a special neural\noperator that implements the union bound of the join operator in probabilistic\npredicate space. Our analysis not only provides a new perspective on the\nmechanism of the pretrained models such as BERT for natural language\nunderstanding but also suggests several important future improvement\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering. (arXiv:2210.03078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03078","description":"<p>Knowledge underpins reasoning. Recent research demonstrates that when\nrelevant knowledge is provided as additional context to commonsense question\nanswering (QA), it can substantially enhance the performance even on top of\nstate-of-the-art. The fundamental challenge is where and how to find such\nknowledge that is high quality and on point with respect to the question;\nknowledge retrieved from knowledge bases are incomplete and knowledge generated\nfrom language models are inconsistent. We present Rainier, or Reinforced\nKnowledge Introspector, that learns to generate contextually relevant knowledge\nin response to given questions. Our approach starts by imitating knowledge\ngenerated by GPT-3, then learns to generate its own knowledge via reinforcement\nlearning where rewards are shaped based on the increased performance on the\nresulting question answering. Rainier demonstrates substantial and consistent\nperformance gains when tested over 9 different commonsense benchmarks:\nincluding 5 datasets that are seen during model training, as well as 4 datasets\nthat are kept unseen. Our work is the first to report that knowledge generated\nby models that are orders of magnitude smaller than GPT-3, even without direct\nsupervision on the knowledge itself, can exceed the quality of commonsense\nknowledge elicited from GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengfei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03221","description":"<p>With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nportable quantum language model (PQLM) that can be easily transferred to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the portability of the quantum model by\nextracting only the word embeddings and effectively applying them to downstream\ntasks on classical machines. Our PQLM exhibits comparable performance to its\nclassical counterpart on both intrinsic evaluation (loss, perplexity) and\nextrinsic evaluation (multilingual sentiment analysis accuracy) metrics and\nachieves an accuracy of 93.4%, outperforming the classical model. We also\nperform ablation studies on the factors affecting PQLM performance to analyze\nmodel stability. Our work establishes a theoretical foundation for a portable\nquantum pre-trained language model that could be trained on private data and\nmade available for public use with privacy protection guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongchao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ruixing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hexin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Large-scale Paraphrase Acquisition and Generation. (arXiv:2210.03235v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03235","description":"<p>This paper addresses the quality issues in existing Twitter-based paraphrase\ndatasets, and discusses the necessity of using two separate definitions of\nparaphrase for identification and generation tasks. We present a new\nMulti-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of\n130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert\n(MultiPIT_expert) annotations using two different paraphrase definitions for\nparaphrase identification, in addition to a multi-reference test set\n(MultiPIT_NMR) and a large automatically constructed training set\n(MultiPIT_Auto) for paraphrase generation. With improved data annotation\nquality and task-specific paraphrase definition, the best pre-trained language\nmodel fine-tuned on our dataset achieves the state-of-the-art performance of\n84.2 F1 for automatic paraphrase identification. Furthermore, our empirical\nresults also demonstrate that the paraphrase generation models trained on\nMultiPIT_Auto generate more diverse and high-quality paraphrases compared to\ntheir counterparts fine-tuned on other corpora such as Quora, MSCOCO, and\nParaNMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distillation-Resistant Watermarking for Model Protection in NLP. (arXiv:2210.03312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03312","description":"<p>How can we protect the intellectual property of trained NLP models? Modern\nNLP models are prone to stealing by querying and distilling from their publicly\nexposed APIs. However, existing protection methods such as watermarking only\nwork for images but are not applicable to text. We propose\nDistillation-Resistant Watermarking (DRW), a novel technique to protect NLP\nmodels from being stolen via distillation. DRW protects a model by injecting\nwatermarks into the victim's prediction probability corresponding to a secret\nkey and is able to detect such a key by probing a suspect model. We prove that\na protected model still retains the original accuracy within a certain bound.\nWe evaluate DRW on a diverse set of NLP tasks including text classification,\npart-of-speech tagging, and named entity recognition. Experiments show that DRW\nprotects the original model and detects stealing suspects at 100% mean average\nprecision for all four tasks while the prior method fails on two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models. (arXiv:2210.03575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03575","description":"<p>Compositionality, the phenomenon where the meaning of a phrase can be derived\nfrom its constituent parts, is a hallmark of human language. At the same time,\nmany phrases are non-compositional, carrying a meaning beyond that of each part\nin isolation. Representing both of these types of phrases is critical for\nlanguage understanding, but it is an open question whether modern language\nmodels (LMs) learn to do so; in this work we examine this question. We first\nformulate a problem of predicting the LM-internal representations of longer\nphrases given those of their constituents. We find that the representation of a\nparent phrase can be predicted with some accuracy given an affine\ntransformation of its children. While we would expect the predictive accuracy\nto correlate with human judgments of semantic compositionality, we find this is\nlargely not the case, indicating that LMs may not accurately distinguish\nbetween compositional and non-compositional phrases. We perform a variety of\nanalyses, shedding light on when different varieties of LMs do and do not\ngenerate compositional representations, and discuss implications for future\nmodeling work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies. (arXiv:2210.03640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03640","description":"<p>The European Space Agency is well known as a powerful force for scientific\ndiscovery in numerous areas related to Space. The amount and depth of the\nknowledge produced throughout the different missions carried out by ESA and\ntheir contribution to scientific progress is enormous, involving large\ncollections of documents like scientific publications, feasibility studies,\ntechnical reports, and quality management procedures, among many others.\nThrough initiatives like the Open Space Innovation Platform, ESA also acts as a\nhub for new ideas coming from the wider community across different challenges,\ncontributing to a virtuous circle of scientific discovery and innovation.\nHandling such wealth of information, of which large part is unstructured text,\nis a colossal task that goes beyond human capabilities, hence requiring\nautomation. In this paper, we present a methodological framework based on\nartificial intelligence and natural language processing and understanding to\nautomatically extract information from Space documents, generating value from\nit, and illustrate such framework through several case studies implemented\nacross different functional areas of ESA, including Mission Design, Quality\nAssurance, Long-Term Data Preservation, and the Open Space Innovation Platform.\nIn doing so, we demonstrate the value of these technologies in several tasks\nranging from effortlessly searching and recommending Space information to\nautomatically determining how innovative an idea can be, answering questions\nabout Space, and generating quizzes regarding quality procedures. Each of these\naccomplishments represents a step forward in the application of increasingly\nintelligent AI systems in Space, from structuring and facilitating information\naccess to intelligent systems capable to understand and reason with such\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Perez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel G&#xf3;mez-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Silva_A/0/1/0/all/0/1\">Andr&#xe9;s Garc&#xed;a-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leone_R/0/1/0/all/0/1\">Rosemarie Leone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albani_M/0/1/0/all/0/1\">Mirko Albani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontaine_M/0/1/0/all/0/1\">Moritz Fontaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poncet_C/0/1/0/all/0/1\">Charles Poncet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summerer_L/0/1/0/all/0/1\">Leopold Summerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1\">Alessandro Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roma_I/0/1/0/all/0/1\">Ilaria Roma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaglioni_S/0/1/0/all/0/1\">Stefano Scaglioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04325","description":"<p>Data-to-text generation is challenging due to the great variety of the input\ndata in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse\npredicates). Recent end-to-end neural methods thus require substantial training\nexamples to learn to disambiguate and describe the data. Yet, real-world\ndata-to-text problems often suffer from various data-scarce issues: one may\nhave access to only a handful of or no training examples, and/or have to rely\non examples in a different domain or schema. To fill this gap, we propose\nAny-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse\nsettings by making efficient use of any given (or no) examples. ASDOT consists\nof two steps, data disambiguation and sentence fusion, both of which are\namenable to be solved with off-the-shelf pretrained language models (LMs) with\noptional finetuning. In the data disambiguation stage, we employ the prompted\nGPT-3 model to understand possibly ambiguous triples from the input data and\nconvert each into a short sentence with reduced ambiguity. The sentence fusion\nstage then uses an LM like T5 to fuse all the resulting sentences into a\ncoherent paragraph as the final description. We evaluate extensively on various\ndatasets in different scenarios, including the zero-/few-/full-shot settings,\nand generalization to unseen predicates and out-of-domain data. Experimental\nresults show that ASDOT consistently achieves significant improvement over\nbaselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction. (arXiv:2210.04870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04870","description":"<p>Link prediction is the task of inferring missing links between entities in\nknowledge graphs. Embedding-based methods have shown effectiveness in\naddressing this problem by modeling relational patterns in triples. However,\nthe link prediction task often requires contextual information in entity\nneighborhoods, while most existing embedding-based methods fail to capture it.\nAdditionally, little attention is paid to the diversity of entity\nrepresentations in different contexts, which often leads to false prediction\nresults. In this situation, we consider that the schema of knowledge graph\ncontains the specific contextual information, and it is beneficial for\npreserving the consistency of entities across contexts. In this paper, we\npropose a novel Schema-augmented Multi-level contrastive LEarning framework\n(SMiLE) to conduct knowledge graph link prediction. Specifically, we first\nexploit network schema as the prior constraint to sample negatives and\npre-train our model by employing a multi-level contrastive learning method to\nyield both prior schema and contextual information. Then we fine-tune our model\nunder the supervision of individual triples to learn subtler representations\nfor link prediction. Extensive experimental results on four knowledge graph\ndatasets with thorough analysis of each component demonstrate the effectiveness\nof our proposed framework against state-of-the-art baselines. The\nimplementation of SMiLE is available at https://github.com/GKNL/SMiLE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Miao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Ben Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Improving Semantic Diversity of Dialogue Generation. (arXiv:2210.05725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05725","description":"<p>Response diversity has become an important criterion for evaluating the\nquality of open-domain dialogue generation models. However, current evaluation\nmetrics for response diversity often fail to capture the semantic diversity of\ngenerated responses, as they mainly consider lexical aspects of the generated\nresponses. In this paper, we introduce a new automatic evaluation metric to\nmeasure the semantic diversity of generated responses. Through human\nevaluation, we demonstrate that our proposed metric captures human judgments on\nresponse diversity better than existing lexical-level diversity metrics.\nFurthermore, motivated by analyzing an existing dialogue dataset, we propose a\nsimple yet effective learning method that improves the semantic diversity of\ngenerated responses. Our learning method weights training samples based on the\nsemantic distribution of the training set. We show that our learning method\nimproves response diversity and coherency better than other baseline methods\nthrough automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06774","description":"<p>We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models of Code are Few-Shot Commonsense Learners. (arXiv:2210.07128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07128","description":"<p>We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Bias Exposure for Fair Interpretable Predictions. (arXiv:2210.07455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07455","description":"<p>Recent work on reducing bias in NLP models usually focuses on protecting or\nisolating information related to a sensitive attribute (like gender or race).\nHowever, when sensitive information is semantically entangled with the task\ninformation of the input, e.g., gender information is predictive for a\nprofession, a fair trade-off between task performance and bias mitigation is\ndifficult to achieve. Existing approaches perform this trade-off by eliminating\nbias information from the latent space, lacking control over how much bias is\nnecessarily required to be removed. We argue that a favorable debiasing method\nshould use sensitive information 'fairly', rather than blindly eliminating it\n(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we\nprovide a novel debiasing algorithm by adjusting the predictive model's belief\nto (1) ignore the sensitive information if it is not useful for the task; (2)\nuse sensitive information minimally as necessary for the prediction (while also\nincurring a penalty). Experimental results on two text classification tasks\n(influenced by gender) and an open-ended generation task (influenced by race)\nindicate that our model achieves a desirable trade-off between debiasing and\ntask performance along with producing debiased rationales as evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07474","description":"<p>We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1\">Silong Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Pre-Training of Modular Prompt for Few-Shot Learning. (arXiv:2210.07565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07565","description":"<p>Prompt tuning is a parameter-efficient approach to adapting pre-trained\nlanguage models to downstream tasks. Although prompt tuning has been shown to\nmatch the performance of full model tuning when training data is sufficient, it\ntends to struggle in few-shot learning settings. In this paper, we present\nMulti-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot\nlearning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.\nOn downstream tasks, the pre-trained prompts are selectively activated and\ncombined, leading to strong compositional generalization to unseen tasks. To\nbridge the gap between pre-training and fine-tuning, we formulate upstream and\ndownstream tasks into a unified machine reading comprehension task. Extensive\nexperiments under two learning paradigms, i.e., gradient descent and black-box\ntuning, show that MP2 significantly outperforms prompt tuning, full model\ntuning, and prior prompt pre-training methods in few-shot settings. In\naddition, we demonstrate that MP2 can achieve surprisingly fast and strong\nadaptation to downstream tasks by merely learning 8 parameters to combine the\npre-trained modular prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-Centric Story Visualization via Visual Planning and Token Alignment. (arXiv:2210.08465v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.08465","description":"<p>Story visualization advances the traditional text-to-image generation by\nenabling multiple image generation based on a complete story. This task\nrequires machines to 1) understand long text inputs and 2) produce a globally\nconsistent image sequence that illustrates the contents of the story. A key\nchallenge of consistent story visualization is to preserve characters that are\nessential in stories. To tackle the challenge, we propose to adapt a recent\nwork that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a\ntext-tovisual-token (transformer) architecture. Specifically, we modify the\ntext-to-visual-token module with a two-stage framework: 1) character token\nplanning model that predicts the visual tokens for characters only; 2) visual\ntoken completion model that generates the remaining visual token sequence,\nwhich is sent to VQ-VAE for finalizing image generations. To encourage\ncharacters to appear in the images, we further train the two-stage framework\nwith a character-token alignment objective. Extensive experiments and\nevaluations demonstrate that the proposed method excels at preserving\ncharacters and can produce higher quality image sequences compared with the\nstrong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coordinated Topic Modeling. (arXiv:2210.08559v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08559","description":"<p>We propose a new problem called coordinated topic modeling that imitates\nhuman behavior while describing a text corpus. It considers a set of\nwell-defined topics like the axes of a semantic space with a reference\nrepresentation. It then uses the axes to model a corpus for easily\nunderstandable representation. This new task helps represent a corpus more\ninterpretably by reusing existing knowledge and benefits the corpora comparison\ntask. We design ECTM, an embedding-based coordinated topic model that\neffectively uses the reference representation to capture the target\ncorpus-specific aspects while maintaining each topic's global semantics. In\nECTM, we introduce the topic- and document-level supervision with a\nself-training mechanism to solve the problem. Finally, extensive experiments on\nmultiple domains show the superiority of our model over other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akash_P/0/1/0/all/0/1\">Pritom Saha Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling. (arXiv:2210.08709v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08709","description":"<p>Document-level relation extraction (RE) aims to identify relations between\nentities across multiple sentences. Most previous methods focused on\ndocument-level RE under full supervision. However, in real-world scenario, it\nis expensive and difficult to completely label all relations in a document\nbecause the number of entity pairs in document-level RE grows quadratically\nwith the number of entities. To solve the common incomplete labeling problem,\nwe propose a unified positive-unlabeled learning framework - shift and squared\nranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled\n(PU) learning on document-level RE for the first time. Considering that labeled\ndata of a dataset may lead to prior shift of unlabeled data, we introduce a PU\nlearning under prior shift of training data. Also, using none-class score as an\nadaptive threshold, we propose squared ranking loss and prove its Bayesian\nconsistency with multi-label ranking metrics. Extensive experiments demonstrate\nthat our method achieves an improvement of about 14 F1 points relative to the\nprevious baseline with incomplete labeling. In addition, it outperforms\nprevious state-of-the-art results under both fully supervised and extremely\nunlabeled settings as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging a New Spanish Corpus for Multilingual and Crosslingual Metaphor Detection. (arXiv:2210.10358v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10358","description":"<p>The lack of wide coverage datasets annotated with everyday metaphorical\nexpressions for languages other than English is striking. This means that most\nresearch on supervised metaphor detection has been published only for that\nlanguage. In order to address this issue, this work presents the first corpus\nannotated with naturally occurring metaphors in Spanish large enough to develop\nsystems to perform metaphor detection. The presented dataset, CoMeta, includes\ntexts from various domains, namely, news, political discourse, Wikipedia and\nreviews. In order to label CoMeta, we apply the MIPVU method, the guidelines\nmost commonly used to systematically annotate metaphor on real data. We use our\nnewly created dataset to provide competitive baselines by fine-tuning several\nmultilingual and monolingual state-of-the-art large language models.\nFurthermore, by leveraging the existing VUAM English data in addition to\nCoMeta, we present the, to the best of our knowledge, first cross-lingual\nexperiments on supervised metaphor detection. Finally, we perform a detailed\nerror analysis that explores the seemingly high transfer of everyday metaphor\nacross these two languages and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Bayona_E/0/1/0/all/0/1\">Elisa Sanchez-Bayona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10488","description":"<p>Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10709","description":"<p>Information Extraction, which aims to extract structural relational triple or\nevent from unstructured texts, often suffers from data scarcity issues. With\nthe development of pre-trained language models, many prompt-based approaches to\ndata-efficient information extraction have been proposed and achieved\nimpressive performance. However, existing prompt learning methods for\ninformation extraction are still susceptible to several potential limitations:\n(i) semantic gap between natural language and output structure knowledge with\npre-defined schema; (ii) representation learning with locally individual\ninstances limits the performance given the insufficient features. In this\npaper, we propose a novel approach of schema-aware Reference As Prompt (RAP),\nwhich dynamically leverage schema and knowledge inherited from global\n(few-shot) training data for each sample. Specifically, we propose a\nschema-aware reference store, which unifies symbolic schema and relevant\ntextual instances. Then, we employ a dynamic reference integration module to\nretrieve pertinent knowledge from the datastore as prompts during training and\ninference. Experimental results demonstrate that RAP can be plugged into\nvarious existing models and outperforms baselines in low-resource settings on\nfour datasets of relational triple extraction and event extraction. In\naddition, we provide comprehensive empirical ablations and case analysis\nregarding different types and scales of knowledge in order to better understand\nthe mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots. (arXiv:2210.11060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11060","description":"<p>This paper introduces Doc2Bot, a novel dataset for building machines that\nhelp users seek information via conversations. This is of particular interest\nfor companies and organizations that own a large number of manuals or\ninstruction books. Despite its potential, the nature of our task poses several\nchallenges: (1) documents contain various structures that hinder the ability of\nmachines to comprehend, and (2) user information needs are often\nunderspecified. Compared to prior datasets that either focus on a single\nstructural type or overlook the role of questioning to uncover user needs, the\nDoc2Bot dataset is developed to target such challenges systematically. Our\ndataset contains over 100,000 turns based on Chinese documents from five\ndomains, larger than any prior document-grounded dialog dataset for information\nseeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track\nuser intentions, (2) dialog policy learning to plan system actions and\ncontents, and (3) response generation which generates responses based on the\noutputs of the dialog policy. Baseline methods based on the latest deep\nlearning models are presented, indicating that our proposed tasks are\nchallenging and worthy of further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haomin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yeqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovieCLIP: Visual Scene Recognition in Movies. (arXiv:2210.11065v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.11065","description":"<p>Longform media such as movies have complex narrative structures, with events\nspanning a rich variety of ambient visual scenes. Domain specific challenges\nassociated with visual scenes in movies include transitions, person coverage,\nand a wide array of real-life and fictional scenarios. Existing visual scene\ndatasets in movies have limited taxonomies and don't consider the visual scene\ntransition within movie clips. In this work, we address the problem of visual\nscene recognition in movies by first automatically curating a new and extensive\nmovie-centric taxonomy of 179 scene labels derived from movie scripts and\nauxiliary web-based video datasets. Instead of manual annotations which can be\nexpensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips\nbased on our proposed taxonomy. We provide baseline visual models trained on\nthe weakly labeled dataset called MovieCLIP and evaluate them on an independent\ndataset verified by human raters. We show that leveraging features from models\npretrained on MovieCLIP benefits downstream tasks such as multi-label scene and\ngenre classification of web videos and movie trailers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_D/0/1/0/all/0/1\">Digbalay Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebbar_R/0/1/0/all/0/1\">Rajat Hebbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_McLaughlin_K/0/1/0/all/0/1\">Kree Cole-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huisheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval. (arXiv:2210.11773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11773","description":"<p>Sampling proper negatives from a large document pool is vital to effectively\ntrain a dense retrieval model. However, existing negative sampling strategies\nsuffer from the uninformative or false negative problem. In this work, we\nempirically show that according to the measured relevance scores, the negatives\nranked around the positives are generally more informative and less likely to\nbe false negatives. Intuitively, these negatives are not too hard (\\emph{may be\nfalse negatives}) or too easy (\\emph{uninformative}). They are the ambiguous\nnegatives and need more attention during training. Thus, we propose a simple\nambiguous negatives sampling method, SimANS, which incorporates a new sampling\nprobability distribution to sample more ambiguous negatives. Extensive\nexperiments on four public and one industry datasets show the effectiveness of\nour approach. We made the code and models publicly available in\n\\url{https://github.com/microsoft/SimXNS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12023","description":"<p>We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen predicting a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands and math\noperators on the output solution. By grounding the behavioral analysis in a\ncausal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of\nbivariate math word problems. Our analysis shows that robustness does not\nappear to continuously improve as a function of scale, but that the recent LLM,\nGPT-3-Instruct (175B), achieves a dramatic improvement in both robustness and\nsensitivity, compared to all other GPT variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stolfo_A/0/1/0/all/0/1\">Alessandro Stolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}