{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning a Formality-Aware Japanese Sentence Representation. (arXiv:2301.07209v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07209","description":"<p>While the way intermediate representations are generated in encoder-decoder\nsequence-to-sequence models typically allow them to preserve the semantics of\nthe input sentence, input features such as formality might be left out. On the\nother hand, downstream tasks such as translation would benefit from working\nwith a sentence representation that preserves formality in addition to\nsemantics, so as to generate sentences with the appropriate level of social\nformality -- the difference between speaking to a friend versus speaking with a\nsupervisor. We propose a sequence-to-sequence method for learning a\nformality-aware representation for Japanese sentences, where sentence\ngeneration is conditioned on both the original representation of the input\nsentence, and a side constraint which guides the sentence representation\ntowards preserving formality information. Additionally, we propose augmenting\nthe sentence representation with a learned representation of formality which\nfacilitates the extraction of formality in downstream tasks. We address the\nlack of formality-annotated parallel data by adapting previous works on\nprocedural formality classification of Japanese sentences. Experimental results\nsuggest that our techniques not only helps the decoder recover the formality of\nthe input sentence, but also slightly improves the preservation of input\nsentence semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xinyuan_H/0/1/0/all/0/1\">Henry Li Xinyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Ray Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Script Distillation for Multilingual Visual Question Answering. (arXiv:2301.07227v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07227","description":"<p>Pre-trained models with dual and cross encoders have shown remarkable success\nin propelling the landscape of several tasks in vision and language in Visual\nQuestion Answering (VQA). However, since they are limited by the requirements\nof gold annotated data, most of these advancements do not see the light of day\nin other languages beyond English. We aim to address this problem by\nintroducing a curriculum based on the source and target language translations\nto finetune the pre-trained models for the downstream task. Experimental\nresults demonstrate that script plays a vital role in the performance of these\nmodels. Specifically, we show that target languages that share the same script\nperform better (~6%) than other languages and mixed-script code-switched\nlanguages perform better than their counterparts (~5-12%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Multilingual Speech Representation Model for a New, Underresourced Language through Multilingual Fine-tuning and Continued Pretraining. (arXiv:2301.07295v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07295","description":"<p>In recent years, neural models learned through self-supervised pretraining on\nlarge scale multilingual text or speech data have exhibited promising results\nfor underresourced languages, especially when a relatively large amount of data\nfrom related language(s) is available. While the technology has a potential for\nfacilitating tasks carried out in language documentation projects, such as\nspeech transcription, pretraining a multilingual model from scratch for every\nnew language would be highly impractical. We investigate the possibility for\nadapting an existing multilingual wav2vec 2.0 model for a new language,\nfocusing on actual fieldwork data from a critically endangered tongue: Ainu.\nSpecifically, we (i) examine the feasibility of leveraging data from similar\nlanguages also in fine-tuning; (ii) verify whether the model's performance can\nbe improved by further pretraining on target language data. Our results show\nthat continued pretraining is the most effective method to adapt a wav2vec 2.0\nmodel for a new language and leads to considerable reduction in error rates.\nFurthermore, we find that if a model pretrained on a related speech variety or\nan unrelated language with similar phonological characteristics is available,\nmultilingual fine-tuning using additional data from that language can have\npositive impact on speech recognition performance when there is very little\nlabeled data in the target language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_K/0/1/0/all/0/1\">Karol Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murasaki_K/0/1/0/all/0/1\">Kyoko Murasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieuwazny_J/0/1/0/all/0/1\">Jagna Nieuwa&#x17c;ny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KILDST: Effective Knowledge-Integrated Learning for Dialogue State Tracking using Gazetteer and Speaker Information. (arXiv:2301.07341v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07341","description":"<p>Dialogue State Tracking (DST) is core research in dialogue systems and has\nreceived much attention. In addition, it is necessary to define a new problem\nthat can deal with dialogue between users as a step toward the conversational\nAI that extracts and recommends information from the dialogue between users.\nSo, we introduce a new task - DST from dialogue between users about scheduling\nan event (DST-USERS). The DST-USERS task is much more challenging since it\nrequires the model to understand and track dialogue states in the dialogue\nbetween users and to understand who suggested the schedule and who agreed to\nthe proposed schedule. To facilitate DST-USERS research, we develop dialogue\ndatasets between users that plan a schedule. The annotated slot values which\nneed to be extracted in the dialogue are date, time, and location. Previous\napproaches, such as Machine Reading Comprehension (MRC) and traditional DST\ntechniques, have not achieved good results in our extensive evaluations. By\nadopting the knowledge-integrated learning method, we achieve exceptional\nresults. The proposed model architecture combines gazetteer features and\nspeaker information efficiently. Our evaluations of the dialogue datasets\nbetween users that plan a schedule show that our model outperforms the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyungtak Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyeonmok Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_G/0/1/0/all/0/1\">Gurpreet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravuru_L/0/1/0/all/0/1\">Lohith Ravuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1\">Kiranmayi Gandikota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhawar_M/0/1/0/all/0/1\">Manisha Jhawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharani_S/0/1/0/all/0/1\">Simma Dharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1\">Pranamya Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Newsbridge -Telecom SudParis VoxCeleb Speaker Recognition Challenge 2022 System Description. (arXiv:2301.07491v1 [cs.SD])","link":"http://arxiv.org/abs/2301.07491","description":"<p>We describe the system used by our team for the VoxCeleb Speaker Recognition\nChallenge 2022 (VoxSRC 2022) in the speaker diarization track. Our solution was\ndesigned around a new combination of voice activity detection algorithms that\nuses the strengths of several systems. We introduce a novel multi stream\napproach with a decision protocol based on classifiers entropy. We called this\nmethod a multi-stream voice activity detection and used it with standard\nbaseline diarization embeddings, clustering and resegmentation. With this work,\nwe successfully demonstrated that using a strong baseline and working only on\nvoice activity detection, one can achieved close to state-of-theart results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tevissen_Y/0/1/0/all/0/1\">Yannis Tevissen</a> (ARMEDIA-SAMOVAR), <a href=\"http://arxiv.org/find/cs/1/au:+Boudy_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Boudy</a> (ARMEDIA-SAMOVAR), <a href=\"http://arxiv.org/find/cs/1/au:+Petitpont_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Petitpont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing. (arXiv:2301.07507v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07507","description":"<p>The task of text-to-SQL parsing, which aims at converting natural language\nquestions into executable SQL queries, has garnered increasing attention in\nrecent years, as it can assist end users in efficiently extracting vital\ninformation from databases without the need for technical background. One of\nthe major challenges in text-to-SQL parsing is domain generalization, i.e., how\nto generalize well to unseen databases. Recently, the pre-trained text-to-text\ntransformer model, namely T5, though not specialized for text-to-SQL parsing,\nhas achieved state-of-the-art performance on standard benchmarks targeting\ndomain generalization. In this work, we explore ways to further augment the\npre-trained T5 model with specialized components for text-to-SQL parsing. Such\ncomponents are expected to introduce structural inductive bias into text-to-SQL\nparsers thus improving model's capacity on (potentially multi-hop) reasoning,\nwhich is critical for generating structure-rich SQLs. To this end, we propose a\nnew architecture GRAPHIX-T5, a mixed model with the standard pre-trained\ntransformer model augmented by some specially-designed graph-aware layers.\nExtensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5\nacross four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5\nsurpass all other T5-based parsers with a significant margin, achieving new\nstate-of-the-art performance. Notably, GRAPHIX-T5-large reach performance\nsuperior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6%\non execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and\n1.5% on EX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Reynold Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_N/0/1/0/all/0/1\">Nan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantitative Exploration of Natural Language Processing Applications for Electricity Demand Analysis. (arXiv:2301.07535v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07535","description":"<p>The relationship between electricity demand and weather has been established\nfor a long time and is one of the cornerstones in load prediction for operation\nand planning, along with behavioral and social aspects such as calendars or\nsignificant events. This paper explores how and why the social information\ncontained in the news can be used better to understand aggregate population\nbehaviour in terms of energy demand. The work is done through experiments\nanalysing the impact of predicting features extracted from national news on\nday-ahead electric demand prediction. The results are compared to a benchmark\nmodel trained exclusively on the calendar and meteorological information.\nExperimental results showed that the best-performing model reduced the official\nstandard errors around 4%, 11%, and 10% in terms of RMSE, MAE, and SMAPE. The\nbest-performing methods are: word frequency identified COVID-19-related\nkeywords; topic distribution that identified news on the pandemic and internal\npolitics; global word embeddings that identified news about international\nconflicts. This study brings a new perspective to traditional electricity\ndemand analysis and confirms the feasibility of improving its predictions with\nunstructured information contained in texts, with potential consequences in\nsociology and economics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camal_S/0/1/0/all/0/1\">Simon Camal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michiorri_A/0/1/0/all/0/1\">Andrea Michiorri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Holistic Understanding of Mathematical Questions with Contrastive Pre-training. (arXiv:2301.07558v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07558","description":"<p>Understanding mathematical questions effectively is a crucial task, which can\nbenefit many applications, such as difficulty estimation. Researchers have\ndrawn much attention to designing pre-training models for question\nrepresentations due to the scarcity of human annotations (e.g., labeling\ndifficulty). However, unlike general free-format texts (e.g., user comments),\nmathematical questions are generally designed with explicit purposes and\nmathematical logic, and usually consist of more complex content, such as\nformulas, and related mathematical knowledge (e.g., Function). Therefore, the\nproblem of holistically representing mathematical questions remains\nunderexplored. To this end, in this paper, we propose a novel contrastive\npre-training approach for mathematical question representations, namely QuesCo,\nwhich attempts to bring questions with more similar purposes closer.\nSpecifically, we first design two-level question augmentations, including\ncontent-level and structure-level, which generate literally diverse question\npairs with similar purposes. Then, to fully exploit hierarchical information of\nknowledge concepts, we propose a knowledge hierarchy-aware rank strategy\n(KHAR), which ranks the similarities between questions in a fine-grained\nmanner. Next, we adopt a ranking contrastive learning task to optimize our\nmodel based on the augmented and ranked questions. We conduct extensive\nexperiments on two real-world mathematical datasets. The experimental results\ndemonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuting Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shiwei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. (arXiv:2301.07597v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07597","description":"<p>The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Biyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jinran Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jianwei Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yupeng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v1 [cs.CL])","link":"http://arxiv.org/abs/2301.07695","description":"<p>We present a new text-to-SQL dataset for electronic health records (EHRs).\nThe utterances were collected from 222 hospital staff, including physicians,\nnurses, insurance review and health records teams, and more. To construct the\nQA dataset on structured EHR data, we conducted a poll at a university hospital\nand templatized the responses to create seed questions. Then, we manually\nlinked them to two open-source EHR databases, MIMIC-III and eICU, and included\nthem with various time expressions and held-out unanswerable questions in the\ndataset, which were all collected from the poll. Our dataset poses a unique set\nof challenges: the model needs to 1) generate SQL queries that reflect a wide\nrange of needs in the hospital, including simple retrieval and complex\noperations such as calculating survival rate, 2) understand various time\nexpressions to answer time-sensitive questions in healthcare, and 3)\ndistinguish whether a given question is answerable or unanswerable based on the\nprediction confidence. We believe our dataset, EHRSQL, could serve as a\npractical benchmark to develop and assess QA models on structured EHR data and\ntake one step further towards bridging the gap between text-to-SQL research and\nits real-life deployment in healthcare. EHRSQL is available at\nhttps://github.com/glee4810/EHRSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonji Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Neural Story Plot Generation via Reward Shaping. (arXiv:1809.10736v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1809.10736","description":"<p>Language-modeling--based approaches to story plot generation attempt to\nconstruct a plot by sampling from a language model (LM) to predict the next\ncharacter, word, or sentence to add to the story. LM techniques lack the\nability to receive guidance from the user to achieve a specific goal, resulting\nin stories that don't have a clear sense of progression and lack coherence. We\npresent a reward-shaping technique that analyzes a story corpus and produces\nintermediate rewards that are backpropagated into a pre-trained LM in order to\nguide the model towards a given goal. Automated evaluations show our technique\ncan create a model that generates story plots which consistently achieve a\nspecified goal. Human-subject studies show that the generated stories have more\nplausible event ordering than baseline plot generation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_P/0/1/0/all/0/1\">Pradyumna Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_M/0/1/0/all/0/1\">Murtaza Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1\">Animesh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Tensor Notation. (arXiv:2102.13196v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.13196","description":"<p>We propose a notation for tensors with named axes, which relieves the author,\nreader, and future implementers of machine learning models from the burden of\nkeeping track of the order of axes and the purpose of each. The notation makes\nit easy to lift operations on low-order tensors to higher order ones, for\nexample, from images to minibatches of images, or from an attention mechanism\nto multiple attention heads.\n</p>\n<p>After a brief overview and formal definition of the notation, we illustrate\nit through several examples from modern machine learning, from building blocks\nlike attention and convolution to full models like Transformers and LeNet. We\nthen discuss differential calculus in our notation and compare with some\nalternative notations. Our proposals build on ideas from many previous papers\nand software libraries. We hope that our notation will encourage more authors\nto use named tensors, resulting in clearer papers and more precise\nimplementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02359","description":"<p>Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at <a href=\"http://github.com/zhijing-jin/nlp4sg_acl2021.\">this http URL</a> In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeticka Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1\">Brian Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Out-of-Distribution Performance on Document Image Classifiers. (arXiv:2210.07448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07448","description":"<p>The ability of a document classifier to handle inputs that are drawn from a\ndistribution different from the training distribution is crucial for robust\ndeployment and generalizability. The RVL-CDIP corpus is the de facto standard\nbenchmark for document classification, yet to our knowledge all studies that\nuse this corpus do not include evaluation on out-of-distribution documents. In\nthis paper, we curate and release a new out-of-distribution benchmark for\nevaluating out-of-distribution performance for document classifiers. Our new\nout-of-distribution benchmark consists of two types of documents: those that\nare not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and\nthose that are one of the 16 in-domain categories yet are drawn from a\ndistribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N).\nWhile prior work on document classification for in-domain RVL-CDIP documents\nreports high accuracy scores, we find that these models exhibit accuracy drops\nof between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and\nfurther struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain\nRVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new\nresource for analyzing out-of-distribution performance on document classifiers.\nOur new out-of-distribution data can be found at\nhttps://github.com/gxlarson/rvl-cdip-ood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gordon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Y/0/1/0/all/0/1\">Yutong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_D/0/1/0/all/0/1\">David Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher Forcing Recovers Reward Functions for Text Generation. (arXiv:2210.08708v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.08708","description":"<p>Reinforcement learning (RL) has been widely used in text generation to\nalleviate the exposure bias issue or to utilize non-parallel datasets. The\nreward function plays an important role in making RL training successful.\nHowever, previous reward functions are typically task-specific and sparse,\nrestricting the use of RL. In our work, we propose a task-agnostic approach\nthat derives a step-wise reward function directly from a model trained with\nteacher forcing. We additionally propose a simple modification to stabilize the\nRL training on non-parallel datasets with our induced reward function.\nEmpirical results show that our method outperforms self-training and reward\nregression methods on several text generation tasks, confirming the\neffectiveness of our reward function.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yongchang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.12874","description":"<p>Contrastive Learning has recently achieved state-of-the-art performance in a\nwide range of tasks. Many contrastive learning approaches use mined hard\nnegatives to make batches more informative during training but these approaches\nare inefficient as they increase epoch length proportional to the number of\nmined negatives and require frequent updates of nearest neighbor indices or\nmining from recent batches. In this work, we provide an alternative to hard\nnegative mining, Global Contrastive Batch Sampling (GCBS), an efficient\napproximation to the batch assignment problem that upper bounds the gap between\nthe global and training losses, $\\mathcal{L}^{Global} - \\mathcal{L}^{Train}$,\nin contrastive learning settings. Through experimentation we find GCBS improves\nstate-of-the-art performance in sentence embedding and code-search tasks.\nAdditionally, GCBS is easy to implement as it requires only a few additional\nlines of code, does not maintain external data structures such as nearest\nneighbor indices, is more computationally efficient than the most minimal hard\nnegative mining approaches, and makes no changes to the model being trained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachidananda_V/0/1/0/all/0/1\">Vin Sachidananda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions. (arXiv:2211.09800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.09800","description":"<p>We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models -- a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) -- to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_T/0/1/0/all/0/1\">Tim Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1\">Aleksander Holynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation. (arXiv:2211.11554v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11554","description":"<p>Dialogue systems is an increasingly popular task of natural language\nprocessing. However, the dialogue paths tend to be deterministic, restricted to\nthe system rails, regardless of the given request or input text. Recent\nadvances in program synthesis have led to systems which can synthesize programs\nfrom very general search spaces, e.g. Programming by Example, and to systems\nwith very accessible interfaces for writing programs, e.g. text-to-code\ntranslation, but have not achieved both of these qualities in the same system.\nWe propose Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a\nmethod for integrating Programming by Example and text-to-code systems which\noffers an accessible natural language interface for synthesizing general\nprograms. We present a program representation that allows our method to be\napplied to the problem of task-oriented dialogue. Finally, we demo MPaTHS using\nour program representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_E/0/1/0/all/0/1\">Eli Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerard_W/0/1/0/all/0/1\">William Gerard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimovich_Y/0/1/0/all/0/1\">Yauhen Klimovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03029","description":"<p>Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griciute_B/0/1/0/all/0/1\">Bernadeta Grici&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Model for Machine Translation: A Case Study. (arXiv:2301.07069v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07069","description":"<p>Research on prompting has shown excellent performance with little or even no\nsupervised training across many tasks. However, prompting for machine\ntranslation is still under-explored in the literature. We fill this gap by\noffering a systematic study on prompting strategies for translation, examining\nvarious factors for prompt template and demonstration example selection. We\nfurther explore the use of monolingual data and the feasibility of\ncross-lingual, cross-domain, and sentence-to-document transfer learning in\nprompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the\ntestbed show that 1) the number and the quality of prompt examples matter,\nwhere using suboptimal examples degenerates translation; 2) several features of\nprompt examples, such as semantic similarity, show significant Spearman\ncorrelation with their prompting performance; yet, none of the correlations are\nstrong enough; 3) using pseudo parallel prompt examples constructed from\nmonolingual data via zero-shot prompting could improve translation; and 4)\nimproved performance is achievable by transferring knowledge from prompt\nexamples selected in other settings. We finally provide an analysis on the\nmodel outputs and discuss several problems that prompting still suffers from.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}