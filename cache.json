{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])","link":"http://arxiv.org/abs/2303.09601","description":"<p>We introduce a Reinforcement Learning Psychotherapy AI Companion that\ngenerates topic recommendations for therapists based on patient responses. The\nsystem uses Deep Reinforcement Learning (DRL) to generate multi-objective\npolicies for four different psychiatric conditions: anxiety, depression,\nschizophrenia, and suicidal cases. We present our experimental results on the\naccuracy of recommended topics using three different scales of working alliance\nratings: task, bond, and goal. We show that the system is able to capture the\nreal data (historical topics discussed by the therapists) relatively well, and\nthat the best performing models vary by disorder and rating scale. To gain\ninterpretable insights into the learned policies, we visualize policy\ntrajectories in a 2D principal component analysis space and transition\nmatrices. These visualizations reveal distinct patterns in the policies trained\nwith different reward signals and trained on different clinical diagnoses. Our\nsystem's success in generating DIsorder-Specific Multi-Objective Policies\n(DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in\nproviding personalized and efficient therapeutic recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])","link":"http://arxiv.org/abs/2303.09618","description":"<p>Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Chih Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09639","description":"<p>Large pre-trained language models have achieved state-of-the-art results on a\nvariety of downstream tasks. Knowledge Distillation (KD) of a smaller student\nmodel addresses their inefficiency, allowing for deployment in\nresource-constraint environments. KD however remains ineffective, as the\nstudent is manually selected from a set of existing options already pre-trained\non large corpora, a sub-optimal choice within the space of all possible student\narchitectures. This paper proposes KD-NAS, the use of Neural Architecture\nSearch (NAS) guided by the Knowledge Distillation process to find the optimal\nstudent model for distillation from a teacher, for a given natural language\ntask. In each episode of the search process, a NAS controller predicts a reward\nbased on a combination of accuracy on the downstream task and latency of\ninference. The top candidate architectures are then distilled from the teacher\non a small proxy set. Finally the architecture(s) with the highest reward is\nselected, and distilled on the full downstream task training set. When\ndistilling on the MNLI task, our KD-NAS model produces a 2 point improvement in\naccuracy on GLUE tasks with equivalent GPU latency with respect to a\nhand-crafted student architecture available in the literature. Using Knowledge\nDistillation, this model also achieves a 1.4x speedup in GPU Latency (3.2x\nspeedup on CPU) with respect to a BERT-Base Teacher, while maintaining 97%\nperformance on GLUE Tasks (without CoLA). We also obtain an architecture with\nequivalent performance as the hand-crafted student model on the GLUE benchmark,\nbut with a 15% speedup in GPU latency (20% speedup in CPU latency) and 0.8\ntimes the number of parameters\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Aashka Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udagawa_T/0/1/0/all/0/1\">Takuma Udagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merler_M/0/1/0/all/0/1\">Michele Merler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kurdi_Y/0/1/0/all/0/1\">Yousef El-Kurdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_B/0/1/0/all/0/1\">Bishwaranjan Bhattacharjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos. (arXiv:2303.09713v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09713","description":"<p>Visual information is central to conversation: body gestures and facial\nexpressions, for example, contribute to meaning that transcends words alone. To\ndate, however, most neural conversational models are limited to just text. We\nintroduce CHAMPAGNE, a generative model of conversations that can account for\nvisual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a\nlarge-scale corpus of 18M video-based dialogues. YTD-18M is constructed from\nweb videos: crucial to our data collection pipeline is a pretrained language\nmodel that converts error-prone automatic transcripts to a cleaner dialogue\nformat while maintaining meaning. Human evaluation reveals that YTD-18M is more\nsensible and specific than prior resources (MMDialog, 1M dialogues), while\nmaintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE\nlearns to conduct conversation from YTD-18M; and 2) when fine-tuned, it\nachieves state-of-the-art results on four vision-language tasks focused on\nreal-world conversations. We release data, models, and code at\nhttps://seungjuhan.me/champagne.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning towards Selective Data Augmentation for Dialogue Generation. (arXiv:2303.09719v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09719","description":"<p>As it is cumbersome and expensive to acquire a huge amount of data for\ntraining neural dialog models, data augmentation is proposed to effectively\nutilize existing training samples. However, current data augmentation\ntechniques on the dialog generation task mostly augment all cases in the\ntraining dataset without considering the intrinsic attributes between different\ncases. We argue that not all cases are beneficial for augmentation task, and\nthe cases suitable for augmentation should obey the following two attributes:\n(1) low-quality (the dialog model cannot generate a high-quality response for\nthe case), (2) representative (the case should represent the property of the\nwhole dataset). Herein, we explore this idea by proposing a Selective Data\nAugmentation framework (SDA) for the response generation task. SDA employs a\ndual adversarial network to select the lowest quality and most representative\ndata points for augmentation in one stage. Extensive experiments conducted on\ntwo publicly available datasets, i.e., DailyDialog and OpenSubtitles, show that\nour framework can improve the response generation performance with respect to\nvarious metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaoqiang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09752","description":"<p>Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09823","description":"<p>This paper describes our participation in the shared task of hate speech\ndetection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our\nexperiments evaluate the performance of six transformer models and their\ncombination using 2 ensemble approaches. The best results on the training set,\nin a five-fold cross validation scenario, were obtained by using the ensemble\napproach based on the majority vote. The evaluation of this approach on the\ntest set resulted in an F1-score of 0.60 and an Accuracy of 0.86.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_I/0/1/0/all/0/1\">Imene Bensalem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing. (arXiv:2303.09827v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09827","description":"<p>We present our work on Track 2 in the Dialog System Technology Challenges 11\n(DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot,\ncross-domain, intent-set induction. In the absence of in-domain training\ndataset, robust utterance representation that can be used across domains is\nnecessary to induce users' intentions. To achieve this, we leveraged a\nmulti-domain dialogue dataset to fine-tune the language model and proposed\nextracting Verb-Object pairs to remove the artifacts of unnecessary\ninformation. Furthermore, we devised the method that generates each cluster's\nname for the explainability of clustered results. Our approach achieved 3rd\nplace in the precision score and showed superior accuracy and normalized mutual\ninformation (NMI) score than the baseline model on various domain datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seungyeon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09859","description":"<p>While modern masked language models (LMs) are trained on ever larger corpora,\nwe here explore the effects of down-scaling training to a modestly-sized but\nrepresentative, well-balanced, and publicly available English text source --\nthe British National Corpus. We show that pre-training on this carefully\ncurated corpus can reach better performance than the original BERT model. We\nargue that this type of corpora has great potential as a language modeling\nbenchmark. To showcase this potential, we present fair, reproducible and\ndata-efficient comparative studies of LMs, in which we evaluate several\ntraining objectives and model architectures and replicate previous empirical\nresults in a systematic way. We propose an optimized LM architecture called\nLTG-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09892","description":"<p>Memes are the new-age conveyance mechanism for humor on social media sites.\nMemes often include an image and some text. Memes can be used to promote\ndisinformation or hatred, thus it is crucial to investigate in details. We\nintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other\nprevalent datasets in the domain, including prior iterations of Memotion,\nMemotion 3 introduces Hindi-English Codemixed memes while prior works in the\narea were limited to only the English memes. We describe the Memotion task, the\ndata collection and the dataset creation methodologies. We also provide a\nbaseline for the task. The baseline code and dataset will be made available at\nhttps://github.com/Shreyashm16/Memotion-3.0\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shreyash Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryavardan_S/0/1/0/all/0/1\">S Suryavardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Megha Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1\">Anku Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_M/0/1/0/all/0/1\">Manoj Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Srijan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09901","description":"<p>This paper presents the winning system for the zero-shot Spanish framing\ndetection task, which also achieves competitive places in eight additional\nlanguages. The challenge of the framing detection task lies in identifying a\nset of 14 frames when only a few or zero samples are available, i.e., a\nmultilingual multi-label few- or zero-shot setting. Our developed solution\nemploys a pre-training procedure based on multilingual Transformers using a\nlabel-aware contrastive loss function. In addition to describing the system, we\nperform an embedding space analysis and ablation study to demonstrate how our\npre-training procedure supports framing detection to advance computational\nframing analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1\">Markus Reiter-Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_A/0/1/0/all/0/1\">Alexander Ertl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Innerhofer_K/0/1/0/all/0/1\">Kevin Innerhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking. (arXiv:2303.09905v1 [cs.CL])","link":"http://arxiv.org/abs/2303.09905","description":"<p>The schema-guided paradigm overcomes scalability issues inherent in building\ntask-oriented dialogue (TOD) agents with static ontologies. Instead of\noperating on dialogue context alone, agents have access to hierarchical schemas\ncontaining task-relevant natural language descriptions. Fine-tuned language\nmodels excel at schema-guided dialogue state tracking (DST) but are sensitive\nto the writing style of the schemas. We explore methods for improving the\nrobustness of DST models. We propose a framework for generating synthetic\nschemas which uses tree-based ranking to jointly optimise lexical diversity and\nsemantic faithfulness. The generalisation of strong baselines is improved when\naugmenting their training data with prompts generated by our framework, as\ndemonstrated by marked improvements in average joint goal accuracy (JGA) and\nschema sensitivity (SS) on the SGD-X benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coca_A/0/1/0/all/0/1\">A. Coca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">B.H. Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">W. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">B. Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])","link":"http://arxiv.org/abs/2303.10093","description":"<p>Vision-language pretraining to learn a fine-grained, region-word alignment\nbetween image-caption pairs has propelled progress in open-vocabulary object\ndetection. We observe that region-word alignment methods are typically used in\ndetection with respect to only object nouns, and the impact of other rich\ncontext in captions, such as attributes, is unclear. In this study, we explore\nhow language context affects downstream object detection and propose to enhance\nthe role of context. In particular, we show how to strategically contextualize\nthe grounding pretraining objective for improved alignment. We further hone in\non attributes as especially useful object context and propose a novel adjective\nand noun-based negative sampling strategy for increasing their focus in\ncontrastive learning. Overall, our methods enhance object detection when\ncompared to the state-of-the-art in region-word pretraining. We also highlight\nthe fine-grained utility of an attribute-sensitive model through text-region\nretrieval and phrase grounding analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1\">Kyle Buettner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited. (arXiv:2303.10128v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10128","description":"<p>Zipf's law of abbreviation, the tendency of more frequent words to be\nshorter, is one of the most solid candidates for a linguistic universal, in the\nsense that it has the potential for being exceptionless or with a number of\nexceptions that is vanishingly small compared to the number of languages on\nEarth. Since Zipf's pioneering research, this law has been viewed as a\nmanifestation of a universal principle of communication, i.e. the minimization\nof word lengths, to reduce the effort of communication. Here we revisit the\nconcordance of written language with the law of abbreviation. Crucially, we\nprovide wider evidence that the law holds also in speech (when word length is\nmeasured in time), in particular in 46 languages from 14 linguistic families.\nAgreement with the law of abbreviation provides indirect evidence of\ncompression of languages via the theoretical argument that the law of\nabbreviation is a prediction of optimal coding. Motivated by the need of direct\nevidence of compression, we derive a simple formula for a random baseline\nindicating that word lengths are systematically below chance, across linguistic\nfamilies and writing systems, and independently of the unit of measurement\n(length in characters or duration in time). Our work paves the way to measure\nand compare the degree of optimality of word lengths in languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrini_S/0/1/0/all/0/1\">Sonia Petrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_i_Munoz_A/0/1/0/all/0/1\">Antoni Casas-i-Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cluet_i_Martinell_J/0/1/0/all/0/1\">Jordi Cluet-i-Martinell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentz_C/0/1/0/all/0/1\">Chris Bentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01636","description":"<p>With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2206.06924","description":"<p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping\n$\\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers\nthat maximizes $D(G)=\\sum_{uv\\in E(G)}|\\pi(u) - \\pi(v)|$. In this setting,\nvertices are considered to lie on a horizontal line and edges are drawn as\nsemicircles above the line. There exist variants of MaxLA in which the\narrangements are constrained. In the planar variant, edge crossings are\nforbidden. In the projective variant for rooted trees, arrangements are planar\nand the root cannot be covered by any edge. Here we present $O(n)$-time and\n$O(n)$-space algorithms that solve planar and projective MaxLA for trees. We\nalso prove several properties of maximum projective and planar arrangements,\nand show that caterpillar trees maximize planar MaxLA over all trees of a fixed\nsize thereby generalizing a previous extremal result on trees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09015","description":"<p>Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_L/0/1/0/all/0/1\">Lovish Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10723","description":"<p>We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hegselmann_S/0/1/0/all/0/1\">Stefan Hegselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buendia_A/0/1/0/all/0/1\">Alejandro Buendia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Monica Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12353","description":"<p>While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2210.15042","description":"<p>Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behnia_R/0/1/0/all/0/1\">Rouzbeh Behnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mohamamdreza Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_J/0/1/0/all/0/1\">Jason Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_b/0/1/0/all/0/1\">balaji Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.00792","description":"<p>We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.00795","description":"<p>This paper presents InterMPL, a semi-supervised learning method of end-to-end\nautomatic speech recognition (ASR) that performs pseudo-labeling (PL) with\nintermediate supervision. Momentum PL (MPL) trains a connectionist temporal\nclassification (CTC)-based model on unlabeled data by continuously generating\npseudo-labels on the fly and improving their quality. In contrast to\nautoregressive formulations, such as the attention-based encoder-decoder and\ntransducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in\ngeneral, owing to its simple/fast inference algorithm and robustness against\ngenerating collapsed labels. However, CTC generally yields inferior performance\nthan the autoregressive models due to the conditional independence assumption,\nthereby limiting the performance of MPL. We propose to enhance MPL by\nintroducing intermediate loss, inspired by the recent advances in CTC-based\nmodeling. Specifically, we focus on self-conditional and hierarchical\nconditional CTC, that apply auxiliary CTC losses to intermediate layers such\nthat the conditional independence assumption is explicitly relaxed. We also\nexplore how pseudo-labels should be generated and used as supervision for\nintermediate losses. Experimental results in different semi-supervised settings\ndemonstrate that the proposed approach outperforms MPL and improves an ASR\nmodel by up to a 12.1% absolute performance gain. In addition, our detailed\nanalysis validates the importance of the intermediate loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03929","description":"<p>Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive successes on downstream tasks, we still have a\nlimited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14228","description":"<p>In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelghani_R/0/1/0/all/0/1\">Rania Abdelghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yen-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_P/0/1/0/all/0/1\">Pauline Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who are you referring to? Coreference resolution in image narrations. (arXiv:2211.14563v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.14563","description":"<p>Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arushi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02021","description":"<p>The focus of this work is to investigate unsupervised approaches to overcome\nquintessential challenges in designing task-oriented dialog schema: assigning\nintent labels to each dialog turn (intent clustering) and generating a set of\nintents based on the intent clustering methods (intent induction). We postulate\nthere are two salient factors for automatic induction of intents: (1)\nclustering algorithm for intent labeling and (2) user utterance embedding\nspace. We compare existing off-the-shelf clustering models and embeddings based\non DSTC11 evaluation. Our extensive experiments demonstrate that the combined\nselection of utterance embedding and clustering method in the intent induction\ntask should be carefully considered. We also present that pretrained MiniLM\nwith Agglomerative clustering shows significant improvement in NMI, ARI, F1,\naccuracy and example coverage in intent induction tasks. The source codes are\navailable at https://github.com/Jeiyoon/dstc11-track2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeiyoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks. (arXiv:2212.08489v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08489","description":"<p>In this paper, we perform an exhaustive evaluation of different\nrepresentations to address the intent classification problem in a Spoken\nLanguage Understanding (SLU) setup. We benchmark three types of systems to\nperform the SLU intent detection task: 1) text-based, 2) lattice-based, and a\nnovel 3) multimodal approach. Our work provides a comprehensive analysis of\nwhat could be the achievable performance of different state-of-the-art SLU\nsystems under different circumstances, e.g., automatically- vs.\nmanually-generated transcripts. We evaluate the systems on the publicly\navailable SLURP spoken language resource corpus. Our results indicate that\nusing richer forms of Automatic Speech Recognition (ASR) outputs, namely\nword-consensus-networks, allows the SLU system to improve in comparison to the\n1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,\nlearning from acoustic and text embeddings, obtains performance similar to the\noracle setup, a relative improvement of 17.8% over the 1-best configuration,\nbeing a recommended alternative to overcome the limitations of working with\nautomatically generated transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villatoro_Tello_E/0/1/0/all/0/1\">Esa&#xfa; Villatoro-Tello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Bidisha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_A/0/1/0/all/0/1\">Alexei V. Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathiraju_A/0/1/0/all/0/1\">Aravind Ganapathiraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08756","description":"<p>Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhenyuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08771","description":"<p>Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models (PLMs)\ncan be adapted to downstream tasks without fine-tuning with prompts. However,\nno research has employed such a prompt approach in science education. As\nstudent responses are presented with natural language, aligning the scoring\nprocedure as the next sentence prediction task using prompts can skip the\ncostly fine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02123","description":"<p>Prior work has attempted to understand the internal structures and\nfunctionalities of Transformer-based encoder-decoder architectures on the level\nof multi-head attention and feed-forward sublayers. Interpretations have\nfocused on the encoder and decoder, along with the combinatorial possibilities\nof the self-attention, cross-attention, and feed-forward sublayers. However,\nwithout examining the low-level structures, one gains limited understanding of\nthe motivation behind sublayer reordering. Could we dive into the sublayer\nabstraction and permute layer weight matrices to improve the quality of\ntranslation? We propose AEIUOrder to greedily reorder layer weight matrices in\nthe encoder by their well-trainedness, as measured by Heavy-Tailed\nSelf-Regularization (HT-SR) metrics, and order the decoder matrices\ncorrespondingly. Our results suggest that greedily reordering layer weight\nmatrices to maximize Total well-trainedness facilitates the model to learn\nrepresentations and generate translations more effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elicia Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2302.10186","description":"<p>This paper reimagines some aspects of speech processing using speech\nencoders, specifically about extracting entities directly from speech, with no\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, postal addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech, ignoring the superfluous portions such\nas carrier phrases and spellings of entities. In the context of dialogs from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step cascade of first generating lexical transcriptions followed\nby text-based entity extraction for identifying spoken entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08954","description":"<p>Research interest in task-oriented dialogs has increased as systems such as\nGoogle Assistant, Alexa and Siri have become ubiquitous in everyday life.\nHowever, the impact of academic research in this area has been limited by the\nlack of datasets that realistically capture the wide array of user pain points.\nTo enable research on some of the more challenging aspects of parsing realistic\nconversations, we introduce PRESTO, a public dataset of over 550K contextual\nmultilingual conversations between humans and virtual assistants. PRESTO\ncontains a diverse array of challenges that occur in real-world NLU tasks such\nas disfluencies, code-switching, and revisions. It is the only large scale\nhuman generated conversational parsing dataset that provides structured context\nsuch as a user's contacts and lists for each example. Our mT5 model based\nbaselines demonstrate that the conversational phenomenon present in PRESTO are\nchallenging to model, which is further pronounced in a low-resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammar_W/0/1/0/all/0/1\">Waleed Ammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1\">Siddharth Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_M/0/1/0/all/0/1\">Motoki Sano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surani_F/0/1/0/all/0/1\">Faiz Surani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Max Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">HyunJeong Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">David Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kyle He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitisaroj_R/0/1/0/all/0/1\">Rattima Nitisaroj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trukhina_A/0/1/0/all/0/1\">Anna Trukhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shachi Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pararth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09306","description":"<p>Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n</p>\n<p>The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n</p>\n<p>We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahgir_H/0/1/0/all/0/1\">HAZ Sameen Shahgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_R/0/1/0/all/0/1\">Ramisa Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md. Zarif Ul Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}