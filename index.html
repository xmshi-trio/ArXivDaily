<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-01T01:30:00Z">01-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Sentiment Classification: A Comparative Survey. (arXiv:2312.17253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17253">
<div class="article-summary-box-inner">
<span><p>Recently, Deep Learning (DL) approaches have been applied to solve the
Sentiment Classification (SC) problem, which is a core task in reviews mining
or Sentiment Analysis (SA). The performances of these approaches are affected
by different factors. This paper addresses these factors and classifies them
into three categories: data preparation based factors, feature representation
based factors and the classification techniques based factors. The paper is a
comprehensive literature-based survey that compares the performance of more
than 100 DL-based SC approaches by using 21 public datasets of reviews given by
customers within three specific application domains (products, movies and
restaurants). These 21 datasets have different characteristics
(balanced/imbalanced, size, etc.) to give a global vision for our study. The
comparison explains how the proposed factors quantitatively affect the
performance of the studied DL-based SC approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful Model Evaluation for Model-Based Metrics. (arXiv:2312.17254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17254">
<div class="article-summary-box-inner">
<span><p>Statistical significance testing is used in natural language processing (NLP)
to determine whether the results of a study or experiment are likely to be due
to chance or if they reflect a genuine relationship. A key step in significance
testing is the estimation of confidence interval which is a function of sample
variance. Sample variance calculation is straightforward when evaluating
against ground truth. However, in many cases, a metric model is often used for
evaluation. For example, to compare toxicity of two large language models, a
toxicity classifier is used for evaluation. Existing works usually do not
consider the variance change due to metric model errors, which can lead to
wrong conclusions. In this work, we establish the mathematical foundation of
significance testing for model-based metrics. With experiments on public
benchmark datasets and a production system, we show that considering metric
model errors to calculate sample variances for model-based metrics changes the
conclusions in certain experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models. (arXiv:2312.17256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17256">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are able to engage in natural-sounding
conversations with humans, showcasing unprecedented capabilities for
information retrieval and automated decision support. They have disrupted
human-technology interaction and the way businesses operate. However,
technologies based on generative artificial intelligence (GenAI) are known to
hallucinate, misinform, and display biases introduced by the massive datasets
on which they are trained. Existing research indicates that humans may
unconsciously internalize these biases, which can persist even after they stop
using the programs. This study explores the cultural self-perception of LLMs by
prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from
the GLOBE project. The findings reveal that their cultural self-perception is
most closely aligned with the values of English-speaking countries and
countries characterized by sustained economic competitiveness. Recognizing the
cultural biases of LLMs and understanding how they work is crucial for all
members of society because one does not want the black box of artificial
intelligence to perpetuate bias in humans, who might, in turn, inadvertently
create and train even more biased algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolving Large Language Model Assistant with Long-Term Conditional Memory. (arXiv:2312.17257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17257">
<div class="article-summary-box-inner">
<span><p>With the rapid development of large language models, AI assistants like
ChatGPT have widely entered people's works and lives. In this paper, we present
an evolving large language model assistant that utilizes verbal long-term
memory. It focuses on preserving the knowledge and experience from the history
dialogue between the user and AI assistant, which can be applied to future
dialogue for generating a better response. The model generates a set of records
for each finished dialogue and stores them in the memory. In later usage, given
a new user input, the model uses it to retrieve its related memory to improve
the quality of the response. To find the best form of memory, we explore
different ways of constructing the memory and propose a new memorizing
mechanism called conditional memory to solve the problems in previous methods.
We also investigate the retrieval and usage of memory in the generation
process. The assistant uses GPT-4 as the backbone and we evaluate it on three
constructed test datasets focusing on different abilities required by an AI
assistant with long-term memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Working Memory for Large Language Model Agents. (arXiv:2312.17259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17259">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved impressive linguistic
capabilities. However, a key limitation persists in their lack of human-like
memory faculties. LLMs exhibit constrained memory retention across sequential
interactions, hindering complex reasoning. This paper explores the potential of
applying cognitive psychology's working memory frameworks, to enhance LLM
architecture. The limitations of traditional LLM memory designs are analyzed,
including their isolation of distinct dialog episodes and lack of persistent
memory links. To address this, an innovative model is proposed incorporating a
centralized Working Memory Hub and Episodic Buffer access to retain memories
across episodes. This architecture aims to provide greater continuity for
nuanced contextual reasoning during intricate tasks and collaborative
scenarios. While promising, further research is required into optimizing
episodic memory encoding, storage, prioritization, retrieval, and security.
Overall, this paper provides a strategic blueprint for developing LLM agents
with more sophisticated, human-like memory capabilities, highlighting memory
mechanisms as a vital frontier in artificial general intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Classification of Teaching Activities from University Lecture Recordings. (arXiv:2312.17262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17262">
<div class="article-summary-box-inner">
<span><p>The way of understanding online higher education has greatly changed due to
the worldwide pandemic situation. Teaching is undertaken remotely, and the
faculty incorporate lecture audio recordings as part of the teaching material.
This new online teaching-learning setting has largely impacted university
classes. While online teaching technology that enriches virtual classrooms has
been abundant over the past two years, the same has not occurred in supporting
students during online learning. {To overcome this limitation, our aim is to
work toward enabling students to easily access the piece of the lesson
recording in which the teacher explains a theoretical concept, solves an
exercise, or comments on organizational issues of the course. To that end, we
present a multimodal classification algorithm that identifies the type of
activity that is being carried out at any time of the lesson by using a
transformer-based language model that exploits features from the audio file and
from the automated lecture transcription. The experimental results will show
that some academic activities are more easily identifiable with the audio
signal while resorting to the text transcription is needed to identify others.
All in all, our contribution aims to recognize the academic activities of a
teacher during a lesson.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TACIT: A Target-Agnostic Feature Disentanglement Framework for Cross-Domain Text Classification. (arXiv:2312.17263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17263">
<div class="article-summary-box-inner">
<span><p>Cross-domain text classification aims to transfer models from label-rich
source domains to label-poor target domains, giving it a wide range of
practical applications. Many approaches promote cross-domain generalization by
capturing domain-invariant features. However, these methods rely on unlabeled
samples provided by the target domains, which renders the model ineffective
when the target domain is agnostic. Furthermore, the models are easily
disturbed by shortcut learning in the source domain, which also hinders the
improvement of domain generalization ability. To solve the aforementioned
issues, this paper proposes TACIT, a target domain agnostic feature
disentanglement framework which adaptively decouples robust and unrobust
features by Variational Auto-Encoders. Additionally, to encourage the
separation of unrobust features from robust features, we design a feature
distillation task that compels unrobust features to approximate the output of
the teacher. The teacher model is trained with a few easy samples that are easy
to carry potential unknown shortcuts. Experimental results verify that our
framework achieves comparable results to state-of-the-art baselines while
utilizing only source domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESGReveal: An LLM-based approach for extracting structured data from ESG reports. (arXiv:2312.17264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17264">
<div class="article-summary-box-inner">
<span><p>ESGReveal is an innovative method proposed for efficiently extracting and
analyzing Environmental, Social, and Governance (ESG) data from corporate
reports, catering to the critical need for reliable ESG information retrieval.
This approach utilizes Large Language Models (LLM) enhanced with Retrieval
Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG
metadata module for targeted queries, a preprocessing module for assembling
databases, and an LLM agent for data extraction. Its efficacy was appraised
using ESG reports from 166 companies across various sectors listed on the Hong
Kong Stock Exchange in 2022, ensuring comprehensive industry and market
capitalization representation. Utilizing ESGReveal unearthed significant
insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in
data extraction and 83.7% in disclosure analysis, which is an improvement over
baseline models. This highlights the framework's capacity to refine ESG data
analysis precision. Moreover, it revealed a demand for reinforced ESG
disclosures, with environmental and social data disclosures standing at 69.5%
and 57.2%, respectively, suggesting a pursuit for more corporate transparency.
While current iterations of ESGReveal do not process pictorial information, a
functionality intended for future enhancement, the study calls for continued
research to further develop and compare the analytical capabilities of various
LLMs. In summary, ESGReveal is a stride forward in ESG data processing,
offering stakeholders a sophisticated tool to better evaluate and advance
corporate sustainability efforts. Its evolution is promising in promoting
transparency in corporate reporting and aligning with broader sustainable
development aims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17267">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning with pre-trained language models (PLMs) has
demonstrated the significantly enhancing ability of relation extraction (RE)
tasks. However, in low-resource scenarios, where the available training data is
scarce, previous prompt-based methods may still perform poorly for prompt-based
representation learning due to a superficial understanding of the relation. To
this end, we highlight the importance of learning high-quality relation
representation in low-resource scenarios for RE, and propose a novel
prompt-based relation representation method, named MVRE
(\underline{M}ulti-\underline{V}iew \underline{R}elation
\underline{E}xtraction), to better leverage the capacity of PLMs to improve the
performance of RE within the low-resource prompt-tuning paradigm. Specifically,
MVRE decouples each relation into different perspectives to encompass
multi-view relation representations for maximizing the likelihood during
relation inference. Furthermore, we also design a Global-Local loss and a
Dynamic-Initialization method for better alignment of the multi-view
relation-representing virtual words, containing the semantics of relation
labels during the optimization learning process and initialization. Extensive
experiments on three benchmark datasets show that our method can achieve
state-of-the-art in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Question Answering with Reformulations over Knowledge Graph. (arXiv:2312.17269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17269">
<div class="article-summary-box-inner">
<span><p>conversational question answering (convQA) over knowledge graphs (KGs)
involves answering multi-turn natural language questions about information
contained in a KG. State-of-the-art methods of ConvQA often struggle with
inexplicit question-answer pairs. These inputs are easy for human beings to
understand given a conversation history, but hard for a machine to interpret,
which can degrade ConvQA performance. To address this problem, we propose a
reinforcement learning (RL) based model, CornNet, which utilizes question
reformulations generated by large language models (LLMs) to improve ConvQA
performance. CornNet adopts a teacher-student architecture where a teacher
model learns question representations using human writing reformulations, and a
student model to mimic the teacher model's output via reformulations generated
by LLMs. The learned question representation is then used by an RL model to
locate the correct answer in a KG. Extensive experimental results show that
CornNet outperforms state-of-the-art convQA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanGu-$\pi$: Enhancing Language Model Architectures via Nonlinearity Compensation. (arXiv:2312.17276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17276">
<div class="article-summary-box-inner">
<span><p>The recent trend of large language models (LLMs) is to increase the scale of
both model size (\aka the number of parameters) and dataset to achieve better
generative ability, which is definitely proved by a lot of work such as the
famous GPT and Llama. However, large models often involve massive computational
costs, and practical applications cannot afford such high prices. However, the
method of constructing a strong model architecture for LLMs is rarely
discussed. We first analyze the state-of-the-art language model architectures
and observe the feature collapse problem. Based on the theoretical analysis, we
propose that the nonlinearity is also very important for language models, which
is usually studied in convolutional neural networks for vision tasks. The
series informed activation function is then introduced with tiny calculations
that can be ignored, and an augmented shortcut is further used to enhance the
model nonlinearity. We then demonstrate that the proposed approach is
significantly effective for enhancing the model nonlinearity through carefully
designed ablations; thus, we present a new efficient model architecture for
establishing modern, namely, PanGu-$\pi$. Experiments are then conducted using
the same dataset and training strategy to compare PanGu-$\pi$ with
state-of-the-art LLMs. The results show that PanGu-$\pi$-7B can achieve a
comparable performance to that of benchmarks with about 10\% inference
speed-up, and PanGu-$\pi$-1B can achieve state-of-the-art performance in terms
of accuracy and efficiency. In addition, we have deployed PanGu-$\pi$-7B in the
high-value domains of finance and law, developing an LLM named YunShan for
practical application. The results show that YunShan can surpass other models
with similar scales on benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Conducting Advanced Text Analytics Information Systems Research. (arXiv:2312.17278v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17278">
<div class="article-summary-box-inner">
<span><p>The exponential growth of digital content has generated massive textual
datasets, necessitating advanced analytical approaches. Large Language Models
(LLMs) have emerged as tools capable of processing and extracting insights from
massive unstructured textual datasets. However, how to leverage LLMs for
text-based Information Systems (IS) research is currently unclear. To assist IS
research in understanding how to operationalize LLMs, we propose a Text
Analytics for Information Systems Research (TAISR) framework. Our proposed
framework provides detailed recommendations grounded in IS and LLM literature
on how to conduct meaningful text-based IS research. We conducted three case
studies in business intelligence using our TAISR framework to demonstrate its
application across several IS research contexts. We also outline potential
challenges and limitations in adopting LLMs for IS. By offering a systematic
approach and evidence of its utility, our TAISR framework contributes to future
IS research streams looking to incorporate powerful LLMs for text analytics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stateful FastConformer with Cache-based Inference for Streaming Automatic Speech Recognition. (arXiv:2312.17279v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17279">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an efficient and accurate streaming speech
recognition model based on the FastConformer architecture. We adapted the
FastConformer architecture for streaming applications through: (1) constraining
both the look-ahead and past contexts in the encoder, and (2) introducing an
activation caching mechanism to enable the non-autoregressive encoder to
operate autoregressively during inference. The proposed model is thoughtfully
designed in a way to eliminate the accuracy disparity between the train and
inference time which is common for many streaming models. Furthermore, our
proposed encoder works with various decoder configurations including
Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders.
Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a
shared encoder with both a CTC and RNNT decoder to boost the accuracy and save
computation. We evaluate the proposed model on LibriSpeech dataset and a
multi-domain large scale dataset and demonstrate that it can achieve better
accuracy with lower latency and inference time compared to a conventional
buffered streaming model baseline. We also showed that training a model with
multiple latencies can achieve better accuracy than single latency models while
it enables us to support multiple latencies with a single model. Our
experiments also showed the hybrid architecture would not only speedup the
convergence of the CTC decoder but also improves the accuracy of streaming
models compared to single decoder models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Content Self-Detection for Transformer-based Large Language Models. (arXiv:2312.17289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17289">
<div class="article-summary-box-inner">
<span><p>$ $The usage of generative artificial intelligence (AI) tools based on large
language models, including ChatGPT, Bard, and Claude, for text generation has
many exciting applications with the potential for phenomenal productivity
gains. One issue is authorship attribution when using AI tools. This is
especially important in an academic setting where the inappropriate use of
generative AI tools may hinder student learning or stifle research by creating
a large amount of automatically generated derivative work. Existing plagiarism
detection systems can trace the source of submitted text but are not yet
equipped with methods to accurately detect AI-generated text. This paper
introduces the idea of direct origin detection and evaluates whether generative
AI systems can recognize their output and distinguish it from human-written
texts. We argue why current transformer-based models may be able to self-detect
their own generated text and perform a small empirical study using zero-shot
learning to investigate if that is the case. Results reveal varying
capabilities of AI systems to identify their generated text. Google's Bard
model exhibits the largest capability of self-detection with an accuracy of
94\%, followed by OpenAI's ChatGPT with 83\%. On the other hand, Anthropic's
Claude model seems to be not able to self-detect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of dimensionality change on the bias of word embeddings. (arXiv:2312.17292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17292">
<div class="article-summary-box-inner">
<span><p>Word embedding methods (WEMs) are extensively used for representing text
data. The dimensionality of these embeddings varies across various tasks and
implementations. The effect of dimensionality change on the accuracy of the
downstream task is a well-explored question. However, how the dimensionality
change affects the bias of word embeddings needs to be investigated. Using the
English Wikipedia corpus, we study this effect for two static (Word2Vec and
fastText) and two context-sensitive (ElMo and BERT) WEMs. We have two
observations. First, there is a significant variation in the bias of word
embeddings with the dimensionality change. Second, there is no uniformity in
how the dimensionality change affects the bias of word embeddings. These
factors should be considered while selecting the dimensionality of word
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing watermarks for large language models. (arXiv:2312.17295v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17295">
<div class="article-summary-box-inner">
<span><p>With the rise of large language models (LLMs) and concerns about potential
misuse, watermarks for generative LLMs have recently attracted much attention.
An important aspect of such watermarks is the trade-off between their
identifiability and their impact on the quality of the generated text. This
paper introduces a systematic approach to this trade-off in terms of a
multi-objective optimization problem. For a large class of robust, efficient
watermarks, the associated Pareto optimal solutions are identified and shown to
outperform the currently default watermark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17296">
<div class="article-summary-box-inner">
<span><p>Recent advances in long-context Large Language Models (LCLMs) have generated
significant interest, especially in applications such as querying scientific
research papers. However, their potential is often limited by inadequate
context utilization. We identify the absence of long-range semantic
dependencies in typical training data as a primary hindrance. To address this,
we delve into the benefits of frequently incorporating related documents into
training inputs. Using the inherent directory structure of code data as a
source of training examples, we demonstrate improvements in perplexity, even
for tasks unrelated to coding. Building on these findings, but with a broader
focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an
innovative method for creating training examples by using a retrieval method to
collate the most mutually relevant documents into a single training context.
Our results indicate that \method{} enhances model performance and can be used
to train large models to utilize long contexts better. We validate our results
by training a large $3$B model, showing both perplexity improvements and better
long-context performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Nature: Datasets and Models for Analyzing Nature-Related Disclosures. (arXiv:2312.17337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17337">
<div class="article-summary-box-inner">
<span><p>Nature is an amorphous concept. Yet, it is essential for the planet's
well-being to understand how the economy interacts with it. To address the
growing demand for information on corporate nature disclosure, we provide
datasets and classifiers to detect nature communication by companies. We ground
our approach in the guidelines of the Taskforce on Nature-related Financial
Disclosures (TNFD). Particularly, we focus on the specific dimensions of water,
forest, and biodiversity. For each dimension, we create an expert-annotated
dataset with 2,200 text samples and train classifier models. Furthermore, we
show that nature communication is more prevalent in hotspot areas and directly
effected industries like agriculture and utilities. Our approach is the first
to respond to calls to assess corporate nature communication on a large scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference. (arXiv:2312.17342v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17342">
<div class="article-summary-box-inner">
<span><p>This paper addresses the privacy and security concerns associated with deep
neural language models, which serve as crucial components in various modern
AI-based applications. These models are often used after being pre-trained and
fine-tuned for specific tasks, with deployment on servers accessed through the
internet. However, this introduces two fundamental risks: (a) the transmission
of user inputs to the server via the network gives rise to interception
vulnerabilities, and (b) privacy concerns emerge as organizations that deploy
such models store user data with restricted context. To address this, we
propose a novel method to adapt and fine-tune transformer-based language models
on passkey-encrypted user-specific text. The original pre-trained language
model first undergoes a quick adaptation (without any further pre-training)
with a series of irreversible transformations applied to the tokenizer and
token embeddings. This enables the model to perform inference on encrypted
inputs while preventing reverse engineering of text from model parameters and
intermediate outputs. After adaptation, models are fine-tuned on encrypted
versions of existing training datasets. Experimental evaluation employing
adapted versions of renowned models (e.g., BERT, RoBERTa) across established
benchmark English and multilingual datasets for text classification and
sequence labeling shows that encrypted models achieve performance parity with
their original counterparts. This serves to safeguard performance, privacy, and
security cohesively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AQUALLM: Audio Question Answering Data Generation Using Large Language Models. (arXiv:2312.17343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17343">
<div class="article-summary-box-inner">
<span><p>Audio Question Answering (AQA) constitutes a pivotal task in which machines
analyze both audio signals and natural language questions to produce precise
natural language answers. The significance of possessing high-quality, diverse,
and extensive AQA datasets cannot be overstated when aiming for the precision
of an AQA system. While there has been notable focus on developing accurate and
efficient AQA models, the creation of high-quality, diverse, and extensive
datasets for the specific task at hand has not garnered considerable attention.
To address this challenge, this work makes several contributions. We introduce
a scalable AQA data generation pipeline, denoted as the AQUALLM framework,
which relies on Large Language Models (LLMs). This framework utilizes existing
audio-caption annotations and incorporates state-of-the-art LLMs to generate
expansive, high-quality AQA datasets. Additionally, we present three extensive
and high-quality benchmark datasets for AQA, contributing significantly to the
progression of AQA research. AQA models trained on the proposed datasets set
superior benchmarks compared to the existing state-of-the-art. Moreover, models
trained on our datasets demonstrate enhanced generalizability when compared to
models trained using human-annotated AQA data. Code and datasets will be
accessible on GitHub~\footnote{\url{https://github.com/swarupbehera/AQUALLM}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model as an Annotator: Unsupervised Context-aware Quality Phrase Generation. (arXiv:2312.17349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17349">
<div class="article-summary-box-inner">
<span><p>Phrase mining is a fundamental text mining task that aims to identify quality
phrases from context. Nevertheless, the scarcity of extensive gold labels
datasets, demanding substantial annotation efforts from experts, renders this
task exceptionally challenging. Furthermore, the emerging, infrequent, and
domain-specific nature of quality phrases presents further challenges in
dealing with this task. In this paper, we propose LMPhrase, a novel
unsupervised context-aware quality phrase mining framework built upon large
pre-trained language models (LMs). Specifically, we first mine quality phrases
as silver labels by employing a parameter-free probing technique called
Perturbed Masking on the pre-trained language model BERT (coined as Annotator).
In contrast to typical statistic-based or distantly-supervised methods, our
silver labels, derived from large pre-trained language models, take into
account rich contextual information contained in the LMs. As a result, they
bring distinct advantages in preserving informativeness, concordance, and
completeness of quality phrases. Secondly, training a discriminative span
prediction model heavily relies on massive annotated data and is likely to face
the risk of overfitting silver labels. Alternatively, we formalize phrase
tagging task as the sequence generation problem by directly fine-tuning on the
Sequence-to-Sequence pre-trained language model BART with silver labels (coined
as Generator). Finally, we merge the quality phrases from both the Annotator
and Generator as the final predictions, considering their complementary nature
and distinct characteristics. Extensive experiments show that our LMPhrase
consistently outperforms all the existing competitors across two different
granularity phrase mining tasks, where each task is tested on two different
domain datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense for Zero-Shot Natural Language Video Localization. (arXiv:2312.17429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17429">
<div class="article-summary-box-inner">
<span><p>Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited
promising results in training NLVL models exclusively with raw video data by
dynamically generating video segments and pseudo-query annotations. However,
existing pseudo-queries often lack grounding in the source video, resulting in
unstructured and disjointed content. In this paper, we investigate the
effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we
present CORONET, a zero-shot NLVL framework that leverages commonsense to
bridge the gap between videos and generated pseudo-queries via a commonsense
enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode
commonsense information extracted from a knowledge graph, conditioned on the
video, and cross-attention mechanisms to enhance the encoded video and
pseudo-query representations prior to localization. Through empirical
evaluations on two benchmark datasets, we demonstrate that CORONET surpasses
both zero-shot and weakly supervised baselines, achieving improvements up to
32.13% across various recall thresholds and up to 6.33% in mIoU. These results
underscore the significance of leveraging commonsense reasoning for zero-shot
NLVL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17432">
<div class="article-summary-box-inner">
<span><p>With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. With Large Language Models (LLMs) showcasing
remarkable capabilities in key language tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey also presents a comprehensive study of the tasks and
datasets for Vid-LLMs, along with the methodologies employed for evaluation.
Additionally, the survey explores the expansive applications of Vid-LLMs across
various domains, thereby showcasing their remarkable scalability and
versatility in addressing challenges in real-world video understanding.
Finally, the survey summarizes the limitations of existing Vid-LLMs and the
directions for future research. For more information, we recommend readers
visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHR Interaction Between Patients and AI: NoteAid EHR Interaction. (arXiv:2312.17475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17475">
<div class="article-summary-box-inner">
<span><p>With the rapid advancement of Large Language Models (LLMs) and their
outstanding performance in semantic and contextual comprehension, the potential
of LLMs in specialized domains warrants exploration. This paper introduces the
NoteAid EHR Interaction Pipeline, an innovative approach developed using
generative LLMs to assist in patient education, a task stemming from the need
to aid patients in understanding Electronic Health Records (EHRs). Building
upon the NoteAid work, we designed two novel tasks from the patient's
perspective: providing explanations for EHR content that patients may not
understand and answering questions posed by patients after reading their EHRs.
We extracted datasets containing 10,000 instances from MIMIC Discharge
Summaries and 876 instances from the MADE medical notes collection,
respectively, executing the two tasks through the NoteAid EHR Interaction
Pipeline with these data. Performance data of LLMs on these tasks were
collected and constructed as the corresponding NoteAid EHR Interaction Dataset.
Through a comprehensive evaluation of the entire dataset using LLM assessment
and a rigorous manual evaluation of 64 instances, we showcase the potential of
LLMs in patient education. Besides, the results provide valuable data support
for future exploration and applications in this domain while also supplying
high-quality synthetic datasets for in-house system training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters. (arXiv:2312.17476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17476">
<div class="article-summary-box-inner">
<span><p>The advancement of Large Language Models (LLMs) has led to their widespread
use across a broad spectrum of tasks including decision making. Prior studies
have compared the decision making abilities of LLMs with those of humans from a
psychological perspective. However, these studies have not always properly
accounted for the sensitivity of LLMs' behavior to hyperparameters and
variations in the prompt. In this study, we examine LLMs' performance on the
Horizon decision making task studied by Binz and Schulz (2023) analyzing how
LLMs respond to variations in prompts and hyperparameters. By experimenting on
three OpenAI language models possessing different capabilities, we observe that
the decision making abilities fluctuate based on the input prompts and
temperature settings. Contrary to previous findings language models display a
human-like exploration exploitation tradeoff after simple adjustments to the
prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17482">
<div class="article-summary-box-inner">
<span><p>Although BERT-style encoder models are heavily used in NLP research, many
researchers do not pretrain their own BERTs from scratch due to the high cost
of training. In the past half-decade since BERT first rose to prominence, many
advances have been made with other transformer architectures and training
configurations that have yet to be systematically incorporated into BERT. Here,
we introduce MosaicBERT, a BERT-style encoder architecture and training recipe
that is empirically optimized for fast pretraining. This efficient architecture
incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear
Units (GLU), a module to dynamically remove padded tokens, and low precision
LayerNorm into the classic transformer encoder block. The training recipe
includes a 30% masking ratio for the Masked Language Modeling (MLM) objective,
bfloat16 precision, and vocabulary size optimized for GPU throughput, in
addition to best-practices from RoBERTa and other encoder models. When
pretrained from scratch on the C4 dataset, this base model achieves a
downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs
at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed
Pareto curves and show that MosaicBERT base and large are consistently Pareto
optimal when compared to a competitive BERT base and large. This empirical
speed up in pretraining enables researchers and engineers to pretrain custom
BERT-style models at low cost instead of finetune on existing generic models.
We open source our model weights and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17484">
<div class="article-summary-box-inner">
<span><p>Despite the great success of large language models (LLMs) in various tasks,
they suffer from generating hallucinations. We introduce Truth Forest, a method
that enhances truthfulness in LLMs by uncovering hidden truth representations
using multi-dimensional orthogonal probes. Specifically, it creates multiple
orthogonal bases for modeling truth by incorporating orthogonal constraints
into the probes. Moreover, we introduce Random Peek, a systematic technique
considering an extended range of positions within the sequence, reducing the
gap between discerning and generating truth features in LLMs. By employing this
approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on
TruthfulQA. Likewise, significant improvements are observed in fine-tuned
models. We conducted a thorough analysis of truth features using probes. Our
visualization results show that orthogonal probes capture complementary
truth-related features, forming well-defined clusters that reveal the inherent
structure of the dataset. Code: \url{https://github.com/jongjyh/trfr}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Open-Vocabulary Diffusion to Camouflaged Instance Segmentation. (arXiv:2312.17505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17505">
<div class="article-summary-box-inner">
<span><p>Text-to-image diffusion techniques have shown exceptional capability of
producing high-quality images from text descriptions. This indicates that there
exists a strong correlation between the visual and textual domains. In
addition, text-image discriminative models such as CLIP excel in image
labelling from text prompts, thanks to the rich and diverse information
available from open concepts. In this paper, we leverage these technical
advances to solve a challenging problem in computer vision: camouflaged
instance segmentation. Specifically, we propose a method built upon a
state-of-the-art diffusion model, empowered by open-vocabulary to learn
multi-scale textual-visual features for camouflaged object representations.
Such cross-domain representations are desirable in segmenting camouflaged
objects where visual cues are subtle to distinguish the objects from the
background, especially in segmenting novel objects which are not seen in
training. We also develop technically supportive components to effectively fuse
cross-domain features and engage relevant features towards respective
foreground objects. We validate our method and compare it with existing ones on
several benchmark datasets of camouflaged instance segmentation and generic
open-vocabulary instance segmentation. Experimental results confirm the
advances of our method over existing ones. We will publish our code and
pre-trained models to support future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game. (arXiv:2312.17515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17515">
<div class="article-summary-box-inner">
<span><p>Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the PromptCBLUE Shared Task in CHIP2023. (arXiv:2312.17522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17522">
<div class="article-summary-box-inner">
<span><p>This paper presents an overview of the PromptCBLUE shared task
(<a href="http://cips-chip.org.cn/2023/eval1">this http URL</a>) held in the CHIP-2023 Conference. This
shared task reformualtes the CBLUE benchmark, and provide a good testbed for
Chinese open-domain or medical-domain large language models (LLMs) in general
medical natural language processing. Two different tracks are held: (a) prompt
tuning track, investigating the multitask prompt tuning of LLMs, (b) probing
the in-context learning capabilities of open-sourced LLMs. Many teams from both
the industry and academia participated in the shared tasks, and the top teams
achieved amazing test results. This paper describes the tasks, the datasets,
evaluation metrics, and the top systems for both tasks. Finally, the paper
summarizes the techniques and results of the evaluation of the various
approaches explored by the participating teams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception. (arXiv:2312.17532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17532">
<div class="article-summary-box-inner">
<span><p>Quantities are distinct and critical components of texts that characterize
the magnitude properties of entities, providing a precise perspective for the
understanding of natural language, especially for reasoning tasks. In recent
years, there has been a flurry of research on reasoning tasks based on large
language models (LLMs), most of which solely focus on numerical values,
neglecting the dimensional concept of quantities with units despite its
importance. We argue that the concept of dimension is essential for precisely
understanding quantities and of great significance for LLMs to perform
quantitative reasoning. However, the lack of dimension knowledge and
quantity-related benchmarks has resulted in low performance of LLMs. Hence, we
present a framework to enhance the quantitative reasoning ability of language
models based on dimension perception. We first construct a dimensional unit
knowledge base (DimUnitKB) to address the knowledge gap in this area. We
propose a benchmark DimEval consisting of seven tasks of three categories to
probe and enhance the dimension perception skills of LLMs. To evaluate the
effectiveness of our methods, we propose a quantitative reasoning task and
conduct experiments. The experimental results show that our dimension
perception method dramatically improves accuracy (43.55%-&gt;50.67%) on
quantitative reasoning tasks compared to GPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of LLMs. (arXiv:2312.17535v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17535">
<div class="article-summary-box-inner">
<span><p>CoT (Chain-of-Thought) is a way to solve reasoning problems for LLMs .
Recently, many researches appear for improving the CoT capability of LLMs. In
this work, we also proposed Olapa-MCoT, which is a LLMs based on llama2-13B PLM
for finetuning and alignment learning. During the alignment training, we
proposed the SimRRHF algorithm and Incorrect Data Relearning and mainly focused
on optimizing the Chinese mathematical reasoning ability of Olapa-MCoT. The
experiment achieved significant results, with the accuracy of Chinese
mathematical reasoning up to 50%, 36% rise compared to llama2-13B. In addition,
the accuracy of English reasoning ability also increased by nearly 4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Efficient Universal Classifiers with Natural Language Inference. (arXiv:2312.17543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17543">
<div class="article-summary-box-inner">
<span><p>Generative Large Language Models (LLMs) have become the mainstream choice for
fewshot and zeroshot learning thanks to the universality of text generation.
Many users, however, do not need the broad capabilities of generative LLMs when
they only want to automate a classification task. Smaller BERT-like models can
also learn universal tasks, which allow them to do any text classification task
without requiring fine-tuning (zeroshot classification) or to learn new tasks
with only a few examples (fewshot), while being significantly more efficient
than generative LLMs. This paper (1) explains how Natural Language Inference
(NLI) can be used as a universal classification task that follows similar
principles as instruction fine-tuning of generative LLMs, (2) provides a
step-by-step guide with reusable Jupyter notebooks for building a universal
classifier, and (3) shares the resulting universal classifier that is trained
on 33 datasets with 389 diverse classes. Parts of the code we share has been
used to train our older zeroshot classifiers that have been downloaded more
than 55 million times via the Hugging Face Hub as of December 2023. Our new
classifier improves zeroshot performance by 9.4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17581">
<div class="article-summary-box-inner">
<span><p>The increased prevalence of online meetings has significantly enhanced the
practicality of a model that can automatically generate the summary of a given
meeting. This paper introduces a novel and effective approach to automate the
generation of meeting summaries. Current approaches to this problem generate
general and basic summaries, considering the meeting simply as a long dialogue.
However, our novel algorithms can generate abstractive meeting summaries that
are driven by the action items contained in the meeting transcript. This is
done by recursively generating summaries and employing our action-item
extraction algorithm for each section of the meeting in parallel. All of these
sectional summaries are then combined and summarized together to create a
coherent and action-item-driven summary. In addition, this paper introduces
three novel methods for dividing up long transcripts into topic-based sections
to improve the time efficiency of our algorithm, as well as to resolve the
issue of large language models (LLMs) forgetting long-term dependencies. Our
pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an
approximately 4.98% increase from the current state-of-the-art result produced
by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training. (arXiv:2312.17591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17591">
<div class="article-summary-box-inner">
<span><p>Feature attribution methods highlight the important input tokens as
explanations to model predictions, which have been widely applied to deep
neural networks towards trustworthy AI. However, recent works show that
explanations provided by these methods face challenges of being faithful and
robust. In this paper, we propose a method with Robustness improvement and
Explanation Guided training towards more faithful EXplanations (REGEX) for text
classification. First, we improve model robustness by input gradient
regularization technique and virtual adversarial training. Secondly, we use
salient ranking to mask noisy tokens and maximize the similarity between model
attention and feature attribution, which can be seen as a self-training
procedure without importing other external information. We conduct extensive
experiments on six datasets with five attribution methods, and also evaluate
the faithfulness in the out-of-domain setting. The results show that REGEX
improves fidelity metrics of explanations in all settings and further achieves
consistent gains based on two randomization tests. Moreover, we show that using
highlight explanations produced by REGEX to train select-then-predict models
results in comparable task performance to the end-to-end method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Generative Information Extraction: A Survey. (arXiv:2312.17617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17617">
<div class="article-summary-box-inner">
<span><p>Information extraction (IE) aims to extract structural knowledge (such as
entities, relations, and events) from plain natural language texts. Recently,
generative Large Language Models (LLMs) have demonstrated remarkable
capabilities in text understanding and generation, allowing for generalization
across various domains and tasks. As a result, numerous works have been
proposed to harness abilities of LLMs and offer viable solutions for IE tasks
based on a generative paradigm. To conduct a comprehensive systematic review
and exploration of LLM efforts for IE tasks, in this study, we survey the most
recent advancements in this field. We first present an extensive overview by
categorizing these works in terms of various IE subtasks and learning
paradigms, then we empirically analyze the most advanced methods and discover
the emerging trend of IE tasks with LLMs. Based on thorough review conducted,
we identify several insights in technique and promising research directions
that deserve further exploration in future studies. We maintain a public
repository and consistently update related resources at:
\url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on the Laws of Multimodal Perception and Cognition from a Cross-cultural Perspective -- Taking Overseas Chinese Gardens as an Example. (arXiv:2312.17642v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17642">
<div class="article-summary-box-inner">
<span><p>This study aims to explore the complex relationship between perceptual and
cognitive interactions in multimodal data analysis,with a specific emphasis on
spatial experience design in overseas Chinese gardens. It is found that
evaluation content and images on social media can reflect individuals' concerns
and sentiment responses, providing a rich data base for cognitive research that
contains both sentimental and image-based cognitive information. Leveraging
deep learning techniques, we analyze textual and visual data from social media,
thereby unveiling the relationship between people's perceptions and sentiment
cognition within the context of overseas Chinese gardens. In addition, our
study introduces a multi-agent system (MAS)alongside AI agents. Each agent
explores the laws of aesthetic cognition through chat scene simulation combined
with web search. This study goes beyond the traditional approach of translating
perceptions into sentiment scores, allowing for an extension of the research
methodology in terms of directly analyzing texts and digging deeper into
opinion data. This study provides new perspectives for understanding aesthetic
experience and its impact on architecture and landscape design across diverse
cultural contexts, which is an essential contribution to the field of cultural
communication and aesthetic understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Normalization of Lithuanian Text Using Regular Expressions. (arXiv:2312.17660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17660">
<div class="article-summary-box-inner">
<span><p>Text Normalization is an integral part of any text-to-speech synthesis
system. In a natural language text, there are elements such as numbers, dates,
abbreviations, etc. that belong to other semiotic classes. They are called
non-standard words (NSW) and need to be expanded into ordinary words. For this
purpose, it is necessary to identify the semiotic class of each NSW. The
taxonomy of semiotic classes adapted to the Lithuanian language is presented in
the work. Sets of rules are created for detecting and expanding NSWs based on
regular expressions. Experiments with three completely different data sets were
performed and the accuracy was assessed. Causes of errors are explained and
recommendations are given for the development of text normalization rules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models. (arXiv:2312.17661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17661">
<div class="article-summary-box-inner">
<span><p>The burgeoning interest in Multimodal Large Language Models (MLLMs), such as
OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial
realms. These models enhance Large Language Models (LLMs) with advanced visual
understanding capabilities, facilitating their application in a variety of
multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM
designed specifically for multimodal integration. Despite its advancements,
preliminary benchmarks indicate that Gemini lags behind GPT models in
commonsense reasoning tasks. However, this assessment, based on a limited
dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic
commonsense reasoning potential. To address this gap, our study undertakes a
thorough evaluation of Gemini's performance in complex reasoning tasks that
necessitate the integration of commonsense knowledge across modalities. We
carry out a comprehensive analysis of 12 commonsense reasoning datasets,
ranging from general to domain-specific tasks. This includes 11 datasets
focused solely on language, as well as one that incorporates multimodal
elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's
competitive commonsense reasoning capabilities. Additionally, we identify
common challenges faced by current LLMs and MLLMs in addressing commonsense
problems, underscoring the need for further advancements in enhancing the
commonsense reasoning abilities of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jatmo: Prompt Injection Defense by Task-Specific Finetuning. (arXiv:2312.17673v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17673">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are attracting significant research attention
due to their instruction-following abilities, allowing users and developers to
leverage LLMs for a variety of tasks. However, LLMs are vulnerable to
prompt-injection attacks: a class of attacks that hijack the model's
instruction-following abilities, changing responses to prompts to undesired,
possibly malicious ones. In this work, we introduce Jatmo, a method for
generating task-specific models resilient to prompt-injection attacks. Jatmo
leverages the fact that LLMs can only follow instructions once they have
undergone instruction tuning. It harnesses a teacher instruction-tuned model to
generate a task-specific dataset, which is then used to fine-tune a base model
(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a
dataset of inputs for the task: it uses the teacher model to generate outputs.
For situations with no pre-existing datasets, Jatmo can use a single example,
or in some cases none at all, to produce a fully synthetic dataset. Our
experiments on six tasks show that Jatmo models provide the same quality of
outputs on their specific task as standard LLMs, while being resilient to
prompt injections. The best attacks succeeded in less than 0.5% of cases
against our models, versus over 90% success rate against GPT-3.5-Turbo. We
release Jatmo at https://github.com/wagner-group/prompt-injection-defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TuPy-E: detecting hate speech in Brazilian Portuguese social media with a novel dataset and comprehensive analysis of models. (arXiv:2312.17704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17704">
<div class="article-summary-box-inner">
<span><p>Social media has become integral to human interaction, providing a platform
for communication and expression. However, the rise of hate speech on these
platforms poses significant risks to individuals and communities. Detecting and
addressing hate speech is particularly challenging in languages like Portuguese
due to its rich vocabulary, complex grammar, and regional variations. To
address this, we introduce TuPy-E, the largest annotated Portuguese corpus for
hate speech detection. TuPy-E leverages an open-source approach, fostering
collaboration within the research community. We conduct a detailed analysis
using advanced techniques like BERT models, contributing to both academic
understanding and practical applications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Principled Gradient-based Markov Chain Monte Carlo for Text Generation. (arXiv:2312.17710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17710">
<div class="article-summary-box-inner">
<span><p>Recent papers have demonstrated the possibility of energy-based text
generation by adapting gradient-based sampling algorithms, a paradigm of MCMC
algorithms that promises fast convergence. However, as we show in this paper,
previous attempts on this approach to text generation all fail to sample
correctly from the target language model distributions. To address this
limitation, we consider the problem of designing text samplers that are
faithful, meaning that they have the target text distribution as its limiting
distribution. We propose several faithful gradient-based sampling algorithms to
sample from the target energy-based text distribution correctly, and study
their theoretical properties. Through experiments on various forms of text
generation, we demonstrate that faithful samplers are able to generate more
fluent text while adhering to the control objectives better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating Hanja Historical Documents to Contemporary Korean and English. (arXiv:2205.10019v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10019">
<div class="article-summary-box-inner">
<span><p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of
Joseon, the 500-year kingdom preceding the modern nation of Korea. The Annals
were originally written in an archaic Korean writing system, `Hanja', and were
translated into Korean from 1968 to 1993. The resulting translation was however
too literal and contained many archaic Korean words; thus, a new expert
translation effort began in 2012. Since then, the records of only one king have
been completed in a decade. In parallel, expert translators are working on
English translation, also at a slow pace and produced only one king's records
in English so far. Thus, we propose H2KE, a neural machine translation model,
that translates historical documents in Hanja to more easily understandable
Korean and to English. Built on top of multilingual neural machine translation,
H2KE learns to translate a historical document written in Hanja, from both a
full dataset of outdated Korean translation and a small dataset of more
recently translated contemporary Korean and English. We compare our method
against two baselines: a recent model that simultaneously learns to restore and
translate Hanja historical document and a Transformer based model trained only
on newly translated corpora. The experiments reveal that our method
significantly outperforms the baselines in terms of BLEU scores for both
contemporary Korean and English translations. We further conduct extensive
human evaluation which shows that our translation is preferred over the
original expert translations by both experts and non-expert Korean speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Lifelong Learning. (arXiv:2205.11152v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11152">
<div class="article-summary-box-inner">
<span><p>The longstanding goal of multi-lingual learning has been to develop a
universal cross-lingual model that can withstand the changes in multi-lingual
data distributions. There has been a large amount of work to adapt such
multi-lingual models to unseen target languages. However, the majority of work
in this direction focuses on the standard one-hop transfer learning pipeline
from source to target languages, whereas in realistic scenarios, new languages
can be incorporated at any time in a sequential manner. In this paper, we
present a principled Cross-lingual Continual Learning (CCL) evaluation
paradigm, where we analyze different categories of approaches used to
continually adapt to emerging data from different languages. We provide
insights into what makes multilingual sequential learning particularly
challenging. To surmount such challenges, we benchmark a representative set of
cross-lingual continual learning algorithms and analyze their knowledge
preservation, accumulation, and generalization capabilities compared to
baselines on carefully curated datastreams. The implications of this analysis
include a recipe for how to measure and balance different cross-lingual
continual learning desiderata, which go beyond conventional transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Math Word Problems via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16257">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (PLMs) bring new opportunities to
challenging problems, especially those that need high-level intelligence, such
as the math word problem (MWPs). However, directly applying existing PLMs to
MWPs can fail as the generation process lacks sufficient supervision and thus
lacks fast adaptivity as humans. We notice that human reasoning has a dual
reasoning framework that consists of an immediate reaction system (system 1)
and a delicate reasoning system (system 2), where the entire reasoning is
determined by their interaction. This inspires us to develop a cooperative
reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),
resulting in a human-like reasoning architecture with system 1 as the generator
and system 2 as the verifier. In our approach, the generator is responsible for
generating reasoning paths, and the verifiers are used to supervise the
evaluation in order to obtain reliable feedback for the generator. We evaluate
our CoRe framework on several mathematical reasoning datasets and achieve
decent improvement over state-of-the-art methods, up to 9.6% increase over best
baselines. Our codes are available at https://github.com/TianHongZXY/CoRe
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Prompts for Text-to-Image Generation. (arXiv:2212.09611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09611">
<div class="article-summary-box-inner">
<span><p>Well-designed prompts can guide text-to-image models to generate amazing
images. However, the performant prompts are often model-specific and misaligned
with user input. Instead of laborious human engineering, we propose prompt
adaptation, a general framework that automatically adapts original user input
to model-preferred prompts. Specifically, we first perform supervised
fine-tuning with a pretrained language model on a small collection of manually
engineered prompts. Then we use reinforcement learning to explore better
prompts. We define a reward function that encourages the policy to generate
more aesthetically pleasing images while preserving the original user
intentions. Experimental results on Stable Diffusion show that our method
outperforms manual prompt engineering in terms of both automatic metrics and
human preference ratings. Moreover, reinforcement learning further boosts
performance, especially on out-of-domain prompts. The pretrained checkpoints
are available at https://aka.ms/promptist. The demo can be found at
https://aka.ms/promptist-demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v9 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03109">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15427">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable generalization capability
with exceptional performance in various language modeling tasks. However, they
still exhibit inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored utilizing knowledge graphs
(KGs) to enhance language modeling via joint training and customized model
architectures, applying this to LLMs is problematic owing to their large number
of parameters and high computational cost. Therefore, how to enhance
pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented
generation, remains an open question. In this work, we propose Graph Neural
Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in
learning beneficial knowledge from KGs. GNP encompasses various designs,
including a standard graph neural network encoder, a cross-modality pooling
module, a domain projector, and a self-supervised link prediction objective.
Extensive experiments on multiple datasets demonstrate the superiority of GNP
on both commonsense and biomedical reasoning tasks across different LLM sizes
and settings. Code is available at https://github.com/meettyj/GNP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes. (arXiv:2310.15959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15959">
<div class="article-summary-box-inner">
<span><p>We introduce NoteChat, a novel cooperative multi-agent framework leveraging
Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat
embodies the principle that an ensemble of role-specific LLMs, through
structured role-play and strategic prompting, can perform their assigned roles
more effectively. The synergy among these role-playing LLMs results in a
cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a
benchmark dataset for patient-physician dialogues-note pairs, shows that models
trained with the augmented synthetic patient-physician dialogues by NoteChat
outperforms other state-of-the-art models for generating clinical notes. Our
comprehensive automatic and human evaluation demonstrates that NoteChat
substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to
22.78% by domain experts in generating superior synthetic patient-physician
dialogues based on clinical notes. NoteChat has the potential to engage
patients directly and help clinical documentation, a leading cause of physician
burnout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords. (arXiv:2311.12475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.12475">
<div class="article-summary-box-inner">
<span><p>While WangchanBERTa has become the de facto standard in transformer-based
Thai language modeling, it still has shortcomings in regard to the
understanding of foreign words, most notably English words, which are often
borrowed without orthographic assimilation into Thai in many contexts. We
identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the
main source of these shortcomings. We then expand WangchanBERTa's vocabulary
via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new
model using the expanded tokenizer, starting from WangchanBERTa's checkpoint,
on a new dataset that is larger than the one used to train WangchanBERTa. Our
results show that our new pretrained model, PhayaThaiBERT, outperforms
WangchanBERTa in many downstream tasks and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction. (arXiv:2312.03022v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.03022">
<div class="article-summary-box-inner">
<span><p>Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling SQL Query Hardness Parsing for Text-to-SQL. (arXiv:2312.06172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.06172">
<div class="article-summary-box-inner">
<span><p>The fundamental goal of the Text-to-SQL task is to translate natural language
question into SQL query. Current research primarily emphasizes the information
coupling between natural language questions and schemas, and significant
progress has been made in this area. The natural language questions as the
primary task requirements source determines the hardness of correspond SQL
queries, the correlation between the two always be ignored. However, when the
correlation between questions and queries was decoupled, it may simplify the
task. In this paper, we introduce an innovative framework for Text-to-SQL based
on decoupling SQL query hardness parsing. This framework decouples the
Text-to-SQL task based on query hardness by analyzing questions and schemas,
simplifying the multi-hardness task into a single-hardness challenge. This
greatly reduces the parsing pressure on the language model. We evaluate our
proposed framework and achieve a new state-of-the-art performance of
fine-turning methods on Spider dev.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.09085">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs' susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs' belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs' correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.10997">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Augmented Generation (RAG) has emerged as a
promising solution to these issues by incorporating real-time data from
external databases into LLM responses. This enhances the accuracy and
credibility of the models, particularly for knowledge-intensive tasks, and
allows for continuous knowledge updates and integration of domain-specific
information. RAG synergistically merges LLMs' intrinsic knowledge with the
vast, dynamic repositories of external databases. This survey paper provides an
in-depth analysis of the evolution of RAG, focusing on three key paradigms:
Naive RAG, Advanced RAG, and Modular RAG. It methodically examines the three
fundamental components of RAG systems: the retriever, the generator, and the
augmentation methods, underscoring the cutting-edge technologies within each
componenet. Additionally, the paper introduces novel metrics and capabilities
for evaluating RAG models, as well as the most recent evaluation framework.
Finally, the paper outlines future research directions from three perspectives:
future challenges,modality extension,and the development of the RAG technical
stack and ecosystem
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model (LLM) Bias Index -- LLMBI. (arXiv:2312.14769v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.14769">
<div class="article-summary-box-inner">
<span><p>The Large Language Model Bias Index (LLMBI) is a pioneering approach designed
to quantify and address biases inherent in large language models (LLMs), such
as GPT-4. We recognise the increasing prevalence and impact of LLMs across
diverse sectors. This research introduces a novel metric, LLMBI, to
systematically measure and mitigate biases potentially skewing model responses.
We formulated LLMBI using a composite scoring system incorporating multiple
dimensions of bias, including but not limited to age, gender, and racial
biases. To operationalise this metric, we engaged in a multi-step process
involving collecting and annotating LLM responses, applying sophisticated
Natural Language Processing (NLP) techniques for bias detection, and computing
the LLMBI score through a specially crafted mathematical formula. The formula
integrates weighted averages of various bias dimensions, a penalty for dataset
diversity deficiencies, and a correction for sentiment biases. Our empirical
analysis, conducted using responses from OpenAI's API, employs advanced
sentiment analysis as a representative method for bias detection. The research
reveals LLMs, whilst demonstrating impressive capabilities in text generation,
exhibit varying degrees of bias across different dimensions. LLMBI provides a
quantifiable measure to compare biases across models and over time, offering a
vital tool for systems engineers, researchers and regulators in enhancing the
fairness and reliability of LLMs. It highlights the potential of LLMs in
mimicking unbiased human-like responses. Additionally, it underscores the
necessity of continuously monitoring and recalibrating such models to align
with evolving societal norms and ethical standards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. (arXiv:2312.15166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.15166">
<div class="article-summary-box-inner">
<span><p>We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion
parameters, demonstrating superior performance in various natural language
processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale
LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which
encompasses depthwise scaling and continued pretraining. In contrast to other
LLM up-scaling methods that use mixture-of-experts, DUS does not require
complex changes to train and inference efficiently. We show experimentally that
DUS is simple yet effective in scaling up high-performance LLMs from small
ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,
a variant fine-tuned for instruction-following capabilities, surpassing
Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0
license, promoting broad access and application in the LLM field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis. (arXiv:2312.16374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.16374">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized various domains with
extensive knowledge and creative capabilities. However, a critical issue with
LLMs is their tendency to produce outputs that diverge from factual reality.
This phenomenon is particularly concerning in sensitive applications such as
medical consultation and legal advice, where accuracy is paramount. In this
paper, we introduce the LLM factoscope, a novel Siamese network-based model
that leverages the inner states of LLMs for factual detection. Our
investigation reveals distinguishable patterns in LLMs' inner states when
generating factual versus non-factual content. We demonstrate the LLM
factoscope's effectiveness across various architectures, achieving over 96%
accuracy in factual detection. Our work opens a new avenue for utilizing LLMs'
inner states for factual detection and encourages further exploration into
LLMs' inner workings for enhanced reliability and transparency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiential Co-Learning of Software-Developing Agents. (arXiv:2312.17025v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17025">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have brought significant
changes to various domains, especially through LLM-driven autonomous agents.
These agents are now capable of collaborating seamlessly, splitting tasks and
enhancing accuracy, thus minimizing the need for human involvement. However,
these agents often approach a diverse range of tasks in isolation, without
benefiting from past experiences. This isolation can lead to repeated mistakes
and inefficient trials in task solving. To this end, this paper introduces
Experiential Co-Learning, a novel framework in which instructor and assistant
agents gather shortcut-oriented experiences from their historical trajectories
and use these past experiences for mutual reasoning. This paradigm, enriched
with previous experiences, equips agents to more effectively address unseen
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding. (arXiv:2312.17044v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17044">
<div class="article-summary-box-inner">
<span><p>Transformer has taken the natural language processing (NLP) field by storm
since birth, owing to its superior ability to model complex dependencies in
sequences. Despite the great success of pretrained language models (PLMs) based
on Transformer across almost all NLP tasks, they all suffer from a preset
length limit and thus can hardly extend this success to longer sequences beyond
seen data, namely the length extrapolation problem. Length extrapolation has
aroused great interest among researchers, as it is the core feature of human
language capacity. To enhance length extrapolation of Transformers, a plethora
of methods have been proposed, mostly focusing on extrapolatable position
encodings. In this article, we provide an organized and systematical review of
these research efforts in a unified notation from a position encoding
perspective, aiming to enable the reader to gain a deep understanding of
existing methods and provide stimuli for future research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-01 23:12:17.285917691 UTC">2024-01-01 23:12:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>