<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-25T01:30:00Z">10-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. (arXiv:2310.15205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15205">
<div class="article-summary-box-inner">
<span><p>We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15213">
<div class="article-summary-box-inner">
<span><p>We report the presence of a simple neural mechanism that represents an
input-output function as a vector within autoregressive transformer language
models (LMs). Using causal mediation analysis on a diverse range of
in-context-learning (ICL) tasks, we find that a small number attention heads
transport a compact representation of the demonstrated task, which we call a
function vector (FV). FVs are robust to changes in context, i.e., they trigger
execution of the task on inputs such as zero-shot and natural text settings
that do not resemble the ICL contexts from which they are collected. We test
FVs across a range of tasks, models, and layers and find strong causal effects
across settings in middle layers. We investigate the internal structure of FVs
and find while that they often contain information that encodes the output
space of the function, this information alone is not sufficient to reconstruct
an FV. Finally, we test semantic vector composition in FVs, and find that to
some extent they can be summed to create vectors that trigger new complex
tasks. Taken together, our findings suggest that LLMs contain internal
abstractions of general-purpose functions that can be invoked in a variety of
contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15239">
<div class="article-summary-box-inner">
<span><p>Recent efforts in natural language processing (NLP) commonsense reasoning
research have yielded a considerable number of new datasets and benchmarks.
However, most of these datasets formulate commonsense reasoning challenges in
artificial scenarios that are not reflective of the tasks which real-world NLP
systems are designed to solve. In this work, we present CRoW, a
manually-curated, multi-task benchmark that evaluates the ability of models to
apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is
constructed using a multi-stage data collection pipeline that rewrites examples
from existing datasets using commonsense-violating perturbations. We use CRoW
to study how NLP systems perform across different dimensions of commonsense
knowledge, such as physical, temporal, and social reasoning. We find a
significant performance gap when NLP systems are evaluated on CRoW compared to
humans, showcasing that commonsense reasoning is far from being solved in
real-world task settings. We make our dataset and leaderboard available to the
research community at https://github.com/mismayil/crow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention. (arXiv:2310.15258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15258">
<div class="article-summary-box-inner">
<span><p>In this work, we study whether multilingual language models (MultiLMs) can
transfer logical reasoning abilities to other languages when they are
fine-tuned for reasoning in a different language. We evaluate the cross-lingual
reasoning abilities of MultiLMs in two schemes: (1) where the language of the
context and the question remain the same in the new languages that are tested
(i.e., the reasoning is still monolingual, but the model must transfer the
learned reasoning ability across languages), and (2) where the language of the
context and the question is different (which we term code-switched reasoning).
On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate
that although MultiLMs can transfer reasoning ability across languages in a
monolingual setting, they struggle to transfer reasoning abilities in a
code-switched setting. Following this observation, we propose a novel attention
mechanism that uses a dedicated set of parameters to encourage cross-lingual
attention in code-switched sequences, which improves the reasoning performance
by up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15259">
<div class="article-summary-box-inner">
<span><p>Community Question-Answering (CQA) portals serve as a valuable tool for
helping users within an organization. However, making them accessible to
non-English-speaking users continues to be a challenge. Translating questions
can broaden the community's reach, benefiting individuals with similar
inquiries in various languages. Translating questions using Neural Machine
Translation (NMT) poses more challenges, especially in noisy environments,
where the grammatical correctness of the questions is not monitored. These
questions may be phrased as statements by non-native speakers, with incorrect
subject-verb order and sometimes even missing question marks. Creating a
synthetic parallel corpus from such data is also difficult due to its noisy
nature. To address this issue, we propose a training methodology that
fine-tunes the NMT system only using source-side data. Our approach balances
adequacy and fluency by utilizing a loss function that combines BERTScore and
Masked Language Model (MLM) Score. Our method surpasses the conventional
Maximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on
synthetic target data, by achieving a 1.9 BLEU score improvement. Our model
exhibits robustness while we add noise to our baseline, and still achieve 1.1
BLEU improvement and large improvements on TER and BLEURT metrics. Our proposed
methodology is model-agnostic and is only necessary during the training phase.
We make the codes and datasets publicly available at
\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt} for
facilitating further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study. (arXiv:2310.15262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15262">
<div class="article-summary-box-inner">
<span><p>Code-switching (CSW) text generation has been receiving increasing attention
as a solution to address data scarcity. In light of this growing interest, we
need more comprehensive studies comparing different augmentation approaches. In
this work, we compare three popular approaches: lexical replacements,
linguistic theories, and back-translation (BT), in the context of Egyptian
Arabic-English CSW. We assess the effectiveness of the approaches on machine
translation and the quality of augmentations through human evaluation. We show
that BT and CSW predictive-based lexical replacement, being trained on CSW
parallel data, perform best on both tasks. Linguistic theories and random
lexical replacement prove to be effective in the lack of CSW parallel data,
where both approaches achieve similar results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey. (arXiv:2310.15264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15264">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15269">
<div class="article-summary-box-inner">
<span><p>Most languages of the world pose low-resource challenges to natural language
processing models. With multilingual training, knowledge can be shared among
languages. However, not all languages positively influence each other and it is
an open research question how to select the most suitable set of languages for
multilingual training and avoid negative interference among languages whose
characteristics or data distributions are not compatible. In this paper, we
propose GradSim, a language grouping method based on gradient similarity. Our
experiments on three diverse multilingual benchmark datasets show that it leads
to the largest performance gains compared to other similarity measures and it
is better correlated with cross-lingual model performance. As a result, we set
the new state of the art on AfriSenti, a benchmark dataset for sentiment
analysis on low-resource African languages. In our extensive analysis, we
further reveal that besides linguistic features, the topics of the datasets
play an important role for language grouping and that lower layers of
transformer models encode language-specific features while higher layers
capture task-specific information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages. (arXiv:2310.15276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15276">
<div class="article-summary-box-inner">
<span><p>The class of tree-adjoining languages can be characterized by various
two-level formalisms, consisting of a context-free grammar (CFG) or pushdown
automaton (PDA) controlling another CFG or PDA. These four formalisms are
equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG),
pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We
define semiring-weighted versions of the above two-level formalisms, and we
design new algorithms for computing their stringsums (the weight of all
derivations of a string) and allsums (the weight of all derivations). From
these, we also immediately obtain stringsum and allsum algorithms for TAG, LIG,
PAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of
$\mathcal{O}(n|\mathcal{N}|)$ (where $n$ is the string length and
$|\mathcal{N}|$ is the size of the nonterminal set) and more space-efficient by
a factor of $\mathcal{O}(|\Gamma|)$ (where $|\Gamma|$ is the size of the stack
alphabet) than the algorithm of Vijay-Shanker and Weir (1989). For EPDA, our
algorithm is both more space-efficient and time-efficient than the algorithm of
Alonso et al. (2001) by factors of $\mathcal{O}(|\Gamma|^2)$ and
$\mathcal{O}(|\Gamma|^3)$, respectively. Finally, we give the first PAA
stringsum and allsum algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Dimensionality of Sentence Embeddings. (arXiv:2310.15285v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15285">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings is a fundamental problem in natural language
processing. While existing research primarily focuses on enhancing the quality
of sentence embeddings, the exploration of sentence embedding dimensions is
limited. Here we present a comprehensive and empirical analysis of the
dimensionality of sentence embeddings. First, we demonstrate that the optimal
dimension of sentence embeddings is usually smaller than the default value.
Subsequently, to compress the dimension of sentence embeddings with minimum
performance degradation, we identify two components contributing to the overall
performance loss: the encoder's performance loss and the pooler's performance
loss. Therefore, we propose a two-step training method for sentence
representation learning models, wherein the encoder and the pooler are
optimized separately to mitigate the overall performance loss in low-dimension
scenarios. Experimental results on seven STS tasks and seven sentence
classification tasks demonstrate that our method significantly improves the
performance of low-dimensional sentence embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling. (arXiv:2310.15294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15294">
<div class="article-summary-box-inner">
<span><p>Recently slot filling has witnessed great development thanks to deep learning
and the availability of large-scale annotated data. However, it poses a
critical challenge to handle a novel domain whose samples are never seen during
training. The recognition performance might be greatly degraded due to severe
domain shifts. Most prior works deal with this problem in a two-pass pipeline
manner based on metric learning. In practice, these dominant pipeline models
may be limited in computational efficiency and generalization capacity because
of non-parallel inference and context-free discrete label embeddings. To this
end, we re-examine the typical metric-based methods, and propose a new adaptive
end-to-end metric learning scheme for the challenging zero-shot slot filling.
Considering simplicity, efficiency and generalizability, we present a
cascade-style joint learning framework coupled with context-aware soft label
representations and slot-level contrastive representation learning to mitigate
the data and label shift problems effectively. Extensive experiments on public
benchmarks demonstrate the superiority of the proposed approach over a series
of competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15296">
<div class="article-summary-box-inner">
<span><p>In the burgeoning field of natural language processing, Neural Topic Models
(NTMs) and Large Language Models (LLMs) have emerged as areas of significant
research interest. Despite this, NTMs primarily utilize contextual embeddings
from LLMs, which are not optimal for clustering or capable for topic
generation. Our study addresses this gap by introducing a novel framework named
Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).
DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable
embeddings that could generate topics that exhibit both superior clusterability
and enhanced semantic coherence compared to existing methods. Additionally, by
exploiting the power of diffusion, our framework also provides the capability
to generate content relevant to the identified topics. This dual functionality
allows users to efficiently produce highly clustered topics and related content
simultaneously. DeTiME's potential extends to generating clustered embeddings
as well. Notably, our proposed framework proves to be efficient to train and
exhibits high adaptability, demonstrating its potential for a wide array of
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15298">
<div class="article-summary-box-inner">
<span><p>The popularity of conversational digital assistants has resulted in the
availability of large amounts of conversational data which can be utilized for
improved user experience and personalized response generation. Building these
assistants using popular large language models like ChatGPT also require
additional emphasis on prompt engineering and evaluation methods. Textual
similarity metrics are a key ingredient for such analysis and evaluations.
While many similarity metrics have been proposed in the literature, they have
not proven effective for task-oriented conversations as they do not take
advantage of unique conversational features. To address this gap, we present
TaskDiff, a novel conversational similarity metric that utilizes different
dialogue components (utterances, intents, and slots) and their distributions to
compute similarity. Extensive experimental evaluation of TaskDiff on a
benchmark dataset demonstrates its superior performance and improved robustness
over other related approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City. (arXiv:2310.15302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15302">
<div class="article-summary-box-inner">
<span><p>Critical toponymy examines the dynamics of power, capital, and resistance
through place names and the sites to which they refer. Studies here have
traditionally focused on the semantic content of toponyms and the top-down
institutional processes that produce them. However, they have generally ignored
the ways in which toponyms are used by ordinary people in everyday discourse,
as well as the other strategies of geospatial description that accompany and
contextualize toponymic reference. Here, we develop computational methods to
measure how cultural and economic capital shape the ways in which people refer
to places, through a novel annotated dataset of 47,440 New York City Airbnb
listings from the 2010s. Building on this dataset, we introduce a new named
entity recognition (NER) model able to identify important discourse categories
integral to the characterization of place. Our findings point toward new
directions for critical toponymy and to a range of previously understudied
linguistic signals relevant to research on neighborhood status, housing and
tourism markets, and gentrification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15316">
<div class="article-summary-box-inner">
<span><p>The probing classifiers framework has been employed for interpreting deep
neural network models for a variety of natural language processing (NLP)
applications. Studies, however, have largely focused on sentencelevel NLP
tasks. This work is the first to apply the probing paradigm to representations
learned for document-level information extraction (IE). We designed eight
embedding probes to analyze surface, semantic, and event-understanding
capabilities relevant to document-level event extraction. We apply them to the
representations acquired by learning models from three different LLM-based
document-level IE approaches on a standard dataset. We found that trained
encoders from these models yield embeddings that can modestly improve argument
detections and labeling but only slightly enhance event-level tasks, albeit
trade-offs in information helpful for coherence and event-type prediction. We
further found that encoder models struggle with document length and
cross-sentence discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses. (arXiv:2310.15317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15317">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15319">
<div class="article-summary-box-inner">
<span><p>We investigate the problem of generating instructions to guide humans to
navigate in simulated residential environments. A major issue with current
models is hallucination: they generate references to actions or objects that
are inconsistent with what a human follower would perform or encounter along
the described path. We develop a model that detects these hallucinated
references by adopting a model pre-trained on a large corpus of image-text
pairs, and fine-tuning it with a contrastive loss that separates correct
instructions from instructions containing synthesized hallucinations. Our final
model outperforms several baselines, including using word probability estimated
by the instruction-generation model, and supervised models based on LSTM and
Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LXMERT Model Compression for Visual Question Answering. (arXiv:2310.15325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15325">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained models such as LXMERT are becoming popular for
learning cross-modal representations on text-image pairs for vision-language
tasks. According to the lottery ticket hypothesis, NLP and computer vision
models contain smaller subnetworks capable of being trained in isolation to
full performance. In this paper, we combine these observations to evaluate
whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA
task. In addition, we perform a model size cost-benefit analysis by
investigating how much pruning can be done without significant loss in
accuracy. Our experiment results demonstrate that LXMERT can be effectively
pruned by 40%-60% in size with 3% loss in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15326">
<div class="article-summary-box-inner">
<span><p>The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15337">
<div class="article-summary-box-inner">
<span><p>Moral foundations theory (MFT) is a psychological assessment tool that
decomposes human moral reasoning into five factors, including care/harm,
liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary
in the weight they place on these dimensions when making moral decisions, in
part due to their cultural upbringing and political ideology. As large language
models (LLMs) are trained on datasets collected from the internet, they may
reflect the biases that are present in such corpora. This paper uses MFT as a
lens to analyze whether popular LLMs have acquired a bias towards a particular
set of moral values. We analyze known LLMs and find they exhibit particular
moral foundations, and show how these relate to human moral foundations and
political affiliations. We also measure the consistency of these biases, or
whether they vary strongly depending on the context of how the model is
prompted. Finally, we show that we can adversarially select prompts that
encourage the moral to exhibit a particular set of moral foundations, and that
this can affect the model's behavior on downstream tasks. These findings help
illustrate the potential risks and unintended consequences of LLMs assuming a
particular moral stance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation. (arXiv:2310.15355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15355">
<div class="article-summary-box-inner">
<span><p>We show that LLMs hallucinate because their output is not constrained to be
synonymous with claims for which they have evidence: a condition that we call
evidential closure. Information about the truth or falsity of sentences is not
statistically identified in the standard neural probabilistic language model
setup, and so cannot be conditioned on to generate new strings. We then show
how to constrain LLMs to produce output that does satisfy evidential closure. A
multimodal LLM must learn about the external world (perceptual learning); it
must learn a mapping from strings to states of the world (extensional
learning); and, to achieve fluency when generalizing beyond a body of evidence,
it must learn mappings from strings to their synonyms (intensional learning).
The output of a unimodal LLM must be synonymous with strings in a validated
evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,
that yields faithful output from an LLM by rejecting output that is not
synonymous with claims for which the LLM has evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15372">
<div class="article-summary-box-inner">
<span><p>In the age of artificial intelligence, the role of large language models
(LLMs) is becoming increasingly central. Despite their growing prevalence,
their capacity to consolidate knowledge from different training documents - a
crucial ability in numerous applications - remains unexplored. This paper
presents the first study examining the capability of LLMs to effectively
combine such information within their parameter space. We introduce EpiK-Eval,
a novel question-answering benchmark tailored to evaluate LLMs' proficiency in
formulating a coherent and consistent knowledge representation from segmented
narratives. Evaluations across various LLMs reveal significant weaknesses in
this domain. We contend that these shortcomings stem from the intrinsic nature
of prevailing training objectives. Consequently, we advocate for refining the
approach towards knowledge consolidation, as it harbors the potential to
dramatically improve their overall effectiveness and performance. The findings
from this study offer insights for developing more robust and reliable LLMs.
Our code and benchmark are available at
https://github.com/chandar-lab/EpiK-Eval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Data Management in Data Lakes. (arXiv:2310.15373v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15373">
<div class="article-summary-box-inner">
<span><p>In recent years, data lakes emerged as away to manage large amounts of
heterogeneous data for modern data analytics. One way to prevent data lakes
from turning into inoperable data swamps is semantic data management. Some
approaches propose the linkage of metadata to knowledge graphs based on the
Linked Data principles to provide more meaning and semantics to the data in the
lake. Such a semantic layer may be utilized not only for data management but
also to tackle the problem of data integration from heterogeneous sources, in
order to make data access more expressive and interoperable. In this survey, we
review recent approaches with a specific focus on the application within data
lake systems and scalability to Big Data. We classify the approaches into (i)
basic semantic data management, (ii) semantic modeling approaches for enriching
metadata in data lakes, and (iii) methods for ontologybased data access. In
each category, we cover the main techniques and their background, and compare
latest research. Finally, we point out challenges for future work in this
research area, which needs a closer integration of Big Data and Semantic Web
technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GD-COMET: A Geo-Diverse Commonsense Inference Model. (arXiv:2310.15383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15383">
<div class="article-summary-box-inner">
<span><p>With the increasing integration of AI into everyday life, it's becoming
crucial to design AI systems that serve users from diverse backgrounds by
making them culturally aware. In this paper, we present GD-COMET, a geo-diverse
version of the COMET commonsense inference model. GD-COMET goes beyond Western
commonsense knowledge and is capable of generating inferences pertaining to a
broad range of cultures. We demonstrate the effectiveness of GD-COMET through a
comprehensive human evaluation across 5 diverse cultures, as well as extrinsic
evaluation on a geo-diverse task. The evaluation shows that GD-COMET captures
and generates culturally nuanced commonsense knowledge, demonstrating its
potential to benefit NLP applications across the board and contribute to making
NLP more inclusive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15389">
<div class="article-summary-box-inner">
<span><p>Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15393">
<div class="article-summary-box-inner">
<span><p>The coverage and composition of the pretraining data corpus significantly
impacts the generalization ability of large language models. Conventionally,
the pretraining corpus is composed of various source domains (e.g. CommonCrawl,
Wikipedia, Github etc.) according to certain sampling probabilities (domain
weights). However, current methods lack a principled way to optimize domain
weights for ultimate goal for generalization. We propose DOmain reweighting
with Generalization Estimation (DoGE), where we reweigh the sampling
probability from each domain based on its contribution to the final
generalization objective assessed by a gradient-based generalization estimation
function. First, we train a small-scale proxy model with a min-max optimization
to obtain the reweighted domain weights. At each step, the domain weights are
updated to maximize the overall generalization gain by mirror descent. Finally
we use the obtained domain weights to train a larger scale full-size language
model. On SlimPajama-6B dataset, with universal generalization objective, DoGE
achieves better average perplexity and zero-shot reasoning accuracy. On
out-of-domain generalization tasks, DoGE reduces perplexity on the target
domain by a large margin. We further apply a parameter-selection scheme which
improves the efficiency of generalization estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15398">
<div class="article-summary-box-inner">
<span><p>Fairness-related assumptions about what constitutes appropriate NLG system
behaviors range from invariance, where systems are expected to respond
identically to social groups, to adaptation, where responses should instead
vary across them. We design and conduct five case studies, in which we perturb
different types of identity-related language features (names, roles, locations,
dialect, and style) in NLG system inputs to illuminate tensions around
invariance and adaptation. We outline people's expectations of system
behaviors, and surface potential caveats of these two contrasting yet
commonly-held assumptions. We find that motivations for adaptation include
social norms, cultural differences, feature-specific information, and
accommodation; motivations for invariance include perspectives that favor
prescriptivism, view adaptation as unnecessary or too difficult for NLG systems
to do appropriately, and are wary of false assumptions. Our findings highlight
open challenges around defining what constitutes fair NLG system behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15405">
<div class="article-summary-box-inner">
<span><p>There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation. (arXiv:2310.15415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15415">
<div class="article-summary-box-inner">
<span><p>Knowing how to end and resume conversations over time is a natural part of
communication, allowing for discussions to span weeks, months, or years. The
duration of gaps between conversations dictates which topics are relevant and
which questions to ask, and dialogue systems which do not explicitly model time
may generate responses that are unnatural. In this work we explore the idea of
making dialogue models aware of time, and present GapChat, a multi-session
dialogue dataset in which the time between each session varies. While the
dataset is constructed in real-time, progress on events in speakers' lives is
simulated in order to create realistic dialogues occurring across a long
timespan. We expose time information to the model and compare different
representations of time and event progress. In human evaluation we show that
time-aware models perform better in metrics that judge the relevance of the
chosen topics and the information gained from the conversation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the Pretrained Language Models "Imagine" for Short Texts Topic Modeling. (arXiv:2310.15420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15420">
<div class="article-summary-box-inner">
<span><p>Topic models are one of the compelling methods for discovering latent
semantics in a document collection. However, it assumes that a document has
sufficient co-occurrence information to be effective. However, in short texts,
co-occurrence information is minimal, which results in feature sparsity in
document representation. Therefore, existing topic models (probabilistic or
neural) mostly fail to mine patterns from them to generate coherent topics. In
this paper, we take a new approach to short-text topic modeling to address the
data-sparsity issue by extending short text into longer sequences using
existing pre-trained language models (PLMs). Besides, we provide a simple
solution extending a neural topic model to reduce the effect of noisy
out-of-topics text generation from PLMs. We observe that our model can
substantially improve the performance of short-text topic modeling. Extensive
experiments on multiple real-world datasets under extreme data sparsity
scenarios show that our models can generate high-quality topics outperforming
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15421">
<div class="article-summary-box-inner">
<span><p>Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation. (arXiv:2310.15425v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15425">
<div class="article-summary-box-inner">
<span><p>Forced alignment systems automatically determine boundaries between segments
in speech data, given an orthographic transcription. These tools are
commonplace in phonetics to facilitate the use of speech data that would be
infeasible to manually transcribe and segment. In the present paper, we
describe a new neural network-based forced alignment system, the Mason-Alberta
Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two
possible improvements we pursue for forced alignment systems. The first is
treating the acoustic model in a forced aligner as a tagging task, rather than
a classification task, motivated by the common understanding that segments in
speech are not truly discrete and commonly overlap. The second is an
interpolation technique to allow boundaries more precise than the common 10 ms
limit in modern forced alignment systems. We compare configurations of our
system to a state-of-the-art system, the Montreal Forced Aligner. The tagging
approach did not generally yield improved results over the Montreal Forced
Aligner. However, a system with the interpolation technique had a 27.92%
increase relative to the Montreal Forced Aligner in the amount of boundaries
within 10 ms of the target on the test set. We also reflect on the task and
training process for acoustic modeling in forced alignment, highlighting how
the output targets for these models do not match phoneticians' conception of
similarity between phones and that reconciliation of this tension may require
rethinking the task and output targets or how speech itself should be
segmented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification. (arXiv:2310.15429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15429">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15431">
<div class="article-summary-box-inner">
<span><p>Moral or ethical judgments rely heavily on the specific contexts in which
they occur. Understanding varying shades of defeasible contextualizations
(i.e., additional information that strengthens or attenuates the moral
acceptability of an action) is critical to accurately represent the subtlety
and intricacy of grounded human moral judgment in real-life scenarios.
</p>
<p>We introduce defeasible moral reasoning: a task to provide grounded contexts
that make an action more or less morally acceptable, along with commonsense
rationales that justify the reasoning. To elicit high-quality task data, we
take an iterative self-distillation approach that starts from a small amount of
unstructured seed knowledge from GPT-3 and then alternates between (1)
self-distillation from student models; (2) targeted filtering with a critic
model trained by human judgment (to boost validity) and NLI (to boost
diversity); (3) self-imitation learning (to amplify the desired data quality).
This process yields a student model that produces defeasible contexts with
improved validity, diversity, and defeasibility. From this model we distill a
high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of
contextualizations and rationales for 115K defeasible moral actions rated
highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we
obtain a final student model that wins over all intermediate student models by
a notable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings. (arXiv:2310.15439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15439">
<div class="article-summary-box-inner">
<span><p>Numerous datasets have been proposed to combat the spread of online hate.
Despite these efforts, a majority of these resources are English-centric,
primarily focusing on overt forms of hate. This research gap calls for
developing high-quality corpora in diverse languages that also encapsulate more
subtle hate expressions. This study introduces K-HATERS, a new corpus for hate
speech detection in Korean, comprising approximately 192K news comments with
target-specific offensiveness ratings. This resource is the largest offensive
language corpus in Korean and is the first to offer target-specific ratings on
a three-point Likert scale, enabling the detection of hate expressions in
Korean across varying degrees of offensiveness. We conduct experiments showing
the effectiveness of the proposed corpus, including a comparison with existing
datasets. Additionally, to address potential noise and bias in human
annotations, we explore a novel idea of adopting the Cognitive Reflection Test,
which is widely used in social science for assessing an individual's cognitive
ability, as a proxy of labeling quality. Findings indicate that annotations
from individuals with the lowest test scores tend to yield detection models
that make biased predictions toward specific target groups and are less
accurate. This study contributes to the NLP research on hate speech detection
and resource construction. The code and dataset can be accessed at
https://github.com/ssu-humane/K-HATERS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring. (arXiv:2310.15461v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15461">
<div class="article-summary-box-inner">
<span><p>Self-guided mental health interventions, such as "do-it-yourself" tools to
learn and practice coping strategies, show great promise to improve access to
mental health care. However, these interventions are often cognitively
demanding and emotionally triggering, creating accessibility barriers that
limit their wide-scale implementation and adoption. In this paper, we study how
human-language model interaction can support self-guided mental health
interventions. We take cognitive restructuring, an evidence-based therapeutic
technique to overcome negative thinking, as a case study. In an IRB-approved
randomized field study on a large mental health website with 15,531
participants, we design and evaluate a system that uses language models to
support people through various steps of cognitive restructuring. Our findings
reveal that our system positively impacts emotional intensity for 67% of
participants and helps 65% overcome negative thoughts. Although adolescents
report relatively worse outcomes, we find that tailored interventions that
simplify language model generations improve overall effectiveness and equity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Answers to Yes-No Questions in User-Generated Content. (arXiv:2310.15464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15464">
<div class="article-summary-box-inner">
<span><p>Interpreting answers to yes-no questions in social media is difficult. Yes
and no keywords are uncommon, and the few answers that include them are rarely
to be interpreted what the keywords suggest. In this paper, we present a new
corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss
linguistic characteristics of answers whose interpretation is yes or no, as
well as answers whose interpretation is unknown. We show that large language
models are far from solving this problem, even after fine-tuning and blending
other corpora for the same problem but outside social media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15469">
<div class="article-summary-box-inner">
<span><p>The era post-2018 marked the advent of Large Language Models (LLMs), with
innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.
As the industry galloped toward augmenting model parameters and capitalizing on
vast swaths of human language data, security and privacy challenges also
emerged. Foremost among these is the potential inadvertent accrual of Personal
Identifiable Information (PII) during web-based data acquisition, posing risks
of unintended PII disclosure. While strategies like RLHF during training and
Catastrophic Forgetting have been marshaled to control the risk of privacy
infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning
interface for GPT-3.5, have reignited concerns. One may ask: can the
fine-tuning of LLMs precipitate the leakage of personal information embedded
within training datasets? This paper reports the first endeavor to seek the
answer to the question, particularly our discovery of a new LLM exploitation
avenue, called the Janus attack. In the attack, one can construct a PII
association task, whereby an LLM is fine-tuned using a minuscule PII dataset,
to potentially reinstate and reveal concealed PIIs. Our findings indicate that,
with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from
being impermeable to PII extraction to a state where they divulge a substantial
proportion of concealed PII. This research, through its deep dive into the
Janus attack vector, underscores the imperative of navigating the intricate
interplay between LLM utility and privacy preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15470">
<div class="article-summary-box-inner">
<span><p>We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model. (arXiv:2310.15477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15477">
<div class="article-summary-box-inner">
<span><p>Instruction tuning has recently been recognized as an effective way of
aligning Large Language Models (LLMs) to enhance their generalization ability
across various tasks. However, when tuning publicly accessible, centralized
LLMs with private instruction data, privacy concerns are inevitable. While
direct transfer of parameterized modules between models is a plausible approach
to address this, its implications and effectiveness need further exploration.
This paper focuses on Offsite-Tuning (OFT), a representative technique that
transfers transformer blocks between centralized LLMs and downstream emulators.
Given the limited understanding of the underlying mechanism of OFT, we perform
an empirical analysis on LLMs from the perspectives of representation and
functional similarity. Interestingly, our findings reveal a unique modular
structure within the layers of LLMs that appears to emerge as the model size
expands. Simultaneously, we note subtle but potentially significant changes in
representation and intermediate predictions across the layers. Inspired by
these observations, we propose CRaSh, involving Clustering, Removing, and
Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh
significantly boosts performance of OFT with billions of parameters.
Furthermore, we investigate the optimal solutions yielded by fine-tuning with
and without full model through the lens of loss landscape. Our findings
demonstrate a linear connectivity among these optima falling over the same
basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code
is publicly available at https://github.com/TsinghuaC3I/CRaSh.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. (arXiv:2310.15484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15484">
<div class="article-summary-box-inner">
<span><p>Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves
retrieving nodes from a knowledge graph (KG) to answer natural language
questions. Recent GNN-based approaches formulate this task as a KG path
searching problem, where messages are sequentially propagated from the seed
node towards the answer nodes. However, these messages are past-oriented, and
they do not consider the full KG context. To make matters worse, KG nodes often
represent proper noun entities and are sometimes encrypted, being uninformative
in selecting between paths. To address these problems, we propose Neural Tree
Search (NuTrea), a tree search-based GNN model that incorporates the broader KG
context. Our model adopts a message-passing scheme that probes the unreached
subtree regions to boost the past-oriented embeddings. In addition, we
introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node
embedding that considers the global KG context to better characterize ambiguous
KG nodes. The general effectiveness of our approach is demonstrated through
experiments on three major multi-hop KGQA benchmark datasets, and our extensive
analyses further validate its expressiveness and robustness. Overall, NuTrea
provides a powerful means to query the KG with complex natural language
questions. Code is available at https://github.com/mlvlab/NuTrea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15494">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture is crucial for numerous AI models, but it still
faces challenges in long-range language modeling. Though several specific
transformer architectures have been designed to tackle issues of long-range
dependencies, existing methods like Transformer-XL are plagued by a high
percentage of ineffective memories. In this study, we present a plug-and-play
strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens
participating in attention calculation based on one simple metric. This
strategy allows us to keep tokens that are likely to have a high attention
score with the current queries and ignore the other ones. We have tested our
approach on the word-level benchmark (WikiText-103) and the character-level
benchmark (enwik8), and the results indicate an improvement without having
additional training or adding additional parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15511">
<div class="article-summary-box-inner">
<span><p>We study the ability of state-of-the art models to answer constraint
satisfaction queries for information retrieval (e.g., 'a list of ice cream
shops in San Diego'). In the past, such queries were considered to be tasks
that could only be solved via web-search or knowledge bases. More recently,
large language models (LLMs) have demonstrated initial emergent abilities in
this task. However, many current retrieval benchmarks are either saturated or
do not measure constraint satisfaction. Motivated by rising concerns around
factual incorrectness and hallucinations of LLMs, we present KITAB, a new
dataset for measuring constraint satisfaction abilities of language models.
KITAB consists of book-related data across more than 600 authors and 13,000
queries, and also offers an associated dynamic data collection and constraint
verification approach for acquiring similar test data for other authors. Our
extended experiments on GPT4 and GPT3.5 characterize and decouple common
failure modes across dimensions such as information popularity, constraint
types, and context availability. Results show that in the absence of context,
models exhibit severe limitations as measured by irrelevant information,
factual errors, and incompleteness, many of which exacerbate as information
popularity decreases. While context availability mitigates irrelevant
information, it is not helpful for satisfying constraints, identifying
fundamental barriers to constraint satisfaction. We open source our
contributions to foster further research on improving constraint satisfaction
abilities of future models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Matrix Factorization Analysis of Multilingual Representations. (arXiv:2310.15513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15513">
<div class="article-summary-box-inner">
<span><p>We present an analysis tool based on joint matrix factorization for comparing
latent representations of multilingual and monolingual models. An alternative
to probing, this tool allows us to analyze multiple sets of representations in
a joint manner. Using this tool, we study to what extent and how
morphosyntactic features are reflected in the representations learned by
multilingual pre-trained models. We conduct a large-scale empirical study of
over 33 languages and 17 morphosyntactic categories. Our findings demonstrate
variations in the encoding of morphosyntactic information across upper and
lower layers, with category-specific differences influenced by language
properties. Hierarchical clustering of the factorization outputs yields a tree
structure that is related to phylogenetic trees manually crafted by linguists.
Moreover, we find the factorization outputs exhibit strong associations with
performance observed across different cross-lingual tasks. We release our code
to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation. (arXiv:2310.15515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15515">
<div class="article-summary-box-inner">
<span><p>Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarkQA: A large scale KBQA dataset with numerical reasoning. (arXiv:2310.15517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15517">
<div class="article-summary-box-inner">
<span><p>While question answering over knowledge bases (KBQA) has shown progress in
addressing factoid questions, KBQA with numerical reasoning remains relatively
unexplored. In this paper, we focus on the complex numerical reasoning in KBQA
and propose a new task, NR-KBQA, which necessitates the ability to perform both
multi-hop reasoning and numerical reasoning. We design a logic form in Python
format called PyQL to represent the reasoning process of numerical reasoning
questions. To facilitate the development of NR-KBQA, we present a large dataset
called MarkQA, which is automatically constructed from a small set of seeds.
Each question in MarkQA is equipped with its corresponding SPARQL query,
alongside the step-by-step reasoning process in the QDMR format and PyQL
program. Experimental results of some state-of-the-art QA methods on the MarkQA
show that complex numerical reasoning in KBQA faces great challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15539">
<div class="article-summary-box-inner">
<span><p>With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder's parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary. (arXiv:2310.15541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15541">
<div class="article-summary-box-inner">
<span><p>The non-humanlike behaviour of contemporary pre-trained language models
(PLMs) is a leading cause undermining their trustworthiness. A striking
phenomenon of such faulty behaviours is the generation of inconsistent
predictions, which produces logically contradictory results, such as generating
different predictions for texts delivering the same meaning or violating
logical properties. Previous studies exploited data augmentation or implemented
specialised loss functions to alleviate the issue. However, their usage is
limited, because they consume expensive training resources for large-sized PLMs
and can only handle a certain consistency type. To this end, we propose a
practical approach that alleviates the inconsistent behaviour issue by
fundamentally improving PLMs' meaning awareness. Based on the conceptual role
theory, our method allows PLMs to capture accurate meaning by learning precise
interrelationships between concepts from word-definition pairs in a dictionary.
Next, we propose an efficient parameter integration technique that updates only
a few additional parameters to combine the learned interrelationship with PLMs'
pre-trained knowledge. Our experimental results reveal that the approach can
concurrently improve multiple types of consistency, enables efficient knowledge
integration, and easily applies to other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15552">
<div class="article-summary-box-inner">
<span><p>Recent research suggests that the feed-forward module within Transformers can
be viewed as a collection of key-value memories, where the keys learn to
capture specific patterns from the input based on the training examples. The
values then combine the output from the 'memories' of the keys to generate
predictions about the next token. This leads to an incremental process of
prediction that gradually converges towards the final token choice near the
output layers. This interesting perspective raises questions about how
multilingual models might leverage this mechanism. Specifically, for
autoregressive models trained on two or more languages, do all neurons (across
layers) respond equally to all languages? No! Our hypothesis centers around the
notion that during pretraining, certain model parameters learn strong
language-specific features, while others learn more language-agnostic (shared
across languages) features. To validate this, we conduct experiments utilizing
parallel corpora of two languages that the model was initially pretrained on.
Our findings reveal that the layers closest to the network's input or output
tend to exhibit more language-specific behaviour compared to the layers in the
middle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15556">
<div class="article-summary-box-inner">
<span><p>Since ChatGPT released its API for public use, the number of applications
built on top of commercial large language models (LLMs) increase exponentially.
One popular usage of such models is leveraging its in-context learning ability
and generating responses given user queries leveraging knowledge obtained by
retrieval augmentation. One problem of deploying commercial retrieval-augmented
LLMs is the cost due to the additionally retrieved context that largely
increases the input token size of the LLMs. To mitigate this, we propose a
token compression scheme that includes two methods: summarization compression
and semantic compression. The first method applies a T5-based model that is
fine-tuned by datasets generated using self-instruct containing samples with
varying lengths and reduce token size by doing summarization. The second method
further compresses the token size by removing words with lower impact on the
semantic. In order to adequately evaluate the effectiveness of the proposed
methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)
focusing on food recommendation for women around pregnancy period or infants.
Our summarization compression can reduce 65% of the retrieval token size with
further 0.3% improvement on the accuracy; semantic compression provides a more
flexible way to trade-off the token size with performance, for which we can
reduce the token size by 20% with only 1.6% of accuracy drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain. (arXiv:2310.15569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15569">
<div class="article-summary-box-inner">
<span><p>Keeping track of all relevant recent publications and experimental results
for a research area is a challenging task. Prior work has demonstrated the
efficacy of information extraction models in various scientific areas.
Recently, several datasets have been released for the yet understudied
materials science domain. However, these datasets focus on sub-problems such as
parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells.
In this resource paper, we present MuLMS, a new dataset of 50 open-access
articles, spanning seven sub-domains of materials science. The corpus has been
annotated by domain experts with several layers ranging from named entities
over relations to frame structures. We present competitive neural models for
all tasks and demonstrate that multi-task training with existing related
resources leads to benefits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Continual Language Learning with Selective Specialization. (arXiv:2310.15571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15571">
<div class="article-summary-box-inner">
<span><p>A desirable trait of an artificial agent acting in the visual world is to
continually learn a sequence of language-informed tasks while striking a
balance between sufficiently specializing in each task and building a
generalized knowledge for transfer. Selective specialization, i.e., a careful
selection of model components to specialize in each task, is a strategy to
provide control over this trade-off. However, the design of selection
strategies requires insights on the role of each model component in learning
rather specialized or generalizable representations, which poses a gap in
current research. Thus, our aim with this work is to provide an extensive
analysis of selection strategies for visually grounded continual language
learning. Due to the lack of suitable benchmarks for this purpose, we introduce
two novel diagnostic datasets that provide enough control and flexibility for a
thorough model analysis. We assess various heuristics for module specialization
strategies as well as quantifiable measures for two different types of model
architectures. Finally, we design conceptually simple approaches based on our
analysis that outperform common continual learning baselines. Our results
demonstrate the need for further efforts towards better aligning continual
learning algorithms with the learning behaviors of individual model parts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls. (arXiv:2310.15572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15572">
<div class="article-summary-box-inner">
<span><p>Building and analysing knowledge graphs (KGs) to aid drug discovery is a
topical area of research. A salient feature of KGs is their ability to combine
many heterogeneous data sources in a format that facilitates discovering
connections. The utility of KGs has been exemplified in areas such as drug
repurposing, with insights made through manual exploration and modelling of the
data. In this article, we discuss promises and pitfalls of using natural
language processing (NLP) to mine unstructured text typically from scientific
literature as a data source for KGs. This draws on our experience of initially
parsing structured data sources such as ChEMBL as the basis for data within a
KG, and then enriching or expanding upon them using NLP. The fundamental
promise of NLP for KGs is the automated extraction of data from millions of
documents a task practically impossible to do via human curation alone.
However, there are many potential pitfalls in NLP-KG pipelines such as
incorrect named entity recognition and ontology linking all of which could
ultimately lead to erroneous inferences and conclusions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POE: Process of Elimination for Multiple Choice Reasoning. (arXiv:2310.15575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15575">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15577">
<div class="article-summary-box-inner">
<span><p>Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus
on developing more efficient fine-tuning techniques for the task. Instead, our
motivation is to come up with a generic approach that can improve the
downstream performances of multiple ABSA tasks simultaneously. Towards this, we
present CONTRASTE, a novel pre-training strategy using CONTRastive learning to
enhance the ASTE performance. While we primarily focus on ASTE, we also
demonstrate the advantage of our proposed technique on other ABSA tasks such as
ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion,
sentiment) triplets, first, we design aspect-based prompts with corresponding
sentiments masked. We then (pre)train an encoder-decoder model by applying
contrastive learning on the decoder-generated aspect-aware sentiment
representations of the masked terms. For fine-tuning the model weights thus
obtained, we then propose a novel multi-task approach where the base
encoder-decoder model is combined with two complementary modules, a
tagging-based Opinion Term Detector, and a regression-based Triplet Count
Estimator. Exhaustive experiments on four benchmark datasets and a detailed
ablation study establish the importance of each of our proposed components as
we achieve new state-of-the-art ASTE results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15585">
<div class="article-summary-box-inner">
<span><p>Neural Module Networks (NMN) are a compelling method for visual question
answering, enabling the translation of a question into a program consisting of
a series of reasoning sub-tasks that are sequentially executed on the image to
produce an answer. NMNs provide enhanced explainability compared to integrated
models, allowing for a better understanding of the underlying reasoning
process. To improve the effectiveness of NMNs we propose to exploit features
obtained by a large-scale cross-modal encoder. Also, the current training
approach of NMNs relies on the propagation of module outputs to subsequent
modules, leading to the accumulation of prediction errors and the generation of
false answers. To mitigate this, we introduce an NMN learning strategy
involving scheduled teacher guidance. Initially, the model is fully guided by
the ground-truth intermediate outputs, but gradually transitions to an
autonomous behavior as training progresses. This reduces error accumulation,
thus improving training efficiency and final performance.We demonstrate that by
incorporating cross-modal features and employing more effective training
techniques for NMN, we achieve a favorable balance between performance and
transparency in the reasoning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts. (arXiv:2310.15587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15587">
<div class="article-summary-box-inner">
<span><p>Eye movements in reading play a crucial role in psycholinguistic research
studying the cognitive mechanisms underlying human language processing. More
recently, the tight coupling between eye movements and cognition has also been
leveraged for language-related machine learning tasks such as the
interpretability, enhancement, and pre-training of language models, as well as
the inference of reader- and text-specific properties. However, scarcity of eye
movement data and its unavailability at application time poses a major
challenge for this line of research. Initially, this problem was tackled by
resorting to cognitive models for synthesizing eye movement data. However, for
the sole purpose of generating human-like scanpaths, purely data-driven
machine-learning-based methods have proven to be more suitable. Following
recent advances in adapting diffusion processes to discrete data, we propose
ScanDL, a novel discrete sequence-to-sequence diffusion model that generates
synthetic scanpaths on texts. By leveraging pre-trained word representations
and jointly embedding both the stimulus text and the fixation sequence, our
model captures multi-modal interactions between the two inputs. We evaluate
ScanDL within- and across-dataset and demonstrate that it significantly
outperforms state-of-the-art scanpath generation methods. Finally, we provide
an extensive psycholinguistic analysis that underlines the model's ability to
exhibit human-like reading behavior. Our implementation is made available at
https://github.com/DiLi-Lab/ScanDL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15594">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUSER: A Multi-View Similar Case Retrieval Dataset. (arXiv:2310.15602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15602">
<div class="article-summary-box-inner">
<span><p>Similar case retrieval (SCR) is a representative legal AI application that
plays a pivotal role in promoting judicial fairness. However, existing SCR
datasets only focus on the fact description section when judging the similarity
between cases, ignoring other valuable sections (e.g., the court's opinion)
that can provide insightful reasoning process behind. Furthermore, the case
similarities are typically measured solely by the textual semantics of the fact
descriptions, which may fail to capture the full complexity of legal cases from
the perspective of legal knowledge. In this work, we present MUSER, a similar
case retrieval dataset based on multi-view similarity measurement and
comprehensive legal element with sentence-level legal element annotations.
Specifically, we select three perspectives (legal fact, dispute focus, and law
statutory) and build a comprehensive and structured label schema of legal
elements for each of them, to enable accurate and knowledgeable evaluation of
case similarities. The constructed dataset originates from Chinese civil cases
and contains 100 query cases and 4,024 candidate cases. We implement several
text classification algorithms for legal element prediction and various
retrieval methods for retrieving similar cases on MUSER. The experimental
results indicate that incorporating legal elements can benefit the performance
of SCR models, but further efforts are still required to address the remaining
challenges posed by MUSER. The source code and dataset are released at
https://github.com/THUlawtech/MUSER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15612">
<div class="article-summary-box-inner">
<span><p>Currently, there is no usable machine translation system for Nko, a language
spoken by tens of millions of people across multiple West African countries,
which holds significant cultural and educational value. To address this issue,
we present a set of tools, resources, and baseline results aimed towards the
development of usable machine translation systems for Nko and other languages
that do not currently have sufficiently large parallel text corpora available.
(1) Friallel: A novel collaborative parallel text curation software that
incorporates quality control through copyedit-based workflows. (2) Expansion of
the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko
translations in parallel with 204 and 40 other languages. (3) nicolingua-0005:
A collection of trilingual and bilingual corpora with 130,850 parallel segments
and monolingual corpora containing over 3 million Nko words. (4) Baseline
bilingual and multilingual neural machine translation results with the best
model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection. (arXiv:2310.15632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15632">
<div class="article-summary-box-inner">
<span><p>The 64-bit architectures that have become standard today offer unprecedented
low-level programming possibilities. For the first time in the history of
computing, the size of address registers far exceeded the physical capacity of
their bus.After a brief reminder of the possibilities offered by the small size
of addresses compared to the available 64 bits,we develop three concrete
examples of how the vacant bits of these registers can be used.Among these
examples, two of them concern the implementation of a library for a new
statically typed programming language.Firstly, the implementation of
multi-precision integers, with the aim of improving performance in terms of
both calculation speed and RAM savings.The second example focuses on the
library's handling of UTF-8 character strings.Here, the idea is to make
indexing easier by ignoring the physical size of each UTF-8 characters.Finally,
the third example is a possible enhancement of garbage collectors, in
particular the mark \&amp; sweep for the object marking phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Career Path Prediction using Resume Representation Learning and Skill-based Matching. (arXiv:2310.15636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15636">
<div class="article-summary-box-inner">
<span><p>The impact of person-job fit on job satisfaction and performance is widely
acknowledged, which highlights the importance of providing workers with next
steps at the right time in their career. This task of predicting the next step
in a career is known as career path prediction, and has diverse applications
such as turnover prevention and internal job mobility. Existing methods to
career path prediction rely on large amounts of private career history data to
model the interactions between job titles and companies. We propose leveraging
the unexplored textual descriptions that are part of work experience sections
in resumes. We introduce a structured dataset of 2,164 anonymized career
histories, annotated with ESCO occupation labels. Based on this dataset, we
present a novel representation learning approach, CareerBERT, specifically
designed for work history data. We develop a skill-based model and a text-based
model for career path prediction, which achieve 35.24% and 39.61% recall@10
respectively on our dataset. Finally, we show that both approaches are
complementary as a hybrid approach achieves the strongest result with 43.01%
recall@10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15638">
<div class="article-summary-box-inner">
<span><p>Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15654">
<div class="article-summary-box-inner">
<span><p>The burgeoning capabilities of advanced large language models (LLMs) such as
ChatGPT have led to an increase in synthetic content generation with
implications across a variety of sectors, including media, cybersecurity,
public discourse, and education. As such, the ability to detect LLMs-generated
content has become of paramount importance. We aim to provide a detailed
overview of existing detection strategies and benchmarks, scrutinizing their
differences and identifying key challenges and prospects in the field,
advocating for more adaptable and robust models to enhance detection accuracy.
We also posit the necessity for a multi-faceted approach to defend against
various attacks to counter the rapidly advancing capabilities of LLMs. To the
best of our knowledge, this work is the first comprehensive survey on the
detection in the era of LLMs. We hope it will provide a broad understanding of
the current landscape of LLMs-generated content detection, offering a guiding
reference for researchers and practitioners striving to uphold the integrity of
digital information in an era increasingly dominated by synthetic content. The
relevant papers are summarized and will be consistently updated at
https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression Syntax Information Bottleneck for Math Word Problems. (arXiv:2310.15664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15664">
<div class="article-summary-box-inner">
<span><p>Math Word Problems (MWP) aims to automatically solve mathematical questions
given in texts. Previous studies tend to design complex models to capture
additional information in the original text so as to enable the model to gain
more comprehensive features. In this paper, we turn our attention in the
opposite direction, and work on how to discard redundant features containing
spurious correlations for MWP. To this end, we design an Expression Syntax
Information Bottleneck method for MWP (called ESIB) based on variational
information bottleneck, which extracts essential features of expression syntax
tree while filtering latent-specific redundancy containing syntax-irrelevant
features. The key idea of ESIB is to encourage multiple models to predict the
same expression syntax tree for different problem representations of the same
problem by mutual learning so as to capture consistent information of
expression syntax tree and discard latent-specific redundancy. To improve the
generalization ability of the model and generate more diverse expressions, we
design a self-distillation loss to encourage the model to rely more on the
expression syntax information in the latent space. Experimental results on two
large-scale benchmarks show that our model not only achieves state-of-the-art
results but also generates more diverse solutions. The code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15672">
<div class="article-summary-box-inner">
<span><p>For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon, and under-investigated in
literature. In this work, we examine the effect of scaling the sequence length
used to train/evaluate (dense-attention based) acoustic and language models on
speech recognition performance. For these experiments a dataset of roughly
100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5
seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets
Earnings-22 and Tedlium demonstrate a benefit from training with around 80
seconds of acoustic context, showing up to a 14.9% relative improvement from a
limited context baseline. Furthermore, we perform a system combination with
long-context transformer language models via beam search for a fully
long-context ASR system, with results that are competitive with the current
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prevalence and prevention of large language model use in crowd work. (arXiv:2310.15683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15683">
<div class="article-summary-box-inner">
<span><p>We show that the use of large language models (LLMs) is prevalent among crowd
workers, and that targeted mitigation strategies can significantly reduce, but
not eliminate, LLM use. On a text summarization task where workers were not
directed in any way regarding their LLM use, the estimated prevalence of LLM
use was around 30%, but was reduced by about half by asking workers to not use
LLMs and by raising the cost of using them, e.g., by disabling copy-pasting.
Secondary analyses give further insight into LLM use and its prevention: LLM
use yields high-quality but homogeneous responses, which may harm research
concerned with human (rather than model) behavior and degrade future models
trained with crowdsourced data. At the same time, preventing LLM use may be at
odds with obtaining high-quality responses; e.g., when requesting workers not
to use LLMs, summaries contained fewer keywords carrying essential information.
Our estimates will likely change as LLMs increase in popularity or
capabilities, and as norms around their usage change. Yet, understanding the
co-evolution of LLM-based tools and users is key to maintaining the validity of
research done using crowdsourcing, and we provide a critical baseline before
widespread adoption ensues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers. (arXiv:2310.15684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15684">
<div class="article-summary-box-inner">
<span><p>Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating a silver standard for patent simplification. (arXiv:2310.15689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15689">
<div class="article-summary-box-inner">
<span><p>Patents are legal documents that aim at protecting inventions on the one hand
and at making technical knowledge circulate on the other. Their complex style
-- a mix of legal, technical, and extremely vague language -- makes their
content hard to access for humans and machines and poses substantial challenges
to the information retrieval community. This paper proposes an approach to
automatically simplify patent text through rephrasing. Since no in-domain
parallel simplification data exist, we propose a method to automatically
generate a large-scale silver standard for patent sentences. To obtain
candidates, we use a general-domain paraphrasing system; however, the process
is error-prone and difficult to control. Thus, we pair it with proper filters
and construct a cleaner corpus that can successfully be used to train a
simplification system. Human evaluation of the synthetic silver corpus shows
that it is considered grammatical, adequate, and contains simple sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automated Recipe Genre Classification using Semi-Supervised Learning. (arXiv:2310.15693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15693">
<div class="article-summary-box-inner">
<span><p>Sharing cooking recipes is a great way to exchange culinary ideas and provide
instructions for food preparation. However, categorizing raw recipes found
online into appropriate food genres can be challenging due to a lack of
adequate labeled data. In this study, we present a dataset named the
``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking
Recipe Dataset" that contains two million culinary recipes labeled in
respective categories with extended named entities extracted from recipe
descriptions. This collection of data includes various features such as title,
NER, directions, and extended NER, as well as nine different labels
representing genres including bakery, drinks, non-veg, vegetables, fast food,
cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends
the size of the Named Entity Recognition (NER) list to address missing named
entities like heat, time or process from the recipe directions using two NER
extraction tools. 3A2M+ dataset provides a comprehensive solution to the
various challenging recipe-related tasks, including classification, named
entity recognition, and recipe generation. Furthermore, we have demonstrated
traditional machine learning, deep learning and pre-trained language models to
classify the recipes into their corresponding genre and achieved an overall
accuracy of 98.6\%. Our investigation indicates that the title feature played a
more significant role in classifying the genre.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15694">
<div class="article-summary-box-inner">
<span><p>The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Biomedical Lay Summarisation with External Knowledge Graphs. (arXiv:2310.15702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15702">
<div class="article-summary-box-inner">
<span><p>Previous approaches for automatic lay summarisation are exclusively reliant
on the source article that, given it is written for a technical audience (e.g.,
researchers), is unlikely to explicitly define all technical concepts or state
all of the background information that is relevant for a lay audience. We
address this issue by augmenting eLife, an existing biomedical lay
summarisation dataset, with article-specific knowledge graphs, each containing
detailed information on relevant biomedical concepts. Using both automatic and
human evaluations, we systematically investigate the effectiveness of three
different approaches for incorporating knowledge graphs within lay
summarisation models, with each method targeting a distinct area of the
encoder-decoder model architecture. Our results confirm that integrating
graph-based domain knowledge can significantly benefit lay summarisation by
substantially increasing the readability of generated text and improving the
explanation of technical concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble of Task-Specific Language Models for Brain Encoding. (arXiv:2310.15720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15720">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to be rich enough to encode fMRI activations
of certain Regions of Interest in our Brains. Previous works have explored
transfer learning from representations learned for popular natural language
processing tasks for predicting brain responses. In our work, we improve the
performance of such encoders by creating an ensemble model out of 10 popular
Language Models (2 syntactic and 8 semantic). We beat the current baselines by
10% on average across all ROIs through our ensembling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion. (arXiv:2310.15722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15722">
<div class="article-summary-box-inner">
<span><p>Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting
aims to predict the missing entity from a fact in the future, posing a
challenge that aligns more closely with real-world prediction problems.
Existing research mostly encodes entities and relations using sequential graph
neural networks applied to recent snapshots. However, these approaches tend to
overlook the ability to skip irrelevant snapshots according to entity-related
relations in the query and disregard the importance of explicit temporal
information. To address this, we propose our model, Re-Temp (Relation-Aware
Temporal Representation Learning), which leverages explicit temporal embedding
as input and incorporates skip information flow after each timestamp to skip
unnecessary information for prediction. Additionally, we introduce a two-phase
forward propagation method to prevent information leakage. Through the
evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model
outperforms all eight recent state-of-the-art models by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15724">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have achieved remarkable results on NLP
tasks but at the expense of huge parameter sizes and the consequent
computational costs. In this paper, we propose Variator, a parameter-efficient
acceleration method that enhances computational efficiency through
plug-and-play compression plugins. Compression plugins are designed to reduce
the sequence length via compressing multiple hidden vectors into one and
trained with original PLMs frozen. Different from traditional model
acceleration methods, which compress PLMs to smaller sizes, Variator offers two
distinct advantages: (1) In real-world applications, the plug-and-play nature
of our compression plugins enables dynamic selection of different compression
plugins with varying acceleration ratios based on the current workload. (2) The
compression plugin comprises a few compact neural network layers with minimal
parameters, significantly saving storage and memory overhead, particularly in
scenarios with a growing number of tasks. We validate the effectiveness of
Variator on seven datasets. Experimental results show that Variator can save
53% computational costs using only 0.9% additional parameters with a
performance drop of less than 2%. Moreover, when the model scales to billions
of parameters, Variator matches the strong performance of uncompressed PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction. (arXiv:2310.15743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15743">
<div class="article-summary-box-inner">
<span><p>How to identify semantic relations among entities in a document when only a
few labeled documents are available? Few-shot document-level relation
extraction (FSDLRE) is crucial for addressing the pervasive data scarcity
problem in real-world scenarios. Metric-based meta-learning is an effective
framework widely adopted for FSDLRE, which constructs class prototypes for
classification. However, existing works often struggle to obtain class
prototypes with accurate relational semantics: 1) To build prototype for a
target relation type, they aggregate the representations of all entity pairs
holding that relation, while these entity pairs may also hold other relations,
thus disturbing the prototype. 2) They use a set of generic NOTA
(none-of-the-above) prototypes across all tasks, neglecting that the NOTA
semantics differs in tasks with different target relation types. In this paper,
we propose a relation-aware prototype learning method for FSDLRE to strengthen
the relational semantics of prototype representations. By judiciously
leveraging the relation descriptions and realistic NOTA instances as guidance,
our method effectively refines the relation prototypes and generates
task-specific NOTA prototypes. Extensive experiments demonstrate that our
method outperforms state-of-the-art approaches by average 2.61% $F_1$ across
various settings of two FSDLRE benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation. (arXiv:2310.15746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15746">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection. (arXiv:2310.15752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15752">
<div class="article-summary-box-inner">
<span><p>When translating words referring to the speaker, speech translation (ST)
systems should not resort to default masculine generics nor rely on potentially
misleading vocal traits. Rather, they should assign gender according to the
speakers' preference. The existing solutions to do so, though effective, are
hardly feasible in practice as they involve dedicated model re-training on
gender-labeled ST data. To overcome these limitations, we propose the first
inference-time solution to control speaker-related gender inflections in ST.
Our approach partially replaces the (biased) internal language model (LM)
implicitly learned by the ST decoder with gender-specific external LMs.
Experiments on en-&gt;es/fr/it show that our solution outperforms the base models
and the best training-time mitigation strategy by up to 31.0 and 1.6 points in
gender accuracy, respectively, for feminine forms. The gains are even larger
(up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits
conflict with their gender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Differences in Values Influence Disagreements in Online Discussions?. (arXiv:2310.15757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15757">
<div class="article-summary-box-inner">
<span><p>Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?. (arXiv:2310.15758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15758">
<div class="article-summary-box-inner">
<span><p>Learning from free-text human feedback is essential for dialog systems, but
annotated data is scarce and usually covers only a small fraction of error
types known in conversational AI. Instead of collecting and annotating new
datasets from scratch, recent advances in synthetic dialog generation could be
used to augment existing dialog datasets with the necessary annotations.
However, to assess the feasibility of such an effort, it is important to know
the types and frequency of free-text human feedback included in these datasets.
In this work, we investigate this question for a variety of commonly used
dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat,
Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot.
Using our observations, we derive new taxonomies for the annotation of
free-text human feedback in dialogs and investigate the impact of including
such data in response generation for three SOTA language generation models,
including GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the
composition of the datasets examined, including error types, user response
types, and the relations between them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLESS: Benchmarking Large Language Models on Sentence Simplification. (arXiv:2310.15773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15773">
<div class="article-summary-box-inner">
<span><p>We present BLESS, a comprehensive performance benchmark of the most recent
state-of-the-art large language models (LLMs) on the task of text
simplification (TS). We examine how well off-the-shelf LLMs can solve this
challenging task, assessing a total of 44 models, differing in size,
architecture, pre-training methods, and accessibility, on three test sets from
different domains (Wikipedia, news, and medical) under a few-shot setting. Our
analysis considers a suite of automatic metrics as well as a large-scale
quantitative investigation into the types of common edit operations performed
by the different models. Furthermore, we perform a manual qualitative analysis
on a subset of model outputs to better gauge the quality of the generated
simplifications. Our evaluation indicates that the best LLMs, despite not being
trained on TS, perform comparably with state-of-the-art TS baselines.
Additionally, we find that certain LLMs demonstrate a greater range and
diversity of edit operations. Our performance benchmark will be available as a
resource for the development of future TS methods and evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15777">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language tasks, marking significant strides towards general
artificial intelligence. While general artificial intelligence is leveraged by
developing increasingly large-scale models, there could be another branch to
develop lightweight custom models that better serve certain domains, taking
into account the high cost of training and deploying LLMs and the scarcity of
resources. In this paper, we present MindLLM, a novel series of bilingual
lightweight large language models, trained from scratch, alleviating such
burdens by offering models with 1.3 billion and 3 billion parameters. A
thorough account of experiences accrued during large model development is
given, covering every step of the process, including data construction, model
architecture, evaluation, and applications. Such insights are hopefully
valuable for fellow academics and developers. MindLLM consistently matches or
surpasses the performance of other open-source larger models on some public
benchmarks. We also introduce an innovative instruction tuning framework
tailored for smaller models to enhance their capabilities efficiently.
Moreover, we explore the application of MindLLM in specific vertical domains
such as law and finance, underscoring the agility and adaptability of our
lightweight models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15793">
<div class="article-summary-box-inner">
<span><p>This article focuses on large language models (LLMs) fine-tuning in the
scarce data regime (also known as the "few-shot" learning setting). We propose
a method to increase the generalization capabilities of LLMs based on neural
network subspaces. This optimization method, recently introduced in computer
vision, aims to improve model generalization by identifying wider local optima
through the joint optimization of an entire simplex of models in parameter
space. Its adaptation to massive, pretrained transformers, however, poses some
challenges. First, their considerable number of parameters makes it difficult
to train several models jointly, and second, their deterministic parameter
initialization schemes make them unfit for the subspace method as originally
proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT)
methods, however, are perfectly compatible with this original approach, and
propose to learn entire simplex of continuous prefixes. We test our method on a
variant of the GLUE benchmark adapted to the few-shot learning setting, and
show that both our contributions jointly lead to a gain in average performances
compared to sota methods. The implementation can be found at the following
link: https://github.com/Liloulou/prefix_subspace
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15797">
<div class="article-summary-box-inner">
<span><p>Representation Learning on Knowledge Graphs (KGs) is essential for downstream
tasks. The dominant approach, KG Embedding (KGE), represents entities with
independent vectors and faces the scalability challenge. Recent studies propose
an alternative way for parameter efficiency, which represents entities by
composing entity-corresponding codewords matched from predefined small-scale
codebooks. We refer to the process of obtaining corresponding codewords of each
entity as entity quantization, for which previous works have designed
complicated strategies. Surprisingly, this paper shows that simple random
entity quantization can achieve similar results to current strategies. We
analyze this phenomenon and reveal that entity codes, the quantization outcomes
for expressing entities, have higher entropy at the code level and Jaccard
distance at the codeword level under random entity quantization. Therefore,
different entities become more easily distinguished, facilitating effective KG
representation. The above results show that current quantization strategies are
not critical for KG representation, and there is still room for improvement in
entity distinguishability beyond current strategies. The code to reproduce our
results is available at https://github.com/JiaangL/RandomQuantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15799">
<div class="article-summary-box-inner">
<span><p>We present DALE, a novel and effective generative Data Augmentation framework
for low-resource LEgal NLP. DALE addresses the challenges existing frameworks
pose in generating effective data augmentations of legal documents - legal
language, with its specialized vocabulary and complex semantics, morphology,
and syntax, does not benefit from data augmentations that merely rephrase the
source sentence. To address this, DALE, built on an Encoder-Decoder Language
Model, is pre-trained on a novel unsupervised text denoising objective based on
selective masking - our masking strategy exploits the domain-specific language
characteristics of templatized legal documents to mask collocated spans of
text. Denoising these spans helps DALE acquire knowledge about legal concepts,
principles, and language usage. Consequently, it develops the ability to
generate coherent and diverse augmentations with novel contexts. Finally, DALE
performs conditional generation to generate synthetic augmentations for
low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13
datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our
baselines, including LLMs, qualitatively and quantitatively, with improvements
of 1%-50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15819">
<div class="article-summary-box-inner">
<span><p>The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. In this study, we
investigate whether ingroup solidarity and outgroup hostility, fundamental
social biases known from social science, are present in 51 large language
models. We find that almost all foundational language models and some
instruction fine-tuned models exhibit clear ingroup-positive and
outgroup-negative biases when prompted to complete sentences (e.g., "We
are..."). A comparison of LLM-generated sentences with human-written sentences
on the internet reveals that these models exhibit similar level, if not
greater, levels of bias than human text. To investigate where these biases stem
from, we experimentally varied the amount of ingroup-positive or
outgroup-negative sentences the model was exposed to during fine-tuning in the
context of the United States Democrat-Republican divide. Doing so resulted in
the models exhibiting a marked increase in ingroup solidarity and an even
greater increase in outgroup hostility. Furthermore, removing either
ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning
data leads to a significant reduction in both ingroup solidarity and outgroup
hostility, suggesting that biases can be reduced by removing biased training
data. Our findings suggest that modern language models exhibit fundamental
social identity biases and that such biases can be mitigated by curating
training data. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15823">
<div class="article-summary-box-inner">
<span><p>A Reverse Dictionary is a tool enabling users to discover a word based on its
provided definition, meaning, or description. Such a technique proves valuable
in various scenarios, aiding language learners who possess a description of a
word without its identity, and benefiting writers seeking precise terminology.
These scenarios often encapsulate what is referred to as the
"Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning
solution for the Arabic Reverse Dictionary shared task. This task focuses on
deriving a vector representation of an Arabic word from its accompanying
description. The shared task encompasses two distinct subtasks: the first
involves an Arabic definition as input, while the second employs an English
definition. For the first subtask, our approach relies on an ensemble of
finetuned Arabic BERT-based models, predicting the word embedding for a given
definition. The final representation is obtained through averaging the output
embeddings from each model within the ensemble. In contrast, the most effective
solution for the second subtask involves translating the English test
definitions into Arabic and applying them to the finetuned models originally
trained for the first subtask. This straightforward method achieves the highest
score across both subtasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unnatural language processing: How do language models handle machine-generated prompts?. (arXiv:2310.15829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15829">
<div class="article-summary-box-inner">
<span><p>Language model prompt optimization research has shown that semantically and
grammatically well-formed manually crafted prompts are routinely outperformed
by automatically generated token sequences with no apparent meaning or
syntactic structure, including sequences of vectors from a model's embedding
space. We use machine-generated prompts to probe how models respond to input
that is not composed of natural language expressions. We study the behavior of
models of different sizes in multiple semantic tasks in response to both
continuous and discrete machine-generated prompts, and compare it to the
behavior in response to human-generated natural-language prompts. Even when
producing a similar output, machine-generated and human prompts trigger
different response patterns through the network processing pathways, including
different perplexities, different attention and output entropy distributions,
and different unit activation profiles. We provide preliminary insight into the
nature of the units activated by different prompt types, suggesting that only
natural language prompts recruit a genuinely linguistic circuit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15836">
<div class="article-summary-box-inner">
<span><p>New Intent Discovery (NID) aims to recognize both new and known intents from
unlabeled data with the aid of limited labeled data containing only known
intents. Without considering structure relationships between samples, previous
methods generate noisy supervisory signals which cannot strike a balance
between quantity and quality, hindering the formation of new intent clusters
and effective transfer of the pre-training knowledge. To mitigate this
limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to
capture both semantic similarities and structure relationships inherent in
data, enabling more sufficient and reliable supervisory signals. Specifically,
for each sample, we diffuse neighborhood relationships along semantic paths
guided by the nearest neighbors for multiple hops to characterize its local
structure discriminately. Then, we sample its positive keys and weigh them
based on semantic similarities and local structures for contrastive learning.
During inference, we further propose Graph Smoothing Filter (GSF) to explicitly
utilize the structure relationships to filter high-frequency noise embodied in
semantically ambiguous samples on the cluster boundary. Extensive experiments
show that our method outperforms state-of-the-art models on all evaluation
metrics across multiple benchmark datasets. Code and data are available at
https://github.com/yibai-shi/DWGF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15851">
<div class="article-summary-box-inner">
<span><p>The jailbreak attack can bypass the safety measures of a Large Language Model
(LLM), generating harmful content. This misuse of LLM has led to negative
societal consequences. Currently, there are two main approaches to address
jailbreak attacks: safety training and safeguards. Safety training focuses on
further training LLM to enhance its safety. On the other hand, safeguards
involve implementing external models or filters to prevent harmful outputs.
However, safety training has constraints in its ability to adapt to new attack
types and often leads to a drop in model performance. Safeguards have proven to
be of limited help. To tackle these issues, we propose a novel approach called
Self-Guard, which combines the strengths of both safety methods. Self-Guard
includes two stages. In the first stage, we enhance the model's ability to
assess harmful content, and in the second stage, we instruct the model to
consistently perform harmful content detection on its own responses. The
experiment has demonstrated that Self-Guard is robust against jailbreak
attacks. In the bad case analysis, we find that LLM occasionally provides
harmless responses to harmful queries. Additionally, we evaluated the general
capabilities of the LLM before and after safety training, providing evidence
that Self-Guard does not result in the LLM's performance degradation. In
sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM
but also can even mitigate this issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models. (arXiv:2310.15852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15852">
<div class="article-summary-box-inner">
<span><p>Numerous studies have demonstrated the ability of neural language models to
learn various linguistic properties without direct supervision. This work takes
an initial step towards exploring the less researched topic of how neural
models discover linguistic properties of words, such as gender, as well as the
rules governing their usage. We propose to use an artificial corpus generated
by a PCFG based on French to precisely control the gender distribution in the
training data and determine under which conditions a model correctly captures
gender information or, on the contrary, appears gender-biased.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15896">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have performed well in providing general and
extensive health suggestions in single-turn conversations, exemplified by
systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the
limited information provided by users during single turn results in inadequate
personalization and targeting of the generated suggestions, which requires
users to independently select the useful part. It is mainly caused by the
missing ability to engage in multi-turn questioning. In real-world medical
consultations, doctors usually employ a series of iterative inquiries to
comprehend the patient's condition thoroughly, enabling them to provide
effective and personalized suggestions subsequently, which can be defined as
chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose
BianQue, a ChatGLM-based LLM finetuned with the self-constructed health
conversation dataset BianQueCorpus that is consist of multiple turns of
questioning and health suggestions polished by ChatGPT. Experimental results
demonstrate that the proposed BianQue can simultaneously balance the
capabilities of both questioning and health suggestions, which will help
promote the research and application of LLMs in the field of proactive health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Text Classification Via Prototype Trajectories. (arXiv:2007.01777v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01777">
<div class="article-summary-box-inner">
<span><p>We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning. (arXiv:2202.10739v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10739">
<div class="article-summary-box-inner">
<span><p>In online job marketplaces, it is important to establish a well-defined job
title taxonomy for various downstream tasks (e.g., job recommendation, users'
career analysis, and turnover prediction). Job Title Normalization (JTN) is
such a cleaning step to classify user-created non-standard job titles into
normalized ones. However, solving the JTN problem is non-trivial with
challenges: (1) semantic similarity of different job titles, (2) non-normalized
user-created job titles, and (3) large-scale and long-tailed job titles in
real-world applications. To this end, we propose a novel solution, named JAMES,
that constructs three unique embeddings (i.e., graph, contextual, and
syntactic) of a target job title to effectively capture its various traits. We
further propose a multi-aspect co-attention mechanism to attentively combine
these embeddings, and employ neural logical reasoning representations to
collaboratively estimate similarities between messy job titles and normalized
job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive
experiments against ten competing models on a large-scale real-world dataset
with over 350,000 job titles. Our experimental results show that JAMES
significantly outperforms the best baseline by 10.06% in Precision@10 and by
17.52% in NDCG@10, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER. (arXiv:2209.01646v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.01646">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition is the task to locate and classify the entities in
the text. However, Unlabeled Entity Problem in NER datasets seriously hinders
the improvement of NER performance. This paper proposes SCL-RAI to cope with
this problem. Firstly, we decrease the distance of span representations with
the same label while increasing it for different ones via span-based
contrastive learning, which relieves the ambiguity among entities and improves
the robustness of the model over unlabeled entities. Then we propose retrieval
augmented inference to mitigate the decision boundary shifting problem. Our
method significantly outperforms the previous SOTA method by 4.21% and 8.64%
F1-score on two real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05401">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) has been primarily studied through the lens
of the English language. Yet, tackling VQA in other languages in the same
manner would require a considerable amount of resources. In this paper, we
propose scalable solutions to multilingual visual question answering (mVQA), on
both data and modeling fronts. We first propose a translation-based framework
to mVQA data generation that requires much less human annotation efforts than
the conventional approach of directly collection questions and answers. Then,
we apply our framework to the multilingual captions in the Crossmodal-3600
dataset and develop an efficient annotation protocol to create MaXM, a
test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple,
lightweight, and effective approach as well as benchmark state-of-the-art
English and multilingual VQA models. We hope that our benchmark encourages
further research on mVQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10465">
<div class="article-summary-box-inner">
<span><p>Data scarcity has been a long standing issue in the field of open-domain
social dialogue. To quench this thirst, we present SODA: the first publicly
available, million-scale high-quality social dialogue dataset. By
contextualizing social commonsense knowledge from a knowledge graph, we are
able to distill an exceptionally broad spectrum of social interactions from a
large language model. Human evaluation shows that conversations in SODA are
more consistent, specific, and (surprisingly) natural than those in prior
human-authored datasets.
</p>
<p>Using SODA, we train COSMO: a generalizable conversation model that is
significantly more natural and consistent on unseen datasets than
best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).
Experiments reveal COSMO is sometimes even preferred to the original
human-written gold responses. Additionally, our results shed light on the
distinction between knowledge-enriched conversations and natural social
chitchats. We plan to make our data, model, and code public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks. (arXiv:2212.10548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10548">
<div class="article-summary-box-inner">
<span><p>In the absence of readily available labeled data for a given sequence
labeling task and language, annotation projection has been proposed as one of
the possible strategies to automatically generate annotated data. Annotation
projection has often been formulated as the task of transporting, on parallel
corpora, the labels pertaining to a given span in the source language into its
corresponding span in the target language. In this paper we present
T-Projection, a novel approach for annotation projection that leverages large
pretrained text-to-text language models and state-of-the-art machine
translation technology. T-Projection decomposes the label projection task into
two subtasks: (i) A candidate generation step, in which a set of projection
candidates using a multilingual T5 model is generated and, (ii) a candidate
selection step, in which the generated candidates are ranked based on
translation probabilities. We conducted experiments on intrinsic and extrinsic
tasks in 5 Indo-European and 8 low-resource African languages. We demostrate
that T-projection outperforms previous annotation projection methods by a wide
margin. We believe that T-Projection can help to automatically alleviate the
lack of high-quality training data for sequence labeling tasks. Code and data
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08721">
<div class="article-summary-box-inner">
<span><p>Performing inference on large volumes of samples with large language models
(LLMs) can be computationally and financially costly in industry and real-world
use. We propose batch prompting, a simple yet effective prompting approach that
enables the LLM to run inference in batches, instead of one sample at a time.
Our method reduces both token and time costs while retaining downstream
performance. We theoretically demonstrate that under a few-shot in-context
learning setting, the inference costs decrease almost inverse linearly with the
number of samples in each batch. We extensively validate the effectiveness of
batch prompting on ten datasets across commonsense QA, arithmetic reasoning,
and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)
reduces the LLM (Codex) inference token and time costs while achieving better
or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5
and GPT-4, we show the benefits of batch prompting also hold. Further analysis
shows that the number of samples in each batch and the complexity of tasks
affect its performance. Moreover, batch prompting can be applied across
different reasoning methods using LLMs. Our code can be found at the site
https://github.com/xlang-ai/batch-prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11757">
<div class="article-summary-box-inner">
<span><p>Recent years have seen the rapid development of large generative models for
text; however, much less research has explored the connection between text and
another "language" of communication -- music. Music, much like text, can convey
emotions, stories, and ideas, and has its own unique structure and syntax. In
our work, we bridge text and music via a text-to-music generation model that is
highly efficient, expressive, and can handle long-term structure. Specifically,
we develop Mo\^usai, a cascading two-stage latent diffusion model that can
generate multiple minutes of high-quality stereo music at 48kHz from textual
descriptions. Moreover, our model features high efficiency, which enables
real-time inference on a single consumer GPU with a reasonable speed. Through
experiments and property analyses, we show our model's competence over a
variety of criteria compared with existing music generation models. Lastly, to
promote the open-source culture, we provide a collection of open-source
libraries with the hope of facilitating future work in the field. We
open-source the following: Codes:
https://github.com/archinetai/audio-diffusion-pytorch; music samples for this
paper: <a href="http://bit.ly/44ozWDH">this http URL</a>; all music samples for all models:
https://bit.ly/audio-diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03169">
<div class="article-summary-box-inner">
<span><p>Selecting a suitable pretraining dataset is crucial for both general-domain
(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We
formalize this problem as selecting a subset of a large raw unlabeled dataset
to match a desired target distribution given some unlabeled target samples. Due
to the large scale and dimensionality of the raw text data, existing methods
use simple heuristics or use experts to manually curate data. Instead, we
extend the classic importance resampling approach used in low-dimensions for LM
data selection. We propose Data Selection with Importance Resampling (DSIR), an
efficient and scalable framework that estimates importance weights in a reduced
feature space for tractability and selects data with importance resampling
according to these weights. To determine an appropriate feature space, we show
that KL reduction, a data metric that measures the proximity between selected
pretraining data and the target in a feature space, has high correlation with
average downstream accuracy (r=0.89) when computed with simple n-gram features.
This motivates our instantiation of DSIR using n-gram features. When performing
continued pretraining towards a specific domain, DSIR performs comparably to
expert curation across 8 target distributions. When pretraining general-domain
models (target is Wikipedia + books), DSIR improves over random selection and
heuristic filtering baselines by 2-2.5% on the GLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14229">
<div class="article-summary-box-inner">
<span><p>Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has
attracted wide attention from the computational linguistics community. However,
it is not yet known the performance of LLMs on CLS. In this report, we
empirically use various prompts to guide LLMs to perform zero-shot CLS from
different paradigms (i.e., end-to-end and pipeline), and provide a preliminary
evaluation on the generated summaries. We find that ChatGPT and GPT-4
originally prefer to produce lengthy summaries with detailed information. These
two LLMs can further balance informativeness and conciseness with the help of
an interactive prompt, significantly improving their CLS performance.
Experimental results on three widely-used CLS datasets show that GPT-4 achieves
state-of-the-art zero-shot CLS performance, and performs competitively compared
with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and
bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited
zero-shot CLS ability. Due to the composite nature of CLS, which requires
models to perform summarization and translation simultaneously, accomplishing
this task in a zero-shot manner is even a challenge for LLMs. Therefore, we
sincerely hope and recommend future LLM research could use CLS as a testbed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.03387">
<div class="article-summary-box-inner">
<span><p>The tremendous growth of social media users interacting in online
conversations has led to significant growth in hate speech, affecting people
from various demographics. Most of the prior works focus on detecting explicit
hate speech, which is overt and leverages hateful phrases, with very little
work focusing on detecting hate speech that is implicit or denotes hatred
through indirect or coded language. In this paper, we present CoSyn, a
context-synergized neural network that explicitly incorporates user- and
conversational context for detecting implicit hate speech in online
conversations. CoSyn introduces novel ways to encode these external contexts
and employs a novel context interaction mechanism that clearly captures the
interplay between them, making independent assessments of the amounts of
information to be retrieved from these noisy contexts. Additionally, it carries
out all these operations in the hyperbolic space to account for the scale-free
dynamics of social media. We demonstrate the effectiveness of CoSyn on 6 hate
speech datasets and show that CoSyn outperforms all our baselines in detecting
implicit hate speech with absolute improvements in the range of 1.24% - 57.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04048">
<div class="article-summary-box-inner">
<span><p>Recently, the emergence of ChatGPT has attracted wide attention from the
computational linguistics community. Many prior studies have shown that ChatGPT
achieves remarkable performance on various NLP tasks in terms of automatic
evaluation metrics. However, the ability of ChatGPT to serve as an evaluation
metric is still underexplored. Considering assessing the quality of natural
language generation (NLG) models is an arduous task and NLG metrics notoriously
show their poor correlation with human judgments, we wonder whether ChatGPT is
a good NLG evaluation metric. In this report, we provide a preliminary
meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,
we regard ChatGPT as a human evaluator and give task-specific (e.g.,
summarization) and aspect-specific (e.g., relevance) instruction to prompt
ChatGPT to evaluate the generated results of NLG models. We conduct experiments
on five NLG meta-evaluation datasets (including summarization, story generation
and data-to-text tasks). Experimental results show that compared with previous
automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation
with human judgments in most cases. In addition, we find that the effectiveness
of the ChatGPT evaluator might be influenced by the creation method of the
meta-evaluation datasets. For the meta-evaluation datasets which are created
greatly depending on the reference and thus are biased, the ChatGPT evaluator
might lose its effectiveness. We hope our preliminary study could prompt the
emergence of a general-purposed reliable NLG metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency Analysis of ChatGPT. (arXiv:2303.06273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.06273">
<div class="article-summary-box-inner">
<span><p>ChatGPT has gained a huge popularity since its introduction. Its positive
aspects have been reported through many media platforms, and some analyses even
showed that ChatGPT achieved a decent grade in professional exams, adding extra
support to the claim that AI can now assist and even replace humans in
industrial fields. Others, however, doubt its reliability and trustworthiness.
This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding
logically consistent behaviour, focusing specifically on semantic consistency
and the properties of negation, symmetric, and transitive consistency. Our
findings suggest that while both models appear to show an enhanced language
understanding and reasoning ability, they still frequently fall short of
generating logically consistent predictions. We also ascertain via experiments
that prompt designing, few-shot learning and employing larger large language
models (LLMs) are unlikely to be the ultimate solution to resolve the
inconsistency issue of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09752">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks benefit from long inputs, but
processing long documents with Transformers is expensive -- not only due to
quadratic attention complexity but also from applying feedforward and
projection layers to every token. However, not all tokens are equally
important, especially for longer documents. We propose CoLT5, a long-input
Transformer model that builds on this intuition by employing conditional
computation, devoting more resources to important tokens in both feedforward
and attention layers. We show that CoLT5 achieves stronger performance than
LongT5 with much faster training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably
make use of extremely long inputs, showing strong gains up to 64k input length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13988">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called "machine psychology". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02210">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,
relevant, and fluent answers for various natural language processing (NLP)
tasks. Taking document-level machine translation (MT) as a testbed, this paper
provides an in-depth evaluation of LLMs' ability on discourse modeling. The
study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we
investigate the impact of different prompts on document-level translation
quality and discourse phenomena; 2) Comparison of Translation Models, where we
compare the translation performance of ChatGPT with commercial MT systems and
advanced document-level MT methods; 3) Analysis of Discourse Modelling
Abilities, where we further probe discourse knowledge encoded in LLMs and shed
light on impacts of training techniques on discourse modeling. By evaluating on
a number of benchmarks, we surprisingly find that LLMs have demonstrated
superior performance and show potential to become a new paradigm for
document-level translation: 1) leveraging their powerful long-text modeling
capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of
human evaluation; 2) GPT-4 demonstrates a stronger ability for probing
linguistic knowledge than GPT-3.5. This work highlights the challenges and
opportunities of LLMs for MT, which we hope can inspire the future design and
evaluation of LLMs.We release our data and annotations at
https://github.com/longyuewangdcu/Document-MT-LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09848">
<div class="article-summary-box-inner">
<span><p>Generative search engines directly generate responses to user queries, along
with in-line citations. A prerequisite trait of a trustworthy generative search
engine is verifiability, i.e., systems should cite comprehensively (high
citation recall; all statements are fully supported by citations) and
accurately (high citation precision; every cite supports its associated
statement). We conduct human evaluation to audit four popular generative search
engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse
set of queries from a variety of sources (e.g., historical Google user queries,
dynamically-collected open-ended questions on Reddit, etc.). We find that
responses from existing generative search engines are fluent and appear
informative, but frequently contain unsupported statements and inaccurate
citations: on average, a mere 51.5% of generated sentences are fully supported
by citations and only 74.5% of citations support their associated sentence. We
believe that these results are concerningly low for systems that may serve as a
primary tool for information-seeking users, especially given their facade of
trustworthiness. We hope that our results further motivate the development of
trustworthy generative search engines and help researchers and users better
understand the shortcomings of existing commercial systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00586">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as "The war lasted from the year 1732 to the year 17", and predict valid
two-digit end years (years &gt; 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPGTrans: Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01278">
<div class="article-summary-box-inner">
<span><p>While developing a new multimodal LLM (MLLM) by pre-training on tremendous
image-text pairs from scratch can be exceedingly resource-consuming, connecting
an existing LLM with a comparatively lightweight visual prompt generator (VPG)
becomes a feasible paradigm. However, further tuning the VPG part of the MLLM
still suffers from indispensable computational costs, i.e., requiring thousands
of GPU hours and millions of training data. One alternative solution is to
transfer an existing VPG from any existing MLLMs for the target MLLM.
</p>
<p>In this work, we for the first time investigate the VPG transferability
across LLMs, and explore a solution to reduce the cost of VPG transfer. We
first study the VPG transfer across different LLM sizes (e.g., small-to-large),
and across different LLM types, through which we diagnose the key factors to
maximize the transfer efficiency. Based on our observation, we design a
two-stage transfer framework named VPGTrans, which is simple yet highly
effective. Through extensive experiments, we demonstrate that VPGTrans helps
significantly speed up the transfer learning process without compromising
performance. Remarkably, it helps achieve the VPG transfer from BLIP-2
OPT$_\text{2.7B}$ to BLIP-2 OPT$_\text{6.7B}$ with over 10 times speed-up and
10.7% training data compared with connecting a VPG to OPT$_\text{6.7B}$ from
scratch. Further, a series of intriguing findings and potential rationales
behind them are provided and discussed. Finally, we showcase the practical
value of our VPGTrans approach, by customizing two novel MLLMs, including
VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09857">
<div class="article-summary-box-inner">
<span><p>We introduce CoEdIT, a state-of-the-art text editing system for writing
assistance. CoEdIT takes instructions from the user specifying the attributes
of the desired text, such as "Make the sentence simpler" or "Write it in a more
neutral style," and outputs the edited text. We present a large language model
fine-tuned on a diverse collection of task-specific instructions for text
editing (a total of 82K instructions). Our model (1) achieves state-of-the-art
performance on various text editing benchmarks, (2) is competitive with
publicly available largest-sized LLMs trained on instructions while being
nearly 60x smaller, (3) is capable of generalizing to unseen edit instructions,
and (4) exhibits abilities to generalize to composite instructions containing
different combinations of edit actions. Through extensive qualitative and
quantitative analysis, we show that writers prefer the edits suggested by
CoEdIT relative to other state-of-the-art text editing models. Our code, data,
and models are publicly available at https://github.com/vipulraheja/coedit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10387">
<div class="article-summary-box-inner">
<span><p>Automated text simplification, a technique useful for making text more
accessible to people such as children and emergent bilinguals, is often thought
of as a monolingual translation task from complex sentences to simplified
sentences using encoder-decoder models. This view fails to account for
elaborative simplification, where new information is added into the simplified
text. This paper proposes to view elaborative simplification through the lens
of the Question Under Discussion (QUD) framework, providing a robust way to
investigate what writers elaborate upon, how they elaborate, and how
elaborations fit into the discourse context by viewing elaborations as explicit
answers to implicit questions. We introduce ElabQUD, consisting of 1.3K
elaborations accompanied with implicit QUDs, to study these phenomena. We show
that explicitly modeling QUD (via question generation) not only provides
essential understanding of elaborative simplification and how the elaborations
connect with the rest of the discourse, but also substantially improves the
quality of elaboration generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. (arXiv:2305.12001v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12001">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a thorough investigation into the reasoning
capabilities of Large Language Models (LLMs), focusing specifically on the Open
Pretrained Transformers (OPT) models as a representative of such models. Our
study entails finetuning three different sizes of OPT on a carefully curated
reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned
without explanations, and OPT-RE, finetuned with explanations. We then evaluate
all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS
benchmark, covering 26 distinct reasoning skills, utilizing three prompting
techniques. Through a comprehensive grid of 27 configurations and 6,156 test
evaluations, we investigate the dimensions of finetuning, prompting, and scale
to understand the role of explanations on different reasoning skills. Our
findings reveal that having explanations in the fewshot exemplar has no
significant impact on the model's performance when the model is finetuned,
while positively affecting the non-finetuned counterpart. Moreover, we observe
a slight yet consistent increase in classification accuracy as we incorporate
explanations during prompting and finetuning, respectively. Finally, we offer
insights on which skills benefit the most from incorporating explanations
during finetuning and prompting, such as Numerical (+20.4%) and Analogical
(+13.9%) reasoning, as well as skills that exhibit negligible or negative
effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?. (arXiv:2305.12920v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12920">
<div class="article-summary-box-inner">
<span><p>Understanding the fundamental concepts and trends in a scientific field is
crucial for keeping abreast of its continuous advancement. In this study, we
propose a systematic framework for analyzing the evolution of research topics
in a scientific field using causal discovery and inference techniques. We
define three variables to encompass diverse facets of the evolution of research
topics within NLP and utilize a causal discovery algorithm to unveil the causal
connections among these variables using observational data. Subsequently, we
leverage this structure to measure the intensity of these relationships. By
conducting extensive experiments on the ACL Anthology corpus, we demonstrate
that our framework effectively uncovers evolutionary trends and the underlying
causes for a wide range of NLP research topics. Specifically, we show that
tasks and methods are primary drivers of research in NLP, with datasets
following, while metrics have minimal impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling ChatGPT for Explainable Automated Student Answer Assessment. (arXiv:2305.12962v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12962">
<div class="article-summary-box-inner">
<span><p>Providing explainable and faithful feedback is crucial for automated student
answer assessment. In this paper, we introduce a novel framework that explores
using ChatGPT, a cutting-edge large language model, for the concurrent tasks of
student answer scoring and rationale generation. We identify the appropriate
instructions by prompting ChatGPT with different templates to collect the
rationales, where inconsistent rationales are refined to align with marking
standards. The refined ChatGPT outputs enable us to fine-tune a smaller
language model that simultaneously assesses student answers and provides
rationales. Extensive experiments on the benchmark dataset show that the
proposed method improves the overall QWK score by 11% compared to ChatGPT.
Furthermore, our thorough analysis and human evaluation demonstrate that the
rationales generated by our proposed method are comparable to those of ChatGPT.
Our approach provides a viable solution to achieve explainable automated
assessment in education. Code available at
https://github.com/lijiazheng99/aera.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer. (arXiv:2305.13034v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13034">
<div class="article-summary-box-inner">
<span><p>Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in
domain adaptation tasks by integrating pre-trained Neural Machine Translation
(NMT) models with domain-specific token-level retrieval. However, the reasons
underlying its success have not been thoroughly investigated. In this paper, we
comprehensively analyze $k$NN-MT through theoretical and empirical studies.
Initially, we provide new insights into the working mechanism of $k$NN-MT as an
efficient technique to implicitly execute gradient descent on the output
projection layer of NMT, indicating that it is a specific case of model
fine-tuning. Subsequently, we conduct multi-domain experiments and word-level
analysis to examine the differences in performance between $k$NN-MT and
entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT
with adapters yields comparable translation performance to fine-tuning on
in-domain test sets, while achieving better performance on out-of-domain test
sets; (2) Fine-tuning significantly outperforms $k$NN-MT on the recall of
in-domain low-frequency words, but this gap could be bridged by optimizing the
context representations with additional adapter layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents. (arXiv:2305.13040v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13040">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) models have made significant progress in recent
years. However, previous studies primarily focus on datasets written by
annotators, which has resulted in a gap between academic research and
real-world spoken conversation scenarios. While several small-scale spoken TOD
datasets are proposed to address robustness issues such as ASR errors, they
ignore the unique challenges in spoken conversation. To tackle the limitations,
we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,
containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from
human-to-human spoken conversations. SpokenWOZ further incorporates common
spoken characteristics such as word-by-word processing and reasoning in spoken
language. Based on these characteristics, we present cross-turn slot and
reasoning slot detection as new challenges. We conduct experiments on various
baselines, including text-modal models, newly proposed dual-modal models, and
LLMs, e.g., ChatGPT. The results show that the current models still have
substantial room for improvement in spoken conversation, where the most
advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and
the SOTA end-to-end model only correctly completes the user request in 52.1% of
dialogues. The dataset, code, and leaderboard are available:
https://spokenwoz.github.io/SpokenWOZ-github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13245">
<div class="article-summary-box-inner">
<span><p>Multi-query attention (MQA), which only uses a single key-value head,
drastically speeds up decoder inference. However, MQA can lead to quality
degradation, and moreover it may not be desirable to train a separate model
just for faster inference. We (1) propose a recipe for uptraining existing
multi-head language model checkpoints into models with MQA using 5% of original
pre-training compute, and (2) introduce grouped-query attention (GQA), a
generalization of multi-query attention which uses an intermediate (more than
one, less than number of query heads) number of key-value heads. We show that
uptrained GQA achieves quality close to multi-head attention with comparable
speed to MQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning. (arXiv:2305.13627v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13627">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) that are tuned with instructions have
demonstrated remarkable capabilities in various tasks and languages. However,
their ability to generalize to underrepresented languages is limited due to the
scarcity of available data. Additionally, directly adapting new languages to
instruction-tuned LLMs can result in catastrophic forgetting, which leads to
the loss of multitasking ability. To address this issue, we propose
InstructAlign which uses continual crosslingual instruction tuning to enable
LLMs to align new unseen languages with previously learned high-resource
languages. Our results demonstrate the effectiveness of InstructAlign in
enabling the model to understand low-resource languages with limited parallel
data while preventing catastrophic forgetting. Our work contributes to the
advancement of language adaptation methods, particularly for adapting
instruction-tuned LLMs to underrepresented languages. Our code is released on
https://github.com/HLTCHKUST/InstructAlign
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection. (arXiv:2305.13658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13658">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques are widely used in low-resource automatic
morphological inflection to overcome data sparsity. However, the full
implications of these techniques remain poorly understood. In this study, we
aim to shed light on the theoretical aspects of the prominent data augmentation
strategy StemCorrupt (Silfverberg et al., 2017; Anastasopoulos and Neubig,
2019), a method that generates synthetic examples by randomly substituting stem
characters in gold standard training examples. To begin, we conduct an
information-theoretic analysis, arguing that StemCorrupt improves compositional
generalization by eliminating spurious correlations between morphemes,
specifically between the stem and the affixes. Our theoretical analysis further
leads us to study the sample efficiency with which StemCorrupt reduces these
spurious correlations. Through evaluation across seven typologically distinct
languages, we demonstrate that selecting a subset of datapoints with both high
diversity and high predictive uncertainty significantly enhances the
data-efficiency of StemCorrupt. However, we also explore the impact of
typological features on the choice of the data selection strategy and find that
languages incorporating a high degree of allomorphy and phonological
alternations derive less benefit from synthetic examples with high uncertainty.
We attribute this effect to phonotactic violations induced by StemCorrupt,
emphasizing the need for further research to ensure optimal performance across
the entire spectrum of natural language morphology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Context-Aware Neural Machine Translation. (arXiv:2305.13751v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13751">
<div class="article-summary-box-inner">
<span><p>Context-aware neural machine translation involves leveraging information
beyond sentence-level context to resolve inter-sentential discourse
dependencies and improve document-level translation quality, and has given rise
to a number of recent techniques. However, despite well-reasoned intuitions,
most context-aware translation models show only modest improvements over
sentence-level systems. In this work, we investigate several challenges that
impede progress within this field, relating to discourse phenomena, context
usage, model architectures, and document-level evaluation. To address these
problems, we propose a more realistic setting for document-level translation,
called paragraph-to-paragraph (para2para) translation, and collect a new
dataset of Chinese-English novels to promote future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Mistakes via Cooperative Study Assistant for Large Language Models. (arXiv:2305.13829v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13829">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated their potential to refine
their generation based on their own feedback. However, the feedback from LLM
itself is often inaccurate, thereby limiting its benefits. In this paper, we
propose Study Assistant for Large LAnguage Model (SALAM), a novel framework
with an auxiliary agent to assist the main LLM in learning from mistakes
through interactive cooperation. In the gathering phase, the student assistant
agent probes the main LLM, analyzes its errors, and collects the interaction in
a mistake memory. During the examination phase, the study assistant provides
guidelines by retrieving relevant cases to help the main LLM anticipate and
avoid similar errors. We first investigate the effectiveness of a general study
assistant and then customize it to provide LLM-specific guidance through
imitation learning from successful guidance experiences. Our experiments on
three LLMs using two challenging frameworks demonstrate that SALAM can
significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on
BBQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13981">
<div class="article-summary-box-inner">
<span><p>The robustness to distribution changes ensures that NLP models can be
successfully applied in the realistic world, especially for information
extraction tasks. However, most prior evaluation benchmarks have been devoted
to validating pairwise matching correctness, ignoring the crucial measurement
of robustness. In this paper, we present the first benchmark that simulates the
evaluation of open information extraction models in the real world, where the
syntactic and expressive distributions under the same knowledge meaning may
drift variously. We design and annotate a large-scale testbed in which each
example is a knowledge-invariant clique that consists of sentences with
structured knowledge of the same meaning but with different syntactic and
expressive forms. By further elaborating the robustness metric, a model is
judged to be robust if its performance is consistently accurate on the overall
cliques. We perform experiments on typical models published in the last decade
as well as a popular large language model, the results show that the existing
successful models exhibit a frustrating degradation, with a maximum drop of
23.43 F1 score. Our resources and code are available at
https://github.com/qijimrc/ROBUST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13999">
<div class="article-summary-box-inner">
<span><p>Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE)
have proven effective in scaling up Transformers model size for
\textit{pretraining} large language models. By only activating part of the FFN
parameters conditioning on input, S-FFN improves generalization performance
while keeping training and inference costs (in FLOPs) fixed. In this work, we
analyzed two major design choices of S-FFN: the memory block (a.k.a. expert)
size and the memory block selection method under a general conceptual framework
of sparse neural memory. Using this unified framework, we compare several S-FFN
architectures for language modeling and provide insights into their relative
efficacy and efficiency. We found a simpler selection method --
\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated
hidden states, achieving lower perplexity in language model pretraining
compared to existing MoE architectures including Switch Transformer (Fedus et
al., 2021) and HashLayer (Roller et al., 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer. (arXiv:2305.14280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14280">
<div class="article-summary-box-inner">
<span><p>We introduce and demonstrate how to effectively train multilingual machine
translation models with pixel representations. We experiment with two different
data settings with a variety of language and script coverage, demonstrating
improved performance compared to subword embeddings. We explore various
properties of pixel representations such as parameter sharing within and across
scripts to better understand where they lead to positive transfer. We observe
that these properties not only enable seamless cross-lingual transfer to unseen
scripts, but make pixel representations more data-efficient than alternatives
such as vocabulary expansion. We hope this work contributes to more extensible
multilingual models for all languages and scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Affordance and Situated Meaning in Image Captions: A Multimodal Analysis. (arXiv:2305.14616v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14616">
<div class="article-summary-box-inner">
<span><p>This paper explores the grounding issue regarding multimodal semantic
representation from a computational cognitive-linguistic view. We annotate
images from the Flickr30k dataset with five perceptual properties: Affordance,
Perceptual Salience, Object Number, Gaze Cueing, and Ecological Niche
Association (ENA), and examine their association with textual elements in the
image captions. Our findings reveal that images with Gibsonian affordance show
a higher frequency of captions containing 'holding-verbs' and 'container-nouns'
compared to images displaying telic affordance. Perceptual Salience, Object
Number, and ENA are also associated with the choice of linguistic expressions.
Our study demonstrates that comprehensive understanding of objects or events
requires cognitive attention, semantic nuances in language, and integration
across multiple modalities. We highlight the vital importance of situated
meaning and affordance grounding in natural language understanding, with the
potential to advance human-like interpretation in various scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning For Vision-and-language Cross-lingual Transfer. (arXiv:2305.14843v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14843">
<div class="article-summary-box-inner">
<span><p>Current pre-trained vison-language models (PVLMs) achieve excellent
performance on a range of multi-modal datasets. Recent work has aimed at
building multilingual models, and a range of novel multilingual multi-modal
datasets have been proposed. Current PVLMs typically perform poorly on these
datasets when used for multi-modal zero-shot or few-shot cross-lingual
transfer, especially for low-resource languages. To alleviate this problem, we
propose a novel meta-learning fine-tuning framework. Our framework makes
current PVLMs rapidly adaptive to new languages in vision-language scenarios by
designing MAML in a cross-lingual multi-modal manner. Experiments show that our
method boosts the performance of current state-of-the-art PVLMs in both
zero-shot and few-shot cross-lingual transfer on a range of vision-language
understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&amp;Co)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging GPT-4 for Automatic Translation Post-Editing. (arXiv:2305.14878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14878">
<div class="article-summary-box-inner">
<span><p>While Neural Machine Translation (NMT) represents the leading approach to
Machine Translation (MT), the outputs of NMT models still require translation
post-editing to rectify errors and enhance quality under critical settings. In
this work, we formalize the task of direct translation post-editing with Large
Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit
NMT outputs across several language pairs. Our results demonstrate that GPT-4
is adept at translation post-editing, producing meaningful and trustworthy
edits to translations that help improve its general quality as well as remove
different classes of major errors in translations. In particular, human
evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large
improvement over the prior state-of-the-art LLM. Notably, we improve upon
state-of-the-art performance on WMT-22 English-Chinese, English-German,
Chinese-English and German-English language pairs using GPT-4 based
post-editing, as evaluated by state-of-the-art MT quality metrics. However, we
also show that GPT-4 could produce hallucinated edits, thereby urging caution
in its use as an expert translation post-editor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14879">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the capacity of language models to generate
explicit, interpretable, and interactive world models of scientific and
common-sense reasoning tasks. We operationalize this as a task of generating
text games, expressed as hundreds of lines of Python code. To facilitate this
task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a
corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We
empirically demonstrate that GPT-4 can use these games as templates for
single-shot in-context learning, successfully producing runnable games on
unseen topics in 28% of cases. When allowed to self-reflect on program errors,
game runnability substantially increases to 57%. While evaluating simulation
fidelity is labor-intensive, we introduce a suite of automated metrics to
assess game fidelity, technical validity, adherence to task specifications, and
winnability, showing a high degree of agreement with expert human ratings. We
pose this as a challenge task to spur further development at the juncture of
world modeling and code generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRACE: Discriminator-Guided Chain-of-Thought Reasoning. (arXiv:2305.14934v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14934">
<div class="article-summary-box-inner">
<span><p>In the context of multi-step reasoning, e.g., with chain-of-thought, language
models (LMs) can easily assign a high likelihood to incorrect steps. As a
result, decoding strategies that optimize for solution likelihood often yield
incorrect solutions. To address this issue, we propose Guiding chain-of-thought
ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding
approach that steers the decoding process towards producing correct reasoning
steps. GRACE employs a discriminator trained with a contrastive loss over
correct and incorrect steps, which is used during decoding to score next-step
candidates based on their correctness. Importantly, GRACE only requires
sampling from the LM, without the need for LM training or fine-tuning. Using
models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and
two symbolic reasoning tasks, where it exhibits substantial performance gains
compared to greedy decoding, verifiers, and self-consistency in most settings.
When further combined with self-consistency, GRACE outperforms all the
baselines by sizeable margins. Human and LLM evaluations over GSM8K show that
GRACE not only improves the final answer accuracy but also the correctness of
the intermediate reasoning. Our implementation can be accessed at
\url{https://github.com/mukhal/grace}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. (arXiv:2305.14975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14975">
<div class="article-summary-box-inner">
<span><p>A trustworthy real-world prediction system should produce well-calibrated
confidence scores; that is, its confidence in an answer should be indicative of
the likelihood that the answer is correct, enabling deferral to an expert in
cases of low-confidence predictions. Recent studies have shown that
unsupervised pre-training produces large language models (LMs) whose
conditional probabilities are remarkably well-calibrated. However, the most
widely-used LMs are fine-tuned with reinforcement learning from human feedback
(RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional
probabilities that are very poorly calibrated. In light of this perceived
weakness, we conduct a broad evaluation of methods for extracting confidence
scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find
that verbalized confidences emitted as output tokens are typically
better-calibrated than the model's conditional probabilities on the TriviaQA,
SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error
by a relative 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dolphin: A Challenging and Diverse Benchmark for Arabic NLG. (arXiv:2305.14989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14989">
<div class="article-summary-box-inner">
<span><p>We present Dolphin, a novel benchmark that addresses the need for a natural
language generation (NLG) evaluation framework dedicated to the wide collection
of Arabic languages and varieties. The proposed benchmark encompasses a broad
range of 13 different NLG tasks, including dialogue generation, question
answering, machine translation, summarization, among others. Dolphin comprises
a substantial corpus of 40 diverse and representative public datasets across 50
test splits, carefully curated to reflect real-world scenarios and the
linguistic richness of Arabic. It sets a new standard for evaluating the
performance and generalization capabilities of Arabic and multilingual models,
promising to enable researchers to push the boundaries of current
methodologies. We provide an extensive analysis of Dolphin, highlighting its
diversity and identifying gaps in current Arabic NLG research. We also offer a
public leaderboard that is both interactive and modular and evaluate several
models on our benchmark, allowing us to set strong baselines against which
researchers can compare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ACL OCL Corpus: Advancing Open Science in Computational Linguistics. (arXiv:2305.14996v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14996">
<div class="article-summary-box-inner">
<span><p>We present ACL OCL, a scholarly corpus derived from the ACL Anthology to
assist Open scientific research in the Computational Linguistics domain.
Integrating and enhancing the previous versions of the ACL Anthology, the ACL
OCL contributes metadata, PDF files, citation graphs and additional structured
full texts with sections, figures, and links to a large knowledge resource
(Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers,
alongside 210K figures.
</p>
<p>We spotlight how ACL OCL applies to observe trends in computational
linguistics. By detecting paper topics with a supervised neural model, we note
that interest in "Syntax: Tagging, Chunking and Parsing" is waning and "Natural
Language Generation" is resurging. Our dataset is available from HuggingFace
(https://huggingface.co/datasets/WINGNUS/ACL-OCL).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation. (arXiv:2305.15025v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15025">
<div class="article-summary-box-inner">
<span><p>Current variational dialog models have employed pre-trained language models
(PLMs) to parameterize the likelihood and posterior distributions. However, the
Gaussian assumption made on the prior distribution is incompatible with these
distributions, thus restricting the diversity of generated responses. These
models also suffer from posterior collapse, i.e., the decoder tends to ignore
latent variables and directly access information captured in the encoder
through the cross-attention mechanism. In this work, we propose Dior-CVAE, a
hierarchical conditional variational autoencoder (CVAE) with diffusion priors
to address these challenges. We employ a diffusion model to increase the
complexity of the prior distribution and its compatibility with the
distributions produced by a PLM. Also, we propose memory dropout to the
cross-attention mechanism, which actively encourages the use of latent
variables for response generation. Overall, experiments across two commonly
used open-domain dialog datasets show that our method can generate more diverse
responses without large-scale dialog pre-training. Code is available at
https://github.com/UKPLab/dior-cvae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15068">
<div class="article-summary-box-inner">
<span><p>Theory of Mind (ToM), the capacity to comprehend the mental states of
distinct individuals, is essential for numerous practical applications. With
the development of large language models (LLMs), there is a heated debate about
whether they are able to perform ToM tasks. Previous studies have used
different tasks and prompts to test the ToM on LLMs and the results are
inconsistent: some studies asserted these models are capable of exhibiting ToM,
while others suggest the opposite. In this study, We present ToMChallenges, a
dataset for comprehensively evaluating the Theory of Mind based on the
Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also
propose an auto-grader to streamline the answer evaluation process. We tested
three models: davinci, turbo, and gpt-4. Our evaluation results and error
analyses show that LLMs have inconsistent behaviors across prompts and tasks.
Performing the ToM tasks robustly remains a challenge for the LLMs. In
addition, our paper wants to raise awareness in evaluating the ToM in LLMs and
we want to invite more discussion on how to design the prompts and tasks for
ToM tasks that can better assess the LLMs' ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Sentence Embeddings from Scratch. (arXiv:2305.15077v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15077">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been the dominant approach to train state-of-the-art
sentence embeddings. Previous studies have typically learned sentence
embeddings either through the use of human-annotated natural language inference
(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.
However, even in the case of unlabeled data, their acquisition presents
challenges in certain domains due to various reasons. To address these issues,
we present SynCSE, a contrastive learning framework that trains sentence
embeddings with synthesized data. Specifically, we explore utilizing large
language models to synthesize the required data samples for contrastive
learning, including (1) producing positive and negative annotations given
unlabeled sentences (SynCSE-partial), and (2) generating sentences along with
their corresponding annotations from scratch (SynCSE-scratch). Experimental
results on sentence similarity and reranking tasks indicate that both
SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,
and SynCSE-partial even achieves comparable performance to the supervised
models in most settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs. (arXiv:2305.16339v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16339">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03838">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusations of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a robust AI-text detector via adversarial
learning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic content to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5-Turbo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.13854">
<div class="article-summary-box-inner">
<span><p>With advances in generative AI, there is now potential for autonomous agents
to manage daily tasks via natural language commands. However, current agents
are primarily created and tested in simplified synthetic environments, leading
to a disconnect with real-world scenarios. In this paper, we build an
environment for language-guided agents that is highly realistic and
reproducible. Specifically, we focus on agents that perform tasks on the web,
and create an environment with fully functional websites from four common
domains: e-commerce, social forum discussions, collaborative software
development, and content management. Our environment is enriched with tools
(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage
human-like task-solving. Building upon our environment, we release a set of
benchmark tasks focusing on evaluating the functional correctness of task
completions. The tasks in our benchmark are diverse, long-horizon, and designed
to emulate tasks that humans routinely perform on the internet. We experiment
with several baseline agents, integrating recent techniques such as reasoning
before acting. The results demonstrate that solving complex tasks is
challenging: our best GPT-4-based agent only achieves an end-to-end task
success rate of 14.41%, significantly lower than the human performance of
78.24%. These results highlight the need for further development of robust
agents, that current state-of-the-art large language models are far from
perfect performance in these real-life tasks, and that WebArena can be used to
measure such progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. (arXiv:2308.02019v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02019">
<div class="article-summary-box-inner">
<span><p>We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02490">
<div class="article-summary-box-inner">
<span><p>We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties. (arXiv:2308.03051v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03051">
<div class="article-summary-box-inner">
<span><p>Despite the purported multilingual proficiency of instruction-finetuned large
language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of
these models remains insufficiently explored. Considering this constraint, we
present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5
and GPT-4) regarding their machine translation proficiencies across ten
varieties of Arabic. Our evaluation covers diverse Arabic varieties such as
Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level
dialectal variants. Our analysis indicates that LLMs may encounter challenges
with dialects for which minimal public datasets exist, but on average are
better translators of dialects than existing commercial systems. On CA and MSA,
instruction-tuned LLMs, however, trail behind commercial systems such as Google
Translate. Finally, we undertake a human-centric study to scrutinize the
efficacy of the relatively recent model, Bard, in following human instructions
during translation tasks. Our analysis reveals a circumscribed capability of
Bard in aligning with human instructions in translation contexts. Collectively,
our findings underscore that prevailing LLMs remain far from inclusive, with
only limited ability to cater for the linguistic and cultural intricacies of
diverse communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07308">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are popular for high-quality text generation but
can produce harmful content, even when aligned with human values through
reinforcement learning. Adversarial prompts can bypass their safety measures.
We propose LLM Self Defense, a simple approach to defend against these attacks
by having an LLM screen the induced responses. Our method does not require any
fine-tuning, input preprocessing, or iterative output generation. Instead, we
incorporate the generated content into a pre-defined prompt and employ another
instance of an LLM to analyze the text and predict whether it is harmful. We
test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent
LLMs against various types of attacks, such as forcefully inducing affirmative
responses to prompts and prompt engineering attacks. Notably, LLM Self Defense
succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5
and Llama 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources. (arXiv:2309.02373v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02373">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language models like T5 have revolutionized the NLP
landscape, but their computational demands hinder a large portion of the
research community. To address this challenge, we present nanoT5, a
specially-optimized PyTorch framework for efficient pre-training and
fine-tuning of T5 models. Drawing on insights from optimizer differences and
prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a
single GPU in just 16 hours, without any loss in performance. With the
introduction of this open-source framework, we hope to widen the accessibility
to language modelling research and cater to the community's demand for more
user-friendly T5 (Encoder-Decoder) implementations. We make our contributions,
including configurations, codebase, pre-training insights, and pre-trained
models, available to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06132">
<div class="article-summary-box-inner">
<span><p>We present a hybrid approach to the automated measurement of vagueness and
subjectivity in texts. We first introduce the expert system VAGO, we illustrate
it on a small benchmark of fact vs. opinion sentences, and then test it on the
larger French press corpus FreSaDa to confirm the higher prevalence of
subjective markers in satirical vs. regular texts. We then build a neural clone
of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores
obtained on FreSaDa. Using explainability tools (LIME), we show the interest of
this neural version for the enrichment of the lexicons of the symbolic version,
and for the production of versions in other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07430">
<div class="article-summary-box-inner">
<span><p>Sifting through vast textual data and summarizing key information from
electronic health records (EHR) imposes a substantial burden on how clinicians
allocate their time. Although large language models (LLMs) have shown immense
promise in natural language processing (NLP) tasks, their efficacy on a diverse
range of clinical summarization tasks has not yet been rigorously demonstrated.
In this work, we apply domain adaptation methods to eight LLMs, spanning six
datasets and four distinct clinical summarization tasks: radiology reports,
patient questions, progress notes, and doctor-patient dialogue. Our thorough
quantitative assessment reveals trade-offs between models and adaptation
methods in addition to instances where recent advances in LLMs may not improve
results. Further, in a clinical reader study with ten physicians, we show that
summaries from our best-adapted LLMs are preferable to human summaries in terms
of completeness and correctness. Our ensuing qualitative analysis highlights
challenges faced by both LLMs and human experts. Lastly, we correlate
traditional quantitative NLP metrics with reader study scores to enhance our
understanding of how these metrics align with physician preferences. Our
research marks the first evidence of LLMs outperforming human experts in
clinical text summarization across multiple tasks. This implies that
integrating LLMs into clinical workflows could alleviate documentation burden,
empowering clinicians to focus more on personalized patient care and the
inherently human aspects of medicine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08030">
<div class="article-summary-box-inner">
<span><p>Speech enhancement systems are typically trained using pairs of clean and
noisy speech. In audio-visual speech enhancement (AVSE), there is not as much
ground-truth clean data available; most audio-visual datasets are collected in
real-world environments with background noise and reverberation, hampering the
development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based
audio-visual speech enhancement approach that can generate clean speech despite
the challenges of real-world training data. We obtain a subset of nearly clean
speech from an audio-visual corpus using a neural quality estimator, and then
train a diffusion model on this subset to generate waveforms conditioned on
continuous speech representations from AV-HuBERT with noise-robust training. We
use continuous rather than discrete representations to retain prosody and
speaker information. With this vocoding task alone, the model can perform
speech enhancement better than a masking-based baseline. We further fine-tune
the diffusion model on clean/noisy utterance pairs to improve the performance.
Our approach outperforms a masking-based baseline in terms of both automatic
metrics and a human listening test and is close in quality to the target speech
in the listening test. Audio samples can be found at
https://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnglE-optimized Text Embeddings. (arXiv:2309.12871v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12871">
<div class="article-summary-box-inner">
<span><p>High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unify word-level and span-level tasks: NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13230">
<div class="article-summary-box-inner">
<span><p>We introduce the submissions of the NJUNLP team to the WMT 2023 Quality
Estimation (QE) shared task. Our team submitted predictions for the
English-German language pair on all two sub-tasks: (i) sentence- and word-level
quality prediction; and (ii) fine-grained error span detection. This year, we
further explore pseudo data methods for QE based on NJUQE framework
(https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel
data from the WMT translation task. We pre-train the XLMR large model on pseudo
QE data, then fine-tune it on real QE data. At both stages, we jointly learn
sentence-level scores and word-level tags. Empirically, we conduct experiments
to find the key hyper-parameters that improve the performance. Technically, we
propose a simple method that covert the word-level outputs to fine-grained
error span results. Overall, our models achieved the best results in
English-German for both word-level and fine-grained error span detection
sub-tasks by a considerable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01320">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02971">
<div class="article-summary-box-inner">
<span><p>Prompting and adapter tuning have emerged as efficient alternatives to
fine-tuning (FT) methods. However, existing studies on speech prompting focused
on classification tasks and failed on more complex sequence generation tasks.
Besides, adapter tuning is primarily applied with a focus on encoder-only
self-supervised models. Our experiments show that prompting on Wav2Seq, a
self-supervised encoder-decoder model, surpasses previous works in sequence
generation tasks. It achieves a remarkable 53% relative improvement in word
error rate for ASR and a 27% in F1 score for slot filling. Additionally,
prompting competes with the FT method in the low-resource scenario. Moreover,
we show the transferability of prompting and adapter tuning on Wav2Seq in
cross-lingual ASR. When limited trainable parameters are involved, prompting
and adapter tuning consistently outperform conventional FT across 7 languages.
Notably, in the low-resource scenario, prompting consistently outperforms
adapter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03128">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have garnered significant attention due to their
impressive natural language processing (NLP) capabilities. Recently, many
studies have focused on the tool utilization ability of LLMs. They primarily
investigated how LLMs effectively collaborate with given specific tools.
However, in scenarios where LLMs serve as intelligent agents, as seen in
applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate
decision-making processes that involve deciding whether to employ a tool and
selecting the most suitable tool(s) from a collection of available tools to
fulfill user requests. Therefore, in this paper, we introduce MetaTool, a
benchmark designed to evaluate whether LLMs have tool usage awareness and can
correctly choose tools. Specifically, we create a dataset called ToolE within
the benchmark. This dataset contains various types of user queries in the form
of prompts that trigger LLMs to use tools, including both single-tool and
multi-tool scenarios. Subsequently, we set the tasks for both tool usage
awareness and tool selection. We define four subtasks from different
perspectives in tool selection, including tool selection with similar choices,
tool selection in specific scenarios, tool selection with possible reliability
issues, and multi-tool selection. We conduct experiments involving nine popular
LLMs and find that the majority of them still struggle to effectively select
tools, highlighting the existing gaps between LLMs and genuine intelligent
agents. However, through the error analysis, we found there is still
significant room for improvement. Finally, we conclude with insights for tool
developers that follow ChatGPT to provide detailed descriptions that can
enhance the tool selection performance of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03368">
<div class="article-summary-box-inner">
<span><p>In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index. (arXiv:2310.05030v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05030">
<div class="article-summary-box-inner">
<span><p>With the rise of prolific ChatGPT, the risk and consequences of AI-generated
text has increased alarmingly. To address the inevitable question of ownership
attribution for AI-generated artifacts, the US Copyright Office released a
statement stating that 'If a work's traditional elements of authorship were
produced by a machine, the work lacks human authorship and the Office will not
register it'. Furthermore, both the US and the EU governments have recently
drafted their initial proposals regarding the regulatory framework for AI.
Given this cynosural spotlight on generative AI, AI-generated text detection
(AGTD) has emerged as a topic that has already received immediate attention in
research, with some initial methods having been proposed, soon followed by
emergence of techniques to bypass detection. This paper introduces the Counter
Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a
comprehensive evaluation of the robustness of existing AGTD techniques. Our
empirical findings unequivocally highlight the fragility of the proposed AGTD
methods under scrutiny. Amidst the extensive deliberations on policy-making for
regulating AI development, it is of utmost importance to assess the
detectability of content generated by LLMs. Thus, to establish a quantifiable
spectrum facilitating the evaluation and ranking of LLMs according to their
detectability levels, we propose the AI Detectability Index (ADI). We conduct a
thorough examination of 15 contemporary LLMs, empirically demonstrating that
larger LLMs tend to have a higher ADI, indicating they are less detectable
compared to smaller LLMs. We firmly believe that ADI holds significant value as
a tool for the wider NLP community, with the potential to serve as a rubric in
AI-related policy-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05374">
<div class="article-summary-box-inner">
<span><p>Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence. (arXiv:2310.05388v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05388">
<div class="article-summary-box-inner">
<span><p>Conditional story generation is significant in human-machine interaction,
particularly in producing stories with complex plots. While Large language
models (LLMs) perform well on multiple NLP tasks, including story generation,
it is challenging to generate stories with both complex and creative plots.
Existing methods often rely on detailed prompts to guide LLMs to meet target
conditions, which inadvertently restrict the creative potential of the
generated stories. We argue that leveraging information from exemplary
human-written stories facilitates generating more diverse plotlines. Delving
deeper into story details helps build complex and credible plots. In this
paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation
framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to
enhance stories' complexity. We build a retrieval repository for target
conditions to produce few-shot examples to prompt LLMs. Additionally, we design
an ``asking-why'' prompting scheme that extracts a forest of evidence,
providing compensation for the ambiguities that may occur in the generated
story. This iterative process uncovers underlying story backgrounds. Finally,
we select the most fitting chains of evidence from the evidence forest and
integrate them into the generated story, thereby enhancing the narrative's
complexity and credibility. Experimental results and numerous examples verify
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection. (arXiv:2310.06498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06498">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown their ability to collaborate
effectively with humans in real-world scenarios. However, LLMs are apt to
generate hallucinations, i.e., makeup incorrect text and unverified
information, which can cause significant damage when deployed for
mission-critical tasks. In this paper, we propose a self-check approach based
on reverse validation to detect factual errors automatically in a zero-resource
fashion. To facilitate future studies and assess different methods, we
construct a hallucination detection benchmark named PHD, which is generated by
ChatGPT and annotated by human annotators. Contrasting previous studies of
zero-resource hallucination detection, our method and benchmark concentrate on
passage-level detection instead of sentence-level. We empirically evaluate our
method and existing zero-resource detection methods on two datasets. The
experimental results demonstrate that the proposed method considerably
outperforms the baselines while costing fewer tokens and less time.
Furthermore, we manually analyze some hallucination cases that LLM failed to
capture, revealing the shared limitation of zero-resource methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PuoBERTa: Training and evaluation of a curated language model for Setswana. (arXiv:2310.09141v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09141">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) has made significant progress for
well-resourced languages such as English but lagged behind for low-resource
languages like Setswana. This paper addresses this gap by presenting PuoBERTa,
a customised masked language model trained specifically for Setswana. We cover
how we collected, curated, and prepared diverse monolingual texts to generate a
high-quality corpus for PuoBERTa's training. Building upon previous efforts in
creating monolingual resources for Setswana, we evaluated PuoBERTa across
several NLP tasks, including part-of-speech (POS) tagging, named entity
recognition (NER), and news categorisation. Additionally, we introduced a new
Setswana news categorisation dataset and provided the initial benchmarks using
PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP
capabilities for understudied languages like Setswana and paves the way for
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09168">
<div class="article-summary-box-inner">
<span><p>Instruction-tuning can be substantially optimized through enhanced diversity,
resulting in models capable of handling a broader spectrum of tasks. However,
existing data employed for such tuning often exhibit an inadequate coverage of
individual domains, limiting the scope for nuanced comprehension and
interactions within these areas. To address this deficiency, we propose
Explore-Instruct, a novel approach to enhance the data coverage to be used in
domain-specific instruction-tuning through active exploration via Large
Language Models (LLMs). Built upon representative domain use cases,
Explore-Instruct explores a multitude of variations or possibilities by
implementing a search algorithm to obtain diversified and domain-focused
instruction-tuning data. Our data-centric analysis validates the effectiveness
of this proposed approach in improving domain-specific instruction coverage.
Moreover, our model's performance demonstrates considerable advancements over
multiple baselines, including those utilizing domain-specific data enhancement.
Our findings offer a promising opportunity to improve instruction coverage,
especially in domain-specific contexts, thereby advancing the development of
adaptable language models. Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/Explore-Instruct}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models. (arXiv:2310.10180v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10180">
<div class="article-summary-box-inner">
<span><p>Automated theorem proving (ATP) has become an appealing domain for exploring
the reasoning ability of the recent successful generative language models.
However, current ATP benchmarks mainly focus on symbolic inference, but rarely
involve the understanding of complex number combination reasoning. In this
work, we propose TRIGO, an ATP benchmark that not only requires a model to
reduce a trigonometric expression with step-by-step proofs but also evaluates a
generative LM's reasoning ability on formulas and its capability to manipulate,
group, and factor number terms. We gather trigonometric expressions and their
reduced forms from the web, annotate the simplification process manually, and
translate it into the Lean formal language system. We then automatically
generate additional examples from the annotated samples to expand the dataset.
Furthermore, we develop an automatic generator based on Lean-Gym to create
dataset splits of varying difficulties and distributions in order to thoroughly
analyze the model's generalization ability. Our extensive experiments show our
proposed TRIGO poses a new challenge for advanced generative LM's including
GPT-4 which is pre-trained on a considerable amount of open-source formal
theorem-proving language data, and provide a new tool to study the generative
LM's ability on both formal and mathematical reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Weak Supervision To Generate Indonesian Conservation Dataset. (arXiv:2310.11258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11258">
<div class="article-summary-box-inner">
<span><p>Weak supervision has emerged as a promising approach for rapid and
large-scale dataset creation in response to the increasing demand for
accelerated NLP development. By leveraging labeling functions, weak supervision
allows practitioners to generate datasets quickly by creating learned label
models that produce soft-labeled datasets. This paper aims to show how such an
approach can be utilized to build an Indonesian NLP dataset from conservation
news text. We construct two types of datasets: multi-class classification and
sentiment classification. We then provide baseline experiments using various
pretrained language models. These baseline results demonstrate test
performances of 59.79% accuracy and 55.72% F1-score for sentiment
classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC
for multi-class classification. Additionally, we release the datasets and
labeling functions used in this work for further research and exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11368">
<div class="article-summary-box-inner">
<span><p>Recognizing vulnerability is crucial for understanding and implementing
targeted support to empower individuals in need. This is especially important
at the European Court of Human Rights (ECtHR), where the court adapts
Convention standards to meet actual individual needs and thus ensures effective
human rights protection. However, the concept of vulnerability remains elusive
at the ECtHR and no prior NLP research has dealt with it. To enable future
research in this area, we present VECHR, a novel expert-annotated multi-label
dataset comprising of vulnerability type classification and explanation
rationale. We benchmark the performance of state-of-the-art models on VECHR
from both prediction and explainability perspectives. Our results demonstrate
the challenging nature of the task with lower prediction performance and
limited agreement between models and experts. Further, we analyze the
robustness of these models in dealing with out-of-domain (OOD) data and observe
overall limited performance. Our dataset poses unique challenges offering
significant room for improvement regarding performance, explainability, and
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11398">
<div class="article-summary-box-inner">
<span><p>In the realm of deep learning, the self-attention mechanism has substantiated
its pivotal role across a myriad of tasks, encompassing natural language
processing and computer vision. Despite achieving success across diverse
applications, the traditional self-attention mechanism primarily leverages
linear transformations for the computation of query, key, and value (QKV),
which may not invariably be the optimal choice under specific circumstances.
This paper probes into a novel methodology for QKV computation-implementing a
specially-designed neural network structure for the calculation. Utilizing a
modified Marian model, we conducted experiments on the IWSLT 2017
German-English translation task dataset and juxtaposed our method with the
conventional approach. The experimental results unveil a significant
enhancement in BLEU scores with our method. Furthermore, our approach also
manifested superiority when training the Roberta model with the Wikitext-103
dataset, reflecting a notable reduction in model perplexity compared to its
original counterpart. These experimental outcomes not only validate the
efficacy of our method but also reveal the immense potential in optimizing the
self-attention mechanism through neural network-based QKV computation, paving
the way for future research and practical applications. The source code and
implementation details for our proposed method can be accessed at
https://github.com/ocislyjrti/NeuralAttention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification. (arXiv:2310.11878v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11878">
<div class="article-summary-box-inner">
<span><p>In legal NLP, Case Outcome Classification (COC) must not only be accurate but
also trustworthy and explainable. Existing work in explainable COC has been
limited to annotations by a single expert. However, it is well-known that
lawyers may disagree in their assessment of case facts. We hence collect a
novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two
experts in the domain of international human rights law, for whom we observe
weak agreement. We study their disagreements and build a two-level
task-independent taxonomy, supplemented with COC-specific subcategories. To our
knowledge, this is the first work in the legal NLP that focuses on human label
variation. We quantitatively assess different taxonomy categories and find that
disagreements mainly stem from underspecification of the legal context, which
poses challenges given the typically limited granularity and noise in COC
metadata. We further assess the explainablility of SOTA COC models on RAVE and
observe limited agreement between models and experts. Overall, our case study
reveals hitherto underappreciated complexities in creating benchmark datasets
in legal NLP that revolve around identifying aspects of a case's facts
supposedly relevant to its outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining. (arXiv:2310.12172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12172">
<div class="article-summary-box-inner">
<span><p>This paper presents an overview of the ImageArg shared task, the first
multimodal Argument Mining shared task co-located with the 10th Workshop on
Argument Mining at EMNLP 2023. The shared task comprises two classification
subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image
Persuasiveness Classification. The former determines the stance of a tweet
containing an image and a piece of text toward a controversial topic (e.g., gun
control and abortion). The latter determines whether the image makes the tweet
text more persuasive. The shared task received 31 submissions for Subtask-A and
21 submissions for Subtask-B from 9 different teams across 6 countries. The top
submission in Subtask-A achieved an F1-score of 0.8647 while the best
submission in Subtask-B achieved an F1-score of 0.5561.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13469">
<div class="article-summary-box-inner">
<span><p>Transformer models have demonstrated remarkable performance in neural machine
translation (NMT). However, their vulnerability to noisy input poses a
significant challenge in practical implementation, where generating clean
output from noisy input is crucial. The MTNT dataset is widely used as a
benchmark for evaluating the robustness of NMT models against noisy input.
Nevertheless, its utility is limited due to the presence of noise in both the
source and target sentences. To address this limitation, we focus on cleaning
the noise from the target sentences in MTNT, making it more suitable as a
benchmark for noise evaluation. Leveraging the capabilities of large language
models (LLMs), we observe their impressive abilities in noise removal. For
example, they can remove emojis while considering their semantic meaning.
Additionally, we show that LLM can effectively rephrase slang, jargon, and
profanities. The resulting datasets, called C-MTNT, exhibit significantly less
noise in the target sentences while preserving the semantic integrity of the
original sentences. Our human and GPT-4 evaluations also lead to a consistent
conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT
showcased its effectiveness in evaluating the robustness of NMT models,
highlighting the potential of advanced language models for data cleaning and
emphasizing C-MTNT as a valuable resource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13548">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) is a popular technique for
training high-quality AI assistants. However, RLHF may also encourage model
responses that match user beliefs over truthful responses, a behavior known as
sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models
and whether human preference judgements are responsible. We first demonstrate
that five state-of-the-art AI assistants consistently exhibit sycophantic
behavior across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior of RLHF models, we
analyze existing human preference data. We find that when a response matches a
user's views, it is more likely to be preferred. Moreover, both humans and
preference models (PMs) prefer convincingly-written sycophantic responses over
correct ones a non-negligible fraction of the time. Optimizing model outputs
against PMs also sometimes sacrifices truthfulness in favor of sycophancy.
Overall, our results indicate that sycophancy is a general behavior of RLHF
models, likely driven in part by human preference judgements favoring
sycophantic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities. (arXiv:2310.14512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14512">
<div class="article-summary-box-inner">
<span><p>Event coreference resolution (ECR) aims to group event mentions referring to
the same real-world event into clusters. Most previous studies adopt the
"encoding first, then scoring" framework, making the coreference judgment rely
on event encoding. Furthermore, current methods struggle to leverage
human-summarized ECR rules, e.g., coreferential events should have the same
event type, to guide the model. To address these two issues, we propose a
prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM
(masked language model) task. This allows for simultaneous event modeling and
coreference discrimination within a single template, with a fully shared
context. In addition, we introduce two auxiliary prompt tasks, event-type
compatibility and argument compatibility, to explicitly demonstrate the
reasoning process of ECR, which helps the model make final predictions.
Experimental results show that our method CorefPrompt performs well in a
state-of-the-art (SOTA) benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Word-Level Auto-Completion in Computer-Aided Translation. (arXiv:2310.14523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14523">
<div class="article-summary-box-inner">
<span><p>Word-Level Auto-Completion (WLAC) plays a crucial role in Computer-Assisted
Translation. It aims at providing word-level auto-completion suggestions for
human translators. While previous studies have primarily focused on designing
complex model architectures, this paper takes a different perspective by
rethinking the fundamental question: what kind of words are good
auto-completions? We introduce a measurable criterion to answer this question
and discover that existing WLAC models often fail to meet this criterion.
Building upon this observation, we propose an effective approach to enhance
WLAC performance by promoting adherence to the criterion. Notably, the proposed
approach is general and can be applied to various encoder-based architectures.
Through extensive experiments, we demonstrate that our approach outperforms the
top-performing system submitted to the WLAC shared tasks in WMT2022, while
utilizing significantly smaller model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing ChatGPT for thematic analysis: Are we ready?. (arXiv:2310.14545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14545">
<div class="article-summary-box-inner">
<span><p>ChatGPT is an advanced natural language processing tool with growing
applications across various disciplines in medical research. Thematic analysis,
a qualitative research method to identify and interpret patterns in data, is
one application that stands to benefit from this technology. This viewpoint
explores the utilization of ChatGPT in three core phases of thematic analysis
within a medical context: 1) direct coding of transcripts, 2) generating themes
from a predefined list of codes, and 3) preprocessing quotes for manuscript
inclusion. Additionally, we explore the potential of ChatGPT to generate
interview transcripts, which may be used for training purposes. We assess the
strengths and limitations of using ChatGPT in these roles, highlighting areas
where human intervention remains necessary. Overall, we argue that ChatGPT can
function as a valuable tool during analysis, enhancing the efficiency of the
thematic analysis and offering additional insights into the qualitative data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab, IIT Madras. (arXiv:2310.14654v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14654">
<div class="article-summary-box-inner">
<span><p>India is home to a multitude of languages of which 22 languages are
recognised by the Indian Constitution as official. Building speech based
applications for the Indian population is a difficult problem owing to limited
data and the number of languages and accents to accommodate. To encourage the
language technology community to build speech based applications in Indian
languages, we are open sourcing SPRING-INX data which has about 2000 hours of
legally sourced and manually transcribed speech data for ASR system building in
Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi
and Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology
Madras and is a part of National Language Translation Mission (NLTM), funded by
the Indian Ministry of Electronics and Information Technology (MeitY),
Government of India. We describe the data collection and data cleaning process
along with the data statistics in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14724">
<div class="article-summary-box-inner">
<span><p>The powerful ability to understand, follow, and generate complex language
emerging from large language models (LLMs) makes LLM-generated text flood many
areas of our daily lives at an incredible speed and is widely accepted by
humans. As LLMs continue to expand, there is an imperative need to develop
detectors that can detect LLM-generated text. This is crucial to mitigate
potential misuse of LLMs and safeguard realms like artistic expression and
social networks from harmful influence of LLM-generated content. The
LLM-generated text detection aims to discern if a piece of text was produced by
an LLM, which is essentially a binary classification task. The detector
techniques have witnessed notable advancements recently, propelled by
innovations in watermarking techniques, zero-shot methods, fine-turning LMs
methods, adversarial learning methods, LLMs as detectors, and human-assisted
methods. In this survey, we collate recent research breakthroughs in this area
and underscore the pressing need to bolster detector research. We also delve
into prevalent datasets, elucidating their limitations and developmental
requirements. Furthermore, we analyze various LLM-generated text detection
paradigms, shedding light on challenges like out-of-distribution problems,
potential attacks, and data ambiguity. Conclusively, we highlight interesting
directions for future research in LLM-generated text detection to advance the
implementation of responsible artificial intelligence (AI). Our aim with this
survey is to provide a clear and comprehensive introduction for newcomers while
also offering seasoned researchers a valuable update in the field of
LLM-generated text detection. The useful resources are publicly available at:
https://github.com/NLP2CT/LLM-generated-Text-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14747">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have showcased remarkable capabilities in
complex reasoning through chain of thought (CoT) prompting. Recently, there has
been a growing interest in transferring these reasoning abilities from LLMs to
smaller models. However, achieving both the diversity and consistency in
rationales presents a challenge. In this paper, we focus on enhancing these two
aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to
efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple
rationales for each question and enforce consistency among the corresponding
predictions by minimizing the bidirectional KL-divergence between the answer
distributions. We investigate the effectiveness of MCC-KD with different model
architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both
mathematical reasoning and commonsense reasoning benchmarks. The empirical
results not only confirm MCC-KD's superior performance on in-distribution
datasets but also highlight its robust generalization ability on
out-of-distribution datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14892">
<div class="article-summary-box-inner">
<span><p>Controllable text generation (CTG) aims to generate text with desired
attributes, and decoding-time-based methods have shown promising performance on
this task. However, in this paper, we identify the phenomenon of Attribute
Collapse for the first time. It causes the fluency of generated text to rapidly
decrease when the control strength exceeds a critical value, rendering the text
completely unusable. This limitation hinders the effectiveness of decoding
methods in achieving high levels of controllability. To address this problem,
we propose a novel lightweight decoding framework named Air-Decoding. Its main
idea is reconstructing the attribute distributions to balance the weights
between attribute words and non-attribute words to generate more fluent text.
Specifically, we train prefixes by prefix-tuning to obtain attribute
distributions. Then we design a novel attribute distribution reconstruction
method to balance the obtained distributions and use the reconstructed
distributions to guide language models for generation, effectively avoiding the
issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our
method achieves a new state-of-the-art control performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15019">
<div class="article-summary-box-inner">
<span><p>Detecting out of policy speech (OOPS) content is important but difficult.
While machine learning is a powerful tool to tackle this challenging task, it
is hard to break the performance ceiling due to factors like quantity and
quality limitations on training data and inconsistencies in OOPS definition and
data labeling. To realize the full potential of available limited resources, we
propose a meta learning technique (MLT) that combines individual models built
with different text representations. We analytically show that the resulting
technique is numerically stable and produces reasonable combining weights. We
combine the MLT with a threshold-moving (TM) technique to further improve the
performance of the combined predictor on highly-imbalanced in-distribution and
out-of-distribution datasets. We also provide computational results to show the
statistically significant advantages of the proposed MLT approach.
</p>
<p>All authors contributed equally to this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14356">
<div class="article-summary-box-inner">
<span><p>Computer vision often treats perception as objective, and this assumption
gets reflected in the way that datasets are collected and models are trained.
For instance, image descriptions in different languages are typically assumed
to be translations of the same semantic content. However, work in
cross-cultural psychology and linguistics has shown that individuals differ in
their visual perception depending on their cultural background and the language
they speak. In this paper, we demonstrate significant differences in semantic
content across languages in both dataset and model-produced captions. When data
is multilingual as opposed to monolingual, captions have higher semantic
coverage on average, as measured by scene graph, embedding, and linguistic
complexity. For example, multilingual captions have on average 21.8% more
objects, 24.5% more relations, and 27.1% more attributes than a set of
monolingual captions. Moreover, models trained on content from different
languages perform best against test data from those languages, while those
trained on multilingual content perform consistently well across all evaluation
data compositions. Our research provides implications for how diverse modes of
perception can improve image understanding.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-25 23:11:17.566271290 UTC">2023-10-25 23:11:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>