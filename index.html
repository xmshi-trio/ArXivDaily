<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-13T01:30:00Z">04-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05406">
<div class="article-summary-box-inner">
<span><p>We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Temporal Relation Extraction with ChatGPT. (arXiv:2304.05454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05454">
<div class="article-summary-box-inner">
<span><p>The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Resources and Methods for Natural Language Processing of Serbian Language. (arXiv:2304.05468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05468">
<div class="article-summary-box-inner">
<span><p>The Serbian language is a Slavic language spoken by over 12 million speakers
and well understood by over 15 million people. In the area of natural language
processing, it can be considered a low-resourced language. Also, Serbian is
considered a high-inflectional language. The combination of many word
inflections and low availability of language resources makes natural language
processing of Serbian challenging. Nevertheless, over the past three decades,
there have been a number of initiatives to develop resources and methods for
natural language processing of Serbian, ranging from developing a corpus of
free text from books and the internet, annotated corpora for classification and
named entity recognition tasks to various methods and models performing these
tasks. In this paper, we review the initiatives, resources, methods, and their
availability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User Adaptive Language Learning Chatbots with a Curriculum. (arXiv:2304.05489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05489">
<div class="article-summary-box-inner">
<span><p>Along with the development of systems for natural language understanding and
generation, dialog systems have been widely adopted for language learning and
practicing. Many current educational dialog systems perform chitchat, where the
generated content and vocabulary are not constrained. However, for learners in
a school setting, practice through dialog is more effective if it aligns with
students' curriculum and focuses on textbook vocabulary. Therefore, we adapt
lexically constrained decoding to a dialog system, which urges the dialog
system to include curriculum-aligned words and phrases in its generated
utterances. We adopt a generative dialog system, BlenderBot3, as our backbone
model and evaluate our curriculum-based dialog system with middle school
students learning English as their second language. The constrained words and
phrases are derived from their textbooks, suggested by their English teachers.
The evaluation result demonstrates that the dialog system with curriculum
infusion improves students' understanding of target words and increases their
interest in practicing English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">chatIPCC: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05510">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made significant progress in recent years,
achieving remarkable results in question-answering tasks (QA). However, they
still face two major challenges: hallucination and outdated information after
the training phase. These challenges take center stage in critical domains like
climate change, where obtaining accurate and up-to-date information from
reliable sources in a limited time is essential and difficult. To overcome
these barriers, one potential solution is to provide LLMs with access to
external, scientifically accurate, and robust sources (long-term memory) to
continuously update their knowledge and prevent the propagation of inaccurate,
incorrect, or outdated information. In this study, we enhanced GPT-4 by
integrating the information from the Sixth Assessment Report of the
Intergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable
source in this domain. We present our conversational AI prototype, available at
www.chatclimate.ai/ipcc and demonstrate its ability to answer challenging
questions accurately in three different QA scenarios: asking from 1) GPT-4, 2)
chatIPCC, and 3) hybrid chatIPCC. The answers and their sources were evaluated
by our team of IPCC authors, who used their expert knowledge to score the
accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation
showed that the hybrid chatIPCC provided more accurate answers, highlighting
the effectiveness of our solution. This approach can be easily scaled for
chatbots in specific domains, enabling the delivery of reliable and accurate
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematical and Linguistic Characterization of Orhan Pamuk's Nobel Works. (arXiv:2304.05512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05512">
<div class="article-summary-box-inner">
<span><p>In this study, Nobel Laureate Orhan Pamuk's works are chosen as examples of
Turkish literature. By counting the number of letters and words in his texts,
we find it possible to study his works statistically. It has been known that
there is a geometrical order in text structures. Here the method based on the
basic assumption of fractal geometry is introduced for calculating the fractal
dimensions of Pamuk's texts. The results are compared with the applications of
Zipf's law, which is successfully applied for letters and words, where two
concepts, namely Zipf's dimension and Zipf's order, are introduced. The Zipf
dimension of the novel My Name is Red is found to be much different than his
other novels. However, it is linguistically observed that there is no
fundamental difference between his corpora. The results are interpreted in
terms of fractal dimensions and the Turkish language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoMo: A shared encoder Model for text, image and multi-Modal representations. (arXiv:2304.05523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05523">
<div class="article-summary-box-inner">
<span><p>We propose a self-supervised shared encoder model that achieves strong
results on several visual, language and multimodal benchmarks while being data,
memory and run-time efficient. We make three key contributions. First, in
contrast to most existing works, we use a single transformer with all the
encoder layers processing both the text and the image modalities. Second, we
propose a stage-wise training strategy where the model is first trained on
images, then jointly with unimodal text and image datasets and finally jointly
with text and text-image datasets. Third, to preserve information across both
the modalities, we propose a training pipeline that learns simultaneously from
gradient updates of different modalities at each training update step. The
results on downstream text-only, image-only and multimodal tasks show that our
model is competitive with several strong models while using fewer parameters
and lesser pre-training data. For example, MoMo performs competitively with
FLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasks
despite having 2/5th the number of parameters and using 1/3rd the image-text
training pairs. Finally, we ablate various design choices and further show that
increasing model size produces significant performance gains indicating
potential for substantial improvements with larger models using our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Causality with Large Language Models: Feasibility and Opportunities. (arXiv:2304.05524v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05524">
<div class="article-summary-box-inner">
<span><p>We assess the ability of large language models (LLMs) to answer causal
questions by analyzing their strengths and weaknesses against three types of
causal question. We believe that current LLMs can answer causal questions with
existing causal knowledge as combined domain experts. However, they are not yet
able to provide satisfactory answers for discovering new knowledge or for
high-stakes decision-making tasks with high precision. We discuss possible
future directions and opportunities, such as enabling explicit and implicit
causal modules as well as deep causal-aware LLMs. These will not only enable
LLMs to answer many different types of causal questions for greater impact but
also enable LLMs to be more trustworthy and efficient in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05534">
<div class="article-summary-box-inner">
<span><p>Text-generative artificial intelligence (AI), including ChatGPT, equipped
with GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention
worldwide. In this study, first, we compared Japanese stylometric features
generated by GPT (-3.5 and -4) and those written by humans. In this work, we
performed multi-dimensional scaling (MDS) to confirm the classification of 216
texts into three classes (72 academic papers written by 36 single authors, 72
texts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the
titles of the aforementioned papers) focusing on the following stylometric
features: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle
words, (3) positioning of commas, and (4) rate of function words. MDS revealed
distinct distributions at each stylometric feature of GPT (-3.5 and -4) and
human. Although GPT-4 is more powerful than GPT-3.5 because it has more
parameters, both GPT (-3.5 and -4) distributions are likely to overlap. These
results indicate that although the number of parameters may increase in the
future, AI-generated texts may not be close to that written by humans in terms
of stylometric features. Second, we verified the classification performance of
random forest (RF) for two classes (GPT and human) focusing on Japanese
stylometric features. This study revealed the high performance of RF in each
stylometric feature. Furthermore, the RF classifier focusing on the rate of
function words achieved 98.1% accuracy. The RF classifier focusing on all
stylometric features reached 100% in terms of all performance indexes
(accuracy, recall, precision, and F1 score). This study concluded that at this
stage we human discriminate ChatGPT from human limited to Japanese language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05578">
<div class="article-summary-box-inner">
<span><p>Dialogue Acts (DAs) can be used to explain what expert tutors do and what
students know during the tutoring process. Most empirical studies adopt the
random sampling method to obtain sentence samples for manual annotation of DAs,
which are then used to train DA classifiers. However, these studies have paid
little attention to sample informativeness, which can reflect the information
quantity of the selected samples and inform the extent to which a classifier
can learn patterns. Notably, the informativeness level may vary among the
samples and the classifier might only need a small amount of low informative
samples to learn the patterns. Random sampling may overlook sample
informativeness, which consumes human labelling costs and contributes less to
training the classifiers. As an alternative, researchers suggest employing
statistical sampling methods of Active Learning (AL) to identify the
informative samples for training the classifiers. However, the use of AL
methods in educational DA classification tasks is under-explored. In this
paper, we examine the informativeness of annotated sentence samples. Then, the
study investigates how the AL methods can select informative samples to support
DA classifiers in the AL sampling process. The results reveal that most
annotated sentences present low informativeness in the training dataset and the
patterns of these sentences can be easily captured by the DA classifier. We
also demonstrate how AL methods can reduce the cost of manual annotation in the
AL sampling process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05591">
<div class="article-summary-box-inner">
<span><p>This study evaluates the potential of a large language model for aiding in
generation of semantic feature norms - a critical tool for evaluating
conceptual structure in cognitive science. Building from an existing
human-generated dataset, we show that machine-verified norms capture aspects of
conceptual structure beyond what is expressed in human norms alone, and better
explain human judgments of semantic similarity amongst items that are distally
related. The results suggest that LLMs can greatly enhance traditional methods
of semantic feature norm verification, with implications for our understanding
of conceptual representation in humans and machines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. (arXiv:2304.05613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05613">
<div class="article-summary-box-inner">
<span><p>Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Prompt Cell: A Portable Control Module for Effective Prompt. (arXiv:2304.05642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05642">
<div class="article-summary-box-inner">
<span><p>As a novel approach to tuning pre-trained models, prompt tuning involves
freezing the parameters in downstream tasks while inserting trainable
embeddings into inputs in the first layer.However,previous methods have mainly
focused on the initialization of prompt embeddings. The question of how to
train and utilize prompt embeddings in a reasonable way has become aa limiting
factor in the effectiveness of prompt tuning. To address this issue, we
introduce the Global Prompt Cell (GPC), a portable control module for prompt
tuning that selectively preserves prompt information across all encoder layers.
Our experimental results demonstrate a 5.8% improvement on SuperGLUE datasets
compared to vanilla prompt tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Normative and Descriptive Biases in Language Models Using Census Data. (arXiv:2304.05764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05764">
<div class="article-summary-box-inner">
<span><p>We investigate in this paper how distributions of occupations with respect to
gender is reflected in pre-trained language models. Such distributions are not
always aligned to normative ideals, nor do they necessarily reflect a
descriptive assessment of reality. In this paper, we introduce an approach for
measuring to what degree pre-trained language models are aligned to normative
and descriptive occupational distributions. To this end, we use official
demographic information about gender--occupation distributions provided by the
national statistics agencies of France, Norway, United Kingdom, and the United
States. We manually generate template-based sentences combining gendered
pronouns and nouns with occupations, and subsequently probe a selection of ten
language models covering the English, French, and Norwegian languages. The
scoring system we introduce in this work is language independent, and can be
used on any combination of template-based sentences, occupations, and
languages. The approach could also be extended to other dimensions of national
census data and other demographic variables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05783">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been known to perpetuate biases from the
underlying datasets to downstream tasks. However, these findings are
predominantly based on monolingual language models for English, whereas there
are few investigative studies of biases encoded in language models for
languages beyond English. In this paper, we fill this gap by analysing gender
bias in West Slavic language models. We introduce the first template-based
dataset in Czech, Polish, and Slovak for measuring gender bias towards male,
female and non-binary subjects. We complete the sentences using both mono- and
multilingual language models and assess their suitability for the masked
language modelling objective. Next, we measure gender bias encoded in West
Slavic language models by quantifying the toxicity and genderness of the
generated words. We find that these language models produce hurtful completions
that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and
Polish language models produce more hurtful completions with men as subjects,
which, upon inspection, we find is due to completions being related to
violence, death, and sickness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Dense Retrieval's Few-Shot Ability. (arXiv:2304.05845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05845">
<div class="article-summary-box-inner">
<span><p>Few-shot dense retrieval (DR) aims to effectively generalize to novel search
scenarios by learning a few samples. Despite its importance, there is little
study on specialized datasets and standardized evaluation protocols. As a
result, current methods often resort to random sampling from supervised
datasets to create "few-data" setups and employ inconsistent training
strategies during evaluations, which poses a challenge in accurately comparing
recent progress. In this paper, we propose a customized FewDR dataset and a
unified evaluation benchmark. Specifically, FewDR employs class-wise sampling
to establish a standardized "few-shot" setting with finely-defined classes,
reducing variability in multiple sampling rounds. Moreover, the dataset is
disjointed into base and novel classes, allowing DR models to be continuously
trained on ample data from base classes and a few samples in novel classes.
This benchmark eliminates the risk of novel class leakage, providing a reliable
estimation of the DR model's few-shot ability. Our extensive empirical results
reveal that current state-of-the-art DR models still face challenges in the
standard few-shot scene. Our code and data will be open-sourced at
https://github.com/OpenMatch/ANCE-Tele.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05860">
<div class="article-summary-box-inner">
<span><p>Homographs, words with the same spelling but different meanings, remain
challenging in Neural Machine Translation (NMT). While recent works leverage
various word embedding approaches to differentiate word sense in NMT, they do
not focus on the pivotal components in resolving ambiguities of homographs in
NMT: the hidden states of an encoder. In this paper, we propose a novel
approach to tackle homographic issues of NMT in the latent space. We first
train an encoder (aka "HDR-encoder") to learn universal sentence
representations in a natural language inference (NLI) task. We further
fine-tune the encoder using homograph-based synset sentences from WordNet,
enabling it to learn word-level homographic disambiguation representations
(HDR). The pre-trained HDR-encoder is subsequently integrated with a
transformer-based NMT in various schemes to improve translation accuracy.
Experiments on four translation directions demonstrate the effectiveness of the
proposed method in enhancing the performance of NMT systems in the BLEU scores
(up to +2.3 compared to a solid baseline). The effects can be verified by other
metrics (F1, precision, and recall) of translation accuracy in an additional
disambiguation task. Visualization methods like heatmaps, T-SNE and translation
examples are also utilized to demonstrate the effects of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research. (arXiv:2304.05929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05929">
<div class="article-summary-box-inner">
<span><p>Rehabilitation research focuses on determining the components of a treatment
intervention, the mechanism of how these components lead to recovery and
rehabilitation, and ultimately the optimal intervention strategies to maximize
patients' physical, psychologic, and social functioning. Traditional randomized
clinical trials that study and establish new interventions face several
challenges, such as high cost and time commitment. Observational studies that
use existing clinical data to observe the effect of an intervention have shown
several advantages over RCTs. Electronic Health Records (EHRs) have become an
increasingly important resource for conducting observational studies. To
support these studies, we developed a clinical research datamart, called
ReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch),
that transforms the rehabilitation-related EHR data collected from the UPMC
health care system to the Observational Health Data Sciences and Informatics
(OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model
(CDM) to facilitate rehabilitation research. The standardized EHR data stored
in ReDWINE will further reduce the time and effort required by investigators to
pool, harmonize, clean, and analyze data from multiple sources, leading to more
robust and comprehensive research findings. ReDWINE also includes deployment of
data visualization and data analytics tools to facilitate cohort definition and
clinical data analysis. These include among others the Open Health Natural
Language Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide
text analytical capabilities at scale in ReDWINE. Using this comprehensive
representation of patient data in ReDWINE for rehabilitation research will
facilitate real-world evidence for health interventions and outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. (arXiv:2304.05934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05934">
<div class="article-summary-box-inner">
<span><p>Sign languages are used as a primary language by approximately 70 million
D/deaf people world-wide. However, most communication technologies operate in
spoken and written languages, creating inequities in access. To help tackle
this problem, we release ASL Citizen, the largest Isolated Sign Language
Recognition (ISLR) dataset to date, collected with consent and containing
83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety of
environments. We propose that this dataset be used for sign language dictionary
retrieval for American Sign Language (ASL), where a user demonstrates a sign to
their own webcam with the aim of retrieving matching signs from a dictionary.
We show that training supervised machine learning classifiers with our dataset
greatly advances the state-of-the-art on metrics relevant for dictionary
retrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%,
evaluated entirely on videos of users who are not present in the training or
validation sets. An accessible PDF of this article is available at
https://aashakadesai.github.io/research/ASL_Dataset__arxiv_.pdf
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Angler: Helping Machine Translation Practitioners Prioritize Model Improvements. (arXiv:2304.05967v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05967">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) models can fail in unexpected ways in the real world,
but not all model failures are equal. With finite time and resources, ML
practitioners are forced to prioritize their model debugging and improvement
efforts. Through interviews with 13 ML practitioners at Apple, we found that
practitioners construct small targeted test sets to estimate an error's nature,
scope, and impact on users. We built on this insight in a case study with
machine translation models, and developed Angler, an interactive visual
analytics tool to help practitioners prioritize model improvements. In a user
study with 7 machine translation experts, we used Angler to understand
prioritization practices when the input space is infinite, and obtaining
reliable signals of model quality is expensive. Our study revealed that
participants could form more interesting and user-focused hypotheses for
prioritization by analyzing quantitative summary statistics and qualitatively
assessing data by reading sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosted Prompt Ensembles for Large Language Models. (arXiv:2304.05970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05970">
<div class="article-summary-box-inner">
<span><p>Methods such as chain-of-thought prompting and self-consistency have pushed
the frontier of language model reasoning performance with no additional
training. To further improve performance, we propose a prompt ensembling method
for large language models, which uses a small dataset to construct a set of few
shot prompts that together comprise a ``boosted prompt ensemble''. The few shot
examples for each prompt are chosen in a stepwise fashion to be ``hard''
examples on which the previous step's ensemble is uncertain. We show that this
outperforms single-prompt output-space ensembles and bagged prompt-space
ensembles on the GSM8k and AQuA datasets, among others. We propose both
train-time and test-time versions of boosted prompting that use different
levels of available annotation and conduct a detailed empirical study of our
algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting. (arXiv:2304.05973v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05973">
<div class="article-summary-box-inner">
<span><p>Medical decision-making processes can be enhanced by comprehensive biomedical
knowledge bases, which require fusing knowledge graphs constructed from
different sources via a uniform index system. The index system often organizes
biomedical terms in a hierarchy to provide the aligned entities with
fine-grained granularity. To address the challenge of scarce supervision in the
biomedical knowledge fusion (BKF) task, researchers have proposed various
unsupervised methods. However, these methods heavily rely on ad-hoc lexical and
structural matching algorithms, which fail to capture the rich semantics
conveyed by biomedical entities and terms. Recently, neural embedding models
have proved effective in semantic-rich tasks, but they rely on sufficient
labeled data to be adequately trained. To bridge the gap between the
scarce-labeled BKF and neural embedding models, we propose HiPrompt, a
supervision-efficient knowledge fusion framework that elicits the few-shot
reasoning ability of large language models through hierarchy-oriented prompts.
Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the
effectiveness of HiPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10050">
<div class="article-summary-box-inner">
<span><p>We present a combined scaling method - named BASIC - that achieves 85.7%
top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from
any labeled ImageNet example. This accuracy surpasses best published similar
models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant
improvements in robustness benchmarks. For instance, on 5 test sets with
natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our
model achieves 84.3% top-1 average accuracy, only a small drop from its
original ImageNet accuracy. To achieve these results, we scale up the
contrastive learning framework of CLIP and ALIGN in three dimensions: data
size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs,
which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has
3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than
ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and
4x more than ALIGN. We encountered two main challenges with the scaling rules
of BASIC. First, the main challenge with implementing the combined scaling
rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To
overcome the memory limit, we propose two simple methods which make use of
gradient checkpointing and model parallelism. Second, while increasing the
dataset size and the model size has been the defacto method to improve the
performance of deep learning models like BASIC, the effect of a large
contrastive batch size on such contrastive-trained image-text models is not
well-understood. To shed light on the benefits of large contrastive batch
sizes, we develop a theoretical framework which shows that larger contrastive
batch sizes lead to smaller generalization gaps for image-text models such as
BASIC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. (arXiv:2204.14211v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14211">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) become outdated as the world changes; they often fail
to perform tasks requiring recent factual information which was absent or
different during training, a phenomenon called temporal misalignment. This is
especially a challenging problem because the research community still lacks a
coherent dataset for assessing the adaptability of LMs to frequently-updated
knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a
lifelong benchmark for ever-evolving LMs that utilizes the difference between
consecutive snapshots of English Wikipedia and English Wikidata for training
and evaluation, respectively. The benchmark hence allows researchers to
periodically track an LM's ability to retain previous knowledge and acquire
updated/new knowledge at each point in time. We also find that training an LM
on the diff data through continual learning methods achieves similar or better
perplexity than on the entire snapshot in our benchmark with 12 times less
computational cost, which verifies that factual knowledge in LMs can be safely
updated with minimal training data via continual learning. The dataset and the
code are available at https://github.com/joeljang/temporalwiki.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12428">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a prominent neural model compression technique
that heavily relies on teacher network predictions to guide the training of a
student model. Considering the ever-growing size of pre-trained language models
(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is
evident that in KD, deploying the teacher network during training adds to the
memory and computational requirements of training. In the computer vision
literature, the necessity of the teacher network is put under scrutiny by
showing that KD is a label regularization technique that can be replaced with
lighter teacher-free variants such as the label-smoothing technique. However,
to the best of our knowledge, this issue is not investigated in NLP. Therefore,
this work concerns studying different label regularization techniques and
whether we actually need them to improve the fine-tuning of smaller PLM
networks on downstream tasks. In this regard, we did a comprehensive set of
experiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600
distinct trials and ran each configuration five times. This investigation led
to a surprising observation that KD and other label regularization techniques
do not play any meaningful role over regular fine-tuning when the student model
is pre-trained. We further explore this phenomenon in different settings of NLP
and computer vision tasks and demonstrate that pre-training itself acts as a
kind of regularization, and additional label regularization is unnecessary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Diversity, Equity and Inclusion of NLP Technology: A Case Study for Indian Languages. (arXiv:2205.12676v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12676">
<div class="article-summary-box-inner">
<span><p>In order for NLP technology to be widely applicable, fair, and useful, it
needs to serve a diverse set of speakers across the world's languages, be
equitable, i.e., not unduly biased towards any particular language, and be
inclusive of all users, particularly in low-resource settings where compute
constraints are common. In this paper, we propose an evaluation paradigm that
assesses NLP technologies across all three dimensions. While diversity and
inclusion have received attention in recent literature, equity is currently
unexplored. We propose to address this gap using the Gini coefficient, a
well-established metric used for estimating societal wealth inequality. Using
our paradigm, we highlight the distressed state of current technologies for
Indian (IN) languages (a linguistically large and diverse set, with a varied
speaker population), across all three dimensions. To improve upon these
metrics, we demonstrate the importance of region-specific choices in model
building and dataset creation, and more importantly, propose a novel,
generalisable approach to optimal resource allocation during fine-tuning.
Finally, we discuss steps to mitigate these biases and encourage the community
to employ multi-faceted evaluation when building linguistically diverse and
equitable technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages. (arXiv:2205.15960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15960">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Innovations in Neural Data-to-text Generation: A Survey. (arXiv:2207.12571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12571">
<div class="article-summary-box-inner">
<span><p>The neural boom that has sparked natural language processing (NLP) research
through the last decade has similarly led to significant innovations in
data-to-text generation (DTG). This survey offers a consolidated view into the
neural DTG paradigm with a structured examination of the approaches, benchmark
datasets, and evaluation protocols. This survey draws boundaries separating DTG
from the rest of the natural language generation (NLG) landscape, encompassing
an up-to-date synthesis of the literature, and highlighting the stages of
technological adoption from within and outside the greater NLG umbrella. With
this holistic view, we highlight promising avenues for DTG research that not
only focus on the design of linguistically capable systems but also systems
that exhibit fairness and accountability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology. (arXiv:2208.05140v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.05140">
<div class="article-summary-box-inner">
<span><p>Oversight AI is an emerging concept in radiology where the AI forms a
symbiosis with radiologists by continuously supporting radiologists in their
decision-making. Recent advances in vision-language models sheds a light on the
long-standing problems of the oversight AI by the understanding both visual and
textual concepts and their semantic correspondences. However, there have been
limited successes in the application of vision-language models in the medical
domain, as the current vision-language models and learning strategies for
photographic images and captions call for the web-scale data corpus of image
and text pairs which was not often feasible in the medical domain. To address
this, here we present a model dubbed Medical Cross-attention Vision-Language
model (Medical X-VL), leveraging the key components to be tailored for the
medical domain. Our medical X-VL model is based on the following components:
self-supervised uni-modal models in medical domain and fusion encoder to bridge
them, momentum distillation, sentence-wise contrastive learning for medical
reports, and the sentence similarity-adjusted hard negative mining. We
experimentally demonstrated that our model enables various zero-shot tasks for
oversight AI, ranging from the zero-shot classification to zero-shot error
correction. Our model outperformed the current state-of-the-art models in two
different medical image database, suggesting the novel clinical usage of our
oversight AI model for monitoring human errors. Our method was especially
successful in the data-limited setting, which is frequently encountered in the
clinics, suggesting the potential widespread applicability in medical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spillover of Antisocial Behavior from Fringe Platforms: The Unintended Consequences of Community Banning. (arXiv:2209.09803v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09803">
<div class="article-summary-box-inner">
<span><p>Online platforms face pressure to keep their communities civil and
respectful. Thus, the bannings of problematic online communities from
mainstream platforms like Reddit and Facebook are often met with enthusiastic
public reactions. However, this policy can lead users to migrate to alternative
fringe platforms with lower moderation standards and where antisocial behaviors
like trolling and harassment are widely accepted. As users of these communities
often remain co-active across mainstream and fringe platforms, antisocial
behaviors may spill over onto the mainstream platform. We study this possible
spillover by analyzing around 70,000 users from three banned communities that
migrated to fringe platforms: r/The_Donald, r/GenderCritical, and r/Incels.
Using a difference-in-differences design, we contrast co-active users with
matched counterparts to estimate the causal effect of fringe platform
participation on users' antisocial behavior on Reddit. Our results show that
participating in the fringe communities increases users' toxicity on Reddit (as
measured by Perspective API) and involvement with subreddits similar to the
banned community -- which often also breach platform norms. The effect
intensifies with time and exposure to the fringe platform. In short, we find
evidence for a spillover of antisocial behavior from fringe platforms onto
Reddit via co-participation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposed Prompting: A Modular Approach for Solving Complex Tasks. (arXiv:2210.02406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02406">
<div class="article-summary-box-inner">
<span><p>Few-shot prompting is a surprisingly powerful way to use Large Language
Models (LLMs) to solve various tasks. However, this approach struggles as the
task complexity increases or when the individual reasoning steps of the task
themselves are hard to learn, especially when embedded in more complex tasks.
To address this, we propose Decomposed Prompting, a new approach to solve
complex tasks by decomposing them (via prompting) into simpler sub-tasks that
can be delegated to a library of prompting-based LLMs dedicated to these
sub-tasks. This modular structure allows each prompt to be optimized for its
specific sub-task, further decomposed if necessary, and even easily replaced
with more effective prompts, trained models, or symbolic functions if desired.
We show that the flexibility and modularity of Decomposed Prompting allows it
to outperform prior work on few-shot prompting using GPT3. On symbolic
reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into
even simpler solvable sub-tasks. When the complexity comes from the input
length, we can recursively decompose the task into the same task but with
smaller inputs. We also evaluate our approach on textual multi-step reasoning
tasks: on long-context multi-hop QA task, we can more effectively teach the
sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,
we can incorporate a symbolic information retrieval within our decomposition
framework, leading to improved performance on both tasks. Datasets, Code and
Prompts available at https://github.com/allenai/DecomP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LittleBird: Efficient Faster & Longer Transformer for Question Answering. (arXiv:2210.11870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11870">
<div class="article-summary-box-inner">
<span><p>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a
limitation dealing with long inputs due to its attention mechanism. Longformer,
ETC and BigBird addressed this issue and effectively solved the quadratic
dependency problem. However we find that these models are not sufficient, and
propose LittleBird, a novel model based on BigBird with improved speed and
memory footprint while maintaining accuracy. In particular, we devise a more
flexible and efficient position representation method based on Attention with
Linear Biases (ALiBi). We also show that replacing the method of global
information represented in the BigBird with pack and unpack attention is more
effective. The proposed model can work on long inputs even after being
pre-trained on short inputs, and can be trained efficiently reusing existing
pre-trained language model for short inputs. This is a significant benefit for
low-resource languages where large amounts of long text data are difficult to
obtain. As a result, our experiments show that LittleBird works very well in a
variety of languages, achieving high performance in question answering tasks,
particularly in KorQuAD2.0, Korean Question Answering Dataset for long
paragraphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gendered Mental Health Stigma in Masked Language Models. (arXiv:2210.15144v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15144">
<div class="article-summary-box-inner">
<span><p>Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05853">
<div class="article-summary-box-inner">
<span><p>While large pretrained language models (PLMs) demonstrate incredible fluency
and performance on many natural language tasks, recent work has shown that
well-performing PLMs are very sensitive to what prompts are feed into them.
Even when prompts are semantically identical, language models may give very
different answers. When considering safe and trustworthy deployments of PLMs we
would like their outputs to be consistent under prompts that mean the same
thing or convey the same intent. While some work has looked into how
state-of-the-art PLMs address this need, they have been limited to only
evaluating lexical equality of single- or multi-word answers and do not address
consistency of generative text sequences. In order to understand consistency of
PLMs under text generation settings, we develop a measure of semantic
consistency that allows the comparison of open-ended text outputs. We implement
several versions of this consistency metric to evaluate the performance of a
number of PLMs on paraphrased versions of questions in the TruthfulQA dataset,
we find that our proposed metrics are considerably more consistent than
traditional metrics embodying lexical consistency, and also correlate with
human evaluation of output consistency to a higher degree.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02908">
<div class="article-summary-box-inner">
<span><p>Autonomous cars are indispensable when humans go further down the hands-free
route. Although existing literature highlights that the acceptance of the
autonomous car will increase if it drives in a human-like manner, sparse
research offers the naturalistic experience from a passenger's seat perspective
to examine the human likeness of current autonomous cars. The present study
tested whether the AI driver could create a human-like ride experience for
passengers based on 69 participants' feedback in a real-road scenario. We
designed a ride experience-based version of the non-verbal Turing test for
automated driving. Participants rode in autonomous cars (driven by either human
or AI drivers) as a passenger and judged whether the driver was human or AI.
The AI driver failed to pass our test because passengers detected the AI driver
above chance. In contrast, when the human driver drove the car, the passengers'
judgement was around chance. We further investigated how human passengers
ascribe humanness in our test. Based on Lewin's field theory, we advanced a
computational model combining signal detection theory with pre-trained language
models to predict passengers' humanness rating behaviour. We employed affective
transition between pre-study baseline emotions and corresponding post-stage
emotions as the signal strength of our model. Results showed that the
passengers' ascription of humanness would increase with the greater affective
transition. Our study suggested an important role of affective transition in
passengers' ascription of humanness, which might become a future direction for
autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05762">
<div class="article-summary-box-inner">
<span><p>Existing pre-training methods for extractive Question Answering (QA) generate
cloze-like queries different from natural questions in syntax structure, which
could overfit pre-trained models to simple keyword matching. In order to
address this problem, we propose a novel Momentum Contrastive pRe-training fOr
queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS
introduces a momentum contrastive learning framework to align the answer
probability between cloze-like and natural query-passage sample pairs. Hence,
the pre-trained models can better transfer the knowledge learned in cloze-like
samples to answering natural questions. Experimental results on three
benchmarking QA datasets show that our method achieves noticeable improvement
compared with all baselines in both supervised and zero-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03241">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have been instrumental for the rapid advance of natural
language processing. This paper studies continual pre-training of LMs, in
particular, continual domain-adaptive pre-training (or continual DAP-training).
Existing research has shown that further pre-training an LM using a domain
corpus to adapt the LM to the domain can improve the end-task performance in
the domain. This paper proposes a novel method to continually DAP-train an LM
with a sequence of unlabeled domain corpora to adapt the LM to these domains to
improve their end-task performances. The key novelty of our method is a
soft-masking mechanism that directly controls the update to the LM. A novel
proxy is also proposed to preserve the general knowledge in the original LM.
Additionally, it contrasts the representations of the previously learned domain
knowledge (including the general knowledge in the pre-trained LM) and the
knowledge from the current full network to achieve knowledge integration. The
method not only overcomes catastrophic forgetting, but also achieves knowledge
transfer to improve end-task performances. Empirical evaluation demonstrates
the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07731">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative models such as GPT may be used to fabricate
indistinguishable fake customer reviews at a much lower cost, thus posing
challenges for social media platforms to detect these machine-generated fake
reviews. We propose to leverage the high-quality elite restaurant reviews
verified by Yelp to generate fake reviews from the OpenAI GPT review creator
and ultimately fine-tune a GPT output detector to predict fake reviews that
significantly outperform existing solutions. We further apply the model to
predict non-elite reviews and identify the patterns across several dimensions,
such as review, user and restaurant characteristics, and writing style. We show
that social media platforms are continuously challenged by machine-generated
fake reviews, although they may implement detection systems to filter out
suspicious reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11752">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a challenging task of natural language
processing (NLP) and computer vision (CV), attracting significant attention
from researchers. English is a resource-rich language that has witnessed
various developments in datasets and models for visual question answering.
Visual question answering in other languages also would be developed for
resources and models. In addition, there is no multilingual dataset targeting
the visual content of a particular country with its own objects and cultural
characteristics. To address the weakness, we provide the research community
with a benchmark dataset named EVJVQA, including 33,000+ pairs of
question-answer over three languages: Vietnamese, English, and Japanese, on
approximately 5,000 images taken from Vietnam for evaluating multilingual VQA
systems or models. EVJVQA is used as a benchmark dataset for the challenge of
multilingual visual question answering at the 9th Workshop on Vietnamese
Language and Speech Processing (VLSP 2022). This task attracted 62 participant
teams from various universities and organizations. In this article, we present
details of the organization of the challenge, an overview of the methods
employed by shared-task participants, and the results. The highest performances
are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The
multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained
vision model and mT5 for the pre-trained language model, a powerful pre-trained
language model based on the transformer architecture. EVJVQA is a challenging
dataset that motivates NLP and CV researchers to further explore the
multilingual models or systems for visual question answering systems. We
released the challenge on the Codalab evaluation system for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prototypical Semantic Decoupling Method via Joint Contrastive Learning for Few-Shot Name Entity Recognition. (arXiv:2302.13610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13610">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) aims at identifying named entities
based on only few labeled instances. Most existing prototype-based sequence
labeling models tend to memorize entity mentions which would be easily confused
by close prototypes. In this paper, we proposed a Prototypical Semantic
Decoupling method via joint Contrastive learning (PSDC) for few-shot NER.
Specifically, we decouple class-specific prototypes and contextual semantic
prototypes by two masking strategies to lead the model to focus on two
different semantic information for inference. Besides, we further introduce
joint contrastive learning objectives to better integrate two kinds of
decoupling information and prevent semantic collapse. Experimental results on
two few-shot NER benchmarks demonstrate that PSDC consistently outperforms the
previous SOTA methods in terms of overall performance. Extensive analysis
further validates the effectiveness and generalization of PSDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11436">
<div class="article-summary-box-inner">
<span><p>Cognitive psychology delves on understanding perception, attention, memory,
language, problem-solving, decision-making, and reasoning. Large language
models (LLMs) are emerging as potent tools increasingly capable of performing
human-level tasks. The recent development in the form of GPT-4 and its
demonstrated success in tasks complex to humans exam and complex problems has
led to an increased confidence in the LLMs to become perfect instruments of
intelligence. Although GPT-4 report has shown performance on some cognitive
psychology tasks, a comprehensive assessment of GPT-4, via the existing
well-established datasets is required. In this study, we focus on the
evaluation of GPT-4's performance on a set of cognitive psychology datasets
such as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how
GPT-4 processes and integrates cognitive psychology with contextual
information, providing insight into the underlying cognitive processes that
enable its ability to generate the responses. We show that GPT-4 exhibits a
high level of accuracy in cognitive psychology tasks relative to the prior
state-of-the-art models. Our results strengthen the already available
assessments and confidence on GPT-4's cognitive psychology abilities. It has
significant potential to revolutionize the field of AI, by enabling machines to
bridge the gap between human and machine reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12712">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13375">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01890">
<div class="article-summary-box-inner">
<span><p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for
the countries of Brazil, Germany, India and Kenya, to aid training and
interpretability of models. We demonstrate how our lexicon can be used to
interpret model predictions, showing that models developed to classify extreme
speech rely heavily on target words when making predictions. Further, we
propose a method to aid shot selection for training in low-resource settings
via HATELEXICON. In few-shot learning, the selection of shots is of paramount
importance to model performance. In our work, we simulate a few-shot setting
for German and Hindi, using HASOC data for training and the Multilingual
HateCheck (MHC) as a benchmark. We show that selecting shots based on our
lexicon leads to models performing better on MHC than models trained on shots
sampled randomly. Thus, when given only a few training examples, using our
lexicon to select shots containing more sociocultural information leads to
better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02213">
<div class="article-summary-box-inner">
<span><p>The amount of data has growing significance in exploring cutting-edge
materials and a number of datasets have been generated either by hand or
automated approaches. However, the materials science field struggles to
effectively utilize the abundance of data, especially in applied disciplines
where materials are evaluated based on device performance rather than their
properties. This article presents a new natural language processing (NLP) task
called structured information inference (SII) to address the complexities of
information extraction at the device level in materials science. We
accomplished this task by tuning GPT-3 on an existing perovskite solar cell
FAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%
F1-score and extended the dataset with data published since its release. The
produced data is formatted and normalized, enabling its direct utilization as
input in subsequent data analysis. This feature empowers materials scientists
to develop models by selecting high-quality review articles within their
domain. Additionally, we designed experiments to predict the electrical
performance of solar cells and design materials or devices with targeted
parameters using large language models (LLMs). Our results demonstrate
comparable performance to traditional machine learning methods without feature
selection, highlighting the potential of LLMs to acquire scientific knowledge
and design new materials akin to materials scientists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05265">
<div class="article-summary-box-inner">
<span><p>The recent large-scale generative modeling has attained unprecedented
performance especially in producing high-fidelity images driven by text
prompts. Text inversion (TI), alongside the text-to-image model backbones, is
proposed as an effective technique in personalizing the generation when the
prompts contain user-defined, unseen or long-tail concept tokens. Despite that,
we find and show that the deployment of TI remains full of "dark-magics" -- to
name a few, the harsh requirement of additional datasets, arduous human efforts
in the loop and lack of robustness. In this work, we propose a much-enhanced
version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all
the aforementioned problems and in turn delivering a robust, data-efficient and
easy-to-use framework. The core to COTI is a theoretically-guided loss
objective instantiated with a comprehensive and novel weighted scoring
mechanism, encapsulated by an active-learning paradigm. The extensive results
show that COTI significantly outperforms the prior TI-related approaches with a
26.05 decrease in the FID score and a 23.00% boost in the R-precision.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-13 23:11:40.216479538 UTC">2023-04-13 23:11:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>