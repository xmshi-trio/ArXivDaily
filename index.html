<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-13T01:30:00Z">10-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05675">
<div class="article-summary-box-inner">
<span><p>Transformer models can use two fundamentally different kinds of information:
information stored in weights during training, and information provided
``in-context'' at inference time. In this work, we show that transformers
exhibit different inductive biases in how they represent and generalize from
the information in these two sources. In particular, we characterize whether
they generalize via parsimonious rules (rule-based generalization) or via
direct comparison with observed examples (exemplar-based generalization). This
is of important practical consequence, as it informs whether to encode
information in weights or in context, depending on how we want models to use
that information. In transformers trained on controlled stimuli, we find that
generalization from weights is more rule-based whereas generalization from
context is largely exemplar-based. In contrast, we find that in transformers
pre-trained on natural language, in-context learning is significantly
rule-based, with larger models showing more rule-basedness. We hypothesise that
rule-based generalization from in-context information might be an emergent
consequence of large-scale training on language, which has sparse rule-like
structure. Using controlled stimuli, we verify that transformers pretrained on
data containing sparse rule-like structure exhibit more rule-based
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers. (arXiv:2210.05709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05709">
<div class="article-summary-box-inner">
<span><p>Multilingual transformer-based models demonstrate remarkable zero and
few-shot transfer across languages by learning and reusing language-agnostic
features. However, as a fixed-size model acquires more languages, its
performance across all languages degrades, a phenomenon termed interference.
Often attributed to limited model capacity, interference is commonly addressed
by adding additional parameters despite evidence that transformer-based models
are overparameterized. In this work, we show that it is possible to reduce
interference by instead identifying and pruning language-specific parameters.
First, we use Shapley Values, a credit allocation metric from coalitional game
theory, to identify attention heads that introduce interference. Then, we show
that removing identified attention heads from a fixed model improves
performance for a target language on both sentence classification and
structural prediction, seeing gains as large as 24.7\%. Finally, we provide
insights on language-agnostic and language-specific attention heads using
attention visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Language Maps for Robot Navigation. (arXiv:2210.05714v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05714">
<div class="article-summary-box-inner">
<span><p>Grounding language to the visual observations of a navigating agent can be
performed using off-the-shelf visual-language models pretrained on
Internet-scale data (e.g., image captions). While this is useful for matching
images to natural language descriptions of object goals, it remains disjoint
from the process of mapping the environment, so that it lacks the spatial
precision of classic geometric maps. To address this problem, we propose
VLMaps, a spatial map representation that directly fuses pretrained
visual-language features with a 3D reconstruction of the physical world. VLMaps
can be autonomously built from video feed on robots using standard exploration
approaches and enables natural language indexing of the map without additional
labeled data. Specifically, when combined with large language models (LLMs),
VLMaps can be used to (i) translate natural language commands into a sequence
of open-vocabulary navigation goals (which, beyond prior work, can be spatial
by construction, e.g., "in between the sofa and TV" or "three meters to the
right of the chair") directly localized in the map, and (ii) can be shared
among multiple robots with different embodiments to generate new obstacle maps
on-the-fly (by using a list of obstacle categories). Extensive experiments
carried out in simulated and real world environments show that VLMaps enable
navigation according to more complex language instructions than existing
methods. Videos are available at https:vlmaps.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Embeddings for Language Independent Stance Detection. (arXiv:2210.05715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05715">
<div class="article-summary-box-inner">
<span><p>The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating relational embeddings,
namely, dense vector representations of interaction pairs. Our method can be
applied to any language and target without any manual tuning. Our experiments
on seven publicly available datasets and four different languages show that
combining our relational embeddings with textual methods helps to substantially
improve performance, obtaining best results for six out of seven evaluation
settings, outperforming strong baselines based on large pre-trained language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Text Representations under Tight Annotation Budgets: Measuring Structural Alignment. (arXiv:2210.05721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05721">
<div class="article-summary-box-inner">
<span><p>Annotating large collections of textual data can be time consuming and
expensive. That is why the ability to train models with limited annotation
budgets is of great importance. In this context, it has been shown that under
tight annotation budgets the choice of data representation is key. The goal of
this paper is to better understand why this is so. With this goal in mind, we
propose a metric that measures the extent to which a given representation is
structurally aligned with a task. We conduct experiments on several text
classification datasets testing a variety of models and representations. Using
our proposed metric we show that an efficient representation for a task (i.e.
one that enables learning from few samples) is a representation that induces a
good alignment between latent input structure and class structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Improving Semantic Diversity of Dialogue Generation. (arXiv:2210.05725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05725">
<div class="article-summary-box-inner">
<span><p>Response diversity has become an important criterion for evaluating the
quality of open-domain dialogue generation models. However, current evaluation
metrics for response diversity often fail to capture the semantic diversity of
generated responses, as they mainly consider lexical aspects of the generated
responses. In this paper, we introduce a new automatic evaluation metric to
measure the semantic diversity of generated responses. Through human
evaluation, we demonstrate that our proposed metric captures human judgments on
response diversity better than existing lexical-level diversity metrics.
Furthermore, motivated by analyzing an existing dialogue dataset, we propose a
simple yet effective learning method that improves the semantic diversity of
generated responses. Our learning method weights training samples based on the
semantic distribution of the training set. We show that our learning method
improves response diversity and coherency better than other baseline methods
through automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition of Low-Resource Languages Based on Chukchi. (arXiv:2210.05726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05726">
<div class="article-summary-box-inner">
<span><p>The following paper presents a project focused on the research and creation
of a new Automatic Speech Recognition (ASR) based in the Chukchi language.
There is no one complete corpus of the Chukchi language, so most of the work
consisted in collecting audio and texts in the Chukchi language from open
sources and processing them. We managed to collect 21:34:23 hours of audio
recordings and 112,719 sentences (or 2,068,273 words) of text in the Chukchi
language. The XLSR model was trained on the obtained data, which showed good
results even with a small amount of data. Besides the fact that the Chukchi
language is a low-resource language, it is also polysynthetic, which
significantly complicates any automatic processing. Thus, the usual WER metric
for evaluating ASR becomes less indicative for a polysynthetic language.
However, the CER metric showed good results. The question of metrics for
polysynthetic languages remains open.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Punctuation for Long-form Dictation with Transformers. (arXiv:2210.05756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05756">
<div class="article-summary-box-inner">
<span><p>While speech recognition Word Error Rate (WER) has reached human parity for
English, long-form dictation scenarios still suffer from segmentation and
punctuation problems resulting from irregular pausing patterns or slow
speakers. Transformer sequence tagging models are effective at capturing long
bi-directional context, which is crucial for automatic punctuation. A typical
Automatic Speech Recognition (ASR) production system, however, is constrained
by real-time requirements, making it hard to incorporate the right context when
making punctuation decisions. In this paper, we propose a streaming approach
for punctuation or re-punctuation of ASR output using dynamic decoding windows
and measure its impact on punctuation and segmentation accuracy in a variety of
scenarios. The new system tackles over-segmentation issues, improving
segmentation F0.5-score by 13.9%. Streaming punctuation achieves an average
BLEU-score gain of 0.66 for the downstream task of Machine Translation (MT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Context Processing for Context Augmented Language Modeling. (arXiv:2210.05758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05758">
<div class="article-summary-box-inner">
<span><p>Language models can be augmented with a context retriever to incorporate
knowledge from large external databases. By leveraging retrieved context, the
neural network does not have to memorize the massive amount of world knowledge
within its internal parameters, leading to better parameter efficiency,
interpretability and modularity. In this paper we examined a simple yet
effective architecture for incorporating external context into language models
based on decoupled Encoder Decoder architecture. We showed that such a simple
architecture achieves competitive results on auto-regressive language modeling
and open domain question answering tasks. We also analyzed the behavior of the
proposed model which performs grounded context transfer. Finally we discussed
the computational implications of such retrieval augmented models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propagators of Disinformation on Twitter Using Quantitative Discursive Analysis. (arXiv:2210.05760v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05760">
<div class="article-summary-box-inner">
<span><p>Efforts by foreign actors to influence public opinion have gained
considerable attention because of their potential to impact democratic
elections. Thus, the ability to identify and counter sources of disinformation
is increasingly becoming a top priority for government entities in order to
protect the integrity of democratic processes. This study presents a method of
identifying Russian disinformation bots on Twitter using centering resonance
analysis and Clauset-Newman-Moore community detection. The data reflect a
significant degree of discursive dissimilarity between known Russian
disinformation bots and a control set of Twitter users during the timeframe of
the 2016 U.S. Presidential Election. The data also demonstrate statistically
significant classification capabilities (MCC = 0.9070) based on community
clustering. The prediction algorithm is very effective at identifying true
positives (bots), but is not able to resolve true negatives (non-bots) because
of the lack of discursive similarity between control users. This leads to a
highly sensitive means of identifying propagators of disinformation with a high
degree of discursive similarity on Twitter, with implications for limiting the
spread of disinformation that could impact democratic processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05769">
<div class="article-summary-box-inner">
<span><p>The development of state-of-the-art systems in different applied areas of
machine learning (ML) is driven by benchmarks, which have shaped the paradigm
of evaluating generalisation capabilities from multiple perspectives. Although
the paradigm is shifting towards more fine-grained evaluation across diverse
tasks, the delicate question of how to aggregate the performances has received
particular interest in the community. In general, benchmarks follow the
unspoken utilitarian principles, where the systems are ranked based on their
mean average score over task-specific metrics. Such aggregation procedure has
been viewed as a sub-optimal evaluation protocol, which may have created the
illusion of progress. This paper proposes Vote'n'Rank, a framework for ranking
systems in multi-task benchmarks under the principles of the social choice
theory. We demonstrate that our approach can be efficiently utilised to draw
new insights on benchmarking in several ML sub-fields and identify the
best-performing systems in research and development case studies. The
Vote'n'Rank's procedures are more robust than the mean average while being able
to handle missing performance scores and determine conditions under which the
system becomes the winner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying FrameNet to Chinese(Poetry). (arXiv:2210.05772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05772">
<div class="article-summary-box-inner">
<span><p>FrameNet( Fillmore and Baker [2009] ) is well-known for its wide use for
knowledge representation in the form of inheritance-based ontologies and
lexica( Trott et al. [2020] ). Although FrameNet is usually applied to
languages like English, Spanish and Italian, there are still plenty of FrameNet
data sets available for other languages like Chinese, which differs
significantly from those languages based on Latin alphabets. In this paper, the
translation from ancient Chinese Poetry to modern Chinese will be first
conducted to further apply the Chinese FrameNet(CFN, provided by Shanxi
University). Afterwards, the translation from modern Chinese will be conducted
as well for the comparison between the applications of CFN and English
FrameNet. Finally, the overall comparison will be draw between CFN to modern
Chinese and English FrameNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bil-DOS: A Bi-lingual Dialogue Ordering System (for Subway). (arXiv:2210.05773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05773">
<div class="article-summary-box-inner">
<span><p>Due to the unfamiliarity to particular words(or proper nouns) for
ingredients, non-native English speakers can be extremely confused about the
ordering process in restaurants like Subway. Thus, We developed a dialogue
system, which supports Chinese(Mandarin)1 and English2 at the same time. In
other words, users can switch arbitrarily between Chinese(Mandarin) and English
as the conversation is being conducted. This system is specifically designed
for Subway ordering3. In BilDOS, we designed a Discriminator module to tell the
language is being used in inputted user utterance, a Translator module to
translate used language into English if it is not English, and a Dialogue
Manager module to detect the intention within inputted user utterances, handle
outlier inputs by throwing clarification requests, map detected Intention and
detailed Keyword4 into a particular intention class, locate the current
ordering process, continue to give queries to finish the order, conclude the
order details once the order is completed, activate the evaluation process when
the conversation is done.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Speaker Identification Using Distant Supervision. (arXiv:2210.05780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05780">
<div class="article-summary-box-inner">
<span><p>Speaker identification, determining which character said each utterance in
literary text, benefits many downstream tasks. Most existing approaches use
expert-defined rules or rule-based features to directly approach this task, but
these approaches come with significant drawbacks, such as lack of contextual
reasoning and poor cross-lingual generalization. In this work, we propose a
speaker identification framework that addresses these issues. We first extract
large-scale distant supervision signals in English via general-purpose tools
and heuristics, and then apply these weakly-labeled instances with a focus on
encouraging contextual reasoning to train a cross-lingual language model. We
show that the resulting model outperforms previous state-of-the-art methods on
two English speaker identification benchmarks by up to 9% in accuracy and 5%
with only distant supervision, as well as two Chinese speaker identification
datasets by up to 4.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Deliberation for Multilingual ASR. (arXiv:2210.05785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05785">
<div class="article-summary-box-inner">
<span><p>Multilingual end-to-end automatic speech recognition models are attractive
due to its simplicity in training and deployment. Recent work on large-scale
training of such models has shown promising results compared to monolingual
models. However, the work often focuses on multilingual models themselves in a
single-pass setup. In this work, we investigate second-pass deliberation for
multilingual speech recognition. Our proposed deliberation is multilingual,
i.e., the text encoder encodes hypothesis text from multiple languages, and the
decoder attends to multilingual text and audio. We investigate scaling the
deliberation text encoder and decoder, and compare scaling the deliberation
decoder and the first-pass cascaded encoder. We show that deliberation improves
the average WER on 9 languages by 4% relative compared to the single-pass
model. By increasing the size of the deliberation up to 1B parameters, the
average WER improvement increases to 9%, with up to 14% for certain languages.
Our deliberation rescorer is based on transformer layers and can be
parallelized during rescoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment Analysis. (arXiv:2210.05790v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05790">
<div class="article-summary-box-inner">
<span><p>Most existing methods focus on sentiment analysis of textual data. However,
recently there has been a massive use of images and videos on social platforms,
motivating sentiment analysis from other modalities. Current studies show that
exploring other modalities (e.g., images) increases sentiment analysis
performance. State-of-the-art multimodal models, such as CLIP and VisualBERT,
are pre-trained on datasets with the text paired with images. Although the
results obtained by these models are promising, pre-training and sentiment
analysis fine-tuning tasks of these models are computationally expensive. This
paper introduces a transfer learning approach using joint fine-tuning for
sentiment analysis. Our proposal achieved competitive results using a more
straightforward alternative fine-tuning strategy that leverages different
pre-trained unimodal models and efficiently combines them in a multimodal
space. Moreover, our proposal allows flexibility when incorporating any
pre-trained model for texts and images during the joint fine-tuning stage,
being especially interesting for sentiment classification in low-resource
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Soft and Hard Target RNN-T Distillation for Large-scale ASR. (arXiv:2210.05793v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05793">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an effective machine learning technique to transfer
knowledge from a teacher model to a smaller student model, especially with
unlabeled data. In this paper, we focus on knowledge distillation for the RNN-T
model, which is widely used in state-of-the-art (SoTA) automatic speech
recognition (ASR). Specifically, we compared using soft and hard target
distillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight
public dataset (60k hours) and our in-house data (600k hours). We found that
hard tar-gets are more effective when the teacher and student have different
architecture, such as large teacher and small streaming student. On the other
hand, soft target distillation works better in self-training scenario like
iterative large teacher training. For a large model with0.6B weights, we
achieve a new SoTA word error rate (WER) on LibriSpeech (8% relative
improvement on dev-other) using Noisy Student Training with soft target
distillation. It also allows our production teacher to adapt new data domain
continuously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustify Transformers with Robust Kernel Density Estimation. (arXiv:2210.05794v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05794">
<div class="article-summary-box-inner">
<span><p>Recent advances in Transformer architecture have empowered its empirical
success in various tasks across different domains. However, existing works
mainly focus on improving the standard accuracy and computational cost, without
considering the robustness of contaminated samples. Existing work has shown
that the self-attention mechanism, which is the center of the Transformer
architecture, can be viewed as a non-parametric estimator based on the
well-known kernel density estimation (KDE). This motivates us to leverage the
robust kernel density estimation (RKDE) in the self-attention mechanism, to
alleviate the issue of the contamination of data by down-weighting the weight
of bad samples in the estimation process. The modified self-attention mechanism
can be incorporated into different Transformer variants. Empirical results on
language modeling and image classification tasks demonstrate the effectiveness
of this approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underspecification in Scene Description-to-Depiction Tasks. (arXiv:2210.05815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05815">
<div class="article-summary-box-inner">
<span><p>Questions regarding implicitness, ambiguity and underspecification are
crucial for understanding the task validity and ethical concerns of multimodal
image+text systems, yet have received little attention to date. This position
paper maps out a conceptual framework to address this gap, focusing on systems
which generate images depicting scenes from scene descriptions. In doing so, we
account for how texts and images convey meaning differently. We outline a set
of core challenges concerning textual and visual ambiguity, as well as risks
that may be amplified by ambiguous and underspecified elements. We propose and
discuss strategies for addressing these challenges, including generating
visually ambiguous images, and generating a set of diverse images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social-Group-Agnostic Word Embedding Debiasing via the Stereotype Content Model. (arXiv:2210.05831v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05831">
<div class="article-summary-box-inner">
<span><p>Existing word embedding debiasing methods require social-group-specific word
pairs (e.g., "man"-"woman") for each social attribute (e.g., gender), which
cannot be used to mitigate bias for other social groups, making these methods
impractical or costly to incorporate understudied social groups in debiasing.
We propose that the Stereotype Content Model (SCM), a theoretical framework
developed in social psychology for understanding the content of stereotypes,
which structures stereotype content along two psychological dimensions -
"warmth" and "competence" - can help debiasing efforts to become
social-group-agnostic by capturing the underlying connection between bias and
stereotypes. Using only pairs of terms for warmth (e.g., "genuine"-"fake") and
competence (e.g.,"smart"-"stupid"), we perform debiasing with established
methods and find that, across gender, race, and age, SCM-based debiasing
performs comparably to group-specific debiasing
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP also Understands Text: Prompting CLIP for Phrase Understanding. (arXiv:2210.05836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05836">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pretraining (CLIP) efficiently learns visual
concepts by pre-training with natural language supervision. CLIP and its visual
encoder have been explored on various vision and language tasks and achieve
strong zero-shot or transfer learning performance. However, the application of
its text encoder solely for text understanding has been less explored. In this
paper, we find that the text encoder of CLIP actually demonstrates strong
ability for phrase understanding, and can even significantly outperform popular
language models such as BERT with a properly designed prompt. Extensive
experiments validate the effectiveness of our method across different datasets
and domains on entity clustering and entity set expansion tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEAL : Interactive Tool for Systematic Error Analysis and Labeling. (arXiv:2210.05839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05839">
<div class="article-summary-box-inner">
<span><p>With the advent of Transformers, large language models (LLMs) have saturated
well-known NLP benchmarks and leaderboards with high aggregate performance.
However, many times these models systematically fail on tail data or rare
groups not obvious in aggregate evaluation. Identifying such problematic data
groups is even more challenging when there are no explicit labels (e.g.,
ethnicity, gender, etc.) and further compounded for NLP datasets due to the
lack of visual features to characterize failure modes (e.g., Asian males,
animals indoors, waterbirds on land, etc.). This paper introduces an
interactive Systematic Error Analysis and Labeling (\seal) tool that uses a
two-step approach to first identify high error slices of data and then, in the
second step, introduce methods to give human-understandable semantics to those
underperforming slices. We explore a variety of methods for coming up with
coherent semantics for the error groups using language models for semantic
labeling and a text-to-image model for generating visual features. SEAL toolkit
and demo screencast is available at https://huggingface.co/spaces/nazneen/seal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score. (arXiv:2210.05875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05875">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new natural language processing (NLP) application for
identifying medical jargon terms potentially difficult for patients to
comprehend from electronic health record (EHR) notes. We first present a novel
and publicly available dataset with expert-annotated medical jargon terms from
18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon
extraction ($MedJEx$) model which has been shown to outperform existing
state-of-the-art NLP models. First, MedJEx improved the overall performance
when it was trained on an auxiliary Wikipedia hyperlink span dataset, where
hyperlink spans provide additional Wikipedia articles to explain the spans (or
terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that
a contextualized masked language model score was beneficial for detecting
domain-specific unfamiliar jargon terms. Moreover, our results show that
training on the auxiliary Wikipedia hyperlink span datasets improved six out of
eight biomedical named entity recognition benchmark datasets. Both MedJ and
MedJEx are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning. (arXiv:2210.05883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05883">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained language models on downstream tasks is apt to
suffer from overfitting when limited training data is available. While dropout
proves to be an effective antidote by randomly dropping a proportion of units,
existing research has not examined its effect on the self-attention mechanism.
In this paper, we investigate this problem through self-attention attribution
and find that dropping attention positions with low attribution scores can
accelerate training and increase the risk of overfitting. Motivated by this
observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly
discards some high-attribution positions to encourage the model to make
predictions by relying more on low-attribution positions to reduce overfitting.
We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to
avoid dropping high-attribution positions excessively. Extensive experiments on
various benchmarks show that AD-DROP yields consistent improvements over
baselines. Analysis further confirms that AD-DROP serves as a strategic
regularizer to prevent overfitting during fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05892">
<div class="article-summary-box-inner">
<span><p>Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality
of the generated text. They suppose that if the value of PPL is smaller, the
quality(i.e. fluency) of the text to be evaluated is better. However, we find
that the PPL referee is unqualified and it cannot evaluate the generated text
fairly for the following reasons: (i) The PPL of short text is larger than long
text, which goes against common sense, (ii) The repeated text span could damage
the performance of PPL, and (iii) The punctuation marks could affect the
performance of PPL heavily. Experiments show that the PPL is unreliable for
evaluating the quality of given text. Last, we discuss the key problems with
evaluating text quality using language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning. (arXiv:2210.05901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05901">
<div class="article-summary-box-inner">
<span><p>Intelligent virtual assistants are currently designed to perform tasks or
services explicitly mentioned by users, so multiple related domains or tasks
need to be performed one by one through a long conversation with many explicit
intents. Instead, human assistants are capable of reasoning (multiple) implicit
intents based on user utterances via commonsense knowledge, reducing complex
interactions and improving practicality. Therefore, this paper proposes a
framework of multi-domain dialogue systems, which can automatically infer
implicit intents based on user utterances and then perform zero-shot prompting
using a large pre-trained language model to trigger suitable single
task-oriented bots. The proposed framework is demonstrated effective to realize
implicit intents and recommend associated bots in a zero-shot manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion. (arXiv:2210.05905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05905">
<div class="article-summary-box-inner">
<span><p>Automatic discourse processing, which can help understand how sentences
connect to each other, is bottlenecked by data: current discourse formalisms
pose highly demanding annotation tasks involving large taxonomies of discourse
relations, making them inaccessible to lay annotators. This work instead adopts
the linguistic framework of Questions Under Discussion (QUD) for discourse
analysis and seeks to derive QUD structures automatically. QUD views each
sentence as an answer to a question triggered in prior context; thus, we
characterize relationships between sentences as free-form questions, in
contrast to exhaustive fine-grained taxonomies. We develop the
first-of-its-kind QUD parser that derives a dependency structure of questions
over full documents, trained using a large question-answering dataset DCQA
annotated in a manner consistent with the QUD framework. Importantly, data
collection is easily crowdsourced using DCQA's paradigm. We show that this
leads to a parser attaining strong performance according to human evaluation.
We illustrate how our QUD structure is distinct from RST trees, and demonstrate
the utility of QUD analysis in the context of document simplification. Our
findings show that QUD parsing is an appealing alternative for automatic
discourse processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05916">
<div class="article-summary-box-inner">
<span><p>Hateful memes are a growing menace on social media. While the image and its
corresponding text in a meme are related, they do not necessarily convey the
same meaning when viewed individually. Hence, detecting hateful memes requires
careful consideration of both visual and textual information. Multimodal
pre-training can be beneficial for this task because it effectively captures
the relationship between the image and the text by representing them in a
similar feature space. Furthermore, it is essential to model the interactions
between the image and text features through intermediate fusion. Most existing
methods either employ multimodal pre-training or intermediate fusion, but not
both. In this work, we propose the Hate-CLIPper architecture, which explicitly
models the cross-modal interactions between the image and text representations
obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a
feature interaction matrix (FIM). A simple classifier based on the FIM
representation is able to achieve state-of-the-art performance on the Hateful
Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the
human performance of 82.65. Experiments on other meme datasets such as
Propaganda Memes and TamilMemes also demonstrate the generalizability of the
proposed approach. Finally, we analyze the interpretability of the FIM
representation and show that cross-modal interactions can indeed facilitate the
learning of meaningful concepts. The code for this work is available at
https://github.com/gokulkarthik/hateclipper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval and Reading Comprehension. (arXiv:2210.05921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05921">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs, as the cornerstone of many AI applications, usually face
serious incompleteness problems. In recent years, there have been many efforts
to study automatic knowledge graph completion (KGC), most of which use existing
knowledge to infer new knowledge. However, in our experiments, we find that not
all relations can be obtained by inference, which constrains the performance of
existing models. To alleviate this problem, we propose a new model based on
information retrieval and reading comprehension, namely IR4KGC. Specifically,
we pre-train a knowledge-based information retrieval module that can retrieve
documents related to the triples to be completed. Then, the retrieved documents
are handed over to the reading comprehension module to generate the predicted
answers. In experiments, we find that our model can well solve relations that
cannot be inferred from existing knowledge, and achieve good results on KGC
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Knowledge from Language Models for Video-based Action Anticipation. (arXiv:2210.05991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05991">
<div class="article-summary-box-inner">
<span><p>Anticipating future actions in a video is useful for many autonomous and
assistive technologies. Prior action anticipation work mostly treats this as a
vision modality problem, where the models learn the task information primarily
from the video features in the target action anticipation datasets. In this
work, we propose a method to make use of the text-modality that is available
during the training, to bring in complementary information that is not present
in the target action anticipation datasets. In particular, we leverage
pre-trained language models to build a text-modality teacher that is able to
predict future actions based on text labels of the past actions extracted from
the input video. To further adapt the teacher to the target domain (cooking),
we also pretrain the teacher on textual instructions from a recipes dataset
(Recipe1M). Then, we distill the knowledge gained by the text-modality teacher
into a vision-modality student to further improve it's performance. We
empirically evaluate this simple cross-modal distillation strategy on two video
datasets EGTEA-GAZE+ and EPIC-KITCHEN 55. Distilling this text-modality
knowledge into a strong vision model (Anticipative Vision Transformer) yields
consistent gains across both datasets, 3.5% relative improvement on top1 class
mean recall for EGTEA-GAZE+, 7.2% on top5 many-shot class mean recall for
EPIC-KITCHEN 55 and achieves new state-of-the-results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Graph-Based Text Representations with Character and Word Level N-grams. (arXiv:2210.05999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05999">
<div class="article-summary-box-inner">
<span><p>Graph-based text representation focuses on how text documents are represented
as graphs for exploiting dependency information between tokens and documents
within a corpus. Despite the increasing interest in graph representation
learning, there is limited research in exploring new ways for graph-based text
representation, which is important in downstream natural language processing
tasks. In this paper, we first propose a new heterogeneous word-character text
graph that combines word and character n-gram nodes together with document
nodes, allowing us to better learn dependencies among these entities.
Additionally, we propose two new graph-based neural models, WCTextGCN and
WCTextGAT, for modeling our proposed text graph. Extensive experiments in text
classification and automatic text summarization benchmarks demonstrate that our
proposed models consistently outperform competitive baselines and
state-of-the-art graph-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Machine Translation with Translation Memories. (arXiv:2210.06020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06020">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive machine translation (NAT) has recently made great
progress. However, most works to date have focused on standard translation
tasks, even though some edit-based NAT models, such as the Levenshtein
Transformer (LevT), seem well suited to translate with a Translation Memory
(TM). This is the scenario considered here. We first analyze the vanilla LevT
model and explain why it does not do well in this setting. We then propose a
new variant, TM-LevT, and show how to effectively train this model. By
modifying the data presentation and introducing an extra deletion operation, we
obtain performance that are on par with an autoregressive approach, while
reducing the decoding load. We also show that incorporating TMs during training
dispenses to use knowledge distillation, a well-known trick used to mitigate
the multimodality issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics. (arXiv:2210.06023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06023">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the task of retrieving documents with predefined
topics from an unlabeled document dataset using an unsupervised approach. The
proposed unsupervised approach requires only a small number of keywords
describing the respective topics and no labeled document. Existing approaches
either heavily relied on a large amount of additionally encoded world knowledge
or on term-document frequencies. Contrariwise, we introduce a method that
learns jointly embedded document and word vectors solely from the unlabeled
document dataset in order to find documents that are semantically similar to
the topics described by the keywords. The proposed method requires almost no
text preprocessing but is simultaneously effective at retrieving relevant
documents with high probability. When successively retrieving documents on
different predefined topics from publicly available and commonly used datasets,
we achieved an average area under the receiver operating characteristic curve
value of 0.95 on one dataset and 0.92 on another. Further, our method can be
used for multiclass document classification, without the need to assign labels
to the dataset in advance. Compared with an unsupervised classification
baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the
respective datasets. For easy replication of our approach, we make the
developed Lbl2Vec code publicly available as a ready-to-use tool under the
3-Clause BSD license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning. (arXiv:2210.06044v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06044">
<div class="article-summary-box-inner">
<span><p>Learning medical visual representations directly from paired radiology
reports has become an emerging topic in representation learning. However,
existing medical image-text joint learning methods are limited by instance or
local supervision analysis, ignoring disease-level semantic correspondences. In
this paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA)
framework for generalized medical visual representation learning by harnessing
the naturally exhibited semantic correspondences between medical image and
radiology reports at three different levels, i.e., pathological region-level,
instance-level, and disease-level. Specifically, we first incorporate the
instance-wise alignment module by maximizing the agreement between image-report
pairs. Further, for token-wise alignment, we introduce a bidirectional
cross-attention strategy to explicitly learn the matching between fine-grained
visual tokens and text tokens, followed by contrastive learning to align them.
More important, to leverage the high-level inter-subject relationship semantic
(e.g., disease) correspondences, we design a novel cross-modal disease-level
alignment paradigm to enforce the cross-modal cluster assignment consistency.
Extensive experimental results on seven downstream medical image datasets
covering image classification, object detection, and semantic segmentation
tasks demonstrate the stable and superior performance of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain. (arXiv:2210.06068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06068">
<div class="article-summary-box-inner">
<span><p>Massively multilingual pre-trained language models (MMPLMs) are developed in
recent years demonstrating superpowers and the pre-knowledge they acquire for
downstream tasks. In this work, we investigate whether MMPLMs can be applied to
zero-shot machine translation (MT) toward entirely new language pairs and new
domains. We carry out an experimental investigation using Meta-AI's MMPLMs
"wmt21-dense-24-wide-en-X and X-en (WMT21fb)" which were pre-trained on 7
language pairs and 14 translation directions including English to Czech,
German, Hausa, Icelandic, Japanese, Russian, and Chinese, and opposite
direction. We fine-tune these MMPLMs towards English-Spanish language pair
which did not exist at all in their original pre-trained corpora both
implicitly and explicitly. We prepare carefully aligned clinical domain data
for this fine-tuning, which is different from their original mixed domain
knowledge as well. Our experimental result shows that the fine-tuning is very
successful using just 250k well-aligned in-domain EN-ES pairs/sentences for
three sub-task translation tests: clinical cases, clinical terms, and ontology
concepts. It achieves very close evaluation scores to another MMPLM NLLB from
Meta-AI, which included Spanish as a high-resource setting in the pre-training.
To the best of our knowledge, this is the first work on using MMPLMs towards
real zero-shot NMT successfully for totally unseen languages during
pre-training, and also the first in clinical domain for such a study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge. (arXiv:2210.06091v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06091">
<div class="article-summary-box-inner">
<span><p>Code-switching automatic speech recognition becomes one of the most
challenging and the most valuable scenarios of automatic speech recognition,
due to the code-switching phenomenon between multilingual language and the
frequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022
Chinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge
aims to promote the development of code-switching automatic speech recognition.
The ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus
and MagicData-RAMC corpus, a development and a test set for participants, which
are used for CSASR model training and evaluation. Along with the challenge, we
also provide the baseline system performance for reference. As a result, more
than 40 teams participated in this challenge, and the winner team achieved
16.70% Mixture Error Rate (MER) performance on the test set and has achieved
9.8% MER absolute improvement compared with the baseline system. In this paper,
we will describe the datasets, the associated baselines system and the
requirements, and summarize the CSASR challenge results and major techniques
and tricks used in the submitted systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Continual Learning for Text Classification via Selective Inter-client Transfer. (arXiv:2210.06101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06101">
<div class="article-summary-box-inner">
<span><p>In this work, we combine the two paradigms: Federated Learning (FL) and
Continual Learning (CL) for text classification task in cloud-edge continuum.
The objective of Federated Continual Learning (FCL) is to improve deep learning
models over life time at each client by (relevant and efficient) knowledge
transfer without sharing data. Here, we address challenges in minimizing
inter-client interference while knowledge sharing due to heterogeneous tasks
across clients in FCL setup. In doing so, we propose a novel framework,
Federated Selective Inter-client Transfer (FedSeIT) which selectively combines
model parameters of foreign clients. To further maximize knowledge transfer, we
assess domain overlap and select informative tasks from the sequence of
historical tasks at each foreign client while preserving privacy. Evaluating
against the baselines, we show improved performance, a gain of (average) 12.4\%
in text classification over a sequence of tasks using five datasets from
diverse domains. To the best of our knowledge, this is the first work that
applies FCL to NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain. (arXiv:2210.06104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06104">
<div class="article-summary-box-inner">
<span><p>We introduce a high-quality dataset that contains 3,397 samples comprising
(i) multiple choice questions, (ii) answers (including distractors), and (iii)
their source documents, from the educational domain. Each question is phrased
in two forms, normal and close. Correct answers are linked to source documents
with sentence-level annotations. Thus, our versatile dataset can be used for
both question and distractor generation, as well as to explore new challenges
such as question format conversion. Furthermore, 903 questions are accompanied
by their cognitive complexity level as per Bloom's taxonomy. All questions have
been generated by educational experts rather than crowd workers to ensure they
are maintaining educational and learning standards. Our analysis and
experiments suggest distinguishable differences between our dataset and
commonly used ones for question generation for educational purposes. We believe
this new dataset can serve as a valuable resource for research and evaluation
in the educational domain. The dataset and baselines will be released to
support further research in question generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Data Augmentation for Translation Suggestion. (arXiv:2210.06138v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06138">
<div class="article-summary-box-inner">
<span><p>Translation suggestion (TS) models are used to automatically provide
alternative suggestions for incorrect spans in sentences generated by machine
translation. This paper introduces the system used in our submission to the
WMT'22 Translation Suggestion shared task. Our system is based on the ensemble
of different translation architectures, including Transformer, SA-Transformer,
and DynamicConv. We use three strategies to construct synthetic data from
parallel corpora to compensate for the lack of supervised data. In addition, we
introduce a multi-phase pre-training strategy, adding an additional
pre-training phase with in-domain data. We rank second and third on the
English-German and English-Chinese bidirectional tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating Norwegian Language Varieties on Twitter for Part-of-Speech. (arXiv:2210.06150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06150">
<div class="article-summary-box-inner">
<span><p>Norwegian Twitter data poses an interesting challenge for Natural Language
Processing (NLP) tasks. These texts are difficult for models trained on
standardized text in one of the two Norwegian written forms (Bokm{\aa}l and
Nynorsk), as they contain both the typical variation of social media text, as
well as a large amount of dialectal variety. In this paper we present a novel
Norwegian Twitter dataset annotated with POS-tags. We show that models trained
on Universal Dependency (UD) data perform worse when evaluated against this
dataset, and that models trained on Bokm{\aa}l generally perform better than
those trained on Nynorsk. We also see that performance on dialectal tweets is
comparable to the written standards for some models. Finally we perform a
detailed analysis of the errors that models commonly make on this data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06155">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the rise and success of pre-training techniques
in visually-rich document understanding. However, most existing methods lack
the systematic mining and utilization of layout-centered knowledge, leading to
sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel
document pre-training solution with layout knowledge enhancement in the whole
workflow, to learn better representations that combine the features from text,
layout, and image. Specifically, we first rearrange input sequences in the
serialization stage, and then present a correlative pre-training task, reading
order prediction, to learn the proper reading order of documents. To improve
the layout awareness of the model, we integrate a spatial-aware disentangled
attention into the multi-modal transformer and a replaced regions prediction
task into the pre-training phase. Experimental results show that ERNIE-Layout
achieves superior performance on various downstream tasks, setting new
state-of-the-art on key information extraction, document image classification,
and document question answering datasets. The code and models are publicly
available at
<a href="http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focusing on Context is NICE: Improving Overshadowed Entity Disambiguation. (arXiv:2210.06164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06164">
<div class="article-summary-box-inner">
<span><p>Entity disambiguation (ED) is the task of mapping an ambiguous entity mention
to the corresponding entry in a structured knowledge base. Previous research
showed that entity overshadowing is a significant challenge for existing ED
models: when presented with an ambiguous entity mention, the models are much
more likely to rank a more frequent yet less contextually relevant entity at
the top. Here, we present NICE, an iterative approach that uses entity type
information to leverage context and avoid over-relying on the frequency-based
prior. Our experiments show that NICE achieves the best performance results on
the overshadowed entities while still performing competitively on the frequent
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VCSE: Time-Domain Visual-Contextual Speaker Extraction Network. (arXiv:2210.06177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06177">
<div class="article-summary-box-inner">
<span><p>Speaker extraction seeks to extract the target speech in a multi-talker
scenario given an auxiliary reference. Such reference can be auditory, i.e., a
pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic
sequence. References in different modalities provide distinct and complementary
information that could be fused to form top-down attention on the target
speaker. Previous studies have introduced visual and contextual modalities in a
single model. In this paper, we propose a two-stage time-domain
visual-contextual speaker extraction network named VCSE, which incorporates
visual and self-enrolled contextual cues stage by stage to take full advantage
of every modality. In the first stage, we pre-extract a target speech with
visual cues and estimate the underlying phonetic sequence. In the second stage,
we refine the pre-extracted target speech with the self-enrolled contextual
cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)
database demonstrate that our proposed VCSE network consistently outperforms
other state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word Alignment. (arXiv:2210.06207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06207">
<div class="article-summary-box-inner">
<span><p>Word alignments are essential for a variety of NLP tasks. Therefore, choosing
the best approaches for their creation is crucial. However, the scarce
availability of gold evaluation data makes the choice difficult. We propose
SilverAlign, a new method to automatically create silver data for the
evaluation of word aligners by exploiting machine translation and minimal
pairs. We show that performance on our silver data correlates well with gold
benchmarks for 9 language pairs, making our approach a valid resource for
evaluation of different domains and languages when gold data are not available.
This addresses the important scenario of missing gold data alignments for
low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06210">
<div class="article-summary-box-inner">
<span><p>To overcome the overparameterized problem in Pre-trained Language Models
(PLMs), pruning is widely used as a simple and straightforward compression
method by directly removing unimportant weights. Previous first-order methods
successfully compress PLMs to extremely high sparsity with little performance
drop. These methods, such as movement pruning, use first-order information to
prune PLMs while fine-tuning the remaining weights. In this work, we argue
fine-tuning is redundant for first-order pruning, since first-order pruning is
sufficient to converge PLMs to downstream tasks without fine-tuning. Under this
motivation, we propose Static Model Pruning (SMP), which only uses first-order
pruning to adapt PLMs to downstream tasks while achieving the target sparsity
level. In addition, we also design a new masking function and training
objective to further improve SMP. Extensive experiments at various sparsity
levels show SMP has significant improvements over first-order and zero-order
methods. Unlike previous first-order methods, SMP is also applicable to low
sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter
efficient than other methods due to it does not require fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards visually prompted keyword localisation for zero-resource spoken languages. (arXiv:2210.06229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06229">
<div class="article-summary-box-inner">
<span><p>Imagine being able to show a system a visual depiction of a keyword and
finding spoken utterances that contain this keyword from a zero-resource speech
corpus. We formalise this task and call it visually prompted keyword
localisation (VPKL): given an image of a keyword, detect and predict where in
an utterance the keyword occurs. To do VPKL, we propose a speech-vision model
with a novel localising attention mechanism which we train with a new keyword
sampling scheme. We show that these innovations give improvements in VPKL over
an existing speech-vision model. We also compare to a visual bag-of-words (BoW)
model where images are automatically tagged with visual labels and paired with
unlabelled speech. Although this visual BoW can be queried directly with a
written keyword (while our's takes image queries), our new model still
outperforms the visual BoW in both detection and localisation, giving a 16%
relative improvement in localisation F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quasi-symbolic explanatory NLI via disentanglement: A geometrical examination. (arXiv:2210.06230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06230">
<div class="article-summary-box-inner">
<span><p>Disentangling the encodings of neural models is a fundamental aspect for
improving interpretability, semantic control, and understanding downstream task
performance in Natural Language Processing. The connection points between
disentanglement and downstream tasks, however, remains underexplored from a
explanatory standpoint. This work presents a methodology for assessment of
geometrical properties of the resulting latent space w.r.t. vector operations
and semantic disentanglement in quantitative and qualitative terms, based on a
VAE-based supervised framework. Empirical results indicate that the
role-contents of explanations, such as \textit{ARG0-animal}, are disentangled
in the latent space, which provides us a chance for controlling the explanation
generation by manipulating the traversal of vector over latent space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A context-aware knowledge transferring strategy for CTC-based ASR. (arXiv:2210.06244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06244">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: On Potential Histories in NLP. (arXiv:2210.06245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06245">
<div class="article-summary-box-inner">
<span><p>Machine learning and NLP require the construction of datasets to train and
fine-tune models. In this context, previous work has demonstrated the
sensitivity of these data sets. For instance, potential societal biases in this
data are likely to be encoded and to be amplified in the models we deploy. In
this work, we draw from developments in the field of history and take a novel
perspective on these problems: considering datasets and models through the lens
of historical fiction surfaces their political nature, and affords
re-configuring how we view the past, such that marginalized discourses are
surfaced. Building on such insights, we argue that contemporary methods for
machine learning are prejudiced towards dominant and hegemonic histories.
Employing the example of neopronouns, we show that by surfacing marginalized
histories within contemporary conditions, we can create models that better
represent the lived realities of traditionally marginalized and excluded
communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm. (arXiv:2210.06246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06246">
<div class="article-summary-box-inner">
<span><p>Recently, the community has achieved substantial progress on many commonsense
reasoning benchmarks. However, it is still unclear what is learned from the
training process: the knowledge, inference capability, or both? We argue that
due to the large scale of commonsense knowledge, it is infeasible to annotate a
large enough training set for each task to cover all commonsense for learning.
Thus we should separate the commonsense knowledge acquisition and inference
over commonsense knowledge as two separate tasks. In this work, we focus on
investigating models' commonsense inference capabilities from two perspectives:
(1) Whether models can know if the knowledge they have is enough to solve the
task; (2) Whether models can develop commonsense inference capabilities that
generalize across commonsense tasks. We first align commonsense tasks with
relevant knowledge from commonsense knowledge bases and ask humans to annotate
whether the knowledge is enough or not. Then, we convert different commonsense
tasks into a unified question answering format to evaluate models'
generalization capabilities. We name the benchmark as Commonsense Inference
with Knowledge-in-the-loop Question Answering (CIKQA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot On-the-Fly Event Schema Induction. (arXiv:2210.06254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06254">
<div class="article-summary-box-inner">
<span><p>What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Compass: Scaling Multi-task Pre-training with Task Prefix. (arXiv:2210.06277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06277">
<div class="article-summary-box-inner">
<span><p>Leveraging task-aware annotated data as supervised signals to assist with
self-supervised learning on large-scale unlabeled data has become a new trend
in pre-training language models. Existing studies show that multi-task learning
with large-scale supervised tasks suffers from negative effects across tasks.
To tackle the challenge, we propose a task prefix guided multi-task
pre-training framework to explore the relationships among tasks. We conduct
extensive experiments on 40 datasets, which show that our model can not only
serve as the strong foundation backbone for a wide range of tasks but also be
feasible as a probing tool for analyzing task relationships. The task
relationships reflected by the prefixes align transfer learning performance
between tasks. They also suggest directions for data augmentation with
complementary tasks, which help our model achieve human-parity results on
commonsense reasoning leaderboards. Code is available at
https://github.com/cooelf/CompassMTL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06281">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed much interest in temporal reasoning over
knowledge graphs (KG) for complex question answering (QA), but there remains a
substantial gap in human capabilities. We explore how to generalize relational
graph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose
a novel, intuitive and interpretable scheme to modulate the messages passed
through a KG edge during convolution, based on the relevance of its associated
time period to the question. We also introduce a gating device to predict if
the answer to a complex temporal question is likely to be a KG entity or time
and use this prediction to guide our scoring mechanism. We evaluate the
resulting system, which we call TwiRGCN, on TimeQuestions, a recently released,
challenging dataset for multi-hop complex temporal QA. We show that TwiRGCN
significantly outperforms state-of-the-art systems on this dataset across
diverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage
points for the most difficult ordinal and implicit question types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalized and Explainable Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06282">
<div class="article-summary-box-inner">
<span><p>Context representation is crucial to both dialogue understanding and
generation. Recently, the most popular method for dialog context representation
is to concatenate the last-$k$ previous utterances as context and use a large
transformer-based model to generate the next response. However, this method may
not be ideal for conversations containing long-range dependencies. In this
work, we propose DialoGX, a novel encoder-decoder based framework for
conversational response generation with a generalized and explainable context
representation that can look beyond the last-$k$ utterances. Hence the method
is adaptive to conversations with long-range dependencies. Our proposed
solution is based on two key ideas: a) computing a dynamic representation of
the entire context, and b) finding the previous utterances that are relevant
for generating the next response. Instead of last-$k$ utterances, DialoGX uses
the concatenation of the dynamic context vector and encoding of the most
relevant utterances as input which enables it to represent conversations of any
length in a compact and generalized fashion. We conduct our experiments on
DailyDialog, a popular open-domain chit-chat dataset. DialoGX achieves
comparable performance with the state-of-the-art models on the automated
metrics. We also justify our context representation through the lens of
psycholinguistics and show that the relevance score of previous utterances
agrees well with human cognition which makes DialoGX explainable as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Changing the Representation: Examining Language Representation for Neural Sign Language Production. (arXiv:2210.06312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06312">
<div class="article-summary-box-inner">
<span><p>Neural Sign Language Production (SLP) aims to automatically translate from
spoken language sentences to sign language videos. Historically the SLP task
has been broken into two steps; Firstly, translating from a spoken language
sentence to a gloss sequence and secondly, producing a sign language video
given a sequence of glosses. In this paper we apply Natural Language Processing
techniques to the first step of the SLP pipeline. We use language models such
as BERT and Word2Vec to create better sentence level embeddings, and apply
several tokenization techniques, demonstrating how these improve performance on
the low resource translation task of Text to Gloss. We introduce Text to
HamNoSys (T2H) translation, and show the advantages of using a phonetic
representation for sign language translation rather than a sign level gloss
representation. Furthermore, we use HamNoSys to extract the hand shape of a
sign and use this as additional supervision during training, further increasing
the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of
26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers. (arXiv:2210.06313v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06313">
<div class="article-summary-box-inner">
<span><p>This paper studies the curious phenomenon for machine learning models with
Transformer architectures that their activation maps are sparse. By activation
map we refer to the intermediate output of the multi-layer perceptrons (MLPs)
after a ReLU activation function, and by "sparse" we mean that on average very
few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each
input to MLP. Moreover, larger Transformers with more layers and wider MLP
hidden dimensions are sparser as measured by the percentage of nonzero entries.
Through extensive experiments we demonstrate that the emergence of sparsity is
a prevalent phenomenon that occurs for both natural language processing and
vision tasks, on both training and evaluation data, for Transformers of various
configurations, at layers of all depth levels, as well as for other
architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also
emerges using training datasets with random labels, or with random inputs, or
with infinite amount of data, demonstrating that sparsity is not a result of a
specific family of datasets. We discuss how sparsity immediately implies a way
to significantly reduce the FLOP count and improve efficiency for Transformers.
Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser
activation via Top-k thresholding with a small value of k brings a collection
of desired but missing properties for Transformers, namely less sensitivity to
noisy training data, more robustness to input corruptions, and better
calibration for their prediction confidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06324">
<div class="article-summary-box-inner">
<span><p>Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media. (arXiv:2210.06331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06331">
<div class="article-summary-box-inner">
<span><p>We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly
annotated social media posts from Reddit spanning 24 health conditions.
Annotations include demarcations of spans corresponding to medical claims,
personal experiences, and questions. We collect additional granular annotations
on identified claims. Specifically, we mark snippets that describe patient
Populations, Interventions, and Outcomes (PIO elements) within these. Using
this corpus, we introduce the task of retrieving trustworthy evidence relevant
to a given claim made on social media. We propose a new method to automatically
derive (noisy) supervision for this task which we use to train a dense
retrieval model; this outperforms baseline models. Manual evaluation of
retrieval results performed by medical doctors indicate that while our system
performance is promising, there is considerable room for improvement. Collected
annotations (and scripts to assemble the dataset), are available at
https://github.com/sominw/redhot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Text Detection: Systemic Literature Review. (arXiv:2210.06336v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06336">
<div class="article-summary-box-inner">
<span><p>Within the text analysis and processing fields, generated text attacks have
been made easier to create than ever before. To combat these attacks open
sourcing models and datasets have become a major trend to create automated
detection algorithms in defense of authenticity. For this purpose, synthetic
text detection has become an increasingly viable topic of research. This review
is written for the purpose of creating a snapshot of the state of current
literature and easing the barrier to entry for future authors. Towards that
goal, we identified few research trends and challenges in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06340">
<div class="article-summary-box-inner">
<span><p>Current deep learning models trained to generate radiology reports from chest
radiographs are capable of producing clinically accurate, clear, and actionable
text that can advance patient care. However, such systems all succumb to the
same problem: making hallucinated references to non-existent prior reports.
Such hallucinations occur because these models are trained on datasets of
real-world patient reports that inherently refer to priors. To this end, we
propose two methods to remove references to priors in radiology reports: (1) a
GPT-3-based few-shot approach to rewrite medical reports without references to
priors; and (2) a BioBERT-based token classification approach to directly
remove words referring to priors. We use the aforementioned approaches to
modify MIMIC-CXR, a publicly available dataset of chest X-rays and their
associated free-text radiology reports; we then retrain CXR-RePaiR, a radiology
report generation system, on the adapted MIMIC-CXR dataset. We find that our
re-trained model--which we call CXR-ReDonE--outperforms previous report
generation methods on clinical metrics, achieving an average BERTScore of
0.2351 (2.57% absolute improvement). We expect our approach to be broadly
valuable in enabling current radiology report generation systems to be more
directly integrated into clinical pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TaskMix: Data Augmentation for Meta-Learning of Spoken Intent Understanding. (arXiv:2210.06341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06341">
<div class="article-summary-box-inner">
<span><p>Meta-Learning has emerged as a research direction to better transfer
knowledge from related tasks to unseen but related tasks. However,
Meta-Learning requires many training tasks to learn representations that
transfer well to unseen tasks; otherwise, it leads to overfitting, and the
performance degenerates to worse than Multi-task Learning. We show that a
state-of-the-art data augmentation method worsens this problem of overfitting
when the task diversity is low. We propose a simple method, TaskMix, which
synthesizes new tasks by linearly interpolating existing tasks. We compare
TaskMix against many baselines on an in-house multilingual intent
classification dataset of N-Best ASR hypotheses derived from real-life
human-machine telephony utterances and two datasets derived from MTOP. We show
that TaskMix outperforms baselines, alleviates overfitting when task diversity
is low, and does not degrade performance even when it is high.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Open-Domain Question Answering. (arXiv:2210.06345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06345">
<div class="article-summary-box-inner">
<span><p>We introduce the Variational Open-Domain (VOD) framework for end-to-end
training and evaluation of retrieval-augmented models (open-domain question
answering and language modelling). We show that the R\'enyi variational bound,
a lower bound to the task marginal likelihood, can be exploited to aid
optimization and use importance sampling to estimate the task log-likelihood
lower bound and its gradients using samples drawn from an auxiliary retriever
(approximate posterior). The framework can be used to train modern
retrieval-augmented systems end-to-end using tractable and consistent estimates
of the R\'enyi variational bound and its gradients. We demonstrate the
framework's versatility by training reader-retriever BERT-based models on
multiple-choice medical exam questions (MedMCQA and USMLE). We registered a new
state-of-the-art for both datasets (MedMCQA: $62.9$\%, USMLE: $55.0$\%). Last,
we show that the retriever part of the learned reader-retriever model trained
on the medical board exam questions can be used in search engines for a medical
knowledge base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06346">
<div class="article-summary-box-inner">
<span><p>The number of clinical citations received from clinical guidelines or
clinical trials has been considered as one of the most appropriate indicators
for quantifying the clinical impact of biomedical papers. Therefore, the early
prediction of the clinical citation count of biomedical papers is critical to
scientific activities in biomedicine, such as research evaluation, resource
allocation, and clinical translation. In this study, we designed a four-layer
multilayer perceptron neural network (MPNN) model to predict the clinical
citation count of biomedical papers in the future by using 9,822,620 biomedical
papers published from 1985 to 2005. We extracted ninety-one paper features from
three dimensions as the input of the model, including twenty-one features in
the paper dimension, thirty-five in the reference dimension, and thirty-five in
the citing paper dimension. In each dimension, the features can be classified
into three categories, i.e., the citation-related features, the clinical
translation-related features, and the topic-related features. Besides, in the
paper dimension, we also considered the features that have previously been
demonstrated to be related to the citation counts of research papers. The
results showed that the proposed MPNN model outperformed the other five
baseline models, and the features in the reference dimension were the most
important.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06349">
<div class="article-summary-box-inner">
<span><p>Closed-book question answering (QA) requires a model to directly answer an
open-domain question without access to any external knowledge. Prior work on
closed-book QA either directly finetunes or prompts a pretrained language model
(LM) to leverage the stored knowledge. However, they do not fully exploit the
parameterized knowledge. To address this issue, we propose a two-stage,
closed-book QA framework which employs a coarse-to-fine approach to extract
relevant knowledge and answer a question. Our approach first generates a
related context for a given question by prompting a pretrained LM. We then
prompt the same LM for answer prediction using the generated context and the
question. Additionally, to eliminate failure caused by context uncertainty, we
marginalize over generated contexts. Experimental results on three QA
benchmarks show that our method significantly outperforms previous closed-book
QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book
methods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our
method is able to better exploit the stored knowledge in pretrained LMs without
adding extra learnable parameters or needing finetuning, and paves the way for
hybrid models that integrate pretrained LMs with external knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter. (arXiv:2210.06351v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06351">
<div class="article-summary-box-inner">
<span><p>Harmful content detection models tend to have higher false positive rates for
content from marginalized groups. In the context of marginal abuse modeling on
Twitter, such disproportionate penalization poses the risk of reduced
visibility, where marginalized communities lose the opportunity to voice their
opinion on the platform. Current approaches to algorithmic harm mitigation, and
bias detection for NLP models are often very ad hoc and subject to human bias.
We make two main contributions in this paper. First, we design a novel
methodology, which provides a principled approach to detecting and measuring
the severity of potential harms associated with a text-based model. Second, we
apply our methodology to audit Twitter's English marginal abuse model, which is
used for removing amplification eligibility of marginally abusive content.
Without utilizing demographic labels or dialect classifiers, we are still able
to detect and measure the severity of issues related to the over-penalization
of the speech of marginalized communities, such as the use of reclaimed speech,
counterspeech, and identity related terms. In order to mitigate the associated
harms, we experiment with adding additional true negative examples and find
that doing so provides improvements to our fairness metrics without large
degradations in model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Russian Web Tables: A Public Corpus of Web Tables for Russian Language Based on Wikipedia. (arXiv:2210.06353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06353">
<div class="article-summary-box-inner">
<span><p>Corpora that contain tabular data such as WebTables are a vital resource for
the academic community. Essentially, they are the backbone of any modern
research in information management. They are used for various tasks of data
extraction, knowledge base construction, question answering, column semantic
type detection and many other. Such corpora are useful not only as a source of
data, but also as a base for building test datasets. So far, there were no such
corpora for the Russian language and this seriously hindered research in the
aforementioned areas.
</p>
<p>In this paper, we present the first corpus of Web tables created specifically
out of Russian language material. It was built via a special toolkit we have
developed to crawl the Russian Wikipedia. Both the corpus and the toolkit are
open-source and publicly available. Finally, we present a short study that
describes Russian Wikipedia tables and their statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-Audio Grounding Based Novel Metric for Evaluating Audio Caption Similarity. (arXiv:2210.06354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06354">
<div class="article-summary-box-inner">
<span><p>Automatic Audio Captioning (AAC) refers to the task of translating an audio
sample into a natural language (NL) text that describes the audio events,
source of the events and their relationships. Unlike NL text generation tasks,
which rely on metrics like BLEU, ROUGE, METEOR based on lexical semantics for
evaluation, the AAC evaluation metric requires an ability to map NL text
(phrases) that correspond to similar sounds in addition lexical semantics.
Current metrics used for evaluation of AAC tasks lack an understanding of the
perceived properties of sound represented by text. In this paper, wepropose a
novel metric based on Text-to-Audio Grounding (TAG), which is, useful for
evaluating cross modal tasks like AAC. Experiments on publicly available AAC
data-set shows our evaluation metric to perform better compared to existing
metrics used in NL text and image captioning literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extractive Question Answering on Queries in Hindi and Tamil. (arXiv:2210.06356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06356">
<div class="article-summary-box-inner">
<span><p>Indic languages like Hindi and Tamil are underrepresented in the natural
language processing (NLP) field compared to languages like English. Due to this
underrepresentation, performance on NLP tasks (such as search algorithms) in
Indic languages are inferior to their English counterparts. This difference
disproportionately affects those who come from lower socioeconomic statuses
because they consume the most Internet content in local languages. The goal of
this project is to build an NLP model that performs better than pre-existing
models for the task of extractive question-answering (QA) on a public dataset
in Hindi and Tamil. Extractive QA is an NLP task where answers to questions are
extracted from a corresponding body of text. To build the best solution, we
used three different models. The first model is an unmodified cross-lingual
version of the NLP model RoBERTa, known as XLM-RoBERTa, that is pretrained on
100 languages. The second model is based on the pretrained RoBERTa model with
an extra classification head for the question answering, but we used a custom
Indic tokenizer, then optimized hyperparameters and fine tuned on the Indic
dataset. The third model is based on XLM-RoBERTa, but with extra finetuning and
training on the Indic dataset. We hypothesize the third model will perform best
because of the variety of languages the XLM-RoBERTa model has been pretrained
on and the additional finetuning on the Indic dataset. This hypothesis was
proven wrong because the paired RoBERTa models performed the best as the
training data used was most specific to the task performed as opposed to the
XLM-RoBERTa models which had much data that was not in either Hindi or Tamil.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01065">
<div class="article-summary-box-inner">
<span><p>Do question answering (QA) modeling improvements (e.g., choice of
architecture and training procedure) hold across the diverse landscape of QA
benchmarks? To study this question, we introduce the notion of concurrence --
two benchmarks have high concurrence on a set of modeling approaches if they
rank the modeling approaches similarly. We measure the concurrence between 32
QA benchmarks on a set of 20 diverse modeling approaches and find that
human-constructed benchmarks have high concurrence amongst themselves, even if
their passage and question distributions are very different. Surprisingly, even
downsampled human-constructed benchmarks (i.e., collecting less data) and
programmatically-generated benchmarks (e.g., cloze-formatted examples) have
high concurrence with human-constructed benchmarks. These results indicate
that, despite years of intense community focus on a small number of benchmarks,
the modeling improvements studied hold broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oh My Mistake!: Toward Realistic Dialogue State Tracking including Turnback Utterances. (arXiv:2108.12637v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12637">
<div class="article-summary-box-inner">
<span><p>The primary purpose of dialogue state tracking (DST), a critical component of
an end-to-end conversational system, is to build a model that responds well to
real-world situations. Although we often change our minds from time to time
during ordinary conversations, current benchmark datasets do not adequately
reflect such occurrences and instead consist of over-simplified conversations,
in which no one changes their mind during a conversation. As the main question
inspiring the present study, "Are current benchmark datasets sufficiently
diverse to handle casual conversations in which one changes their mind after a
certain topic is over?" We found that the answer is "No" because DST models
cannot refer to previous user preferences when template-based turnback
utterances are injected into the dataset. Even in the the simplest
mind-changing (turnback) scenario, the performance of DST models significantly
degenerated. However, we found that this performance degeneration can be
recovered when the turnback scenarios are explicitly designed in the training
set, implying that the problem is not with the DST models but rather with the
construction of the benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and straightforward attempts at applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained language models; (2) non-standard hyperparameters that suit DP
optimization; and (3) fine-tuning objectives which are aligned with the
pretraining procedure. With the above, we obtain NLP models that outperform
state-of-the-art DP-trained models under the same privacy budget and strong
non-private baselines -- by directly fine-tuning pretrained models with DP
optimization on moderately-sized corpora. To address the computational
challenge of running DP-SGD with large Transformers, we propose a memory saving
technique that allows clipping in DP-SGD to run without instantiating
per-example gradients for any linear layer in the model. The technique enables
privately training Transformers with almost the same memory cost as non-private
training at a modest run-time overhead. Contrary to conventional wisdom that DP
optimization fails at learning high-dimensional models (due to noise that
scales with dimension) empirical results reveal that private learning with
pretrained language models doesn't tend to suffer from dimension-dependent
performance degradation. Code to reproduce results can be found at
https://github.com/lxuechen/private-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10962">
<div class="article-summary-box-inner">
<span><p>Knowledge-enhanced language representation learning has shown promising
results across various knowledge-intensive NLP tasks. However, prior methods
are limited in efficient utilization of multilingual knowledge graph (KG) data
for language model (LM) pretraining. They often train LMs with KGs in indirect
ways, relying on extra entity/relation embeddings to facilitate knowledge
injection. In this work, we explore methods to make better use of the
multilingual annotation and language agnostic property of KG triples, and
present novel knowledge based multilingual language models (KMLMs) trained
directly on the knowledge triples. We first generate a large amount of
multilingual synthetic sentences using the Wikidata KG triples. Then based on
the intra- and inter-sentence structures of the generated data, we design
pretraining tasks to enable the LMs to not only memorize the factual knowledge
but also learn useful logical patterns. Our pretrained KMLMs demonstrate
significant performance improvements on a wide range of knowledge-intensive
cross-lingual tasks, including named entity recognition (NER), factual
knowledge retrieval, relation classification, and a newly designed logical
reasoning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers. (arXiv:2112.09237v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09237">
<div class="article-summary-box-inner">
<span><p>Building natural language inference (NLI) benchmarks that are both
challenging for modern techniques, and free from cheating feature biases is
difficult. Chief among these biases is single sentence label leakage, where
annotator-introduced spurious correlations yield datasets where the logical
relation between (premise, hypothesis) pairs can be accurately predicted from
only a single sentence, something that should in principle be impossible. We
demonstrate that despite efforts to reduce this leakage, it persists in modern
datasets that have been introduced since its 2018 discovery. To enable future
amelioration efforts, introduce a novel model-driven technique, the progressive
evaluation of cluster outliers (PECO) which enables both the objective
measurement of leakage, and the automated detection of subpopulations in the
data which maximally exhibit it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13593">
<div class="article-summary-box-inner">
<span><p>Stock prices move as piece-wise trending fluctuation rather than a purely
random walk. Traditionally, the prediction of future stock movements is based
on the historical trading record. Nowadays, with the development of social
media, many active participants in the market choose to publicize their
strategies, which provides a window to glimpse over the whole market's attitude
towards future movements by extracting the semantics behind social media.
However, social media contains conflicting information and cannot replace
historical records completely. In this work, we propose a multi-modality
attention network to reduce conflicts and integrate semantic and numeric
features to predict future stock movements comprehensively. Specifically, we
first extract semantic information from social media and estimate their
credibility based on posters' identity and public reputation. Then we
incorporate the semantic from online posts and numeric features from historical
records to make the trading strategy. Experimental results show that our
approach outperforms previous methods by a significant margin in both
prediction accuracy (61.20\%) and trading profits (9.13\%). It demonstrates
that our method improves the performance of stock movements prediction and
informs future research on multi-modality fusion towards stock prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCROLLS: Standardized CompaRison Over Long Language Sequences. (arXiv:2201.03533v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03533">
<div class="article-summary-box-inner">
<span><p>NLP benchmarks have largely focused on short texts, such as sentences and
paragraphs, even though long texts comprise a considerable amount of natural
language in the wild. We introduce SCROLLS, a suite of tasks that require
reasoning over long texts. We examine existing long-text datasets, and handpick
ones where the text is naturally long, while prioritizing tasks that involve
synthesizing information across the input. SCROLLS contains summarization,
question answering, and natural language inference tasks, covering multiple
domains, including literature, science, business, and entertainment. Initial
baselines, including Longformer Encoder-Decoder, indicate that there is ample
room for improvement on SCROLLS. We make all datasets available in a unified
text-to-text format and host a live leaderboard to facilitate research on model
architecture and pretraining methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04538">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have demonstrated remarkable performance in
various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are
well known for their superior text generation capabilities; bidirectional PLMs
(e.g., BERT) have been the prominent choice for natural language understanding
(NLU) tasks. While both types of models have achieved promising few-shot
learning performance, their potential for zero-shot learning has been
underexplored. In this paper, we present a simple approach that uses both types
of PLMs for fully zero-shot learning of NLU tasks without requiring any
task-specific data: A unidirectional PLM generates class-conditioned texts
guided by prompts, which are used as the training data for fine-tuning a
bidirectional PLM. With quality training data selected based on the generation
probability and regularization techniques (label smoothing and temporal
ensembling) applied to the fine-tuning stage for better generalization and
stability, our approach demonstrates strong performance across seven
classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and
92.8 on SST-2), significantly outperforming zero-shot prompting methods and
achieving even comparable results to strong few-shot approaches using 32
training samples per class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12865">
<div class="article-summary-box-inner">
<span><p>Multilingual evaluation benchmarks usually contain limited high-resource
languages and do not test models for specific linguistic capabilities.
CheckList is a template-based evaluation approach that tests models for
specific capabilities. The CheckList template creation process requires native
speakers, posing a challenge in scaling to hundreds of languages. In this work,
we explore multiple approaches to generate Multilingual CheckLists. We device
an algorithm - Template Extraction Algorithm (TEA) for automatically extracting
target language CheckList templates from machine translated instances of a
source language templates. We compare the TEA CheckLists with CheckLists
created with different levels of human intervention. We further introduce
metrics along the dimensions of cost, diversity, utility, and correctness to
compare the CheckLists. We thoroughly analyze different approaches to creating
CheckLists in Hindi. Furthermore, we experiment with 9 more different
languages. We find that TEA followed by human verification is ideal for scaling
Checklist-based evaluation to multiple languages while TEA gives a good
estimates of model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09179">
<div class="article-summary-box-inner">
<span><p>Sparse mixture of experts provides larger model capacity while requiring a
constant computational overhead. It employs the routing mechanism to distribute
input tokens to the best-matched experts according to their hidden
representations. However, learning such a routing mechanism encourages token
clustering around expert centroids, implying a trend toward representation
collapse. In this work, we propose to estimate the routing scores between
tokens and experts on a low-dimensional hypersphere. We conduct extensive
experiments on cross-lingual language model pre-training and fine-tuning on
downstream tasks. Experimental results across seven multilingual benchmarks
show that our method achieves consistent gains. We also present a comprehensive
analysis on the representation and routing behaviors of our models. Our method
alleviates the representation collapse issue and achieves more consistent
routing than the baseline mixture-of-experts methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Revise References for Faithful Summarization. (arXiv:2204.10290v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10290">
<div class="article-summary-box-inner">
<span><p>In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech. (arXiv:2205.07211v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07211">
<div class="article-summary-box-inner">
<span><p>Style transfer for out-of-domain (OOD) speech synthesis aims to generate
speech samples with unseen style (e.g., speaker identity, emotion, and prosody)
derived from an acoustic reference, while facing the following challenges: 1)
The highly dynamic style features in expressive voice are difficult to model
and transfer; and 2) the TTS models should be robust enough to handle diverse
OOD conditions that differ from the source data. This paper proposes
GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style
transfer of OOD custom voice. GenerSpeech decomposes the speech variation into
the style-agnostic and style-specific parts by introducing two components: 1) a
multi-level style adaptor to efficiently model a large range of style
conditions, including global speaker and emotion characteristics, and the local
(utterance, phoneme, and word-level) fine-grained prosodic representations; and
2) a generalizable content adaptor with Mix-Style Layer Normalization to
eliminate style information in the linguistic content representation and thus
improve model generalization. Our evaluations on zero-shot style transfer
demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of
audio quality and style similarity. The extension studies to adaptive style
transfer further show that GenerSpeech performs robustly in the few-shot data
setting. Audio samples are available at \url{https://GenerSpeech.github.io/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Induction for Efficient and Interpretable Semi-Supervised Learning. (arXiv:2205.09067v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09067">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning has shown promise in allowing NLP models to
generalize from small amounts of labeled data. Meanwhile, pretrained
transformer models act as black-box correlation engines that are difficult to
explain and sometimes behave unreliably. In this paper, we propose tackling
both of these challenges via Automatic Rule Induction (ARI), a simple and
general-purpose framework for the automatic discovery and integration of
symbolic rules into pretrained transformer models. First, we extract weak
symbolic rules from low-capacity machine learning models trained on small
amounts of labeled data. Next, we use an attention mechanism to integrate these
rules into high-capacity pretrained transformer models. Last, the
rule-augmented system becomes part of a self-training framework to boost
supervision signal on unlabeled data. These steps can be layered beneath a
variety of existing weak supervision and semi-supervised NLP algorithms in
order to improve performance and interpretability. Experiments across nine
sequence classification and relation extraction tasks suggest that ARI can
improve state-of-the-art methods with no manual effort and minimal
computational overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation. (arXiv:2205.09661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09661">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogue systems, response generation from meaning
representations (MRs) often suffers from limited training examples, due to the
high cost of annotating MR-to-Text pairs. Previous works on self-training
leverage fine-tuned conversational models to automatically generate
pseudo-labeled MR-to-Text pairs for further fine-tuning. However, some
self-augmented data may be noisy or uninformative for the model to learn from.
In this work, we propose a two-phase self-augmentation procedure to generate
high-quality pseudo-labeled MR-to-Text pairs: the first phase selects the most
informative MRs based on model's prediction uncertainty; with the selected MRs,
the second phase generates accurate responses by aggregating multiple perturbed
latent representations from each MR. Empirical experiments on two benchmark
datasets, FewShotWOZ and FewShotSGD, show that our method generally outperforms
existing self-training methods on both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Label Errors using Pre-Trained Language Models. (arXiv:2205.12702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12702">
<div class="article-summary-box-inner">
<span><p>For identifying label errors in natural language datasets, we show that large
pre-trained language models are extremely capable: simply hand-verifying data
points in descending order of out-of-distribution model loss significantly
outperforms more complex mechanisms proposed in previous work.
</p>
<p>We also contribute a novel method for producing human-originated label noise
using existing crowdsourced datasets, apply it to SNLI and TweetNLP, and show
that the resulting errors have similar characteristics to real label errors. We
present evidence that pre-training provides limited robustness to this more
realistic form of label noise.
</p>
<p>Finally, we use crowdsourced verification to evaluate performance on IMDB,
Amazon Reviews, and Recon, and find that pre-trained models detect errors with
9-36% higher absolute Area Under the Precision-Recall Curve compared to
existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14014">
<div class="article-summary-box-inner">
<span><p>Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. (arXiv:2205.14140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14140">
<div class="article-summary-box-inner">
<span><p>The increasing size and complexity of modern ML systems has improved their
predictive capabilities but made their behavior harder to explain. Many
techniques for model explanation have been developed in response, but we lack
clear criteria for assessing these techniques. In this paper, we cast model
explanation as the causal inference problem of estimating causal effects of
real-world concepts on the output behavior of ML models given actual input
data. We introduce CEBaB, a new benchmark dataset for assessing concept-based
explanation methods in Natural Language Processing (NLP). CEBaB consists of
short restaurant reviews with human-generated counterfactual reviews in which
an aspect (food, noise, ambiance, service) of the dining experience was
modified. Original and counterfactual reviews are annotated with
multiply-validated sentiment ratings at the aspect-level and review-level. The
rich structure of CEBaB allows us to go beyond input features to study the
effects of abstract, real-world concepts on model behavior. We use CEBaB to
compare the quality of a range of concept-based explanation methods covering
different assumptions and conceptions of the problem, and we seek to establish
natural metrics for comparative assessments of these methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder. (arXiv:2206.02512v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02512">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel unsupervised text-to-speech (UTTS)
framework which does not require text-audio pairs for the TTS acoustic modeling
(AM). UTTS is a multi-speaker speech synthesizer that supports zero-shot voice
cloning, it is developed from a perspective of disentangled speech
representation learning. The framework offers a flexible choice of a speaker's
duration model, timbre feature (identity) and content for TTS inference. We
leverage recent advancements in self-supervised speech representation learning
as well as speech synthesis front-end techniques for system development.
Specifically, we employ our recently formulated Conditional Disentangled
Sequential Variational Auto-encoder (C-DSVAE) as the backbone UTTS AM, which
offers well-structured content representations given unsupervised alignment
(UA) as condition during training. For UTTS inference, we utilize a lexicon to
map input text to the phoneme sequence, which is expanded to the frame-level
forced alignment (FA) with a speaker-dependent duration model. Then, we develop
an alignment mapping module that converts FA to UA. Finally, the C-DSVAE,
serving as the self-supervised TTS AM, takes the predicted UA and a target
speaker embedding to generate the mel spectrogram, which is ultimately
converted to waveform with a neural vocoder. We show how our method enables
speech synthesis without using a paired TTS corpus. Experiments demonstrate
that UTTS can synthesize speech of high naturalness and intelligibility
measured by human and objective evaluations. Audio samples are available at our
demo page https://neurtts.github.io/utts_demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04045">
<div class="article-summary-box-inner">
<span><p>The output structure of database-like tables, consisting of values structured
in horizontal rows and vertical columns identifiable by name, can cover a wide
range of NLP tasks. Following this constatation, we propose a framework for
text-to-table neural models applicable to problems such as extraction of line
items, joint entity and relation extraction, or knowledge base population. The
permutation-based decoder of our proposal is a generalized sequential method
that comprehends information from all cells in the table. The training
maximizes the expected log-likelihood for a table's content across all random
permutations of the factorization order. During the content inference, we
exploit the model's ability to generate cells in any order by searching over
possible orderings to maximize the model's confidence and avoid substantial
error accumulation, which other sequential models are prone to. Experiments
demonstrate a high practical value of the framework, which establishes
state-of-the-art results on several challenging datasets, outperforming
previous solutions by up to 15%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05836">
<div class="article-summary-box-inner">
<span><p>We present GLIPv2, a grounded VL understanding model, that serves both
localization tasks (e.g., object detection, instance segmentation) and
Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2
elegantly unifies localization pre-training and Vision-Language Pre-training
(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of
the detection task, region-word contrastive learning as a novel region-word
level contrastive learning task, and the masked language modeling. This
unification not only simplifies the previous multi-stage VLP procedure but also
achieves mutual benefits between localization and understanding tasks.
Experimental results show that a single GLIPv2 model (all model weights are
shared) achieves near SoTA performance on various localization and
understanding tasks. The model also shows (1) strong zero-shot and few-shot
adaption performance on open-vocabulary object detection tasks and (2) superior
grounding capability on VL understanding tasks. Code will be released at
https://github.com/microsoft/GLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models. (arXiv:2208.11445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.11445">
<div class="article-summary-box-inner">
<span><p>The ability to extrapolate, i.e., to make predictions on sequences that are
longer than those presented as training examples, is a challenging problem for
current deep learning models. Recent work shows that this limitation persists
in state-of-the-art Transformer-based models. Most solutions to this problem
use specific architectures or training methods that do not generalize to other
tasks. We demonstrate that large language models can succeed in extrapolation
without modifying their architecture or training procedure. Our experimental
results show that generating step-by-step rationales and introducing marker
tokens are both required for effective extrapolation. First, we induce a
language model to produce step-by-step rationales before outputting the answer
to effectively communicate the task to the model. However, as sequences become
longer, we find that current models struggle to keep track of token positions.
To address this issue, we interleave output tokens with markup tokens that act
as explicit positional and counting symbols. Our findings show how these two
complementary approaches enable remarkable sequence extrapolation and highlight
a limitation of current architectures to effectively generalize without
explicit surface form guidance. Code available at
https://github.com/MirelleB/induced-rationales-markup-tokens
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10372">
<div class="article-summary-box-inner">
<span><p>Large Language Models pre-trained with self-supervised learning have
demonstrated impressive zero-shot generalization capabilities on a wide
spectrum of tasks. In this work, we present WeLM: a well-read pre-trained
language model for Chinese that is able to seamlessly perform different types
of tasks with zero or few-shot demonstrations. WeLM is trained with 10B
parameters by "reading" a curated high-quality corpus covering a wide range of
topics. We show that WeLM is equipped with broad knowledge on various domains
and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly
outperform existing pre-trained models with similar sizes and match the
performance of models up to 25 times larger. WeLM also exhibits strong
capabilities in multi-lingual and code-switching understanding, outperforming
existing multilingual language models pre-trained on 30 languages. Furthermore,
We collected human-written prompts for a large set of supervised datasets in
Chinese and fine-tuned WeLM with multi-prompted training. The resulting model
can attain strong generalization on unseen types of tasks and outperform the
unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has
basic skills at explaining and calibrating the decisions from itself, which can
be promising directions for future research. Our models can be applied from
https://welm.weixin.qq.com/docs/api/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12226">
<div class="article-summary-box-inner">
<span><p>Recent research has revealed undesirable bi-ases in NLP data and models.
However, theseefforts focus of social disparities in West, andare not directly
portable to other geo-culturalcontexts. In this paper, we focus on NLP
fair-ness in the context of India. We start witha brief account of the
prominent axes of so-cial disparities in India. We build resourcesfor fairness
evaluation in the Indian contextand use them to demonstrate prediction bi-ases
along some of the axes. We then delvedeeper into social stereotypes for Region
andReligion, demonstrating its prevalence in cor-pora and models. Finally, we
outline a holis-tic research agenda to re-contextualize NLPfairness research
for the Indian context, ac-counting for Indiansocietal context,
bridgingtechnologicalgaps in NLP capabilities and re-sources, and adapting to
Indian culturalvalues.While we focus on India, this framework canbe generalized
to other geo-cultural contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions. (arXiv:2209.15565v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15565">
<div class="article-summary-box-inner">
<span><p>We describe an augmented intelligence system for simplifying and enhancing
the modeling experience for operations research. Using this system, the user
receives a suggested formulation of an optimization problem based on its
description. To facilitate this process, we build an intuitive user interface
system that enables the users to validate and edit the suggestions. We
investigate controlled generation techniques to obtain an automatic suggestion
of formulation. Then, we evaluate their effectiveness with a newly created
dataset of linear programming problems drawn from various application domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02762">
<div class="article-summary-box-inner">
<span><p>Visual Story-Telling is the process of forming a multi-sentence story from a
set of images. Appropriately including visual variation and contextual
information captured inside the input images is one of the most challenging
aspects of visual storytelling. Consequently, stories developed from a set of
images often lack cohesiveness, relevance, and semantic relationship. In this
paper, we propose a novel Vision Transformer Based Model for describing a set
of images as a story. The proposed method extracts the distinct features of the
input images using a Vision Transformer (ViT). Firstly, input images are
divided into 16X16 patches and bundled into a linear projection of flattened
patches. The transformation from a single image to multiple image patches
captures the visual variety of the input visual patterns. These features are
used as input to a Bidirectional-LSTM which is part of the sequence encoder.
This captures the past and future image context of all image patches. Then, an
attention mechanism is implemented and used to increase the discriminatory
capacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The
performance of our proposed model is evaluated using the Visual Story-Telling
dataset (VIST), and the results show that our model outperforms the current
state of the art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04074">
<div class="article-summary-box-inner">
<span><p>Natural language often describes events in different granularities, such that
more coarse-grained (goal) events can often be decomposed into fine-grained
sequences of (step) events. A critical but overlooked challenge in
understanding an event process lies in the fact that the step events are not
equally important to the central goal. In this paper, we seek to fill this gap
by studying how well current models can understand the essentiality of
different step events towards a goal event. As discussed by cognitive studies,
such an ability enables the machine to mimic human's commonsense reasoning
about preconditions and necessary efforts of daily-life tasks. Our work
contributes with a high-quality corpus of (goal, step) pairs from a community
guideline website WikiHow, where the steps are manually annotated with their
essentiality w.r.t. the goal. The high IAA indicates that humans have a
consistent understanding of the events. Despite evaluating various statistical
and massive pre-trained NLU models, we observe that existing SOTA models all
perform drastically behind humans, indicating the need for future investigation
of this crucial yet challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue. (arXiv:2210.04443v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04443">
<div class="article-summary-box-inner">
<span><p>Embodied dialogue instruction following requires an agent to complete a
complex sequence of tasks from a natural language exchange. The recent
introduction of benchmarks (Padmakumar et al., 2022) raises the question of how
best to train and evaluate models for this multi-turn, multi-agent,
long-horizon task. This paper contributes to that conversation, by arguing that
imitation learning (IL) and related low-level metrics are actually misleading
and do not align with the goals of embodied dialogue research and may hinder
progress. We provide empirical comparisons of metrics, analysis of three
models, and make suggestions for how the field might best progress. First, we
observe that models trained with IL take spurious actions during evaluation.
Second, we find that existing models fail to ground query utterances, which are
essential for task completion. Third, we argue evaluation should focus on
higher-level semantic goals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YFACC: A Yor\`ub\'a speech-image dataset for cross-lingual keyword localisation through visual grounding. (arXiv:2210.04600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04600">
<div class="article-summary-box-inner">
<span><p>Visually grounded speech (VGS) models are trained on images paired with
unlabelled spoken captions. Such models could be used to build speech systems
in settings where it is impossible to get labelled data, e.g. for documenting
unwritten languages. However, most VGS studies are in English or other
high-resource languages. This paper attempts to address this shortcoming. We
collect and release a new single-speaker dataset of audio captions for 6k
Flickr images in Yor\`ub\'a -- a real low-resource language spoken in Nigeria.
We train an attention-based VGS model where images are automatically tagged
with English visual labels and paired with Yor\`ub\'a utterances. This enables
cross-lingual keyword localisation: a written English query is detected and
located in Yor\`ub\'a speech. To quantify the effect of the smaller dataset, we
compare to English systems trained on similar and more data. We hope that this
new dataset will stimulate research in the use of VGS models for real
low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Readability Controllable Biomedical Document Summarization. (arXiv:2210.04705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04705">
<div class="article-summary-box-inner">
<span><p>Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Neural Referential Form Selectors on a Realistic Multilingual Dataset. (arXiv:2210.04828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04828">
<div class="article-summary-box-inner">
<span><p>Previous work on Neural Referring Expression Generation (REG) all uses
WebNLG, an English dataset that has been shown to reflect a very limited range
of referring expression (RE) use. To tackle this issue, we build a dataset
based on the OntoNotes corpus that contains a broader range of RE use in both
English and Chinese (a language that uses zero pronouns). We build neural
Referential Form Selection (RFS) models accordingly, assess them on the dataset
and conduct probing experiments. The experiments suggest that, compared to
WebNLG, OntoNotes is better for assessing REG/RFS models. We compare English
and Chinese RFS and confirm that, in line with linguistic theories, Chinese RFS
depends more on discourse context than English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04992">
<div class="article-summary-box-inner">
<span><p>In this paper, we seek to improve the faithfulness of TempRel extraction
models from two perspectives. The first perspective is to extract genuinely
based on contextual description. To achieve this, we propose to conduct
counterfactual analysis to attenuate the effects of two significant types of
training biases: the event trigger bias and the frequent label bias. We also
add tense information into event representations to explicitly place an
emphasis on the contextual description. The second perspective is to provide
proper uncertainty estimation and abstain from extraction when no relation is
described in the text. By parameterization of Dirichlet Prior over the
model-predicted categorical distribution, we improve the model estimates of the
correctness likelihood and make TempRel predictions more selective. We also
employ temperature scaling to recalibrate the model confidence measure after
bias mitigation. Through experimental analysis on MATRES, MATRES-DS, and
TDDiscourse, we demonstrate that our model extracts TempRel and timelines more
faithfully compared to SOTA methods, especially under distribution shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05245">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction is the process of automatically selecting a small set of
most relevant phrases from a given text. Supervised keyphrase extraction
approaches need large amounts of labeled training data and perform poorly
outside the domain of the training data. In this paper, we present PatternRank,
which leverages pretrained language models and part-of-speech for unsupervised
keyphrase extraction from single documents. Our experiments show PatternRank
achieves higher precision, recall and F1-scores than previous state-of-the-art
approaches. In addition, we present the KeyphraseVectorizers package, which
allows easy modification of part-of-speech patterns for candidate keyphrase
selection, and hence adaptation of our approach to any domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training. (arXiv:2210.05287v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05287">
<div class="article-summary-box-inner">
<span><p>Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve
context-aware representations via learning from structured relations in
knowledge graphs, and/or linguistic knowledge from syntactic or dependency
analysis. Unlike English, there is a lack of high-performing open-source
Chinese KEPLMs in the natural language processing (NLP) community to support
various language understanding applications. In this paper, we revisit and
advance the development of Chinese natural language understanding with a series
of novel Chinese KEPLMs released in various parameter sizes, namely CKBERT
(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic
knowledge is effectively injected into CKBERT based on two novel pre-training
tasks, i.e., linguistic-aware masked language modeling and contrastive
multi-hop relation modeling. Based on the above two pre-training paradigms and
our in-house implemented TorchAccelerator, we have pre-trained base (110M),
large (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.
Experiments demonstrate that CKBERT outperforms strong baselines for Chinese
over various benchmark NLP tasks and in terms of different model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05425">
<div class="article-summary-box-inner">
<span><p>Billions of people across the globe have been using social media platforms in
their local languages to voice their opinions about the various topics related
to the COVID-19 pandemic. Several organizations, including the World Health
Organization, have developed automated social media analysis tools that
classify COVID-19-related tweets into various topics. However, these tools that
help combat the pandemic are limited to very few languages, making several
countries unable to take their benefit. While multi-lingual or low-resource
language-specific tools are being developed, they still need to expand their
coverage, such as for the Nepali language. In this paper, we identify the eight
most common COVID-19 discussion topics among the Twitter community using the
Nepali language, set up an online platform to automatically gather Nepali
tweets containing the COVID-19-related keywords, classify the tweets into the
eight topics, and visualize the results across the period in a web-based
dashboard. We compare the performance of two state-of-the-art multi-lingual
language models for Nepali tweet classification, one generic (mBERT) and the
other Nepali language family-specific model (MuRIL). Our results show that the
models' relative performance depends on the data size, with MuRIL doing better
for a larger dataset. The annotated data, models, and the web-based dashboard
are open-sourced at https://github.com/naamiinepal/covid-tweet-classification.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-13 23:21:03.463557701 UTC">2022-10-13 23:21:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>