<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-12T01:30:00Z">05-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Accessible Instruction-Following Agent. (arXiv:2305.06358v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06358">
<div class="article-summary-box-inner">
<span><p>Humans can collaborate and complete tasks based on visual signals and
instruction from the environment. Training such a robot is difficult especially
due to the understanding of the instruction and the complicated environment.
Previous instruction-following agents are biased to English-centric corpus,
making it unrealizable to be applied to users that use multiple languages or
even low-resource languages. Nevertheless, the instruction-following agents are
pre-trained in a mode that assumes the user can observe the environment, which
limits its accessibility. In this work, we're trying to generalize the success
of instruction-following agents to non-English languages with little corpus
resources, and improve its intractability and accessibility. We introduce UVLN
(Universal Vision-Language Navigation), a novel machine-translation
instructional augmented framework for cross-lingual vision-language navigation,
with a novel composition of state-of-the-art large language model (GPT3) with
the image caption model (BLIP). We first collect a multilanguage
vision-language navigation dataset via machine translation. Then we extend the
standard VLN training objectives to a multilingual setting via a cross-lingual
language encoder. The alignment between different languages is captured through
a shared vision and action context via a cross-modal transformer, which encodes
the inputs of language instruction, visual observation, and action decision
sequences. To improve the intractability, we connect our agent with the large
language model that informs the situation and current state to the user and
also explains the action decisions. Experiments over Room Across Room Dataset
prove the effectiveness of our approach. And the qualitative results show the
promising intractability and accessibility of our instruction-following agent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM. (arXiv:2305.06404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06404">
<div class="article-summary-box-inner">
<span><p>Text embeddings are useful features for several NLP applications, such as
sentence similarity, text clustering, and semantic search. In this paper, we
present a Low-rank Adaptation with a Contrastive objective on top of 8-bit
Siamese-BLOOM, a multilingual large language model optimized to produce
semantically meaningful word embeddings. The innovation is threefold. First, we
cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable
adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.
Third, we apply a Siamese architecture on BLOOM model with a contrastive
objective to ease the multi-lingual labeled data scarcity. The experiment
results show the quality of learned embeddings from LACoS-BLOOM is proportional
to the number of model parameters and the amount of unlabeled training data.
With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1
billion parameters end-to-end on a single GPU machine with 32GB memory.
Compared to previous solution Sentence-BERT, we achieve significant improvement
on both English and multi-lingual STS tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06416">
<div class="article-summary-box-inner">
<span><p>Generation of automated clinical notes have been posited as a strategy to
mitigate physician burnout. In particular, an automated narrative summary of a
patient's hospital stay could supplement the hospital course section of the
discharge summary that inpatient physicians document in electronic health
record (EHR) systems. In the current study, we developed and evaluated an
automated method for summarizing the hospital course section using
encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and
BART models and optimized for factuality through constraining beam search,
which we trained and tested using EHR data from patients admitted to the
neurology unit of an academic medical center. The approach demonstrated good
ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified
physicians rated 62% of the automated summaries as meeting the standard of
care, which suggests the method may be useful clinically. To our knowledge,
this study is among the first to demonstrate an automated method for generating
a discharge summary hospital course that approaches a quality level of what a
physician would write.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06424">
<div class="article-summary-box-inner">
<span><p>Large language models like ChatGPT have recently demonstrated impressive
capabilities in natural language understanding and generation, enabling various
applications including translation, essay writing, and chit-chatting. However,
there is a concern that they can be misused for malicious purposes, such as
fraud or denial-of-service attacks. Therefore, it is crucial to develop methods
for detecting whether the party involved in a conversation is a bot or a human.
In this paper, we propose a framework named FLAIR, Finding Large language model
Authenticity via a single Inquiry and Response, to detect conversational bots
in an online manner. Specifically, we target a single question scenario that
can effectively differentiate human users from bots. The questions are divided
into two categories: those that are easy for humans but difficult for bots
(e.g., counting, substitution, positioning, noise filtering, and ASCII art),
and those that are easy for bots but difficult for humans (e.g., memorization
and computation). Our approach shows different strengths of these questions in
their effectiveness, providing a new way for online service providers to
protect themselves against nefarious activities and ensure that they are
serving real users. We open-sourced our dataset on
https://github.com/hongwang600/FLAIR and welcome contributions from the
community to enrich such detection datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning. (arXiv:2305.06429v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06429">
<div class="article-summary-box-inner">
<span><p>In Islam, readers must apply a set of pronunciation rules called Tajweed
rules to recite the Quran in the same way that the angel Jibrael taught the
Prophet, Muhammad. The traditional process of learning the correct application
of these rules requires a human who must have a license and great experience to
detect mispronunciation. Due to the increasing number of Muslims around the
world, the number of Tajweed teachers is not enough nowadays for daily
recitation practice for every Muslim. Therefore, lots of work has been done for
automatic Tajweed rules' mispronunciation detection to help readers recite
Quran correctly in an easier way and shorter time than traditional learning
ways. All previous works have three common problems. First, most of them
focused on machine learning algorithms only. Second, they used private datasets
with no benchmark to compare with. Third, they did not take into consideration
the sequence of input data optimally, although the speech signal is time
series. To overcome these problems, we proposed a solution that consists of
Mel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory
(LSTM) neural networks which use the time series, to detect mispronunciation in
Tajweed rules. In addition, our experiments were performed on a public dataset,
the QDAT dataset, which contains more than 1500 voices of the correct and
incorrect recitation of three Tajweed rules (Separate stretching , Tight Noon ,
and Hide ). To the best of our knowledge, the QDAT dataset has not been used by
any research paper yet. We compared the performance of the proposed LSTM model
with traditional machine learning algorithms used in SoTA. The LSTM model with
time series showed clear superiority over traditional machine learning. The
accuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for the
three rules (Separate stretching, Tight Noon, and Hide), respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Grounded Graph Convolutional Network. (arXiv:2305.06434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06434">
<div class="article-summary-box-inner">
<span><p>Graph Convolutional Networks (GCNs) have shown strong performance in learning
text representations for various tasks such as text classification, due to its
expressive power in modeling graph structure data (e.g., a literature citation
network). Most existing GCNs are limited to deal with documents included in a
pre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To
address this issue, we propose to transform the document graph into a word
graph, to decouple data samples (i.e., documents in training and test sets) and
a GCN model by using a document-independent graph. Such word-level GCN could
therefore naturally inference out-of-graph documents in an inductive way. The
proposed Word-level Graph (WGraph) can not only implicitly learning word
presentation with commonly-used word co-occurrences in corpora, but also
incorporate extra global semantic dependency derived from inter-document
relationships (e.g., literature citations). An inductive Word-grounded Graph
Convolutional Network (WGCN) is proposed to learn word and document
representations based on WGraph in a supervised manner. Experiments on text
classification with and without citation networks evidence that the proposed
WGCN model outperforms existing methods in terms of effectiveness and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06472">
<div class="article-summary-box-inner">
<span><p>Prognostics and health management (PHM) technology plays a critical role in
industrial production and equipment maintenance by identifying and predicting
possible equipment failures and damages, thereby allowing necessary maintenance
measures to be taken to enhance equipment service life and reliability while
reducing production costs and downtime. In recent years, PHM technology based
on artificial intelligence (AI) has made remarkable achievements in the context
of the industrial IoT and big data, and it is widely used in various
industries, such as railway, energy, and aviation, for condition monitoring,
fault prediction, and health management. The emergence of large-scale
foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of
AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved
from a research paradigm of single-modal, single-task, and limited-data to a
multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT
represents a landmark achievement in this research paradigm, offering hope for
general artificial intelligence due to its highly intelligent natural language
understanding ability. However, the PHM field lacks a consensus on how to
respond to this significant change in the AI field, and a systematic review and
roadmap is required to elucidate future development directions. To fill this
gap, this paper systematically expounds on the key components and latest
developments of LSF-Models. Then, we systematically answered how to build the
LSF-Model applicable to PHM tasks and outlined the challenges and future
development roadmaps for this research paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Model for Translation of Text from Indian Languages to Bharti Braille Characters. (arXiv:2305.06475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06475">
<div class="article-summary-box-inner">
<span><p>People who are visually impaired face a lot of difficulties while studying.
One of the major causes to this is lack of available text in Bharti Braille
script. In this paper, we have suggested a scheme to convert text in major
Indian languages into Bharti Braille. The system uses a hybrid approach where
at first the text in Indian language is given to a rule based system and in
case if there is any ambiguity then it is resolved by applying a LSTM based
model. The developed model has also been tested and found to have produced near
accurate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Contextualized Plan Prediction for Embodied Task Completion. (arXiv:2305.06485v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06485">
<div class="article-summary-box-inner">
<span><p>Task planning is an important component of traditional robotics systems
enabling robots to compose fine grained skills to perform more complex tasks.
Recent work building systems for translating natural language to executable
actions for task completion in simulated embodied agents is focused on directly
predicting low level action sequences that would be expected to be directly
executable by a physical robot. In this work, we instead focus on predicting a
higher level plan representation for one such embodied task completion dataset
- TEACh, under the assumption that techniques for high-level plan prediction
from natural language are expected to be more transferable to physical robot
systems. We demonstrate that better plans can be predicted using multimodal
context, and that plan prediction and plan execution modules are likely
dependent on each other and hence it may not be ideal to fully decouple them.
Further, we benchmark execution of oracle plans to quantify the scope for
improvement in plan prediction models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (arXiv:2305.06522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06522">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06530">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Natural Language Processing (NLP) has led to the
proliferation of large pretrained language models. These models have been shown
to yield good performance, using in-context learning, even on unseen tasks and
languages. They have also been exposed as commercial APIs as a form of
language-model-as-a-service, with great adoption. However, their performance on
African languages is largely unknown. We present a preliminary analysis of
commercial large language models on two tasks (machine translation and text
classification) across eight African languages, spanning different language
families and geographical areas. Our results suggest that commercial language
models produce below-par performance on African languages. We also find that
they perform better on text classification than machine translation. In
general, our findings present a call-to-action to ensure African languages are
well represented in commercial large language models, given their growing
popularity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment. (arXiv:2305.06535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06535">
<div class="article-summary-box-inner">
<span><p>Recent legislation of the "right to be forgotten" has led to the interest in
machine unlearning, where the learned models are endowed with the function to
forget information about specific training instances as if they have never
existed in the training set. Previous work mainly focuses on computer vision
scenarios and largely ignores the essentials of unlearning in NLP field, where
text data contains more explicit and sensitive personal information than
images. In this paper, we propose a general unlearning framework called KGA to
induce forgetfulness. Different from previous work that tries to recover
gradients or forces models to perform close to one specific distribution, KGA
maintains distribution differences (i.e., knowledge gap). This relaxes the
distribution assumption. Furthermore, we first apply the unlearning method to
various NLP tasks (i.e., classification, translation, response generation) and
propose several unlearning evaluation metrics with pertinence. Experiments on
large-scale datasets show that KGA yields comprehensive improvements over
baselines, where extensive analyses further validate the effectiveness of KGA
and provide insight into unlearning for NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic uncertainty guides the extension of conventions to new referents. (arXiv:2305.06539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06539">
<div class="article-summary-box-inner">
<span><p>A long tradition of studies in psycholinguistics has examined the formation
and generalization of ad hoc conventions in reference games, showing how newly
acquired conventions for a given target transfer to new referential contexts.
However, another axis of generalization remains understudied: how do
conventions formed for one target transfer to completely distinct targets, when
specific lexical choices are unlikely to repeat? This paper presents two dyadic
studies (N = 240) that address this axis of generalization, focusing on the
role of nameability -- the a priori likelihood that two individuals will share
the same label. We leverage the recently-released KiloGram dataset, a
collection of abstract tangram images that is orders of magnitude larger than
previously available, exhibiting high diversity of properties like nameability.
Our first study asks how nameability shapes convention formation, while the
second asks how new conventions generalize to entirely new targets of
reference. Our results raise new questions about how ad hoc conventions extend
beyond target-specific re-use of specific lexical choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark. (arXiv:2305.06545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06545">
<div class="article-summary-box-inner">
<span><p>With a fast developing pace of geographic applications, automatable and
intelligent models are essential to be designed to handle the large volume of
information. However, few researchers focus on geographic natural language
processing, and there has never been a benchmark to build a unified standard.
In this work, we propose a GeoGraphic Language Understanding Evaluation
benchmark, named GeoGLUE. We collect data from open-released geographic
resources and introduce six natural language understanding tasks, including
geographic textual similarity on recall, geographic textual similarity on
rerank, geographic elements tagging, geographic composition analysis,
geographic where what cut, and geographic entity alignment. We also pro vide
evaluation experiments and analysis of general baselines, indicating the
effectiveness and significance of the GeoGLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06555">
<div class="article-summary-box-inner">
<span><p>Lifelong learning (LL) is an important ability for NLP models to learn new
tasks continuously. Architecture-based approaches are reported to be effective
implementations for LL models. However, it is non-trivial to extend previous
approaches to domain incremental LL scenarios since they either require access
to task identities in the testing phase or cannot handle samples from unseen
tasks. In this paper, we propose \textbf{Diana}: a
\underline{d}ynam\underline{i}c \underline{a}rchitecture-based
lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence
of tasks with a prompt-enhanced language model. Four types of hierarchically
organized prompts are used in Diana to capture knowledge from different
granularities. Specifically, we dedicate task-level prompts to capture
task-specific knowledge to retain high LL performances and maintain
instance-level prompts to learn knowledge shared across input samples to
improve the model's generalization performance. Moreover, we dedicate separate
prompts to explicitly model unseen tasks and introduce a set of prompt key
vectors to facilitate knowledge sharing between tasks. Extensive experiments
demonstrate that Diana outperforms state-of-the-art LL models, especially in
handling unseen tasks. We release the code and data at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Tailed Question Answering in an Open World. (arXiv:2305.06557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06557">
<div class="article-summary-box-inner">
<span><p>Real-world data often have an open long-tailed distribution, and building a
unified QA model supporting various tasks is vital for practical QA
applications. However, it is non-trivial to extend previous QA approaches since
they either require access to seen tasks of adequate samples or do not
explicitly model samples from unseen tasks. In this paper, we define Open
Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and
optimizing performance over seen and unseen QA tasks. We propose an OLTQA model
that encourages knowledge sharing between head, tail and unseen tasks, and
explicitly mines knowledge from a large pre-trained language model (LM).
Specifically, we organize our model through a pool of fine-grained components
and dynamically combine these components for an input to facilitate knowledge
sharing. A retrieve-then-rerank frame is further introduced to select
in-context examples, which guild the LM to generate text that express knowledge
for QA tasks. Moreover, a two-stage training approach is introduced to
pre-train the framework by knowledge distillation (KD) from the LM and then
jointly train the frame and a QA model through an adaptive mutual KD method. On
a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model
consistently outperforms the state-of-the-art. We release the code and data at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06566">
<div class="article-summary-box-inner">
<span><p>Personalized news recommendation systems have become essential tools for
users to navigate the vast amount of online news content, yet existing news
recommenders face significant challenges such as the cold-start problem, user
profile modeling, and news content understanding. Previous works have typically
followed an inflexible routine to address a particular challenge through model
design, but are limited in their ability to understand news content and capture
user interests. In this paper, we introduce GENRE, an LLM-powered generative
news recommendation framework, which leverages pretrained semantic knowledge
from large language models to enrich news data. Our aim is to provide a
flexible and unified solution for news recommendation by moving from model
design to prompt design. We showcase the use of GENRE for personalized news
generation, user profiling, and news summarization. Extensive experiments with
various popular recommendation models demonstrate the effectiveness of GENRE.
We will publish our code and data for other researchers to reproduce our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06569">
<div class="article-summary-box-inner">
<span><p>Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment. (arXiv:2305.06574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06574">
<div class="article-summary-box-inner">
<span><p>Entity alignment is the task of identifying corresponding entities across
different knowledge graphs (KGs). Although recent embedding-based entity
alignment methods have shown significant advancements, they still struggle to
fully utilize KG structural information. In this paper, we introduce FGWEA, an
unsupervised entity alignment framework that leverages the Fused
Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of
entity semantics and KG structures within a joint optimization framework. To
address the computational challenges associated with optimizing FGW, we devise
a three-stage progressive optimization algorithm. It starts with a basic
semantic embedding matching, proceeds to approximate cross-KG structural and
relational similarity matching based on iterative updates of high-confidence
entity links, and ultimately culminates in a global structural comparison
between KGs. We perform extensive experiments on four entity alignment datasets
covering 14 distinct KGs across five languages. Without any supervision or
hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including
cutting-edge supervised entity alignment methods. Our code is available at
https://github.com/squareRoot3/FusedGW-Entity-Alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06575">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown surprisingly good performance in
multilingual neural machine translation (MNMT) even when trained without
parallel data. Yet, despite the fact that the amount of training data is
gigantic, they still struggle with translating rare words, particularly for
low-resource languages. Even worse, it is usually unrealistic to retrieve
relevant demonstrations for in-context learning with low-resource languages on
LLMs, which restricts the practical use of LLMs for translation -- how should
we mitigate this problem? To this end, we present a novel method, CoD, which
augments LLMs with prior knowledge with the chains of multilingual dictionaries
for a subset of input words to elicit translation abilities for LLMs. Extensive
experiments indicate that augmenting ChatGPT with CoD elicits large gains by up
to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in
Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the
importance of chaining the multilingual dictionaries, as well as the
superiority of CoD to few-shot demonstration for low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06586">
<div class="article-summary-box-inner">
<span><p>We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task
focused on methods to identify complex fine-grained named entities (like
WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and
multilingual scenarios, as well as noisy settings. The task used the MultiCoNER
V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and
Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It
attracted 842 submissions from 47 teams, and 34 teams submitted system papers.
Results showed that complex entity types such as media titles and product names
were the most challenging. Methods fusing external knowledge into transformer
models achieved the best performance, and the largest gains were on the
Creative Work and Group classes, which are still challenging even with external
knowledge. Some fine-grained classes proved to be more challenging than others,
such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data
has a significant impact on model performance, with an average drop of 10% on
the noisy subset. The task highlights the need for future research on improving
NER robustness on noisy data containing complex entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06590">
<div class="article-summary-box-inner">
<span><p>In real world applications, knowledge graphs (KG) are widely used in various
domains (e.g. medical applications and dialogue agents). However, for fact
verification, KGs have not been adequately utilized as a knowledge source. KGs
can be a valuable knowledge source in fact verification due to their
reliability and broad applicability. A KG consists of nodes and edges which
makes it clear how concepts are linked together, allowing machines to reason
over chains of topics. However, there are many challenges in understanding how
these machine-readable concepts map to information in text. To enable the
community to better use KGs, we introduce a new dataset, FactKG: Fact
Verification via Reasoning on Knowledge Graphs. It consists of 108k natural
language claims with five types of reasoning: One-hop, Conjunction, Existence,
Multi-hop, and Negation. Furthermore, FactKG contains various linguistic
patterns, including colloquial style claims as well as written style claims to
increase practicality. Lastly, we develop a baseline approach and analyze
FactKG over these reasoning types. We believe FactKG can advance both
reliability and practicality in KG-based fact verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06595">
<div class="article-summary-box-inner">
<span><p>The analysis of consumer sentiment, as expressed through reviews, can provide
a wealth of insight regarding the quality of a product. While the study of
sentiment analysis has been widely explored in many popular languages,
relatively less attention has been given to the Bangla language, mostly due to
a lack of relevant data and cross-domain adaptability. To address this
limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews
consisting of 158,065 samples classified into three broad categories: positive,
negative, and neutral. We provide a detailed statistical analysis of the
dataset and employ a range of machine learning models to establish baselines
including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial
performance advantage of pre-trained models over models that rely on manually
crafted features, emphasizing the necessity for additional training resources
in this domain. Additionally, we conduct an in-depth error analysis by
examining sentiment unigrams, which may provide insight into common
classification errors in under-resourced languages like Bangla. Our codes and
data are publicly available at https://github.com/mohsinulkabir14/BanglaBook.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Programming Thinking in Large Language Models Toward Code Generation. (arXiv:2305.06599v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06599">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive
performance in code generation. A large-scale study released that writing
programs requires programming thinking, i.e., analyzing and implementing
requirements in programming logic (e.g., sequence, branch, loop). Existing
studies use LLMs to generate programs from requirements directly and do not
explicitly introduce the programming thinking.
</p>
<p>This paper explores how to unlock the programming thinking of LLMs in code
generation and proposes an approach named TiP. Our idea is to decompose code
generation into two steps and progressively lead LLMs to analyze&amp;implement
requirements in programming logic. Specifically, TiP first generates a code
sketch, which provides a high-level solving process using programming logic but
omits implementation details (e.g., APIs). Then, TiP implements the sketch into
a program using specific programming languages. We conduct extensive
experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1)
TiP outperforms the state-of-the-art baseline - ChatGPT by up to 17.5% in
Pass@1, 11.02% in Pass@3, and 9.84% in Pass@5. (2) Human evaluation shows that
TiP outperforms ChatGPT in three aspects (i.e., correctness, code quality, and
maintainability). (3) TiP is effective for different LLMs. (4) We explore
multiple choices (e.g., chain-of-thought) for the code sketch and validate the
superiority of our design. (5) We discuss the complementarity between TiP and
post-processing approaches (e.g., CodeT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autocorrelations Decay in Texts and Applicability Limits of Language Models. (arXiv:2305.06615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06615">
<div class="article-summary-box-inner">
<span><p>We show that the laws of autocorrelations decay in texts are closely related
to applicability limits of language models. Using distributional semantics we
empirically demonstrate that autocorrelations of words in texts decay according
to a power law. We show that distributional semantics provides coherent
autocorrelations decay exponents for texts translated to multiple languages.
The autocorrelations decay in generated texts is quantitatively and often
qualitatively different from the literary texts. We conclude that language
models exhibiting Markov behavior, including large autoregressive language
models, may have limitations when applied to long texts, whether analysis or
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06616">
<div class="article-summary-box-inner">
<span><p>Continual few-shot relation extraction (RE) aims to continuously train a
model for new relations with few labeled training data, of which the major
challenges are the catastrophic forgetting of old relations and the overfitting
caused by data sparsity. In this paper, we propose a new model, namely SCKD, to
accomplish the continual few-shot RE task. Specifically, we design serial
knowledge distillation to preserve the prior knowledge from previous models and
conduct contrastive learning with pseudo samples to keep the representations of
samples in different relations sufficiently distinguishable. Our experiments on
two benchmark datasets validate the effectiveness of SCKD for continual
few-shot RE and its superiority in knowledge transfer and memory utilization
over state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Continual Relation Extraction by Distinguishing Analogous Semantics. (arXiv:2305.06620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06620">
<div class="article-summary-box-inner">
<span><p>Continual relation extraction (RE) aims to learn constantly emerging
relations while avoiding forgetting the learned relations. Existing works store
a small number of typical samples to re-train the model for alleviating
forgetting. However, repeatedly replaying these samples may cause the
overfitting problem. We conduct an empirical study on existing works and
observe that their performance is severely affected by analogous relations. To
address this issue, we propose a novel continual extraction model for analogous
relations. Specifically, we design memory-insensitive relation prototypes and
memory augmentation to overcome the overfitting problem. We also introduce
integrated training and focal knowledge distillation to enhance the performance
on analogous relations. Experimental results show the superiority of our model
and demonstrate its effectiveness in distinguishing analogous relations and
overcoming overfitting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06626">
<div class="article-summary-box-inner">
<span><p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences among group opinions, not noise.
Thus, a crucial problem in hate speech detection is whether a statement is
offensive to the demographic group that it targets, which may constitute a
small fraction of the annotator pool. We construct a model that predicts
individual annotator ratings on potentially offensive text and combines this
information with the predicted target group of the text to model the opinions
of target group members. We show gains across a range of metrics, including
raising performance over the baseline by 22% at predicting individual
annotators' ratings and 33% at predicting variance among annotators, which
provides a method of measuring model uncertainty downstream. We find that
annotators' ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06647">
<div class="article-summary-box-inner">
<span><p>Based on the remarkable achievements of pre-trained language models in
abstractive summarization, the copying mechanism has proved helpful by
improving the factuality, stability, and overall performance. This work
proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on
n-grams, which can be applied to zero-shot summarization with pre-training.
PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be
copied from the source, and calculates an auxiliary loss for the copying
prediction. Empirical studies show that PROM makes significant improvements in
fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the
self-supervised pre-training on raw corpora and provides new general baselines
on a wide range of summarization datasets. Further analysis shows that PROM
performs more reasonable copying and contributes to faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06655">
<div class="article-summary-box-inner">
<span><p>Context-dependent Text-to-SQL aims to translate multi-turn natural language
questions into SQL queries. Despite various methods have exploited
context-dependence information implicitly for contextual SQL parsing, there are
few attempts to explicitly address the dependencies between current question
and question context. This paper presents QURG, a novel Question Rewriting
Guided approach to help the models achieve adequate contextual understanding.
Specifically, we first train a question rewriting model to complete the current
question based on question context, and convert them into a rewriting edit
matrix. We further design a two-stream matrix encoder to jointly model the
rewriting relations between question and context, and the schema linking
relations between natural language and structured schema. Experimental results
show that QURG significantly improves the performances on two large-scale
context-dependent datasets SParC and CoSQL, especially for hard and long-turn
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06677">
<div class="article-summary-box-inner">
<span><p>A salient characteristic of large pre-trained language models (PTLMs) is a
remarkable improvement in their generalization capability and emergence of new
capabilities with increasing model capacity and pre-training dataset size.
Consequently, we are witnessing the development of enormous models pushing the
state-of-the-art. It is, however, imperative to realize that this inevitably
leads to prohibitively long training times, extortionate computing costs, and a
detrimental environmental impact. Significant efforts are underway to make PTLM
training more efficient through innovations in model architectures, training
pipelines, and loss function design, with scant attention being paid to
optimizing the utility of training data. The key question that we ask is
whether it is possible to train PTLMs by employing only highly informative
subsets of the training data while maintaining downstream performance? Building
upon the recent progress in informative data subset selection, we show how we
can employ submodular optimization to select highly representative subsets of
the training corpora. Our results demonstrate that the proposed framework can
be applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using
only a fraction of data while retaining up to $\sim99\%$ of the performance of
the fully-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06683">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel worker selection algorithm, enhancing
annotation quality and reducing costs in challenging span-based sequence
labeling tasks in Natural Language Processing (NLP). Unlike previous studies
targeting simpler tasks, this study contends with the complexities of label
interdependencies in sequence labeling tasks. The proposed algorithm utilizes a
Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The
challenge of dealing with imbalanced and small-scale datasets, which hinders
offline simulation of worker selection, is tackled using an innovative data
augmentation method termed shifting, expanding, and shrinking (SES). The SES
method is designed specifically for sequence labeling tasks. Rigorous testing
on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's
efficiency, with an increase in F1 score up to 100.04% of the expert-only
baseline, alongside cost savings up to 65.97%. The paper also encompasses a
dataset-independent test emulating annotation evaluation through a Bernoulli
distribution, which still led to an impressive 97.56% F1 score of the expert
baseline and 59.88% cost savings. This research addresses and overcomes
numerous obstacles in worker selection for complex NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06721">
<div class="article-summary-box-inner">
<span><p>To advance the neural encoding of Portuguese (PT), and a fortiori the
technological preparation of this language for the digital age, we developed a
Transformer-based foundation model that sets a new state of the art in this
respect for two of its variants, namely European Portuguese from Portugal
(PT-PT) and American Portuguese from Brazil (PT-BR).
</p>
<p>To develop this encoder, which we named Albertina PT-*, a strong model was
used as a starting point, DeBERTa, and its pre-training was done over data sets
of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC
corpus for PT-BR. The performance of Albertina and competing models was
assessed by evaluating them on prominent downstream language processing tasks
adapted for Portuguese.
</p>
<p>Both Albertina PT-PT and PT-BR versions are distributed free of charge and
under the most permissive license possible and can be run on consumer-grade
hardware, thus seeking to contribute to the advancement of research and
innovation in language technology for Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The First Parallel Corpora for Kurdish Sign Language. (arXiv:2305.06747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06747">
<div class="article-summary-box-inner">
<span><p>Kurdish Sign Language (KuSL) is the natural language of the Kurdish Deaf
people. We work on automatic translation between spoken Kurdish and KuSL. Sign
languages evolve rapidly and follow grammatical rules that differ from spoken
languages. Consequently,those differences should be considered during any
translation. We proposed an avatar-based automatic translation of Kurdish texts
in the Sorani (Central Kurdish) dialect into the Kurdish Sign language. We
developed the first parallel corpora for that pair that we use to train a
Statistical Machine Translation (SMT) engine. We tested the outcome
understandability and evaluated it using the Bilingual Evaluation Understudy
(BLEU). Results showed 53.8% accuracy. Compared to the previous experiments in
the field, the result is considerably high. We suspect the reason to be the
similarity between the structure of the two pairs. We plan to make the
resources publicly available under CC BY-NC-SA 4.0 license on the Kurdish-BLARK
(https://kurdishblark.github.io/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06754">
<div class="article-summary-box-inner">
<span><p>Transformer architectures are complex and their use in NLP, while it has
engendered many successes, makes their interpretability or explainability
challenging. Recent debates have shown that attention maps and attribution
methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this
paper, we present some of their limitations and introduce COCKATIEL, which
successfully addresses some of them. COCKATIEL is a novel, post-hoc,
concept-based, model-agnostic XAI technique that generates meaningful
explanations from the last layer of a neural net model trained on an NLP
classification task by using Non-Negative Matrix Factorization (NMF) to
discover the concepts the model leverages to make predictions and by exploiting
a Sensitivity Analysis to estimate accurately the importance of each of these
concepts for the model. It does so without compromising the accuracy of the
underlying model or requiring a new one to be trained. We conduct experiments
in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's
superior ability to discover concepts that align with humans' on Transformer
models without any supervision, we objectively verify the faithfulness of its
explanations through fidelity metrics, and we showcase its ability to provide
meaningful explanations in two different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning. (arXiv:2305.06801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06801">
<div class="article-summary-box-inner">
<span><p>This paper shines a light on the potential of definition-based semantic
models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)
in clinical terminology. Our study focuses on biomedical entities defined in
the UMLS ontology and aims to help prioritize the translation efforts of these
entities. In particular, we develop an effective tool for scoring the
idiomaticity of biomedical MWEs based on the degree of similarity between the
semantic representations of those MWEs and a weighted average of the
representation of their constituents. We achieve this using a biomedical
language model trained to produce similar representations for entity names and
their definitions, called BioLORD. The importance of this definition-based
approach is highlighted by comparing the BioLORD model to two other
state-of-the-art biomedical language models based on Transformer: SapBERT and
CODER. Our results show that the BioLORD model has a strong ability to identify
idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity
estimation helps ontology translators to focus on more challenging MWEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THUIR@COLIEE 2023: Incorporating Structural Knowledge into Pre-trained Language Models for Legal Case Retrieval. (arXiv:2305.06812v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06812">
<div class="article-summary-box-inner">
<span><p>Legal case retrieval techniques play an essential role in modern intelligent
legal systems. As an annually well-known international competition, COLIEE is
aiming to achieve the state-of-the-art retrieval model for legal texts. This
paper summarizes the approach of the championship team THUIR in COLIEE 2023. To
be specific, we design structure-aware pre-trained language models to enhance
the understanding of legal cases. Furthermore, we propose heuristic
pre-processing and post-processing approaches to reduce the influence of
irrelevant messages. In the end, learning-to-rank methods are employed to merge
features with different dimensions. Experimental results demonstrate the
superiority of our proposal. Official results show that our run has the best
performance among all submissions. The implementation of our method can be
found at https://github.com/CSHaitao/THUIR-COLIEE2023.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment. (arXiv:2305.06817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06817">
<div class="article-summary-box-inner">
<span><p>This paper describes the approach of the THUIR team at the COLIEE 2023 Legal
Case Entailment task. This task requires the participant to identify a specific
paragraph from a given supporting case that entails the decision for the query
case. We try traditional lexical matching methods and pre-trained language
models with different sizes. Furthermore, learning-to-rank methods are employed
to further improve performance. However, learning-to-rank is not very robust on
this task. which suggests that answer passages cannot simply be determined with
information retrieval techniques. Experimental results show that more
parameters and legal knowledge contribute to the legal case entailment task.
Finally, we get the third place in COLIEE 2023. The implementation of our
method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Computational Analysis of Suspense: Detecting Dangerous Situations. (arXiv:2305.06818v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06818">
<div class="article-summary-box-inner">
<span><p>Suspense is an important tool in storytelling to keep readers engaged and
wanting to read more. However, it has so far not been studied extensively in
Computational Literary Studies. In this paper, we focus on one of the elements
authors can use to build up suspense: dangerous situations. We introduce a
corpus of texts annotated with dangerous situations, distinguishing between 7
types of danger. Additionally, we annotate parts of the text that describe fear
experienced by a character, regardless of the actual presence of danger. We
present experiments towards the automatic detection of these situations,
finding that unsupervised baseline methods can provide valuable signals for the
detection, but more complex methods are necessary for further analysis. Not
unexpectedly, the description of danger and fear often relies heavily on the
context, both local (e.g., situations where danger is only mentioned, but not
actually present) and global (e.g., "storm" being used in a literal sense in an
adventure novel, but metaphorically in a romance novel).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06841">
<div class="article-summary-box-inner">
<span><p>While the Large Language Models (LLMs) dominate a majority of language
understanding tasks, previous work shows that some of these results are
supported by modelling spurious correlations of training datasets. Authors
commonly assess model robustness by evaluating their models on
out-of-distribution (OOD) datasets of the same task, but these datasets might
share the bias of the training dataset.
</p>
<p>We propose a simple method for measuring a scale of models' reliance on any
identified spurious feature and assess the robustness towards a large set of
known and newly found prediction biases for various pre-trained models and
debiasing methods in Question Answering (QA). We find that the reported OOD
gains of debiasing methods can not be explained by mitigated reliance on biased
features, suggesting that biases are shared among QA datasets. We further
evidence this by measuring that performance of OOD models depends on bias
features comparably to the ID model, motivating future work to refine the
reports of LLMs' robustness to a level of known spurious features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebCPM: Interactive Web Search for Chinese Long-form Question Answering. (arXiv:2305.06849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06849">
<div class="article-summary-box-inner">
<span><p>Long-form question answering (LFQA) aims at answering complex, open-ended
questions with detailed, paragraph-length responses. The de facto paradigm of
LFQA necessitates two procedures: information retrieval, which searches for
relevant supporting facts, and information synthesis, which integrates these
facts into a coherent answer. In this paper, we introduce WebCPM, the first
Chinese LFQA dataset. One unique feature of WebCPM is that its information
retrieval is based on interactive web search, which engages with a search
engine in real time. Following WebGPT, we develop a web search interface. We
recruit annotators to search for relevant information using our interface and
then answer questions. Meanwhile, the web search behaviors of our annotators
would be recorded. In total, we collect 5,500 high-quality question-answer
pairs, together with 14,315 supporting facts and 121,330 web search actions. We
fine-tune pre-trained language models to imitate human behaviors for web search
and to generate answers based on the collected facts. Our LFQA pipeline, built
on these fine-tuned models, generates answers that are no worse than
human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining. (arXiv:2305.06892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06892">
<div class="article-summary-box-inner">
<span><p>This paper describes our system on SemEval-2023 Task 10: Explainable
Detection of Online Sexism (EDOS). This work aims to design an automatic system
for detecting and classifying sexist content in online spaces. We propose a set
of transformer-based pre-trained models with task-adaptive pretraining and
ensemble learning. The main contributions of our system include analyzing the
performance of different transformer-based pre-trained models and combining
these models, as well as providing an efficient method using large amounts of
unlabeled data for model adaptive pretraining. We have also explored several
other strategies. On the test dataset, our system achieves F1-scores of 83%,
64%, and 47% on subtasks A, B, and C, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages. (arXiv:2305.06897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06897">
<div class="article-summary-box-inner">
<span><p>African languages have far less in-language content available digitally,
making it challenging for question answering systems to satisfy the information
needs of users. Cross-lingual open-retrieval question answering (XOR QA)
systems -- those that retrieve answer content from other languages while
serving people in their native language -- offer a means of filling this gap.
To this end, we create AfriQA, the first cross-lingual QA dataset with a focus
on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African
languages. While previous datasets have focused primarily on languages where
cross-lingual QA augments coverage from the target language, AfriQA focuses on
languages where cross-lingual answer content is the only high-coverage source
of answer content. Because of this, we argue that African languages are one of
the most important and realistic use cases for XOR QA. Our experiments
demonstrate the poor performance of automatic translation and multilingual
retrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA
models. We hope that the dataset enables the development of more equitable QA
technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06908">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a "Co"nsistency "Mo"del-based
"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06934">
<div class="article-summary-box-inner">
<span><p>Since the release of ChatGPT, numerous studies have highlighted the
remarkable performance of ChatGPT, which often rivals or even surpasses human
capabilities in various tasks and domains. However, this paper presents a
contrasting perspective by demonstrating an instance where human performance
excels in typical tasks suited for ChatGPT, specifically in the domain of
computer programming. We utilize the IEEExtreme Challenge competition as a
benchmark, a prestigious, annual international programming contest encompassing
a wide range of problems with different complexities. To conduct a thorough
evaluation, we selected and executed a diverse set of 102 challenges, drawn
from five distinct IEEExtreme editions, using three major programming
languages: Python, Java, and C++. Our empirical analysis provides evidence that
contrary to popular belief, human programmers maintain a competitive edge over
ChatGPT in certain aspects of problem-solving within the programming context.
In fact, we found that the average score obtained by ChatGPT on the set of
IEEExtreme programming problems is 3.9 to 5.8 times lower than the average
human score, depending on the programming language. This paper elaborates on
these findings, offering critical insights into the limitations and potential
areas of improvement for AI-based language models like ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06983">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing
retrieval-augmented LMs employ a retrieve-and-generate setup that only
retrieves information once based on the input. This is limiting, however, in
more general scenarios involving generation of long texts, where continually
gathering information throughout the generation process is essential. There
have been some past efforts to retrieve information multiple times while
generating outputs, which mostly retrieve documents at fixed intervals using
the previous context as queries. In this work, we provide a generalized view of
active retrieval augmented generation, methods that actively decide when and
what to retrieve across the course of the generation. We propose
Forward-Looking Active REtrieval augmented generation (FLARE), a generic
retrieval-augmented generation method which iteratively uses a prediction of
the upcoming sentence to anticipate future content, which is then utilized as a
query to retrieve relevant documents to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along with baselines comprehensively over
4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves
superior or competitive performance on all tasks, demonstrating the
effectiveness of our method. Code and datasets are available at
https://github.com/jzbjyb/FLARE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06984">
<div class="article-summary-box-inner">
<span><p>Lexical matching remains the de facto evaluation method for open-domain
question answering (QA). Unfortunately, lexical matching fails completely when
a plausible candidate answer does not appear in the list of gold answers, which
is increasingly the case as we shift from extractive to generative models. The
recent success of large language models (LLMs) for QA aggravates lexical
matching failures since candidate answers become longer, thereby making
matching with the gold answers even more challenging. Without accurate
evaluation, the true progress in open-domain QA remains unknown. In this paper,
we conduct a thorough analysis of various open-domain QA models, including
LLMs, by manually evaluating their answers on a subset of NQ-open, a popular
benchmark. Our assessments reveal that while the true performance of all models
is significantly underestimated, the performance of the InstructGPT (zero-shot)
LLM increases by nearly +60%, making it on par with existing top models, and
the InstructGPT (few-shot) model actually achieves a new state-of-the-art on
NQ-open. We also find that more than 50% of lexical matching failures are
attributed to semantically equivalent answers. We further demonstrate that
regex matching ranks QA models consistent with human judgments, although still
suffering from unnecessary strictness. Finally, we demonstrate that automated
evaluation models are a reasonable surrogate for lexical matching in some
circumstances, but not for long-form answers generated by LLMs. The automated
models struggle in detecting hallucinations in LLM answers and are thus unable
to evaluate LLMs. At this time, there appears to be no substitute for human
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06988">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown promising results on utilizing pre-trained
image-language models for video question answering. While these image-language
models can efficiently bootstrap the representation learning of video-language
models, they typically concatenate uniformly sampled video frames as visual
inputs without explicit language-aware, temporal modeling. When only a portion
of a video input is relevant to the language query, such uniform frame sampling
can often lead to missing important visual cues. Although humans often find a
video moment to focus on and rewind the moment to answer questions, training a
query-aware video moment localizer often requires expensive annotations and
high computational costs. To address this issue, we propose Self-Chained Video
Localization-Answering (SeViLA), a novel framework that leverages a single
image-language model (BLIP-2) to tackle both temporal keyframe localization and
QA on videos. SeViLA framework consists of two modules: Localizer and Answerer,
where both are parameter-efficiently fine-tuned from BLIP-2. We chain these
modules for cascaded inference and self-refinement. First, in the forward
chain, the Localizer finds multiple language-aware keyframes in a video, which
the Answerer uses to predict the answer. Second, in the reverse chain, the
Answerer generates keyframe pseudo-labels to refine the Localizer, alleviating
the need for expensive video moment localization annotations. SeViLA
outperforms several strong baselines/previous works on five video QA and event
prediction tasks, and achieves the state-of-the-art in both fine-tuning
(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show a
comprehensive analysis, e.g., the impact of Localizer, comparisons of Localizer
with other temporal localization models, pre-training/self-refinement of
Localizer, and varying the number of keyframes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. (arXiv:2305.06993v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06993">
<div class="article-summary-box-inner">
<span><p>The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
</p>
<p>For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. (arXiv:2305.07001v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07001">
<div class="article-summary-box-inner">
<span><p>In the past decades, recommender systems have attracted much attention in
both research and industry communities, and a large number of studies have been
devoted to developing effective recommendation models. Basically speaking,
these models mainly learn the underlying user preference from historical
behavior data, and then estimate the user-item matching relationships for
recommendations. Inspired by the recent progress on large language models
(LLMs), we take a different approach to developing the recommendation models,
considering recommendation as instruction following by LLMs. The key idea is
that the preferences or needs of a user can be expressed in natural language
descriptions (called instructions), so that LLMs can understand and further
execute the instruction for fulfilling the recommendation task. Instead of
using public APIs of LLMs, we instruction tune an open-source LLM (3B
Flan-T5-XL), in order to better adapt LLMs to recommender systems. For this
purpose, we first design a general instruction format for describing the
preference, intention, task form and context of a user in natural language.
Then we manually design 39 instruction templates and automatically generate a
large amount of user-personalized instruction data (252K instructions) with
varying types of preferences and intentions. To demonstrate the effectiveness
of our approach, we instantiate the instruction templates into several
widely-studied recommendation (or search) tasks, and conduct extensive
experiments on these tasks with real-world datasets. Experiment results show
that the proposed approach can outperform several competitive baselines,
including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds
light on developing more user-friendly recommender systems, in which users can
freely communicate with the system and obtain more accurate recommendations via
natural language instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07004">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) demonstrate impressive multilingual capability,
but their performance varies substantially across different languages. In this
work, we introduce a simple yet effective method, called cross-lingual-thought
prompting (XLT), to systematically improve the multilingual capability of LLMs.
Specifically, XLT is a generic template prompt that stimulates cross-lingual
and logical reasoning skills to enhance task performance across languages. We
conduct comprehensive evaluations on 7 typical benchmarks related to reasoning,
understanding, and generation tasks, covering both high-resource and
low-resource languages. Experimental results show that XLT not only remarkably
enhances the performance of various multilingual tasks but also significantly
reduces the gap between the average performance and the best performance of
each task in different languages. Notably, XLT brings over 10 points of average
improvement in arithmetic reasoning and open-domain question-answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07005">
<div class="article-summary-box-inner">
<span><p>Subword segmenters like BPE operate as a preprocessing step in neural machine
translation and other (conditional) language models. They are applied to
datasets before training, so translation or text generation quality relies on
the quality of segmentations. We propose a departure from this paradigm, called
subword segmental machine translation (SSMT). SSMT unifies subword segmentation
and MT in a single trainable model. It learns to segment target sentence words
while jointly learning to generate target sentences. To use SSMT during
inference we propose dynamic decoding, a text generation algorithm that adapts
segmentations as it generates translations. Experiments across 6 translation
directions show that SSMT improves chrF scores for morphologically rich
agglutinative languages. Gains are strongest in the very low-resource scenario.
SSMT also learns subwords that are closer to morphemes compared to baselines
and proves more robust on a test set constructed for evaluating morphological
compositional generalisation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07011">
<div class="article-summary-box-inner">
<span><p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a
contrastive image-text pretraining recipe to bridge the gap between image-level
pretraining and open-vocabulary object detection. At the pretraining phase, we
propose to randomly crop and resize regions of positional embeddings instead of
using the whole image positional embeddings. This better matches the use of
positional embeddings at region-level in the detection finetuning phase. In
addition, we replace the common softmax cross entropy loss in contrastive
learning with focal loss to better learn the informative yet difficult
examples. Finally, we leverage recent advances in novel object proposals to
improve open-vocabulary detection finetuning. We evaluate our full model on the
LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.
RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best
existing approach by +5.8 points in addition to competitive zero-shot transfer
detection. Surprisingly, RO-ViT improves the image-level representation as well
and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr
image-text retrieval benchmarks, outperforming competitive approaches with
larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07016">
<div class="article-summary-box-inner">
<span><p>Massively multilingual pretrained transformers (MMTs) have tremendously
pushed the state of the art on multilingual NLP and cross-lingual transfer of
NLP models in particular. While a large body of work leveraged MMTs to mine
parallel data and induce bilingual document embeddings, much less effort has
been devoted to training general-purpose (massively) multilingual document
encoder that can be used for both supervised and unsupervised document-level
tasks. In this work, we pretrain a massively multilingual document encoder as a
hierarchical transformer model (HMDE) in which a shallow document transformer
contextualizes sentence representations produced by a state-of-the-art
pretrained multilingual sentence encoder. We leverage Wikipedia as a readily
available source of comparable documents for creating training data, and train
HMDE by means of a cross-lingual contrastive objective, further exploiting the
category hierarchy of Wikipedia for creation of difficult negatives. We
evaluate the effectiveness of HMDE in two arguably most common and prominent
cross-lingual document-level tasks: (1) cross-lingual transfer for topical
document classification and (2) cross-lingual document retrieval. HMDE is
significantly more effective than (i) aggregations of segment-based
representations and (ii) multilingual Longformer. Crucially, owing to its
massively multilingual lower transformer, HMDE successfully generalizes to
languages unseen in document-level pretraining. We publicly release our code
and models at
https://github.com/ogaloglu/pre-training-multilingual-document-encoders .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorical Vector Space Semantics for Lambek Calculus with a Relevant Modality. (arXiv:2005.03074v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.03074">
<div class="article-summary-box-inner">
<span><p>We develop a categorical compositional distributional semantics for Lambek
Calculus with a Relevant Modality !L*, which has a limited edition of the
contraction and permutation rules. The categorical part of the semantics is a
monoidal biclosed category with a coalgebra modality, very similar to the
structure of a Differential Category. We instantiate this category to finite
dimensional vector spaces and linear maps via "quantisation" functors and work
with three concrete interpretations of the coalgebra modality. We apply the
model to construct categorical and concrete semantic interpretations for the
motivating example of !L*: the derivation of a phrase with a parasitic gap. The
effectiveness of the concrete interpretations are evaluated via a
disambiguation task, on an extension of a sentence disambiguation dataset to
parasitic gap phrases, using BERT, Word2Vec, and FastText vectors and
Relational tensors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00542">
<div class="article-summary-box-inner">
<span><p>The large attention-based encoder-decoder network (Transformer) has become
prevailing recently due to its effectiveness. But the high computation
complexity of its decoder raises the inefficiency issue. By examining the
mathematic formulation of the decoder, we show that under some mild conditions,
the architecture could be simplified by compressing its sub-layers, the basic
building block of Transformer, and achieves a higher parallelism. We thereby
propose Compressed Attention Network, whose decoder layer consists of only one
sub-layer instead of three. Extensive experiments on 14 WMT machine translation
tasks show that our model is 1.42x faster with performance on par with a strong
baseline. This strong baseline is already 2x faster than the widely used
standard baseline without loss in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04939">
<div class="article-summary-box-inner">
<span><p>In computational linguistics, it has been shown that hierarchical structures
make language models (LMs) more human-like. However, the previous literature
has been agnostic about a parsing strategy of the hierarchical models. In this
paper, we investigated whether hierarchical structures make LMs more
human-like, and if so, which parsing strategy is most cognitively plausible. In
order to address this question, we evaluated three LMs against human reading
times in Japanese with head-final left-branching structures: Long Short-Term
Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars
(RNNGs) with top-down and left-corner parsing strategies as hierarchical
models. Our computational modeling demonstrated that left-corner RNNGs
outperformed top-down RNNGs and LSTM, suggesting that hierarchical and
left-corner architectures are more cognitively plausible than top-down or
sequential architectures. In addition, the relationships between the cognitive
plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be
discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11155">
<div class="article-summary-box-inner">
<span><p>Quantum density matrix represents all the information of the entire quantum
system, and novel models of meaning employing density matrices naturally model
linguistic phenomena such as hyponymy and linguistic ambiguity, among others in
quantum question answering tasks. Naturally, we argue that applying the quantum
density matrix into classical Question Answering (QA) tasks can show more
effective performance. Specifically, we (i) design a new mechanism based on
Long Short-Term Memory (LSTM) to accommodate the case when the inputs are
matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural
Network (CNN) and gain the LSTM-based QA model with the quantum density matrix.
Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging
results. Similarly, we argue that the quantum density matrix can also enhance
the image feature information and the relationship between the features for the
classical image classification. Thus, we (i) combine density matrices and CNN
to design a new mechanism; (ii) apply the new mechanism to some representative
classical image classification tasks. A series of experiments show that the
application of quantum density matrix in image classification has the
generalization and high efficiency on different datasets. The application of
quantum density matrix both in classical question answering tasks and classical
image classification tasks show more effective performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11370">
<div class="article-summary-box-inner">
<span><p>Modern language models can generate high-quality short texts. However, they
often meander or are incoherent when generating longer texts. These issues
arise from the next-token-only language modeling objective. Recent work in
self-supervised learning suggests that models can learn good latent
representations via contrastive learning, which can be effective for
discriminative tasks. Our work analyzes the application of contrastive
representations for generative tasks, like long text generation. We propose one
approach for leveraging constrastive representations, which we call Time
Control (TC). TC first learns a contrastive representation of the target text
domain, then generates text by decoding from these representations. Compared to
domain-specific methods and fine-tuning GPT2 across a variety of text domains,
TC performs competitively to methods specific for learning sentence
representations on discourse coherence. On long text generation settings, TC
preserves the text structure both in terms of ordering (up to $+15\%$ better)
and text length consistency (up to $+90\%$ better).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07182">
<div class="article-summary-box-inner">
<span><p>Recent advances in Artificial Intelligence (AI) have leveraged promising
results in solving complex problems in the area of Natural Language Processing
(NLP), being an important tool to help in the expeditious resolution of
judicial proceedings in the legal area. In this context, this work targets the
problem of detecting the degree of similarity between judicial documents that
can be achieved in the inference group, by applying six NLP techniques based on
the transformers architecture to a case study of legal proceedings in the
Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2
and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian
Portuguese language, and then were fine-tuned and specialised for the legal
sector using 210,000 legal proceedings. Vector representations of each legal
document were calculated based on their embeddings, which were used to cluster
the lawsuits, calculating the quality of each model based on the cosine of the
distance between the elements of the group to its centroid. We noticed that
models based on transformers presented better performance when compared to
previous traditional NLP techniques, with the RoBERTa model specialised for the
Brazilian Portuguese language presenting the best results. This methodology can
be also applied to other case studies for different languages, making it
possible to advance in the current state of the art in the area of NLP applied
to the legal sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Personality of White-Box Language Models. (arXiv:2204.12000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12000">
<div class="article-summary-box-inner">
<span><p>Technology for open-ended language generation, a key application of
artificial intelligence, has advanced to a great extent in recent years.
Large-scale language models, which are trained on large corpora of text, are
being used in a wide range of applications everywhere, from virtual assistants
to conversational bots. While these language models output fluent text,
existing research shows that these models can and do capture human biases. Many
of these biases, especially those that could potentially cause harm, are being
well-investigated. On the other hand, studies that infer and change human
personality traits inherited by these models have been scarce or non-existent.
Our work seeks to address this gap by exploring the personality traits of
several large-scale language models designed for open-ended text generation and
the datasets used for training them. We build on the popular Big Five factors
and develop robust methods that quantify the personality traits of these models
and their underlying datasets. In particular, we trigger the models with a
questionnaire designed for personality assessment and subsequently classify the
text responses into quantifiable traits using a Zero-shot classifier. Our
estimation scheme sheds light on an important anthropomorphic element found in
such AI models and can help stakeholders decide how they should be applied as
well as how society could perceive them. Additionally, we examined approaches
to alter these personalities, adding to our understanding of how AI models can
be adapted to specific contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic. (arXiv:2205.09511v3 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09511">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has disproportionately impacted the lives of
minorities, such as members of the LGBTQ community (lesbian, gay, bisexual,
transgender, and queer) due to pre-existing social disadvantages and health
disparities. Although extensive research has been carried out on the impact of
the COVID-19 pandemic on different aspects of the general population's lives,
few studies are focused on the LGBTQ population. In this paper, we develop and
evaluate two sets of machine learning classifiers using a pre-pandemic and a
during-pandemic dataset to identify Twitter posts exhibiting minority stress,
which is a unique pressure faced by the members of the LGBTQ population due to
their sexual and gender identities. We demonstrate that our best pre- and
during-pandemic models show strong and stable performance for detecting posts
that contain minority stress. We investigate the linguistic differences in
minority stress posts across pre- and during-pandemic periods. We find that
anger words are strongly associated with minority stress during the COVID-19
pandemic. We explore the impact of the pandemic on the emotional states of the
LGBTQ population by adopting propensity score-based matching to perform a
causal analysis. The results show that the LGBTQ population have a greater
increase in the usage of cognitive words and worsened observable attribute in
the usage of positive emotion words than the group of the general population
with similar pre-pandemic behavioral attributes. Our findings have implications
for the public health domain and policy-makers to provide adequate support,
especially with respect to mental health, to the LGBTQ population during future
crises.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06049">
<div class="article-summary-box-inner">
<span><p>NLP in the legal domain has seen increasing success with the emergence of
Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.
PLMs trained over European and US legal text are available publicly; however,
legal text from other domains (countries), such as India, have a lot of
distinguishing characteristics. With the rapidly increasing volume of Legal NLP
applications in various countries, it has become necessary to pre-train such
LMs over legal text of other countries as well. In this work, we attempt to
investigate pre-training in the Indian legal domain. We re-train (continue
pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian
legal data, as well as train a model from scratch with a vocabulary based on
Indian legal text. We apply these PLMs over three benchmark legal NLP tasks --
Legal Statute Identification from facts, Semantic Segmentation of Court
Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian
and non-Indian (EU, UK) datasets. We observe that our approach not only
enhances performance on the new domain (Indian texts) but also over the
original domain (European and UK texts). We also conduct explainability
experiments for a qualitative comparison of all these different PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02334">
<div class="article-summary-box-inner">
<span><p>Artistic pieces can be studied from several perspectives, one example being
their reception among readers over time. In the present work, we approach this
interesting topic from the standpoint of literary works, particularly assessing
the task of predicting whether a book will become a best seller. Dissimilarly
from previous approaches, we focused on the full content of books and
considered visualization and classification tasks. We employed visualization
for the preliminary exploration of the data structure and properties, involving
SemAxis and linear discriminant analyses. Then, to obtain quantitative and more
objective results, we employed various classifiers. Such approaches were used
along with a dataset containing (i) books published from 1895 to 1924 and
consecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii)
literary works published in the same period but not being mentioned in that
list. Our comparison of methods revealed that the best-achieved result -
combining a bag-of-words representation with a logistic regression classifier -
led to an average accuracy of 0.75 both for the leave-one-out and 10-fold
cross-validations. Such an outcome suggests that it is unfeasible to predict
the success of books with high accuracy using only the full content of the
texts. Nevertheless, our findings provide insights into the factors leading to
the relative success of a literary work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs. (arXiv:2210.04490v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04490">
<div class="article-summary-box-inner">
<span><p>Answering factual questions with temporal intent over knowledge graphs
(temporal KGQA) attracts rising attention in recent years. In the generation of
temporal queries, existing KGQA methods ignore the fact that some intrinsic
connections between events can make them temporally related, which may limit
their capability. We systematically analyze the possible interpretation of
temporal constraints and conclude the interpretation structures as the Semantic
Framework of Temporal Constraints, SF-TCons. Based on the semantic framework,
we propose a temporal question answering method, SF-TQA, which generates query
graphs by exploring the relevant facts of mentioned entities, where the
exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA
significantly outperforms existing methods on two benchmarks over different
knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating self-supervised, weakly supervised and fully supervised training approaches for multi-domain automatic speech recognition: a study on Bangladeshi Bangla. (arXiv:2210.12921v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12921">
<div class="article-summary-box-inner">
<span><p>Despite huge improvements in automatic speech recognition (ASR) employing
neural networks, ASR systems still suffer from a lack of robustness and
generalizability issues due to domain shifting. This is mainly because
principal corpus design criteria are often not identified and examined
adequately while compiling ASR datasets. In this study, we investigate the
robustness of the state-of-the-art transfer learning approaches such as
self-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully
supervised convolutional neural networks (CNNs) for multi-domain ASR. We also
demonstrate the significance of domain selection while building a corpus by
assessing these models on a novel multi-domain Bangladeshi Bangla ASR
evaluation benchmark - BanSpeech, which contains approximately 6.52 hours of
human-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO,
a mostly read speech corpus for the morphologically rich language Bangla, has
been used to train the ASR systems. Experimental evaluation reveals that
self-supervised cross-lingual pre-training is the best strategy compared to
weak supervision and full supervision to tackle the multi-domain ASR task.
Moreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech
from domains with mostly spontaneous speech. The BanSpeech will be publicly
available to meet the need for a challenging evaluation benchmark for Bangla
ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition, Attention, or Both?. (arXiv:2210.12958v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12958">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel architecture called Composition Attention
Grammars (CAGs) that recursively compose subtrees into a single vector
representation with a composition function, and selectively attend to previous
structural information with a self-attention mechanism. We investigate whether
these components -- the composition function and the self-attention mechanism
-- can both induce human-like syntactic generalization. Specifically, we train
language models (LMs) with and without these two components with the model
sizes carefully controlled, and evaluate their syntactic generalization
performance against six test circuits on the SyntaxGym benchmark. The results
demonstrated that the composition function and the self-attention mechanism
both play an important role to make LMs more human-like, and closer inspection
of linguistic phenomenon implied that the composition function allowed
syntactic features, but not semantic features, to percolate into subtree
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01568">
<div class="article-summary-box-inner">
<span><p>Language models often pre-train on large unsupervised text corpora, then
fine-tune on additional task-specific data. However, typical fine-tuning
schemes do not prioritize the examples that they tune on. We show that, if you
can prioritize informative training data, you can achieve better performance
while using fewer labels. To do this we augment a language model with an
epinet: a small additional network that helps to estimate model uncertainty and
forms an \textit{epistemic neural network} (ENN). ENNs are neural networks that
can know what they don't know. Using an epinet to prioritize uncertain data, we
can fine-tune BERT on GLUE tasks to the same performance while using 2x less
data than training without prioritization. We also investigate performance in
synthetic neural network generative models designed to build understanding. In
each setting, using an epinet outperforms heuristic active learning schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Omission in Dialogue Summarization. (arXiv:2211.07145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07145">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization aims to condense the lengthy dialogue into a concise
summary, and has recently achieved significant progress. However, the result of
existing methods is still far from satisfactory. Previous works indicated that
omission is a major factor in affecting the quality of summarization, but few
of them have further explored the omission problem, such as how omission
affects summarization results and how to detect omission, which is critical for
reducing omission and improving summarization quality. Moreover, analyzing and
detecting omission relies on summarization datasets with omission labels (i.e.,
which dialogue utterances are omitted in the summarization), which are not
available in the current literature. In this paper, we propose the OLDS
dataset, which provides high-quality Omission Labels for Dialogue
Summarization. By analyzing this dataset, we find that a large improvement in
summarization quality can be achieved by providing ground-truth omission labels
for the summarization model to recover omission information, which demonstrates
the importance of omission detection for omission mitigation in dialogue
summarization. Therefore, we formulate an omission detection task and
demonstrate our proposed dataset can support the training and evaluation of
this task well. We also call for research action on omission detection based on
our proposed datasets. Our dataset and codes are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery. (arXiv:2211.08316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08316">
<div class="article-summary-box-inner">
<span><p>Understanding users' intentions in e-commerce platforms requires commonsense
knowledge. In this paper, we present FolkScope, an intention knowledge graph
construction framework to reveal the structure of humans' minds about
purchasing items. As commonsense knowledge is usually ineffable and not
expressed explicitly, it is challenging to perform information extraction.
Thus, we propose a new approach that leverages the generation power of large
language models~(LLMs) and human-in-the-loop annotation to semi-automatically
construct the knowledge graph. LLMs first generate intention assertions via
e-commerce-specific prompts to explain shopping behaviors, where the intention
can be an open reason or a predicate falling into one of 18 categories aligning
with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility
and typicality labels of sampled intentions as training data in order to
populate human judgments to all automatic generations. Last, to structurize the
assertions, we propose pattern mining and conceptualization to form more
condensed and abstract knowledge. Extensive evaluations and studies demonstrate
that our constructed knowledge graph can well model e-commerce knowledge and
have many potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08369">
<div class="article-summary-box-inner">
<span><p>A popular approach to unveiling the black box of neural NLP models is to
leverage saliency methods, which assign scalar importance scores to each input
component. A common practice for evaluating whether an interpretability method
is faithful has been to use evaluation-by-agreement -- if multiple methods
agree on an explanation, its credibility increases. However, recent work has
found that saliency methods exhibit weak rank correlations even when applied to
the same model instance and advocated for the use of alternative diagnostic
methods. In our work, we demonstrate that rank correlation is not a good fit
for evaluating agreement and argue that Pearson-$r$ is a better-suited
alternative. We further show that regularization techniques that increase
faithfulness of attention explanations also increase agreement between saliency
methods. By connecting our findings to instance categories based on training
dynamics, we show that the agreement of saliency method explanations is very
low for easy-to-learn instances. Finally, we connect the improvement in
agreement across instance categories to local representation space statistics
of instances, paving the way for work on analyzing which intrinsic model
properties improve their predisposition to interpretability methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08726">
<div class="article-summary-box-inner">
<span><p>Disfluency detection has mainly been solved in a pipeline approach, as
post-processing of speech recognition. In this study, we propose
Transformer-based encoder-decoder models that jointly solve speech recognition
and disfluency detection, which work in a streaming manner. Compared to
pipeline approaches, the joint models can leverage acoustic information that
makes disfluency detection robust to recognition errors and provide non-verbal
clues. Moreover, joint modeling results in low-latency and lightweight
inference. We investigate two joint model variants for streaming disfluency
detection: a transcript-enriched model and a multi-task model. The
transcript-enriched model is trained on text with special tags indicating the
starting and ending points of the disfluent part. However, it has problems with
latency and standard language model adaptation, which arise from the additional
disfluency tags. We propose a multi-task model to solve such problems, which
has two output layers at the Transformer decoder; one for speech recognition
and the other for disfluency detection. It is modeled to be conditioned on the
currently recognized token with an additional token-dependency mechanism. We
show that the proposed joint models outperformed a BERT-based pipeline approach
in both accuracy and latency, on both the Switchboard and the corpus of
spontaneous Japanese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08794">
<div class="article-summary-box-inner">
<span><p>Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12701">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is a learning paradigm that emulates the human
capability of learning and accumulating knowledge continually without
forgetting the previously learned knowledge and also transferring the learned
knowledge to help learn new tasks better. This survey presents a comprehensive
review and analysis of the recent progress of CL in NLP, which has significant
differences from CL in computer vision and machine learning. It covers (1) all
CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting
(CF) prevention, (3) knowledge transfer (KT), which is particularly important
for NLP tasks; and (4) some theory and the hidden challenge of inter-task class
separation (ICS). (1), (3) and (4) have not been included in the existing
survey. Finally, a list of future directions is discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v4 [cs.FL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01944">
<div class="article-summary-box-inner">
<span><p>Automaton-based representations of task knowledge play an important role in
control and planning for sequential decision-making problems. However,
obtaining the high-level task knowledge required to build such automata is
often difficult. Meanwhile, large-scale generative language models (GLMs) can
automatically generate relevant task knowledge. However, the textual outputs
from GLMs cannot be formally verified or used for sequential decision-making.
We propose a novel algorithm named GLM2FSA, which constructs a finite state
automaton (FSA) encoding high-level task knowledge from a brief
natural-language description of the task goal. GLM2FSA first sends queries to a
GLM to extract task knowledge in textual form, and then it builds an FSA to
represent this text-based knowledge. The proposed algorithm thus fills the gap
between natural-language task descriptions and automaton-based representations,
and the constructed FSA can be formally verified against user-defined
specifications. We accordingly propose a method to iteratively refine the
queries to the GLM based on the outcomes, e.g., counter-examples, from
verification. We demonstrate GLM2FSA's ability to build and refine
automaton-based representations of everyday tasks (e.g., crossing a road), and
also of tasks that require highly-specialized knowledge (e.g., executing secure
multi-party computation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation. (arXiv:2212.02560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02560">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction aims to recognize novel relations with few
labeled sentences in each relation. Previous metric-based few-shot relation
extraction algorithms identify relationships by comparing the prototypes
generated by the few labeled sentences embedding with the embeddings of the
query sentences using a trained metric function. However, as these domains
always have considerable differences from those in the training dataset, the
generalization ability of these approaches on unseen relations in many domains
is limited. Since the prototype is necessary for obtaining relationships
between entities in the latent space, we suggest learning more interpretable
and efficient prototypes from prior knowledge and the intrinsic semantics of
relations to extract new relations in various domains more effectively. By
exploring the relationships between relations using prior information, we
effectively improve the prototype representation of relations. By using
contrastive learning to make the classification margins between sentence
embedding more distinct, the prototype's geometric interpretability is
enhanced. Additionally, utilizing a transfer learning approach for the
cross-domain problem allows the generation process of the prototype to account
for the gap between other domains, making the prototype more robust and
enabling the better extraction of associations across multiple domains. The
experiment results on the benchmark FewRel dataset demonstrate the advantages
of the suggested method over some state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention as a Guide for Simultaneous Speech Translation. (arXiv:2212.07850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07850">
<div class="article-summary-box-inner">
<span><p>The study of the attention mechanism has sparked interest in many fields,
such as language modeling and machine translation. Although its patterns have
been exploited to perform different tasks, from neural network understanding to
textual alignment, no previous work has analysed the encoder-decoder attention
behavior in speech translation (ST) nor used it to improve ST on a specific
task. In this paper, we fill this gap by proposing an attention-based policy
(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the
existing attention relations between audio input and textual output. Its goal
is to leverage the encoder-decoder attention scores to guide inference in real
time. Results on en-&gt;{de, es} show that the EDAtt policy achieves overall
better results compared to the SimulST state of the art, especially in terms of
computational-aware latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08853">
<div class="article-summary-box-inner">
<span><p>Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can GPT-3 Perform Statutory Reasoning?. (arXiv:2302.06100v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06100">
<div class="article-summary-box-inner">
<span><p>Statutory reasoning is the task of reasoning with facts and statutes, which
are rules written in natural language by a legislature. It is a basic legal
skill. In this paper we explore the capabilities of the most capable GPT-3
model, text-davinci-003, on an established statutory-reasoning dataset called
SARA. We consider a variety of approaches, including dynamic few-shot
prompting, chain-of-thought prompting, and zero-shot prompting. While we
achieve results with GPT-3 that are better than the previous best published
results, we also identify several types of clear errors it makes. We
investigate why these errors happen. We discover that GPT-3 has imperfect prior
knowledge of the actual U.S. statutes on which SARA is based. More importantly,
we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen
during training. We find GPT-3 performs poorly at answering straightforward
questions about these simple synthetic statutes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14220">
<div class="article-summary-box-inner">
<span><p>Pretrained character-level language models were recently shown to be
competitive with popular subword models across a range of NLP tasks. However,
there has been little research on their effectiveness for neural machine
translation (NMT). This work performs an extensive comparison across multiple
languages and experimental conditions of state-of-the-art character- and
subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing
the effectiveness of character-level modeling in translation, particularly in
cases where training data is limited. In our analysis, we show how character
models' performance gains are reflected in better translations of
orthographically similar words and rare words. While evaluating the importance
of source texts in driving model predictions, we highlight ByT5 word-level
patterns suggesting an ability to modulate word and character-level information
during the translation, providing insights into a potential weakness of
character-level modeling. We conclude by assessing the efficiency tradeoff of
character models, suggesting their usage in non-time-critical scenarios to
boost translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-contained Beta-with-Spikes Approximation for Inference Under a Wright-Fisher Model. (arXiv:2303.04691v2 [q-bio.PE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04691">
<div class="article-summary-box-inner">
<span><p>We construct a reliable estimation of evolutionary parameters within the
Wright-Fisher model, which describes changes in allele frequencies due to
selection and genetic drift, from time-series data. Such data exists for
biological populations, for example via artificial evolution experiments, and
for the cultural evolution of behavior, such as linguistic corpora that
document historical usage of different words with similar meanings. Our method
of analysis builds on a Beta-with-Spikes approximation to the distribution of
allele frequencies predicted by the Wright-Fisher model. We introduce a
self-contained scheme for estimating the parameters in the approximation, and
demonstrate its robustness with synthetic data, especially in the
strong-selection and near-extinction regimes where previous approaches fail. We
further apply to allele frequency data for baker's yeast (Saccharomyces
cerevisiae), finding a significant signal of selection in cases where
independent evidence supports such a conclusion. We further demonstrate the
possibility of detecting time-points at which evolutionary parameters change in
the context of a historical spelling reform in the Spanish language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01852">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of ChatGPT and GPT-4,
state-of-the-art large language models (LLM) from the GPT series, and their
prospective applications across diverse domains. Indeed, key innovations such
as large-scale pre-training that captures knowledge across the entire world
wide web, instruction fine-tuning and Reinforcement Learning from Human
Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability
and performance. We performed an in-depth analysis of 194 relevant papers on
arXiv, encompassing trend analysis, word cloud representation, and distribution
analysis across various application domains. The findings reveal a significant
and increasing interest in ChatGPT/GPT-4 research, predominantly centered on
direct natural language processing applications, while also demonstrating
considerable potential in areas ranging from education and history to
mathematics, medicine, and physics. This study endeavors to furnish insights
into ChatGPT's capabilities, potential implications, ethical concerns, and
offer direction for future advancements in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09797">
<div class="article-summary-box-inner">
<span><p>The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted an extensive and comprehensive evaluation to demonstrate the
effectiveness of the proposed method. Our experimental results on seven
benchmarks show that combining CoT and self-consistency with PHP significantly
improves accuracy while remaining highly efficient. For instance, with
text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding
compared to Complex CoT, and a 46.17% reduction in sample paths with
self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances
on SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH
(50.3% -&gt; 53.9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00217">
<div class="article-summary-box-inner">
<span><p>In a recent paper published in the Journal of Language Evolution, Kauhanen,
Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the
results presented in one of my papers (Koplenig, Royal Society Open Science, 6,
181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show
through a series of statistical analyses that large numbers of L2 (second
language) speakers do not seem to affect the (grammatical or statistical)
complexity of a language. To this end, I focus on the way in which the
Ethnologue assesses language status: a language is characterised as vehicular
if, in addition to being used by L1 (first language) speakers, it should also
have a significant number of L2 users. KEW criticise both the use of
vehicularity as a (binary) indicator of whether a language has a significant
number of L2 users and the idea of imputing a zero proportion of L2 speakers to
non-vehicular languages whenever a direct estimate of that proportion is
unavailable. While I recognise the importance of post-publication commentary on
published research, I show in this rejoinder that both points of criticism are
explicitly mentioned and analysed in my paper. In addition, I also comment on
other points raised by KEW and demonstrate that both alternative analyses
offered by KEW do not stand up to closer scrutiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge. (arXiv:2305.01194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01194">
<div class="article-summary-box-inner">
<span><p>This paper describes our system for the low-resource domain adaptation track
(Track 3) in Spoken Language Understanding Grand Challenge, which is a part of
ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a
pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain
with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on
low-resource domain data. We apply masked LM (MLM) -based data augmentation,
where some of input tokens and corresponding target labels are replaced using
MLM. We also apply a retrieval-based approach, where model input is augmented
with similar training samples. As a result, we achieved exact match (EM)
accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the
1st place at the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01735">
<div class="article-summary-box-inner">
<span><p>Extractive summarization aims to form a summary by directly extracting
sentences from the source document. Existing works mostly formulate it as a
sequence labeling problem by making individual sentence label predictions. This
paper proposes DiffuSum, a novel paradigm for extractive summarization, by
directly generating the desired summary sentence representations with diffusion
models and extracting sentences based on sentence representation matching. In
addition, DiffuSum jointly optimizes a contrastive sentence encoder with a
matching loss for sentence representation alignment and a multi-class
contrastive loss for representation diversity. Experimental results show that
DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail
with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets
with different summary lengths also demonstrate the effectiveness of DiffuSum.
The strong performance of our framework shows the great potential of adapting
generative models for extractive summarization. To encourage more following
work in the future, we have released our codes at
\url{https://github.com/hpzhang94/DiffuSum}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02993">
<div class="article-summary-box-inner">
<span><p>This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence
Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2
tasks, a Natural Language Inference (NLI) task, and an evidence selection task
on clinical trial data. The proposed challenges require multi-hop biomedical
and numerical reasoning, which are of significant importance to the development
of systems capable of large-scale interpretation and retrieval of medical
evidence, to provide personalized evidence-based care.
</p>
<p>Task 1, the entailment task, received 643 submissions from 40 participants,
and Task 2, the evidence selection task, received 364 submissions from 23
participants. The tasks are challenging, with the majority of submitted systems
failing to significantly outperform the majority class baseline on the
entailment task, and we observe significantly better performance on the
evidence selection task than on the entailment task. Increasing the number of
model parameters leads to a direct increase in performance, far more
significant than the effect of biomedical pre-training. Future works could
explore the limitations of large models for generalization and numerical
inference, and investigate methods to augment clinical datasets to allow for
more rigorous testing and to facilitate fine-tuning.
</p>
<p>We envisage that the dataset, models, and results of this task will be useful
to the biomedical NLI and evidence retrieval communities. The dataset,
competition leaderboard, and website are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05711">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning ability on many NLP tasks. A common practice is to
recast the task into a text-to-text format such that generative LLMs of natural
language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is
nontrivial to perform information extraction (IE) tasks with NL-LLMs since the
output of the IE task is usually structured and therefore is hard to be
converted into plain text. In this paper, we propose to recast the structured
output in the form of code instead of natural language and utilize generative
LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,
named entity recognition and relation extraction. In contrast to NL-LLMs, we
show that Code-LLMs can be well-aligned with these IE tasks by designing
code-style prompts and formulating these IE tasks as code generation tasks.
Experiment results on seven benchmarks show that our method consistently
outperforms fine-tuning moderate-size pre-trained models specially designed for
IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further
conduct a series of in-depth analyses to demonstrate the merits of leveraging
Code-LLMs for IE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06162">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis is an important area for understanding the
user's internal states. Deep learning methods were effective, but the problem
of poor interpretability has gradually gained attention. Previous works have
attempted to use attention weights or vector distributions to provide
interpretability. However, their explanations were not intuitive and can be
influenced by different trained models. This study proposed a novel approach to
provide interpretability by converting nonverbal modalities into text
descriptions and by using large-scale language models for sentiment
predictions. This provides an intuitive approach to directly interpret what
models depend on with respect to making decisions from input texts, thus
significantly improving interpretability. Specifically, we convert descriptions
based on two feature patterns for the audio modality and discrete action units
for the facial modality. Experimental results on two sentiment analysis tasks
demonstrated that the proposed approach maintained, or even improved
effectiveness for sentiment analysis compared to baselines using conventional
features, with the highest improvement of 2.49% on the F1 score. The results
also showed that multimodal descriptions have similar characteristics on fusing
modalities as those of conventional fusion methods. The results demonstrated
that the proposed approach is interpretable and effective for multimodal
sentiment analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06299">
<div class="article-summary-box-inner">
<span><p>Large language models, particularly GPT-3, are able to produce high quality
summaries of general domain news articles in few- and zero-shot settings.
However, it is unclear if such models are similarly capable in more
specialized, high-stakes domains such as biomedicine. In this paper, we enlist
domain experts (individuals with medical training) to evaluate summaries of
biomedical articles generated by GPT-3, given zero supervision. We consider
both single- and multi-document settings. In the former, GPT-3 is tasked with
generating regular and plain-language summaries of articles describing
randomized controlled trials; in the latter, we assess the degree to which
GPT-3 is able to \emph{synthesize} evidence reported across a collection of
articles. We design an annotation scheme for evaluating model outputs, with an
emphasis on assessing the factual accuracy of generated summaries. We find that
while GPT-3 is able to summarize and simplify single biomedical articles
faithfully, it struggles to provide accurate aggregations of findings over
multiple documents. We release all data and annotations used in this work.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-14 23:11:15.578484971 UTC">2023-05-14 23:11:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>