<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-12T01:30:00Z">10-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06846">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging. (arXiv:2310.06913v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06913">
<div class="article-summary-box-inner">
<span><p>Often, the first step in managing bug reports is related to triaging a bug to
the appropriate developer who is best suited to understand, localize, and fix
the target bug. Additionally, assigning a given bug to a particular part of a
software project can help to expedite the fixing process. However, despite the
importance of these activities, they are quite challenging, where days can be
spent on the manual triaging process. Past studies have attempted to leverage
the limited textual data of bug reports to train text classification models
that automate this process -- to varying degrees of success. However, the
textual representations and machine learning models used in prior work are
limited by their expressiveness, often failing to capture nuanced textual
patterns that might otherwise aid in the triaging process. Recently, large,
transformer-based, pre-trained neural text representation techniques such as
BERT have achieved greater performance in several natural language processing
tasks. However, the potential for using these techniques to improve upon prior
approaches for automated bug triaging is not well studied or understood.
</p>
<p>Therefore, in this paper we offer one of the first investigations that
fine-tunes transformer-based language models for the task of bug triaging on
four open source datasets, spanning a collective 53 years of development
history with over 400 developers and over 150 software project components. Our
study includes both a quantitative and qualitative analysis of effectiveness.
Our findings illustrate that DeBERTa is the most effective technique across the
triaging tasks of developer and component assignment, and the measured
performance delta is statistically significant compared to other techniques.
However, through our qualitative analysis, we also observe that each technique
possesses unique abilities best suited to certain types of bug reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06918">
<div class="article-summary-box-inner">
<span><p>The recent success of SimCSE has greatly advanced state-of-the-art sentence
representations. However, the original formulation of SimCSE does not fully
exploit the potential of hard negative samples in contrastive learning. This
study introduces an unsupervised contrastive learning framework that combines
SimCSE with hard negative mining, aiming to enhance the quality of sentence
embeddings. The proposed focal-InfoNCE function introduces self-paced
modulation terms in the contrastive objective, downweighting the loss
associated with easy negatives and encouraging the model focusing on hard
negatives. Experimentation on various STS benchmarks shows that our method
improves sentence embeddings in terms of Spearman's correlation and
representation alignment and uniformity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06927">
<div class="article-summary-box-inner">
<span><p>We consider the problem of accurate sparse finetuning of large language
models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while
inducing sparsity in their weights. On the accuracy side, we observe that
standard loss-based finetuning may fail to recover accuracy, especially at high
sparsities. To address this, we perform a detailed study of distillation-type
losses, determining an L2-based distillation approach we term SquareHead which
enables accurate recovery even at higher sparsities, across all model types. On
the practical efficiency side, we show that sparse LLMs can be executed with
speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While
the standard approach is to leverage sparsity for computational reduction, we
observe that in the case of memory-bound LLMs sparsity can also be leveraged
for reducing memory bandwidth. We exhibit end-to-end results showing speedups
due to sparsity, while recovering accuracy, on T5 (language translation),
Whisper (speech translation), and open GPT-type (MPT for text generation). For
MPT text generation, we show for the first time that sparse finetuning can
reach 75% sparsity without accuracy drops, provide notable end-to-end speedups
for both CPU and GPU inference, and highlight that sparsity is also compatible
with quantization approaches. Models and software for reproducing our results
are provided in Section 6.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels. (arXiv:2310.06940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06940">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often
trained through supervision from human annotations of opinionated texts. These
fine-grained annotations include identifying aspects towards which a user
expresses their sentiment, and their associated polarities (aspect-based
sentiments). Such fine-grained annotations can be expensive and often
infeasible to obtain in real-world settings. There is, however, an abundance of
scenarios where user-generated text contains an overall sentiment, such as a
rating of 1-5 in user reviews or user-generated feedback, which may be
leveraged for this task. In this paper, we propose a VAE-based topic modeling
approach that performs ABSA using document-level supervision and without
requiring fine-grained labels for either aspects or sentiments. Our approach
allows for the detection of multiple aspects in a document, thereby allowing
for the possibility of reasoning about how sentiment expressed through multiple
aspects comes together to form an observable overall document-level sentiment.
We demonstrate results on two benchmark datasets from two different domains,
significantly outperforming a state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06977">
<div class="article-summary-box-inner">
<span><p>A recent body of work has demonstrated that Transformer embeddings can be
linearly decomposed into well-defined sums of factors, that can in turn be
related to specific network inputs or components. There is however still a
dearth of work studying whether these mathematical reformulations are
empirically meaningful. In the present work, we study representations from
machine-translation decoders using two of such embedding decomposition methods.
Our results indicate that, while decomposition-derived indicators effectively
correlate with model performance, variation across different runs suggests a
more nuanced take on this question. The high variability of our measurements
indicate that geometry reflects model-specific characteristics more than it
does sentence-specific computations, and that similar training conditions do
not guarantee similar vector spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06983">
<div class="article-summary-box-inner">
<span><p>Recent research shows that Large Language Models (LLMs) exhibit a compelling
level of proficiency in Theory of Mind (ToM) tasks. This ability to impute
unobservable mental states to others is vital to human social cognition and may
prove equally important in principal-agent relations between individual humans
and Artificial Intelligences (AIs). In this paper, we explore how a mechanism
studied in developmental psychology known as Violation of Expectation (VoE) can
be implemented to reduce errors in LLM prediction about users by leveraging
emergent ToM affordances. And we introduce a \textit{metacognitive prompting}
framework to apply VoE in the context of an AI tutor. By storing and retrieving
facts derived in cases where LLM expectation about the user was violated, we
find that LLMs are able to learn about users in ways that echo theories of
human learning. Finally, we discuss latent hazards and augmentative
opportunities associated with modeling user psychology and propose ways to
mitigate risk along with possible directions for future inquiry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06987">
<div class="article-summary-box-inner">
<span><p>The rapid progress in open-source large language models (LLMs) is
significantly advancing AI development. Extensive efforts have been made before
model release to align their behavior with human values, with the primary goal
of ensuring their helpfulness and harmlessness. However, even carefully aligned
models can be manipulated maliciously, leading to unintended behaviors, known
as "jailbreaks". These jailbreaks are typically triggered by specific text
inputs, often referred to as adversarial prompts. In this work, we propose the
generation exploitation attack, an extremely simple approach that disrupts
model alignment by only manipulating variations of decoding methods. By
exploiting different generation strategies, including varying decoding
hyper-parameters and sampling methods, we increase the misalignment rate from
0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,
and MPT families, outperforming state-of-the-art attacks with $30\times$ lower
computational cost. Finally, we propose an effective alignment method that
explores diverse generation strategies, which can reasonably reduce the
misalignment rate under our attack. Altogether, our study underscores a major
failure in current safety evaluation and alignment procedures for open-source
LLMs, strongly advocating for more comprehensive red teaming and better
alignment before releasing such models. Our code is available at
https://github.com/Princeton-SysML/Jailbreak_LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07008">
<div class="article-summary-box-inner">
<span><p>Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield
promising results in the Knowledge Graph Question Answering (KGQA) task.
However, the capacity of the models is limited and the quality decreases for
questions with less popular entities. In this paper, we present a novel
approach which works on top of the pre-trained Text-to-Text QA system to
address this issue. Our simple yet effective method performs filtering and
re-ranking of generated candidates based on their types derived from Wikidata
"instance_of" property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07018">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), through their contextualized representations,
have been empirically proven to encapsulate syntactic, semantic, word sense,
and common-sense knowledge. However, there has been limited exploration of
their physical reasoning abilities, specifically concerning the crucial
attributes for comprehending everyday objects. To address this gap, we
introduce NEWTON, a repository and benchmark for evaluating the physics
reasoning skills of LLMs. Further, to enable domain-specific adaptation of this
benchmark, we present a pipeline to enable researchers to generate a variant of
this benchmark that has been customized to the objects and attributes relevant
for their application. The NEWTON repository comprises a collection of 2800
object-attribute pairs, providing the foundation for generating infinite-scale
assessment templates. The NEWTON benchmark consists of 160K QA questions,
curated using the NEWTON repository to investigate the physical reasoning
capabilities of several mainstream language models across foundational,
explicit, and implicit reasoning tasks. Through extensive empirical analysis,
our results highlight the capabilities of LLMs for physical reasoning. We find
that LLMs like GPT-4 demonstrate strong reasoning capabilities in
scenario-based tasks but exhibit less consistency in object-attribute reasoning
compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates
its potential for evaluating and enhancing language models, paving the way for
their integration into physically grounded settings, such as robotic
manipulation. Project site: https://newtonreasoning.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07023">
<div class="article-summary-box-inner">
<span><p>Macros are building block tasks of our everyday smartphone activity (e.g.,
"login", or "booking a flight"). Effectively extracting macros is important for
understanding mobile interaction and enabling task automation. These macros are
however difficult to extract at scale as they can be comprised of multiple
steps yet hidden within programmatic components of the app. In this paper, we
introduce a novel approach based on Large Language Models (LLMs) to
automatically extract semantically meaningful macros from both random and
user-curated mobile interaction traces. The macros produced by our approach are
automatically tagged with natural language descriptions and are fully
executable. To examine the quality of extraction, we conduct multiple studies,
including user evaluation, comparative analysis against human-curated tasks,
and automatic execution of these macros. These experiments and analyses show
the effectiveness of our approach and the usefulness of extracted macros in
various downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07059">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification (MLTC) tasks in the medical domain often face
long-tail label distribution, where rare classes have fewer training samples
than frequent classes. Although previous works have explored different model
architectures and hierarchical label structures to find important features,
most of them neglect to incorporate the domain knowledge from medical
guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced
Classifier for medical diagnosis prediction with two innovations: (1) a
label-wise attention mechanism that incorporates a heterogeneous graph and
domain ontologies to capture the semantic relationships between medical
entities, (2) a simple yet effective group-wise training method based on
similarity of labels to increase samples of rare classes. We evaluate DKEC on
two real-world medical datasets: the RAA dataset, a collection of 4,417 patient
care reports from emergency medical services (EMS) incidents, and a subset of
53,898 reports from the MIMIC-III dataset. Experimental results show that our
method outperforms the state-of-the-art, particularly for the few-shot (tail)
classes. More importantly, we study the applicability of DKEC to different
language models and show that DKEC can help the smaller language models achieve
comparable performance to large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07064">
<div class="article-summary-box-inner">
<span><p>When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often hallucinate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on both numerical
reasoning and relational reasoning problems show that HtT improves existing
prompting methods, with an absolute gain of 11-27% in accuracy. The learned
rules are also transferable to different models and to different forms of the
same problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07075">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promising capabilities in using
external tools to solve complex problems. However, existing approaches either
involve fine-tuning on tool demonstrations, which do not generalize to new
tools without additional training, or providing tool documentation in context,
limiting the number of tools. Both approaches often generate syntactically
invalid tool calls. In this paper, we propose ToolDec, a finite-state
machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates
tool-related errors for any tool-augmented LLMs by ensuring valid tool names
and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively
select tools using only the information contained in their names, with no need
for fine-tuning or in-context documentation. We evaluated multiple prior
methods and their ToolDec-enhanced versions on a variety of tasks involving
tools like math functions, knowledge graph relations, and complex real-world
RESTful APIs. Our experiments show that ToolDec reduces syntactic errors to
zero, consequently achieving significantly better performance and as much as a
2x speedup. We also show that ToolDec achieves superior generalization
performance on unseen tools, performing up to 8x better than the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07078">
<div class="article-summary-box-inner">
<span><p>This paper makes two key contributions. First, it argues that highly
specialized rare content classifiers trained on small data typically have
limited exposure to the richness and topical diversity of the negative class
(dubbed anticontent) as observed in the wild. As a result, these classifiers'
strong performance observed on the test set may not translate into real-world
settings. In the context of COVID-19 misinformation detection, we conduct an
in-the-wild audit of multiple datasets and demonstrate that models trained with
several prominently cited recent datasets are vulnerable to anticontent when
evaluated in the wild. Second, we present a novel active learning pipeline that
requires zero manual annotation and iteratively augments the training data with
challenging anticontent, robustifying these classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07081">
<div class="article-summary-box-inner">
<span><p>Idioms are common in everyday language, but often pose a challenge to
translators because their meanings do not follow from the meanings of their
parts. Despite significant advances, machine translation systems still struggle
to translate idiomatic expressions. We provide a simple characterization of
idiomatic translation and related issues. This allows us to conduct a synthetic
experiment revealing a tipping point at which transformer-based machine
translation models correctly default to idiomatic translations. To expand
multilingual resources, we compile a dataset of ~4k natural sentences
containing idiomatic expressions in French, Finnish, and Japanese. To improve
translation of natural idioms, we introduce two straightforward yet effective
techniques: the strategic upweighting of training loss on potentially idiomatic
sentences, and using retrieval-augmented models. This not only improves the
accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in
absolute accuracy, but also holds potential benefits for non-idiomatic
sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07088">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are documented to struggle in settings that
require complex reasoning. Nevertheless, instructing the model to break down
the problem into smaller reasoning steps (Wei et al., 2022), or ensembling
various generations through modifying decoding steps (Wang et al., 2023) boosts
performance. Current methods assume that the input prompt is fixed and expect
the decoding strategies to introduce the diversity needed for ensembling. In
this work, we relax this assumption and discuss how one can create and leverage
variations of the input prompt as a means to diversity of thought to improve
model performance. We propose a method that automatically improves prompt
diversity by soliciting feedback from the LLM to ideate approaches that fit for
the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse
reasoning path Self-Ensemble) across multiple inference calls. We also propose
a cost-effective alternative where diverse prompts are used within a single
inference call; we call this IDIV-SE (In-call DIVerse reasoning path
Self-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform
the previously discussed baselines using both GPT-3.5 and GPT-4 on several
reasoning benchmarks, without modifying the decoding process. Additionally,
DIV-SE advances state-of-the-art performance on recent planning benchmarks
(Valmeekam et al., 2023), exceeding the highest previously reported accuracy by
at least 29.6 percentage points on the most challenging 4/5 Blocksworld task.
Our results shed light on how to enforce prompt diversity toward LLM reasoning
and thereby improve the pareto frontier of the accuracy-cost trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07091">
<div class="article-summary-box-inner">
<span><p>Document-based Visual Question Answering poses a challenging task between
linguistic sense disambiguation and fine-grained multimodal retrieval. Although
there has been encouraging progress in document-based question answering due to
the utilization of large language and open-world prior models\cite{1}, several
challenges persist, including prolonged response times, extended inference
durations, and imprecision in matching. In order to overcome these challenges,
we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive
question features, we leverage the exceptional capabilities of RoBERTa
large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we
subject the outputs from both models to a concatenation process. This operation
allows the model to consider information from diverse sources concurrently,
strengthening its representational capability. By leveraging pre-trained models
for feature extraction, our approach has the potential to amplify the
performance of these models through concatenation. After concatenation, we
apply dimensionality reduction to the output features, reducing the model's
computational effectiveness and inference time. Empirical results demonstrate
that our proposed model achieves competitive performance on Task C of the
PDF-VQA Dataset. If the user adds any new data, they should make sure to style
it as per the instructions provided in previous sections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07093">
<div class="article-summary-box-inner">
<span><p>To advance argumentative stance prediction as a multimodal problem, the First
Shared Task in Multimodal Argument Mining hosted stance prediction in crucial
social topics of gun control and abortion. Our exploratory study attempts to
evaluate the necessity of images for stance prediction in tweets and compare
out-of-the-box text-based large-language models (LLM) in few-shot settings
against fine-tuned unimodal and multimodal models. Our work suggests an
ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms
both the multimodal (0.677 F1-score) and text-based few-shot prediction using a
recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in
performance, our findings suggest that the multimodal models tend to perform
better when image content is summarized as natural language over their native
pixel structure and, using in-context examples improves few-shot performance of
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07096">
<div class="article-summary-box-inner">
<span><p>The Universal Transformer (UT) is a variant of the Transformer that shares
parameters across its layers. Empirical evidence shows that UTs have better
compositional generalization than Vanilla Transformers (VTs) in formal language
tasks. The parameter-sharing also affords it better parameter efficiency than
VTs. Despite its many advantages, scaling UT parameters is much more compute
and memory intensive than scaling up a VT. This paper proposes the Sparse
Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE)
and a new stick-breaking-based dynamic halting mechanism to reduce UT's
computation complexity while retaining its parameter efficiency and
generalization ability. Experiments show that SUT achieves the same performance
as strong baseline models while only using half computation and parameters on
WMT'14 and strong generalization results on formal language tasks (Logical
inference and CFQ). The new halting mechanism also enables around 50\%
reduction in computation during inference with very little performance decrease
on formal language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07106">
<div class="article-summary-box-inner">
<span><p>Deep Language Models (DLMs) provide a novel computational paradigm for
understanding the mechanisms of natural language processing in the human brain.
Unlike traditional psycholinguistic models, DLMs use layered sequences of
continuous numerical vectors to represent words and context, allowing a
plethora of emerging applications such as human-like text generation. In this
paper we show evidence that the layered hierarchy of DLMs may be used to model
the temporal dynamics of language comprehension in the brain by demonstrating a
strong correlation between DLM layer depth and the time at which layers are
most predictive of the human brain. Our ability to temporally resolve
individual layers benefits from our use of electrocorticography (ECoG) data,
which has a much higher temporal resolution than noninvasive methods like fMRI.
Using ECoG, we record neural activity from participants listening to a
30-minute narrative while also feeding the same narrative to a high-performing
DLM (GPT2-XL). We then extract contextual embeddings from the different layers
of the DLM and use linear encoding models to predict neural activity. We first
focus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our
model to track the increasing temporal receptive window along the linguistic
processing hierarchy from auditory to syntactic and semantic areas. Our results
reveal a connection between human language processing and DLMs, with the DLM's
layer-by-layer accumulation of contextual information mirroring the timing of
neural activity in high-order language areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Styles across Languages. (arXiv:2310.07135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07135">
<div class="article-summary-box-inner">
<span><p>Understanding how styles differ across languages is advantageous for training
both humans and computers to generate culturally appropriate text. We introduce
an explanation framework to extract stylistic differences from multilingual LMs
and compare styles across languages. Our framework (1) generates comprehensive
style lexica in any language and (2) consolidates feature importances from LMs
into comparable lexical categories. We apply this framework to compare
politeness, creating the first holistic multilingual politeness dataset and
exploring how politeness varies across four languages. Our approach enables an
effective evaluation of how distinct linguistic categories contribute to
stylistic variations and provides interpretable insights into how people
communicate differently around the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07137">
<div class="article-summary-box-inner">
<span><p>Product attribute value extraction plays an important role for many
real-world applications in e-Commerce such as product search and
recommendation. Previous methods treat it as a sequence labeling task that
needs more annotation for position of values in the product text. This limits
their application to real-world scenario in which only attribute values are
weakly-annotated for each product without their position. Moreover, these
methods only use product text (i.e., product title and description) and do not
consider the semantic connection between the multiple attribute values of a
given product and its text, which can help attribute value extraction. In this
paper, we reformulate this task as a multi-label classification task that can
be applied for real-world scenario in which only annotation of attribute values
is available to train models (i.e., annotation of positional information of
attribute values is not available). We propose a classification model with
semantic matching and negative label sampling for attribute value extraction.
Semantic matching aims to capture semantic interactions between attribute
values of a given product and its text. Negative label sampling aims to enhance
the model's ability of distinguishing similar values belonging to the same
attribute. Experimental results on three subsets of a large real-world
e-Commerce dataset demonstrate the effectiveness and superiority of our
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting. (arXiv:2310.07146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07146">
<div class="article-summary-box-inner">
<span><p>Mental illness remains one of the most critical public health issues of our
time, due to the severe scarcity and accessibility limit of professionals.
Psychotherapy requires high-level expertise to conduct deep, complex reasoning
and analysis on the cognition modeling of the patients. In the era of Large
Language Models, we believe it is the right time to develop AI assistance for
computational psychotherapy. We study the task of cognitive distortion
detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs
diagnosis on the patient's speech via three stages: subjectivity assessment to
separate the facts and the thoughts; contrastive reasoning to elicit the
reasoning processes supporting and contradicting the thoughts; and schema
analysis to summarize the cognition schemas. The generated diagnosis rationales
through the three stages are essential for assisting the professionals.
Experiments demonstrate that DoT obtains significant improvements over ChatGPT
for cognitive distortion detection, while generating high-quality rationales
approved by human experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07147">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have showcased remarkable impacts across a wide
spectrum of natural language processing tasks. Fine-tuning these pre-trained
models on downstream datasets provides further significant performance gains,
but this process has been challenging due to its extraordinary resource
requirements. To this end, existing efforts focus on parameter-efficient
fine-tuning, which, unfortunately, fail to capitalize on the powerful potential
of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized
Full-parameter Tuning framework for LLMs that enables memory-efficient
fine-tuning without harming performance. Our framework incorporates two novel
ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the
momentum and has consistent update magnitudes for each parameter, an inherent
advantage for robust quantization; and (ii) we quantize all model states and
store them as integer values, and present a gradient flow and parameter update
scheme for the quantized weights. As a result, QFT reduces the model state
memory to 21% of the standard solution while achieving comparable performance,
e.g., tuning a LLaMA-7B model requires only &lt;30GB of memory, satisfied by a
single A6000 GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07155">
<div class="article-summary-box-inner">
<span><p>Social media has become a major driver of social change, by facilitating the
formation of online social movements. Automatically understanding the
perspectives driving the movement and the voices opposing it, is a challenging
task as annotated data is difficult to obtain. We propose a weakly supervised
graph-based approach that explicitly models perspectives in
#BackLivesMatter-related tweets. Our proposed approach utilizes a
social-linguistic representation of the data. We convert the text to a graph by
breaking it into structured elements and connect it with the social network of
authors, then structured prediction is done over the elements for identifying
perspectives. Our approach uses a small seed set of labeled examples. We
experiment with large language models for generating artificial training
examples, compare them to manual annotation, and find that it achieves
comparable performance. We perform quantitative and qualitative analyses using
a human-annotated test set. Our model outperforms multitask baselines by a
large margin, successfully characterizing the perspectives supporting and
opposing #BLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07161">
<div class="article-summary-box-inner">
<span><p>Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,
the complexities introduced by acoustic transformations merit rigorous
analysis. This research, rooted in the exploration of proprietary sender-side
denoising effects, meticulously evaluates platforms such as Google Meets and
Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,
ensuring a structured examination tailored to various denoising settings and
receiver interfaces. A methodological novelty is introduced via the Oaxaca
decomposition, traditionally an econometric tool, repurposed herein to analyze
acoustic-phonetic perturbations within VoIP systems. To further ground the
implications of these transformations, psychoacoustic metrics, specifically
PESQ and STOI, were harnessed to furnish a comprehensive understanding of
speech alterations. Cumulatively, the insights garnered underscore the
intricate landscape of VoIP-influenced acoustic dynamics. In addition to the
primary findings, a multitude of metrics are reported, extending the research
purview. Moreover, out-of-domain benchmarking for both time and time-frequency
domain speech enhancement models is included, thereby enhancing the depth and
applicability of this inquiry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07170">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07177">
<div class="article-summary-box-inner">
<span><p>Speculative decoding is a pivotal technique to accelerate the inference of
large language models (LLMs) by employing a smaller draft model to predict the
target model's outputs. However, its efficacy can be limited due to the low
predictive accuracy of the draft model, particularly when faced with diverse
text inputs and a significant capability gap between the draft and target
models. We introduce online speculative decoding (OSD) to address this
challenge. The main idea is to continually update (multiple) draft model(s) on
observed user query data using the abundant excess computational power in an
LLM serving cluster. Given that LLM inference is memory-bounded, the surplus
computational power in a typical LLM serving cluster can be repurposed for
online retraining of draft models, thereby making the training cost-neutral.
Since the query distribution of an LLM service is relatively simple, retraining
on query distribution enables the draft model to more accurately predict the
target model's outputs, particularly on data originating from query
distributions. As the draft model evolves online, it aligns with the query
distribution in real time, mitigating distribution shifts. We develop a
prototype of online speculative decoding based on online knowledge distillation
and evaluate it using both synthetic and real query data on several popular
LLMs. The results show a substantial increase in the token acceptance rate by
0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07188">
<div class="article-summary-box-inner">
<span><p>Large language models, such as OpenAI's ChatGPT, have demonstrated
exceptional language understanding capabilities in various NLP tasks. Sparsely
activated mixture-of-experts (MoE) has emerged as a promising solution for
scaling models while maintaining a constant number of computational operations.
Existing MoE model adopts a fixed gating network where each token is computed
by the same number of experts. However, this approach contradicts our intuition
that the tokens in each sequence vary in terms of their linguistic complexity
and, consequently, require different computational costs. Little is discussed
in prior research on the trade-off between computation per token and model
performance. This paper introduces adaptive gating in MoE, a flexible training
strategy that allows tokens to be processed by a variable number of experts
based on expert probability distribution. The proposed framework preserves
sparsity while improving training efficiency. Additionally, curriculum learning
is leveraged to further reduce training time. Extensive experiments on diverse
NLP tasks show that adaptive gating reduces at most 22.5% training time while
maintaining inference quality. Moreover, we conduct a comprehensive analysis of
the routing decisions and present our insights when adaptive gating is used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07225">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown promise in medical question answering
by achieving passing scores in standardised exams and have been suggested as
tools for supporting healthcare workers. Deploying LLMs into such a high-risk
context requires a clear understanding of the limitations of these models. With
the rapid development and release of new LLMs, it is especially valuable to
identify patterns which exist across models and may, therefore, continue to
appear in newer versions. In this paper, we evaluate a wide range of popular
LLMs on their knowledge of medical questions in order to better understand
their properties as a group. From this comparison, we provide preliminary
observations and raise open questions for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07251">
<div class="article-summary-box-inner">
<span><p>In this position paper, we argue that instead of morally aligning LLMs to
specific set of ethical principles, we should infuse generic ethical reasoning
capabilities into them so that they can handle value pluralism at a global
scale. When provided with an ethical policy, an LLM should be capable of making
decisions that are ethically consistent to the policy. We develop a framework
that integrates moral dilemmas with moral principles pertaining to different
foramlisms of normative ethics, and at different levels of abstractions.
Initial experiments with GPT-x models shows that while GPT-4 is a nearly
perfect ethical reasoner, the models still have bias towards the moral values
of Western and English speaking societies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07276">
<div class="article-summary-box-inner">
<span><p>Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing expressivity transfer in textless speech-to-speech translation. (arXiv:2310.07279v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07279">
<div class="article-summary-box-inner">
<span><p>Textless speech-to-speech translation systems are rapidly advancing, thanks
to the integration of self-supervised learning techniques. However, existing
state-of-the-art systems fall short when it comes to capturing and transferring
expressivity accurately across different languages. Expressivity plays a vital
role in conveying emotions, nuances, and cultural subtleties, thereby enhancing
communication across diverse languages. To address this issue this study
presents a novel method that operates at the discrete speech unit level and
leverages multilingual emotion embeddings to capture language-agnostic
information. Specifically, we demonstrate how these embeddings can be used to
effectively predict the pitch and duration of speech units in the target
language. Through objective and subjective experiments conducted on a
French-to-English translation task, our findings highlight the superior
expressivity transfer achieved by our approach compared to current
state-of-the-art systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07282">
<div class="article-summary-box-inner">
<span><p>This paper conducts a comprehensive investigation into applying large
language models, particularly on BioBERT, in healthcare. It begins with
thoroughly examining previous natural language processing (NLP) approaches in
healthcare, shedding light on the limitations and challenges these methods
face. Following that, this research explores the path that led to the
incorporation of BioBERT into healthcare applications, highlighting its
suitability for addressing the specific requirements of tasks related to
biomedical text mining. The analysis outlines a systematic methodology for
fine-tuning BioBERT to meet the unique needs of the healthcare domain. This
approach includes various components, including the gathering of data from a
wide range of healthcare sources, data annotation for tasks like identifying
medical entities and categorizing them, and the application of specialized
preprocessing techniques tailored to handle the complexities found in
biomedical texts. Additionally, the paper covers aspects related to model
evaluation, with a focus on healthcare benchmarks and functions like processing
of natural language in biomedical, question-answering, clinical document
classification, and medical entity recognition. It explores techniques to
improve the model's interpretability and validates its performance compared to
existing healthcare-focused language models. The paper thoroughly examines
ethical considerations, particularly patient privacy and data security. It
highlights the benefits of incorporating BioBERT into healthcare contexts,
including enhanced clinical decision support and more efficient information
retrieval. Nevertheless, it acknowledges the impediments and complexities of
this integration, encompassing concerns regarding data privacy, transparency,
resource-intensive requirements, and the necessity for model customization to
align with diverse healthcare domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07284">
<div class="article-summary-box-inner">
<span><p>Humans possess an extraordinary ability to selectively focus on the sound
source of interest amidst complex acoustic environments, commonly referred to
as cocktail party scenarios. In an attempt to replicate this remarkable
auditory attention capability in machines, target speaker extraction (TSE)
models have been developed. These models leverage the pre-registered cues of
the target speaker to extract the sound source of interest. However, the
effectiveness of these models is hindered in real-world scenarios due to the
potential variation or even absence of pre-registered cues. To address this
limitation, this study investigates the integration of natural language to
enhance the flexibility and controllability of existing TSE models.
Specifically, we propose a model named LLM-TSE, wherein a large language model
(LLM) to extract useful semantic cues from the user's typed text input, which
can complement the pre-registered cues or work independently to control the TSE
process. Our experimental results demonstrate competitive performance when only
text-based cues are presented, and a new state-of-the-art is set when combined
with pre-registered acoustic cues. To the best of our knowledge, this is the
first work that has successfully incorporated text-based cues to guide target
speaker extraction, which can be a cornerstone for cocktail party problem
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07289">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) outperform information retrieval techniques for
downstream knowledge-intensive tasks when being prompted to generate world
knowledge. However, community concerns abound regarding the factuality and
potential implications of using this uncensored knowledge. In light of this, we
introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to
systematically and automatically evaluate generated knowledge from six
important perspectives -- Factuality, Relevance, Coherence, Informativeness,
Helpfulness and Validity. We conduct an extensive empirical analysis of the
generated knowledge from three different types of LLMs on two widely studied
knowledge-intensive tasks, i.e., open-domain question answering and
knowledge-grounded dialogue. Surprisingly, our study reveals that the
factuality of generated knowledge, even if lower, does not significantly hinder
downstream tasks. Instead, the relevance and coherence of the outputs are more
important than small factual mistakes. Further, we show how to use CONNER to
improve knowledge-intensive tasks by designing two strategies: Prompt
Engineering and Knowledge Selection. Our evaluation code and LLM-generated
knowledge with human annotations will be released to facilitate future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07299">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07301">
<div class="article-summary-box-inner">
<span><p>Impressive progress has been made on chat models based on Large Language
Models (LLMs) recently; however, there is a noticeable lag in multi-turn
conversations between open-source chat models (e.g., Alpaca and Vicuna) and the
leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we
attribute the lag to the lack of enough high-quality multi-turn
instruction-tuning data. The available instruction-tuning data for the
community are either single-turn conversations or multi-turn ones with certain
issues, such as non-human-like instructions, less detailed responses, or rare
topic shifts. In this paper, we address these challenges by introducing Parrot,
a highly scalable solution designed to automatically generate high-quality
instruction-tuning data, which are then used to enhance the effectiveness of
chat models in multi-turn conversations. Specifically, we start by training the
Parrot-Ask model, which is designed to emulate real users in generating
instructions. We then utilize Parrot-Ask to engage in multi-turn conversations
with ChatGPT across a diverse range of topics, resulting in a collection of 40K
high-quality multi-turn dialogues (Parrot-40K). These data are subsequently
employed to train a chat model that we have named Parrot-Chat. We demonstrate
that the dialogues gathered from Parrot-Ask markedly outperform existing
multi-turn instruction-following datasets in critical metrics, including topic
diversity, number of turns, and resemblance to human conversation. With only
40K training examples, Parrot-Chat achieves strong performance against other
13B open-source models across a range of instruction-following benchmarks, and
particularly excels in evaluations of multi-turn capabilities. We make all
codes, datasets, and two versions of the Parrot-Ask model based on LLaMA2-13B
and KuaiYii-13B available at https://github.com/kwai/KwaiYii/Parrot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07306">
<div class="article-summary-box-inner">
<span><p>This paper presents a Soft Labeling and Noisy Mixup-based open intent
classification model (SNOiC). Most of the previous works have used
threshold-based methods to identify open intents, which are prone to
overfitting and may produce biased predictions. Additionally, the need for more
available data for an open intent class presents another limitation for these
existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to
reduce the biasing and generate pseudo-data for open intent class. The
experimental results on four benchmark datasets show that the SNOiC model
achieves a minimum and maximum performance of 68.72\% and 94.71\%,
respectively, in identifying open intents. Moreover, compared to
state-of-the-art models, the SNOiC model improves the performance of
identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's
efficacy is further established by analyzing various parameters used in the
proposed model. An ablation study is also conducted, which involves creating
three model variants to validate the effectiveness of the SNOiC model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07321">
<div class="article-summary-box-inner">
<span><p>Traditionally, large language models have been either trained on general web
crawls or domain-specific data. However, recent successes of generative large
language models, have shed light on the benefits of cross-domain datasets. To
examine the significance of prioritizing data diversity over quality, we
present a German dataset comprising texts from five domains, along with another
dataset aimed at containing high-quality data. Through training a series of
models ranging between 122M and 750M parameters on both datasets, we conduct a
comprehensive benchmark on multiple downstream tasks. Our findings demonstrate
that the models trained on the cross-domain dataset outperform those trained on
quality data alone, leading to improvements up to $4.45\%$ over the previous
state-of-the-art. The models are available at
https://huggingface.co/ikim-uk-essen
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07328">
<div class="article-summary-box-inner">
<span><p>The success of ChatGPT validates the potential of large language models
(LLMs) in artificial general intelligence (AGI). Subsequently, the release of
LLMs has sparked the open-source community's interest in instruction-tuning,
which is deemed to accelerate ChatGPT's replication process. However, research
on instruction-tuning LLMs in Chinese, the world's most spoken language, is
still in its early stages. Therefore, this paper makes an in-depth empirical
study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that
provides valuable findings for effectively customizing LLMs that can better
respond to Chinese instructions. Specifically, we systematically explore the
impact of LLM bases, parameter-efficient methods, instruction data types, which
are the three most important elements for instruction-tuning. Besides, we also
conduct experiment to study the impact of other factors, e.g., chain-of-thought
data and human-value alignment. We hope that this empirical study can make a
modest contribution to the open Chinese version of ChatGPT. This paper will
release a powerful Chinese LLMs that is comparable to ChatGLM. The code and
data are available at https://github.com/PhoebusSi/Alpaca-CoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances. (arXiv:2310.07343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07343">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) are impressive in solving various
tasks, they can quickly be outdated after deployment. Maintaining their
up-to-date status is a pressing concern in the current era. This paper provides
a comprehensive review of recent advances in aligning LLMs with the
ever-changing world knowledge without re-training from scratch. We categorize
research works systemically and provide in-depth comparisons and discussion. We
also discuss existing challenges and highlight future directions to facilitate
research in this field. We release the paper list at
https://github.com/hyintell/awesome-refreshing-llms
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07345">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the effect of language models (LMs) with
different context lengths and label units (phoneme vs. word) used in sequence
discriminative training for phoneme-based neural transducers. Both lattice-free
and N-best-list approaches are examined. For lattice-free methods with
phoneme-level LMs, we propose a method to approximate the context history to
employ LMs with full-context dependency. This approximation can be extended to
arbitrary context length and enables the usage of word-level LMs in
lattice-free methods. Moreover, a systematic comparison is conducted across
lattice-free and N-best-list-based methods. Experimental results on Librispeech
show that using the word-level LM in training outperforms the phoneme-level LM.
Besides, we find that the context size of the LM used for probability
computation has a limited effect on performance. Moreover, our results reveal
the pivotal importance of the hypothesis space quality in sequence
discriminative training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07347">
<div class="article-summary-box-inner">
<span><p>ELECTRA pre-trains language models by detecting tokens in a sequence that
have been replaced by an auxiliary model. Although ELECTRA offers a significant
boost in efficiency, its potential is constrained by the training cost brought
by the auxiliary model. Notably, this model, which is jointly trained with the
main model, only serves to assist the training of the main model and is
discarded post-training. This results in a substantial amount of training cost
being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which
leverages an existing language model as the auxiliary model. To construct a
learning curriculum for the main model, we smooth its output distribution via
temperature scaling following a descending schedule. Our approach rivals the
performance of state-of-the-art ELECTRA-style pre-training methods, while
significantly eliminating the computation and memory cost brought by the joint
training of the auxiliary model. Our method also reduces the sensitivity to
hyper-parameters and enhances the pre-training stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic laws in biology. (arXiv:2310.07387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07387">
<div class="article-summary-box-inner">
<span><p>Linguistic laws, the common statistical patterns of human language, have been
investigated by quantitative linguists for nearly a century. Recently,
biologists from a range of disciplines have started to explore the prevalence
of these laws beyond language, finding patterns consistent with linguistic laws
across multiple levels of biological organisation, from molecular (genomes,
genes, and proteins) to organismal (animal behaviour) to ecological
(populations and ecosystems). We propose a new conceptual framework for the
study of linguistic laws in biology, comprising and integrating distinct levels
of analysis, from description to prediction to theory building. Adopting this
framework will provide critical new insights into the fundamental rules of
organisation underpinning natural systems, unifying linguistic laws and core
theory in biology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07397">
<div class="article-summary-box-inner">
<span><p>Target-oriented dialogue systems, designed to proactively steer conversations
toward predefined targets or accomplish specific system-side goals, are an
exciting area in conversational AI. In this work, by formulating a &lt;dialogue
act, topic&gt; pair as the conversation target, we explore a novel problem of
personalized target-oriented dialogue by considering personalization during the
target accomplishment process. However, there remains an emergent need for
high-quality datasets, and building one from scratch requires tremendous human
effort. To address this, we propose an automatic dataset curation framework
using a role-playing approach. Based on this framework, we construct a
large-scale personalized target-oriented dialogue dataset, TopDial, which
comprises about 18K multi-turn dialogues. The experimental results show that
this dataset is of high quality and could contribute to exploring personalized
target-oriented dialogue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07403">
<div class="article-summary-box-inner">
<span><p>Direct speech-to-speech translation (S2ST) translates speech from one
language into another using a single model. However, due to the presence of
linguistic and acoustic diversity, the target speech follows a complex
multimodal distribution, posing challenges to achieving both high-quality
translations and fast decoding speeds for S2ST models. In this paper, we
propose DASpeech, a non-autoregressive direct S2ST model which realizes both
fast and high-quality S2ST. To better capture the complex distribution of the
target speech, DASpeech adopts the two-pass architecture to decompose the
generation process into two steps, where a linguistic decoder first generates
the target text, and an acoustic decoder then generates the target speech based
on the hidden states of the linguistic decoder. Specifically, we use the
decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as
the acoustic decoder. DA-Transformer models translations with a directed
acyclic graph (DAG). To consider all potential paths in the DAG during
training, we calculate the expected hidden states for each target token via
dynamic programming, and feed them into the acoustic decoder to predict the
target mel-spectrogram. During inference, we select the most probable path and
take hidden states on that path as input to the acoustic decoder. Experiments
on the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or
even better performance than the state-of-the-art S2ST model Translatotron 2,
while preserving up to 18.53x speedup compared to the autoregressive baseline.
Compared with the previous non-autoregressive S2ST model, DASpeech does not
rely on knowledge distillation and iterative decoding, achieving significant
improvements in both translation quality and decoding speed. Furthermore,
DASpeech shows the ability to preserve the speaker's voice of the source speech
during translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07423">
<div class="article-summary-box-inner">
<span><p>Recently, large pre-trained multilingual speech models have shown potential
in scaling Automatic Speech Recognition (ASR) to many low-resource languages.
Some of these models employ language adapters in their formulation, which helps
to improve monolingual performance and avoids some of the drawbacks of
multi-lingual modeling on resource-rich languages. However, this formulation
restricts the usability of these models on code-switched speech, where two
languages are mixed together in the same utterance. In this work, we propose
ways to effectively fine-tune such models on code-switched speech, by
assimilating information from both language adapters at each language
adaptation point in the network. We also model code-switching as a sequence of
latent binary sequences that can be used to guide the flow of information from
each language adapter at the frame level. The proposed approaches are evaluated
on three code-switched datasets encompassing Arabic, Mandarin, and Hindi
languages paired with English, showing consistent improvements in
code-switching performance with at least 10\% absolute reduction in CER across
all test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07487">
<div class="article-summary-box-inner">
<span><p>Phonological reconstruction is one of the central problems in historical
linguistics where a proto-word of an ancestral language is determined from the
observed cognate words of daughter languages. Computational approaches to
historical linguistics attempt to automate the task by learning models on
available linguistic data. Several ideas and techniques drawn from
computational biology have been successfully applied in the area of
computational historical linguistics. Following these lines, we adapt MSA
Transformer, a protein language model, to the problem of automated phonological
reconstruction. MSA Transformer trains on multiple sequence alignments as input
and is, thus, apt for application on aligned cognate words. We, hence, name our
model as Cognate Transformer. We also apply the model on another associated
task, namely, cognate reflex prediction, where a reflex word in a daughter
language is predicted based on cognate words from other daughter languages. We
show that our model outperforms the existing models on both tasks, especially
when it is pre-trained on masked word prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07488">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have demonstrated
remarkable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical
reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)
and Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated
by the models. Empirical studies demonstrate that KwaiYiiMath can achieve
state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07521">
<div class="article-summary-box-inner">
<span><p>This survey addresses the crucial issue of factuality in Large Language
Models (LLMs). As LLMs find applications across diverse domains, the
reliability and accuracy of their outputs become vital. We define the
Factuality Issue as the probability of LLMs to produce content inconsistent
with established facts. We first delve into the implications of these
inaccuracies, highlighting the potential consequences and challenges posed by
factual errors in LLM outputs. Subsequently, we analyze the mechanisms through
which LLMs store and process facts, seeking the primary causes of factual
errors. Our discussion then transitions to methodologies for evaluating LLM
factuality, emphasizing key metrics, benchmarks, and studies. We further
explore strategies for enhancing LLM factuality, including approaches tailored
for specific domains. We focus two primary LLM configurations standalone LLMs
and Retrieval-Augmented LLMs that utilizes external data, we detail their
unique challenges and potential enhancements. Our survey offers a structured
guide for researchers aiming to fortify the factual reliability of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v8 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over RDF data like knowledge graphs has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, the IR and NLP
communities have addressed QA over text, but such systems barely utilize
semantic data and knowledge. This paper presents a method for complex questions
that can seamlessly operate over a mixture of RDF datasets and text corpora, or
individual sources, in a unified framework. Our method, called UNIQORN, builds
a context graph on-the-fly, by retrieving question-relevant evidences from the
RDF data and/or a text corpus, using fine-tuned BERT models. The resulting
graph typically contains all question-relevant evidences but also a lot of
noise. UNIQORN copes with this input by a graph algorithm for Group Steiner
Trees, that identifies the best answer candidates in the context graph.
Experimental results on several benchmarks of complex questions with multiple
entities and relations, show that UNIQORN significantly outperforms
state-of-the-art methods for heterogeneous QA -- in a full training mode, as
well as in zero-shot settings. The graph-based methodology provides
user-interpretable evidence for the complete answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07994">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) like BERT have made significant progress
in various downstream NLP tasks. However, by asking models to do cloze-style
tests, recent work finds that PLMs are short in acquiring knowledge from
unstructured text. To understand the internal behaviour of PLMs in retrieving
knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free
(K-F) tokens for unstructured text and ask professional annotators to label
some samples manually. Then, we find that PLMs are more likely to give wrong
predictions on K-B tokens and attend less attention to those tokens inside the
self-attention module. Based on these observations, we develop two solutions to
help the model learn more knowledge from unstructured text in a fully
self-supervised manner. Experiments on knowledge-intensive tasks show the
effectiveness of the proposed methods. To our best knowledge, we are the first
to explore fully self-supervised learning of knowledge in continual
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12920">
<div class="article-summary-box-inner">
<span><p>Current multilingual semantic parsing (MSP) datasets are almost all collected
by translating the utterances in the existing datasets from the resource-rich
language to the target language. However, manual translation is costly. To
reduce the translation effort, this paper proposes the first active learning
procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing
datasets to be translated. We also propose a novel selection method that
prioritizes the examples diversifying the logical form structures with more
lexical choices, and a novel hyperparameter tuning method that needs no extra
annotation cost. Our experiments show that AL-MSP significantly reduces
translation costs with ideal selection methods. Our selection method with
proper hyperparameters yields better parsing performance than the other
baselines on two multilingual datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07678">
<div class="article-summary-box-inner">
<span><p>This paper introduces a simple yet effective query expansion approach,
denoted as query2doc, to improve both sparse and dense retrieval systems. The
proposed method first generates pseudo-documents by few-shot prompting large
language models (LLMs), and then expands the query with generated
pseudo-documents. LLMs are trained on web-scale text corpora and are adept at
knowledge memorization. The pseudo-documents from LLMs often contain highly
relevant information that can aid in query disambiguation and guide the
retrievers. Experimental results demonstrate that query2doc boosts the
performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and
TREC DL, without any model fine-tuning. Furthermore, our method also benefits
state-of-the-art dense retrievers in terms of both in-domain and out-of-domain
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat with the Environment: Interactive Multimodal Perception Using Large Language Models. (arXiv:2303.08268v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08268">
<div class="article-summary-box-inner">
<span><p>Programming robot behavior in a complex world faces challenges on multiple
levels, from dextrous low-level skills to high-level planning and reasoning.
Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning
ability in few-shot robotic planning. However, it remains challenging to ground
LLMs in multimodal sensory input and continuous action output, while enabling a
robot to interact with its environment and acquire novel information as its
policies unfold. We develop a robot interaction scenario with a partially
observable state, which necessitates a robot to decide on a range of epistemic
actions in order to sample sensory information among multiple modalities,
before being able to execute the task correctly. Matcha (Multimodal environment
chatting) agent, an interactive perception framework, is therefore proposed
with an LLM as its backbone, whose ability is exploited to instruct epistemic
actions and to reason over the resulting multimodal sensations (vision, sound,
haptics, proprioception), as well as to plan an entire task execution based on
the interactively acquired information. Our study demonstrates that LLMs can
provide high-level planning and reasoning skills and control interactive robot
behavior in a multimodal environment, while multimodal modules with the context
of the environmental state help ground the LLMs and extend their processing
ability. The project website can be found at https://matcha-agent.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08518">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are popular for their impressive abilities, but
the need for model-specific fine-tuning or task-specific prompt engineering can
hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for
Improving zero-Shot Evaluation), which tunes a lightweight and versatile
retriever that automatically retrieves prompts for a given zero-shot task
input. Specifically, we demonstrate universality in a cross-task and
cross-model scenario: the retriever is tuned on a diverse set of tasks, but
tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for
tuning the retriever, but test the retriever on different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that
UPRISE mitigates the hallucination problem in our experiments with ChatGPT,
suggesting its potential to improve even the strongest LLMs. Our model and code
are available at https://github.com/microsoft/LMOps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08896">
<div class="article-summary-box-inner">
<span><p>Generative Large Language Models (LLMs) such as GPT-3 are capable of
generating highly fluent responses to a wide variety of user prompts. However,
LLMs are known to hallucinate facts and make non-factual statements which can
undermine trust in their output. Existing fact-checking approaches either
require access to the output probability distribution (which may not be
available for systems such as ChatGPT) or external databases that are
interfaced via separate, often complex, modules. In this work, we propose
"SelfCheckGPT", a simple sampling-based approach that can be used to fact-check
the responses of black-box models in a zero-resource fashion, i.e. without an
external database. SelfCheckGPT leverages the simple idea that if an LLM has
knowledge of a given concept, sampled responses are likely to be similar and
contain consistent facts. However, for hallucinated facts, stochastically
sampled responses are likely to diverge and contradict one another. We
investigate this approach by using GPT-3 to generate passages about individuals
from the WikiBio dataset, and manually annotate the factuality of the generated
passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and
factual sentences; and ii) rank passages in terms of factuality. We compare our
approach to several baselines and show that our approach has considerably
higher AUC-PR scores in sentence-level hallucination detection and higher
correlation scores in passage-level factuality assessment compared to grey-box
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Interpretable Mental Health Analysis with Large Language Models. (arXiv:2304.03347v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03347">
<div class="article-summary-box-inner">
<span><p>The latest large language models (LLMs) such as ChatGPT, exhibit strong
capabilities in automated mental health analysis. However, existing relevant
studies bear several limitations, including inadequate evaluations, lack of
prompting strategies, and ignorance of exploring LLMs for explainability. To
bridge these gaps, we comprehensively evaluate the mental health analysis and
emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore
the effects of different prompting strategies with unsupervised and distantly
supervised emotional information. Based on these prompts, we explore LLMs for
interpretable mental health analysis by instructing them to generate
explanations for each of their decisions. We convey strict human evaluations to
assess the quality of the generated explanations, leading to a novel dataset
with 163 human-assessed explanations. We benchmark existing automatic
evaluation metrics on this dataset to guide future related works. According to
the results, ChatGPT shows strong in-context learning ability but still has a
significant gap with advanced task-specific methods. Careful prompt engineering
with emotional cues and expert-written few-shot examples can also effectively
improve performance on mental health analysis. In addition, ChatGPT generates
explanations that approach human performance, showing its great potential in
explainable mental health analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11082">
<div class="article-summary-box-inner">
<span><p>An important aspect in developing language models that interact with humans
is aligning their behavior to be useful and unharmful for their human users.
This is usually achieved by tuning the model in a way that enhances desired
behaviors and inhibits undesired ones, a process referred to as alignment. In
this paper, we propose a theoretical approach called Behavior Expectation
Bounds (BEB) which allows us to formally investigate several inherent
characteristics and limitations of alignment in large language models.
Importantly, we prove that within the limits of this framework, for any
behavior that has a finite probability of being exhibited by the model, there
exist prompts that can trigger the model into outputting this behavior, with
probability that increases with the length of the prompt. This implies that any
alignment process that attenuates an undesired behavior but does not remove it
altogether, is not safe against adversarial prompting attacks. Furthermore, our
framework hints at the mechanism by which leading alignment approaches such as
reinforcement learning from human feedback make the LLM prone to being prompted
into the undesired behaviors. This theoretical result is being experimentally
demonstrated in large scale by the so called contemporary "chatGPT jailbreaks",
where adversarial users trick the LLM into breaking its alignment guardrails by
triggering it into acting as a malicious persona. Our results expose
fundamental limitations in alignment of LLMs and bring to the forefront the
need to devise reliable mechanisms for ensuring AI safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14767">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) are known to capture factual
knowledge in their parameters. While previous work looked into where factual
associations are stored, only little is known about how they are retrieved
internally during inference. We investigate this question through the lens of
information flow. Given a subject-relation query, we study how the model
aggregates information about the subject and relation to predict the correct
attribute. With interventions on attention edges, we first identify two
critical points where information propagates to the prediction: one from the
relation positions followed by another from the subject positions. Next, by
analyzing the information at these points, we unveil a three-step internal
mechanism for attribute extraction. First, the representation at the
last-subject position goes through an enrichment process, driven by the early
MLP sublayers, to encode many subject-related attributes. Second, information
from the relation propagates to the prediction. Third, the prediction
representation "queries" the enriched subject to extract the attribute. Perhaps
surprisingly, this extraction is typically done via attention heads, which
often encode subject-attribute mappings in their parameters. Overall, our
findings introduce a comprehensive view of how factual associations are stored
and extracted internally in LMs, facilitating future research on knowledge
localization and editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14933">
<div class="article-summary-box-inner">
<span><p>Model merging (e.g., via interpolation or task arithmetic) fuses multiple
models trained on different tasks to generate a multi-task solution. The
technique has been proven successful in previous studies, where the models are
trained on similar tasks and with the same initialization. In this paper, we
expand on this concept to a multimodal setup by merging transformers trained on
different modalities. Furthermore, we conduct our study for a novel goal where
we can merge vision, language, and cross-modal transformers of a
modality-specific architecture to create a parameter-efficient
modality-agnostic architecture. Through comprehensive experiments, we
systematically investigate the key factors impacting model performance after
merging, including initialization, merging mechanisms, and model architectures.
We also propose two metrics that assess the distance between weights to be
merged and can serve as an indicator of the merging outcomes. Our analysis
leads to an effective training recipe for matching the performance of the
modality-agnostic baseline (i.e., pre-trained from scratch) via model merging.
Our method also outperforms naive merging significantly on various tasks, with
improvements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k
and 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05658">
<div class="article-summary-box-inner">
<span><p>For a robot to personalize physical assistance effectively, it must learn
user preferences that can be generally reapplied to future scenarios. In this
work, we investigate personalization of household cleanup with robots that can
tidy up rooms by picking up objects and putting them away. A key challenge is
determining the proper place to put each object, as people's preferences can
vary greatly depending on personal taste or cultural background. For instance,
one person may prefer storing shirts in the drawer, while another may prefer
them on the shelf. We aim to build systems that can learn such preferences from
just a handful of examples via prior interactions with a particular person. We
show that robots can combine language-based planning and perception with the
few-shot summarization capabilities of large language models (LLMs) to infer
generalized user preferences that are broadly applicable to future
interactions. This approach enables fast adaptation and achieves 91.2% accuracy
on unseen objects in our benchmark dataset. We also demonstrate our approach on
a real-world mobile manipulator called TidyBot, which successfully puts away
85.0% of objects in real-world test scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks. (arXiv:2305.05862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05862">
<div class="article-summary-box-inner">
<span><p>The most recent large language models(LLMs) such as ChatGPT and GPT-4 have
shown exceptional capabilities of generalist models, achieving state-of-the-art
performance on a wide range of NLP tasks with little or no adaptation. How
effective are such models in the financial domain? Understanding this basic
question would have a significant impact on many downstream financial
analytical tasks. In this paper, we conduct an empirical study and provide
experimental evidences of their performance on a wide variety of financial text
analytical problems, using eight benchmark datasets from five categories of
tasks. We report both the strengths and limitations of the current models by
comparing them to the state-of-the-art fine-tuned approaches and the recently
released domain-specific pretrained models. We hope our study can help
understand the capability of the existing models in the financial domain and
facilitate further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08703">
<div class="article-summary-box-inner">
<span><p>Conventional Knowledge Graph Construction (KGC) approaches typically follow
the static information extraction paradigm with a closed set of pre-defined
schema. As a result, such approaches fall short when applied to dynamic
scenarios or domains, whereas a new type of knowledge emerges. This
necessitates a system that can handle evolving schema automatically to extract
information for KGC. To address this need, we propose a new task called
schema-adaptable KGC, which aims to continually extract entity, relation, and
event based on a dynamically changing schema graph without re-training. We
first split and convert existing datasets based on three principles to build a
benchmark, i.e., horizontal schema expansion, vertical schema expansion, and
hybrid schema expansion; then investigate the schema-adaptable performance of
several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We
further propose a simple yet effective baseline dubbed \textsc{AdaKGC}, which
contains schema-enriched prefix instructor and schema-conditioned dynamic
decoding to better handle evolving schema. Comprehensive experimental results
illustrate that AdaKGC can outperform baselines but still have room for
improvement. We hope the proposed work can deliver benefits to the community.
Code and datasets available at https://github.com/zjunlp/AdaKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08732">
<div class="article-summary-box-inner">
<span><p>Previous studies have revealed that vanilla pre-trained language models
(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,
several works have attempted to integrate external knowledge into PLMs.
However, despite the promising outcome, we empirically observe that PLMs may
have already encoded rich knowledge in their pre-trained parameters but fail to
fully utilize them when applying them to knowledge-intensive tasks. In this
paper, we propose a new paradigm dubbed Knowledge Rumination to help the
pre-trained language model utilize that related latent knowledge without
retrieving it from the external corpus. By simply adding a prompt like "As far
as I know" to the PLMs, we try to review related latent knowledge and inject
them back into the model for knowledge consolidation. We apply the proposed
knowledge rumination to various language models, including RoBERTa, DeBERTa,
and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE
benchmarks demonstrate the effectiveness of our proposed approach, which proves
that the knowledge stored in PLMs can be better exploited to enhance
performance. Code is available in
https://github.com/zjunlp/knowledge-rumination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Equivariant Transfer Learning from Pretrained Models. (arXiv:2305.09900v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09900">
<div class="article-summary-box-inner">
<span><p>Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of Basu
et al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and
optimization-based methods, respectively, over features from group-transformed
inputs to obtain equivariant outputs from non-equivariant neural networks.
While Kaba et al. (2022) are only concerned with training from scratch, we find
that equitune performs poorly on equivariant zero-shot tasks despite good
finetuning results. We hypothesize that this is because pretrained models
provide better quality features for certain transformations than others and
simply averaging them is deleterious. Hence, we propose {\lambda}-equitune that
averages the features using importance weights, {\lambda}s. These weights are
learned directly from the data using a small neural network, leading to
excellent zero-shot and finetuned results that outperform equitune. Further, we
prove that {\lambda}-equitune is equivariant and a universal approximator of
equivariant functions. Additionally, we show that the method of Kaba et al.
(2022) used with appropriate loss functions, which we call equizero, also gives
excellent zero-shot and finetuned performance. Both equitune and equizero are
special cases of {\lambda}-equitune. To show the simplicity and generality of
our method, we validate on a wide range of diverse applications and models such
as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13172">
<div class="article-summary-box-inner">
<span><p>Despite the ability to train capable LLMs, the methodology for maintaining
their relevancy and rectifying errors remains elusive. To this end, the past
few years have witnessed a surge in techniques for editing LLMs, the objective
of which is to efficiently alter the behavior of LLMs within a specific domain
without negatively impacting performance across other inputs. This paper
embarks on a deep exploration of the problems, methods, and opportunities
related to model editing for LLMs. In particular, we provide an exhaustive
overview of the task definition and challenges associated with model editing,
along with an in-depth empirical analysis of the most progressive methods
currently at our disposal. We also build a new benchmark dataset to facilitate
a more robust evaluation and pinpoint enduring issues intrinsic to existing
techniques. Our objective is to provide valuable insights into the
effectiveness and feasibility of each editing technique, thereby assisting the
community in making informed decisions on the selection of the most appropriate
method for a specific task or context. Code and datasets are available at
https://github.com/zjunlp/EasyEdit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14251">
<div class="article-summary-box-inner">
<span><p>Evaluating the factuality of long-form text generated by large language
models (LMs) is non-trivial because (1) generations often contain a mixture of
supported and unsupported pieces of information, making binary judgments of
quality inadequate, and (2) human evaluation is time-consuming and costly. In
this paper, we introduce FACTSCORE, a new evaluation that breaks a generation
into a series of atomic facts and computes the percentage of atomic facts
supported by a reliable knowledge source. We conduct an extensive human
evaluation to obtain FACTSCOREs of people biographies generated by several
state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the
retrieval-augmented PerplexityAI -- and report new analysis demonstrating the
need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since
human evaluation is costly, we also introduce an automated model that estimates
FACTSCORE using retrieval and a strong language model, with less than a 2%
error rate. Finally, we use this automated metric to evaluate 6,500 generations
from a new set of 13 recent LMs that would have cost $26K if evaluated by
humans, with various findings: GPT-4 and ChatGPT are more factual than public
models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is
available for public use via `pip install factscore`.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14718">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) achieve substantial language capabilities when
finetuned using Reinforcement Learning with Human Feedback (RLHF). However,
RLHF is an unstable and data-hungry process that continually requires new
high-quality LM-generated data for finetuning. We introduce Advantage-Leftover
Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable
RL training on any pre-existing data. By assuming the entire LM output sequence
as a single action, A-LoL allows incorporating sequence-level classifiers or
human-designed scoring functions as rewards. Subsequently, by using LM's
internal sequence-level value estimate, A-LoL filters negative advantage
(low-quality) data points during training, making it resilient to noise.
Overall, A-LoL is an easy-to-implement LM training recipe that is
sample-efficient and stable.
</p>
<p>We demonstrate the effectiveness of A-LoL and its variants with a set of four
different language generation tasks. We compare against both online RL (PPO)
and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL
baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant
(HHA), LMs trained with A-LoL methods achieve the highest diversity while also
being rated more safe and helpful than baselines according to humans.
Additionally, in the remaining three tasks, A-LoL could optimize multiple
distinct reward functions even when using noisy or suboptimal training data. We
also release our experimental code. https://github.com/abaheti95/LoL-RL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14761">
<div class="article-summary-box-inner">
<span><p>Charts are very popular for analyzing data, visualizing key insights and
answering complex reasoning questions about data. To facilitate chart-based
data analysis using natural language, several downstream tasks have been
introduced recently such as chart question answering and chart summarization.
However, most of the methods that solve these tasks use pretraining on language
or vision-language tasks that do not attempt to explicitly model the structure
of the charts (e.g., how data is visually encoded and how chart elements are
related to each other). To address this, we first build a large corpus of
charts covering a wide variety of topics and visual styles. We then present
UniChart, a pretrained model for chart comprehension and reasoning. UniChart
encodes the relevant text, data, and visual elements of charts and then uses a
chart-grounded text decoder to generate the expected output in natural
language. We propose several chart-specific pretraining tasks that include: (i)
low-level tasks to extract the visual elements (e.g., bars, lines) and data
from charts, and (ii) high-level tasks to acquire chart understanding and
reasoning skills. We find that pretraining the model on a large corpus with
chart-specific low- and high-level tasks followed by finetuning on three
down-streaming tasks results in state-of-the-art performance on three
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15074">
<div class="article-summary-box-inner">
<span><p>The performance of large language models (LLMs) on existing reasoning
benchmarks has significantly improved over the past years. In response, we
present JEEBench, a considerably more challenging benchmark dataset for
evaluating the problem solving abilities of LLMs. We curate 515 challenging
pre-engineering mathematics, physics and chemistry problems from the highly
competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep
in-domain knowledge is essential for solving problems in this benchmark. Our
evaluation on various open-source and proprietary models reveals that the
highest performance, even after using techniques like self-consistency,
self-refinement and chain-of-thought prompting, is less than 40\%. The typical
failure modes of GPT-4, the best model, are errors in algebraic manipulation,
difficulty in grounding abstract concepts into mathematical equations
accurately and failure in retrieving relevant domain-specific concepts. We also
observe that by mere prompting, GPT-4 is unable to assess risk introduced by
negative marking for incorrect answers. For this, we develop a post-hoc
confidence-thresholding method over self-consistency, which enables effective
response selection. We hope that our challenging benchmark will guide future
re-search in problem-solving using LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16340">
<div class="article-summary-box-inner">
<span><p>Transformers have shown dominant performance across a range of domains
including language and vision. However, their computational cost grows
quadratically with the sequence length, making their usage prohibitive for
resource-constrained applications. To counter this, our approach is to divide
the whole sequence into segments and use local attention mechanism on the
individual segments. We propose a segmented recurrent transformer (SRformer)
that combines segmented (local) attention with recurrent attention. The loss
caused by reducing the attention window length is compensated by aggregating
information across segments with recurrent attention. SRformer leverages
Recurrent Accumulate-and-Fire (RAF) neurons' inherent memory to update the
cumulative product of keys and values. The segmented attention and lightweight
RAF neurons ensure the efficiency of the proposed transformer. Such an approach
leads to models with sequential processing capability at a lower
computation/memory cost. We apply the proposed method to T5 and BART
transformers. The modified models are tested on summarization datasets
including CNN-dailymail, XSUM, ArXiv, and MediaSUM. Notably, using segmented
inputs of varied sizes, the proposed model achieves $6-22\%$ higher ROUGE1
scores than a segmented transformer and outperforms other recurrent transformer
approaches. Furthermore, compared to full attention, the proposed model reduces
the computational complexity of cross attention by around $40\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09442">
<div class="article-summary-box-inner">
<span><p>Deploying large language models (LMs) can pose hazards from harmful outputs
such as toxic or false text. Prior work has introduced automated tools that
elicit harmful outputs to identify these risks. While this is a valuable step
toward securing models, these approaches rely on a pre-existing way to
efficiently classify undesirable outputs. Using a pre-existing classifier does
not allow for red-teaming to be tailored to the target model. Furthermore, when
failures can be easily classified in advance, red-teaming has limited marginal
value because problems can be avoided by simply filtering training data and/or
model outputs. Here, we consider red-teaming "from scratch," in which the
adversary does not begin with a way to classify failures. Our framework
consists of three steps: 1) Exploring the model's range of behaviors in the
desired context; 2) Establishing a definition and measurement for undesired
behavior (e.g., a classifier trained to reflect human evaluations); and 3)
Exploiting the model's flaws using this measure to develop diverse adversarial
prompts. We use this approach to red-team GPT-3 to discover classes of inputs
that elicit false statements. In doing so, we construct the CommonClaim dataset
of 20,000 statements labeled by humans as common-knowledge-true, common
knowledge-false, or neither. We are making code and data available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12424">
<div class="article-summary-box-inner">
<span><p>We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related biases within a
hegemonic system of binary gender, inspired by Winograd and Winogender schemas,
where each image is associated with a caption containing a pronoun relationship
of subjects and objects in the scene. VisoGender is balanced by gender
representation in professional roles, supporting bias evaluation in two ways:
i) resolution bias, where we evaluate the difference between pronoun resolution
accuracies for image subjects with gender presentations perceived as masculine
versus feminine by human annotators and ii) retrieval bias, where we compare
ratios of professionals perceived to have masculine and feminine gender
presentations retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they demonstrate bias in
resolving binary gender in complex scenes. While the direction and magnitude of
gender bias depends on the task and the model being evaluated, captioning
models are generally less biased than Vision-Language Encoders. Dataset and
code are available at https://github.com/oxai/visogender
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with a Focus on Candidate Response Distribution. (arXiv:2306.13047v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13047">
<div class="article-summary-box-inner">
<span><p>Multiple choice exams are widely used to assess candidates across a diverse
range of domains and tasks. To moderate question quality, newly proposed
questions often pass through pre-test evaluation stages before being deployed
into real-world exams. Currently, this evaluation process is manually
intensive, which can lead to time lags in the question development cycle.
Streamlining this process via automation can significantly enhance efficiency,
however, there's a current lack of datasets with adequate pre-test analysis
information. In this paper we analyse the Cambridge Multiple-Choice Questions
Reading Dataset; a multiple-choice comprehension dataset of questions at
different target levels, with corresponding candidate selection distributions.
We introduce the task of candidate distribution matching, propose several
evaluation metrics for the task, and demonstrate that automatic systems trained
on RACE++ can be leveraged as baselines for our task. We further demonstrate
that these automatic systems can be used for practical pre-test evaluation
tasks such as detecting underperforming distractors, where our detection
systems can automatically identify poor distractors that few candidates select.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VerifAI: Verified Generative AI. (arXiv:2307.02796v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02796">
<div class="article-summary-box-inner">
<span><p>Generative AI has made significant strides, yet concerns about the accuracy
and reliability of its outputs continue to grow. Such inaccuracies can have
serious consequences such as inaccurate decision-making, the spread of false
information, privacy violations, legal liabilities, and more. Although efforts
to address these risks are underway, including explainable AI and responsible
AI practices such as transparency, privacy protection, bias mitigation, and
social and environmental responsibility, misinformation caused by generative AI
will remain a significant challenge. We propose that verifying the outputs of
generative AI from a data management perspective is an emerging issue for
generative AI. This involves analyzing the underlying data from multi-modal
data lakes, including text files, tables, and knowledge graphs, and assessing
its quality and consistency. By doing so, we can establish a stronger
foundation for evaluating the outputs of generative AI models. Such an approach
can ensure the correctness of generative AI, promote transparency, and enable
decision-making with greater confidence. Our vision is to promote the
development of verifiable generative AI and contribute to a more trustworthy
and responsible use of AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models. (arXiv:2307.14539v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14539">
<div class="article-summary-box-inner">
<span><p>We introduce new jailbreak attacks on vision language models (VLMs), which
use aligned LLMs and are resilient to text-only jailbreak attacks.
Specifically, we develop cross-modality attacks on alignment where we pair
adversarial images going through the vision encoder with textual prompts to
break the alignment of the language model. Our attacks employ a novel
compositional strategy that combines an image, adversarially targeted towards
toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the
LLM draws the context to answer the generic prompt from the adversarial image.
The generation of benign-appearing adversarial images leverages a novel
embedding-space-based methodology, operating with no access to the LLM model.
Instead, the attacks require access only to the vision encoder and utilize one
of our four embedding space targeting strategies. By not requiring access to
the LLM, the attacks lower the entry barrier for attackers, particularly when
vision encoders such as CLIP are embedded in closed-source LLMs. The attacks
achieve a high success rate across different VLMs, highlighting the risk of
cross-modality alignment vulnerabilities, and the need for new alignment
approaches for multi-modal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15770">
<div class="article-summary-box-inner">
<span><p>In the face of climate change, are companies really taking substantial steps
toward more sustainable operations? A comprehensive answer lies in the dense,
information-rich landscape of corporate sustainability reports. However, the
sheer volume and complexity of these reports make human analysis very costly.
Therefore, only a few entities worldwide have the resources to analyze these
reports at scale, which leads to a lack of transparency in sustainability
reporting. Empowering stakeholders with LLM-based automatic analysis tools can
be a promising way to democratize sustainability report analysis. However,
developing such tools is challenging due to (1) the hallucination of LLMs and
(2) the inefficiency of bringing domain experts into the AI development loop.
In this paper, we ChatReport, a novel LLM-based system to automate the analysis
of corporate sustainability reports, addressing existing challenges by (1)
making the answers traceable to reduce the harm of hallucination and (2)
actively involving domain experts in the development loop. We make our
methodology, annotated datasets, and generated analyses of 1015 reports
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12067">
<div class="article-summary-box-inner">
<span><p>Multimodal large language models are typically trained in two stages: first
pre-training on image-text pairs, and then fine-tuning using supervised
vision-language instruction data. Recent studies have shown that large language
models can achieve satisfactory results even with a limited amount of
high-quality instruction-following data. In this paper, we introduce
InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200
examples, amounting to approximately 6\% of the instruction-following data used
in the alignment dataset for MiniGPT-4. To achieve this, we first propose
several metrics to access the quality of multimodal instruction data. Based on
these metrics, we present an effective and trainable data selector to
automatically identify and filter low-quality vision-language data. By
employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on
various evaluations. Overall, our findings demonstrate that less but
high-quality instruction tuning data is efficient in enabling multimodal large
language models to generate better output. Our code is available at
https://github.com/waltonfuture/InstructionGPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). (arXiv:2309.17421v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17421">
<div class="article-summary-box-inner">
<span><p>Large multimodal models (LMMs) extend large language models (LLMs) with
multi-sensory skills, such as visual understanding, to achieve stronger generic
intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to
deepen the understanding of LMMs. The analysis focuses on the intriguing tasks
that GPT-4V can perform, containing test samples to probe the quality and
genericity of GPT-4V's capabilities, its supported inputs and working modes,
and the effective ways to prompt the model. In our approach to exploring
GPT-4V, we curate and organize a collection of carefully designed qualitative
samples spanning a variety of domains and tasks. Observations from these
samples demonstrate that GPT-4V's unprecedented ability in processing
arbitrarily interleaved multimodal inputs and the genericity of its
capabilities together make GPT-4V a powerful multimodal generalist system.
Furthermore, GPT-4V's unique capability of understanding visual markers drawn
on input images can give rise to new human-computer interaction methods such as
visual referring prompting. We conclude the report with in-depth discussions on
the emerging application scenarios and the future research directions for
GPT-4V-based systems. We hope that this preliminary exploration will inspire
future research on the next-generation multimodal task formulation, new ways to
exploit and enhance LMMs to solve real-world problems, and gaining better
understanding of multimodal foundation models. Finally, we acknowledge that the
model under our study is solely the product of OpenAI's innovative work, and
they should be fully credited for its development. Please see the GPT-4V
contributions paper for the authorship and credit attribution:
https://cdn.openai.com/contributions/gpt-4v.pdf
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04381">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Hermes, an end-to-end framework to automatically
generate formal representations from natural language cellular specifications.
We first develop a neural constituency parser, NEUTREX, to process
transition-relevant texts and extract transition components (i.e., states,
conditions, and actions). We also design a domain-specific language to
translate these transition components to logical formulas by leveraging
dependency parse trees. Finally, we compile these logical formulas to generate
transitions and create the formal model as finite state machines. To
demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and
5G RRC specifications and obtain an overall accuracy of 81-87%, which is a
substantial improvement over the state-of-the-art. Our security analysis of the
extracted models uncovers 3 new vulnerabilities and identifies 19 previous
attacks in 4G and 5G specifications, and 7 deviations in commercial 4G
basebands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05028">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05057">
<div class="article-summary-box-inner">
<span><p>The success of language models has inspired the NLP community to attend to
tasks that require implicit and complex reasoning, relying on human-like
commonsense mechanisms. While such vertical thinking tasks have been relatively
popular, lateral thinking puzzles have received little attention. To bridge
this gap, we devise BRAINTEASER: a multiple-choice Question Answering task
designed to test the model's ability to exhibit lateral thinking and defy
default commonsense associations. We design a three-step procedure for creating
the first lateral thinking benchmark, consisting of data collection, distractor
generation, and generation of adversarial examples, leading to 1,100 puzzles
with high-quality annotations. To assess the consistency of lateral reasoning
by models, we enrich BRAINTEASER based on a semantic and contextual
reconstruction of its questions. Our experiments with state-of-the-art
instruction- and commonsense language models reveal a significant gap between
human and model performance, which is further widened when consistency across
adversarial formats is considered. We make all of our code and data available
to stimulate work on developing and evaluating lateral thinking models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05280">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. Generic personas refer to an individual from a demographic group
(e.g. an Asian person), whereas specific personas can be actual names of
historical figures. While the adoption of personas allows dialogue systems to
be more engaging and approachable to users, it also carries the potential risk
of exacerbating social biases in model responses, further causing societal
harms through interactions with users. In this paper, we systematically study
"persona biases", which we define to be the sensitivity of harmful dialogue
model behaviors to different persona adoptions. We categorize persona biases
into biases in harmful expression and harmful agreement, as well as establish a
comprehensive evaluation framework to measure persona biases in five aspects:
Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic
Agreement. Additionally, we propose to comprehensively investigate persona
biases through experimenting with UniversalPersona, a systematized persona
dataset with a comprehensive list of both generic and specific model personas.
Through benchmarking on four different models, including Blender, ChatGPT,
Alpaca, and Vicuna, our study uncovers significant persona biases in these
dialogue systems.Findings of our study underscores the immediate need to
revisit the use of persona traits in dialogue agents, to ensure their safe
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05797">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are increasingly used as powerful tools for a
plethora of natural language processing (NLP) applications. A recent
innovation, in-context learning (ICL), enables LLMs to learn new tasks by
supplying a few examples in the prompt during inference time, thereby
eliminating the need for model fine-tuning. While LLMs have been utilized in
several applications, their applicability in explaining the behavior of other
models remains relatively unexplored. Despite the growing number of new
explanation techniques, many require white-box access to the model and/or are
computationally expensive, highlighting a need for next-generation post hoc
explainers. In this work, we present the first framework to study the
effectiveness of LLMs in explaining other predictive models. More specifically,
we propose a novel framework encompassing multiple prompting strategies: i)
Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,
and iv) Explanation-based ICL, with varying levels of information about the
underlying ML model and the local neighborhood of the test sample. We conduct
extensive experiments with real-world benchmark datasets to demonstrate that
LLM-generated explanations perform on par with state-of-the-art post hoc
explainers using their ability to leverage ICL examples and their internal
knowledge in generating model explanations. On average, across four datasets
and two ML models, we observe that LLMs identify the most important feature
with 72.19% accuracy, opening up new frontiers in explainable artificial
intelligence (XAI) to explore LLM-based explanation frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06200">
<div class="article-summary-box-inner">
<span><p>Recent advances have greatly increased the capabilities of large language
models (LLMs), but our understanding of the models and their safety has not
progressed as fast. In this paper we aim to understand LLMs deeper by studying
their individual neurons. We build upon previous work showing large language
models such as GPT-4 can be useful in explaining what each neuron in a language
model does. Specifically, we analyze the effect of the prompt used to generate
explanations and show that reformatting the explanation prompt in a more
natural way can significantly improve neuron explanation quality and greatly
reduce computational cost. We demonstrate the effects of our new prompts in
three different ways, incorporating both automated and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06450">
<div class="article-summary-box-inner">
<span><p>In recent research on large language models (LLMs), there has been a growing
emphasis on aligning these models with human values to reduce the impact of
harmful content. However, current alignment methods often rely solely on
singular forms of human feedback, such as preferences, annotated labels, or
natural language critiques, overlooking the potential advantages of combining
these feedback types. This limitation leads to suboptimal performance, even
when ample training data is available. In this paper, we introduce Constructive
and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired
by constructivist learning theory. Our approach involves collecting three
distinct types of feedback tailored to problems of varying difficulty levels
within the training dataset. Specifically, we exploit critique feedback for
easy problems, refinement feedback for medium problems, and preference feedback
for hard problems. By training our model with this diversified feedback, we
achieve enhanced alignment performance while using less training data. To
assess the effectiveness of CDF, we evaluate it against previous methods in
three downstream tasks: question answering, dialog generation, and text
summarization. Experimental results demonstrate that CDF achieves superior
performance even with a smaller training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06692">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have unveiled remarkable reasoning capabilities
by exploiting chain-of-thought (CoT) prompting, which generates intermediate
reasoning chains to serve as the rationale for deriving the answer. However,
current CoT methods either simply employ general prompts such as Let's think
step by step, or heavily rely on handcrafted task-specific demonstrations to
attain preferable performances, thereby engendering an inescapable gap between
performance and generalization. To bridge this gap, we propose Meta-CoT, a
generalizable CoT prompting method in mixed-task scenarios where the type of
input questions is unknown. Meta-CoT firstly categorizes the scenario based on
the input question and subsequently constructs diverse demonstrations from the
corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys
remarkable performances on ten public benchmark reasoning tasks and superior
generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art
result on SVAMP (93.7%) without any additional program-aided methods. Our
further experiments on five out-of-distribution datasets verify the stability
and generality of Meta-CoT.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-12 23:11:40.757568935 UTC">2023-10-12 23:11:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>