<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-26T01:30:00Z">10-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation. (arXiv:2210.13459v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13459">
<div class="article-summary-box-inner">
<span><p>Overconfidence has been shown to impair generalization and calibration of a
neural network. Previous studies remedy this issue by adding a regularization
term to a loss function, preventing a model from making a peaked distribution.
Label smoothing smoothes target labels with a pre-defined prior label
distribution; as a result, a model is learned to maximize the likelihood of
predicting the soft label. Nonetheless, the amount of smoothing is the same in
all samples and remains fixed in training. In other words, label smoothing does
not reflect the change in probability distribution mapped by a model over the
course of training. To address this issue, we propose a regularization scheme
that brings dynamic nature into the smoothing parameter by taking model
probability distribution into account, thereby varying the parameter per
instance. A model in training self-regulates the extent of smoothing on the fly
during forward propagation. Furthermore, inspired by recent work in bridging
label smoothing and knowledge distillation, our work utilizes self-knowledge as
a prior label distribution in softening target labels, and presents theoretical
support for the regularization effect by knowledge distillation and the dynamic
smoothing parameter. Our regularizer is validated comprehensively, and the
result illustrates marked improvements in model generalization and calibration,
enhancing robustness and trustworthiness of a model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExPUNations: Augmenting Puns with Keywords and Explanations. (arXiv:2210.13513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13513">
<div class="article-summary-box-inner">
<span><p>The tasks of humor understanding and generation are challenging and
subjective even for humans, requiring commonsense and real-world knowledge to
master. Puns, in particular, add the challenge of fusing that knowledge with
the ability to interpret lexical-semantic ambiguity. In this paper, we present
the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of
puns with detailed crowdsourced annotations of keywords denoting the most
distinctive words that make the text funny, pun explanations describing why the
text is funny, and fine-grained funniness ratings. This is the first humor
dataset with such extensive and fine-grained annotations specifically for puns.
Based on these annotations, we propose two tasks: explanation generation to aid
with pun classification and keyword-conditioned pun generation, to challenge
the current state-of-the-art natural language understanding and generation
models' ability to understand and generate humor. We showcase that the
annotated keywords we collect are helpful for generating better novel humorous
texts in human evaluation, and that our natural language explanations can be
leveraged to improve both the accuracy and robustness of humor classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Situated Pun Generation. (arXiv:2210.13522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13522">
<div class="article-summary-box-inner">
<span><p>Previous work on pun generation commonly begins with a given pun word (a pair
of homophones for heterographic pun generation and a polyseme for homographic
pun generation) and seeks to generate an appropriate pun. While this may enable
efficient pun generation, we believe that a pun is most entertaining if it fits
appropriately within a given context, e.g., a given situation or dialogue. In
this work, we propose a new task, context-situated pun generation, where a
specific context represented by a set of keywords is provided, and the task is
to first identify suitable pun words that are appropriate for the context, then
generate puns based on the context keywords and the identified pun words. We
collect CUP (Context-sitUated Pun), containing 4.5k tuples of context words and
pun pairs. Based on the new data and setup, we propose a pipeline system for
context-situated pun generation, including a pun word retrieval module that
identifies suitable pun words for a given context, and a generation module that
generates puns from context keywords and pun words. Human evaluation shows that
69% of our top retrieved pun words can be used to generate context-situated
puns, and our generation module yields successful puns 31% of the time given a
plausible tuple of context words and pun pair, almost tripling the yield of a
state-of-the-art pun generation model. With an end-to-end evaluation, our
pipeline system with the top-1 retrieved pun pair for a given context can
generate successful puns 40% of the time, better than all other modeling
variations but 32% lower than the human success rate. This highlights the
difficulty of the task, and encourages more research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations. (arXiv:2210.13526v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13526">
<div class="article-summary-box-inner">
<span><p>Emerging research frontiers and computational advances have gradually
transformed cognitive science into a multidisciplinary and data-driven field.
As a result, there is a proliferation of cognitive theories investigated and
interpreted from different academic lens and in different levels of
abstraction. We formulate this applied aspect of this challenge as the
computational cognitive inference, and describe the major routes of
computational approaches. To balance the potential optimism alongside the speed
and scale of the data-driven era of cognitive science, we propose to inspect
this trend in more empirical terms by identifying the operational challenges,
societal impacts and ethical guidelines in conducting research and interpreting
results from the computational inference in cognitive science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Misinformation in New Articles using Natural Language Processing and a Recurrent Neural Network. (arXiv:2210.13534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13534">
<div class="article-summary-box-inner">
<span><p>This paper seeks to address the classification of misinformation in news
articles using a Long Short Term Memory Recurrent Neural Network. Articles were
taken from 2018; a year that was filled with reporters writing about President
Donald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.
The model presented successfully classifies these articles with an accuracy
score of 0.779944. We consider this to be successful because the model was
trained on articles that included languages other than English as well as
incomplete, or fragmented, articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Pre-Training Objectives for Transformer-based Autoencoders. (arXiv:2210.13536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13536">
<div class="article-summary-box-inner">
<span><p>In this paper, we study trade-offs between efficiency, cost and accuracy when
pre-training Transformer encoders with different pre-training objectives. For
this purpose, we analyze features of common objectives and combine them to
create new effective pre-training approaches. Specifically, we designed light
token generators based on a straightforward statistical approach, which can
replace ELECTRA computationally heavy generators, thus highly reducing cost.
Our experiments also show that (i) there are more efficient alternatives to
BERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based
models using lighter generators without a significant drop in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13569">
<div class="article-summary-box-inner">
<span><p>When a language model is trained to predict natural language sequences, its
prediction at each moment depends on a representation of prior context. What
kind of information about the prior context can language models retrieve? We
tested whether language models could retrieve the exact words that occurred
previously in a text. In our paradigm, language models (transformers and an
LSTM) processed English text in which a list of nouns occurred twice. We
operationalized retrieval as the reduction in surprisal from the first to the
second list. We found that the transformers retrieved both the identity and
ordering of nouns from the first list. Further, the transformers' retrieval was
markedly enhanced when they were trained on a larger corpus and with greater
model depth. Lastly, their ability to index prior tokens was dependent on
learned attention patterns. In contrast, the LSTM exhibited less precise
retrieval, which was limited to list-initial tokens and to short intervening
texts. The LSTM's retrieval was not sensitive to the order of nouns and it
improved when the list was semantically coherent. We conclude that transformers
implemented something akin to a working memory system that could flexibly
retrieve individual token representations across arbitrary delays; conversely,
the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior
tokens, weighted toward the earliest items.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Self-Rationalization Improve Robustness to Spurious Correlations?. (arXiv:2210.13575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13575">
<div class="article-summary-box-inner">
<span><p>Rationalization is fundamental to human reasoning and learning. NLP models
trained to produce rationales along with predictions, called
self-rationalization models, have been investigated for their interpretability
and utility to end-users. However, the extent to which training with
human-written rationales facilitates learning remains an under-explored
question. We ask whether training models to self-rationalize can aid in their
learning to solve tasks for the right reasons. Specifically, we evaluate how
training self-rationalization models with free-text rationales affects
robustness to spurious correlations in fine-tuned encoder-decoder and
decoder-only models of six different sizes. We evaluate robustness to spurious
correlations by measuring performance on 1) manually annotated challenge
datasets and 2) subsets of original test sets where reliance on spurious
correlations would fail to produce correct answers. We find that while
self-rationalization can improve robustness to spurious correlations in
low-resource settings, it tends to hurt robustness in higher-resource settings.
Furthermore, these effects depend on model family and size, as well as on
rationale content. Together, our results suggest that explainability can come
at the cost of robustness; thus, appropriate care should be taken when training
self-rationalizing models with the goal of creating more trustworthy models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speeding Up Question Answering Task of Language Models via Inverted Index. (arXiv:2210.13578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13578">
<div class="article-summary-box-inner">
<span><p>Natural language processing applications, such as conversational agents and
their question-answering capabilities, are widely used in the real world.
Despite the wide popularity of large language models (LLMs), few real-world
conversational agents take advantage of LLMs. Extensive resources consumed by
LLMs disable developers from integrating them into end-user applications. In
this study, we leverage an inverted indexing mechanism combined with LLMs to
improve the efficiency of question-answering models for closed-domain
questions. Our experiments show that using the index improves the average
response time by 97.44%. In addition, due to the reduced search scope, the
average BLEU score improved by 0.23 while using the inverted index.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LANS: Large-scale Arabic News Summarization Corpus. (arXiv:2210.13600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13600">
<div class="article-summary-box-inner">
<span><p>Text summarization has been intensively studied in many languages, and some
languages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is
still in its developing stages. Existing ATS datasets are either small or lack
diversity. We build, LANS, a large-scale and diverse dataset for Arabic Text
Summarization task. LANS offers 8.4 million articles and their summaries
extracted from newspapers websites metadata between 1999 and 2019. The
high-quality and diverse summaries are written by journalists from 22 major
Arab newspapers, and include an eclectic mix of at least more than 7 topics
from each source. We conduct an intrinsic evaluation on LANS by both automatic
and human evaluations. Human evaluation of 1000 random samples reports 95.4%
accuracy for our collected summaries, and automatic evaluation quantifies the
diversity and abstractness of the summaries. The dataset is publicly available
upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapters for Enhanced Modeling of Multilingual Knowledge and Text. (arXiv:2210.13617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13617">
<div class="article-summary-box-inner">
<span><p>Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13623">
<div class="article-summary-box-inner">
<span><p>In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge. (arXiv:2210.13626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13626">
<div class="article-summary-box-inner">
<span><p>There has been a growing interest in solving Visual Question Answering (VQA)
tasks that require the model to reason beyond the content present in the image.
In this work, we focus on questions that require commonsense reasoning. In
contrast to previous methods which inject knowledge from static knowledge
bases, we investigate the incorporation of contextualized knowledge using
Commonsense Transformer (COMET), an existing knowledge model trained on
human-curated knowledge bases. We propose a method to generate, select, and
encode external commonsense knowledge alongside visual and textual cues in a
new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT.
Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets,
we show that VLC-BERT is capable of outperforming existing models that utilize
static knowledge bases. Furthermore, through a detailed analysis, we explain
which questions benefit, and which don't, from contextualized commonsense
knowledge from COMET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Long-Term Citations from Short-Term Linguistic Influence. (arXiv:2210.13628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13628">
<div class="article-summary-box-inner">
<span><p>A standard measure of the influence of a research paper is the number of
times it is cited. However, papers may be cited for many reasons, and citation
count offers limited information about the extent to which a paper affected the
content of subsequent publications. We therefore propose a novel method to
quantify linguistic influence in timestamped document collections. There are
two main steps: first, identify lexical and semantic changes using contextual
embeddings and word frequencies; second, aggregate information about these
changes into per-document influence scores by estimating a high-dimensional
Hawkes process with a low-rank parameter matrix. We show that this measure of
linguistic influence is predictive of $\textit{future}$ citations: the estimate
of linguistic influence from the two years after a paper's publication is
correlated with and predictive of its citation count in the following three
years. This is demonstrated using an online evaluation with incremental
temporal training/test splits, in comparison with a strong baseline that
includes predictors for initial citation counts, topics, and lexical features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward an Intelligent Tutoring System for Argument Mining in Legal Texts. (arXiv:2210.13635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13635">
<div class="article-summary-box-inner">
<span><p>We propose an adaptive environment (CABINET) to support caselaw analysis
(identifying key argument elements) based on a novel cognitive computing
framework that carefully matches various machine learning (ML) capabilities to
the proficiency of a user. CABINET supports law students in their learning as
well as professionals in their work. The results of our experiments focused on
the feasibility of the proposed framework are promising. We show that the
system is capable of identifying a potential error in the analysis with very
low false positives rate (2.0-3.5%), as well as of predicting the key argument
element type (e.g., an issue or a holding) with a reasonably high F1-score
(0.74).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs. (arXiv:2210.13650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13650">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Question Answering (KGQA) involves retrieving entities as
answers from a Knowledge Graph (KG) using natural language queries. The
challenge is to learn to reason over question-relevant KG facts that traverse
KG entities and lead to the question answers. To facilitate reasoning, the
question is decoded into instructions, which are dense question representations
used to guide the KG traversals. However, if the derived instructions do not
exactly match the underlying KG information, they may lead to reasoning under
irrelevant context. Our method, termed ReaRev, introduces a new way to KGQA
reasoning with respect to both instruction decoding and execution. To improve
instruction decoding, we perform reasoning in an adaptive manner, where
KG-aware information is used to iteratively update the initial instructions. To
improve instruction execution, we emulate breadth-first search (BFS) with graph
neural networks (GNNs). The BFS strategy treats the instructions as a set and
allows our method to decide on their execution order on the fly. Experimental
results on three KGQA benchmarks demonstrate the ReaRev's effectiveness
compared with previous state-of-the-art, especially when the KG is incomplete
or when we tackle complex questions. Our code is publicly available at
https://github.com/cmavro/ReaRev_KGQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing. (arXiv:2210.13669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13669">
<div class="article-summary-box-inner">
<span><p>Recent work in training large language models (LLMs) to follow natural
language instructions has opened up exciting opportunities for natural language
interface design. Building on the prior success of LLMs in the realm of
computer-assisted creativity, we aim to study if LLMs can improve the quality
of user-generated content through collaboration. We present CoPoet, a
collaborative poetry writing system. In contrast to auto-completing a user's
text, CoPoet is controlled by user instructions that specify the attributes of
the desired text, such as Write a sentence about `love' or Write a sentence
ending in `fly'. The core component of our system is a language model
fine-tuned on a diverse collection of instructions for poetry writing. Our
model is not only competitive with publicly available LLMs trained on
instructions (InstructGPT), but is also capable of satisfying unseen
compositional instructions. A study with 15 qualified crowdworkers shows that
users successfully write poems with CoPoet on diverse topics ranging from
Monarchy to Climate change. Further, the collaboratively written poems are
preferred by third-party evaluators over those written without the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Parameter Efficient Learning for Generation. (arXiv:2210.13673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13673">
<div class="article-summary-box-inner">
<span><p>Parameter efficient learning methods (PERMs) have recently gained significant
attention as they provide an efficient way for pre-trained language models
(PLMs) to adapt to a downstream task. However, these conclusions are mostly
drawn from in-domain evaluations over the full training set. In this paper, we
present comparisons between PERMs and finetuning from three new perspectives:
(1) the effect of sample and model size to in-domain evaluations, (2)
generalization to unseen domains and new datasets, and (3) the faithfulness of
generations. Our results show that for in-domain settings (a) there is a cross
point of sample size for which PERMs will perform better than finetuning when
training with fewer samples, and (b) larger PLMs have larger cross points. For
cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,
2019) performs the best amongst all the PERMs studied here, and (b) it
outperforms finetuning if the task dataset is below a certain size. We also
compare the faithfulness of generations and show that PERMs can achieve better
faithfulness score than finetuning, especially for small training set, by as
much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and
achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all
ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Training-Inference Gap for Dense Phrase Retrieval. (arXiv:2210.13678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13678">
<div class="article-summary-box-inner">
<span><p>Building dense retrievers requires a series of standard procedures, including
training and validating neural models and creating indexes for efficient
search. However, these procedures are often misaligned in that training
objectives do not exactly reflect the retrieval scenario at inference time. In
this paper, we explore how the gap between training and inference in dense
retrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021)
where billions of representations are indexed at inference. Since validating
every dense retriever with a large-scale index is practically infeasible, we
propose an efficient way of validating dense retrievers using a small subset of
the entire corpus. This allows us to validate various training strategies
including unifying contrastive loss terms and using hard negatives for phrase
retrieval, which largely reduces the training-inference discrepancy. As a
result, we improve top-1 phrase retrieval accuracy by 2~3 points and top-20
passage retrieval accuracy by 2~4 points for open-domain question answering.
Our work urges modeling dense retrievers with careful consideration of training
and inference via efficient validation while advancing phrase retrieval as a
general solution for dense retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing. (arXiv:2210.13693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13693">
<div class="article-summary-box-inner">
<span><p>In-context learning using large language models has recently shown surprising
results for semantic parsing tasks such as Text-to-SQL translation. Prompting
GPT-3 or Codex using several examples of question-SQL pairs can produce
excellent results, comparable to state-of-the-art finetuning-based models.
However, existing work primarily focuses on English datasets, and it is unknown
whether large language models can serve as competitive semantic parsers for
other languages. To bridge this gap, our work focuses on cross-lingual
Text-to-SQL semantic parsing for translating non-English utterances into SQL
queries based on an English schema. We consider a zero-shot transfer learning
setting with the assumption that we do not have any labeled examples in the
target language (but have annotated examples in English). This work introduces
the XRICL framework, which learns to retrieve relevant English exemplars for a
given query to construct prompts. We also include global translation exemplars
for a target language to facilitate the translation process for large language
models. To systematically evaluate our model, we construct two new benchmark
datasets, XSpider and XKaggle-dbqa, which include questions in Chinese,
Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively
leverages large pre-trained language models to outperform existing baselines.
Data and code are publicly available at https://github.com/Impavidity/XRICL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Joint Training Really Help Cascaded Speech Translation?. (arXiv:2210.13700v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13700">
<div class="article-summary-box-inner">
<span><p>Currently, in speech translation, the straightforward approach - cascading a
recognition system with a translation system - delivers state-of-the-art
results. However, fundamental challenges such as error propagation from the
automatic speech recognition system still remain. To mitigate these problems,
recently, people turn their attention to direct data and propose various joint
training methods. In this work, we seek to answer the question of whether joint
training really helps cascaded speech translation. We review recent papers on
the topic and also investigate a joint training criterion by marginalizing the
transcription posterior probabilities. Our findings show that a strong cascaded
baseline can diminish any improvements obtained using joint training, and we
suggest alternatives to joint training. We hope this work can serve as a
refresher of the current speech translation landscape, and motivate research in
finding more efficient and creative ways to utilize the direct data for speech
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence. (arXiv:2210.13701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13701">
<div class="article-summary-box-inner">
<span><p>Question answering models can use rich knowledge sources -- up to one hundred
retrieved passages and parametric knowledge in the large-scale language model
(LM). Prior work assumes information in such knowledge sources is consistent
with each other, paying little attention to how models blend information stored
in their LM parameters with that from retrieved evidence documents. In this
paper, we simulate knowledge conflicts (i.e., where parametric knowledge
suggests one answer and different passages suggest different answers) and
examine model behaviors. We find retrieval performance heavily impacts which
sources models rely on, and current models mostly rely on non-parametric
knowledge in their best-performing settings. We discover a troubling trend that
contradictions among knowledge sources affect model confidence only marginally.
To address this issue, we present a new calibration study, where models are
discouraged from presenting any single answer when presented with multiple
conflicting answer candidates in retrieved evidences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Legal Domain Adaptation. (arXiv:2210.13712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13712">
<div class="article-summary-box-inner">
<span><p>Seeking legal advice is often expensive. Recent advancement in machine
learning for solving complex problems can be leveraged to help make legal
services more accessible to the public. However, real-life applications
encounter significant challenges. State-of-the-art language models are growing
increasingly large, making parameter-efficient learning increasingly important.
Unfortunately, parameter-efficient methods perform poorly with small amounts of
data, which are common in the legal domain (where data labelling costs are
high). To address these challenges, we propose parameter-efficient legal domain
adaptation, which uses vast unsupervised legal data from public legal forums to
perform legal pre-training. This method exceeds or matches the fewshot
performance of existing models such as LEGAL-BERT on various legal tasks while
tuning only approximately 0.1% of model parameters. Additionally, we show that
our method can achieve calibration comparable to existing methods across
several tasks. To the best of our knowledge, this work is among the first to
explore parameter-efficient methods of tuning language models toward the legal
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion. (arXiv:2210.13715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13715">
<div class="article-summary-box-inner">
<span><p>This paper presents a parameter-lite transfer learning approach of pretrained
language models (LM) for knowledge graph (KG) completion. Instead of
finetuning, which modifies all LM parameters, we only tune a few new parameters
while keeping the original LM parameters fixed. We establish this via
reformulating KG completion as a "fill-in-the-blank" task, and introducing a
parameter-lite encoder on top of the original LMs. We show that, by tuning far
fewer parameters than finetuning, LMs transfer non-trivially to most tasks and
reach competitiveness with prior state-of-the-art approaches. For instance, we
outperform the fully finetuning approaches on a KG completion benchmark by
tuning only 1% of the parameters. The code and datasets are available at
\url{https://github.com/yuanyehome/PALT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Reinforced Medical Report Generation with M-Linear Attention and Repetition Penalty. (arXiv:2210.13729v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13729">
<div class="article-summary-box-inner">
<span><p>To reduce doctors' workload, deep-learning-based automatic medical report
generation has recently attracted more and more research efforts, where deep
convolutional neural networks (CNNs) are employed to encode the input images,
and recurrent neural networks (RNNs) are used to decode the visual features
into medical reports automatically. However, these state-of-the-art methods
mainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)
low-order and unidimensional attention mechanisms, and (iii) repeated
generation. In this article, we propose a hybrid reinforced medical report
generation method with m-linear attention and repetition penalty mechanism
(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with
different weights is employed to remedy the limitations of single-metric-based
rewards. We also propose a search algorithm with linear complexity to
approximate the best weight combination. Furthermore, we use m-linear attention
modules to explore high-order feature interactions and to achieve multi-modal
reasoning, while a repetition penalty applies penalties to repeated terms
during the model's training process. Extensive experimental studies on two
public datasets show that HReMRG-MR greatly outperforms the state-of-the-art
baselines in terms of all metrics. We also conducted a series of ablation
experiments to prove the effectiveness of all our proposed components. We also
performed a reward search toy experiment to give evidence that our proposed
search approach can significantly reduce the search time while approximating
the best performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Few-Shot Relation Extraction with Label Prompt Dropout. (arXiv:2210.13733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13733">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEMETR: Diagnosing Evaluation Metrics for Translation. (arXiv:2210.13746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13746">
<div class="article-summary-box-inner">
<span><p>While machine translation evaluation metrics based on string overlap (e.g.,
BLEU) have their limitations, their computations are transparent: the BLEU
score assigned to a particular candidate translation can be traced back to the
presence or absence of certain words. The operations of newer learned metrics
(e.g., BLEURT, COMET), which leverage pretrained language models to achieve
higher correlations with human quality judgments than BLEU, are opaque in
comparison. In this paper, we shed light on the behavior of these learned
metrics by creating DEMETR, a diagnostic dataset with 31K English examples
(translated from 10 source languages) for evaluating the sensitivity of MT
evaluation metrics to 35 different linguistic perturbations spanning semantic,
syntactic, and morphological error categories. All perturbations were carefully
designed to form minimal pairs with the actual translation (i.e., differ in
only one aspect). We find that learned metrics perform substantially better
than string-based metrics on DEMETR. Additionally, learned metrics differ in
their sensitivity to various phenomena (e.g., BERTScore is sensitive to
untranslated words but relatively insensitive to gender manipulation, while
COMET is much more sensitive to word repetition than to aspectual changes). We
publicly release DEMETR to spur more informed future development of machine
translation evaluation metrics
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugCSE: Contrastive Sentence Embedding with Diverse Augmentations. (arXiv:2210.13749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13749">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques have been proven useful in many applications in
NLP fields. Most augmentations are task-specific, and cannot be used as a
general-purpose tool. In our work, we present AugCSE, a unified framework to
utilize diverse sets of data augmentations to achieve a better, general
purpose, sentence embedding model. Building upon the latest sentence embedding
models, our approach uses a simple antagonistic discriminator that
differentiates the augmentation types. With the finetuning objective borrowed
from domain adaptation, we show that diverse augmentations, which often lead to
conflicting contrastive signals, can be tamed to produce a better and more
robust sentence representation. Our methods achieve state-of-the-art results on
downstream transfer tasks and perform competitively on semantic textual
similarity tasks, using only unsupervised data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciFact-Open: Towards open-domain scientific claim verification. (arXiv:2210.13777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13777">
<div class="article-summary-box-inner">
<span><p>While research on scientific claim verification has led to the development of
powerful systems that appear to approach human performance, these approaches
have yet to be tested in a realistic setting against large corpora of
scientific literature. Moving to this open-domain evaluation setting, however,
poses unique challenges; in particular, it is infeasible to exhaustively
annotate all evidence documents. In this work, we present SciFact-Open, a new
test collection designed to evaluate the performance of scientific claim
verification systems on a corpus of 500K research abstracts. Drawing upon
pooling techniques from information retrieval, we collect evidence for
scientific claims by pooling and annotating the top predictions of four
state-of-the-art scientific claim verification models. We find that systems
developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting
performance drops of at least 15 F1. In addition, analysis of the evidence in
SciFact-Open reveals interesting phenomena likely to appear when claim
verification systems are deployed in practice, e.g., cases where the evidence
supports only a special case of the claim. Our dataset is available at
https://github.com/dwadden/scifact-open.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension. (arXiv:2210.13778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13778">
<div class="article-summary-box-inner">
<span><p>Machine Reading Comprehension (MRC) has become one of the essential tasks in
Natural Language Understanding (NLU) as it is often included in several NLU
benchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets
only have answerable question type, overlooking the importance of unanswerable
questions. MRC models trained only on answerable questions will select the span
that is most likely to be the answer, even when the answer does not actually
exist in the given passage (Rajpurkar et al., 2018). This problem especially
remains in medium- to low-resource languages like Indonesian. Existing
Indonesian MRC datasets (Purwarianti et al., 2007; Clark et al., 2020) are
still inadequate because of the small size and limited question types, i.e.,
they only cover answerable questions. To fill this gap, we build a new
Indonesian MRC dataset called I(n)don'tKnow- MRC (IDK-MRC) by combining the
automatic and manual unanswerable question generation to minimize the cost of
manual dataset construction while maintaining the dataset quality. Combined
with the existing answerable questions, IDK-MRC consists of more than 10K
questions in total. Our analysis shows that our dataset significantly improves
the performance of Indonesian MRC models, showing a large improvement for
unanswerable questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies. (arXiv:2210.13783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13783">
<div class="article-summary-box-inner">
<span><p>The task of topical segmentation is well studied, but previous work has
mostly addressed it in the context of structured, well-defined segments, such
as segmentation into paragraphs, chapters, or segmenting text that originated
from multiple sources. We tackle the task of segmenting running (spoken)
narratives, which poses hitherto unaddressed challenges. As a test case, we
address Holocaust survivor testimonies, given in English. Other than the
importance of studying these testimonies for Holocaust research, we argue that
they provide an interesting test case for topical segmentation, due to their
unstructured surface level, relative abundance (tens of thousands of such
testimonies were collected), and the relatively confined domain that they
cover. We hypothesize that boundary points between segments correspond to low
mutual information between the sentences proceeding and following the boundary.
Based on this hypothesis, we explore a range of algorithmic approaches to the
task, building on previous work on segmentation that uses generative Bayesian
modeling and state-of-the-art neural machinery. Compared to manually annotated
references, we find that the developed approaches show considerable
improvements over previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation. (arXiv:2210.13800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13800">
<div class="article-summary-box-inner">
<span><p>We present Referee, a novel framework for sentence summarization that can be
trained reference-free (i.e., requiring no gold summaries for supervision),
while allowing direct control for compression ratio. Our work is the first to
demonstrate that reference-free, controlled sentence summarization is feasible
via the conceptual framework of Symbolic Knowledge Distillation (West et al.,
2022), where latent knowledge in pre-trained language models is distilled via
explicit examples sampled from the teacher models, further purified with three
types of filters: length, fidelity, and Information Bottleneck. Moreover, we
uniquely propose iterative distillation of knowledge, where student models from
the previous iteration of distillation serve as teacher models in the next
iteration. Starting off from a relatively modest set of GPT3-generated
summaries, we demonstrate how iterative knowledge distillation can lead to
considerably smaller, but better summarizers with sharper controllability. A
useful by-product of this iterative distillation process is a high-quality
dataset of sentence-summary pairs with varying degrees of compression ratios.
Empirical results demonstrate that the final student models vastly outperform
the much larger GPT3-Instruct model in terms of the controllability of
compression ratios, without compromising the quality of resulting
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch Disentangling with Untranscribed Data. (arXiv:2210.13803v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13803">
<div class="article-summary-box-inner">
<span><p>In this paper, we proposed Adapitch, a multi-speaker TTS method that makes
adaptation of the supervised module with untranscribed data. We design two self
supervised modules to train the text encoder and mel decoder separately with
untranscribed data to enhance the representation of text and mel. To better
handle the prosody information in a synthesized voice, a supervised TTS module
is designed conditioned on content disentangling of pitch, text, and speaker.
The training phase was separated into two parts, pretrained and fixed the text
encoder and mel decoder with unsupervised mode, then the supervised mode on the
disentanglement of TTS. Experiment results show that the Adaptich achieved much
better quality than baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach. (arXiv:2210.13805v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13805">
<div class="article-summary-box-inner">
<span><p>Recovering the masked speech frames is widely applied in speech
representation learning. However, most of these models use random masking in
the pre-training. In this work, we proposed two kinds of masking approaches:
(1) speech-level masking, making the model to mask more speech segments than
silence segments, (2) phoneme-level masking, forcing the model to mask the
whole frames of the phoneme, instead of phoneme pieces. We pre-trained the
model via these two approaches, and evaluated on two downstream tasks, phoneme
classification and speaker recognition. The experiments demonstrated that the
proposed masking approaches are beneficial to improve the performance of speech
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaSpeech: Speech Effects Switch Along with Environment for Metaverse. (arXiv:2210.13811v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13811">
<div class="article-summary-box-inner">
<span><p>Metaverse expands the physical world to a new dimension, and the physical
environment and Metaverse environment can be directly connected and entered.
Voice is an indispensable communication medium in the real world and Metaverse.
Fusion of the voice with environment effects is important for user immersion in
Metaverse. In this paper, we proposed using the voice conversion based method
for the conversion of target environment effect speech. The proposed method was
named MetaSpeech, which introduces an environment effect module containing an
effect extractor to extract the environment information and an effect encoder
to encode the environment effect condition, in which gradient reversal layer
was used for adversarial training to keep the speech content and speaker
information while disentangling the environmental effects. From the experiment
results on the public dataset of LJSpeech with four environment effects, the
proposed model could complete the specific environment effect conversion and
outperforms the baseline methods from the voice conversion task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Chinese Spelling Check Framework Based on Reverse Contrastive Learning. (arXiv:2210.13823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13823">
<div class="article-summary-box-inner">
<span><p>Chinese spelling check is a task to detect and correct spelling mistakes in
Chinese text. Existing research aims to enhance the text representation and use
multi-source information to improve the detection and correction capabilities
of models, but does not pay too much attention to improving their ability to
distinguish between confusable words. Contrastive learning, whose aim is to
minimize the distance in representation space between similar sample pairs, has
recently become a dominant technique in natural language processing. Inspired
by contrastive learning, we present a novel framework for Chinese spelling
checking, which consists of three modules: language representation, spelling
check and reverse contrastive learning. Specifically, we propose a reverse
contrastive learning strategy, which explicitly forces the model to minimize
the agreement between the similar examples, namely, the phonetically and
visually confusable characters. Experimental results show that our framework is
model-agnostic and could be combined with existing Chinese spelling check
models to yield state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Filter upon Diversity-Improved Decoding for Diversity-Faithfulness Tradeoff in NLG. (arXiv:2210.13829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13829">
<div class="article-summary-box-inner">
<span><p>Some Natural Language Generation (NLG) tasks require both faithfulness and
diversity. The decoding strategy is intensively related to the quality of the
generated text. Strategies such as beam search, greedy search, etc., perform
with low diversity and high repetition. On the other hand, guided decoding, the
solution towards diversity, may generate unfaithful expressions. To this end,
this paper presents Information Filter upon Diversity-Improved Decoding (IFDID)
to obtain the tradeoff between diversity and faithfulness. IFDID is a two-stage
decoding strategy leveraging the proposed Enhance-Filter framework, which
achieves the tradeoff by increasing the probabilities of some typical tokens
being selected and subsequently filtering them by their information amount. To
verify the effectiveness, we compare our method with other baselines on related
CommonGEN, RocStories and AdGen benchmarks, which cover Chinese and English
datasets. Our numerical experimental results and human evaluation outcomes
verify the effectiveness of the proposed approach, as our approach achieves a
1.24 higher ROUGE score describing faithfulness as well as higher diversity
represented by 62.5% higher upon Dist-2 than traditional approaches,
demonstrating that IFDID is a novel SOTA decoding strategy for the tradeoff
between diversity and faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation. (arXiv:2210.13832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13832">
<div class="article-summary-box-inner">
<span><p>Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts. (arXiv:2210.13836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13836">
<div class="article-summary-box-inner">
<span><p>This work demonstrates that Legal Judgement Prediction systems without
expert-informed adjustments can be vulnerable to shallow, distracting surface
signals that arise from corpus construction, case distribution, and confounding
factors. To mitigate this, we use domain expertise to strategically identify
statistically predictive but legally irrelevant information. We adopt
adversarial training to prevent the system from relying on it. We evaluate our
deconfounded models by employing interpretability techniques and comparing to
expert annotations. Quantitative experiments and qualitative analysis show that
our deconfounded model consistently aligns better with expert rationales than
baselines trained for prediction only. We further contribute a set of reference
expert annotations to the validation and testing partitions of an existing
benchmark dataset of European Court of Human Rights cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Relation Classification via Efficient and Effective Prompting. (arXiv:2210.13838v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13838">
<div class="article-summary-box-inner">
<span><p>Prompting pre-trained language models has achieved impressive performance on
various NLP tasks, especially in low data regimes. Despite the success of
prompting in monolingual settings, applying prompt-based methods in
multilingual scenarios has been limited to a narrow set of tasks, due to the
high cost of handcrafting multilingual prompts. In this paper, we present the
first work on prompt-based multilingual relation classification (RC), by
introducing an efficient and effective method that constructs prompts from
relation triples and involves only minimal translation for the class labels. We
evaluate its performance in fully supervised, few-shot and zero-shot scenarios,
and analyze its effectiveness across 14 languages, prompt variants, and
English-task training in cross-lingual settings. We find that in both fully
supervised and few-shot scenarios, our prompt method beats competitive
baselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the
random baseline by a large margin in zero-shot experiments. Our method requires
little in-language knowledge and can be used as a strong baseline for similar
multilingual classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection. (arXiv:2210.13845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13845">
<div class="article-summary-box-inner">
<span><p>Current end-to-end retrieval-based dialogue systems are mainly based on
Recurrent Neural Networks or Transformers with attention mechanisms. Although
promising results have been achieved, these models often suffer from slow
inference or huge number of parameters. In this paper, we propose a novel
lightweight fully convolutional architecture, called DialogConv, for response
selection. DialogConv is exclusively built on top of convolution to extract
matching features of context and response. Dialogues are modeled in 3D views,
where DialogConv performs convolution operations on embedding view, word view
and utterance view to capture richer semantic information from multiple
contextual views. On the four benchmark datasets, compared with
state-of-the-art baselines, DialogConv is on average about 8.5x smaller in
size, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the
same time, DialogConv achieves the competitive effectiveness of response
selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation. (arXiv:2210.13865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13865">
<div class="article-summary-box-inner">
<span><p>Misinformation emerges in times of uncertainty when credible information is
limited. This is challenging for NLP-based fact-checking as it relies on
counter-evidence, which may not yet be available. Despite increasing interest
in automatic fact-checking, it is still unclear if automated approaches can
realistically refute harmful real-world misinformation. Here, we contrast and
compare NLP fact-checking with how professional fact-checkers combat
misinformation in the absence of counter-evidence. In our analysis, we show
that, by design, existing NLP task definitions for fact-checking cannot refute
misinformation as professional fact-checkers do for the majority of claims. We
then define two requirements that the evidence in datasets must fulfill for
realistic fact-checking: It must be (1) sufficient to refute the claim and (2)
not leaked from existing fact-checking articles. We survey existing
fact-checking datasets and find that all of them fail to satisfy both criteria.
Finally, we perform experiments to demonstrate that models trained on a
large-scale fact-checking dataset rely on leaked evidence, which makes them
unsuitable in real-world scenarios. Taken together, we show that current NLP
fact-checking cannot realistically combat real-world misinformation because it
depends on unrealistic assumptions about counter-evidence in the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Language Models for Secure Data Sharing. (arXiv:2210.13918v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13918">
<div class="article-summary-box-inner">
<span><p>To protect the privacy of individuals whose data is being shared, it is of
high importance to develop methods allowing researchers and companies to
release textual data while providing formal privacy guarantees to its
originators. In the field of NLP, substantial efforts have been directed at
building mechanisms following the framework of local differential privacy,
thereby anonymizing individual text samples before releasing them. In practice,
these approaches are often dissatisfying in terms of the quality of their
output language due to the strong noise required for local differential
privacy. In this paper, we approach the problem at hand using global
differential privacy, particularly by training a generative language model in a
differentially private manner and consequently sampling data from it. Using
natural language prompts and a new prompt-mismatch loss, we are able to create
highly accurate and fluent textual datasets taking on specific desired
attributes such as sentiment or topic and resembling statistical properties of
the training data. We perform thorough experiments indicating that our
synthetic datasets do not leak information from our original data and are of
high language quality and highly suitable for training models for further
analysis on real-world data. Notably, we also demonstrate that training
classifiers on private synthetic data outperforms directly training classifiers
on real data with DP-SGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Mechanism Priming Effects in Hindi Word Order. (arXiv:2210.13938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13938">
<div class="article-summary-box-inner">
<span><p>Word order choices during sentence production can be primed by preceding
sentences. In this work, we test the DUAL MECHANISM hypothesis that priming is
driven by multiple different sources. Using a Hindi corpus of text productions,
we model lexical priming with an n-gram cache model and we capture more
abstract syntactic priming with an adaptive neural language model. We permute
the preverbal constituents of corpus sentences, and then use a logistic
regression model to predict which sentences actually occurred in the corpus
against artificially generated meaning-equivalent variants. Our results
indicate that lexical priming and lexically-independent syntactic priming
affect complementary sets of verb classes. By showing that different priming
influences are separable from one another, our results support the hypothesis
that multiple different cognitive mechanisms underlie priming.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Context Predictability Effects in Hindi Word Order. (arXiv:2210.13940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13940">
<div class="article-summary-box-inner">
<span><p>We test the hypothesis that discourse predictability influences Hindi
syntactic choice. While prior work has shown that a number of factors (e.g.,
information status, dependency length, and syntactic surprisal) influence Hindi
word order preferences, the role of discourse predictability is underexplored
in the literature. Inspired by prior work on syntactic priming, we investigate
how the words and syntactic structures in a sentence influence the word order
of the following sentences. Specifically, we extract sentences from the
Hindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those
sentences, and build a classifier to predict which sentences actually occurred
in the corpus against artificially generated distractors. The classifier uses a
number of discourse-based features and cognitive features to make its
predictions, including dependency length, surprisal, and information status. We
find that information status and LSTM-based discourse predictability influence
word order choices, especially for non-canonical object-fronted orders. We
conclude by situating our results within the broader syntactic priming
literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Divider with Language Grounding in Multi-Agent Reinforcement Learning. (arXiv:2210.13942v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13942">
<div class="article-summary-box-inner">
<span><p>We investigate the use of natural language to drive the generalization of
policies in multi-agent settings. Unlike single-agent settings, the
generalization of policies should also consider the influence of other agents.
Besides, with the increasing number of entities in multi-agent settings, more
agent-entity interactions are needed for language grounding, and the enormous
search space could impede the learning process. Moreover, given a simple
general instruction,e.g., beating all enemies, agents are required to decompose
it into multiple subgoals and figure out the right one to focus on. Inspired by
previous work, we try to address these issues at the entity level and propose a
novel framework for language grounding in multi-agent reinforcement learning,
entity divider (EnDi). EnDi enables agents to independently learn subgoal
division at the entity level and act in the environment based on the associated
entities. The subgoal division is regularized by opponent modeling to avoid
subgoal conflicts and promote coordinated strategies. Empirically, EnDi
demonstrates the strong generalization ability to unseen games with new
dynamics and expresses the superiority over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13952">
<div class="article-summary-box-inner">
<span><p>We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reuse Distractors to support Multiple Choice Question Generation in Education. (arXiv:2210.13964v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13964">
<div class="article-summary-box-inner">
<span><p>Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects &amp; languages and a 77k multilingual pool of
distractor vocabulary for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks. (arXiv:2210.13979v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13979">
<div class="article-summary-box-inner">
<span><p>Large pretrained Transformer-based language models like BERT and GPT have
changed the landscape of Natural Language Processing (NLP). However, fine
tuning such models still requires a large number of training examples for each
target task, thus annotating multiple datasets and training these models on
various downstream tasks becomes time consuming and expensive. In this work, we
propose a simple extension of the Prototypical Networks for few-shot text
classification. Our main idea is to replace the class prototypes by Gaussians
and introduce a regularization term that encourages the examples to be
clustered near the appropriate class centroids. Experimental results show that
our method outperforms various strong baselines on 13 public and 4 internal
datasets. Furthermore, we use the class distributions as a tool for detecting
potential out-of-distribution (OOD) data points during deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">This joke is [MASK]: Recognizing Humor and Offense with Prompting. (arXiv:2210.13985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13985">
<div class="article-summary-box-inner">
<span><p>Humor is a magnetic component in everyday human interactions and
communications. Computationally modeling humor enables NLP systems to entertain
and engage with users. We investigate the effectiveness of prompting, a new
transfer learning paradigm for NLP, for humor recognition. We show that
prompting performs similarly to finetuning when numerous annotations are
available, but gives stellar performance in low-resource humor recognition. The
relationship between humor and offense is also inspected by applying influence
functions to prompting; we show that models could rely on offense to determine
humor during transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens. (arXiv:2210.14011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14011">
<div class="article-summary-box-inner">
<span><p>The term `spurious correlations' has been used in NLP to informally denote
any undesirable feature-label correlations. However, a correlation can be
undesirable because (i) the feature is irrelevant to the label (e.g.
punctuation in a review), or (ii) the feature's effect on the label depends on
the context (e.g. negation words in a review), which is ubiquitous in language
tasks. In case (i), we want the model to be invariant to the feature, which is
neither necessary nor sufficient for prediction. But in case (ii), even an
ideal model (e.g. humans) must rely on the feature, since it is necessary (but
not sufficient) for prediction. Therefore, a more fine-grained treatment of
spurious features is needed to specify the desired model behavior. We formalize
this distinction using a causal model and probabilities of necessity and
sufficiency, which delineates the causal relations between a feature and a
label. We then show that this distinction helps explain results of existing
debiasing methods on different spurious features, and demystifies surprising
results such as the encoding of spurious features in model representations
after debiasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14037">
<div class="article-summary-box-inner">
<span><p>Uncertainty approximation in text classification is an important area with
applications in domain adaptation and interpretability. The most widely used
uncertainty approximation method is Monte Carlo Dropout, which is
computationally expensive as it requires multiple forward passes through the
model. A cheaper alternative is to simply use a softmax to estimate model
uncertainty. However, prior work has indicated that the softmax can generate
overconfident uncertainty estimates and can thus be tricked into producing
incorrect predictions. In this paper, we perform a thorough empirical analysis
of both methods on five datasets with two base neural architectures in order to
reveal insight into the trade-offs between the two. We compare the methods'
uncertainty approximations and downstream text classification performance,
while weighing their performance against their computational complexity as a
cost-benefit analysis, by measuring runtime (cost) and the downstream
performance (benefit). We find that, while Monte Carlo produces the best
uncertainty approximations, using a simple softmax leads to competitive
uncertainty estimation for text classification at a much lower computational
cost, suggesting that softmax can in fact be a sufficient uncertainty estimate
when computational resources are a concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14061">
<div class="article-summary-box-inner">
<span><p>This study is devoted to two of the oldest known manuscripts in which the
oeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,
KBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of
codicological and contextual arguments, it is assumed that the scribe who
produced B used A as an exemplar. While the similarities in both layout and
content between the two manuscripts are striking, the present article seeks to
identify the differences. After all, regardless of the intention to produce a
copy that closely follows the exemplar, subtle linguistic variation is
apparent. Divergences relate to spelling conventions, but also to the way in
which words are abbreviated (and the extent to which abbreviations occur). The
present study investigates the spelling profiles of the scribes who produced
mss. A and B in a computational way. In the first part of this study, we will
present both manuscripts in more detail, after which we will consider prior
research carried out on scribal profiling. The current study both builds and
expands on Kestemont (2015). Next, we outline the methodology used to analyse
and measure the degree of scribal appropriation that took place when ms. B was
copied off the exemplar ms. A. After this, we will discuss the results
obtained, focusing on the scribal variation that can be found both at the level
of individual words and n-grams. To this end, we use machine learning to
identify the most distinctive features that separate manuscript A from B.
Finally, we look at possible diachronic trends in the appropriation by B's
scribe of his exemplar. We argue that scribal takeovers in the exemplar impacts
the practice of the copying scribe, while transitions to a different content
matter cause little to no effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Mode Connectivity for Pre-trained Language Models. (arXiv:2210.14102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14102">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the prevalent application of pre-trained language
models (PLMs) in NLP. From the perspective of parameter space, PLMs provide
generic initialization, starting from which high-performance minima could be
found. Although plenty of works have studied how to effectively and efficiently
adapt PLMs to high-performance minima, little is known about the connection of
various minima reached under different adaptation configurations. In this
paper, we investigate the geometric connections of different minima through the
lens of mode connectivity, which measures whether two minima can be connected
with a low-loss path. We conduct empirical analyses to investigate three
questions: (1) how could hyperparameters, specific tuning methods, and training
data affect PLM's mode connectivity? (2) How does mode connectivity change
during pre-training? (3) How does the PLM's task knowledge change along the
path connecting two minima? In general, exploring the mode connectivity of PLMs
conduces to understanding the geometric connection of different minima, which
may help us fathom the inner workings of PLM downstream adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models. (arXiv:2210.14128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14128">
<div class="article-summary-box-inner">
<span><p>We introduce a new open information extraction (OIE) benchmark for
pre-trained language models (LM). Recent studies have demonstrated that
pre-trained LMs, such as BERT and GPT, may store linguistic and relational
knowledge. In particular, LMs are able to answer ``fill-in-the-blank''
questions when given a pre-defined relation category. Instead of focusing on
pre-defined relations, we create an OIE benchmark aiming to fully examine the
open relational information present in the pre-trained LMs. We accomplish this
by turning pre-trained LMs into zero-shot OIE systems. Surprisingly,
pre-trained LMs are able to obtain competitive performance on both standard OIE
datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets
(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For
instance, the zero-shot pre-trained LMs outperform the F1 score of the
state-of-the-art supervised OIE methods on our factual OIE datasets without
needing to use any training sets. Our code and datasets are available at
https://github.com/cgraywang/IELM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyHope: Dataset Creation for a Two-Level Hope Speech Detection Task from Tweets. (arXiv:2210.14136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14136">
<div class="article-summary-box-inner">
<span><p>Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into "Hope" and "Not Hope",
then into three fine-grained hope categories: "Generalized Hope", "Realistic
Hope", and "Unrealistic Hope" (along with "Not Hope"). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14140">
<div class="article-summary-box-inner">
<span><p>Generating text with autoregressive language models (LMs) is of great
importance to many natural language processing (NLP) applications. Previous
solutions for this task often produce text that contains degenerative
expressions or lacks semantic consistency. Recently, Su et al. introduced a new
decoding method, contrastive search, based on the isotropic representation
space of the language model and obtained new state of the art on various
benchmarks. Additionally, Su et al. argued that the representations of
autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also
shared by previous study. Therefore, to ensure the language model follows an
isotropic distribution, Su et al. proposed a contrastive learning scheme,
SimCTG, which calibrates the language model's representations through
additional training.
</p>
<p>In this study, we first answer the question: "Are autoregressive LMs really
anisotropic?". To this end, we extensively evaluate the isotropy of LMs across
16 major languages. Surprisingly, we find that the anisotropic problem only
exists in the two specific English GPT-2-small/medium models. On the other
hand, all other evaluated LMs are naturally isotropic which is in contrast to
the conclusion drawn by previous studies. Based on our findings, we further
assess the contrastive search decoding method using off-the-shelf LMs on four
generation tasks across 16 languages. Our experimental results demonstrate that
contrastive search significantly outperforms previous decoding methods without
any additional training. More notably, on 12 out of 16 evaluated languages,
contrastive search performs comparably with human-level performances as judged
by human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14169">
<div class="article-summary-box-inner">
<span><p>Dialogue understanding tasks often necessitate abundant annotated data to
achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentation for dialogue
understanding by prompting large pre-trained language models and present a
novel approach that iterates on augmentation quality by applying
weakly-supervised filters. We evaluate our methods on the emotion and act
classification tasks in DailyDialog and the intent classification task in
Facebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our
augmented data mixed with few-shot ground truth data are able to approach or
surpass existing state-of-the-art performance on both datasets. For DailyDialog
specifically, using 10% of the ground truth data we outperform the current
state-of-the-art model which uses 100% of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Interpretable Summary Evaluation via Allocation of Contextual Embeddings to Reference Text Topics. (arXiv:2210.14174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14174">
<div class="article-summary-box-inner">
<span><p>Despite extensive recent advances in summary generation models, evaluation of
auto-generated summaries still widely relies on single-score systems
insufficient for transparent assessment and in-depth qualitative analysis.
Towards bridging this gap, we propose the multifaceted interpretable summary
evaluation method (MISEM), which is based on allocation of a summary's
contextual token embeddings to semantic topics identified in the reference
text. We further contribute an interpretability toolbox for automated summary
evaluation and interactive visual analysis of summary scoring, topic
identification, and token-topic allocation. MISEM achieves a promising .404
Pearson correlation with human judgment on the TAC'08 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence Functions for Sequence Tagging Models. (arXiv:2210.14177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14177">
<div class="article-summary-box-inner">
<span><p>Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,
and Semantic Role Labeling) are naturally framed as sequence tagging problems.
However, there has been comparatively little work on interpretability methods
for sequence tagging models. In this paper, we extend influence functions -
which aim to trace predictions back to the training points that informed them -
to sequence tagging tasks. We define the influence of a training instance
segment as the effect that perturbing the labels within this segment has on a
test segment level prediction. We provide an efficient approximation to compute
this, and show that it tracks with the true segment influence, measured
empirically. We show the practical utility of segment influence by using the
method to identify systematic annotation errors in two named entity recognition
corpora. Code to reproduce our results is available at
https://github.com/successar/Segment_Influence_Functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization. (arXiv:2210.14190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14190">
<div class="article-summary-box-inner">
<span><p>Social media has increasingly played a key role in emergency response: first
responders can use public posts to better react to ongoing crisis events and
deploy the necessary resources where they are most needed. Timeline extraction
and abstractive summarization are critical technical tasks to leverage large
numbers of social media posts about events. Unfortunately, there are few
datasets for benchmarking technical approaches for those tasks. This paper
presents CrisisLTLSum, the largest dataset of local crisis event timelines
available to date. CrisisLTLSum contains 1,000 crisis event timelines across
four domains: wildfires, local fires, traffic, and storms. We built
CrisisLTLSum using a semi-automated cluster-then-refine approach to collect
data from the public Twitter stream. Our initial experiments indicate a
significant gap between the performance of strong baselines compared to the
human performance on both tasks. Our dataset, code, and models are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing. (arXiv:2010.12676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12676">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representations (AMR) are a broad-coverage semantic
formalism which represents sentence meaning as a directed acyclic graph. To
train most AMR parsers, one needs to segment the graph into subgraphs and align
each such subgraph to a word in a sentence; this is normally done at
preprocessing, relying on hand-crafted rules. In contrast, we treat both
alignment and segmentation as latent variables in our model and induce them as
part of end-to-end training.
</p>
<p>As marginalizing over the structured latent variables is infeasible, we use
the variational autoencoding framework.
</p>
<p>To ensure end-to-end differentiable optimization, we introduce a
differentiable relaxation of the segmentation and alignment problems. We
observe that inducing segmentation yields substantial gains over using a
`greedy' segmentation heuristic. The performance of our method also approaches
that of a model that relies on the segmentation rules of
\citet{lyu-titov-2018-amr}, which were hand-crafted to handle individual AMR
constructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Semantics for Commonsense Knowledge Extraction. (arXiv:2011.00905v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00905">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge (CSK) about concepts and their properties is useful for
AI applications such as robust chatbots. Prior works like ConceptNet, TupleKB
and others compiled large CSK collections, but are restricted in their
expressiveness to subject-predicate-object (SPO) triples with simple concepts
for S and monolithic strings for P and O. Also, these projects have either
prioritized precision or recall, but hardly reconcile these complementary
goals. This paper presents a methodology, called Ascent, to automatically build
a large-scale knowledge base (KB) of CSK assertions, with advanced
expressiveness and both better precision and recall than prior works. Ascent
goes beyond triples by capturing composite concepts with subgroups and aspects,
and by refining assertions with semantic facets. The latter are important to
express temporal and spatial validity of assertions and further qualifiers.
Ascent combines open information extraction with judicious cleaning using
language models. Intrinsic evaluation shows the superior size and quality of
the Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the
benefits of Ascent. A web interface, data and code can be found at
https://ascent.mpi-inf.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Interpretations for Explainable Natural Language Processing: A Survey. (arXiv:2103.11072v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11072">
<div class="article-summary-box-inner">
<span><p>As the use of deep learning techniques has grown across various fields over
the past decade, complaints about the opaqueness of the black-box models have
increased, resulting in an increased focus on transparency in deep learning
models. This work investigates various methods to improve the interpretability
of deep neural networks for natural language processing (NLP) tasks, including
machine translation and sentiment analysis. We provide a comprehensive
discussion on the definition of the term \textit{interpretability} and its
various aspects at the beginning of this work. The methods collected and
summarised in this survey are only associated with local interpretation and are
divided into three categories: 1) explaining the model's predictions through
related input features; 2) explaining through natural language explanation; 3)
probing the hidden states of models and word representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Temporal Event Relation with Syntax-guided Graph Transformer. (arXiv:2104.09570v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09570">
<div class="article-summary-box-inner">
<span><p>Extracting temporal relations (e.g., before, after, and simultaneous) among
events is crucial to natural language understanding. One of the key challenges
of this problem is that when the events of interest are far away in text, the
context in-between often becomes complicated, making it challenging to resolve
the temporal relationship between them. This paper thus proposes a new
Syntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1)
explicitly exploiting the connection between two events based on their
dependency parsing trees, and (2) automatically locating temporal cues between
two events via a novel syntax-guided attention mechanism. Experiments on two
benchmark datasets, MATRES and TB-Dense, show that our approach significantly
outperforms previous state-of-the-art methods on both end-to-end temporal
relation extraction and temporal relation classification; This improvement also
proves to be robust on the contrast set of MATRES. The code is publicly
available at https://github.com/VT-NLP/Syntax-Guided-Graph-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04718">
<div class="article-summary-box-inner">
<span><p>In NMT we search for the mode of the model distribution to form predictions.
The mode and other high-probability translations found by beam search have been
shown to often be inadequate in a number of ways. This prevents improving
translation quality through better search, as these idiosyncratic translations
end up selected by the decoding algorithm, a problem known as the beam search
curse. Recently, an approximation to minimum Bayes risk (MBR) decoding has been
proposed as an alternative decision rule that would likely not suffer from the
same problems. We analyse this approximation and establish that it has no
equivalent to the beam search curse. We then design approximations that
decouple the cost of exploration from the cost of robust estimation of expected
utility. This allows for much larger hypothesis spaces, which we show to be
beneficial. We also show that mode-seeking strategies can aid in constructing
compact sets of promising hypotheses and that MBR is effective in identifying
good translations in them. We conduct experiments on three language pairs
varying in amounts of resources available: English into and from German,
Romanian, and Nepali.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Networks for Nomination and Representation Learning of Web Elements. (arXiv:2111.02168v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02168">
<div class="article-summary-box-inner">
<span><p>This paper tackles the under-explored problem of DOM element nomination and
representation learning with three important contributions. First, we present a
large-scale and realistic dataset of webpages, far richer and more diverse than
other datasets proposed for element representation learning, classification and
nomination on the web. The dataset contains $51,701$ manually labeled product
pages from $8,175$ real e-commerce websites. Second, we adapt several Graph
Neural Network (GNN) architectures to website DOM trees and benchmark their
performance on a diverse set of element nomination tasks using our proposed
dataset. In element nomination, a single element on a page is selected for a
given class. We show that on our challenging dataset a simple Convolutional GNN
outperforms state-of-the-art methods on web element nomination. Finally, we
propose a new training method that further boosts the element nomination
accuracy. In nomination for the web, classification (assigning a class to a
given element) is usually used as a surrogate objective for nomination during
training. Our novel training methodology steers the classification objective
towards the more complex and useful nomination objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning. (arXiv:2112.05253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05253">
<div class="article-summary-box-inner">
<span><p>Large-scale pretraining is fast becoming the norm in Vision-Language (VL)
modeling. However, prevailing VL approaches are limited by the requirement for
labeled data and the use of complex multi-step pretraining objectives. We
present MAGMA - a simple method for augmenting generative language models with
additional modalities using adapter-based finetuning. Building on Frozen, we
train a series of VL models that autoregressively generate text from arbitrary
combinations of visual and textual input. The pretraining is entirely
end-to-end using a single language modeling objective, simplifying optimization
compared to previous approaches. Importantly, the language model weights remain
unchanged during training, allowing for transfer of encyclopedic knowledge and
in-context learning abilities from language pretraining. MAGMA outperforms
Frozen on open-ended generative tasks, achieving state of the art results on
the OKVQA benchmark and competitive results on a range of other popular VL
benchmarks, while pretraining on 0.2% of the number of samples used to train
SimVLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10890">
<div class="article-summary-box-inner">
<span><p>Human education system trains one student by multiple experts.
Mixture-of-experts (MoE) is a powerful sparse architecture including multiple
experts. However, sparse MoE model is easy to overfit, hard to deploy, and not
hardware-friendly for practitioners. In this work, inspired by the human
education model, we propose a novel task, knowledge integration, to obtain a
dense student model (OneS) as knowledgeable as one sparse MoE. We investigate
this task by proposing a general training framework including knowledge
gathering and knowledge distillation. Specifically, to gather key knowledge
from different pre-trained experts, we first investigate four different
possible knowledge gathering methods, \ie summation, averaging, Top-K Knowledge
Gathering (Top-KG), and Singular Value Decomposition Knowledge Gathering
(SVD-KG) proposed in this paper. We then refine the dense student model by
knowledge distillation to offset the noise from gathering. On ImageNet, our
OneS preserves $61.7\%$ benefits from MoE and achieves $78.4\%$ top-1 accuracy
ImageNet with only $15$M parameters. On four natural language processing
datasets, OneS obtains $88.2\%$ MoE benefits and outperforms the best baseline
by $51.7\%$ using the same architecture and training data. In addition,
compared with the MoE counterpart, OneS can achieve $3.7 \times$ inference
speedup due to less computation and hardware-friendly architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00964">
<div class="article-summary-box-inner">
<span><p>We show that existing model interpretation methods such as linear probes and
prompts have some key limitations in answering these questions. We revisit KI
from an information-theoretic view and propose a new theoretically sound probe
called Graph Convolution Simulator (GCS) for KI interpretation. GCS uses graph
attention on the corresponding knowledge graph for interpretation. In our
experiments we verify that GCS can provide reasonable interpretation results
for two well-known knowledge-enhanced LMs: ERNIE and K-Adapter. We also find
that only a marginal amount of knowledge is successfully integrated in these
models, and simply increasing the size of the KI corpus may not lead to better
knowledge-enhanced LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Sequence Tagging Into A Seq2Seq Task. (arXiv:2203.08378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08378">
<div class="article-summary-box-inner">
<span><p>Pretrained, large, generative language models (LMs) have had great success in
a wide range of sequence tagging and structured prediction tasks. Casting a
sequence tagging task as a Seq2Seq one requires deciding the formats of the
input and output sequences. However, we lack a principled understanding of the
trade-offs associated with these formats (such as the effect on model accuracy,
sequence length, multilingual generalization, hallucination). In this paper, we
rigorously study different formats one could use for casting input text
sentences and their output labels into the input and target (i.e., output) of a
Seq2Seq model. Along the way, we introduce a new format, which we show to to be
both simpler and more effective. Additionally the new format demonstrates
significant gains in the multilingual settings -- both zero-shot transfer
learning and joint training. Lastly, we find that the new format is more robust
and almost completely devoid of hallucination -- an issue we find common in
existing formats. With well over a 1000 experiments studying 14 different
formats, over 7 diverse public benchmarks -- including 3 multilingual datasets
spanning 7 languages -- we believe our findings provide a strong empirical
basis in understanding how we should tackle sequence tagging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Transformer-based Models for Long Document Classification. (arXiv:2204.06683v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06683">
<div class="article-summary-box-inner">
<span><p>The recent literature in text classification is biased towards short text
sequences (e.g., sentences or paragraphs). In real-world applications,
multi-page multi-paragraph documents are common and they cannot be efficiently
encoded by vanilla Transformer-based models. We compare different
Transformer-based Long Document Classification (TrLDC) approaches that aim to
mitigate the computational overhead of vanilla transformers to encode much
longer text, namely sparse attention and hierarchical encoding methods. We
examine several aspects of sparse attention (e.g., size of local attention
window, use of global attention) and hierarchical (e.g., document splitting
strategy) transformers on four document classification datasets covering
different domains. We observe a clear benefit from being able to process longer
text, and, based on our results, we derive practical advice of applying
Transformer-based models on long document classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding. (arXiv:2205.03656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03656">
<div class="article-summary-box-inner">
<span><p>Despite the great success of spoken language understanding (SLU) in
high-resource languages, it remains challenging in low-resource languages
mainly due to the lack of labeled training data. The recent multilingual
code-switching approach achieves better alignments of model representations
across languages by constructing a mixed-language context in zero-shot
cross-lingual SLU. However, current code-switching methods are limited to
implicit alignment and disregard the inherent semantic structure in SLU, i.e.,
the hierarchical inclusion of utterances, slots, and words. In this paper, we
propose to model the utterance-slot-word structure by a multi-level contrastive
learning framework at the utterance, slot, and word levels to facilitate
explicit alignment. Novel code-switching schemes are introduced to generate
hard negative examples for our contrastive learning framework. Furthermore, we
develop a label-aware joint model leveraging label semantics to enhance the
implicit alignment and feed to contrastive learning. Our experimental results
show that our proposed methods significantly improve the performance compared
with the strong baselines on two zero-shot cross-lingual SLU benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering. (arXiv:2205.07257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07257">
<div class="article-summary-box-inner">
<span><p>Machine learning models are prone to overfitting their training (source)
domains, which is commonly believed to be the reason why they falter in novel
target domains. Here we examine the contrasting view that multi-source domain
generalization (DG) is first and foremost a problem of mitigating source domain
underfitting: models not adequately learning the signal already present in
their multi-domain training data. Experiments on a reading comprehension DG
benchmark show that as a model learns its source domains better -- using
familiar methods such as knowledge distillation (KD) from a bigger model -- its
zero-shot out-of-domain utility improves at an even faster pace. Improved
source domain learning also demonstrates superior out-of-domain generalization
over three popular existing DG approaches that aim to limit overfitting. Our
implementation of KD-based domain generalization is available via PrimeQA at:
https://ibm.biz/domain-generalization-with-kd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Tracing Factual Knowledge in Language Models Back to the Training Data. (arXiv:2205.11482v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11482">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have been shown to memorize a great deal of factual
knowledge contained in their training data. But when an LM generates an
assertion, it is often difficult to determine where it learned this information
and whether it is true. In this paper, we propose the problem of fact tracing:
identifying which training examples taught an LM to generate a particular
factual assertion. Prior work on training data attribution (TDA) may offer
effective tools for identifying such examples, known as "proponents". We
present the first quantitative benchmark to evaluate this. We compare two
popular families of TDA methods -- gradient-based and embedding-based -- and
find that much headroom remains. For example, both methods have lower
proponent-retrieval precision than an information retrieval baseline (BM25)
that does not have access to the LM at all. We identify key challenges that may
be necessary for further improvement such as overcoming the problem of gradient
saturation, and also show how several nuanced implementation details of
existing neural TDA methods can significantly improve overall fact tracing
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. (arXiv:2205.11822v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11822">
<div class="article-summary-box-inner">
<span><p>Despite their impressive capabilities, large pre-trained language models
(LMs) struggle with consistent reasoning; recently, prompting LMs to generate
explanations that self-guide the inference has emerged as a promising direction
to amend this. However, these approaches are fundamentally bounded by the
correctness of explanations, which themselves are often noisy and inconsistent.
In this work, we develop Maieutic Prompting, which infers a correct answer to a
question even from the noisy and inconsistent generations of LM. Maieutic
Prompting induces a tree of explanations abductively (e.g. X is true, because
...) and recursively, then frames the inference as a satisfiability problem
over these explanations and their logical relations. We test Maieutic Prompting
for true/false QA on three challenging benchmarks that require complex
commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy
than state-of-the-art prompting methods, and as a fully unsupervised approach,
performs competitively with supervised models. We also show that Maieutic
Prompting improves robustness in inference while providing interpretable
rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer. (arXiv:2205.12148v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12148">
<div class="article-summary-box-inner">
<span><p>Massively multilingual models are promising for transfer learning across
tasks and languages. However, existing methods are unable to fully leverage
training data when it is available in different task-language combinations. To
exploit such heterogeneous supervision, we propose Hyper-X, a single
hypernetwork that unifies multi-task and multilingual learning with efficient
adaptation. This model generates weights for adapter modules conditioned on
both tasks and language embeddings. By learning to combine task and
language-specific knowledge, our model enables zero-shot transfer for unseen
languages and task-language combinations. Our experiments on a diverse set of
languages demonstrate that Hyper-X achieves the best or competitive gain when a
mixture of multiple resources is available, while being on par with strong
baselines in the standard scenario. Hyper-X is also considerably more efficient
in terms of parameters and resources compared to methods that train separate
adapters. Finally, Hyper-X consistently produces strong results in few-shot
scenarios for new languages, showing the versatility of our approach beyond
zero-shot transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing. (arXiv:2205.12253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12253">
<div class="article-summary-box-inner">
<span><p>Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media. (arXiv:2205.12382v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12382">
<div class="article-summary-box-inner">
<span><p>NLP research on public opinion manipulation campaigns has primarily focused
on detecting overt strategies such as fake news and disinformation. However,
information manipulation in the ongoing Russia-Ukraine war exemplifies how
governments and media also employ more nuanced strategies. We release a new
dataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter
and VKontakte, as well as public activity and responses, immediately preceding
and during the 2022 Russia-Ukraine war. We apply standard and
recently-developed NLP models on VoynaSlov to examine agenda setting, framing,
and priming, several strategies underlying information manipulation, and reveal
variation across media outlet control, social media platform, and time. Our
examination of these media effects and extensive discussion of current
approaches' limitations encourage further development of NLP models for
understanding information manipulation in emerging crises, as well as other
real-world and interdisciplinary tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification. (arXiv:2205.12528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12528">
<div class="article-summary-box-inner">
<span><p>Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging QA Datasets to Improve Generative Data Augmentation. (arXiv:2205.12604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12604">
<div class="article-summary-box-inner">
<span><p>The ability of generative language models (GLMs) to generate text has
improved considerably in the last few years, enabling their use for generative
data augmentation. In this work, we propose CONDA, an approach to further
improve GLMs' ability to generate synthetic data by reformulating data
generation as context generation for a given question-answer (QA) pair and
leveraging QA datasets for training context generators. Then, we cast
downstream tasks into the same question answering format and adapt the
fine-tuned context generators to the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts, which are in turn used as
synthetic training data for their corresponding tasks. We perform extensive
experiments on multiple classification datasets and demonstrate substantial
improvements in performance for both few- and zero-shot settings. Our analysis
reveals that QA datasets that require high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Memory Augmentation. (arXiv:2205.12674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12674">
<div class="article-summary-box-inner">
<span><p>Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProsocialDialog: A Prosocial Backbone for Conversational Agents. (arXiv:2205.12688v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12688">
<div class="article-summary-box-inner">
<span><p>Most existing dialogue systems fail to respond properly to potentially unsafe
user utterances by either ignoring or passively agreeing with them. To address
this issue, we introduce ProsocialDialog, the first large-scale multi-turn
dialogue dataset to teach conversational agents to respond to problematic
content following social norms. Covering diverse unethical, problematic,
biased, and toxic situations, ProsocialDialog contains responses that encourage
prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,
RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists
of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue
safety labels accompanied by free-form rationales.
</p>
<p>With this dataset, we introduce a dialogue safety detection module, Canary,
capable of generating RoTs given conversational context, and a
socially-informed dialogue agent, Prost. Empirical results show that Prost
generates more socially acceptable dialogues compared to other state-of-the-art
language and dialogue models in both in-domain and out-of-domain settings.
Additionally, Canary effectively guides conversational agents and off-the-shelf
language models to generate significantly more prosocial responses. Our work
highlights the promise and importance of creating and steering conversational
AI to be socially responsible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12696">
<div class="article-summary-box-inner">
<span><p>The DocRED dataset is one of the most popular and widely used benchmarks for
document-level relation extraction (RE). It adopts a recommend-revise
annotation scheme so as to have a large-scale annotated dataset. However, we
find that the annotation of DocRED is incomplete, i.e., false negative samples
are prevalent. We analyze the causes and effects of the overwhelming false
negative problem in the DocRED dataset. To address the shortcoming, we
re-annotate 4,053 documents in the DocRED dataset by adding the missed relation
triples back to the original DocRED. We name our revised DocRED dataset
Re-DocRED. We conduct extensive experiments with state-of-the-art neural models
on both datasets, and the experimental results show that the models trained and
evaluated on our Re-DocRED achieve performance improvements of around 13 F1
points. Moreover, we conduct a comprehensive analysis to identify the potential
areas for further improvement. Our dataset is publicly available at
https://github.com/tonytan48/Re-DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0. (arXiv:2206.14348v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14348">
<div class="article-summary-box-inner">
<span><p>Performance in natural language processing, and specifically for the
question-answer task, is typically measured by comparing a model\'s most
confident (primary) prediction to golden answers (the ground truth). We are
making the case that it is also useful to quantify how close a model came to
predicting a correct answer even for examples that failed. We define the Golden
Rank (GR) of an example as the rank of its most confident prediction that
exactly matches a ground truth, and show why such a match always exists. For
the 16 transformer models we analyzed, the majority of exactly matched golden
answers in secondary prediction space hover very close to the top rank. We
refer to secondary predictions as those ranking above 0 in descending
confidence probability order. We demonstrate how the GR can be used to classify
questions and visualize their spectrum of difficulty, from persistent near
successes to persistent extreme failures. We derive a new aggregate statistic
over entire test sets, named the Golden Rank Interpolated Median (GRIM) that
quantifies the proximity of failed predictions to the top choice made by the
model. To develop some intuition and explore the applicability of these metrics
we use the Stanford Question Answering Dataset (SQuAD-2) and a few popular
transformer models from the Hugging Face hub. We first demonstrate that the
GRIM is not directly correlated with the F1 and exact match (EM) scores. We
then calculate and visualize these scores for various transformer
architectures, probe their applicability in error analysis by clustering failed
predictions, and compare how they relate to other training diagnostics such as
the EM and F1 scores. We finally suggest various research goals, such as
broadening data collection for these metrics and their possible use in
adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TweetNLP: Cutting-Edge Natural Language Processing for Social Media. (arXiv:2206.14774v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14774">
<div class="article-summary-box-inner">
<span><p>In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confident Adaptive Language Modeling. (arXiv:2207.07061v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07061">
<div class="article-summary-box-inner">
<span><p>Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology. (arXiv:2208.05140v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.05140">
<div class="article-summary-box-inner">
<span><p>Oversight AI is an emerging concept in radiology where the AI forms a
symbiosis with radiologists by continuously supporting radiologists in their
decision-making. Recent advances in vision-language pre-training sheds a light
on the long-standing problems of the oversight AI by the understanding of both
visual and textual concepts and their semantic correspondences. However, there
have been limited successes in the application of vision-language pre-training
in the medical domain, as the current vision-language models and learning
strategies for photographic images and captions are not optimal to process the
medical data that are usually insufficient in the amount and the diversity. To
address this, here we present medical X-VL, a self-supervised model tailored
for efficient vision-language pre-training that exploits cross attention in the
radiological images and reports' common feature space in a symmetric manner. We
experimentally demonstrate that the pre-trained medical X-VL model outperforms
the current state-of-the-art models in various vision-language tasks in medical
domains. We finally demonstrate practical clinical usages of our oversight AI
for monitoring human errors and in the diagnosis of newly emerging diseases,
which suggests the potential of an oversight AI model for widespread
applicability in different medical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F-coref: Fast, Accurate and Easy to Use Coreference Resolution. (arXiv:2209.04280v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04280">
<div class="article-summary-box-inner">
<span><p>We introduce fastcoref, a python package for fast, accurate, and easy-to-use
English coreference resolution. The package is pip-installable, and allows two
modes: an accurate mode based on the LingMess architecture, providing
state-of-the-art coreference accuracy, and a substantially faster model,
F-coref, which is the focus of this work. F-coref allows to process 2.8K
OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the
LingMess model, and to 12 minutes of the popular AllenNLP coreference model)
with only a modest drop in accuracy. The fast speed is achieved through a
combination of distillation of a compact model from the LingMess model, and an
efficient batching implementation using a technique we call leftover batching.
Our code is available at https://github.com/shon-otmazgin/fastcoref
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Large Pre-Trained Language Models for Machine Translation: What You Don't Know About It. (arXiv:2209.07417v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07417">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) often take advantage of the monolingual
and multilingual dataset that is freely available online to acquire general or
mixed domain knowledge before deployment into specific tasks. Extra-large PLMs
(xLPLMs) are proposed very recently to claim supreme performances over
smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs
include Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this
work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in
fine-tuning toward domain-specific MTs. We use two different in-domain data of
different sizes: commercial automotive in-house data and clinical shared task
data from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian
Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from
Meta-AI as xLPLMs.
</p>
<p>Our experimental investigation shows that 1) on smaller-sized in-domain
commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much
better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized
Marian, even though its score increase rate is lower than Marian after
fine-tuning; 2) on relatively larger-size well prepared clinical data
fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized
Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn
offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on
Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;
3) metrics do not always agree with each other on the same tasks using the same
model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and
Task-3 (via METEOR and ROUGE) among all submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model. (arXiv:2210.00705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00705">
<div class="article-summary-box-inner">
<span><p>Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04185">
<div class="article-summary-box-inner">
<span><p>Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When the full training set is given, our method can still serve as an
effective data augmentation method to further improve performance. Human
evaluation results show that our simulated dialogues have near-human fluency
and annotation accuracy. The code and data are available at
\textbf{\url{https://github.com/Leezekun/dialogic}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Compressing Sequences for Self-Supervised Speech Models. (arXiv:2210.07189v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07189">
<div class="article-summary-box-inner">
<span><p>Compressing self-supervised models has become increasingly necessary, as
self-supervised models become larger. While previous approaches have primarily
focused on compressing the model size, shortening sequences is also effective
in reducing the computational cost. In this work, we study fixed-length and
variable-length subsampling along the time axis in self-supervised learning. We
explore how individual downstream tasks are sensitive to input frame rates.
Subsampling while training self-supervised models not only improves the overall
performance on downstream tasks under certain frame rates, but also brings
significant speed-up in inference. Variable-length subsampling performs
particularly well under low frame rates. In addition, if we have access to
phonetic boundaries, we find no degradation in performance for an average frame
rate as low as 10 Hz.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G-Augment: Searching for the Meta-Structure of Data Augmentation Policies for ASR. (arXiv:2210.10879v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10879">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a ubiquitous technique used to provide robustness to
automatic speech recognition (ASR) training. However, even as so much of the
ASR training process has become automated and more "end-to-end", the data
augmentation policy (what augmentation functions to use, and how to apply them)
remains hand-crafted. We present Graph-Augment, a technique to define the
augmentation space as directed acyclic graphs (DAGs) and search over this space
to optimize the augmentation policy itself. We show that given the same
computational budget, policies produced by G-Augment are able to perform better
than SpecAugment policies obtained by random search on fine-tuning tasks on
CHiME-6 and AMI. G-Augment is also able to establish a new state-of-the-art ASR
performance on the CHiME-6 evaluation set (30.7% WER). We further demonstrate
that G-Augment policies show better transfer properties across warm-start to
cold-start training and model size compared to random-searched SpecAugment
policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11498">
<div class="article-summary-box-inner">
<span><p>Traditional (fickle) adversarial examples involve finding a small
perturbation that does not change an input's true label but confuses the
classifier into outputting a different prediction. Conversely, obstinate
adversarial examples occur when an adversary finds a small perturbation that
preserves the classifier's prediction but changes the true label of an input.
Adversarial training and certified robust training have shown some
effectiveness in improving the robustness of machine learnt models to fickle
adversarial examples. We show that standard adversarial training methods
focused on reducing vulnerability to fickle adversarial examples may make a
model more vulnerable to obstinate adversarial examples, with experiments for
both natural language inference and paraphrase identification tasks. To counter
this phenomenon, we introduce Balanced Adversarial Training, which incorporates
contrastive learning to increase robustness against both fickle and obstinate
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Self-Improve. (arXiv:2210.11610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11610">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate "high-confidence" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and
63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroLID: A Neural Language Identification Tool for African Languages. (arXiv:2210.11744v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11744">
<div class="article-summary-box-inner">
<span><p>Language identification (LID) is a crucial precursor for NLP, especially for
mining web data. Problematically, most of the world's 7000+ languages today are
not covered by LID technologies. We address this pressing issue for Africa by
introducing AfroLID, a neural LID toolkit for $517$ African languages and
varieties. AfroLID exploits a multi-domain web dataset manually curated from
across 14 language families utilizing five orthographic systems. When evaluated
on our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare
AfroLID to five existing LID tools that each cover a small number of African
languages, finding it to outperform them on most languages. We further show the
utility of AfroLID in the wild by testing it on the acutely under-served
Twitter domain. Finally, we offer a number of controlled case studies and
perform a linguistically-motivated error analysis that allow us to both
showcase AfroLID's powerful capabilities and limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models. (arXiv:2210.12403v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12403">
<div class="article-summary-box-inner">
<span><p>A wide range of NLP tasks benefit from the fine-tuning of pretrained language
models (PLMs). However, a number of redundant parameters which contribute less
to the downstream task are observed in a directly fine-tuned model. We consider
the gap between pretraining and downstream tasks hinders the training of these
redundant parameters, and results in a suboptimal performance of the overall
model. In this paper, we present PATS (Perturbation According To Sensitivity),
a noisy training mechanism which considers each parameter's importance in the
downstream task to help fine-tune PLMs. The main idea of PATS is to add bigger
noise to parameters with lower sensitivity and vice versa, in order to activate
more parameters' contributions to downstream tasks without affecting the
sensitive ones much. Extensive experiments conducted on different tasks of the
GLUE benchmark show PATS can consistently empower the fine-tuning of different
sizes of PLMs, and the parameters in the well-performing models always have
more concentrated distributions of sensitivities, which experimentally proves
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Unified M-Tree Coding Solver for MathWord Problem. (arXiv:2210.12432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12432">
<div class="article-summary-box-inner">
<span><p>As one of the challenging NLP tasks, designing math word problem (MWP)
solvers has attracted increasing research attention for the past few years. In
previous work, models designed by taking into account the properties of the
binary tree structure of mathematical expressions at the output side have
achieved better performance. However, the expressions corresponding to a MWP
are often diverse (e.g., $n_1+n_2 \times n_3-n_4$, $n_3\times n_2-n_4+n_1$,
etc.), and so are the corresponding binary trees, which creates difficulties in
model learning due to the non-deterministic output space. In this paper, we
propose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies
a tree with any M branches (M-tree) to unify the output structures. To learn
the M-tree, we use a mapping to convert the M-tree into the M-tree codes, where
codes store the information of the paths from tree root to leaf nodes and the
information of leaf nodes themselves, and then devise a Sequence-to-Code
(seq2code) model to generate the codes. Experimental results on the widely used
MAWPS and Math23K datasets have demonstrated that SUMC-Solver not only
outperforms several state-of-the-art models under similar experimental settings
but also performs much better under low-resource conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexical Generalization Improves with Larger Models and Longer Training. (arXiv:2210.12673v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12673">
<div class="article-summary-box-inner">
<span><p>While fine-tuned language models perform well on many tasks, they were also
shown to rely on superficial surface features such as lexical overlap.
Excessive utilization of such heuristics can lead to failure on challenging
inputs. We analyze the use of lexical overlap heuristics in natural language
inference, paraphrase detection, and reading comprehension (using a novel
contrastive dataset), and find that larger models are much less susceptible to
adopting lexical overlap heuristics. We also find that longer training leads
models to abandon lexical overlap heuristics. Finally, we provide evidence that
the disparity between models size has its source in the pre-trained model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Essay Scoring using Transformers. (arXiv:2210.12809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12809">
<div class="article-summary-box-inner">
<span><p>Despite being investigated for over five decades, the task of automated essay
scoring continues to draw a lot of attention in the NLP community, in part
because of its commercial and educational values as well as the associated
research challenges. Large pre-trained models have made remarkable progress in
NLP. Data augmentation techniques have also helped build state-of-the-art
models for automated essay scoring. Many works in the past have attempted to
solve this problem by using RNNs, LSTMs, etc. This work examines the
transformer models like BERT, RoBERTa, etc. We empirically demonstrate the
effectiveness of transformer models and data augmentation for automated essay
grading across many topics using a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DALL-E 2 Fails to Reliably Capture Common Syntactic Processes. (arXiv:2210.12889v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12889">
<div class="article-summary-box-inner">
<span><p>Machine intelligence is increasingly being linked to claims about sentience,
language processing, and an ability to comprehend and transform natural
language into a range of stimuli. We systematically analyze the ability of
DALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that
are widely discussed in linguistics and pervasive in human language: binding
principles and coreference, passives, word order, coordination, comparatives,
negation, ellipsis, and structural ambiguity. Whereas young children routinely
master these phenomena, learning systematic mappings between syntax and
semantics, DALL-E 2 is unable to reliably infer meanings that are consistent
with the syntax. These results challenge recent claims concerning the capacity
of such systems to understand of human language. We make available the full set
of test materials as a benchmark for future testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models. (arXiv:2210.13029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13029">
<div class="article-summary-box-inner">
<span><p>Zero-shot cross-lingual transfer learning has been shown to be highly
challenging for tasks involving a lot of linguistic specificities or when a
cultural gap is present between languages, such as in hate speech detection. In
this paper, we highlight this limitation for hate speech detection in several
domains and languages using strict experimental settings. Then, we propose to
train on multilingual auxiliary tasks -- sentiment analysis, named entity
recognition, and tasks relying on syntactic information -- to improve zero-shot
transfer of hate speech detection models across languages. We show how hate
speech detection models benefit from a cross-lingual knowledge proxy brought by
auxiliary tasks fine-tuning and highlight these tasks' positive impact on
bridging the hate speech linguistic and cultural gap between languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. (arXiv:2210.13382v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13382">
<div class="article-summary-box-inner">
<span><p>Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-26 23:18:06.595843134 UTC">2022-10-26 23:18:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>