<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-23T01:30:00Z">03-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12112">
<div class="article-summary-box-inner">
<span><p>The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. (arXiv:2303.12132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12132">
<div class="article-summary-box-inner">
<span><p>Generative Language Models gained significant attention in late 2022 / early
2023, notably with the introduction of models refined to act consistently with
users' expectations of interactions with AI (conversational models). Arguably
the focal point of public attention has been such a refinement of the GPT3
model -- the ChatGPT and its subsequent integration with auxiliary
capabilities, including search as part of Microsoft Bing. Despite extensive
prior research invested in their development, their performance and
applicability to a range of daily tasks remained unclear and niche. However,
their wider utilization without a requirement for technical expertise, made in
large part possible through conversational fine-tuning, revealed the extent of
their true capabilities in a real-world environment. This has garnered both
public excitement for their potential applications and concerns about their
capabilities and potential malicious uses. This review aims to provide a brief
overview of the history, state of the art, and implications of Generative
Language Models in terms of their principles, abilities, limitations, and
future prospects -- especially in the context of cyber-defense, with a focus on
the Swiss operational environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12135">
<div class="article-summary-box-inner">
<span><p>The growth of pending legal cases in populous countries, such as India, has
become a major issue. Developing effective techniques to process and understand
legal documents is extremely useful in resolving this problem. In this paper,
we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi
et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that
considers the comprehensive context information in both intra- and
inter-sentence levels to predict rhetorical roles (subtask A) and then train a
Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize
legal entities (subtask B). Our evaluations demonstrate that our designed
models are more accurate than baselines, e.g., with an up to 15.0% better F1
score in subtask B. We achieved notable performance in the task leaderboard,
e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12208">
<div class="article-summary-box-inner">
<span><p>While generative modeling on multimodal image-text data has been actively
developed with large-scale paired datasets, there have been limited attempts to
generate both image and text data by a single model rather than a generation of
one fixed modality conditioned on the other modality. In this paper, we explore
a unified generative vision-and-language (VL) model that can produce both
images and text sequences. Especially, we propose a generative VL transformer
based on the non-autoregressive mask prediction, named MAGVLT, and compare it
with an autoregressive generative VL transformer (ARGVLT). In comparison to
ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast
decoding by parallel token predictions in an iterative refinement, and extended
editing capabilities such as image and text infilling. For rigorous training of
our MAGVLT with image-text pairs from scratch, we combine the image-to-text,
text-to-image, and joint image-and-text mask prediction tasks. Moreover, we
devise two additional tasks based on the step-unrolled mask prediction and the
selective prediction on the mixture of two image-text pairs. Experimental
results on various downstream generation tasks of VL benchmarks show that our
MAGVLT outperforms ARGVLT by a large margin even with significant inference
speedup. Particularly, MAGVLT achieves competitive results on both zero-shot
image-to-text and text-to-image generation tasks from MS-COCO by one
moderate-sized model (fewer than 500M parameters) even without the use of
monomodal data and networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Taxonomy of Deep Syntactic Relations. (arXiv:2303.12220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12220">
<div class="article-summary-box-inner">
<span><p>This paper analyzes multiple deep-syntactic frameworks with the goal of
creating a proposal for a set of universal semantic role labels. The proposal
examines various theoretic linguistic perspectives and focuses on Meaning-Text
Theory and Functional Generative Description frameworks.
</p>
<p>For the purpose of this research, data from four languages is used -- Spanish
and Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English
(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies
(de Marneffe et al., 2021) with a further intention of applying the universal
semantic role labels to the UD data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12294">
<div class="article-summary-box-inner">
<span><p>Neural network models have been proposed to explain the grapheme-phoneme
mapping process in humans for many alphabet languages. These models not only
successfully learned the correspondence of the letter strings and their
pronunciation, but also captured human behavior in nonce word naming tasks. How
would the neural models perform for a non-alphabet language (e.g., Chinese)
unknown character task? How well would the model capture human behavior? In
this study, we evaluate a set of transformer models and compare their
performances with human behaviors on an unknown Chinese character naming task.
We found that the models and humans behaved very similarly, that they had
similar accuracy distribution for each character, and had a substantial overlap
in answers. In addition, the models' answers are highly correlated with humans'
answers. These results suggested that the transformer models can well capture
human's character naming behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network. (arXiv:2303.12300v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12300">
<div class="article-summary-box-inner">
<span><p>In recent years, End-to-End speech recognition technology based on deep
learning has developed rapidly. Due to the lack of Turkish speech data, the
performance of Turkish speech recognition system is poor. Firstly, this paper
studies a series of speech recognition tuning technologies. The results show
that the performance of the model is the best when the data enhancement
technology combining speed perturbation with noise addition is adopted and the
beam search width is set to 16. Secondly, to maximize the use of effective
feature information and improve the accuracy of feature extraction, this paper
proposes a new feature extractor LSPC. LSPC and LiGRU network are combined to
form a shared encoder structure, and model compression is realized. The results
show that the performance of LSPC is better than MSPC and VGGnet when only
using Fbank features, and the WER is improved by 1.01% and 2.53% respectively.
Finally, based on the above two points, a new multi-feature fusion network is
proposed as the main structure of the encoder. The results show that the WER of
the proposed feature fusion network based on LSPC is improved by 0.82% and
1.94% again compared with the single feature (Fbank feature and Spectrogram
feature) extraction using LSPC. Our model achieves performance comparable to
that of advanced End-to-End models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12308">
<div class="article-summary-box-inner">
<span><p>Lack of encyclopedic text contributors, especially on Wikipedia, makes
automated text generation for \emph{low resource (LR) languages} a critical
problem. Existing work on Wikipedia text generation has focused on
\emph{English only} where English reference articles are summarized to generate
English Wikipedia pages. But, for low-resource languages, the scarcity of
reference articles makes monolingual summarization ineffective in solving this
problem. Hence, in this work, we propose \task{}, which is the task of
cross-lingual multi-document summarization of text from multiple reference
articles, written in various languages, to generate Wikipedia-style text.
Accordingly, we contribute a benchmark dataset, \data{}, spanning $\sim$69K
Wikipedia articles covering five domains and eight languages. We harness this
dataset to train a two-stage system where the input is a set of citations and a
section title and the output is a section-specific LR summary. The proposed
system is based on a novel idea of neural unsupervised extractive summarization
to coarsely identify salient information followed by a neural abstractive model
to generate the section-specific text. Extensive experiments show that
multi-domain training is better than the multi-lingual setup on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12314">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a parameter-efficient method, which learns soft prompts and
conditions frozen language models to perform specific downstream tasks. Though
effective, prompt tuning under few-shot settings on the one hand heavily relies
on a good initialization of soft prompts. On the other hand, it can easily
result in overfitting. Existing works leverage pre-training or supervised
meta-learning to initialize soft prompts but they cannot data-efficiently
generalize to unseen downstream tasks. To address the above problems, this
paper proposes a novel Self-sUpervised meta-Prompt learning framework with
meta-gradient Regularization for few-shot generalization (SUPMER). We first
design a set of self-supervised anchor meta-training tasks with different task
formats and further enrich the task distribution with curriculum-based task
augmentation. Then a novel meta-gradient regularization method is integrated
into meta-prompt learning. It meta-learns to transform the raw gradients during
few-shot learning into a domain-generalizable direction, thus alleviating the
problem of overfitting. Extensive experiments show that SUPMER achieves better
performance for different few-shot downstream tasks, and also exhibits a
stronger domain generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12320">
<div class="article-summary-box-inner">
<span><p>Commonsense question-answering (QA) methods combine the power of pre-trained
Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A
typical approach collects nodes relevant to the QA pair from a KG to form a
Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).
This faces two major challenges: (i) it is difficult to capture all the
information from the QA in the WG, and (ii) the WG contains some irrelevant
nodes from the KG. To address these, we propose GrapeQA with two simple
improvements on the WG: (i) Prominent Entities for Graph Augmentation
identifies relevant text chunks from the QA pair and augments the WG with
corresponding latent representations from the LM, and (ii) Context-Aware Node
Pruning removes nodes that are less relevant to the QA pair. We evaluate our
results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows
consistent improvements over its LM + KG predecessor (QA-GNN in particular) and
large improvements on OpenBookQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12489">
<div class="article-summary-box-inner">
<span><p>While few-shot learning as a transfer learning paradigm has gained
significant traction for scenarios with limited data, it has primarily been
explored in the context of building unimodal and unilingual models.
Furthermore, a significant part of the existing literature in the domain of
few-shot multitask learning perform in-context learning which requires manually
generated prompts as the input, yielding varying outcomes depending on the
level of manual prompt-engineering. In addition, in-context learning suffers
from substantial computational, memory, and storage costs which eventually
leads to high inference latency because it involves running all of the prompt's
examples through the model every time a prediction is made. In contrast,
methods based on the transfer learning via the fine-tuning paradigm avoid the
aforementioned issues at a one-time cost of fine-tuning weights on a per-task
basis. However, such methods lack exposure to few-shot multimodal multitask
learning. In this paper, we propose few-shot learning for a multimodal
multitask multilingual (FM3) setting by adapting pre-trained vision and
language models using task-specific hypernetworks and contrastively fine-tuning
them to enable few-shot learning. FM3's architecture combines the best of both
worlds of in-context and fine-tuning based learning and consists of three major
components: (i) multimodal contrastive fine-tuning to enable few-shot learning,
(ii) hypernetwork task adaptation to perform multitask learning, and (iii)
task-specific output heads to cater to a plethora of diverse tasks. FM3 learns
the most prominent tasks in the vision and language domains along with their
intersections, namely visual entailment (VE), visual question answering (VQA),
and natural language understanding (NLU) tasks such as neural entity
recognition (NER) and the GLUE benchmark including QNLI, MNLI, QQP, and SST-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12513">
<div class="article-summary-box-inner">
<span><p>Most humans use visual imagination to understand and reason about language,
but models such as BERT reason about language using knowledge acquired during
text-only pretraining. In this work, we investigate whether vision-and-language
pretraining can improve performance on text-only tasks that involve implicit
visual reasoning, focusing primarily on zero-shot probing methods. We propose a
suite of visual language understanding (VLU) tasks for probing the visual
reasoning abilities of text encoder models, as well as various non-visual
natural language understanding (NLU) tasks for comparison. We also contribute a
novel zero-shot knowledge probing method, Stroop probing, for applying models
such as CLIP to text-only tasks without needing a prediction head such as the
masked language modelling head of models like BERT. We show that SOTA
multimodally trained text encoders outperform unimodally trained text encoders
on the VLU tasks while being underperformed by them on the NLU tasks, lending
new context to previously mixed results regarding the NLU capabilities of
multimodal models. We conclude that exposure to images during pretraining
affords inherent visual reasoning knowledge that is reflected in language-only
tasks that require implicit visual reasoning. Our findings bear importance in
the broader context of multimodal learning, providing principled guidelines for
the choice of text encoders used in such contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12528">
<div class="article-summary-box-inner">
<span><p>Generative AI models have impressive performance on many Natural Language
Processing tasks such as language understanding, reasoning and language
generation. One of the most important questions that is being asked by the AI
community today is about the capabilities and limits of these models, and it is
clear that evaluating generative AI is very challenging. Most studies on
generative Large Language Models (LLMs) are restricted to English and it is
unclear how capable these models are at understanding and generating other
languages. We present the first comprehensive benchmarking of generative LLMs -
MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse
tasks and 33 typologically diverse languages. We also compare the performance
of generative LLMs to State of the Art (SOTA) non-autoregressive models on
these tasks to determine how well generative models perform compared to the
previous generation of LLMs. We present a thorough analysis of the performance
of models across languages and discuss some of the reasons why generative LLMs
are currently not optimal for all languages. We create a framework for
evaluating generative LLMs in the multilingual setting and provide directions
for future progress in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12570">
<div class="article-summary-box-inner">
<span><p>The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model, which allows for the effective
utilization of repository-level information for code completion and grants the
ability to generate code at various levels of granularity. Furthermore,
RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges
the gap between retrieval context and the intended completion target. We also
propose a new benchmark RepoEval, which consists of the latest and high-quality
real-world repositories covering line, API invocation, and function body
completion scenarios. We test the performance of RepoCoder by using various
combinations of code retrievers and generators. Experimental results indicate
that RepoCoder significantly improves the zero-shot code completion baseline by
over 10% in all settings and consistently outperforms the vanilla
retrieval-augmented code completion approach. Furthermore, we validate the
effectiveness of RepoCoder through comprehensive analysis, providing valuable
insights for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12582">
<div class="article-summary-box-inner">
<span><p>The advancement of speech technologies has been remarkable, yet its
integration with African languages remains limited due to the scarcity of
African speech corpora. To address this issue, we present AfroDigits, a
minimalist, community-driven dataset of spoken digits for African languages,
currently covering 38 African languages. As a demonstration of the practical
applications of AfroDigits, we conduct audio digit classification experiments
on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo
(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R
models. Our experiments reveal a useful insight on the effect of mixing African
speech corpora during finetuning. AfroDigits is the first published audio digit
dataset for African languages and we believe it will, among other things, pave
the way for Afro-centric speech applications such as the recognition of
telephone numbers, and street numbers. We release the dataset and platform
publicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and
https://huggingface.co/spaces/chrisjay/afro-speech respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12665">
<div class="article-summary-box-inner">
<span><p>Considering a conversation thread, stance classification aims to identify the
opinion (e.g. agree or disagree) of replies towards a given target. The target
of the stance is expected to be an essential component in this task, being one
of the main factors that make it different from sentiment analysis. However, a
recent study shows that a target-oblivious model outperforms target-aware
models, suggesting that targets are not useful when predicting stance. This
paper re-examines this phenomenon for rumour stance classification (RSC) on
social media, where a target is a rumour story implied by the source tweet in
the conversation. We propose adversarial attacks in the test data, aiming to
assess the models robustness and evaluate the role of the data in the models
performance. Results show that state-of-the-art models, including approaches
that use the entire conversation thread, overly relying on superficial signals.
Our hypothesis is that the naturally high occurrence of target-independent
direct replies in RSC (e.g. "this is fake" or just "fake") results in the
impressive performance of target-oblivious models, highlighting the risk of
target instances being treated as noise during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12671">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a task that requires computers to give
correct answers for the input questions based on the images. This task can be
solved by humans with ease but is a challenge for computers. The
VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the
multilingual domain on a newly released dataset: UIT-EVJVQA, in which the
questions and answers are written in three different languages: English,
Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence
learning task, in which we integrated hints from pre-trained state-of-the-art
VQA models and image features with Convolutional Sequence-to-Sequence network
to generate the desired answers. Our results obtained up to 0.3442 by F1 score
on the public test set, 0.4210 on the private test set, and placed 3rd in the
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12712">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12734">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in self supervised training have led to a new class of
pretrained vision language models. While there have been investigations of bias
in multimodal models, they have mostly focused on gender and racial bias,
giving much less attention to other relevant groups, such as minorities with
regard to religion, nationality, sexual orientation, or disabilities. This is
mainly due to lack of suitable benchmarks for such groups. We seek to address
this gap by providing a visual and textual bias benchmark called MMBias,
consisting of around 3,800 images and phrases covering 14 population subgroups.
We utilize this dataset to assess bias in several prominent self supervised
multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these
models demonstrate meaningful bias favoring certain groups. Finally, we
introduce a debiasing method designed specifically for such large pre-trained
models that can be applied as a post-processing step to mitigate bias, while
preserving the remaining accuracy of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Trajectory and Vision Modalities for Verb Representation. (arXiv:2303.12737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12737">
<div class="article-summary-box-inner">
<span><p>Three-dimensional trajectories, or the 3D position and rotation of objects
over time, have been shown to encode key aspects of verb semantics (e.g., the
meanings of roll vs. slide). However, most multimodal models in NLP use 2D
images as representations of the world. Given the importance of 3D space in
formal models of verb semantics, we expect that these 2D images would result in
impoverished representations that fail to capture nuanced differences in
meaning. This paper tests this hypothesis directly in controlled experiments.
We train self-supervised image and trajectory encoders, and then evaluate them
on the extent to which each learns to differentiate verb concepts. Contrary to
our initial expectations, we find that 2D visual modalities perform similarly
well to 3D trajectories. While further work should be conducted on this
question, our initial findings challenge the conventional wisdom that richer
environment representations necessarily translate into better representation
learning for language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12767">
<div class="article-summary-box-inner">
<span><p>ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Bangla Sarcasm Detection using BERT and Explainable AI. (arXiv:2303.12772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12772">
<div class="article-summary-box-inner">
<span><p>A positive phrase or a sentence with an underlying negative motive is usually
defined as sarcasm that is widely used in today's social media platforms such
as Facebook, Twitter, Reddit, etc. In recent times active users in social media
platforms are increasing dramatically which raises the need for an automated
NLP-based system that can be utilized in various tasks such as determining
market demand, sentiment analysis, threat detection, etc. However, since
sarcasm usually implies the opposite meaning and its detection is frequently a
challenging issue, data meaning extraction through an NLP-based model becomes
more complicated. As a result, there has been a lot of study on sarcasm
detection in English over the past several years, and there's been a noticeable
improvement and yet sarcasm detection in the Bangla language's state remains
the same. In this article, we present a BERT-based system that can achieve
99.60\% while the utilized traditional machine learning algorithms are only
capable of achieving 89.93\%. Additionally, we have employed Local
Interpretable Model-Agnostic Explanations that introduce explainability to our
system. Moreover, we have utilized a newly collected bangla sarcasm dataset,
BanglaSarc that was constructed specifically for the evaluation of this study.
This dataset consists of fresh records of sarcastic and non-sarcastic comments,
the majority of which are acquired from Facebook and YouTube comment sections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12788">
<div class="article-summary-box-inner">
<span><p>While the state-of-the-art for frame semantic parsing has progressed
dramatically in recent years, it is still difficult for end-users to apply
state-of-the-art models in practice. To address this, we present Frame Semantic
Transformer, an open-source Python library which achieves near state-of-the-art
performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model
fine-tuned on Propbank and FrameNet exemplars as a base, and improve
performance by using FrameNet lexical units to provide hints to T5 at inference
time. We enhance robustness to real-world data by using textual data
augmentations during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00363">
<div class="article-summary-box-inner">
<span><p>Spatial relations are a basic part of human cognition. However, they are
expressed in natural language in a variety of ways, and previous work has
suggested that current vision-and-language models (VLMs) struggle to capture
relational information. In this paper, we present Visual Spatial Reasoning
(VSR), a dataset containing more than 10k natural text-image pairs with 66
types of spatial relations in English (such as: under, in front of, and
facing). While using a seemingly simple annotation format, we show how the
dataset includes challenging linguistic phenomena, such as varying reference
frames. We demonstrate a large gap between human and model performance: the
human ceiling is above 95%, while state-of-the-art models only achieve around
70%. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations concerning the orientations of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06807">
<div class="article-summary-box-inner">
<span><p>Ambiguity is a natural language phenomenon occurring at different levels of
syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,
for instance, we have a variety of competing studies for the human
disambiguation processes. These studies are empirical and based on eyetracking
measurements. Here we take first steps towards formalizing these processes for
semantic ambiguities where we identified the presence of two features: (1)
joint plausibility degrees of different possible interpretations, (2) causal
structures according to which certain words play a more substantial role in the
processes. The novel sheaf-theoretic model of definite causality developed by
Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these
features. We applied this theory to a dataset of ambiguous phrases extracted
from Psycholinguistics literature and their human plausibility judgements
collected by us using the Amazon Mechanical Turk engine. We measured the causal
fractions of different disambiguation orders within the phrases and discovered
two prominent orders: from subject to verb in the subject-verb and from object
to verb in the verb object phrases. We also found evidence for delay in the
disambiguation of polysemous vs homonymous verbs, again compatible with
Psycholinguistic findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03038">
<div class="article-summary-box-inner">
<span><p>This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Bias for Robust Visual Question Answering. (arXiv:2208.00690v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00690">
<div class="article-summary-box-inner">
<span><p>The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12764">
<div class="article-summary-box-inner">
<span><p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal
retrieval by tuning the backbone with additional heavy modules, which not only
brings huge computational burdens with much more parameters, but also leads to
the knowledge forgetting from upstream models. In this work, we propose the
VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the
text-video retrieval task. The proposed VoP is an end-to-end framework with
both video &amp; text prompts introducing, which can be regarded as a powerful
baseline with only 0.1% trainable parameters. Further, based on the
spatio-temporal characteristics of videos, we develop three novel video prompt
mechanisms to improve the performance with different scales of trainable
parameters. The basic idea of the VoP enhancement is to model the frame
position, frame context, and layer function with specific trainable prompts,
respectively. Extensive experiments show that compared to full fine-tuning, the
enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval
benchmarks with 6x less parameter overhead. The code will be available at
https://github.com/bighuang624/VoP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12824">
<div class="article-summary-box-inner">
<span><p>Generating a video given the first several static frames is challenging as it
anticipates reasonable future frames with temporal coherence. Besides video
prediction, the ability to rewind from the last frame or infilling between the
head and tail is also crucial, but they have rarely been explored for video
completion. Since there could be different outcomes from the hints of just a
few frames, a system that can follow natural language to perform video
completion may significantly improve controllability. Inspired by this, we
introduce a novel task, text-guided video completion (TVC), which requests the
model to generate a video from partial frames guided by an instruction. We then
propose Multimodal Masked Video Generation (MMVG) to address this TVC task.
During training, MMVG discretizes the video frames into visual tokens and masks
most of them to perform video completion from any time point. At inference
time, a single MMVG model can address all 3 cases of TVC, including video
prediction, rewind, and infilling, by applying corresponding masking
conditions. We evaluate MMVG in various video scenarios, including egocentric,
animation, and gaming. Extensive experimental results indicate that MMVG is
effective in generating high-quality visual appearances with text guidance for
TVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08518">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are popular for their impressive abilities, but
the need for model-specific fine-tuning or task-specific prompt engineering can
hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for
Improving zero-Shot Evaluation), which tunes a lightweight and versatile
retriever that automatically retrieves prompts for a given zero-shot task
input. Specifically, we demonstrate universality in a cross-task and
cross-model scenario: the retriever is tuned on a diverse set of tasks, but
tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for
tuning the retriever, but test the retriever on different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that
UPRISE mitigates the hallucination problem in our experiments with ChatGPT,
suggesting its potential to improve even the strongest LLMs. Our model and code
are available at https://github.com/microsoft/LMOps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09461">
<div class="article-summary-box-inner">
<span><p>We asked ChatGPT to participate in an undergraduate computer science exam on
''Algorithms and Data Structures''. The program was evaluated on the entire
exam as posed to the students. We hand-copied its answers onto an exam sheet,
which was subsequently graded in a blind setup alongside those of 200
participating students. We find that ChatGPT narrowly passed the exam,
obtaining 20.5 out of 40 points. This impressive performance indicates that
ChatGPT can indeed succeed in challenging tasks like university exams. At the
same time, the questions in our exam are structurally similar to those of other
exams, solved homework problems, and teaching materials that can be found
online and might have been part of ChatGPT's training data. Therefore, it would
be inadequate to conclude from this experiment that ChatGPT has any
understanding of computer science. We also assess the improvements brought by
GPT-4. We find that GPT-4 would have obtained about 17\% more exam points than
GPT-3.5, reaching the performance of the average student. The transcripts of
our conversations with ChatGPT are available at
\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire
graded exam is in the appendix of this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 event extraction from Twitter via extractive question answering with continuous prompts. (arXiv:2303.10659v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10659">
<div class="article-summary-box-inner">
<span><p>As COVID-19 ravages the world, social media analytics could augment
traditional surveys in assessing how the pandemic evolves and capturing
consumer chatter that could help healthcare agencies in addressing it. This
typically involves mining disclosure events that mention testing positive for
the disease or discussions surrounding perceptions and beliefs in preventative
or treatment options. The 2020 shared task on COVID-19 event extraction
(conducted as part of the W-NUT workshop during the EMNLP conference)
introduced a new Twitter dataset for benchmarking event extraction from
COVID-19 tweets. In this paper, we cast the problem of event extraction as
extractive question answering using recent advances in continuous prompting in
language models. On the shared task test dataset, our approach leads to over 5%
absolute micro-averaged F1-score improvement over prior best results, across
all COVID-19 event slots. Our ablation study shows that continuous prompts have
a major impact on the eventual performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10893">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code and model have been released
here~\footnote{https://github.com/xnliang98/MigBERT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11117">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation (ERC) has attracted growing attention in
recent years as a result of the advancement and implementation of
human-computer interface technologies. However, previous approaches to modeling
global and local context dependencies lost the diversity of dependency
information and do not take the context dependency into account at the
classification level. In this paper, we propose a novel approach to dependency
modeling driven by Emotional Inertia and Contagion (EmotionIC) for
conversational emotion recognition at the feature extraction and classification
levels. At the feature extraction level, our designed Identity Masked
Multi-head Attention (IM-MHA) captures the identity-based long-distant context
in the dialogue to contain the diverse influence of different participants and
construct the global emotional atmosphere, while the devised Dialogue-based
Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of
dyadic dialogue is applied to refine the contextual features with inter- and
intra-speaker dependencies. At the classification level, by introducing skip
connections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF
(SkipCRF) to capture the high-order dependencies within and between speakers,
and to emulate the emotional flow of distant participants. Experimental results
show that our method can significantly outperform the state-of-the-art models
on four benchmark datasets. The ablation studies confirm that our modules can
effectively model emotional inertia and contagion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11331">
<div class="article-summary-box-inner">
<span><p>We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open &amp; accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest &amp; best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12057">
<div class="article-summary-box-inner">
<span><p>The mass aggregation of knowledge embedded in large language models (LLMs)
holds the promise of new solutions to problems of observability and measurement
in the social sciences. We examine the utility of one such model for a
particularly difficult measurement task: measuring the latent ideology of
lawmakers, which allows us to better understand functions that are core to
democracy, such as how politics shape policy and how political actors represent
their constituents. We scale the senators of the 116th United States Congress
along the liberal-conservative spectrum by prompting ChatGPT to select the more
liberal (or conservative) senator in pairwise comparisons. We show that the LLM
produced stable answers across repeated iterations, did not hallucinate, and
was not simply regurgitating information from a single source. This new scale
strongly correlates with pre-existing liberal-conservative scales such as
NOMINATE, but also differs in several important ways, such as correctly placing
senators who vote against their party for far-left or far-right ideological
reasons on the extreme ends. The scale also highly correlates with ideological
measures based on campaign giving and political activists' perceptions of these
senators. In addition to the potential for better-automated data collection and
information retrieval, our results suggest LLMs are likely to open new avenues
for measuring latent constructs like ideology that rely on aggregating large
quantities of data from public sources.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-23 23:12:25.822799731 UTC">2023-03-23 23:12:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>