<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-04T02:30:00Z">11-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchies over Vector Space: Orienting Word and Graph Embeddings. (arXiv:2211.01430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01430">
<div class="article-summary-box-inner">
<span><p>Word and graph embeddings are widely used in deep learning applications. We
present a data structure that captures inherent hierarchical properties from an
unordered flat embedding space, particularly a sense of direction between pairs
of entities. Inspired by the notion of \textit{distributional generality}, our
algorithm constructs an arborescence (a directed rooted tree) by inserting
nodes in descending order of entity power (e.g., word frequency), pointing each
entity to the closest more powerful node as its parent.
</p>
<p>We evaluate the performance of the resulting tree structures on three tasks:
hypernym relation discovery, least-common-ancestor (LCA) discovery among words,
and Wikipedia page link recovery. We achieve average 8.98\% and 2.70\% for
hypernym and LCA discovery across five languages and 62.76\% accuracy on
directed Wiki-page link recovery, with both substantially above baselines.
Finally, we investigate the effect of insertion order, the power/similarity
trade-off and various power sources to optimize parent selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction. (arXiv:2211.01432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01432">
<div class="article-summary-box-inner">
<span><p>Bi-encoder architectures for distantly-supervised relation extraction are
designed to make use of the complementary information found in text and
knowledge graphs (KG). However, current architectures suffer from two
drawbacks. They either do not allow any sharing between the text encoder and
the KG encoder at all, or, in case of models with KG-to-text attention, only
share information in one direction. Here, we introduce cross-stitch
bi-encoders, which allow full interaction between the text encoder and the KG
encoder via a cross-stitch mechanism. The cross-stitch mechanism allows sharing
and updating representations between the two encoders at any layer, with the
amount of sharing being dynamically controlled via cross-attention-based gates.
Experimental results on two relation extraction benchmarks from two different
domains show that enabling full interaction between the two encoders yields
strong improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01438">
<div class="article-summary-box-inner">
<span><p>This work studies the use of attention masking in transformer transducer
based speech recognition for building a single configurable model for different
deployment scenarios. We present a comprehensive set of experiments comparing
fixed masking, where the same attention mask is applied at every frame, with
chunked masking, where the attention mask for each frame is determined by chunk
boundaries, in terms of recognition accuracy and latency. We then explore the
use of variable masking, where the attention masks are sampled from a target
distribution at training time, to build models that can work in different
configurations. Finally, we investigate how a single configurable model can be
used to perform both first pass streaming recognition and second pass acoustic
rescoring. Experiments show that chunked masking achieves a better accuracy vs
latency trade-off compared to fixed masking, both with and without FastEmit. We
also show that variable masking improves the accuracy by up to 8% relative in
the acoustic re-scoring scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-Shot Code-Switched Speech Recognition. (arXiv:2211.01458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01458">
<div class="article-summary-box-inner">
<span><p>In this work, we seek to build effective code-switched (CS) automatic speech
recognition systems (ASR) under the zero-shot setting where no transcribed CS
speech data is available for training. Previously proposed frameworks which
conditionally factorize the bilingual task into its constituent monolingual
parts are a promising starting point for leveraging monolingual data
efficiently. However, these methods require the monolingual modules to perform
language segmentation. That is, each monolingual module has to simultaneously
detect CS points and transcribe speech segments of one language while ignoring
those of other languages -- not a trivial task. We propose to simplify each
monolingual module by allowing them to transcribe all speech segments
indiscriminately with a monolingual script (i.e. transliteration). This simple
modification passes the responsibility of CS point detection to subsequent
bilingual modules which determine the final output by considering multiple
monolingual transliterations along with external language model information. We
apply this transliteration-based approach in an end-to-end differentiable
neural network and demonstrate its efficacy for zero-shot CS ASR on
Mandarin-English SEAME test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phoneme Segmentation Using Self-Supervised Speech Models. (arXiv:2211.01461v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01461">
<div class="article-summary-box-inner">
<span><p>We apply transfer learning to the task of phoneme segmentation and
demonstrate the utility of representations learned in self-supervised
pre-training for the task. Our model extends transformer-style encoders with
strategically placed convolutions that manipulate features learned in
pre-training. Using the TIMIT and Buckeye corpora we train and test the model
in the supervised and unsupervised settings. The latter case is accomplished by
furnishing a noisy label-set with the predictions of a separate model, it
having been trained in an unsupervised fashion. Results indicate our model
eclipses previous state-of-the-art performance in both settings and on both
datasets. Finally, following observations during published code review and
attempts to reproduce past segmentation results, we find a need to disambiguate
the definition and implementation of widely-used evaluation metrics. We resolve
this ambiguity by delineating two distinct evaluation schemes and describing
their nuances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation. (arXiv:2211.01467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01467">
<div class="article-summary-box-inner">
<span><p>Stance detection is typically framed as predicting the sentiment in a given
text towards a target entity. However, this setup overlooks the importance of
the source entity, i.e., who is expressing the opinion. In this paper, we
emphasize the need for studying interactions among entities when inferring
stances. We first introduce a new task, entity-to-entity (E2E) stance
detection, which primes models to identify entities in their canonical names
and discern stances jointly. To support this study, we curate a new dataset
with 10,619 annotations labeled at the sentence-level from news articles of
different ideological leanings. We present a novel generative framework to
allow the generation of canonical names for entities as well as stances among
them. We further enhance the model with a graph encoder to summarize entity
activities and external knowledge surrounding the entities. Experiments show
that our model outperforms strong comparisons by large margins. Further
analyses demonstrate the usefulness of E2E stance detection for understanding
media quotation and stance landscape, as well as inferring entity ideology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Over-communicate no more: Situated RL agents learn concise communication protocols. (arXiv:2211.01480v1 [cs.MA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01480">
<div class="article-summary-box-inner">
<span><p>While it is known that communication facilitates cooperation in multi-agent
settings, it is unclear how to design artificial agents that can learn to
effectively and efficiently communicate with each other. Much research on
communication emergence uses reinforcement learning (RL) and explores
unsituated communication in one-step referential tasks -- the tasks are not
temporally interactive and lack time pressures typically present in natural
communication. In these settings, agents may successfully learn to communicate,
but they do not learn to exchange information concisely -- they tend towards
over-communication and an inefficient encoding. Here, we explore situated
communication in a multi-step task, where the acting agent has to forgo an
environmental action to communicate. Thus, we impose an opportunity cost on
communication and mimic the real-world pressure of passing time. We compare
communication emergence under this pressure against learning to communicate
with a cost on articulation effort, implemented as a per-message penalty (fixed
and progressively increasing). We find that while all tested pressures can
disincentivise over-communication, situated communication does it most
effectively and, unlike the cost on effort, does not negatively impact
emergence. Implementing an opportunity cost on communication in a temporally
extended environment is a step towards embodiment, and might be a pre-condition
for incentivising efficient, human-like communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01482">
<div class="article-summary-box-inner">
<span><p>Existing metrics for evaluating the quality of automatically generated
questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
predicted questions, providing a high score when there is a considerable
lexical overlap or semantic similarity between the candidate and the reference
questions. This approach has two major shortcomings. First, we need expensive
human-provided reference questions. Second, it penalises valid questions that
may not have high lexical or semantic similarity to the reference questions. In
this paper, we propose a new metric, RQUGE, based on the answerability of the
candidate question given the context. The metric consists of a
question-answering and a span scorer module, in which we use pre-trained models
from the existing literature, and therefore, our metric can be used without
further training. We show that RQUGE has a higher correlation with human
judgment without relying on the reference question. RQUGE is shown to be
significantly more robust to several adversarial corruptions. Additionally, we
illustrate that we can significantly improve the performance of QA models on
out-of-domain datasets by fine-tuning on the synthetic data generated by a
question generation model and re-ranked by RQUGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Resource-Performance Trade-off of Natural Language Models using Data Envelopment Analysis. (arXiv:2211.01486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01486">
<div class="article-summary-box-inner">
<span><p>Natural language models are often summarized through a high-dimensional set
of descriptive metrics including training corpus size, training time, the
number of trainable parameters, inference times, and evaluation statistics that
assess performance across tasks. The high dimensional nature of these metrics
yields challenges with regard to objectively comparing models; in particular it
is challenging to assess the trade-off models make between performance and
resources (compute time, memory, etc.).
</p>
<p>We apply Data Envelopment Analysis (DEA) to this problem of assessing the
resource-performance trade-off. DEA is a nonparametric method that measures
productive efficiency of abstract units that consume one or more inputs and
yield at least one output. We recast natural language models as units suitable
for DEA, and we show that DEA can be used to create an effective framework for
quantifying model performance and efficiency. A central feature of DEA is that
it identifies a subset of models that live on an efficient frontier of
performance. DEA is also scalable, having been applied to problems with
thousands of units. We report empirical results of DEA applied to 14 different
language models that have a variety of architectures, and we show that DEA can
be used to identify a subset of models that effectively balance resource
demands against performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01515">
<div class="article-summary-box-inner">
<span><p>We present Multiscale Audio Spectrogram Transformer (MAST) for audio
classification, which brings the concept of multiscale feature hierarchies to
the Audio Spectrogram Transformer (AST). Given an input audio spectrogram we
first patchify and project it into an initial temporal resolution and embedding
dimension, post which the multiple stages in MAST progressively expand the
embedding dimension while reducing the temporal resolution of the input. We use
a pyramid structure that allows early layers of MAST operating at a high
temporal resolution but low embedding space to model simple low-level acoustic
information and deeper temporally coarse layers to model high-level acoustic
information with high-dimensional embeddings. We also extend our approach to
present a new Self-Supervised Learning (SSL) method called SS-MAST, which
calculates a symmetric contrastive loss between latent representations from a
student and a teacher encoder. In practice, MAST significantly outperforms AST
by an average accuracy of 3.4% across 8 speech and non-speech tasks from the
LAPE Benchmark. Moreover, SS-MAST achieves an absolute average improvement of
2.6% over SSAST for both AST and MAST encoders. We make all our codes available
on GitHub at the time of publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01519">
<div class="article-summary-box-inner">
<span><p>We present a new Self-Supervised Learning (SSL) approach to pre-train
encoders on unlabeled audio data that reduces the need for large amounts of
labeled data for audio and speech classification. Our primary aim is to learn
audio representations that can generalize across a large variety of speech and
non-speech tasks in a low-resource un-labeled audio pre-training setting.
Inspired by the recent success of clustering and contrasting learning paradigms
for SSL-based speech representation learning, we propose SLICER (Symmetrical
Learning of Instance and Cluster-level Efficient Representations), which brings
together the best of both clustering and contrasting learning paradigms. We use
a symmetric loss between latent representations from student and teacher
encoders and simultaneously solve instance and cluster-level contrastive
learning tasks. We obtain cluster representations online by just projecting the
input spectrogram into an output subspace with dimensions equal to the number
of clusters. In addition, we propose a novel mel-spectrogram augmentation
procedure, k-mix, based on mixup, which does not require labels and aids
unsupervised representation learning for audio. Overall, SLICER achieves
state-of-the-art results on the LAPE Benchmark \cite{9868132}, significantly
outperforming DeLoRes-M and other prior approaches, which are pre-trained on
$10\times$ larger of unsupervised data. We will make all our codes available on
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions. (arXiv:2211.01542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01542">
<div class="article-summary-box-inner">
<span><p>This paper considers continual learning of large-scale pretrained neural
machine translation model without accessing the previous training data or
introducing model separation. We argue that the widely used
regularization-based methods, which perform multi-objective learning with an
auxiliary loss, suffer from the misestimate problem and cannot always achieve a
good balance between the previous and new tasks. To solve the problem, we
propose a two-stage training method based on the local features of the real
loss. We first search low forgetting risk regions, where the model can retain
the performance on the previous task as the parameters are updated, to avoid
the catastrophic forgetting problem. Then we can continually train the model
within this region only with the new training data to fit the new task.
Specifically, we propose two methods to search the low forgetting risk regions,
which are based on the curvature of loss and the impacts of the parameters on
the model output, respectively. We conduct experiments on domain adaptation and
more challenging language adaptation tasks, and the experimental results show
that our method can achieve significant improvements compared with several
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01562">
<div class="article-summary-box-inner">
<span><p>Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01568">
<div class="article-summary-box-inner">
<span><p>Large language models are now part of a powerful new paradigm in machine
learning. These models learn a wide range of capabilities from training on
large unsupervised text corpora. In many applications, these capabilities are
then fine-tuned through additional training on specialized data to improve
performance in that setting. In this paper, we augment these models with an
epinet: a small additional network architecture that helps to estimate model
uncertainty and form an epistemic neural network (ENN). ENNs are neural
networks that can know what they don't know. We show that, using an epinet to
prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same
performance while using 2x less data. We also investigate performance in
synthetic neural network generative models designed to build understanding. In
each setting, using an epinet outperforms heuristic active learning schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary Argument Role Prediction for Event Extraction. (arXiv:2211.01577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01577">
<div class="article-summary-box-inner">
<span><p>The argument role in event extraction refers to the relation between an event
and an argument participating in it. Despite the great progress in event
extraction, existing studies still depend on roles pre-defined by domain
experts. These studies expose obvious weakness when extending to emerging event
types or new domains without available roles. Therefore, more attention and
effort needs to be devoted to automatically customizing argument roles. In this
paper, we define this essential but under-explored task: open-vocabulary
argument role prediction. The goal of this task is to infer a set of argument
roles for a given event type. We propose a novel unsupervised framework,
RolePred for this task. Specifically, we formulate the role prediction problem
as an in-filling task and construct prompts for a pre-trained language model to
generate candidate roles. By extracting and analyzing the candidate arguments,
the event-specific roles are further merged and selected. To standardize the
research of this task, we collect a new event extraction dataset from
WikiPpedia including 142 customized argument roles with rich semantics. On this
dataset, RolePred outperforms the existing methods by a large margin. Source
code and dataset are available on our GitHub repository:
https://github.com/yzjiao/RolePred
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation. (arXiv:2211.01587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01587">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale pre-training provide large models with the
potential to learn knowledge from the raw text. It is thus natural to ask
whether it is possible to leverage these large models as knowledge bases for
downstream tasks. In this work, we answer the aforementioned question in
unsupervised knowledge-grounded conversation. We explore various methods that
best elicit knowledge from large models. Our human study indicates that, though
hallucinations exist, large models post the unique advantage of being able to
output common sense and summarize facts that cannot be directly retrieved from
the search engine. To better exploit such generated knowledge in dialogue
generation, we treat the generated knowledge as a noisy knowledge source and
propose the posterior-based reweighing as well as the noisy training strategy.
Empirical results on two benchmarks show advantages over the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction. (arXiv:2211.01625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01625">
<div class="article-summary-box-inner">
<span><p>Chinese Grammatical Error Correction (CGEC) aims to generate a correct
sentence from an erroneous sequence, where different kinds of errors are mixed.
This paper divides the CGEC task into two steps, namely spelling error
correction and grammatical error correction. Specifically, we propose a novel
zero-shot approach for spelling error correction, which is simple but
effective, obtaining a high precision to avoid error accumulation of the
pipeline structure. To handle grammatical error correction, we design
part-of-speech (POS) features and semantic class features to enhance the neural
network model, and propose an auxiliary task to predict the POS sequence of the
target sentence. Our proposed framework achieves a 42.11 F0.5 score on CGEC
dataset without using any synthetic data or data augmentation methods, which
outperforms the previous state-of-the-art by a wide margin of 1.30 points.
Moreover, our model produces meaningful POS representations that capture
different POS words and convey reasonable POS transition rules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Grammatical Error Correction Evaluation and Beyond. (arXiv:2211.01635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01635">
<div class="article-summary-box-inner">
<span><p>Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Chinese Word Segmentation and Span-based Constituency Parsing. (arXiv:2211.01638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01638">
<div class="article-summary-box-inner">
<span><p>In constituency parsing, span-based decoding is an important direction.
However, for Chinese sentences, because of their linguistic characteristics, it
is necessary to utilize other models to perform word segmentation first, which
introduces a series of uncertainties and generally leads to errors in the
computation of the constituency tree afterward. This work proposes a method for
joint Chinese word segmentation and Span-based Constituency Parsing by adding
extra labels to individual Chinese characters on the parse trees. Through
experiments, the proposed algorithm outperforms the recent models for joint
segmentation and constituency parsing on CTB 5.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively. (arXiv:2211.01642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01642">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have achieved impressive results on a
wide range of downstream tasks recently. However, fine-tuning an extremely
large-scale pre-trained language model on limited target datasets is often
plagued by overfitting and representation degradation. In this paper, we
propose a Dynamic Parameter Selection (DPS) algorithm for the large-scale
pre-trained models during fine-tuning, which adaptively selects a more
promising subnetwork to perform staging updates based on gradients of
back-propagation. Experiments on the GLUE benchmark show that DPS outperforms
previous fine-tuning methods in terms of overall performance and stability, and
consistently achieves better results with variable pre-trained language models.
In addition, DPS brings a large magnitude of improvement in out-of-domain
transferring experiments and low-resource scenarios, which shows that it can
maintain stable general contextual features and reduce the representation
collapse. We release our code at https://github.com/ZhangHaojie077/DPS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spam Review Detection Using Deep Learning. (arXiv:2211.01675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01675">
<div class="article-summary-box-inner">
<span><p>A robust and reliable system of detecting spam reviews is a crying need in
todays world in order to purchase products without being cheated from online
sites. In many online sites, there are options for posting reviews, and thus
creating scopes for fake paid reviews or untruthful reviews. These concocted
reviews can mislead the general public and put them in a perplexity whether to
believe the review or not. Prominent machine learning techniques have been
introduced to solve the problem of spam review detection. The majority of
current research has concentrated on supervised learning methods, which require
labeled data - an inadequacy when it comes to online review. Our focus in this
article is to detect any deceptive text reviews. In order to achieve that we
have worked with both labeled and unlabeled data and proposed deep learning
methods for spam review detection which includes Multi-Layer Perceptron (MLP),
Convolutional Neural Network (CNN) and a variant of Recurrent Neural Network
(RNN) that is Long Short-Term Memory (LSTM). We have also applied some
traditional machine learning classifiers such as Nave Bayes (NB), K Nearest
Neighbor (KNN) and Support Vector Machine (SVM) to detect spam reviews and
finally, we have shown the performance comparison for both traditional and deep
learning classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-efficient End-to-end Information Extraction for Statistical Legal Analysis. (arXiv:2211.01692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01692">
<div class="article-summary-box-inner">
<span><p>Legal practitioners often face a vast amount of documents. Lawyers, for
instance, search for appropriate precedents favorable to their clients, while
the number of legal precedents is ever-growing. Although legal search engines
can assist finding individual target documents and narrowing down the number of
candidates, retrieved information is often presented as unstructured text and
users have to examine each document thoroughly which could lead to information
overloading. This also makes their statistical analysis challenging. Here, we
present an end-to-end information extraction (IE) system for legal documents.
By formulating IE as a generation task, our system can be easily applied to
various tasks without domain-specific engineering effort. The experimental
results of four IE tasks on Korean precedents shows that our IE system can
achieve competent scores (-2.3 on average) compared to the rule-based baseline
with as few as 50 training examples per task and higher score (+5.4 on average)
with 200 examples. Finally, our statistical analysis on two case
categories--drunk driving and fraud--with 35k precedents reveals the resulting
structured information from our IE system faithfully reflects the macroscopic
features of Korean legal system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A speech corpus for chronic kidney disease. (arXiv:2211.01705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01705">
<div class="article-summary-box-inner">
<span><p>In this study, we present a speech corpus of patients with chronic kidney
disease (CKD) that will be used for research on pathological voice analysis,
automatic illness identification, and severity prediction. This paper
introduces the steps involved in creating this corpus, including the choice of
speech-related parameters and speech lists as well as the recording technique.
The speakers in this corpus, 289 CKD patients with varying degrees of severity
who were categorized based on estimated glomerular filtration rate (eGFR),
delivered sustained vowels, sentence, and paragraph stimuli. This study
compared and analyzed the voice characteristics of CKD patients with those of
the control group; the results revealed differences in voice quality,
phoneme-level pronunciation, prosody, glottal source, and aerodynamic
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid-SD ($\text{H}_{\text{SD}}$) : A new hybrid evaluation metric for automatic speech recognition tasks. (arXiv:2211.01722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01722">
<div class="article-summary-box-inner">
<span><p>Many studies have examined the shortcomings of word error rate (WER) as an
evaluation metric for automatic speech recognition (ASR) systems, particularly
when used for spoken language understanding tasks such as intent recognition
and dialogue systems. In this paper, we propose Hybrid-SD
($\text{H}_{\text{SD}}$), a new hybrid evaluation metric for ASR systems that
takes into account both semantic correctness and error rate. To generate
sentence dissimilarity scores (SD), we built a fast and lightweight SNanoBERT
model using distillation techniques. Our experiments show that the SNanoBERT
model is 25.9x smaller and 38.8x faster than SRoBERTa while achieving
comparable results on well-known benchmarks. Hence, making it suitable for
deploying with ASR models on edge devices. We also show that
$\text{H}_{\text{SD}}$ correlates more strongly with downstream tasks such as
intent recognition and named-entity recognition (NER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the State-of-the-Art Language Modeling Methods and Data Augmentation Techniques for Multilingual Clause-Level Morphology. (arXiv:2211.01736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01736">
<div class="article-summary-box-inner">
<span><p>This paper describes the KUIS-AI NLP team's submission for the 1$^{st}$
Shared Task on Multilingual Clause-level Morphology (MRL2022). We present our
work on all three parts of the shared task: inflection, reinflection, and
analysis. We mainly explore two approaches: Transformer models in combination
with data augmentation, and exploiting the state-of-the-art language modeling
techniques for morphological analysis. Data augmentation leads a remarkable
performance improvement for most of the languages in the inflection task.
Prefix-tuning on pretrained mGPT model helps us to adapt reinflection and
analysis tasks in a low-data setting. Additionally, we used pipeline
architectures using publicly available open source lemmatization tools and
monolingual BERT-based morphological feature classifiers for reinflection and
analysis tasks, respectively. While Transformer architectures with data
augmentation and pipeline architectures achieved the best results for
inflection and reinflection tasks, pipelines and prefix-tuning on mGPT received
the highest results for the analysis task. Our methods achieved first place in
each of the three tasks and outperforms mT5-baseline with ~89\% for inflection,
~80\% for reinflection and ~12\% for analysis. Our code
https://github.com/emrecanacikgoz/mrl2022 is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning. (arXiv:2211.01761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01761">
<div class="article-summary-box-inner">
<span><p>Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is
challenging due to privacy concerns, which hinders the use of ML for healthcare
applications. Synthetic EHRs generation bypasses the need to share sensitive
real patient records. However, existing methods generate single-modal EHRs by
unconditional generation or by longitudinal inference, which falls short of low
flexibility and makes unrealistic EHRs. In this work, we propose to formulate
EHRs generation as a text-to-text translation task by language models (LMs),
which suffices to highly flexible event imputation during generation. We also
design prompt learning to control the generation conditioned by numerical and
categorical demographic features. We evaluate synthetic EHRs quality by two
perplexity measures accounting for their longitudinal pattern (longitudinal
imputation perplexity, lpl) and the connections cross modalities
(cross-modality imputation perplexity, mpl). Moreover, we utilize two
adversaries: membership and attribute inference attacks for privacy-preserving
evaluation. Experiments on MIMIC-III data demonstrate the superiority of our
methods on realistic EHRs generation (53.1\% decrease of lpl and 45.3\%
decrease of mpl on average compared to the best baselines) with low privacy
risks. Software is available at https://github.com/RyanWangZf/PromptEHR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Event Extraction via Tracking Visual States of Arguments. (arXiv:2211.01781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01781">
<div class="article-summary-box-inner">
<span><p>Video event extraction aims to detect salient events from a video and
identify the arguments for each event as well as their semantic roles. Existing
methods focus on capturing the overall visual scene of each frame, ignoring
fine-grained argument-level information. Inspired by the definition of events
as changes of states, we propose a novel framework to detect video events by
tracking the changes in the visual states of all involved arguments, which are
expected to provide the most informative evidence for the extraction of video
events. In order to capture the visual state changes of arguments, we decompose
them into changes in pixels within objects, displacements of objects, and
interactions among multiple arguments. We further propose Object State
Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to
encode and track these changes respectively. Experiments on various video event
extraction tasks demonstrate significant improvements compared to
state-of-the-art models. In particular, on verb classification, we achieve
3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation
Recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01786">
<div class="article-summary-box-inner">
<span><p>Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are publicly
available at https://github.com/bigscience-workshop/xmtf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-based Instance Discrimination Network for Relational Triple Extraction. (arXiv:2211.01797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01797">
<div class="article-summary-box-inner">
<span><p>Joint entity and relation extraction has been a core task in the field of
information extraction. Recent approaches usually consider the extraction of
relational triples from a stereoscopic perspective, either learning a
relation-specific tagger or separate classifiers for each relation type.
However, they still suffer from error propagation, relation redundancy and lack
of high-level connections between triples. To address these issues, we propose
a novel query-based approach to construct instance-level representations for
relational triples. By metric-based comparison between query embeddings and
token embeddings, we can extract all types of triples in one step, thus
eliminating the error propagation problem. In addition, we learn the
instance-level representation of relational triples via contrastive learning.
In this way, relational triples can not only enclose rich class-level semantics
but also access to high-order global connections. Experimental results show
that our proposed method achieves the state of the art on five widely used
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human in the loop approaches in multi-modal conversational task guidance system development. (arXiv:2211.01824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01824">
<div class="article-summary-box-inner">
<span><p>Development of task guidance systems for aiding humans in a situated task
remains a challenging problem. The role of search (information retrieval) and
conversational systems for task guidance has immense potential to help the task
performers achieve various goals. However, there are several technical
challenges that need to be addressed to deliver such conversational systems,
where common supervised approaches fail to deliver the expected results in
terms of overall performance, user experience and adaptation to realistic
conditions. In this preliminary work we first highlight some of the challenges
involved during the development of such systems. We then provide an overview of
existing datasets available and highlight their limitations. We finally develop
a model-in-the-loop wizard-of-oz based data collection tool and perform a pilot
experiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Prompt Tuning for Text Summarization. (arXiv:2211.01837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01837">
<div class="article-summary-box-inner">
<span><p>Prompts with different control signals (e.g., length, keywords, etc.) can be
used to control text summarization. When control signals are available, they
can control the properties of generated summaries and potentially improve
summarization quality (since more information are given). Unfortunately,
control signals are not already available during inference time. In this paper,
we propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which
is a single model that can be applied in both controlled and uncontrolled
(without control signals) modes. During training, Lotus learns latent prompt
representations from prompts with gold control signals using a contrastive
learning objective. Experiments show Lotus in uncontrolled mode consistently
improves upon strong (uncontrollable) summarization models across four
different summarization datasets. We also demonstrate generated summaries can
be controlled using prompts with user specified control tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Circling Back to Recurrent Models of Language. (arXiv:2211.01848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01848">
<div class="article-summary-box-inner">
<span><p>Just because some purely recurrent models suffer from being hard to optimize
and inefficient on today's hardware, they are not necessarily bad models of
language. We demonstrate this by the extent to which these models can still be
improved by a combination of a slightly better recurrent cell, architecture,
objective, as well as optimization. In the process, we establish a new state of
the art for language modelling on small datasets and on enwik8 with dynamic
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual information integration for stance detection via cross-attention. (arXiv:2211.01874v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01874">
<div class="article-summary-box-inner">
<span><p>Stance detection deals with the identification of an author's stance towards
a target and is applied on various text domains like social media and news. In
many cases, inferring the stance is challenging due to insufficient access to
contextual information. Complementary context can be found in knowledge bases
but integrating the context into pretrained language models is non-trivial due
to their graph structure. In contrast, we explore an approach to integrate
contextual information as text which aligns better with transformer
architectures. Specifically, we train a model consisting of dual encoders which
exchange information via cross-attention. This architecture allows for
integrating contextual information from heterogeneous sources. We evaluate
context extracted from structured knowledge sources and from prompting large
language models. Our approach is able to outperform competitive baselines
(1.9pp on average) on a large and diverse stance detection benchmark, both (1)
in-domain, i.e. for seen targets, and (2) out-of-domain, i.e. for targets
unseen during training. Our analysis shows that it is able to regularize for
spurious label correlations with target-specific cue words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and its Intensity. (arXiv:2211.01889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01889">
<div class="article-summary-box-inner">
<span><p>Prerecorded laughter accompanying dialog in comedy TV shows encourages the
audience to laugh by clearly marking humorous moments in the show. We present
an approach for automatically detecting humor in the Friends TV show using
multimodal data. Our model is capable of recognizing whether an utterance is
humorous or not and assess the intensity of it. We use the prerecorded laughter
in the show as annotation as it marks humor and the length of the audience's
laughter tells us how funny a given joke is. We evaluate the model on episodes
the model has not been exposed to during the training phase. Our results show
that the model is capable of correctly detecting whether an utterance is
humorous 78% of the time and how long the audience's laughter reaction should
last with a mean absolute error of 600 milliseconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Human-Level Prompt Engineers. (arXiv:2211.01910v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01910">
<div class="article-summary-box-inner">
<span><p>By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the "program," optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Trained Mongolian Text-to-Speech System Based On FullConv. (arXiv:2211.01948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01948">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks (RNNs) have become the standard modeling technique
for sequence data, and are used in a number of novel text-to-speech models.
However, training a TTS model including RNN components has certain requirements
for GPU performance and takes a long time. In contrast, studies have shown that
CNN-based sequence synthesis technology can greatly reduce training time in
text-to-speech models while ensuring a certain performance due to its high
parallelism. We propose a new text-to-speech system based on deep convolutional
neural networks that does not employ any RNN components (recurrent units). At
the same time, we improve the generality and robustness of our model through a
series of data augmentation methods such as Time Warping, Frequency Mask, and
Time Mask. The final experimental results show that the TTS model using only
the CNN component can reduce the training time compared to the classic TTS
models such as Tacotron while ensuring the quality of the synthesized speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A BERT-based Deep Learning Approach for Reputation Analysis in Social Media. (arXiv:2211.01954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01954">
<div class="article-summary-box-inner">
<span><p>Social media has become an essential part of the modern lifestyle, with its
usage being highly prevalent. This has resulted in unprecedented amounts of
data generated from users in social media, such as users' attitudes, opinions,
interests, purchases, and activities across various aspects of their lives.
Therefore, in a world of social media, where its power has shifted to users,
actions taken by companies and public figures are subject to constantly being
under scrutiny by influential global audiences. As a result, reputation
management in social media has become essential as companies and public figures
need to maintain their reputation to preserve their reputation capital.
However, domain experts still face the challenge of lacking appropriate
solutions to automate reliable online reputation analysis. To tackle this
challenge, we proposed a novel reputation analysis approach based on the
popular language model BERT (Bidirectional Encoder Representations from
Transformers). The proposed approach was evaluated on the reputational polarity
task using RepLab 2013 dataset. Compared to previous works, we achieved 5.8%
improvement in accuracy, 26.9% improvement in balanced accuracy, and 21.8%
improvement in terms of F-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis. (arXiv:2211.01964v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01964">
<div class="article-summary-box-inner">
<span><p>Embedding paralinguistic properties is a challenging task as there are only a
few hours of training data available for domains such as emotional speech. One
solution to this problem is to pretrain a general self-supervised speech
representation model on large amounts of unlabeled speech. This pretrained
model is then finetuned to a specific task. Paralinguistic properties however
have notoriously high class variance, making the finetuning ineffective. In
this work, we propose a two step approach to this. First we improve the
embedding space, then we train an adapter to bridge the gap from the embedding
space to a classification task. In order to improve the class invariance we use
a combination of contrastive and non-contrastive losses to explicitly optimize
for class invariant, yet discriminative features. Our approach consistently
outperforms baselines that are finetuned end-to-end on multiple tasks and
surpasses a benchmark on state-of-the-art emotion classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. (arXiv:2211.01979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01979">
<div class="article-summary-box-inner">
<span><p>Adapter-tuning is a paradigm that transfers a pretrained language model to
downstream tasks by adding and tuning a small number of new parameters.
Previously proposed adapter architectures are all feed-forward neural networks.
In this paper, we investigate the effectiveness of using tiny-attention --
i.e., attention with extremely small per-head dimensionality -- as adapters.
Our tiny-attention adapter learns to modify the hidden states at each position
directly conditioned on the hidden states at all the other positions, which is
missed by the previously proposed adapters. Moreover, we view its multiple
attention heads as a mixture of experts and propose to average their weights
during deployment, which further reduces its inference computation cost. On the
GLUE benchmark, our tiny-attention adapter outperforms the other
parameter-efficient transfer learning methods as well as full fine-tuning while
only updating 0.05% of the parameters. On the FewGLUE benchmark, its
performance is comparable to that of GPT-3 and PET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation. (arXiv:2211.01981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01981">
<div class="article-summary-box-inner">
<span><p>Topic taxonomies display hierarchical topic structures of a text corpus and
provide topical knowledge to enhance various NLP applications. To dynamically
incorporate new topic information, several recent studies have tried to expand
(or complete) a topic taxonomy by inserting emerging topics identified in a set
of new documents. However, existing methods focus only on frequent terms in
documents and the local topic-subtopic relations in a taxonomy, which leads to
limited topic term coverage and fails to model the global topic hierarchy. In
this work, we propose a novel framework for topic taxonomy expansion, named
TopicExpan, which directly generates topic-related terms belonging to new
topics. Specifically, TopicExpan leverages the hierarchical relation structure
surrounding a new topic and the textual content of an input document for topic
term generation. This approach encourages newly-inserted topics to further
cover important but less frequent terms as well as to keep their relation
consistency within the taxonomy. Experimental results on two real-world text
corpora show that TopicExpan significantly outperforms other baseline methods
in terms of the quality of output taxonomies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Statistical Representations For End-To-End ASR. (arXiv:2211.01993v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01993">
<div class="article-summary-box-inner">
<span><p>End-to-End automatic speech recognition (ASR) models aim to learn a
generalised speech representation to perform recognition. In this domain there
is little research to analyse internal representation dependencies and their
relationship to modelling approaches. This paper investigates cross-domain
language model dependencies within transformer architectures using SVCCA and
uses these insights to exploit modelling approaches. It was found that specific
neural representations within the transformer layers exhibit correlated
behaviour which impacts recognition performance.
</p>
<p>Altogether, this work provides analysis of the modelling approaches affecting
contextual dependencies and ASR performance, and can be used to create or adapt
better performing End-to-End ASR models and also for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01994">
<div class="article-summary-box-inner">
<span><p>We present lilGym, a new benchmark for language-conditioned reinforcement
learning in visual environments. lilGym is based on 2,661 highly-compositional
human-written natural language statements grounded in an interactive visual
environment. We annotate all statements with executable Python programs
representing their meaning to enable exact reward computation in every possible
world state. Each statement is paired with multiple start states and reward
functions to form thousands of distinct Markov Decision Processes of varying
difficulty. We experiment with lilGym with different models and learning
regimes. Our results and analysis show that while existing methods are able to
achieve non-trivial performance, lilGym forms a challenging open problem.
lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Kernels and Channel Attention with Multi-Layer Embedding Aggregation for Speaker Verification. (arXiv:2211.02000v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02000">
<div class="article-summary-box-inner">
<span><p>State-of-the-art speaker verification frameworks have typically focused on
speech enhancement techniques with increasingly deeper (more layers) and wider
(number of channels) models to improve their verification performance. Instead,
this paper proposes an approach to increase the model resolution capability
using attention-based dynamic kernels in a convolutional neural network to
adapt the model parameters to be feature-conditioned. The attention weights on
the kernels are further distilled by channel attention and multi-layer feature
aggregation to learn global features from speech. This approach provides an
efficient solution to improving representation capacity with lower data
resources. This is due to the self-adaptation to inputs of the structures of
the model parameters. The proposed dynamic convolutional model achieved 1.62\%
EER and 0.18 miniDCF on the VoxCeleb1 test set and has a 17\% relative
improvement compared to the ECAPA-TDNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse scaling can become U-shaped. (arXiv:2211.02011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02011">
<div class="article-summary-box-inner">
<span><p>Although scaling language models improves performance on a range of tasks,
there are apparently some scenarios where scaling hurts performance. For
instance, the Inverse Scaling Prize Round 1 identified four ''inverse scaling''
tasks, for which performance gets worse for larger models. These tasks were
evaluated on models of up to 280B parameters, trained up to 500 zettaFLOPs of
compute.
</p>
<p>This paper takes a closer look at these four tasks. We evaluate models of up
to 540B parameters, trained on five times more compute than those evaluated in
the Inverse Scaling Prize. With this increased range of model sizes and
training compute, three out of the four tasks exhibit what we call ''U-shaped
scaling'' -- performance decreases up to a certain model size, and then
increases again up to the largest model evaluated. One hypothesis is that
U-shaped scaling occurs when a task comprises a ''true task'' and a
''distractor task''. Medium-size models can do the distractor task, which hurts
performance, while only large-enough models can ignore the distractor task and
do the true task. The existence of U-shaped scaling implies that inverse
scaling may not hold for larger models.
</p>
<p>Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT)
prompting, in addition to basic prompting without CoT. With CoT prompting, all
four tasks show either U-shaped scaling or positive scaling, achieving perfect
solve rates on two tasks and several sub-tasks. This suggests that the term
"inverse scaling task" is under-specified -- a given task may be inverse
scaling for one prompt but positive or U-shaped scaling for a different prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space. (arXiv:1909.08191v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.08191">
<div class="article-summary-box-inner">
<span><p>The trends of open science have enabled several open scholarly datasets which
include millions of papers and authors. Managing, exploring, and utilizing such
large and complicated datasets effectively are challenging. In recent years,
the knowledge graph has emerged as a universal data format for representing
knowledge about heterogeneous entities and their relationships. The knowledge
graph can be modeled by knowledge graph embedding methods, which represent
entities and relations as embedding vectors in semantic space, then model the
interactions between these embedding vectors. However, the semantic structures
in the knowledge graph embedding space are not well-studied, thus knowledge
graph embedding methods are usually only used for knowledge graph completion
but not data representation and analysis. In this paper, we propose to analyze
these semantic structures based on the well-studied word embedding space and
use them to support data exploration. We also define the semantic queries,
which are algebraic operations between the embedding vectors in the knowledge
graph embedding space, to solve queries such as similarity and analogy between
the entities on the original datasets. We then design a general framework for
data exploration by semantic queries and discuss the solution to some
traditional scholarly data exploration tasks. We also propose some new
interesting tasks that can be solved based on the uncanny semantic structures
of the embedding space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Lexical Complexity in English Texts: The Complex 2.0 Dataset. (arXiv:2102.08773v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08773">
<div class="article-summary-box-inner">
<span><p>Identifying words which may cause difficulty for a reader is an essential
step in most lexical text simplification systems prior to lexical substitution
and can also be used for assessing the readability of a text. This task is
commonly referred to as Complex Word Identification (CWI) and is often modelled
as a supervised classification problem. For training such systems, annotated
datasets in which words and sometimes multi-word expressions are labelled
regarding complexity are required. In this paper we analyze previous work
carried out in this task and investigate the properties of CWI datasets for
English. We develop a protocol for the annotation of lexical complexity and use
this to annotate a new dataset, CompLex 2.0. We present experiments using both
new and old datasets to investigate the nature of lexical complexity. We found
that a Likert-scale annotation protocol provides an objective setting that is
superior for identifying the complexity of words compared to a binary
annotation protocol. We release a new dataset using our new protocol to promote
the task of Lexical Complexity Prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11909">
<div class="article-summary-box-inner">
<span><p>Employing paraphrasing tools to conceal plagiarized text is a severe threat
to academic integrity. To enable the detection of machine-paraphrased text, we
evaluate the effectiveness of five pre-trained word embedding models combined
with machine learning classifiers and state-of-the-art neural language models.
We analyze preprints of research papers, graduation theses, and Wikipedia
articles, which we paraphrased using different configurations of the tools
SpinBot and SpinnerChief. The best performing technique, Longformer, achieved
an average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for
SpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and
F1=65.6% for SpinnerChief cases. We show that the automated classification
alleviates shortcomings of widely-used text-matching systems, such as Turnitin
and PlagScan. To facilitate future research, all data, code, and two web
applications showcasing our contributions are openly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12450">
<div class="article-summary-box-inner">
<span><p>The rise of language models such as BERT allows for high-quality text
paraphrasing. This is a problem to academic integrity, as it is difficult to
differentiate between original and machine-generated content. We propose a
benchmark consisting of paraphrased articles using recent language models
relying on the Transformer architecture. Our contribution fosters future
research of paraphrase detection systems as it offers a large collection of
aligned original and paraphrased documents, a study regarding its structure,
classification experiments with state-of-the-art systems, and we make our
findings publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composing Conversational Negation. (arXiv:2107.06820v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06820">
<div class="article-summary-box-inner">
<span><p>Negation in natural language does not follow Boolean logic and is therefore
inherently difficult to model. In particular, it takes into account the broader
understanding of what is being negated. In previous work, we proposed a
framework for the negation of words that accounts for 'worldly context'. This
paper extends that proposal now accounting for the compositional structure
inherent in language within the DisCoCirc framework. We compose the negations
of single words to capture the negation of sentences. We also describe how to
model the negation of words whose meanings evolve in the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03067">
<div class="article-summary-box-inner">
<span><p>We present a methodology that explores how sentence structure is reflected in
neural representations of machine translation systems. We demonstrate our
model-agnostic approach with the Transformer English-German translation model.
We analyze neuron-level correlation of activations between paraphrases while
discussing the methodology challenges and the need for confound analysis to
isolate the effects of shallow cues. We find that similarity between activation
patterns can be mostly accounted for by similarity in word choice and sentence
length. Following that, we manipulate neuron activations to control the
syntactic form of the output. We show this intervention to be somewhat
successful, indicating that deep models capture sentence-structure
distinctions, despite finding no such indication at the neuron level. To
conduct our experiments, we develop a semi-automatic method to generate
meaning-preserving minimal pair paraphrases (active-passive voice and adverbial
clause-noun phrase) and compile a corpus of such pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing the dimensionality reduction of speaker embeddings for speaker diarisation: disentangling noise and informing speech activity. (arXiv:2110.03380v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03380">
<div class="article-summary-box-inner">
<span><p>The objective of this work is to train noise-robust speaker embeddings
adapted for speaker diarisation. Speaker embeddings play a crucial role in the
performance of diarisation systems, but they often capture spurious information
such as noise, adversely affecting performance. Our previous work has proposed
an auto-encoder-based dimensionality reduction module to help remove the
redundant information. However, they do not explicitly separate such
information and have also been found to be sensitive to hyper-parameter values.
To this end, we propose two contributions to overcome these issues: (i) a novel
dimensionality reduction framework that can disentangle spurious information
from the speaker embeddings; (ii) the use of speech activity vector to prevent
the speaker code from representing the background noise. Through a range of
experiments conducted on four datasets, our approach consistently demonstrates
the state-of-the-art performance among models without system fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07819">
<div class="article-summary-box-inner">
<span><p>A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods'
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02399">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention
recently for its transferable visual representation learning. However, due to
the semantic gap within datasets, CLIP's pre-trained image-text alignment
becomes sub-optimal on downstream tasks, which severely harms its transferring
performance. To better adapt the cross-modality embedding space, we propose to
enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide
textual features of different categories to adaptively explore informative
regions on the image and aggregate visual features by attention mechanisms. In
this way, the texts become visual-guided, namely, more semantically correlated
with downstream images, which greatly benefits the category-wise matching
process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known
classification datasets to demonstrate its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows. (arXiv:2202.06633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06633">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in open-domain dialogue evaluation, how to develop
automatic metrics remains an open problem. We explore the potential of dialogue
evaluation featuring dialog act information, which was hardly explicitly
modeled in previous methods. However, defined at the utterance level in
general, dialog act is of coarse granularity, as an utterance can contain
multiple segments possessing different functions. Hence, we propose segment
act, an extension of dialog act from utterance level to segment level, and
crowdsource a large-scale dataset for it. To utilize segment act flows,
sequences of segment acts, for evaluation, we develop the first consensus-based
dialogue evaluation framework, FlowEval. This framework provides a
reference-free approach for dialog evaluation by finding pseudo-references.
Extensive experiments against strong baselines on three benchmark datasets
demonstrate the effectiveness and other desirable characteristics of our
FlowEval, pointing out a potential path for better dialogue evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10189">
<div class="article-summary-box-inner">
<span><p>In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve psychotherapy
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v3 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13384">
<div class="article-summary-box-inner">
<span><p>DBLP is the largest open-access repository of scientific articles on computer
science and provides metadata associated with publications, authors, and
venues. We retrieved more than 6 million publications from DBLP and extracted
pertinent metadata (e.g., abstracts, author affiliations, citations) from the
publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to
identify trends in research activity, productivity, focus, bias, accessibility,
and impact of computer science research. We present an initial analysis focused
on the volume of computer science research (e.g., number of papers, authors,
research activity), trends in topics of interest, and citation patterns. Our
findings show that computer science is a growing research field (approx. 15%
annually), with an active and collaborative researcher community. While papers
in recent years present more bibliographical entries in comparison to previous
decades, the average number of citations has been declining. Investigating
papers' abstracts reveals that recent topic trends are clearly reflected in D3.
Finally, we list further applications of D3 and pose supplemental research
questions. The D3 dataset, our findings, and source code are publicly available
for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. (arXiv:2205.10770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10770">
<div class="article-summary-box-inner">
<span><p>Despite their wide adoption, the underlying training and memorization
dynamics of very large language models is not well understood. We empirically
study exact memorization in causal and masked language modeling, across model
sizes and throughout the training process. We measure the effects of dataset
size, learning rate, and model size on memorization, finding that larger
language models memorize training data faster across all settings.
Surprisingly, we show that larger models can memorize a larger portion of the
data before over-fitting and tend to forget less throughout the training
process. We also analyze the memorization dynamics of different parts of speech
and find that models memorize nouns and numbers first; we hypothesize and
provide empirical evidence that nouns and numbers act as a unique identifier
for memorizing individual training examples. Together, these findings present
another piece of the broader puzzle of trying to understand what actually
improves as models get bigger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Authenticity Gap in Human Evaluation. (arXiv:2205.11930v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11930">
<div class="article-summary-box-inner">
<span><p>Human ratings are the gold standard in NLG evaluation. The standard protocol
is to collect ratings of generated text, average across annotators, and rank
NLG systems by their average scores. However, little consideration has been
given as to whether this approach faithfully captures human preferences.
Analyzing this standard protocol through the lens of utility theory in
economics, we identify the implicit assumptions it makes about annotators.
These assumptions are often violated in practice, in which case annotator
ratings cease to reflect their preferences. The most egregious violations come
from using Likert scales, which provably reverse the direction of the true
preference in certain cases. We suggest improvements to the standard protocol
to make it more theoretically sound, but even in its improved form, it cannot
be used to evaluate open-ended tasks like story generation. For the latter, we
propose a new human evaluation protocol called $\textit{system-level
probabilistic assessment}$ (SPA). When human evaluation of stories is done with
SPA, we can recover the ordering of GPT-3 models by size, with statistically
significant results. However, when human evaluation is done with the standard
protocol, less than half of the expected preferences can be recovered (e.g.,
there is no significant difference between $\texttt{curie}$ and
$\texttt{davinci}$, despite using a highly powered test).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Governance in the Age of Large-Scale Data-Driven Language Technology. (arXiv:2206.03216v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03216">
<div class="article-summary-box-inner">
<span><p>The recent emergence and adoption of Machine Learning technology, and
specifically of Large Language Models, has drawn attention to the need for
systematic and transparent management of language data. This work proposes an
approach to global language data governance that attempts to organize data
management amongst stakeholders, values, and rights. Our proposal is informed
by prior work on distributed governance that accounts for human values and
grounded by an international research collaboration that brings together
researchers and practitioners from 60 countries. The framework we present is a
multi-party international governance structure focused on language data, and
incorporating technical and organizational tools needed to support its work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01780">
<div class="article-summary-box-inner">
<span><p>Program synthesis or code generation aims to generate a program that
satisfies a problem specification. Recent approaches using large-scale
pretrained language models (LMs) have shown promising results, yet they have
some critical limitations. In particular, they often follow a standard
supervised fine-tuning procedure to train a code generation model only from the
pairs of natural-language problem descriptions and ground-truth programs. Such
paradigm largely ignores some important but potentially useful signals in the
problem specification such as unit tests, which thus often results in poor
performance when solving complex unseen coding tasks. To address the
limitations, we propose "CodeRL", a new framework for program synthesis tasks
through pretrained LMs and deep reinforcement learning (RL). Specifically,
during training, we treat the code-generating LM as an actor network, and
introduce a critic network that is trained to predict the functional
correctness of generated programs and provide dense feedback signals to the
actor. During inference, we introduce a new generation procedure with a
critical sampling strategy that allows a model to automatically regenerate
programs based on feedback from example unit tests and critic scores. For the
model backbones, we extended the encoder-decoder architecture of CodeT5 with
enhanced learning objectives, larger model sizes, and better pretraining data.
Our method not only achieves new SOTA results on the challenging APPS
benchmark, but also shows strong zero-shot transfer capability with new SOTA
results on the simpler MBPP benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can we know about that which we cannot even imagine?. (arXiv:2208.03886v2 [physics.hist-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03886">
<div class="article-summary-box-inner">
<span><p>In this essay I will consider a sequence of questions, ending with one about
the breadth and depth of the epistemic limitations of our our science and
mathematics. I will then suggest a possible way to circumvent such limitations.
I begin by considering questions about the biological function of intelligence.
This will lead into questions concerning human language, perhaps the most
important cognitive prosthesis we have ever developed. While it is traditional
to rhapsodize about the perceptual power provided by human language, I will
emphasize how horribly limited - and therefore limiting - it is. This will lead
to questions of whether human mathematics, being so deeply grounded in our
language, is also deeply limited. I will then combine all of this into a
partial, sort-of, sideways answer to the guiding question of this essay: what
we can ever discern about all that we cannot even conceive of?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05735">
<div class="article-summary-box-inner">
<span><p>Neural network pruning compresses automatic speech recognition (ASR) models
effectively. However, in multilingual ASR, language-agnostic pruning may lead
to severe performance drops on some languages because language-agnostic pruning
masks may not fit all languages and discard important language-specific
parameters. In this work, we present ASR pathways, a sparse multilingual ASR
model that activates language-specific sub-networks ("pathways"), such that the
parameters for each language are learned explicitly. With the overlapping
sub-networks, the shared parameters can also enable knowledge transfer for
lower-resource languages via joint multilingual training. We propose a novel
algorithm to learn ASR pathways, and evaluate the proposed method on 4
languages with a streaming RNN-T model. Our proposed ASR pathways outperform
both dense models and a language-agnostically pruned model, and provide better
performance on low-resource languages compared to the monolingual sparse
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual analysis of intelligibility classification using English, Korean, and Tamil dysarthric speech datasets. (arXiv:2209.13260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13260">
<div class="article-summary-box-inner">
<span><p>This paper analyzes dysarthric speech datasets from three languages with
different prosodic systems: English, Korean, and Tamil. We inspect 39 acoustic
measurements which reflect three speech dimensions including voice quality,
pronunciation, and prosody. As multilingual analysis, examination on the mean
values of acoustic measurements by intelligibility levels is conducted.
Further, automatic intelligibility classification is performed to scrutinize
the optimal feature set by languages. Analyses suggest pronunciation features,
such as Percentage of Correct Consonants, Percentage of Correct Vowels, and
Percentage of Correct Phonemes to be language-independent measurements. Voice
quality and prosody features, however, generally present different aspects by
languages. Experimental results additionally show that different speech
dimension play a greater role for different languages: prosody for English,
pronunciation for Korean, both prosody and pronunciation for Tamil. This paper
contributes to speech pathology in that it differentiates between
language-independent and language-dependent measurements in intelligibility
classification for English, Korean, and Tamil dysarthric speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14610">
<div class="article-summary-box-inner">
<span><p>Mathematical reasoning, a core ability of human intelligence, presents unique
challenges for machines in abstract thinking and logical reasoning. Recent
large pre-trained language models such as GPT-3 have achieved remarkable
progress on mathematical reasoning tasks written in text form, such as math
word problems (MWP). However, it is unknown if the models can handle more
complex problems that involve math reasoning over heterogeneous information,
such as tabular data. To fill the gap, we present Tabular Math Word Problems
(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that
require mathematical reasoning on both textual and tabular data. Each question
in TabMWP is aligned with a tabular context, which is presented as an image,
semi-structured text, and a structured table. There are two types of questions:
free-text and multi-choice, and each problem is annotated with gold solutions
to reveal the multi-step reasoning process. We evaluate different pre-trained
models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier
studies suggest, since few-shot GPT-3 relies on the selection of in-context
examples, its performance is unstable and can degrade to near chance. The
unstable issue is more severe when handling complex problems like TabMWP. To
mitigate this, we further propose a novel approach, PromptPG, which utilizes
policy gradient to learn to select in-context examples from a small amount of
training data and then constructs the corresponding prompt for the test
example. Experimental results show that our method outperforms the best
baseline by 5.31% on the accuracy metric and reduces the prediction variance
significantly compared to random selection, which verifies its effectiveness in
the selection of in-context examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Contextual Commonsense Inference: A New Dataset and Task. (arXiv:2210.02890v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02890">
<div class="article-summary-box-inner">
<span><p>Contextual commonsense inference is the task of generating various types of
explanations around the events in a dyadic dialogue, including cause,
motivation, emotional reaction, and others. Producing a coherent and
non-trivial explanation requires awareness of the dialogue's structure and of
how an event is grounded in the context. In this work, we create CICEROv2, a
dataset consisting of 8,351 instances from 2,379 dialogues, containing multiple
human-written answers for each contextual commonsense inference question,
representing a type of explanation on cause, subsequent event, motivation, and
emotional reaction. We show that the inferences in CICEROv2 are more
semantically diverse than other contextual commonsense inference datasets. To
solve the inference task, we propose a collection of pre-training objectives,
including concept denoising and utterance sorting to prepare a pre-trained
model for the downstream contextual commonsense inference task. Our results
show that the proposed pre-training objectives are effective at adapting the
pre-trained T5-Large model for the contextual commonsense inference task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03454">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models such as BERT have achieved
remarkable results in Semantic Sentence Matching. However, existing models
still suffer from insufficient ability to capture subtle differences. Minor
noise like word addition, deletion, and modification of sentences may cause
flipped predictions. To alleviate this problem, we propose a novel Dual
Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture
fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention
module, which measures soft word matches by introducing a new dual channel
alignment mechanism to model affinity and difference attention. (2) Adaptive
Fusion module, this module uses attention to learn the aggregation of
difference and affinity features, and generates a vector describing the
matching details of sentence pairs. We conduct extensive experiments on
well-studied semantic matching and robustness test datasets, and the
experimental results show the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Large Language Models are Transforming Machine-Paraphrased Plagiarism. (arXiv:2210.03568v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03568">
<div class="article-summary-box-inner">
<span><p>The recent success of large language models for text generation poses a
severe threat to academic integrity, as plagiarists can generate realistic
paraphrases indistinguishable from original work. However, the role of large
autoregressive transformers in generating machine-paraphrased plagiarism and
their detection is still developing in the literature. This work explores T5
and GPT-3 for machine-paraphrase generation on scientific articles from arXiv,
student theses, and Wikipedia. We evaluate the detection performance of six
automated solutions and one commercial plagiarism detection software and
perform a human study with 105 participants regarding their detection
performance and the quality of generated examples. Our results suggest that
large models can rewrite text humans have difficulty identifying as
machine-paraphrased (53% mean acc.). Human experts rate the quality of
paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5,
fluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)
achieves a 66% F1-score in detecting paraphrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08471">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10678">
<div class="article-summary-box-inner">
<span><p>This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing over Long Tail Concepts for Medical Term Normalization. (arXiv:2210.11947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11947">
<div class="article-summary-box-inner">
<span><p>Medical term normalization consists in mapping a piece of text to a large
number of output classes. Given the small size of the annotated datasets and
the extremely long tail distribution of the concepts, it is of utmost
importance to develop models that are capable to generalize to scarce or unseen
concepts. An important attribute of most target ontologies is their
hierarchical structure. In this paper we introduce a simple and effective
learning strategy that leverages such information to enhance the
generalizability of both discriminative and generative models. The evaluation
shows that the proposed strategy produces state-of-the-art performance on seen
concepts and consistent improvements on unseen ones, allowing also for
efficient zero-shot knowledge transfer across text typologies and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$N$-gram Is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14431">
<div class="article-summary-box-inner">
<span><p>$N$-gram language models (LM) have been largely superseded by neural LMs as
the latter exhibits better performance. However, we find that $n$-gram models
can achieve satisfactory performance on a large proportion of testing cases,
indicating they have already captured abundant knowledge of the language with
relatively low computational cost. With this observation, we propose to learn a
neural LM that fits the residual between an $n$-gram LM and the real-data
distribution. The combination of $n$-gram and neural LMs not only allows the
neural part to focus on the deeper understanding of language but also provides
a flexible way to customize an LM by switching the underlying $n$-gram model
without changing the neural model. Experimental results on three typical
language tasks (i.e., language modeling, machine translation, and
summarization) demonstrate that our approach attains additional performance
gains over popular standalone neural models consistently. We also show that our
approach allows for effective domain adaptation by simply switching to a
domain-specific $n$-gram model, without any extra training. Our code is
released at https://github.com/ghrua/NgramRes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16031">
<div class="article-summary-box-inner">
<span><p>Diffusion generative models have recently greatly improved the power of
text-conditioned image generation. Existing image generation models mainly
include text conditional diffusion model and cross-modal guided diffusion
model, which are good at small scene image generation and complex scene image
generation respectively. In this work, we propose a simple yet effective
approach, namely UPainting, to unify simple and complex scene image generation,
as shown in Figure 1. Based on architecture improvements and diverse guidance
schedules, UPainting effectively integrates cross-modal guidance from a
pretrained image-text matching model into a text conditional diffusion model
that utilizes a pretrained Transformer language model as the text encoder. Our
key findings is that combining the power of large-scale Transformer language
model in understanding language and image-text matching model in capturing
cross-modal semantics and style, is effective to improve sample fidelity and
image-text alignment of image generation. In this way, UPainting has a more
general image generation capability, which can generate images of both simple
and complex scenes more effectively. To comprehensively compare text-to-image
models, we further create a more general benchmark, UniBench, with well-written
Chinese and English prompts in both simple and complex scenes. We compare
UPainting with recent models and find that UPainting greatly outperforms other
models in terms of caption similarity and image fidelity in both simple and
complex scenes. UPainting project page \url{https://upainting.github.io/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking. (arXiv:2210.17168v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17168">
<div class="article-summary-box-inner">
<span><p>Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has
widespread applications. Existing systems typically utilize BERT for text
encoding. However, CSC requires the model to account for both phonetic and
graphemic information. To adapt BERT to the CSC task, we propose a token-level
self-distillation contrastive learning method. We employ BERT to encode both
the corrupted and corresponding correct sentence. Then, we use contrastive
learning loss to regularize corrupted tokens' hidden states to be closer to
counterparts in the correct sentence. On three CSC datasets, we confirmed our
method provides a significant improvement above baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17406">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been reported to have strong performance on
natural language processing tasks. However, performance metrics such as
accuracy do not measure the quality of the model in terms of its ability to
robustly represent complex linguistic structure. In this work, we propose a
framework to evaluate the robustness of linguistic representations using
probing tasks. We leverage recent advances in extracting emergent linguistic
constructs from LLMs and apply syntax-preserving perturbations to test the
stability of these constructs in order to better understand the representations
learned by LLMs. Empirically, we study the performance of four LLMs across six
different corpora on the proposed robustness measures. We provide evidence that
context-free representation (e.g., GloVe) are in some cases competitive with
context-dependent representations from modern LLMs (e.g., BERT), yet equally
brittle to syntax-preserving manipulations. Emergent syntactic representations
in neural networks are brittle, thus our work poses the attention on the risk
of comparing such structures to those that are object of a long lasting debate
in linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The future is different: Large pre-trained language models fail in prediction tasks. (arXiv:2211.00384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00384">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LPLM) have shown spectacular success when
fine-tuned on downstream supervised tasks. Yet, it is known that their
performance can drastically drop when there is a distribution shift between the
data used during training and that used at inference time. In this paper we
focus on data distributions that naturally change over time and introduce four
new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and
POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display
average performance drops of about 88% (in the best case!) when predicting the
popularity of future posts from sub-reddits whose topic distribution changes
with time. We then introduce a simple methodology that leverages neural
variational dynamic topic models and attention mechanisms to infer temporal
language model representations for regression tasks. Our models display
performance drops of only about 40% in the worst cases (2% in the best ones)
when predicting the popularity of future posts, while using only about 7% of
the total number of parameters of LPLM and providing interpretable
representations that offer insight into real-world events, like the GameStop
short squeeze of 2021
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00732">
<div class="article-summary-box-inner">
<span><p>Online encyclopedias, such as Wikipedia, have been well-developed and
researched in the last two decades. One can find any attributes or other
information of a wiki item on a wiki page edited by a community of volunteers.
However, the traditional text, images and tables can hardly express some
aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may
care more about ``How to feed it'' or ``How to train it not to protect its
food''. Currently, short-video platforms have become a hallmark in the online
world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts,
short-video apps have changed how we consume and create content today. Except
for producing short videos for entertainment, we can find more and more authors
sharing insightful knowledge widely across all walks of life. These short
videos, which we call knowledge videos, can easily express any aspects (e.g.
hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and
they can be systematically analyzed and organized like an online encyclopedia.
In this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia
consisting of items, aspects, and short videos lined to them, which was
extracted from billions of videos of Kuaishou (Kwai), a well-known short-video
platform in China. We first collected items from multiple sources and mined
user-centered aspects from millions of users' queries to build an item-aspect
tree. Then we propose a new task called ``multi-modal item-aspect linking'' as
an expansion of ``entity linking'' to link short videos into item-aspect pairs
and build the whole short-video encyclopedia. Intrinsic evaluations show that
our encyclopedia is of large scale and highly accurate. We also conduct
sufficient extrinsic experiments to show how Kuaipedia can help fundamental
applications such as entity typing and entity linking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00915">
<div class="article-summary-box-inner">
<span><p>Retriever-reader models achieve competitive performance across many different
NLP tasks such as open question answering and dialogue conversations. In this
work, we notice these models easily overfit the top-rank retrieval passages and
standard training fails to reason over the entire retrieval passages. We
introduce a learnable passage mask mechanism which desensitizes the impact from
the top-rank retrieval passages and prevents the model from overfitting.
Controlling the gradient variance with fewer mask candidates and selecting the
mask candidates with one-shot bi-level optimization, our learnable
regularization strategy enforces the answer generation to focus on the entire
retrieval passages. Experiments on different tasks across open question
answering, dialogue conversation, and fact verification show that our method
consistently outperforms its baselines. Extensive experiments and ablation
studies demonstrate that our method can be general, effective, and beneficial
for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01335">
<div class="article-summary-box-inner">
<span><p>The tremendous success of CLIP (Radford et al., 2021) has promoted the
research and application of contrastive learning for vision-language
pretraining. In this work, we construct a large-scale dataset of image-text
pairs in Chinese, where most data are retrieved from publicly available
datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5
Chinese CLIP models of multiple sizes, spanning from 77 to 958 million
parameters. Furthermore, we propose a two-stage pretraining method, where the
model is first trained with the image encoder frozen and then trained with all
parameters being optimized, to achieve enhanced model performance. Our
comprehensive experiments demonstrate that Chinese CLIP can achieve the
state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups
of zero-shot learning and finetuning, and it is able to achieve competitive
performance in zero-shot image classification based on the evaluation on the
ELEVATER benchmark (Li et al., 2022). We have released our codes, models, and
demos in https://github.com/OFA-Sys/Chinese-CLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01180">
<div class="article-summary-box-inner">
<span><p>This work investigates the use of large-scale, pre-trained models (CLIP and
HuBERT) for multilingual speech-image retrieval. For non-English speech-image
retrieval, we outperform the current state-of-the-art performance by a wide
margin when training separate models for each language, and show that a single
model which processes speech in all three languages still achieves retrieval
scores comparable with the prior state-of-the-art. We identify key differences
in model behavior and performance between English and non-English settings,
presumably attributable to the English-only pre-training of CLIP and HuBERT.
Finally, we show that our models can be used for mono- and cross-lingual
speech-text retrieval and cross-lingual speech-speech retrieval, despite never
having seen any parallel speech-text or speech-speech data during training.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-06 23:15:42.170227739 UTC">2022-11-06 23:15:42 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>