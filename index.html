<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-01T01:30:00Z">02-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchiSound: Audio Generation with Diffusion. (arXiv:2301.13267v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13267">
<div class="article-summary-box-inner">
<span><p>The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
generation. One area that has yet to be fully explored is the application of
diffusion models to audio generation. Audio generation requires an
understanding of multiple aspects, such as the temporal dimension, long term
structure, multiple layers of overlapping sounds, and the nuances that only
trained listeners can detect. In this work, we investigate the potential of
diffusion models for audio generation. We propose a set of models to tackle
multiple aspects, including a new method for text-conditional latent audio
diffusion with stacked 1D U-Nets, that can generate multiple minutes of music
from a textual description. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open source libraries
with the hope of simplifying future work in the field. Samples can be found at
https://bit.ly/audio-diffusion. Codes are at
https://github.com/archinetai/audio-diffusion-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems. (arXiv:2301.13268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13268">
<div class="article-summary-box-inner">
<span><p>Response generation is one of the critical components in task-oriented dialog
systems. Existing studies have shown that large pre-trained language models can
be adapted to this task. The typical paradigm of adapting such extremely large
language models would be by fine-tuning on the downstream tasks which is not
only time-consuming but also involves significant resources and access to
fine-tuning data. Prompting \citep{schick2020exploiting} has been an
alternative to fine-tuning in many NLP tasks. In our work, we explore the idea
of using prompting for response generation in task-oriented dialog systems.
Specifically, we propose an approach that performs \textit{contextual dynamic
prompting} where the prompts are learnt from dialog contexts. We aim to distill
useful prompting signals from the dialog context. On experiments with MultiWOZ
2.2 dataset \cite{zang2020multiwoz}, we show that contextual dynamic prompts
improve response generation in terms of \textit{combined score}
\cite{mehri-etal-2019-structured} by 3 absolute points, and a massive 20 points
when dialog states are incorporated. Furthermore, human annotation on these
conversations found that agents which incorporate context were preferred over
agents with vanilla prefix-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13294">
<div class="article-summary-box-inner">
<span><p>Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and corrected
translations in domain-specific projects. Machine translation (MT) has achieved
significant progress in the area of domain adaptation. However, real-time
adaptation remains challenging. Large-scale language models (LLMs) have
recently shown interesting capabilities of in-context learning, where they
learn to replicate certain input-output text generation patterns, without
further fine-tuning. By feeding an LLM with a prompt that consists of a list of
translation pairs, it can then simulate the domain and style characteristics at
inference time. This work aims to investigate how we can utilize in-context
learning to improve real-time adaptive MT. Our extensive experiments show
promising results at translation time. For example, GPT-3.5 can adapt to a set
of in-domain sentence pairs and/or terminology while translating a new
sentence. We observe that the translation quality with few-shot in-context
learning can surpass that of strong encoder-decoder MT systems, especially for
high-resource languages. Moreover, we investigate whether we can combine MT
from strong encoder-decoder models with fuzzy matches, which can further
improve the translation, especially for less supported languages. We conduct
our experiments across five diverse languages, namely English-to-Arabic
(EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR),
English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES) language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization. (arXiv:2301.13298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13298">
<div class="article-summary-box-inner">
<span><p>While human evaluation remains best practice for accurately judging the
faithfulness of automatically-generated summaries, few solutions exist to
address the increased difficulty and workload when evaluating long-form
summaries. Through a survey of 162 papers on long-form summarization, we first
shed light on current human evaluation practices surrounding long-form
summaries. We find that 73% of these papers do not perform any human evaluation
on model-generated summaries, while other works face new difficulties that
manifest when dealing with long documents (e.g., low inter-annotator
agreement). Motivated by our survey, we present LongEval, a set of guidelines
for human evaluation of faithfulness in long-form summaries that addresses the
following challenges: (1) How can we achieve high inter-annotator agreement on
faithfulness scores? (2) How can we minimize annotator workload while
maintaining accurate faithfulness scores? and (3) Do humans benefit from
automated alignment between summary and source snippets? We deploy LongEval in
annotation studies on two long-form summarization datasets in different domains
(SQuALITY and PubMed), and we find that switching to a finer granularity of
judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness
scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a
partial annotation of fine-grained units highly correlates with scores from a
full annotation workload (0.89 Kendall's tau using 50% judgments). We release
our human judgments, annotation templates, and our software as a Python library
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alternating Updates for Efficient Transformers. (arXiv:2301.13310v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13310">
<div class="article-summary-box-inner">
<span><p>It is well established that increasing scale in deep transformer networks
leads to improved quality and performance. This increase in scale often comes
with an increase in compute cost and inference latency. Consequently, research
into methods which help realize the benefits of increased scale without leading
to an increase in the compute cost becomes important. We introduce Alternating
Updates (AltUp), a simple-to-implement method to increase a model's capacity
without the computational burden. AltUp enables the widening of the learned
representation without increasing the computation time by working on a subblock
of the representation at each layer. Our experiments on various transformer
models and language tasks demonstrate the consistent effectiveness of
alternating updates on a diverse set of benchmarks. Finally, we present
extensions of AltUp to the sequence dimension, and demonstrate how AltUp can be
synergistically combined with existing approaches, such as Sparse
Mixture-of-Experts models, to obtain efficient models with even higher
capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxy-based Zero-Shot Entity Linking by Effective Candidate Retrieval. (arXiv:2301.13318v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13318">
<div class="article-summary-box-inner">
<span><p>A recent advancement in the domain of biomedical Entity Linking is the
development of powerful two-stage algorithms, an initial candidate retrieval
stage that generates a shortlist of entities for each mention, followed by a
candidate ranking stage. However, the effectiveness of both stages are
inextricably dependent on computationally expensive components. Specifically,
in candidate retrieval via dense representation retrieval it is important to
have hard negative samples, which require repeated forward passes and nearest
neighbour searches across the entire entity label set throughout training. In
this work, we show that pairing a proxy-based metric learning loss with an
adversarial regularizer provides an efficient alternative to hard negative
sampling in the candidate retrieval stage. In particular, we show competitive
performance on the recall@1 metric, thereby providing the option to leave out
the expensive candidate ranking step. Finally, we demonstrate how the model can
be used in a zero-shot setting to discover out of knowledge base biomedical
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Entailment for Parameter Efficient Few Shot Learning. (arXiv:2301.13345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13345">
<div class="article-summary-box-inner">
<span><p>Few-shot learning allows pre-trained language models to adapt to downstream
tasks while using a limited number of training examples. However, practical
applications are limited when all model parameters must be optimized. In this
work we apply a new technique for parameter efficient few shot learning while
adopting a strict definition of parameter efficiency. Our training method
combines 1) intermediate training by reformulating natural language tasks as
entailment tasks \cite{wang_entailment_2021} and 2) differentiable optimization
of template and label tokens \cite{zhang_differentiable_2021}. We quantify the
tradeoff between parameter efficiency and performance in the few-shot regime
and propose a simple model agnostic approach that can be extended to any task
By achieving competitive performance while only optimizing 3\% of a model's
parameters and allowing for batched inference, we allow for more efficient
practical deployment of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Identification with BOS and EOS Label Combinations. (arXiv:2301.13352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13352">
<div class="article-summary-box-inner">
<span><p>The sentence is a fundamental unit in many NLP applications. Sentence
segmentation is widely used as the first preprocessing task, where an input
text is split into consecutive sentences considering the end of the sentence
(EOS) as their boundaries. This task formulation relies on a strong assumption
that the input text consists only of sentences, or what we call the sentential
units (SUs). However, real-world texts often contain non-sentential units
(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which
are unreasonable or undesirable to be treated as a part of an SU. To tackle
this issue, we formulate a novel task of sentence identification, where the
goal is to identify SUs while excluding NSUs in a given text. To conduct
sentence identification, we propose a simple yet effective method which
combines the beginning of the sentence (BOS) and EOS labels to determine the
most probable SUs and NSUs based on dynamic programming. To evaluate this task,
we design an automatic, language-independent procedure to convert the Universal
Dependencies corpora into sentence identification benchmarks. Finally, our
experiments on the sentence identification task demonstrate that our proposed
method generally outperforms sentence segmentation baselines which only utilize
EOS labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Open-Domain Dialogue Evaluation with a Causal Inference Model. (arXiv:2301.13372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13372">
<div class="article-summary-box-inner">
<span><p>Effective evaluation methods remain a significant challenge for research on
open-domain conversational dialogue systems. Explicit satisfaction ratings can
be elicited from users, but users often do not provide ratings when asked, and
those they give can be highly subjective. Post-hoc ratings by experts are an
alternative, but these can be both expensive and complex to collect. Here, we
explore the creation of automated methods for predicting both expert and user
ratings of open-domain dialogues. We compare four different approaches. First,
we train a baseline model using an end-to-end transformer to predict ratings
directly from the raw dialogue text. The other three methods are variants of a
two-stage approach in which we first extract interpretable features at the turn
level that capture, among other aspects, user dialogue behaviors indicating
contradiction, repetition, disinterest, compliments, or criticism. We project
these features to the dialogue level and train a dialogue-level MLP regression
model, a dialogue-level LSTM, and a novel causal inference model called
counterfactual-LSTM (CF-LSTM) to predict ratings. The proposed CF-LSTM is a
sequential model over turn-level features which predicts ratings using multiple
regressors depending on hypotheses derived from the turn-level features. As a
causal inference model, CF-LSTM aims to learn the underlying causes of a
specific event, such as a low rating. We also bin the user ratings and perform
classification experiments with all four models. In evaluation experiments on
conversational data from the Alexa Prize SocialBot, we show that the CF-LSTM
achieves the best performance for predicting dialogue ratings and
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13379">
<div class="article-summary-box-inner">
<span><p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)
performance on a gamut of complex reasoning tasks, the generated reasoning
chain does not necessarily reflect how the model arrives at the answer (aka.
faithfulness). We propose Faithful CoT, a faithful-by-construction framework
that decomposes a reasoning task into two stages: Translation (Natural Language
query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning
chain $\rightarrow$ answer), using an LM and a deterministic solver
respectively. We demonstrate the efficacy of our approach on 10 reasoning
datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9
out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word
Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1
on Logical Inference, under greedy decoding. Together with self-consistency
decoding, we achieve new state-of-the-art few-shot performance on 7 out of the
10 datasets, showing a strong synergy between faithfulness and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models. (arXiv:2301.13382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13382">
<div class="article-summary-box-inner">
<span><p>Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique
testbeds for exploring the translation challenges of turning literacy into
numeracy. Previous publicly-available transformer models from eighteen months
prior and 1000 times smaller failed to provide basic arithmetic. The
statistical analysis of four complex datasets described here combines
arithmetic manipulations that cannot be memorized or encoded by simple rules.
The work examines whether next-token prediction succeeds from sentence
completion into the realm of actual numerical understanding. For example, the
work highlights cases for descriptive statistics on in-memory datasets that the
LLM initially loads from memory or generates randomly using python libraries.
The resulting exploratory data analysis showcases the model's capabilities to
group by or pivot categorical sums, infer feature importance, derive
correlations, and predict unseen test cases using linear regression. To extend
the model's testable range, the research deletes and appends random rows such
that recall alone cannot explain emergent numeracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZhichunRoad at Amazon KDD Cup 2022: MultiTask Pre-Training for E-Commerce Product Search. (arXiv:2301.13455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13455">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a robust multilingual model to improve the quality
of search results. Our model not only leverage the processed class-balanced
dataset, but also benefit from multitask pre-training that leads to more
general representations. In pre-training stage, we adopt mlm task,
classification task and contrastive learning task to achieve considerably
performance. In fine-tuning stage, we use confident learning, exponential
moving average method (EMA), adversarial training (FGM) and regularized dropout
strategy (R-Drop) to improve the model's generalization and robustness.
Moreover, we use a multi-granular semantic unit to discover the queries and
products textual metadata for enhancing the representation of the model. Our
approach obtained competitive results and ranked top-8 in three tasks. We
release the source code and pre-trained models associated with this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Archive TimeLine Summarization (ATLS): Conceptual Framework for Timeline Generation over Historical Document Collections. (arXiv:2301.13479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13479">
<div class="article-summary-box-inner">
<span><p>Archive collections are nowadays mostly available through search engines
interfaces, which allow a user to retrieve documents by issuing queries. The
study of these collections may be, however, impaired by some aspects of search
engines, such as the overwhelming number of documents returned or the lack of
contextual knowledge provided. New methods that could work independently or in
combination with search engines are then required to access these collections.
In this position paper, we propose to extend TimeLine Summarization (TLS)
methods on archive collections to assist in their studies. We provide an
overview of existing TLS methods and we describe a conceptual framework for an
Archive TimeLine Summarization (ATLS) system, which aims to generate
informative, readable and interpretable timelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physarum Inspired Bicycle Lane Network Design in a Congested Mega City. (arXiv:2301.13609v1 [physics.soc-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13609">
<div class="article-summary-box-inner">
<span><p>Mobility is a key factor in urban life and transport network plays a vital
role in mobility. Worse transport network having less mobility is one of the
key reasons to decline the living standard in any unplanned mega city.
Transport mobility enhancement in an unplanned mega city is always challenging
due to various constraints including complex design and high cost involvement.
The aim of this thesis is to enhance transport mobility in a megacity
introducing a bicycle lane. To design the bicycle lane natural Physarum,
brainless single celled multi-nucleated protist, is studied and modified for
better optimization. Recently Physarum inspired techniques are drawn
significant attention to the construction of effective networks. Exiting
Physarum inspired models effectively and efficiently solves different problems
including transport network design and modification and implication for bicycle
lane is the unique contribution of this study. Central area of Dhaka, the
capital city of Bangladesh, is considered to analyze and design the bicycle
lane network bypassing primary roads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned BERT. (arXiv:2301.13631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13631">
<div class="article-summary-box-inner">
<span><p>Extracting precise geographical information from textual contents is crucial
in a plethora of applications. For example, during hazardous events, a robust
and unbiased toponym extraction framework can provide an avenue to tie the
location concerned to the topic discussed by news media posts and pinpoint
humanitarian help requests or damage reports from social media. Early studies
have leveraged rule-based, gazetteer-based, deep learning, and hybrid
approaches to address this problem. However, the performance of existing tools
is deficient in supporting operations like emergency rescue, which relies on
fine-grained, accurate geographic information. The emerging pretrained language
models can better capture the underlying characteristics of text information,
including place names, offering a promising pathway to optimize toponym
recognition to underpin practical applications. In this paper, TopoBERT, a
toponym recognition module based on a one dimensional Convolutional Neural
Network (CNN1D) and Bidirectional Encoder Representation from Transformers
(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,
Wikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover
the best training strategy, and train the model. Another two datasets
(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three
distinguished classifiers, linear, multi-layer perceptron, and CNN1D, are
benchmarked to determine the optimal model architecture. TopoBERT achieves
state-of-the-art performance (f1-score=0.865) compared to the other five
baseline models and can be applied to diverse toponym recognition tasks without
additional training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Sentiment and Hate Speech Analysis of Facebook Data by Employing Multilingual Transformer Models. (arXiv:2301.13668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13668">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been a heightened consensus within academia and in
the public discourse that Social Media Platforms (SMPs), amplify the spread of
hateful and negative sentiment content. Researchers have identified how hateful
content, political propaganda, and targeted messaging contributed to real-world
harms including insurrections against democratically elected governments,
genocide, and breakdown of social cohesion due to heightened negative discourse
towards certain communities in parts of the world. To counter these issues,
SMPs have created semi-automated systems that can help identify toxic speech.
In this paper we analyse the statistical distribution of hateful and negative
sentiment contents within a representative Facebook dataset (n= 604,703)
scrapped through 648 public Facebook pages which identify themselves as
proponents (and followers) of far-right Hindutva actors. These pages were
identified manually using keyword searches on Facebook and on CrowdTangleand
classified as far-right Hindutva pages based on page names, page descriptions,
and discourses shared on these pages. We employ state-of-the-art, open-source
XLM-T multilingual transformer-based language models to perform sentiment and
hate speech analysis of the textual contents shared on these pages over a
period of 5.5 years. The result shows the statistical distributions of the
predicted sentiment and the hate speech labels; top actors, and top page
categories. We further discuss the benchmark performances and limitations of
these pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Friend-training: Learning from Models of Different but Related Tasks. (arXiv:2301.13683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13683">
<div class="article-summary-box-inner">
<span><p>Current self-training methods such as standard self-training, co-training,
tri-training, and others often focus on improving model performance on a single
task, utilizing differences in input features, model architectures, and
training processes. However, many tasks in natural language processing are
about different but related aspects of language, and models trained for one
task can be great teachers for other related tasks. In this work, we propose
friend-training, a cross-task self-training framework, where models trained to
do different tasks are used in an iterative training, pseudo-labeling, and
retraining process to help each other for better selection of pseudo-labels.
With two dialogue understanding tasks, conversational semantic role labeling
and dialogue rewriting, chosen for a case study, we show that the models
trained with the friend-training framework achieve the best performance
compared to strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. (arXiv:2301.13688v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13688">
<div class="article-summary-box-inner">
<span><p>We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
https://github.com/google-research/FLAN/tree/main/flan/v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality. (arXiv:2301.13714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13714">
<div class="article-summary-box-inner">
<span><p>A recent line of work in NLP focuses on the (dis)ability of models to
generalise compositionally for artificial languages. However, when considering
natural language tasks, the data involved is not strictly, or locally,
compositional. Quantifying the compositionality of data is a challenging task,
which has been investigated primarily for short utterances. We use recursive
neural models (Tree-LSTMs) with bottlenecks that limit the transfer of
information between nodes. We illustrate that comparing data's representations
in models with and without the bottleneck can be used to produce a
compositionality metric. The procedure is applied to the evaluation of
arithmetic expressions using synthetic data, and sentiment classification using
natural language data. We demonstrate that compression through a bottleneck
impacts non-compositional examples disproportionately and then use the
bottleneck compositionality metric (BCM) to distinguish compositional from
non-compositional samples, yielding a compositionality ranking over a dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot cross-lingual transfer language selection using linguistic similarity. (arXiv:2301.13720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13720">
<div class="article-summary-box-inner">
<span><p>We study the selection of transfer languages for different Natural Language
Processing tasks, specifically sentiment analysis, named entity recognition and
dependency parsing. In order to select an optimal transfer language, we propose
to utilize different linguistic similarity metrics to measure the distance
between languages and make the choice of transfer language based on this
information instead of relying on intuition. We demonstrate that linguistic
similarity correlates with cross-lingual transfer performance for all of the
proposed tasks. We also show that there is a statistically significant
difference in choosing the optimal language as the transfer source instead of
English. This allows us to select a more suitable transfer language which can
be used to better leverage knowledge from high-resource languages in order to
improve the performance of language applications lacking data. For the study,
we used datasets from eight different languages from three language families.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13741">
<div class="article-summary-box-inner">
<span><p>Real-world data contains a vast amount of multimodal information, among which
vision and language are the two most representative modalities. Moreover,
increasingly heavier models, e.g., Transformers, have attracted the attention
of researchers to model compression. However, how to compress multimodal
models, especially vison-language Transformers, is still under-explored. This
paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive
\textbf{P}runing (UPop) as a universal vison-language Transformer compression
framework, which incorporates 1) unifiedly searching multimodal subnets in a
continuous optimization space from the original model, which enables automatic
assignment of pruning ratios among compressible modalities and structures; 2)
progressively searching and retraining the subnet, which maintains convergence
between the search and retrain to attain higher compression ratios. Experiments
on multiple generative and discriminative vision-language tasks, including
Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval,
Text-Image Retrieval, and Image Classification, demonstrate the effectiveness
and versatility of the proposed UPop framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation. (arXiv:2301.13753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13753">
<div class="article-summary-box-inner">
<span><p>State-of-the-art neural text generation models are typically trained to
maximize the likelihood of each token in the ground-truth sequence conditioned
on the previous target tokens. However, during inference, the model needs to
make a prediction conditioned on the tokens generated by itself. This
train-test discrepancy is referred to as exposure bias. Scheduled sampling is a
curriculum learning strategy that gradually exposes the model to its own
predictions during training to mitigate this bias. Most of the proposed
approaches design a scheduler based on training steps, which generally requires
careful tuning depending on the training setup. In this work, we introduce
Dynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the
schedule based solely on the training time accuracy, while enhancing the
curriculum learning by introducing an imitation loss, which attempts to make
the behavior of the decoder indistinguishable from the behavior of a
teacher-forced decoder. DySI is universally applicable across training setups
with minimal tuning. Extensive experiments and analysis show that DySI not only
achieves notable improvements on standard machine translation benchmarks, but
also significantly improves the robustness of other text generation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Touch\'e23-ValueEval Dataset for Identifying Human Values behind Arguments. (arXiv:2301.13771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13771">
<div class="article-summary-box-inner">
<span><p>We present the Touch\'e23-ValueEval Dataset for Identifying Human Values
behind Arguments. To investigate approaches for the automated detection of
human values behind arguments, we collected 9324 arguments from 6 diverse
sources, covering religious texts, political discussions, free-text arguments,
newspaper editorials, and online democracy platforms. Each argument was
annotated by 3 crowdworkers for 54 values. The Touch\'e23-ValueEval dataset
extends the Webis-ArgValues-22. In comparison to the previous dataset, the
effectiveness of a 1-Baseline decreases, but that of an out-of-the-box BERT
model increases. Therefore, though the classification difficulty increased as
per the label distribution, the larger dataset allows for training better
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13808">
<div class="article-summary-box-inner">
<span><p>Table-based reasoning has shown remarkable progress in combining deep models
with discrete reasoning, which requires reasoning over both free-form natural
language (NL) questions and structured tabular data. However, previous
table-based reasoning solutions usually suffer from significant performance
degradation on huge evidence (tables). In addition, most existing methods
struggle to reason over complex questions since the required information is
scattered in different places. To alleviate the above challenges, we exploit
large language models (LLMs) as decomposers for effective table-based
reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence
(a small table) to mitigate the interference of useless information for table
reasoning; and (ii) decompose complex questions into simpler sub-questions for
text reasoning. Specifically, we first use the LLMs to break down the evidence
(tables) involved in the current question, retaining the relevant evidence and
excluding the remaining irrelevant evidence from the huge table. In addition,
we propose a "parsing-execution-filling" strategy to alleviate the
hallucination dilemma of the chain of thought by decoupling logic and numerical
computation in each step. Extensive experiments show that our method can
effectively leverage decomposed evidence and questions and outperforms the
strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,
our model outperforms human performance for the first time on the TabFact
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13816">
<div class="article-summary-box-inner">
<span><p>The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at https://github.com/reddy-lab-code-research/PPOCoder .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis. (arXiv:2301.13819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13819">
<div class="article-summary-box-inner">
<span><p>ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract). (arXiv:2301.13820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13820">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have demonstrated strong capability in
structured prediction tasks such as semantic parsing, few amounts of research
have explored the underlying mechanisms of their success. Our work studies
different methods for explaining an LLM-based semantic parser and qualitatively
discusses the explained model behaviors, hoping to inspire future research
toward better understanding them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Language Models to Images for Multimodal Generation. (arXiv:2301.13823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13823">
<div class="article-summary-box-inner">
<span><p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13826">
<div class="article-summary-box-inner">
<span><p>Recent text-to-image generative models have demonstrated an unparalleled
ability to generate diverse and creative imagery guided by a target text
prompt. While revolutionary, current state-of-the-art diffusion models may
still fail in generating images that fully convey the semantics in the given
text prompt. We analyze the publicly available Stable Diffusion model and
assess the existence of catastrophic neglect, where the model fails to generate
one or more of the subjects from the input prompt. Moreover, we find that in
some cases the model also fails to correctly bind attributes (e.g., colors) to
their corresponding subjects. To help mitigate these failure cases, we
introduce the concept of Generative Semantic Nursing (GSN), where we seek to
intervene in the generative process on the fly during inference time to improve
the faithfulness of the generated images. Using an attention-based formulation
of GSN, dubbed Attend-and-Excite, we guide the model to refine the
cross-attention units to attend to all subject tokens in the text prompt and
strengthen - or excite - their activations, encouraging the model to generate
all subjects described in the text prompt. We compare our approach to
alternative approaches and demonstrate that it conveys the desired concepts
more faithfully across a range of text prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Multi-Document Summarization Models Synthesize?. (arXiv:2301.13844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13844">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization entails producing concise synopses of
collections of inputs. For some applications, the synopsis should accurately
\emph{synthesize} inputs with respect to a key property or aspect. For example,
a synopsis of film reviews all written about a particular movie should reflect
the average critic consensus. As a more consequential example, consider
narrative summaries that accompany biomedical \emph{systematic reviews} of
clinical trial results. These narratives should fairly summarize the
potentially conflicting results from individual trials.
</p>
<p>In this paper we ask: To what extent do modern multi-document summarization
models implicitly perform this type of synthesis? To assess this we perform a
suite of experiments that probe the degree to which conditional generation
models trained for summarization using standard methods yield outputs that
appropriately synthesize inputs. We find that existing models do partially
perform synthesis, but do so imperfectly. In particular, they are
over-sensitive to changes in input ordering and under-sensitive to changes in
input compositions (e.g., the ratio of positive to negative movie reviews). We
propose a simple, general method for improving model synthesis capabilities by
generating an explicitly diverse set of candidate outputs, and then selecting
from these the string best aligned with the expected aggregate measure for the
inputs, or \emph{abstaining} when the model produces no good candidate. This
approach improves model synthesis performance. We hope highlighting the need
for synthesis (in some summarization settings), motivates further research into
multi-document summarization methods and learning objectives that explicitly
account for the need to synthesize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Large Language Models for News Summarization. (arXiv:2301.13848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13848">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promise for automatic summarization
but the reasons behind their successes are poorly understood. By conducting a
human evaluation on ten LLMs across different pretraining methods, prompts, and
model scales, we make two important observations. First, we find instruction
tuning, and not model size, is the key to the LLM's zero-shot summarization
capability. Second, existing studies have been limited by low-quality
references, leading to underestimates of human performance and lower few-shot
and finetuning performance. To better evaluate LLMs, we perform human
evaluation over high-quality summaries we collect from freelance writers.
Despite major stylistic differences such as the amount of paraphrasing, we find
that LMM summaries are judged to be on par with human written summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text. (arXiv:2301.13852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13852">
<div class="article-summary-box-inner">
<span><p>ChatGPT has the ability to generate grammatically flawless and
seemingly-human replies to different types of questions from various domains.
The number of its users and of its applications is growing at an unprecedented
rate. Unfortunately, use and abuse come hand in hand. In this paper, we study
whether a machine learning model can be effectively trained to accurately
distinguish between original human and seemingly human (that is,
ChatGPT-generated) text, especially when this text is short. Furthermore, we
employ an explainable artificial intelligence framework to gain insight into
the reasoning behind the model trained to differentiate between
ChatGPT-generated and human-generated text. The goal is to analyze model's
decisions and determine if any specific patterns or characteristics can be
identified. Our study focuses on short online reviews, conducting two
experiments comparing human-generated and ChatGPT-generated text. The first
experiment involves ChatGPT text generated from custom queries, while the
second experiment involves text generated by rephrasing original
human-generated reviews. We fine-tune a Transformer-based model and use it to
make predictions, which are then explained using SHAP. We compare our model
with a perplexity score-based approach and find that disambiguation between
human and ChatGPT-generated reviews is more challenging for the ML model when
using rephrased text. However, our proposed approach still achieves an accuracy
of 79%. Using explainability, we observe that ChatGPT's writing is polite,
without specific details, using fancy and atypical vocabulary, impersonal, and
typically it does not express feelings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network. (arXiv:2301.13853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13853">
<div class="article-summary-box-inner">
<span><p>Education is a right of all, however, every individual is different than
others. Teachers in post-communism era discover inherent individualism to
equally train all towards job market of fourth industrial revolution. We can
consider scenario of ethnic minority education in academic practices. Ethnic
minority group has grown in their own culture and would prefer to be taught in
their native way. We have formulated such linguistic anthropology(how people
learn)based engagement as semi-supervised problem. Then, we have developed an
conditional deep generative adversarial network algorithm namely LA-GAN to
classify linguistic ethnographic features in student engagement. Theoretical
justification proves the objective, regularization and loss function of our
semi-supervised adversarial model. Survey questions are prepared to reach some
form of assumptions about z-generation and ethnic minority group, whose
learning style, learning approach and preference are our main area of interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13867">
<div class="article-summary-box-inner">
<span><p>We investigate the mathematical capabilities of ChatGPT by testing it on
publicly available datasets, as well as hand-crafted ones, and measuring its
performance against other models trained on a mathematical corpus, such as
Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily
professional activities of mathematicians (question answering, theorem
searching). In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover
elementary mathematics. We address this issue by introducing a new dataset:
GHOSTS. It is the first natural-language dataset made and curated by working
researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of
language models. We benchmark ChatGPT on GHOSTS and evaluate performance
against fine-grained criteria. We make this new dataset publicly available to
assist a community-driven comparison of ChatGPT with (future) large language
models in terms of advanced mathematical comprehension. We conclude that
contrary to many positive reports in the media (a potential case of selection
bias), ChatGPT's mathematical abilities are significantly below those of an
average mathematics graduate student. Our results show that ChatGPT often
understands the question but fails to provide correct solutions. Hence, if your
goal is to use it to pass a university exam, you would be better off copying
from your average peer!
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADL: Language-Directed Physics-Based Character Control. (arXiv:2301.13868v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13868">
<div class="article-summary-box-inner">
<span><p>Developing systems that can synthesize natural and life-like motions for
simulated characters has long been a focus for computer animation. But in order
for these systems to be useful for downstream applications, they need not only
produce high-quality motions, but must also provide an accessible and versatile
interface through which users can direct a character's behaviors. Natural
language provides a simple-to-use and expressive medium for specifying a user's
intent. Recent breakthroughs in natural language processing (NLP) have
demonstrated effective use of language-based interfaces for applications such
as image generation and program synthesis. In this work, we present PADL, which
leverages recent innovations in NLP in order to take steps towards developing
language-directed controllers for physics-based character animation. PADL
allows users to issue natural language commands for specifying both high-level
tasks and low-level skills that a character should perform. We present an
adversarial imitation learning approach for training policies to map high-level
language commands to low-level controls that enable a character to perform the
desired task and skill specified by a user's commands. Furthermore, we propose
a multi-task aggregation method that leverages a language-based multiple-choice
question-answering approach to determine high-level task objectives from
language commands. We show that our framework can be applied to effectively
direct a simulated humanoid character to perform a diverse array of complex
motor skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Influencers: Unboxing the Mystique. (arXiv:2012.12311v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12311">
<div class="article-summary-box-inner">
<span><p>Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent elements in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using a novel "interpretable deep learning"
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that "what is
said" in words (text) is more influential than "how it is said" in imagery
(images) followed by acoustics (audio). Our interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video elements. We
eliminate several spurious and confounded relationships, and identify a smaller
subset of theory-based relationships. We uncover novel findings that establish
distinct effects for measures of shallow and deep engagement which are based on
the dual-system framework of human thinking. Our approach is validated using
simulated data, and we discuss the learnings from our findings for influencers
and brands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From How Humans Correct. (arXiv:2102.00225v12 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and re-label
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we re-label the
noisy data in our dataset for our industry application. The experiment result
shows that our method improve the classification accuracy from 91.7% to 92.5%.
The 91.7% accuracy is trained on the corrected dataset, which improve the
baseline from 83.3% to 91.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05327">
<div class="article-summary-box-inner">
<span><p>Explainable AI was born as a pathway to allow humans to explore and
understand the inner working of complex systems. However, establishing what is
an explanation and objectively evaluating explainability are not trivial tasks.
This paper presents a new model-agnostic metric to measure the Degree of
Explainability of information in an objective way. We exploit a specific
theoretical model from Ordinary Language Philosophy called the Achinstein's
Theory of Explanations, implemented with an algorithm relying on deep language
models for knowledge graph extraction and information retrieval. To understand
whether this metric can measure explainability, we devised a few experiments
and user studies involving more than 190 participants, evaluating two realistic
systems for healthcare and finance using famous AI technology, including
Artificial Neural Networks and TreeSHAP. The results we obtained are
statistically significant (with P values lower than .01), suggesting that our
proposed metric for measuring the Degree of Explainability is robust in several
scenarios, and it aligns with concrete expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Efficient Insertion Transformer with Fractional Positional Encoding. (arXiv:2112.06295v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06295">
<div class="article-summary-box-inner">
<span><p>Auto-regressive neural sequence models have been shown to be effective across
text generation tasks. However, their left-to-right decoding order prevents
generation from being parallelized. Insertion Transformer (Stern et al., 2019)
is an attractive alternative that allows outputting multiple tokens in a single
generation step. Nevertheless, due to the incompatibility between absolute
positional encoding and insertion-based generation schemes, it needs to refresh
the encoding of every token in the generated partial hypothesis at each step,
which could be costly. We design a novel reusable positional encoding scheme
for Insertion Transformers called Fractional Positional Encoding (FPE), which
allows reusing representations calculated in previous steps. Empirical studies
on various text generation tasks demonstrate the effectiveness of FPE, which
leads to floating-point operation reduction and latency improvements on batched
decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks. (arXiv:2112.08159v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08159">
<div class="article-summary-box-inner">
<span><p>Preserving privacy in contemporary NLP models allows us to work with
sensitive data, but unfortunately comes at a price. We know that stricter
privacy guarantees in differentially-private stochastic gradient descent
(DP-SGD) generally degrade model performance. However, previous research on the
efficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this
short paper, we provide an extensive analysis of different privacy preserving
strategies on seven downstream datasets in five different `typical' NLP tasks
with varying complexity using modern neural models based on BERT and
XtremeDistil architectures. We show that unlike standard non-private approaches
to solving NLP tasks, where bigger is usually better, privacy-preserving
strategies do not exhibit a winning pattern, and each task and privacy regime
requires a special treatment to achieve adequate performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization. (arXiv:2202.13100v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13100">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning is the problem of predicting instances over classes not
seen during training. One approach to zero-shot learning is providing auxiliary
class information to the model. Prior work along this vein have largely used
expensive per-instance annotation or singular class-level descriptions, but
per-instance descriptions are hard to scale and single class descriptions may
not be rich enough. Furthermore, these works have used natural-language
descriptions exclusively, simple bi-encoders models, and modality or
task-specific methods. These approaches have several limitations: text
supervision may not always be available or optimal and bi-encoders may only
learn coarse relations between inputs and class descriptions. In this work, we
present SemSup, a novel approach that uses (1) a scalable multiple description
sampling method which improves performance over single descriptions, (2)
alternative description formats such as JSON that are easy to generate and
outperform text on certain settings, and (3) hybrid lexical-semantic similarity
to leverage fine-grained information in class descriptions. We demonstrate the
effectiveness of SemSup across four datasets, two modalities, and three
generalization settings. For example, across text and image datasets, SemSup
increases unseen class generalization accuracy by 15 points on average compared
to the closest baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Recycling for Language Models. (arXiv:2207.04993v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04993">
<div class="article-summary-box-inner">
<span><p>Real-world applications of neural language models often involve running many
different models over the same corpus. The high computational cost of these
runs has led to interest in techniques that can reuse the contextualized
embeddings produced in previous runs to speed training and inference of future
ones. We refer to this approach as embedding recycling (ER). While multiple ER
techniques have been proposed, their practical effectiveness is still unknown
because existing evaluations consider very few models and do not adequately
account for overhead costs. We perform an extensive evaluation of ER across
eight different models (17 to 900 million parameters) and fourteen tasks in
English. We show how a simple ER technique that caches activations from an
intermediate layer of a pretrained model, and learns task-specific adapters on
the later layers, is broadly effective. For the best-performing baseline in our
experiments (DeBERTa-v2 XL), adding a precomputed cache results in a &gt;90%
speedup during training and 87-91% speedup for inference, with negligible
impact on accuracy. Our analysis reveals important areas of future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation. (arXiv:2208.08110v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08110">
<div class="article-summary-box-inner">
<span><p>Curriculum Data Augmentation (CDA) improves neural models by presenting
synthetic data with increasing difficulties from easy to hard. However,
traditional CDA simply treats the ratio of word perturbation as the difficulty
measure and goes through the curriculums only once. This paper presents
\textbf{PCC}: \textbf{P}araphrasing with Bottom-k Sampling and \textbf{C}yclic
Learning for \textbf{C}urriculum Data Augmentation, a novel CDA framework via
paraphrasing, which exploits the textual paraphrase similarity as the
curriculum difficulty measure. We propose a curriculum-aware paraphrase
generation module composed of three units: a paraphrase candidate generator
with bottom-k sampling, a filtering mechanism and a difficulty measure. We also
propose a cyclic learning strategy that passes through the curriculums multiple
times. The bottom-k sampling is proposed to generate super-hard instances for
the later curriculums. Experimental results on few-shot text classification as
well as dialogue generation indicate that PCC surpasses competitive baselines.
Human evaluation and extensive case studies indicate that bottom-k sampling
effectively generates super-hard instances, and PCC significantly improves the
baseline dialogue agent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Efficacy of Self-Supervised Speech Models for Audio Representations. (arXiv:2209.12900v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12900">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models, which can serve as powerful
upstream models to extract meaningful speech representations, have achieved
unprecedented success in speech representation learning. However, their
effectiveness on non-speech datasets is relatively less explored. In this work,
we propose an ensemble framework, with a combination of ensemble techniques, to
fuse SSL speech models' embeddings. Extensive experiments on speech and
non-speech audio datasets are conducted to investigate the representation
abilities of our ensemble method and its single constituent model. Ablation
studies are carried out to evaluate the performances of different ensemble
techniques, such as feature averaging and concatenation. All experiments are
conducted during NeurIPS 2021 HEAR Challenge as a standard evaluation pipeline
provided by competition officials. Results demonstrate SSL speech models'
strong abilities on various non-speech tasks, while we also note that they fail
to deal with fine-grained music tasks, such as pitch classification and note
onset detection. In addition, feature ensemble is shown to have great potential
on producing more holistic representations, as our proposed framework generally
surpasses state-of-the-art SSL speech/audio models and has superior performance
on various datasets compared with other teams in HEAR Challenge. Our code is
available at https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge --
NTU-GURA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks. (arXiv:2210.07499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07499">
<div class="article-summary-box-inner">
<span><p>Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a
target sequence. The Connectionist Temporal Classification (CTC) criterion is
widely used in multiple seq2seq tasks. Besides predicting the target sequence,
a side product of CTC is to predict the alignment, which is the most probable
input-long sequence that specifies a hard aligning relationship between the
input and target units. As there are multiple potential aligning sequences
(called paths) that are equally considered in CTC formulation, the choice of
which path will be most probable and become the predicted alignment is always
uncertain. In addition, it is usually observed that the alignment predicted by
vanilla CTC will drift compared with its reference and rarely provides
practical functionalities. Thus, the motivation of this work is to make the CTC
alignment prediction controllable and thus equip CTC with extra
functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this
work, in which a customizable Bayes risk function is adopted to enforce the
desired characteristics of the predicted alignment. With the risk function, the
BRCTC is a general framework to adopt some customizable preference over the
paths in order to concentrate the posterior into a particular subset of the
paths. In applications, we explore one particular preference which yields
models with the down-sampling ability and reduced inference costs. By using
BRCTC with another preference for early emissions, we obtain an improved
performance-latency trade-off for online models. Experimentally, the proposed
BRCTC reduces the inference cost of offline models by up to 47% without
performance degradation and cuts down the overall latency of online systems to
an unseen level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences. (arXiv:2210.11794v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11794">
<div class="article-summary-box-inner">
<span><p>Efficient Transformers have been developed for long sequence modeling, due to
their subquadratic memory and time complexity. Sparse Transformer is a popular
approach to improving the efficiency of Transformers by restricting
self-attention to locations specified by the predefined sparse patterns.
However, leveraging sparsity may sacrifice expressiveness compared to
full-attention, when important token correlations are multiple hops away. To
combine advantages of both the efficiency of sparse transformer and the
expressiveness of full-attention Transformer, we propose \textit{Diffuser}, a
new state-of-the-art efficient Transformer. Diffuser incorporates all token
interactions within one attention layer while maintaining low computation and
memory costs. The key idea is to expand the receptive field of sparse attention
using Attention Diffusion, which computes multi-hop token correlations based on
all paths between corresponding disconnected tokens, besides attention among
neighboring tokens. Theoretically, we show the expressiveness of Diffuser as a
universal sequence approximator for sequence-to-sequence modeling, and
investigate its ability to approximate full-attention by analyzing the graph
expander property from the spectral perspective. Experimentally, we investigate
the effectiveness of Diffuser with extensive evaluations, including language
modeling, image modeling, and Long Range Arena (LRA). Evaluation results show
that Diffuser achieves improvements by an average of 0.94% on text
classification tasks and 2.30% on LRA, with 1.67$\times$ memory savings
compared to state-of-the-art benchmarks, which demonstrates superior
performance of Diffuser in both expressiveness and efficiency aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models. (arXiv:2210.13432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13432">
<div class="article-summary-box-inner">
<span><p>Large language models (LLM) trained using the next-token-prediction
objective, such as GPT3 and PaLM, have revolutionized natural language
processing in recent years by showing impressive zero-shot and few-shot
capabilities across a wide range of tasks. In this work, we propose a simple
technique that significantly boosts the performance of LLMs without adding
computational cost. Our key observation is that, by performing the next token
prediction task with randomly selected past tokens masked out, we can improve
the quality of the learned representations for downstream language
understanding tasks. We hypothesize that randomly masking past tokens prevents
over-attending to recent tokens and encourages attention to tokens in the
distant past. We find that our method, Forgetful Causal Masking (FCM),
significantly improves both few-shot and finetuning performance of PaLM. We
further consider a simple extension, T-FCM, which introduces bidirectional
context to causal language model without altering the sequence order, and
further improves finetuning performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches. (arXiv:2211.16285v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16285">
<div class="article-summary-box-inner">
<span><p>Text classification of unseen classes is a challenging Natural Language
Processing task and is mainly attempted using two different types of
approaches. Similarity-based approaches attempt to classify instances based on
similarities between text document representations and class description
representations. Zero-shot text classification approaches aim to generalize
knowledge gained from a training task by assigning appropriate labels of
unknown classes to text documents. Although existing studies have already
investigated individual approaches to these categories, the experiments in
literature do not provide a consistent comparison. This paper addresses this
gap by conducting a systematic evaluation of different similarity-based and
zero-shot approaches for text classification of unseen classes. Different
state-of-the-art approaches are benchmarked on four text classification
datasets, including a new dataset from the medical domain. Additionally, novel
SimCSE and SBERT-based baselines are proposed, as other baselines used in
existing work yield weak classification results and are easily outperformed.
Finally, the novel similarity-based Lbl2TransformerVec approach is presented,
which outperforms previous state-of-the-art approaches in unsupervised text
classification. Our experiments show that similarity-based approaches
significantly outperform zero-shot approaches in most cases. Additionally,
using SimCSE or SBERT embeddings instead of simpler text representations
increases similarity-based classification results even further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04325">
<div class="article-summary-box-inner">
<span><p>Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
modes, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Difformer: Empowering Diffusion Models on the Embedding Space for Text Generation. (arXiv:2212.09412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09412">
<div class="article-summary-box-inner">
<span><p>Diffusion models have achieved state-of-the-art synthesis quality on both
visual and audio tasks, and recent works further adapt them to textual data by
diffusing on the embedding space. In this paper, we conduct systematic studies
and analyze the challenges between the continuous data space and the embedding
space which have not been carefully explored. Firstly, the data distribution is
learnable for embeddings, which may lead to the collapse of the loss function.
Secondly, as the norm of embeddings varies between popular and rare words,
adding the same noise scale will lead to sub-optimal results. In addition, we
find the normal level of noise causes insufficient training of the model. To
address the above challenges, we propose Difformer, an embedding diffusion
model based on Transformer, which consists of three essential modules including
an additional anchor loss function, a layer normalization module for
embeddings, and a noise factor to the Gaussian noise. Experiments on two
seminal text generation tasks including machine translation and text
summarization show the superiority of Difformer over compared embedding
diffusion baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated speech- and text-based classification of neuropsychiatric conditions in a multidiagnostic setting. (arXiv:2301.06916v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.06916">
<div class="article-summary-box-inner">
<span><p>Speech patterns have been identified as potential diagnostic markers for
neuropsychiatric conditions. However, most studies only compare a single
clinical group to healthy controls, whereas clinical practice often requires
differentiating between multiple potential diagnoses (multiclass settings). To
address this, we assembled a dataset of repeated recordings from 420
participants (67 with major depressive disorder, 106 with schizophrenia and 46
with autism, as well as matched controls), and tested the performance of a
range of conventional machine learning models and advanced Transformer models
on both binary and multiclass classification, based on voice and text features.
</p>
<p>While binary models performed comparably to previous research (F1 scores
between 0.54-0.75 for autism spectrum disorder, ASD; 0.67-0.92 for major
depressive disorder, MDD; and 0.71-0.83 for schizophrenia); when
differentiating between multiple diagnostic groups performance decreased
markedly (F1 scores between 0.35-0.44 for ASD, 0.57-0.75 for MDD, 0.15-0.66 for
schizophrenia, and 0.38-0.52 macro F1). Combining voice and text-based models
yielded increased performance, suggesting that they capture complementary
diagnostic information.
</p>
<p>Our results indicate that models trained on binary classification may learn
to rely on markers of generic differences between clinical and non-clinical
populations, or markers of clinical features that overlap across conditions,
rather than identifying markers specific to individual conditions. We provide
recommendations for future research in the field, suggesting increased focus on
developing larger transdiagnostic datasets that include more fine-grained
clinical features, and that can support the development of models that better
capture the complexity of neuropsychiatric conditions and naturalistic
diagnostic assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT A Good Translator? A Preliminary Study. (arXiv:2301.08745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08745">
<div class="article-summary-box-inner">
<span><p>This report provides a preliminary evaluation of ChatGPT for machine
translation, including translation prompt, multilingual translation, and
translation robustness. We adopt the prompts advised by ChatGPT to trigger its
translation ability and find that the candidate prompts generally work well and
show minor performance differences. By evaluating on a number of benchmark test
sets, we find that ChatGPT performs competitively with commercial translation
products (e.g., Google Translate) on high-resource European languages but lags
behind significantly on low-resource or distant languages. For distant
languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$
that asks ChatGPT to translate the source sentence into a high-resource pivot
language before into the target language, which improves the translation
performance significantly. As for the translation robustness, ChatGPT does not
perform as well as the commercial systems on biomedical abstracts or Reddit
comments but is potentially a good translator for spoken language. Scripts and
data: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Compositional Semantic Parsing with Concept Pretraining. (arXiv:2301.09809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09809">
<div class="article-summary-box-inner">
<span><p>Semantic parsing plays a key role in digital voice assistants such as Alexa,
Siri, and Google Assistant by mapping natural language to structured meaning
representations. When we want to improve the capabilities of a voice assistant
by adding a new domain, the underlying semantic parsing model needs to be
retrained using thousands of annotated examples from the new domain, which is
time-consuming and expensive. In this work, we present an architecture to
perform such domain adaptation automatically, with only a small amount of
metadata about the new domain and without any new training data (zero-shot) or
with very few examples (few-shot). We use a base seq2seq (sequence-to-sequence)
architecture and augment it with a concept encoder that encodes intent and slot
tags from the new domain. We also introduce a novel decoder-focused approach to
pretrain seq2seq models to be concept aware using Wikidata and use it to help
our model learn important concepts and perform well in low-resource settings.
We report few-shot and zero-shot results for compositional semantic parsing on
the TOPv2 dataset and show that our model outperforms prior approaches in
few-shot settings for the TOPv2 and SNIPS datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11004">
<div class="article-summary-box-inner">
<span><p>Interactions among humans on social media often convey intentions behind
their actions, yielding a psychological language resource for Mental Health
Analysis (MHA) of online users. The success of Computational Intelligence
Techniques (CIT) for inferring mental illness from such social media resources
points to NLP as a lens for causal analysis and perception mining. However, we
argue that more consequential and explainable research is required for optimal
impact on clinical psychology practice and personalized mental healthcare. To
bridge this gap, we posit two significant dimensions: (1) Causal analysis to
illustrate a cause and effect relationship in the user generated text; (2)
Perception mining to infer psychological perspectives of social effects on
online users intentions. Within the scope of Natural Language Processing (NLP),
we further explore critical areas of inquiry associated with these two
dimensions, specifically through recent advancements in discourse analysis.
This position paper guides the community to explore solutions in this space and
advance the state of practice in developing conversational agents for inferring
mental health from social media. We advocate for a more explainable approach
toward modeling computational psychology problems through the lens of language
as we observe an increased number of research contributions in dataset and
problem formulation for causal relation extraction and perception enhancements
while inferring mental states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11719">
<div class="article-summary-box-inner">
<span><p>Despite the great development of document summarization techniques nowadays,
factual inconsistencies between the generated summaries and the original text
still occur from time to time. This paper proposes a prefix-tuning-based
approach that uses a set of trainable continuous prefix prompt together with
discrete prompts to aid model generation, which makes a significant impact on
both CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements
on fact preservation in the generated summaries indicates the effectiveness of
adopting this prefix-tuning-based method in knowledge-enhanced document
summarization, and also shows a great potential on other natural language
processing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker. (arXiv:2301.12331v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12331">
<div class="article-summary-box-inner">
<span><p>Voice synthesis has seen significant improvements in the past decade
resulting in highly intelligible voices. Further investigations have resulted
in models that can produce variable speech, including conditional emotional
expression. The problem lies, however, in a focus on phrase-level modifications
and prosodic vocal features. Using the CREMA-D dataset we have trained a GAN
conditioned on emotion to generate worth lengths for a given input text. These
word lengths are relative to neutral speech and can be provided, through speech
synthesis markup language (SSML) to a text-to-speech (TTS) system to generate
more expressive speech. Additionally, a generative model is also trained using
implicit maximum likelihood estimation (IMLE) and a comparative analysis with
GANs is included. We were able to achieve better performances on objective
measures for neutral speech, and better time alignment for happy speech when
compared to an out-of-box model. However, further investigation of subjective
evaluation is required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11916">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained large language models have demonstrated
remarkable efficiency in achieving an inference-time few-shot learning
capability known as in-context learning. However, existing literature has
highlighted the sensitivity of this capability to the selection of few-shot
demonstrations. The underlying mechanisms by which this capability arises from
regular language model pretraining objectives remain poorly understood. In this
study, we aim to examine the in-context learning phenomenon through a Bayesian
lens, viewing large language models as topic models that implicitly infer
task-related information from demonstrations. On this premise, we propose an
algorithm for selecting optimal demonstrations from a set of annotated data and
demonstrate a significant 12.5% improvement relative to the random selection
baseline, averaged over eight GPT2 and GPT3 models on eight different
real-world text classification datasets. Our empirical findings support our
hypothesis that large language models implicitly infer a latent concept
variable.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-01 23:13:04.083239067 UTC">2023-02-01 23:13:04 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>