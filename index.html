<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-25T01:30:00Z">09-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12339">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) like ChatGPT have excited scientists across
fields; in medicine, one source of excitement is the potential applications of
LLMs trained on electronic health record (EHR) data. But there are tough
questions we must first answer if health care institutions are interested in
having LLMs trained on their own data; should they train an LLM from scratch or
fine-tune it from an open-source model? For healthcare institutions with a
predefined budget, what are the biggest LLMs they can afford? In this study, we
take steps towards answering these questions with an analysis on dataset sizes,
model sizes, and costs for LLM training using EHR data. This analysis provides
a framework for thinking about these questions in terms of data scale, compute
scale, and training budgets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12342">
<div class="article-summary-box-inner">
<span><p>The deployment of large language models (LLMs) raises concerns regarding
their cultural misalignment and potential ramifications on individuals from
various cultural norms. Existing work investigated political and social biases
and public opinions rather than their cultural values. To address this
limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural
alignment using Hofstede's cultural dimension framework, which offers an
explanatory cross-cultural comparison through the latent variable analysis. We
apply our approach to assess the cultural values embedded in state-of-the-art
LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United
States (US), Saudi Arabia, China, and Slovakia, using different prompting
styles and hyperparameter settings. Our results not only quantify cultural
alignment of LLMs with certain countries, but also reveal the difference
between LLMs in explanatory cultural dimensions. While all LLMs did not provide
satisfactory results in understanding cultural values, GPT-4 exhibited the
highest CAT score for the cultural values of the US.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12360">
<div class="article-summary-box-inner">
<span><p>Attention-Aware Social Choice tackles the fundamental conflict faced by some
agent communities between their desire to include all members in the decision
making processes and the limited time and attention that are at the disposal of
the community members. Here, we investigate a combination of two techniques for
attention-aware social choice, namely Natural Language Processing (NLP) and
Sampling. Essentially, we propose a system in which each governance proposal to
change the status quo is first sent to a trained NLP model that estimates the
probability that the proposal would pass if all community members directly vote
on it; then, based on such an estimation, a population sample of a certain size
is being selected and the proposal is decided upon by taking the sample
majority. We develop several concrete algorithms following the scheme described
above and evaluate them using various data, including such from several
Decentralized Autonomous Organizations (DAOs).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on Case Reports. (arXiv:2309.12361v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12361">
<div class="article-summary-box-inner">
<span><p>Objective: To evaluate the efficiency of large language models (LLMs) such as
ChatGPT to assist in diagnosing neuro-ophthalmic diseases based on detailed
case descriptions. Methods: We selected 22 different case reports of
neuro-ophthalmic diseases from a publicly available online database. These
cases included a wide range of chronic and acute diseases that are commonly
seen by neuro-ophthalmic sub-specialists. We inserted the text from each case
as a new prompt into both ChatGPT v3.5 and ChatGPT Plus v4.0 and asked for the
most probable diagnosis. We then presented the exact information to two
neuro-ophthalmologists and recorded their diagnoses followed by comparison to
responses from both versions of ChatGPT. Results: ChatGPT v3.5, ChatGPT Plus
v4.0, and the two neuro-ophthalmologists were correct in 13 (59%), 18 (82%), 19
(86%), and 19 (86%) out of 22 cases, respectively. The agreement between the
various diagnostic sources were as follows: ChatGPT v3.5 and ChatGPT Plus v4.0,
13 (59%); ChatGPT v3.5 and the first neuro-ophthalmologist, 12 (55%); ChatGPT
v3.5 and the second neuro-ophthalmologist, 12 (55%); ChatGPT Plus v4.0 and the
first neuro-ophthalmologist, 17 (77%); ChatGPT Plus v4.0 and the second
neuro-ophthalmologist, 16 (73%); and first and second neuro-ophthalmologists 17
(17%). Conclusions: The accuracy of ChatGPT v3.5 and ChatGPT Plus v4.0 in
diagnosing patients with neuro-ophthalmic diseases was 59% and 82%,
respectively. With further development, ChatGPT Plus v4.0 may have potential to
be used in clinical care settings to assist clinicians in providing quick,
accurate diagnoses of patients in neuro-ophthalmology. The applicability of
using LLMs like ChatGPT in clinical settings that lack access to subspeciality
trained neuro-ophthalmologists deserves further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12367">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have facilitated the
development of chatbots with sophisticated conversational capabilities.
However, LLMs exhibit frequent inaccurate responses to queries, hindering
applications in educational settings. In this paper, we investigate the
effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors
to increase response reliability. To achieve this, we design a scaleable KB
that affords educational supervisors seamless integration of lesson curricula,
which is automatically processed by the intelligent tutoring system. We then
detail an evaluation, where student participants were presented with questions
about the artificial intelligence curriculum to respond to. GPT-4 intelligent
tutors with varying hierarchies of KB access and human domain experts then
assessed these responses. Lastly, students cross-examined the intelligent
tutors' responses to the domain experts' and ranked their various pedagogical
abilities. Results suggest that, although these intelligent tutors still
demonstrate a lower accuracy compared to domain experts, the accuracy of the
intelligent tutors increases when access to a KB is granted. We also observe
that the intelligent tutors with KB access exhibit better pedagogical abilities
to speak like a teacher and understand students than those of domain experts,
while their ability to help students remains lagging behind domain experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constraints First: A New MDD-based Model to Generate Sentences Under Constraints. (arXiv:2309.12415v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12415">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new approach to generating strongly constrained
texts. We consider standardized sentence generation for the typical application
of vision screening. To solve this problem, we formalize it as a discrete
combinatorial optimization problem and utilize multivalued decision diagrams
(MDD), a well-known data structure to deal with constraints. In our context,
one key strength of MDD is to compute an exhaustive set of solutions without
performing any search. Once the sentences are obtained, we apply a language
model (GPT-2) to keep the best ones. We detail this for English and also for
French where the agreement and conjugation rules are known to be more complex.
Finally, with the help of GPT-2, we get hundreds of bona-fide candidate
sentences. When compared with the few dozen sentences usually available in the
well-known vision screening test (MNREAD), this brings a major breakthrough in
the field of standardized sentence generation. Also, as it can be easily
adapted for other languages, it has the potential to make the MNREAD test even
more valuable and usable. More generally, this paper highlights MDD as a
convincing alternative for constrained text generation, especially when the
constraints are hard to satisfy, but also for many other prospects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges. (arXiv:2309.12426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12426">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive zero shot
performance on a wide range of NLP tasks, demonstrating the ability to reason
and apply commonsense. A relevant application is to use them for creating high
quality synthetic datasets for downstream tasks. In this work, we probe whether
GPT-4 can be used to augment existing extractive reading comprehension
datasets. Automating data annotation processes has the potential to save large
amounts of time, money and effort that goes into manually labelling datasets.
In this paper, we evaluate the performance of GPT-4 as a replacement for human
annotators for low resource reading comprehension tasks, by comparing
performance after fine tuning, and the cost associated with annotation. This
work serves to be the first analysis of LLMs as synthetic data augmenters for
QA systems, highlighting the unique opportunities and challenges. Additionally,
we release augmented versions of low resource datasets, that will allow the
research community to create further benchmarks for evaluation of generated
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Multilingual Fingerspelling Corpora. (arXiv:2309.12443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12443">
<div class="article-summary-box-inner">
<span><p>We apply active learning to help with data scarcity problems in sign
languages. In particular, we perform a novel analysis of the effect of
pre-training. Since many sign languages are linguistic descendants of French
sign language, they share hand configurations, which pre-training can hopefully
exploit. We test this hypothesis on American, Chinese, German, and Irish
fingerspelling corpora. We do observe a benefit from pre-training, but this may
be due to visual rather than linguistic similarities
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12444">
<div class="article-summary-box-inner">
<span><p>Generative Artificial Intelligence is set to revolutionize healthcare
delivery by transforming traditional patient care into a more personalized,
efficient, and proactive process. Chatbots, serving as interactive
conversational models, will probably drive this patient-centered transformation
in healthcare. Through the provision of various services, including diagnosis,
personalized lifestyle recommendations, and mental health support, the
objective is to substantially augment patient health outcomes, all the while
mitigating the workload burden on healthcare providers. The life-critical
nature of healthcare applications necessitates establishing a unified and
comprehensive set of evaluation metrics for conversational models. Existing
evaluation metrics proposed for various generic large language models (LLMs)
demonstrate a lack of comprehension regarding medical and health concepts and
their significance in promoting patients' well-being. Moreover, these metrics
neglect pivotal user-centered aspects, including trust-building, ethics,
personalization, empathy, user comprehension, and emotional support. The
purpose of this paper is to explore state-of-the-art LLM-based evaluation
metrics that are specifically applicable to the assessment of interactive
conversational models in healthcare. Subsequently, we present an comprehensive
set of evaluation metrics designed to thoroughly assess the performance of
healthcare chatbots from an end-user perspective. These metrics encompass an
evaluation of language processing abilities, impact on real-world clinical
tasks, and effectiveness in user-interactive conversations. Finally, we engage
in a discussion concerning the challenges associated with defining and
implementing these metrics, with particular emphasis on confounding factors
such as the target audience, evaluation methods, and prompt techniques involved
in the evaluation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12455">
<div class="article-summary-box-inner">
<span><p>Maintaining factual consistency is a critical issue in abstractive text
summarisation, however, it cannot be assessed by traditional automatic metrics
used for evaluating text summarisation, such as ROUGE scoring. Recent efforts
have been devoted to developing improved metrics for measuring factual
consistency using pre-trained language models, but these metrics have
restrictive token limits, and are therefore not suitable for evaluating long
document text summarisation. Moreover, there is limited research evaluating
whether existing automatic evaluation metrics are fit for purpose when applied
to long document data sets. In this work, we evaluate the efficacy of automatic
metrics at assessing factual consistency in long document text summarisation
and propose a new evaluation framework LongDocFACTScore. This framework allows
metrics to be extended to any length document. This framework outperforms
existing state-of-the-art metrics in its ability to correlate with human
measures of factuality when used to evaluate long document summarisation data
sets. Furthermore, we show LongDocFACTScore has performance comparable to
state-of-the-art metrics when evaluated against human measures of factual
consistency on short document data sets. We make our code and annotated data
publicly available: https://github.com/jbshp/LongDocFACTScore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12460">
<div class="article-summary-box-inner">
<span><p>In the domain of scientific imaging, interpreting visual data often demands
an intricate combination of human expertise and deep comprehension of the
subject materials. This study presents a novel methodology to linguistically
emulate and subsequently evaluate human-like interactions with Scanning
Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a
multimodal deep learning framework, our approach distills insights from both
textual and visual data harvested from peer-reviewed articles, further
augmented by the capabilities of GPT-4 for refined data synthesis and
evaluation. Despite inherent challenges--such as nuanced interpretations and
the limited availability of specialized datasets--our model (GlassLLaVA) excels
in crafting accurate interpretations, identifying key features, and detecting
defects in previously unseen SEM images. Moreover, we introduce versatile
evaluation metrics, suitable for an array of scientific imaging applications,
which allows for benchmarking against research-grounded answers. Benefiting
from the robustness of contemporary Large Language Models, our model adeptly
aligns with insights from research papers. This advancement not only
underscores considerable progress in bridging the gap between human and machine
interpretation in scientific imaging, but also hints at expansive avenues for
future research and broader application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12481">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned Large Language Models (It-LLMs) have been exhibiting
outstanding abilities to reason around cognitive states, intentions, and
reactions of all people involved, letting humans guide and comprehend
day-to-day social interactions effectively. In fact, several multiple-choice
questions (MCQ) benchmarks have been proposed to construct solid assessments of
the models' abilities. However, earlier works are demonstrating the presence of
inherent "order bias" in It-LLMs, posing challenges to the appropriate
evaluation. In this paper, we investigate It-LLMs' resilience abilities towards
a series of probing tests using four MCQ benchmarks. Introducing adversarial
examples, we show a significant performance gap, mainly when varying the order
of the choices, which reveals a selection bias and brings into discussion
reasoning abilities. Following a correlation between first positions and model
choices due to positional bias, we hypothesized the presence of structural
heuristics in the decision-making process of the It-LLMs, strengthened by
including significant examples in few-shot scenarios. Finally, by using the
Chain-of-Thought (CoT) technique, we elicit the model to reason and mitigate
the bias by obtaining more robust models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12485">
<div class="article-summary-box-inner">
<span><p>In the present study, we investigate and compare reasoning in large language
models (LLM) and humans using a selection of cognitive psychology tools
traditionally dedicated to the study of (bounded) rationality. To do so, we
presented to human participants and an array of pretrained LLMs new variants of
classical cognitive experiments, and cross-compared their performances. Our
results showed that most of the included models presented reasoning errors akin
to those frequently ascribed to error-prone, heuristic-based human reasoning.
Notwithstanding this superficial similarity, an in-depth comparison between
humans and LLMs indicated important differences with human-like reasoning, with
models limitations disappearing almost entirely in more recent LLMs releases.
Moreover, we show that while it is possible to devise strategies to induce
better performance, humans and machines are not equally-responsive to the same
prompting schemes. We conclude by discussing the epistemological implications
and challenges of comparing human and machine behavior for both artificial
intelligence and cognitive psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12491">
<div class="article-summary-box-inner">
<span><p>We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish "doctora" for "female doctor") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12501">
<div class="article-summary-box-inner">
<span><p>Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12546">
<div class="article-summary-box-inner">
<span><p>Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed
for natural language generation (NLG) tasks, are based on measuring the n-gram
overlap between the generated and reference text. These simple metrics may be
insufficient for more complex tasks, such as question generation (QG), which
requires generating questions that are answerable by the reference answers.
Developing a more sophisticated automatic evaluation metric, thus, remains as
an urgent problem in QG research. This work proposes a Prompting-based Metric
on ANswerability (PMAN), a novel automatic evaluation metric to assess whether
the generated questions are answerable by the reference answers for the QG
tasks. Extensive experiments demonstrate that its evaluation results are
reliable and align with human evaluations. We further apply our metric to
evaluate the performance of QG models, which shows our metric complements
conventional metrics. Our implementation of a ChatGPT-based QG model achieves
state-of-the-art (SOTA) performance in generating answerable questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models. (arXiv:2309.12551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12551">
<div class="article-summary-box-inner">
<span><p>Text simplification is a common task where the text is adapted to make it
easier to understand. Similarly, text elaboration can make a passage more
sophisticated, offering a method to control the complexity of reading
comprehension tests. However, text simplification and elaboration tasks are
limited to only relatively alter the readability of texts. It is useful to
directly modify the readability of any text to an absolute target readability
level to cater to a diverse audience. Ideally, the readability of
readability-controlled generated text should be independent of the source text.
Therefore, we propose a novel readability-controlled text modification task.
The task requires the generation of 8 versions at various target readability
levels for each input text. We introduce novel readability-controlled text
modification metrics. The baselines for this task use ChatGPT and Llama-2, with
an extension approach introducing a two-step process (generating paraphrases by
passing through the language model twice). The zero-shot approaches are able to
push the readability of the paraphrases in the desired direction but the final
readability remains correlated with the original text's readability. We also
find greater drops in semantic and lexical similarity between the source and
target texts with greater shifts in the readability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12555">
<div class="article-summary-box-inner">
<span><p>A personally tailored exercise regimen is crucial to ensuring sufficient
physical activities, yet challenging to create as people have complex schedules
and considerations and the creation of plans often requires iterations with
experts. We present PlanFitting, a conversational AI that assists in
personalized exercise planning. Leveraging generative capabilities of large
language models, PlanFitting enables users to describe various constraints and
queries in natural language, thereby facilitating the creation and refinement
of their weekly exercise plan to suit their specific circumstances while
staying grounded in foundational principles. Through a user study where
participants (N=18) generated a personalized exercise plan using PlanFitting
and expert planners (N=3) evaluated these plans, we identified the potential of
PlanFitting in generating personalized, actionable, and evidence-based exercise
plans. We discuss future design opportunities for AI assistants in creating
plans that better comply with exercise principles and accommodate personal
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12570">
<div class="article-summary-box-inner">
<span><p>The development of large language models (LLMs) capable of following
instructions and engaging in conversational interactions sparked increased
interest in their utilization across various support tools. We investigate the
utility of modern LLMs in assisting professional writers via an empirical user
study (n=30). The design of our collaborative writing interface is grounded in
the cognitive process model of writing that views writing as a goal-oriented
thinking process encompassing non-linear cognitive activities: planning,
translating, and reviewing. Participants are asked to submit a post-completion
survey to provide feedback on the potential and pitfalls of LLMs as writing
collaborators. Upon analyzing the writer-LLM interactions, we find that while
writers seek LLM's help across all three types of cognitive activities, they
find LLMs more helpful in translation and reviewing. Our findings from
analyzing both the interactions and the survey responses highlight future
research directions in creative writing assistance using LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking Model Insights: A Dataset for Automated Model Card Generation. (arXiv:2309.12616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12616">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are no longer restricted to ML community, and
instruction-tuned LMs have led to a rise in autonomous AI agents. As the
accessibility of LMs grows, it is imperative that an understanding of their
capabilities, intended usage, and development cycle also improves. Model cards
are a popular practice for documenting detailed information about an ML model.
To automate model card generation, we introduce a dataset of 500
question-answer pairs for 25 ML models that cover crucial aspects of the model,
such as its training configurations, datasets, biases, architecture details,
and training resources. We employ annotators to extract the answers from the
original paper. Further, we explore the capabilities of LMs in generating model
cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa,
and Galactica showcase a significant gap in the understanding of research
papers by these aforementioned LMs as well as generating factual textual
responses. We posit that our dataset can be used to train models to automate
the generation of model cards from paper text and reduce human effort in the
model card curation process. The complete dataset is available on
https://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12619">
<div class="article-summary-box-inner">
<span><p>Neural language models often fail to generate diverse and informative texts,
limiting their applicability in real-world problems. While previous approaches
have proposed to address these issues by identifying and penalizing undesirable
behaviors (e.g., repetition, overuse of frequent words) from language models,
we propose an alternative approach based on an observation: models primarily
learn attributes within examples that are likely to cause degeneration
problems. Based on this observation, we propose a new approach to prevent
degeneration problems by training two models. Specifically, we first train a
model that is designed to amplify undesirable patterns. We then enhance the
diversity of the second model by focusing on patterns that the first model
fails to learn. Extensive experiments on two tasks, namely language modeling
and dialogue generation, demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12625">
<div class="article-summary-box-inner">
<span><p>In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays
a key role but its current assignment process is time-consuming. We introduce
DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for
improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it
with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With
an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score
of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under
the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously
reported leading models on this task, demonstrating a relative improvement in
macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to
CAML. When DRG-LLaMA is applied to predict base DRGs and complication or
comorbidity (CC) / major complication or comorbidity (MCC), the top-1
prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status.
DRG-LLaMA performance exhibits improvements in correlation with larger model
parameters and longer input context lengths. Furthermore, usage of LoRA enables
training even on smaller GPUs with 48 GB of VRAM, highlighting the viability of
adapting LLMs for DRGs prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction contract risk identification based on knowledge-augmented language model. (arXiv:2309.12626v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12626">
<div class="article-summary-box-inner">
<span><p>Contract review is an essential step in construction projects to prevent
potential losses. However, the current methods for reviewing construction
contracts lack effectiveness and reliability, leading to time-consuming and
error-prone processes. While large language models (LLMs) have shown promise in
revolutionizing natural language processing (NLP) tasks, they struggle with
domain-specific knowledge and addressing specialized issues. This paper
presents a novel approach that leverages LLMs with construction contract
knowledge to emulate the process of contract review by human experts. Our
tuning-free approach incorporates construction contract domain knowledge to
enhance language models for identifying construction contract risks. The use of
a natural language when building the domain knowledge base facilitates
practical implementation. We evaluated our method on real construction
contracts and achieved solid performance. Additionally, we investigated how
large language models employ logical thinking during the task and provide
insights and recommendations for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12646">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Natural Language Processing (NLP) have highlighted the
potential of sentence embeddings in measuring semantic similarity. Yet, its
application in analyzing real-world dyadic interactions and predicting the
affect of conversational participants remains largely uncharted. To bridge this
gap, the present study utilizes verbal conversations within 50 married couples
talking about conflicts and pleasant activities. Transformer-based model
all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from
each speaker. The overall similarity of the conversation was then quantified by
the average cosine similarity between the embeddings of adjacent utterances.
Results showed that semantic similarity had a positive association with wives'
affect during conflict (but not pleasant) conversations. Moreover, this
association was not observed with husbands' affect regardless of conversation
types. Two validation checks further provided support for the validity of the
similarity measure and showed that the observed patterns were not mere
artifacts of data. The present study underscores the potency of sentence
embeddings in understanding the association between interpersonal dynamics and
individual affect, paving the way for innovative applications in affective and
relationship sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12669">
<div class="article-summary-box-inner">
<span><p>Answering numerical questions over hybrid contents from the given tables and
text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)
have gained significant attention in the NLP community. With the emergence of
large language models, In-Context Learning and Chain-of-Thought prompting have
become two particularly popular research topics in this field. In this paper,
we introduce a new prompting strategy called Hybrid prompt strategy and
Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt
the model to develop the ability of retrieval thinking when dealing with hybrid
data. Our method achieves superior performance compared to the fully-supervised
SOTA on the MultiHiertt dataset in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JCoLA: Japanese Corpus of Linguistic Acceptability. (arXiv:2309.12676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12676">
<div class="article-summary-box-inner">
<span><p>Neural language models have exhibited outstanding performance in a range of
downstream tasks. However, there is limited understanding regarding the extent
to which these models internalize syntactic knowledge, so that various datasets
have recently been constructed to facilitate syntactic evaluation of language
models across languages. In this paper, we introduce JCoLA (Japanese Corpus of
Linguistic Acceptability), which consists of 10,020 sentences annotated with
binary acceptability judgments. Specifically, those sentences are manually
extracted from linguistics textbooks, handbooks and journal articles, and split
into in-domain data (86 %; relatively simple acceptability judgments extracted
from textbooks and handbooks) and out-of-domain data (14 %; theoretically
significant acceptability judgments extracted from journal articles), the
latter of which is categorized by 12 linguistic phenomena. We then evaluate the
syntactic knowledge of 9 different types of Japanese language models on JCoLA.
The results demonstrated that several models could surpass human performance
for the in-domain data, while no models were able to exceed human performance
for the out-of-domain data. Error analyses by linguistic phenomena further
revealed that although neural language models are adept at handling local
syntactic dependencies like argument structure, their performance wanes when
confronted with long-distance syntactic dependencies like verbal agreement and
NPI licensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12689">
<div class="article-summary-box-inner">
<span><p>Mixup is an effective data augmentation method that generates new augmented
samples by aggregating linear combinations of different original samples.
However, if there are noises or aberrant features in the original samples,
Mixup may propagate them to the augmented samples, leading to over-sensitivity
of the model to these outliers . To solve this problem, this paper proposes a
new Mixup method called AMPLIFY. This method uses the Attention mechanism of
Transformer itself to reduce the influence of noises and aberrant values in the
original samples on the prediction results, without increasing additional
trainable parameters, and the computational cost is very low, thereby avoiding
the problem of high resource consumption in common Mixup methods such as
Sentence Mixup . The experimental results show that, under a smaller
computational resource cost, AMPLIFY outperforms other Mixup methods in text
classification tasks on 7 benchmark datasets, providing new ideas and new ways
to further improve the performance of pre-trained models based on the Attention
mechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at
https://github.com/kiwi-lilo/AMPLIFY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12697">
<div class="article-summary-box-inner">
<span><p>Semantic similarity between natural language texts is typically measured
either by looking at the overlap between subsequences (e.g., BLEU) or by using
embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we
are only interested in measuring the semantic similarity, it is better to
directly predict the similarity using a fine-tuned model for such a task. Using
a fine-tuned model for the STS-B from the GLUE benchmark, we define the
STSScore approach and show that the resulting similarity is better aligned with
our expectations on a robust semantic similarity measure than other approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12727">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have had a huge impact on society due to their
impressive capabilities and vast knowledge of the world. Various applications
and tools have been created that allow users to interact with these models in a
black-box scenario. However, one limitation of this scenario is that users
cannot modify the internal knowledge of the model, and the only way to add or
modify internal knowledge is by explicitly mentioning it to the model during
the current interaction. This learning process is called in-context training,
and it refers to training that is confined to the user's current session or
context. In-context learning has significant applications, but also has
limitations that are seldom studied. In this paper, we present a study that
shows how the model can suffer from interference between information that
continually flows in the context, causing it to forget previously learned
knowledge, which can reduce the model's performance. Along with showing the
problem, we propose an evaluation benchmark based on the bAbI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models. (arXiv:2309.12763v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12763">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning (SSRL) has improved the performance
on downstream phoneme recognition versus supervised models. Training SSRL
models requires a large amount of pre-training data and this poses a challenge
for low resource languages. A common approach is transferring knowledge from
other languages. Instead, we propose to use audio augmentation to pre-train
SSRL models in a low resource condition and evaluate phoneme recognition as
downstream task. We performed a systematic comparison of augmentation
techniques, namely: pitch variation, noise addition, accented target-language
speech and other language speech. We found combined augmentations (noise/pitch)
was the best augmentation strategy outperforming accent and language knowledge
transfer. We compared the performance with various quantities and types of
pre-training data. We examined the scaling factor of augmented data to achieve
equivalent performance to models pre-trained with target domain speech. Our
findings suggest that for resource constrained languages, in-domain synthetic
augmentation can outperform knowledge transfer from accented or other language
speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models. (arXiv:2309.12767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12767">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), acting as a powerful reasoner and generator,
exhibit extraordinary performance across various natural language tasks, such
as question answering (QA). Among these tasks, Multi-Hop Question Answering
(MHQA) stands as a widely discussed category, necessitating seamless
integration between LLMs and the retrieval of external knowledge. Existing
methods employ LLM to generate reasoning paths and plans, and utilize IR to
iteratively retrieve related knowledge, but these approaches have inherent
flaws. On one hand, Information Retriever (IR) is hindered by the low quality
of generated queries by LLM. On the other hand, LLM is easily misguided by the
irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative
interaction between IR and LLM, lead to a disaster in effectiveness at the end.
To overcome above barriers, in this paper, we propose a novel pipeline for MHQA
called Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved
framework (Furthest Reasoning) and an attached module (Plan Assessor). 1)
Furthest reasoning operates by masking previous reasoning path and generated
queries for LLM, encouraging LLM generating chain of thought from scratch in
each iteration. This approach enables LLM to break the shackle built by
previous misleading thoughts and queries (if any). 2) The Plan Assessor is a
trained evaluator that selects an appropriate plan from a group of candidate
plans proposed by LLM. Our methods are evaluated on three highly recognized
public multi-hop question answering datasets and outperform state-of-the-art on
most metrics (achieving a 10%-12% in answer accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT. (arXiv:2309.12808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12808">
<div class="article-summary-box-inner">
<span><p>As a common approach to learning English, reading comprehension primarily
entails reading articles and answering related questions. However, the
complexity of designing effective exercises results in students encountering
standardized questions, making it challenging to align with individualized
learners' reading comprehension ability. By leveraging the advanced
capabilities offered by large language models, exemplified by ChatGPT, this
paper presents a novel personalized support system for reading comprehension,
referred to as ChatPRCS, based on the Zone of Proximal Development theory.
ChatPRCS employs methods including reading comprehension proficiency
prediction, question generation, and automatic evaluation, among others, to
enhance reading comprehension instruction. First, we develop a new algorithm
that can predict learners' reading comprehension abilities using their
historical data as the foundation for generating questions at an appropriate
level of difficulty. Second, a series of new ChatGPT prompt patterns is
proposed to address two key aspects of reading comprehension objectives:
question generation, and automated evaluation. These patterns further improve
the quality of generated questions. Finally, by integrating personalized
ability and reading comprehension prompt patterns, ChatPRCS is systematically
validated through experiments. Empirical results demonstrate that it provides
learners with high-quality reading comprehension questions that are broadly
aligned with expert-crafted questions at a statistical level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors. (arXiv:2309.12810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12810">
<div class="article-summary-box-inner">
<span><p>This work aims to provide an overview on the open-source multilanguage tool
called StyloMetrix. It offers stylometric text representations that cover
various aspects of grammar, syntax and lexicon. StyloMetrix covers four
languages: Polish as the primary language, English, Ukrainian and Russian. The
normalized output of each feature can become a fruitful course for machine
learning models and a valuable addition to the embeddings layer for any deep
learning algorithm. We strive to provide a concise, but exhaustive overview on
the application of the StyloMetrix vectors as well as explain the sets of the
developed linguistic features. The experiments have shown promising results in
supervised content classification with simple algorithms as Random Forest
Classifier, Voting Classifier, Logistic Regression and others. The deep
learning assessments have unveiled the usefulness of the StyloMetrix vectors at
enhancing an embedding layer extracted from Transformer architectures. The
StyloMetrix has proven itself to be a formidable source for the machine
learning and deep learning algorithms to execute different classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12829">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation is essential for echocardiography-based assessment of
cardiovascular diseases (CVDs). However, the variability among sonographers and
the inherent challenges of ultrasound images hinder precise segmentation. By
leveraging the joint representation of image and text modalities,
Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual
information, potentially aiding in accurate and explainable segmentation.
However, the lack of readily available data in echocardiography hampers the
training of VLSMs. In this study, we explore using synthetic datasets from
Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography
segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)
using seven different kinds of language prompts derived from several
attributes, automatically extracted from echocardiography images, segmentation
masks, and their metadata. Our results show improved metrics and faster
convergence when pretraining VLSMs on SDM-generated synthetic images before
finetuning on real images. The code, configs, and prompts are available at
https://github.com/naamiinepal/synthetic-boost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts. (arXiv:2309.12863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12863">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) has shown impressive performance when
trained on large-scale corpora. However, generic NMT systems have demonstrated
poor performance on out-of-domain translation. To mitigate this issue, several
domain adaptation methods have recently been proposed which often lead to
better translation quality than genetic NMT systems. While there has been some
continuous progress in NMT for English and other European languages, domain
adaption in Arabic has received little attention in the literature. The current
study, therefore, aims to explore the effectiveness of domain-specific
adaptation for Arabic MT (AMT), in yet unexplored domain, financial news
articles. To this end, we developed carefully a parallel corpus for
Arabic-English (AR- EN) translation in the financial domain for benchmarking
different domain adaptation methods. We then fine-tuned several pre-trained NMT
and Large Language models including ChatGPT-3.5 Turbo on our dataset. The
results showed that the fine-tuning is successful using just a few well-aligned
in-domain AR-EN segments. The quality of ChatGPT translation was superior than
other models based on automatic and human evaluations. To the best of our
knowledge, this is the first work on fine-tuning ChatGPT towards financial
domain transfer learning. To contribute to research in domain translation, we
made our datasets and fine-tuned models available at
https://huggingface.co/asas-ai/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12871">
<div class="article-summary-box-inner">
<span><p>High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12881">
<div class="article-summary-box-inner">
<span><p>Affect recognition, encompassing emotions, moods, and feelings, plays a
pivotal role in human communication. In the realm of conversational artificial
intelligence (AI), the ability to discern and respond to human affective cues
is a critical factor for creating engaging and empathetic interactions. This
study delves into the capacity of large language models (LLMs) to recognise
human affect in conversations, with a focus on both open-domain chit-chat
dialogues and task-oriented dialogues. Leveraging three diverse datasets,
namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from
casual conversations to clinical interviews, we evaluated and compared LLMs'
performance in affect recognition. Our investigation explores the zero-shot and
few-shot capabilities of LLMs through in-context learning (ICL) as well as
their model capacities through task-specific fine-tuning. Additionally, this
study takes into account the potential impact of automatic speech recognition
(ASR) errors on LLM predictions. With this work, we aim to shed light on the
extent to which LLMs can replicate human-like affect recognition capabilities
in conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction. (arXiv:2309.12892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12892">
<div class="article-summary-box-inner">
<span><p>Event Relation Extraction (ERE) aims to extract multiple kinds of relations
among events in texts. However, existing methods singly categorize event
relations as different classes, which are inadequately capturing the intrinsic
semantics of these relations. To comprehensively understand their intrinsic
semantics, in this paper, we obtain prototype representations for each type of
event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework
for the joint extraction of multiple kinds of event relations. Specifically,
ProtoEM extracts event relations in a two-step manner, i.e., prototype
representing and prototype matching. In the first step, to capture the
connotations of different event relations, ProtoEM utilizes examples to
represent the prototypes corresponding to these relations. Subsequently, to
capture the interdependence among event relations, it constructs a dependency
graph for the prototypes corresponding to these relations and utilized a Graph
Neural Network (GNN)-based module for modeling. In the second step, it obtains
the representations of new event pairs and calculates their similarity with
those prototypes obtained in the first step to evaluate which types of event
relations they belong to. Experimental results on the MAVEN-ERE dataset
demonstrate that the proposed ProtoEM framework can effectively represent the
prototypes of event relations and further obtain a significant improvement over
baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12931">
<div class="article-summary-box-inner">
<span><p>Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts. (arXiv:2309.12934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12934">
<div class="article-summary-box-inner">
<span><p>Recent advances in Large Language Models (LLMs) have enabled the generation
of open-ended high-quality texts, that are non-trivial to distinguish from
human-written texts. We refer to such LLM-generated texts as \emph{deepfake
texts}. There are currently over 11K text generation models in the huggingface
model repo. As such, users with malicious intent can easily use these
open-sourced LLMs to generate harmful texts and misinformation at scale. To
mitigate this problem, a computational method to determine if a given text is a
deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this
work, we investigate the more general version of the problem, known as
\emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only
determining if a given text is a deepfake text or not but also being able to
pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve
existing AA solutions by capturing more linguistic patterns in deepfake texts
by including a Topological Data Analysis (TDA) layer in the RoBERTa model. We
show the benefits of having a TDA layer when dealing with noisy, imbalanced,
and heterogeneous datasets, by extracting TDA features from the reshaped
$pooled\_output$ of RoBERTa as input. We use RoBERTa to capture contextual
representations (i.e., semantic and syntactic linguistic features), while using
TDA to capture the shape and structure of data (i.e., linguistic structures).
Finally, \textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets,
achieving up to 7\% increase in Macro F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models. (arXiv:2309.12940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12940">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems facilitate users in executing various
activities via multi-turn dialogues, but Large Language Models (LLMs) often
struggle to comprehend these intricate contexts. In this study, we propose a
novel "Self-Explanation" prompting strategy to enhance the comprehension
abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires
the model to analyze each dialogue utterance before task execution, thereby
improving performance across various dialogue-centric tasks. Experimental
results from six benchmark datasets confirm that our method consistently
outperforms other zero-shot prompts and matches or exceeds the efficacy of
few-shot prompts, demonstrating its potential as a powerful tool in enhancing
LLMs' comprehension in complex dialogue tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12960">
<div class="article-summary-box-inner">
<span><p>Nested Event Extraction (NEE) aims to extract complex event structures where
an event contains other events as its arguments recursively. Nested events
involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of
outer events and as triggers of inner events, and thus connect them into nested
structures. This special characteristic of PEs brings challenges to existing
NEE methods, as they cannot well cope with the dual identities of PEs.
Therefore, this paper proposes a new model, called PerNee, which extracts
nested events mainly based on recognizing PEs. Specifically, PerNee first
recognizes the triggers of both inner and outer events and further recognizes
the PEs via classifying the relation type between trigger pairs. In order to
obtain better representations of triggers and arguments to further improve NEE
performance, it incorporates the information of both event types and argument
roles into PerNee through prompt learning. Since existing NEE datasets (e.g.,
Genia11) are limited to specific domains and contain a narrow range of event
types with nested structures, we systematically categorize nested events in
generic domain and construct a new NEE dataset, namely ACE2005-Nest.
Experimental results demonstrate that PerNee consistently achieves
state-of-the-art performance on ACE2005-Nest, Genia11 and Genia13.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wordification: A New Way of Teaching English Spelling Patterns. (arXiv:2309.12981v1 [cs.OH])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12981">
<div class="article-summary-box-inner">
<span><p>Literacy, or the ability to read and write, is a crucial indicator of success
in life and greater society. It is estimated that 85% of people in juvenile
delinquent systems cannot adequately read or write, that more than half of
those with substance abuse issues have complications in reading or writing and
that two-thirds of those who do not complete high school lack proper literacy
skills. Furthermore, young children who do not possess reading skills matching
grade level by the fourth grade are approximately 80% likely to not catch up at
all. Many may believe that in a developed country such as the United States,
literacy fails to be an issue; however, this is a dangerous misunderstanding.
Globally an estimated 1.19 trillion dollars are lost every year due to issues
in literacy; in the USA, the loss is an estimated 300 billion. To put it in
more shocking terms, one in five American adults still fail to comprehend basic
sentences. Making matters worse, the only tools available now to correct a lack
of reading and writing ability are found in expensive tutoring or other
programs that oftentimes fail to be able to reach the required audience. In
this paper, our team puts forward a new way of teaching English spelling and
word recognitions to grade school students in the United States: Wordification.
Wordification is a web application designed to teach English literacy using
principles of linguistics applied to the orthographic and phonological
properties of words in a manner not fully utilized previously in any
computer-based teaching application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audience-specific Explanations for Machine Translation. (arXiv:2309.12998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12998">
<div class="article-summary-box-inner">
<span><p>In machine translation, a common problem is that the translation of certain
words even if translated can cause incomprehension of the target language
audience due to different cultural backgrounds. A solution to solve this
problem is to add explanations for these words. In a first step, we therefore
need to identify these words or phrases. In this work we explore techniques to
extract example explanations from a parallel corpus. However, the sparsity of
sentences containing words that need to be explained makes building the
training dataset extremely difficult. In this work, we propose a semi-automatic
technique to extract these explanations from a large parallel corpus.
Experiments on English-&gt;German language pair show that our method is able to
extract sentence so that more than 10% of the sentences contain explanation,
while only 1.9% of the original sentences contain explanations. In addition,
experiments on English-&gt;French and English-&gt;Chinese language pairs also show
similar conclusions. This is therefore an essential first automatic step to
create a explanation dataset. Furthermore we show that the technique is robust
for all three language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13007">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) still struggle with complex reasoning tasks.
Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a
multi-model multi-agent framework designed as a round table conference among
diverse LLM agents to foster diverse thoughts and discussion for improved
consensus. ReConcile enhances the reasoning capabilities of LLMs by holding
multiple rounds of discussion, learning to convince other agents to improve
their answers, and employing a confidence-weighted voting mechanism. In each
round, ReConcile initiates discussion between agents via a 'discussion prompt'
that consists of (a) grouped answers and explanations generated by each agent
in the previous round, (b) their uncertainties, and (c) demonstrations of
answer-rectifying human explanations, used for convincing other agents. This
discussion prompt enables each agent to revise their responses in light of
insights from other agents. Once a consensus is reached and the discussion
ends, ReConcile determines the final answer by leveraging the confidence of
each agent in a weighted voting scheme. We implement ReConcile with ChatGPT,
Bard, and Claude2 as the three agents. Our experimental results on various
benchmarks demonstrate that ReConcile significantly enhances the reasoning
performance of the agents (both individually and as a team), surpassing prior
single-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 on
some of these datasets. We also experiment with GPT-4 itself as one of the
agents in ReConcile and demonstrate that its initial performance also improves
by absolute 10.0% through discussion and feedback from other agents. Finally,
we also analyze the accuracy after every round and observe that ReConcile
achieves better and faster consensus between agents, compared to a multi-agent
debate baseline. Our code is available at: https://github.com/dinobby/ReConcile
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13018">
<div class="article-summary-box-inner">
<span><p>Neural network pruning offers an effective method for compressing a
multilingual automatic speech recognition (ASR) model with minimal performance
loss. However, it entails several rounds of pruning and re-training needed to
be run for each language. In this work, we propose the use of an adaptive
masking approach in two scenarios for pruning a multilingual ASR model
efficiently, each resulting in sparse monolingual models or a sparse
multilingual model (named as Dynamic ASR Pathways). Our approach dynamically
adapts the sub-network, avoiding premature decisions about a fixed sub-network
structure. We show that our approach outperforms existing pruning methods when
targeting sparse monolingual models. Further, we illustrate that Dynamic ASR
Pathways jointly discovers and trains better sub-networks (pathways) of a
single multilingual model by adapting from different sub-network
initializations, thereby reducing the need for language-specific pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation. (arXiv:2012.02420v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02420">
<div class="article-summary-box-inner">
<span><p>Patients with low health literacy usually have difficulty understanding
medical jargon and the complex structure of professional medical language.
Although some studies are proposed to automatically translate expert language
into layperson-understandable language, only a few of them focus on both
accuracy and readability aspects simultaneously in the clinical domain. Thus,
simplification of the clinical language is still a challenging task, but
unfortunately, it is not yet fully addressed in previous work. To benchmark
this task, we construct a new dataset named MedLane to support the development
and evaluation of automated clinical language simplification approaches.
Besides, we propose a new model called DECLARE that follows the human
annotation procedure and achieves state-of-the-art performance compared with
eight strong baselines. To fairly evaluate the performance, we also propose
three specific evaluation metrics. Experimental results demonstrate the utility
of the annotated MedLane dataset and the effectiveness of the proposed model
DECLARE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smoothing Entailment Graphs with Language Models. (arXiv:2208.00318v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00318">
<div class="article-summary-box-inner">
<span><p>The diversity and Zipfian frequency distribution of natural language
predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by
Open Relation Extraction (ORE). EGs are computationally efficient and
explainable models of natural language inference, but as symbolic models, they
fail if a novel premise or hypothesis vertex is missing at test-time. We
present theory and methodology for overcoming such sparsity in symbolic models.
First, we introduce a theory of optimal smoothing of EGs by constructing
transitive chains. We then demonstrate an efficient, open-domain, and
unsupervised smoothing method using an off-the-shelf Language Model to find
approximations of missing premise predicates. This improves recall by 25.1 and
16.3 percentage points on two difficult directional entailment datasets, while
raising average precision and maintaining model explainability. Further, in a
QA task we show that EG smoothing is most useful for answering questions with
lesser supporting text, where missing premise predicates are more costly.
Finally, controlled experiments with WordNet confirm our theory and show that
hypothesis smoothing is difficult, but possible in principle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13397">
<div class="article-summary-box-inner">
<span><p>Language barriers present a great challenge in our increasingly connected and
global world. Especially within the medical domain, e.g. hospital or emergency
room, communication difficulties and delays may lead to malpractice and
non-optimal patient care. In the HYKIST project, we consider patient-physician
communication, more specifically between a German-speaking physician and an
Arabic- or Vietnamese-speaking patient. Currently, a doctor can call the
Triaphon service to get assistance from an interpreter in order to help
facilitate communication. The HYKIST goal is to support the usually
non-professional bilingual interpreter with an automatic speech translation
system to improve patient care and help overcome language barriers. In this
work, we present our ASR system development efforts for this conversational
telephone speech translation task in the medical domain for two languages
pairs, data collection, various acoustic model architectures and
dialect-induced difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons learned from the evaluation of Spanish Language Models. (arXiv:2212.08390v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08390">
<div class="article-summary-box-inner">
<span><p>Given the impact of language models on the field of Natural Language
Processing, a number of Spanish encoder-only masked language models (aka BERTs)
have been trained and released. These models were developed either within large
projects using very large private corpora or by means of smaller scale academic
efforts leveraging freely available data. In this paper we present a
comprehensive head-to-head comparison of language models for Spanish with the
following results: (i) Previously ignored multilingual models from large
companies fare better than monolingual models, substantially changing the
evaluation landscape of language models in Spanish; (ii) Results across the
monolingual models are not conclusive, with supposedly smaller and inferior
models performing competitively. Based on these empirical results, we argue for
the need of more research to understand the factors underlying them. In this
sense, the effect of corpus size, quality and pre-training techniques need to
be further investigated to be able to obtain Spanish monolingual models
significantly better than the multilingual ones released by large private
companies, specially in the face of rapid ongoing progress in the field. The
recent activity in the development of language technology for Spanish is to be
welcomed, but our results show that building language models remains an open,
resource-heavy problem which requires to marry resources (monetary and/or
computational) with the best research expertise and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversation Style Transfer using Few-Shot Learning. (arXiv:2302.08362v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08362">
<div class="article-summary-box-inner">
<span><p>Conventional text style transfer approaches focus on sentence-level style
transfer without considering contextual information, and the style is described
with attributes (e.g., formality). When applying style transfer in
conversations such as task-oriented dialogues, existing approaches suffer from
these limitations as context can play an important role and the style
attributes are often difficult to define in conversations. In this paper, we
introduce conversation style transfer as a few-shot learning problem, where the
model learns to perform style transfer by observing only a few example
dialogues in the target style. We propose a novel in-context learning approach
to solve the task with style-free dialogues as a pivot. Human evaluation shows
that by incorporating multi-turn context, the model is able to match the target
style while having better appropriateness and semantic correctness compared to
utterance/sentence-level style transfer. Additionally, we show that
conversation style transfer can also benefit downstream tasks. For example, in
multi-domain intent classification tasks, the F1 scores improve after
transferring the style of training data to match the style of the test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12239">
<div class="article-summary-box-inner">
<span><p>Neural networks drive the success of natural language processing. A
fundamental property of language is its compositional structure, allowing
humans to produce forms for new meanings systematically. However, unlike
humans, neural networks notoriously struggle with systematic generalization,
and do not necessarily benefit from compositional structure in emergent
communication simulations. This poses a problem for using neural networks to
simulate human language learning and evolution, and suggests crucial
differences in the biases of the different learning systems. Here, we directly
test how neural networks compare to humans in learning and generalizing
different input languages that vary in their degree of structure. We evaluate
the memorization and generalization capabilities of a pre-trained language
model GPT-3.5 (analagous to an adult second language learner) and recurrent
neural networks trained from scratch (analaogous to a child first language
learner). Our results show striking similarities between deep neural networks
and adult human learners, with more structured linguistic input leading to more
systematic generalization and to better convergence between neural networks and
humans. These findings suggest that all the learning systems are sensitive to
the structure of languages in similar ways with compositionality being
advantageous for learning. Our findings draw a clear prediction regarding
children's learning biases, as well as highlight the challenges of automated
processing of languages spoken by small communities. Notably, the similarity
between humans and machines opens new avenues for research on language learning
and evolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>Making the contents generated by Large Language Model (LLM) such as ChatGPT,
accurate, credible and traceable is crucial, especially in complex
knowledge-intensive tasks that require multi-step reasoning and each of which
needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM
with external knowledge is good potential to solve this problem. However, where
and how to introduce IR into LLM is a big challenge. Previous work has the
disadvantage that the wrong knowledge retrieved by IR misleads the LLM or
breaks the reasoning chain of LLM. In this paper, we propose a novel framework
called Search-in-the-Chain (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the global reasoning chain called
Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the
answer to the query. Second, IR verifies the answer of each node of CoQ, it
corrects the answer that is not consistent with the retrieved information when
IR gives high confidence, which improves the credibility. Third, LLM can mark
its missing knowledge in CoQ and IR can provide this knowledge to LLM. These
three operations improve the accuracy of LLM for complex knowledge-intensive
tasks in terms of reasoning ability and knowledge. Finally, SearChain generates
the reasoning process and marks references to supporting documents for each
reasoning step, which improves traceability. SearChain transforms the topology
of reasoning from chain to tree, which can modify the reasoning direction.
Experiment shows that SearChain outperforms baselines on complex
knowledge-intensive tasks including multi-hop question-answering, slot filling,
fact checking, and long-form question-answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v5 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00969">
<div class="article-summary-box-inner">
<span><p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01711">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) trained on vast quantities of unlabelled data have
greatly advanced the field of natural language processing (NLP). In this study,
we re-visit the widely accepted notion in NLP that continued pre-training LMs
on task-related texts improves the performance of fine-tuning (FT) in
downstream tasks. Through experiments on eight single-sentence tasks and eight
sentence-pair tasks in both semi-supervised and fully-supervised settings, we
find that conventional continued pre-training does not consistently provide
benefits and can even be detrimental for sentence-pair tasks or when
prompt-based FT is used. To tackle these issues, we propose Prompt-based
Continued Pre-training (PCP), which combines the idea of instruction tuning
with conventional continued pre-training. Our approach aims to improve the
performance of prompt-based FT by presenting both task-related texts and prompt
templates to LMs through unsupervised pre-training objectives before
fine-tuning for the target task. Our empirical evaluations on 21 benchmarks
demonstrate that the PCP consistently improves the performance of
state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both
semi-supervised and fully-supervised settings, even with only hundreds of
unlabelled examples. Additionally, prompt-based FT with the PCP outperforms
state-of-the-art semi-supervised approaches with greater simplicity,
eliminating the need for an iterative process and extra data augmentation. Our
further analysis explores the performance lower bound of the PCP and reveals
that the advantages of PCP persist across different sizes of models and
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v5 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06569">
<div class="article-summary-box-inner">
<span><p>Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text and hallucinated recommendation when deciding
which item(s) to recommend, creating LLM-compatible item IDs to uniquely
identify each item is essential for recommendation foundation models. In this
study, we systematically examine the item indexing problem for recommendation
foundation models, using P5 as an example of backbone model. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our study highlights the significant influence of item
indexing methods on the performance of LLM-based recommendation, and our
results on real-world datasets validate the effectiveness of our proposed
solutions. The research also demonstrates how recent advances on language
modeling and traditional IR principles such as indexing can help each other for
better learning and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10307">
<div class="article-summary-box-inner">
<span><p>Measuring the distance between machine-produced and human language is a
critical open problem. Inspired by empirical findings from psycholinguistics on
the periodicity of entropy in language, we propose FACE, a set of metrics based
on Fourier Analysis of the estimated Cross-Entropy of language, for measuring
the similarity between model-generated and human-written languages. Based on an
open-ended generation task and the experimental data from previous studies, we
find that FACE can effectively identify the human-model gap, scales with model
size, reflects the outcomes of different sampling methods for decoding,
correlates well with other evaluation metrics and with human judgment scores.
FACE is computationally efficient and provides intuitive interpretations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation. (arXiv:2306.01966v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01966">
<div class="article-summary-box-inner">
<span><p>We present GENTLE, a new mixed-genre English challenge corpus totaling 17K
tokens and consisting of 8 unusual text types for out-of domain evaluation:
dictionary entries, esports commentaries, legal documents, medical notes,
poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually
annotated for a variety of popular NLP tasks, including syntactic dependency
parsing, entity recognition, coreference resolution, and discourse parsing. We
evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for
at least some genres in their performance on all tasks, which indicates
GENTLE's utility as an evaluation dataset for NLP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03341">
<div class="article-summary-box-inner">
<span><p>We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the truthfulness of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. (arXiv:2306.08129v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08129">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an autonomous information seeking visual question
answering framework, AVIS. Our method leverages a Large Language Model (LLM) to
dynamically strategize the utilization of external tools and to investigate
their outputs, thereby acquiring the indispensable knowledge needed to provide
answers to the posed questions. Responding to visual questions that necessitate
external knowledge, such as "What event is commemorated by the building
depicted in this image?", is a complex task. This task presents a combinatorial
search space that demands a sequence of actions, including invoking APIs,
analyzing their responses, and making informed decisions. We conduct a user
study to collect a variety of instances of human decision-making when faced
with this task. This data is then used to design a system comprised of three
components: an LLM-powered planner that dynamically determines which tool to
use next, an LLM-powered reasoner that analyzes and extracts key information
from the tool outputs, and a working memory component that retains the acquired
information throughout the process. The collected user behavior serves as a
guide for our system in two key ways. First, we create a transition graph by
analyzing the sequence of decisions made by users. This graph delineates
distinct states and confines the set of actions available at each state.
Second, we use examples of user decision-making to provide our LLM-powered
planner and reasoner with relevant contextual instances, enhancing their
capacity to make informed decisions. We show that AVIS achieves
state-of-the-art results on knowledge-intensive visual question answering
benchmarks such as Infoseek and OK-VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personality Traits in Large Language Models. (arXiv:2307.00184v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00184">
<div class="article-summary-box-inner">
<span><p>The advent of large language models (LLMs) has revolutionized natural
language processing, enabling the generation of coherent and contextually
relevant human-like text. As LLMs increasingly power conversational agents used
by the general public world-wide, the synthetic personality embedded in these
models, by virtue of training on large amounts of human data, is becoming
increasingly important. Since personality is a key factor determining the
effectiveness of communication, we present a comprehensive method for
administering and validating personality tests on widely-used LLMs, as well as
for shaping personality in the generated text of such LLMs. Applying this
method, we found: 1) personality measurements in the outputs of some LLMs under
specific prompting configurations are reliable and valid; 2) evidence of
reliability and validity of synthetic LLM personality is stronger for larger
and instruction fine-tuned models; and 3) personality in LLM outputs can be
shaped along desired dimensions to mimic specific human personality profiles.
We discuss application and ethical implications of the measurement and shaping
method, in particular regarding responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03941">
<div class="article-summary-box-inner">
<span><p>The Right to be Forgotten (RTBF) was first established as the result of the
ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and
was later included as the Right to Erasure under the General Data Protection
Regulation (GDPR) of European Union to allow individuals the right to request
personal data be deleted by organizations. Specifically for search engines,
individuals can send requests to organizations to exclude their information
from the query results. It was a significant emergent right as the result of
the evolution of technology. With the recent development of Large Language
Models (LLMs) and their use in chatbots, LLM-enabled software systems have
become popular. But they are not excluded from the RTBF. Compared with the
indexing approach used by search engines, LLMs store, and process information
in a completely different way. This poses new challenges for compliance with
the RTBF. In this paper, we explore these challenges and provide our insights
on how to implement technical solutions for the RTBF, including the use of
differential privacy, machine unlearning, model editing, and prompt
engineering. With the rapid advancement of AI and the increasing need of
regulating this powerful technology, learning from the case of RTBF can provide
valuable lessons for technical practitioners, legal experts, organizations, and
authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07706">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation with deep learning is an important and widely
studied topic because segmentation enables quantifying target structure size
and shape that can help in disease diagnosis, prognosis, surgery planning, and
understanding. Recent advances in the foundation VLMs and their adaptation to
segmentation tasks in natural images with VLSMs have opened up a unique
opportunity to build potentially powerful segmentation models for medical
images that enable providing helpful information via language prompt as input,
leverage the extensive range of other medical imaging datasets by pooled
dataset training, adapt to new classes, and be robust against
out-of-distribution data with human-in-the-loop prompting during inference.
Although transfer learning from natural to medical images for image-only
segmentation models has been studied, no studies have analyzed how the joint
representation of vision-language transfers to medical images in segmentation
problems and understand gaps in leveraging their full potential. We present the
first benchmark study on transfer learning of VLSMs to 2D medical images with
thoughtfully collected 11 existing 2D medical image datasets of diverse
modalities with carefully presented 9 types of language prompts from 14
attributes. Our results indicate that VLSMs trained in natural image-text pairs
transfer reasonably to the medical domain in zero-shot settings when prompted
appropriately for non-radiology photographic modalities; when finetuned, they
obtain comparable performance to conventional architectures, even in X-rays and
ultrasound modalities. However, the additional benefit of language prompts
during finetuning may be limited, with image features playing a more dominant
role; they can better handle training on pooled datasets combining diverse
modalities and are potentially more robust to domain shift than the
conventional segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00723">
<div class="article-summary-box-inner">
<span><p>This paper studies contextual biasing with Large Language Models (LLMs),
where during second-pass rescoring additional contextual information is
provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We
propose to leverage prompts for a LLM without fine tuning during rescoring
which incorporate a biasing list and few-shot examples to serve as additional
information when calculating the score for the hypothesis. In addition to
few-shot prompt learning, we propose multi-task training of the LLM to predict
both the entity class and the next token. To improve the efficiency for
contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we
propose dynamic prompting, where we select the most likely class using the
class tag prediction, and only use entities in this class as contexts for next
token prediction. Word Error Rate (WER) evaluation is performed on i) an
internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli
dataset. Results indicate that biasing lists and few-shot examples can achieve
17.8% and 9.6% relative improvement compared to first pass ASR, and that
multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative
WER improvement, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08636">
<div class="article-summary-box-inner">
<span><p>Historically, proficient writing was deemed essential for human advancement,
with creative expression viewed as one of the hallmarks of human achievement.
However, recent advances in generative AI have marked an inflection point in
this narrative, including for scientific writing. This article provides a
comprehensive analysis of the capabilities and limitations of six AI chatbots
in scholarly writing in the humanities and archaeology. The methodology was
based on tagging AI generated content for quantitative accuracy and qualitative
precision by human experts. Quantitative accuracy assessed the factual
correctness, while qualitative precision gauged the scientific contribution.
While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in
recombining existing knowledge, they failed in generating original scientific
content. As a side note, our results also suggest that with ChatGPT-4 the size
of the LLMs has plateaued. Furthermore, the paper underscores the intricate and
recursive nature of human research. This process of transforming raw data into
refined knowledge is computationally irreducible, which highlights the
challenges AI chatbots face in emulating human originality in scientific
writing. In conclusion, while large language models have revolutionised content
generation, their ability to produce original scientific contributions in the
humanities remains limited. We expect that this will change in the near future
with the evolution of current LLM-based AI chatbots towards LLM-powered
software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09357">
<div class="article-summary-box-inner">
<span><p>Despite the plethora of telehealth applications to assist home-based older
adults and healthcare providers, basic messaging and phone calls are still the
most common communication methods, which suffer from limited availability,
information loss, and process inefficiencies. One promising solution to
facilitate patient-provider communication is to leverage large language models
(LLMs) with their powerful natural conversation and summarization capability.
However, there is a limited understanding of LLMs' role during the
communication. We first conducted two interview studies with both older adults
(N=10) and healthcare providers (N=9) to understand their needs and
opportunities for LLMs in patient-provider asynchronous communication. Based on
the insights, we built an LLM-powered communication system, Talk2Care, and
designed interactive components for both groups: (1) For older adults, we
leveraged the convenience and accessibility of voice assistants (VAs) and built
an LLM-powered VA interface for effective information collection. (2) For
health providers, we built an LLM-based dashboard to summarize and present
important health information based on older adults' conversations with the VA.
We further conducted two user studies with older adults and providers to
evaluate the usability of the system. The results showed that Talk2Care could
facilitate the communication process, enrich the health information collected
from older adults, and considerably save providers' efforts and time. We
envision our work as an initial exploration of LLMs' capability in the
intersection of healthcare and interpersonal communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10654">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated great potential in natural
language processing tasks within the financial domain. In this work, we present
a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,
which includes a dataset~(CFData) for pre-training and supervised fine-tuning,
a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment
framework~(CFAPP) designed to navigate real-world financial applications. The
CFData comprising both a pre-training dataset and a supervised fine-tuning
dataset, where the pre-training dataset collates Chinese financial data and
analytics, alongside a smaller subset of general-purpose text with 584M
documents and 141B tokens in total, and the supervised fine-tuning dataset is
tailored for six distinct financial tasks, embodying various facets of
financial analysis and decision-making with 1.5M instruction pairs and 1.5B
tokens in total. The CFLLM, which is based on InternLM-7B to balance the model
capability and size, is trained on CFData in two stage, continued pre-training
and supervised fine-tuning. The CFAPP is centered on large language models
(LLMs) and augmented with additional modules to ensure multifaceted
functionality in real-world application. Our codes are released at
https://github.com/TongjiFinLab/CFGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Contrastive based Fine-tuning. (arXiv:2309.11895v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11895">
<div class="article-summary-box-inner">
<span><p>Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11911">
<div class="article-summary-box-inner">
<span><p>The development of emotion recognition in dialogue (ERC) has been
consistently hindered by the complexity of pipeline designs, leading to ERC
models that often overfit to specific datasets and dialogue patterns. In this
study, we propose a novel approach, namely
</p>
<p>InstructERC, to reformulates the ERC task from a discriminative framework to
a generative framework based on Large Language Models (LLMs) . InstructERC has
two significant contributions: Firstly, InstructERC introduces a simple yet
effective retrieval template module, which helps the model explicitly integrate
multi-granularity dialogue supervision information by concatenating the
historical dialog content, label statement, and emotional domain demonstrations
with high semantic similarity. Furthermore, we introduce two additional emotion
alignment tasks, namely speaker identification and emotion prediction tasks, to
implicitly model the dialogue role relationships and future emotional
tendencies in conversations. Our LLM-based plug-and-play plugin framework
significantly outperforms all previous models and achieves comprehensive SOTA
on three commonly used ERC datasets. Extensive analysis of parameter-efficient
and data-scaling experiments provide empirical guidance for applying
InstructERC in practical scenarios. Our code will be released after blind
review.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11981">
<div class="article-summary-box-inner">
<span><p>In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11998">
<div class="article-summary-box-inner">
<span><p>Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
https://huggingface.co/datasets/lmsys/lmsys-chat-1m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12053">
<div class="article-summary-box-inner">
<span><p>This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic instructions and GPT-4
responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using
a reward model that is sensitive to local culture and values. The objective is
to train culturally aware and value-aligned Arabic LLMs that can serve the
diverse application-specific needs of Arabic-speaking communities.
</p>
<p>Extensive evaluations demonstrated that the resulting LLM called `AceGPT' is
the SOTA open Arabic LLM in various benchmarks, including instruction-following
benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval), knowledge benchmark
(i.e., Arabic MMLU and EXAMs), as well as the newly-proposed Arabic cultural \&amp;
value alignment benchmark. Notably, AceGPT outperforms ChatGPT in the popular
Vicuna-80 benchmark when evaluated with GPT-4, despite the benchmark's limited
scale. % Natural Language Understanding (NLU) benchmark (i.e., ALUE)
</p>
<p>Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12284">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (e.g., LLaMA-2) are still far
away from satisfactory for solving mathematical problem due to the complex
reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%
on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same
size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of
82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA
dataset, the MetaMath models with different model sizes and the training code
for public use.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-25 23:10:51.560394370 UTC">2023-09-25 23:10:51 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>