<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-31T01:30:00Z">10-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">COMET-QE and Active Learning for Low-Resource Machine Translation. (arXiv:2210.15696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15696">
<div class="article-summary-box-inner">
<span><p>Active learning aims to deliver maximum benefit when resources are scarce. We
use COMET-QE, a reference-free evaluation metric, to select sentences for
low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish
for our experiments, we show that COMET-QE significantly outperforms two
variants of Round Trip Translation Likelihood (RTTL) and random sentence
selection by up to 5 BLEU points for 20k sentences selected by Active Learning
on a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence
selection in the very low-resource limit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulating realistic speech overlaps improves multi-talker ASR. (arXiv:2210.15715v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15715">
<div class="article-summary-box-inner">
<span><p>Multi-talker automatic speech recognition (ASR) has been studied to generate
transcriptions of natural conversation including overlapping speech of multiple
speakers. Due to the difficulty in acquiring real conversation data with
high-quality human transcriptions, a na\"ive simulation of multi-talker speech
by randomly mixing multiple utterances was conventionally used for model
training. In this work, we propose an improved technique to simulate
multi-talker overlapping speech with realistic speech overlaps, where an
arbitrary pattern of speech overlaps is represented by a sequence of discrete
tokens. With this representation, speech overlapping patterns can be learned
from real conversations based on a statistical language model, such as N-gram,
which can be then used to generate multi-talker speech for training. In our
experiments, multi-talker ASR models trained with the proposed method show
consistent improvement on the word error rates across multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation. (arXiv:2210.15718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15718">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown impressive results on a variety of
text understanding tasks. Search queries though pose a unique challenge, given
their short-length and lack of nuance or context. Complicated feature
engineering efforts do not always lead to downstream improvements as their
performance benefits may be offset by increased complexity of knowledge
distillation. Thus, in this paper we make the following contributions: (1) We
demonstrate that Retrieval Augmentation of queries provides LLMs with valuable
additional context enabling improved understanding. While Retrieval
Augmentation typically increases latency of LMs (thus hurting distillation
efficacy), (2) we provide a practical and effective way of distilling Retrieval
Augmentation LLMs. Specifically, we use a novel two-stage distillation approach
that allows us to carry over the gains of retrieval augmentation, without
suffering the increased compute typically associated with it. (3) We
demonstrate the benefits of the proposed approach (QUILL) on a billion-scale,
real-world query understanding system resulting in huge gains. Via extensive
experiments, including on public benchmarks, we believe this work offers a
recipe for practical use of retrieval-augmented query understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models. (arXiv:2210.15734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15734">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) systems are gaining popularity
over cascaded approaches due to their simplicity and ability to avoid error
propagation. However, these systems model sequence labeling as a sequence
prediction task causing a divergence from its well-established token-level
tagging formulation. We build compositional end-to-end SLU systems that
explicitly separate the added complexity of recognizing spoken mentions in SLU
from the NLU task of sequence labeling. By relying on intermediate decoders
trained for ASR, our end-to-end systems transform the input modality from
speech to token-level representations that can be used in the traditional
sequence labeling framework. This composition of ASR and NLU formulations in
our end-to-end SLU system offers direct compatibility with pre-trained ASR and
NLU systems, allows performance monitoring of individual components and enables
the use of globally normalized losses like CRF, making them attractive in
practical scenarios. Our models outperform both cascaded and direct end-to-end
models on a labeling task of named entity recognition across SLU benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge. (arXiv:2210.15759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15759">
<div class="article-summary-box-inner">
<span><p>Recent progress in self-supervised or unsupervised machine learning has
opened the possibility of building a full speech processing system from raw
audio without using any textual representations or expert labels such as
phonemes, dictionaries or parse trees. The contribution of the Zero Resource
Speech Challenge series since 2015 has been to break down this long-term
objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term
Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce
associated metrics and benchmarks enabling model comparison and cumulative
progress. We present an overview of the six editions of this challenge series
since 2015, discuss the lessons learned, and outline the areas which need more
work or give puzzling results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest Neighbor Language Models for Stylistic Controllable Generation. (arXiv:2210.15762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15762">
<div class="article-summary-box-inner">
<span><p>Recent language modeling performance has been greatly improved by the use of
external memory. This memory encodes the context so that similar contexts can
be recalled during decoding. This similarity depends on how the model learns to
encode context, which can be altered to include other attributes, such as
style. We construct and evaluate an architecture for this purpose, using
corpora annotated for politeness, formality, and toxicity. Through extensive
experiments and human evaluation we demonstrate the potential of our method to
generate text while controlling style. We find that style-specific datastores
improve generation performance, though results vary greatly across styles, and
the effect of pretraining data and specific styles should be explored in future
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating context-invariance in unsupervised speech representations. (arXiv:2210.15775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15775">
<div class="article-summary-box-inner">
<span><p>Unsupervised speech representations have taken off, with benchmarks (SUPERB,
ZeroSpeech) demonstrating major progress on semi-supervised speech recognition,
speech synthesis, and speech-only language modelling. Inspiration comes from
the promise of ``discovering the phonemes'' of a language or a similar
low-bitrate encoding. However, one of the critical properties of phoneme
transcriptions is context-invariance: the phonetic context of a speech sound
can have massive influence on the way it is pronounced, while the text remains
stable. This is what allows tokens of the same word to have the same
transcriptions -- key to language understanding. Current benchmarks do not
measure context-invariance. We develop a new version of the ZeroSpeech ABX
benchmark that measures context-invariance, and apply it to recent
self-supervised representations. We demonstrate that the context-independence
of representations is predictive of the stability of word-level
representations. We suggest research concentrate on improving
context-independence of self-supervised and unsupervised representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Question Rewriting for Conversational Question Answering. (arXiv:2210.15777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15777">
<div class="article-summary-box-inner">
<span><p>Conversational Question Answering (CQA) aims to answer questions contained
within dialogues, which are not easily interpretable without context.
Developing a model to rewrite conversational questions into self-contained ones
is an emerging solution in industry settings as it allows using existing
single-turn QA systems to avoid training a CQA model from scratch. Previous
work trains rewriting models using human rewrites as supervision. However, such
objectives are disconnected with QA models and therefore more human-like
rewrites do not guarantee better QA performance. In this paper we propose using
QA feedback to supervise the rewriting model with reinforcement learning.
Experiments show that our approach can effectively improve QA performance over
baselines for both extractive and retrieval QA. Furthermore, human evaluation
shows that our method can generate more accurate and detailed rewrites when
compared to human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmberNet: A Compact End-to-End Model for Spoken Language Identification. (arXiv:2210.15781v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15781">
<div class="article-summary-box-inner">
<span><p>We present AmberNet, a compact end-to-end neural network for Spoken Language
Identification. AmberNet consists of 1D depth-wise separable convolutions and
Squeeze-and-Excitation layers with global context, followed by statistics
pooling and linear layers. AmberNet achieves performance similar to
state-of-the-art(SOTA) models on VoxLingua107 dataset, while being 10x smaller.
AmberNet can be adapted to unseen languages and new acoustic conditions with
simple finetuning. It attains SOTA accuracy of 75.8% on FLEURS benchmark. We
show the model is easily scalable to achieve a better trade-off between
accuracy and speed. We further inspect the model's sensitivity to input length
and show that AmberNet performs well even on short utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15842">
<div class="article-summary-box-inner">
<span><p>Detecting emotions expressed in text has become critical to a range of
fields. In this work, we investigate ways to exploit label correlations in
multi-label emotion recognition models to improve emotion detection. First, we
develop two modeling approaches to the problem in order to capture word
associations of the emotion words themselves, by either including the emotions
in the input, or by leveraging Masked Language Modeling (MLM). Second, we
integrate pairwise constraints of emotion representations as regularization
terms alongside the classification loss of the models. We split these terms
into two categories, local and global. The former dynamically change based on
the gold labels, while the latter remain static during training. We demonstrate
state-of-the-art performance across Spanish, English, and Arabic in SemEval
2018 Task 1 E-c using monolingual BERT-based models. On top of better
performance, we also demonstrate improved robustness. Code is available at
https://github.com/gchochla/Demux-MEmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction. (arXiv:2210.15843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15843">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mappings. (arXiv:2210.15851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15851">
<div class="article-summary-box-inner">
<span><p>The many-to-many multilingual neural machine translation can translate
between language pairs unseen during training, i.e., zero-shot translation.
Improving zero-shot translation requires the model to learn universal
representations and cross-mapping relationships to transfer the knowledge
learned on the supervised directions to the zero-shot directions. In this work,
we propose the state mover's distance based on the optimal theory to model the
difference of the representations output by the encoder. Then, we bridge the
gap between the semantic-equivalent representations of different languages at
the token level by minimizing the proposed distance to learn universal
representations. Besides, we propose an agreement-based training scheme, which
can help the model make consistent predictions based on the semantic-equivalent
sentences to learn universal cross-mapping relationships for all translation
directions. The experimental results on diverse multilingual datasets show that
our method can improve consistently compared with the baseline system and other
contrast methods. The analysis proves that our method can better align the
semantic space and improve the prediction consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM. (arXiv:2210.15859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15859">
<div class="article-summary-box-inner">
<span><p>Retrieval-enhanced language models (LMs), which condition their predictions
on text retrieved from large external datastores, have recently shown
significant perplexity improvements compared to standard LMs. One such
approach, the $k$NN-LM, interpolates any existing LM's predictions with the
output of a $k$-nearest neighbors model and requires no additional training. In
this paper, we explore the importance of lexical and semantic matching in the
context of items retrieved by $k$NN-LM. We find two trends: (1) the presence of
large overlapping $n$-grams between the datastore and evaluation set plays an
important factor in strong performance, even when the datastore is derived from
the training data; and (2) the $k$NN-LM is most beneficial when retrieved items
have high semantic similarity with the query. Based on our analysis, we define
a new formulation of the $k$NN-LM that uses retrieval quality to assign the
interpolation coefficient. We empirically measure the effectiveness of our
approach on two English language modeling datasets, Wikitext-103 and PG-19. Our
re-formulation of the $k$NN-LM is beneficial in both cases, and leads to nearly
4% improvement in perplexity on the Wikitext-103 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation of Machine Translation with Crowdworkers. (arXiv:2210.15861v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15861">
<div class="article-summary-box-inner">
<span><p>Although a machine translation model trained with a large in-domain parallel
corpus achieves remarkable results, it still works poorly when no in-domain
data are available. This situation restricts the applicability of machine
translation when the target domain's data are limited. However, there is great
demand for high-quality domain-specific machine translation models for many
domains. We propose a framework that efficiently and effectively collects
parallel sentences in a target domain from the web with the help of
crowdworkers. With the collected parallel data, we can quickly adapt a machine
translation model to the target domain. Our experiments show that the proposed
method can collect target-domain parallel data over a few days at a reasonable
cost. We tested it with five domains, and the domain-adapted model improved the
BLEU scores to +19.7 by an average of +7.8 points compared to a general-purpose
translation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation. (arXiv:2210.15868v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15868">
<div class="article-summary-box-inner">
<span><p>Adapting a neural text-to-speech (TTS) model to a target speaker typically
involves fine-tuning most if not all of the parameters of a pretrained
multi-speaker backbone model. However, serving hundreds of fine-tuned neural
TTS models is expensive as each of them requires significant footprint and
separate computational resources (e.g., accelerators, memory). To scale speaker
adapted neural TTS voices to hundreds of speakers while preserving the
naturalness and speaker similarity, this paper proposes a parameter-efficient
few-shot speaker adaptation, where the backbone model is augmented with
trainable lightweight modules called residual adapters. This architecture
allows the backbone model to be shared across different target speakers.
Experimental results show that the proposed approach can achieve competitive
naturalness and speaker similarity compared to the full fine-tuning approaches,
while requiring only $\sim$0.1% of the backbone model parameters for each
speaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"It's Not Just Hate'': A Multi-Dimensional Perspective on Detecting Harmful Speech Online. (arXiv:2210.15870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15870">
<div class="article-summary-box-inner">
<span><p>Well-annotated data is a prerequisite for good Natural Language Processing
models. Too often, though, annotation decisions are governed by optimizing time
or annotator agreement. We make a case for nuanced efforts in an
interdisciplinary setting for annotating offensive online speech. Detecting
offensive content is rapidly becoming one of the most important real-world NLP
tasks. However, most datasets use a single binary label, e.g., for hate or
incivility, even though each concept is multi-faceted. This modeling choice
severely limits nuanced insights, but also performance. We show that a more
fine-grained multi-label approach to predicting incivility and hateful or
intolerant content addresses both conceptual and performance issues. We release
a novel dataset of over 40,000 tweets about immigration from the US and UK,
annotated with six labels for different aspects of incivility and intolerance.
Our dataset not only allows for a more nuanced understanding of harmful speech
online, models trained on it also outperform or match performance on benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving short-video speech recognition using random utterance concatenation. (arXiv:2210.15876v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15876">
<div class="article-summary-box-inner">
<span><p>One of the limitations in end-to-end automatic speech recognition framework
is its performance would be compromised if train-test utterance lengths are
mismatched. In this paper, we propose a random utterance concatenation (RUC)
method to alleviate train-test utterance length mismatch issue for short-video
speech recognition task. Specifically, we are motivated by observations our
human-transcribed training utterances tend to be much shorter for short-video
spontaneous speech (~3 seconds on average), while our test utterance generated
from voice activity detection front-end is much longer (~10 seconds on
average). Such a mismatch can lead to sub-optimal performance. Experimentally,
by using the proposed RUC method, the best word error rate reduction (WERR) can
be achieved with around three fold training data size increase as well as two
utterance concatenation for each. In practice, the proposed method consistently
outperforms the strong baseline models, where 3.64% average WERR is achieved on
14 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Current Explainability Help Provide References in Clinical Notes to Support Humans Annotate Medical Codes?. (arXiv:2210.15882v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15882">
<div class="article-summary-box-inner">
<span><p>The medical codes prediction problem from clinical notes has received
substantial interest in the NLP community, and several recent studies have
shown the state-of-the-art (SOTA) code prediction results of full-fledged deep
learning-based methods. However, most previous SOTA works based on deep
learning are still in early stages in terms of providing textual references and
explanations of the predicted codes, despite the fact that this level of
explainability of the prediction outcomes is critical to gaining trust from
professional medical coders. This raises the important question of how well
current explainability methods apply to advanced neural network models such as
transformers to predict correct codes and present references in clinical notes
that support code prediction. First, we present an explainable Read, Attend,
and Code (xRAC) framework and assess two approaches, attention score-based
xRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through
simplified but thorough human-grounded evaluations with SOTA transformer-based
model, RAC. We find that the supporting evidence text highlighted by xRAC-ATTN
is of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in
production deployment scenarios. More importantly, we show for the first time
that, given the current state of explainability methodologies, using the SOTA
medical codes prediction system still requires the expertise and competencies
of professional coders, even though its prediction accuracy is superior to that
of human coders. This, we believe, is a very meaningful step toward developing
explainable and accurate machine learning systems for fully autonomous medical
code prediction from clinical notes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels. (arXiv:2210.15893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15893">
<div class="article-summary-box-inner">
<span><p>Deployed dialogue agents have the potential to integrate human feedback to
continuously improve themselves. However, humans may not always provide
explicit signals when the chatbot makes mistakes during interactions. In this
work, we propose Juicer, a framework to make use of both binary and free-form
textual human feedback. It works by: (i) extending sparse binary feedback by
training a satisfaction classifier to label the unlabeled data; and (ii)
training a reply corrector to map the bad replies to good ones. We find that
augmenting training with model-corrected replies improves the final dialogue
model, and we can further improve performance by using both positive and
negative replies through the recently proposed Director model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis. (arXiv:2210.15937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15937">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness and implementation of
modality-specific large-scale pre-trained encoders for multimodal sentiment
analysis~(MSA). Although the effectiveness of pre-trained encoders in various
fields has been reported, conventional MSA methods employ them for only
linguistic modality, and their application has not been investigated. This
paper compares the features yielded by large-scale pre-trained encoders with
conventional heuristic features. One each of the largest pre-trained encoders
publicly available for each modality are used; CLIP-ViT, WavLM, and BERT for
visual, acoustic, and linguistic modalities, respectively. Experiments on two
datasets reveal that methods with domain-specific pre-trained encoders attain
better performance than those with conventional features in both unimodal and
multimodal scenarios. We also find it better to use the outputs of the
intermediate layers of the encoders than those of the output layer. The codes
are available at https://github.com/ando-hub/MSA_Pretrain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoChBert: Towards Robust BERT Fine-tuning for Chinese. (arXiv:2210.15944v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15944">
<div class="article-summary-box-inner">
<span><p>Despite of the superb performance on a wide range of tasks, pre-trained
language models (e.g., BERT) have been proved vulnerable to adversarial texts.
In this paper, we present RoChBERT, a framework to build more Robust BERT-based
models by utilizing a more comprehensive adversarial graph to fuse Chinese
phonetic and glyph features into pre-trained representations during
fine-tuning. Inspired by curriculum learning, we further propose to augment the
training dataset with adversarial texts in combination with intermediate
samples. Extensive experiments demonstrate that RoChBERT outperforms previous
methods in significant ways: (i) robust -- RoChBERT greatly improves the model
robustness without sacrificing accuracy on benign texts. Specifically, the
defense lowers the success rates of unlimited and limited attacks by 59.43% and
39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible --
RoChBERT can easily extend to various language models to solve different
downstream tasks with excellent performance; and (iii) efficient -- RoChBERT
can be directly applied to the fine-tuning stage without pre-training language
model from scratch, and the proposed data augmentation method is also low-cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stanceosaurus: Classifying Stance Towards Multilingual Misinformation. (arXiv:2210.15954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15954">
<div class="article-summary-box-inner">
<span><p>We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi,
and Arabic annotated with stance towards 251 misinformation claims. As far as
we are aware, it is the largest corpus annotated with stance towards
misinformation claims. The claims in Stanceosaurus originate from 15
fact-checking sources that cover diverse geographical regions and cultures.
Unlike existing stance datasets, we introduce a more fine-grained 5-class
labeling strategy with additional subcategories to distinguish implicit stance.
Pre-trained transformer-based stance classifiers that are fine-tuned on our
corpus show good generalization on unseen claims and regional claims from
countries outside the training data. Cross-lingual experiments demonstrate
Stanceosaurus' capability of training multi-lingual models, achieving 53.1 F1
on Hindi and 50.4 F1 on Arabic without any target-language fine-tuning.
Finally, we show how a domain adaptation method can be used to improve
performance on Stanceosaurus using additional RumourEval-2019 data. We make
Stanceosaurus publicly available to the research community and hope it will
encourage further work on misinformation identification across languages and
cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEBERT: Efficient and robust binary ensemble BERT. (arXiv:2210.15976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15976">
<div class="article-summary-box-inner">
<span><p>Pre-trained BERT models have achieved impressive accuracy on natural language
processing (NLP) tasks. However, their excessive amount of parameters hinders
them from efficient deployment on edge devices. Binarization of the BERT models
can significantly alleviate this issue but comes with a severe accuracy drop
compared with their full-precision counterparts. In this paper, we propose an
efficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.
To the best of our knowledge, this is the first work employing ensemble
techniques on binary BERTs, yielding BEBERT, which achieves superior accuracy
while retaining computational efficiency. Furthermore, we remove the knowledge
distillation procedures during ensemble to speed up the training process
without compromising accuracy. Experimental results on the GLUE benchmark show
that the proposed BEBERT significantly outperforms the existing binary BERT
models in accuracy and robustness with a 2x speedup on training time. Moreover,
our BEBERT has only a negligible accuracy loss of 0.3% compared to the
full-precision baseline while saving 15x and 13x in FLOPs and model size,
respectively. In addition, BEBERT also outperforms other compressed BERTs in
accuracy by up to 6.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of a rule-based lemmatization algorithm through Finite State Machine for Uzbek language. (arXiv:2210.16006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16006">
<div class="article-summary-box-inner">
<span><p>Lemmatization is one of the core concepts in natural language processing,
thus creating a lemmatization tool is an important task. This paper discusses
the construction of a lemmatization algorithm for the Uzbek language. The main
purpose of the work is to remove affixes of words in the Uzbek language by
means of the finite state machine and to identify a lemma (a word that can be
found in the dictionary) of the word. The process of removing affixes uses a
database of affixes and part of speech knowledge. This lemmatization consists
of the general rules and a part of speech data of the Uzbek language, affixes,
classification of affixes, removing affixes on the basis of the finite state
machine for each class, as well as a definition of this word lemma.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UzbekStemmer: Development of a Rule-Based Stemming Algorithm for Uzbek Language. (arXiv:2210.16011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16011">
<div class="article-summary-box-inner">
<span><p>In this paper we present a rule-based stemming algorithm for the Uzbek
language. Uzbek is an agglutinative language, so many words are formed by
adding suffixes, and the number of suffixes is also large. For this reason, it
is difficult to find a stem of words. The methodology is proposed for doing the
stemming of the Uzbek words with an affix stripping approach whereas not
including any database of the normal word forms of the Uzbek language. Word
affixes are classified into fifteen classes and designed as finite state
machines (FSMs) for each class according to morphological rules. We created
fifteen FSMs and linked them together to create the Basic FSM. A lexicon of
affixes in XML format was created and a stemming application for Uzbek words
has been developed based on the FSMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Phrase Break of ESL speech with Pre-trained Language Models. (arXiv:2210.16029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16029">
<div class="article-summary-box-inner">
<span><p>This work introduces an approach to assessing phrase break in ESL learners'
speech with pre-trained language models (PLMs). Different with traditional
methods, this proposal converts speech to token sequences, and then leverages
the power of PLMs. There are two sub-tasks: overall assessment of phrase break
for a speech clip; fine-grained assessment of every possible phrase break
position. Speech input is first force-aligned with texts, then pre-processed to
a token sequence, including words and associated phrase break information. The
token sequence is then fed into the pre-training and fine-tuning pipeline. In
pre-training, a replaced break token detection module is trained with token
data where each token has a certain percentage chance to be randomly replaced.
In fine-tuning, overall and fine-grained scoring are optimized with text
classification and sequence labeling pipeline, respectively. With the
introduction of PLMs, the dependence on labeled training data has been greatly
reduced, and performance has improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16031">
<div class="article-summary-box-inner">
<span><p>Diffusion generative models have recently greatly improved the power of
text-conditioned image generation. Existing image generation models mainly
include text conditional diffusion model and cross-modal guided diffusion
model, which are good at small scene image generation and complex scene image
generation respectively. In this work, we propose a simple yet effective
approach, namely UPainting, to unify simple and complex scene image generation,
as shown in Figure~\ref{fig:leading_samples}. Based on architecture
improvements and diverse guidance schedules, UPainting effectively integrates
cross-modal guidance from a pretrained image-text matching model into a text
conditional diffusion model that utilizes a pretrained Transformer language
model as the text encoder. Our key findings is that combining the power of
large-scale Transformer language model in understanding language and image-text
matching model in capturing cross-modal semantics and style, is effective to
improve sample fidelity and image-text alignment of image generation. In this
way, UPainting has a more general image generation capability, which can
generate images of both simple and complex scenes more effectively. %On the
COCO dataset, UPainting achieves much better performance than Stable Diffusion,
one of the state-of-the-art text-to-image diffusion models. To comprehensively
compare text-to-image models, we further create a more general benchmark,
UniBench, with well-written Chinese and English prompts in both simple and
complex scenes. We compare UPainting with recent models and find that UPainting
greatly outperforms other models in terms of caption similarity and image
fidelity in both simple and complex scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16043">
<div class="article-summary-box-inner">
<span><p>Given the strong results of self-supervised models on various tasks, there
have been surprisingly few studies exploring self-supervised representations
for acoustic word embeddings (AWE), fixed-dimensional vectors representing
variable-length spoken word segments. In this work, we study several
pre-trained models and pooling methods for constructing AWEs with
self-supervised representations. Owing to the contextualized nature of
self-supervised representations, we hypothesize that simple pooling methods,
such as averaging, might already be useful for constructing AWEs. When
evaluating on a standard word discrimination task, we find that HuBERT
representations with mean-pooling rival the state of the art on English AWEs.
More surprisingly, despite being trained only on English, HuBERT
representations evaluated on Xitsonga, Mandarin, and French consistently
outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on
English).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards zero-shot Text-based voice editing using acoustic context conditioning, utterance embeddings, and reference encoders. (arXiv:2210.16045v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16045">
<div class="article-summary-box-inner">
<span><p>Text-based voice editing (TBVE) uses synthetic output from text-to-speech
(TTS) systems to replace words in an original recording. Recent work has used
neural models to produce edited speech that is similar to the original speech
in terms of clarity, speaker identity, and prosody. However, one limitation of
prior work is the usage of finetuning to optimise performance: this requires
further model training on data from the target speaker, which is a costly
process that may incorporate potentially sensitive data into server-side
models. In contrast, this work focuses on the zero-shot approach which avoids
finetuning altogether, and instead uses pretrained speaker verification
embeddings together with a jointly trained reference encoder to encode
utterance-level information that helps capture aspects such as speaker identity
and prosody. Subjective listening tests find that both utterance embeddings and
a reference encoder improve the continuity of speaker identity and prosody
between the edited synthetic speech and unedited original recording in the
zero-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network based Formation of Cognitive Maps of Semantic Spaces and the Emergence of Abstract Concepts. (arXiv:2210.16062v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16062">
<div class="article-summary-box-inner">
<span><p>The hippocampal-entorhinal complex plays a major role in the organization of
memory and thought. The formation of and navigation in cognitive maps of
arbitrary mental spaces via place and grid cells can serve as a representation
of memories and experiences and their relations to each other. The multi-scale
successor representation is proposed to be the mathematical principle
underlying place and grid cell computations. Here, we present a neural network,
which learns a cognitive map of a semantic space based on 32 different animal
species encoded as feature vectors. The neural network successfully learns the
similarities between different animal species, and constructs a cognitive map
of 'animal space' based on the principle of successor representations with an
accuracy of around 30% which is near to the theoretical maximum regarding the
fact that all animal species have more than one possible successor, i.e.
nearest neighbor in feature space. Furthermore, a hierarchical structure, i.e.
different scales of cognitive maps, can be modeled based on multi-scale
successor representations. We find that, in fine-grained cognitive maps, the
animal vectors are evenly distributed in feature space. In contrast, in
coarse-grained maps, animal vectors are highly clustered according to their
biological class, i.e. amphibians, mammals and insects. This could be a
possible mechanism explaining the emergence of new abstract semantic concepts.
Finally, even completely new or incomplete input can be represented by
interpolation of the representations from the cognitive map with remarkable
high accuracy of up to 95%. We conclude that the successor representation can
serve as a weighted pointer to past memories and experiences, and may therefore
be a crucial building block for future machine learning to include prior
knowledge, and to derive context knowledge from novel input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DORE: Document Ordered Relation Extraction based on Generative Framework. (arXiv:2210.16064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16064">
<div class="article-summary-box-inner">
<span><p>In recent years, there is a surge of generation-based information extraction
work, which allows a more direct use of pre-trained language models and
efficiently captures output dependencies. However, previous generative methods
using lexical representation do not naturally fit document-level relation
extraction (DocRE) where there are multiple entities and relational facts. In
this paper, we investigate the root cause of the underwhelming performance of
the existing generative DocRE models and discover that the culprit is the
inadequacy of the training paradigm, instead of the capacities of the models.
We propose to generate a symbolic and ordered sequence from the relation matrix
which is deterministic and easier for model to learn. Moreover, we design a
parallel row generation method to process overlong target sequences. Besides,
we introduce several negative sampling strategies to improve the performance
with balanced signals. Experimental results on four datasets show that our
proposed method can improve the performance of the generative DocRE models. We
have released our code at https://github.com/ayyyq/DORE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Masks: A New Framework for Shortcut Mitigation in NLU. (arXiv:2210.16079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16079">
<div class="article-summary-box-inner">
<span><p>Debiasing language models from unwanted behaviors in Natural Language
Understanding tasks is a topic with rapidly increasing interest in the NLP
community. Spurious statistical correlations in the data allow models to
perform shortcuts and avoid uncovering more advanced and desirable linguistic
features. A multitude of effective debiasing approaches has been proposed, but
flexibility remains a major issue. For the most part, models must be retrained
to find a new set of weights with debiased behavior. We propose a new debiasing
method in which we identify debiased pruning masks that can be applied to a
finetuned model. This enables the selective and conditional application of
debiasing behaviors. We assume that bias is caused by a certain subset of
weights in the network; our method is, in essence, a mask search to identify
and remove biased weights. Our masks show equivalent or superior performance to
the standard counterparts, while offering important benefits. Pruning masks can
be stored with high efficiency in memory, and it becomes possible to switch
among several debiasing behaviors (or revert back to the original biased model)
at inference time. Finally, it opens the doors to further research on how
biases are acquired by studying the generated masks. For example, we observed
that the early layers and attention heads were pruned more aggressively,
possibly hinting towards the location in which biases may be encoded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stop Measuring Calibration When Humans Disagree. (arXiv:2210.16133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16133">
<div class="article-summary-box-inner">
<span><p>Calibration is a popular framework to evaluate whether a classifier knows
when it does not know - i.e., its predictive probabilities are a good
indication of how likely a prediction is to be correct. Correctness is commonly
estimated against the human majority class. Recently, calibration to human
majority has been measured on tasks where humans inherently disagree about
which class applies. We show that measuring calibration to human majority given
inherent disagreements is theoretically problematic, demonstrate this
empirically on the ChaosNLI dataset, and derive several instance-level measures
of calibration that capture key statistical properties of human judgements -
class frequency, ranking and entropy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16147">
<div class="article-summary-box-inner">
<span><p>To model behavioral and neural correlates of language comprehension in
naturalistic environments, researchers have turned to broad-coverage tools from
natural-language processing and machine learning. Where syntactic structure is
explicitly modeled, prior work has relied predominantly on context-free
grammars (CFG), yet such formalisms are not sufficiently expressive for human
languages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive
directly compositional models of grammar with flexible constituency that
affords incremental interpretation. In this work we evaluate whether a more
expressive CCG provides a better model than a CFG for human neural signals
collected with fMRI while participants listen to an audiobook story. We further
test between variants of CCG that differ in how they handle optional adjuncts.
These evaluations are carried out against a baseline that includes estimates of
next-word predictability from a Transformer neural network language model. Such
a comparison reveals unique contributions of CCG structure-building
predominantly in the left posterior temporal lobe: CCG-derived measures offer a
superior fit to neural signals compared to those derived from a CFG. These
effects are spatially distinct from bilateral superior temporal effects that
are unique to predictability. Neural effects for structure-building are thus
separable from predictability during naturalistic listening, and those effects
are best characterized by a grammar whose expressive power is motivated on
independent linguistic grounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Neural Topic Models Broken?. (arXiv:2210.16162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16162">
<div class="article-summary-box-inner">
<span><p>Recently, the relationship between automated and human evaluation of topic
models has been called into question. Method developers have staked the
efficacy of new topic model variants on automated measures, and their failure
to approximate human preferences places these models on uncertain ground.
Moreover, existing evaluation paradigms are often divorced from real-world use.
</p>
<p>Motivated by content analysis as a dominant real-world use case for topic
modeling, we analyze two related aspects of topic models that affect their
effectiveness and trustworthiness in practice for that purpose: the stability
of their estimates and the extent to which the model's discovered categories
align with human-determined categories in the data. We find that neural topic
models fare worse in both respects compared to an established classical method.
We take a step toward addressing both issues in tandem by demonstrating that a
straightforward ensembling method can reliably outperform the members of the
ensemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Engineering vs BERT on Twitter Data. (arXiv:2210.16168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16168">
<div class="article-summary-box-inner">
<span><p>In this paper, we compare the performances of traditional machine learning
models using feature engineering and word vectors and the state-of-the-art
language model BERT using word embeddings on three datasets. We also consider
the time and cost efficiency of feature engineering compared to BERT. From our
results we conclude that the use of the BERT model was only worth the time and
cost trade-off for one of the three datasets we used for comparison, where the
BERT model significantly outperformed any kind of traditional classifier that
uses feature vectors, instead of embeddings. Using the BERT model for the other
datasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score
respectively, which could be argued makes its use not worth the time and cost
of GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for targeted syntactic knowledge through grammatical error detection. (arXiv:2210.16228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16228">
<div class="article-summary-box-inner">
<span><p>Targeted studies testing knowledge of subject-verb agreement (SVA) indicate
that pre-trained language models encode syntactic information. We assert that
if models robustly encode subject-verb agreement, they should be able to
identify when agreement is correct and when it is incorrect. To that end, we
propose grammatical error detection as a diagnostic probe to evaluate
token-level contextual representations for their knowledge of SVA. We evaluate
contextual representations at each layer from five pre-trained English language
models: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated
training data from both English second language learners and Wikipedia edits,
and report results on manually crafted stimuli for subject-verb agreement. We
find that masked language models linearly encode information relevant to the
detection of SVA errors, while the autoregressive models perform on par with
our baseline. However, we also observe a divergence in performance when probes
are trained on different training sets, and when they are evaluated on
different syntactic constructions, suggesting the information pertaining to SVA
error detection is not robustly encoded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Math Word Problem via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16257">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (PLMs) bring new opportunities to
challenge problems, especially those that need high-level intelligence, such as
the math word problem (MWPs). However, directly applying existing PLMs to MWPs
can fail as the generation process lacks sufficient supervision and thus lacks
fast adaptivity as humans. We notice that human reasoning has a dual reasoning
framework that consists of an immediate reaction system (system 1) and a
delicate reasoning system (system 2), where the entire reasoning is determined
by their interaction. This inspires us to develop a cooperative
reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),
resulting in a human-like reasoning architecture with system 1 as the generator
and system 2 as the verifier. In our approach, the generator is responsible for
generating reasoning paths, and the verifiers are used to supervise the
evaluation in order to obtain reliable feedback for the generator. We evaluate
our CoRe framework on several mathematical reasoning datasets and achieve
decent improvement over state-of-the-art methods, up to 9.8% increase over best
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16264">
<div class="article-summary-box-inner">
<span><p>Transformers have been the dominant architecture for Speech Translation in
recent years, achieving significant improvements in translation quality. Since
speech signals are longer than their textual counterparts, and due to the
quadratic complexity of the Transformer, a down-sampling step is essential for
its adoption in Speech Translation. Instead, in this research, we propose to
ease the complexity by using a Perceiver encoder to map the speech inputs to a
fixed-length latent representation. Furthermore, we introduce a novel way of
training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent
spaces without any additional computational overhead. Speech-to-Text Perceivers
with DLA can match the performance of a Transformer baseline across three
language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to
DLA at inference, and can be flexibly deployed with various computational
budgets, without significant drops in translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers. (arXiv:2210.16298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16298">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have shown remarkable performance over the
past few years. These models, however, sometimes learn superficial features
from the dataset and cannot generalize to the distributions that are dissimilar
to the training scenario. There have been several approaches proposed to reduce
model's reliance on these bias features which can improve model robustness in
the out-of-distribution setting. However, existing methods usually use a fixed
low-capacity model to deal with various bias features, which ignore the
learnability of those features. In this paper, we analyze a set of existing
bias features and demonstrate there is no single model that works best for all
the cases. We further show that by choosing an appropriate bias model, we can
obtain a better robustness result than baselines with a more sophisticated
model design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGReE: A system for generating Automated Grammar Reading Exercises. (arXiv:2210.16302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16302">
<div class="article-summary-box-inner">
<span><p>We describe the AGReE system, which takes user-submitted passages as input
and automatically generates grammar practice exercises that can be completed
while reading. Multiple-choice practice items are generated for a variety of
different grammar constructs: punctuation, articles, conjunctions, pronouns,
prepositions, verbs, and nouns. We also conducted a large-scale human
evaluation with around 4,500 multiple-choice practice items. We notice for 95%
of items, a majority of raters out of five were able to identify the correct
answer and for 85% of cases, raters agree that there is only one correct answer
among the choices. Finally, the error analysis shows that raters made the most
mistakes for punctuation and conjunctions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concadia: Towards Image-Based Text Generation with a Purpose. (arXiv:2104.08376v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08376">
<div class="article-summary-box-inner">
<span><p>Current deep learning models often achieve excellent results on benchmark
image-to-text datasets but fail to generate texts that are useful in practice.
We argue that to close this gap, it is vital to distinguish descriptions from
captions based on their distinct communicative roles. Descriptions focus on
visual features and are meant to replace an image (often to increase
accessibility), whereas captions appear alongside an image to supply additional
information. To motivate this distinction and help people put it into practice,
we introduce the publicly available Wikipedia-based dataset Concadia consisting
of 96,918 images with corresponding English-language descriptions, captions,
and surrounding context. Using insights from Concadia, models trained on it,
and a preregistered human-subjects experiment with human- and model-generated
texts, we characterize the commonalities and differences between descriptions
and captions. In addition, we show that, for generating both descriptions and
captions, it is useful to augment image-to-text models with representations of
the textual context in which the image appeared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training. (arXiv:2104.08763v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08763">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) for attention mechanisms has successfully reduced
such drawbacks by considering adversarial perturbations. However, this
technique requires label information, and thus, its use is limited to
supervised settings. In this study, we explore the concept of incorporating
virtual AT (VAT) into the attention mechanisms, by which adversarial
perturbations can be computed even from unlabeled data. To realize this
approach, we propose two general training techniques, namely VAT for attention
mechanisms (Attention VAT) and "interpretable" VAT for attention mechanisms
(Attention iVAT), which extend AT for attention mechanisms to a semi-supervised
setting. In particular, Attention iVAT focuses on the differences in attention;
thus, it can efficiently learn clearer attention and improve model
interpretability, even with unlabeled data. Empirical experiments based on six
public datasets revealed that our techniques provide better prediction
performance than conventional AT-based as well as VAT-based techniques, and
stronger agreement with evidence that is provided by humans in detecting
important words in sentences. Moreover, our proposal offers these advantages
without needing to add the careful selection of unlabeled data. That is, even
if the model using our VAT-based technique is trained on unlabeled data from a
source other than the target task, both the prediction performance and model
interpretability can be improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08079">
<div class="article-summary-box-inner">
<span><p>NLP research has been focused on NER extraction and how to efficiently
extract them from a sentence. However, generating relevant context of entities
from a sentence has remained under-explored. In this work we introduce the task
Context-NER in which relevant context of an entity has to be generated. The
extracted context may not be found exactly as a substring in the sentence. We
also introduce the EDGAR10-Q dataset for the same, which is a corpus of 1,500
publicly traded companies. It is a manually created complex corpus and one of
the largest in terms of number of sentences and entities (1 M and 2.8 M). We
introduce a baseline approach that leverages phrase generation algorithms and
uses the pre-trained BERT model to get 33% ROUGE-L score. We also do a one shot
evaluation with GPT-3 and get 39% score, signifying the hardness and future
scope of this task. We hope that addition of this dataset and our study will
pave the way for further research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Need One Model for Open-domain Question Answering. (arXiv:2112.07381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07381">
<div class="article-summary-box-inner">
<span><p>Recent approaches to Open-domain Question Answering refer to an external
knowledge base using a retriever model, optionally rerank passages with a
separate reranker model and generate an answer using another reader model.
Despite performing related tasks, the models have separate parameters and are
weakly-coupled during training. We propose casting the retriever and the
reranker as internal passage-wise attention mechanisms applied sequentially
within the transformer architecture and feeding computed representations to the
reader, with the hidden representations progressively refined at each stage.
This allows us to use a single question answering model trained end-to-end,
which is a more efficient use of model capacity and also leads to better
gradient flow. We present a pre-training method to effectively train this
architecture and evaluate our model on the Natural Questions and TriviaQA open
datasets. For a fixed parameter budget, our model outperforms the previous
state-of-the-art model by 1.0 and 0.7 exact match scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. (arXiv:2112.08558v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08558">
<div class="article-summary-box-inner">
<span><p>Compared to standard retrieval tasks, passage retrieval for conversational
question answering (CQA) poses new challenges in understanding the current user
question, as each question needs to be interpreted within the dialogue context.
Moreover, it can be expensive to re-train well-established retrievers such as
search engines that are originally developed for non-conversational queries. To
facilitate their use, we develop a query rewriting model CONQRR that rewrites a
conversational question in the context into a standalone question. It is
trained with a novel reward function to directly optimize towards retrieval
using reinforcement learning and can be adapted to any off-the-shelf retriever.
CONQRR achieves state-of-the-art results on a recent open-domain CQA dataset
containing conversations from three different sources, and is effective for two
different off-the-shelf retrievers. Our extensive analysis also shows the
robustness of CONQRR to out-of-domain dialogues as well as to zero query
rewriting supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Deduction through Search over Statement Compositions. (arXiv:2201.06028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06028">
<div class="article-summary-box-inner">
<span><p>In settings from fact-checking to question answering, we frequently want to
know whether a collection of evidence (premises) entails a hypothesis. Existing
methods primarily focus on the end-to-end discriminative version of this task,
but less work has treated the generative version in which a model searches over
the space of statements entailed by the premises to constructively derive the
hypothesis. We propose a system for doing this kind of deductive reasoning in
natural language by decomposing the task into separate steps coordinated by a
search procedure, producing a tree of intermediate conclusions that faithfully
reflects the system's reasoning process. Our experiments on the EntailmentBank
dataset (Dalvi et al., 2021) demonstrate that the proposed system can
successfully prove true statements while rejecting false ones. Moreover, it
produces natural language explanations with a 17% absolute higher step validity
than those produced by an end-to-end T5 model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning. (arXiv:2201.06206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06206">
<div class="article-summary-box-inner">
<span><p>Multi-hop knowledge graph (KG) reasoning has been widely studied in recent
years to provide interpretable predictions on missing links with evidential
paths. Most previous works use reinforcement learning (RL) based methods that
learn to navigate the path towards the target entity. However, these methods
suffer from slow and poor convergence, and they may fail to infer a certain
path when there is a missing edge along the path. Here we present SQUIRE, the
first Sequence-to-sequence based multi-hop reasoning framework, which utilizes
an encoder-decoder Transformer structure to translate the query to a path. Our
framework brings about two benefits: (1) It can learn and predict in an
end-to-end fashion, which gives better and faster convergence; (2) Our
Transformer model does not rely on existing edges to generate the path, and has
the flexibility to complete missing edges along the path, especially in sparse
KGs. Experiments on standard and sparse KGs show that our approach yields
significant improvement over prior methods, while converging 4x-7x faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08071">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks. (arXiv:2202.08011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08011">
<div class="article-summary-box-inner">
<span><p>The research of open-domain dialog systems has been greatly prospered by
neural models trained on large-scale corpora, however, such corpora often
introduce various safety problems (e.g., offensive languages, biases, and toxic
behaviors) that significantly hinder the deployment of dialog systems in
practice. Among all these unsafe issues, addressing social bias is more complex
as its negative impact on marginalized populations is usually expressed
implicitly, thus requiring normative reasoning and rigorous analysis. In this
paper, we focus our investigation on social bias detection of dialog safety
problems. We first propose a novel Dial-Bias Frame for analyzing the social
bias in conversations pragmatically, which considers more comprehensive
bias-related analyses rather than simple dichotomy annotations. Based on the
proposed framework, we further introduce CDail-Bias Dataset that, to our
knowledge, is the first well-annotated Chinese social bias dialog dataset. In
addition, we establish several dialog bias detection benchmarks at different
label granularities and input types (utterance-level and context-level). We
show that the proposed in-depth analyses together with these benchmarks in our
Dial-Bias Frame are necessary and essential to bias detection tasks and can
benefit building safe dialog systems in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification. (arXiv:2203.07216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07216">
<div class="article-summary-box-inner">
<span><p>Many recent deep learning-based solutions have widely adopted the
attention-based mechanism in various tasks of the NLP discipline. However, the
inherent characteristics of deep learning models and the flexibility of the
attention mechanism increase the models' complexity, thus leading to challenges
in model explainability. In this paper, to address this challenge, we propose a
novel practical framework by utilizing a two-tier attention architecture to
decouple the complexity of explanation and the decision-making process. We
apply it in the context of a news article classification task. The experiments
on two large-scaled news corpora demonstrate that the proposed model can
achieve competitive performance with many state-of-the-art alternatives and
illustrate its appropriateness from an explainability perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zoom Out and Observe: News Environment Perception for Fake News Detection. (arXiv:2203.10885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10885">
<div class="article-summary-box-inner">
<span><p>Fake news detection is crucial for preventing the dissemination of
misinformation on social media. To differentiate fake news from real ones,
existing methods observe the language patterns of the news post and "zoom in"
to verify its content with knowledge sources or check its readers' replies.
However, these methods neglect the information in the external news environment
where a fake news post is created and disseminated. The news environment
represents recent mainstream media opinion and public attention, which is an
important inspiration of fake news fabrication because fake news is often
designed to ride the wave of popular events and catch public attention with
unexpected novel content for greater exposure and spread. To capture the
environmental signals of news posts, we "zoom out" to observe the news
environment and propose the News Environment Perception Framework (NEP). For
each post, we construct its macro and micro news environment from recent
mainstream news. Then we design a popularity-oriented and a novelty-oriented
module to perceive useful signals and further assist final prediction.
Experiments on our newly built datasets show that the NEP can efficiently
improve the performance of basic fake news detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularization-based Pruning of Irrelevant Weights in Deep Neural Architectures. (arXiv:2204.04977v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04977">
<div class="article-summary-box-inner">
<span><p>Deep neural networks exploiting millions of parameters are nowadays the norm
in deep learning applications. This is a potential issue because of the great
amount of computational resources needed for training, and of the possible loss
of generalization performance of overparametrized networks. We propose in this
paper a method for learning sparse neural topologies via a regularization
technique which identifies non relevant weights and selectively shrinks their
norm, while performing a classic update for relevant ones. This technique,
which is an improvement of classical weight decay, is based on the definition
of a regularization term which can be added to any loss functional regardless
of its form, resulting in a unified general framework exploitable in many
different contexts. The actual elimination of parameters identified as
irrelevant is handled by an iterative pruning algorithm. We tested the proposed
technique on different image classification and Natural language generation
tasks, obtaining results on par or better then competitors in terms of sparsity
and metrics, while achieving strong models compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART. (arXiv:2204.07367v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07367">
<div class="article-summary-box-inner">
<span><p>Word ordering is a constrained language generation task taking unordered
words as input. Existing work uses linear models and neural networks for the
task, yet pre-trained language models have not been studied in word ordering,
let alone why they help. We use BART as an instance and show its effectiveness
in the task. To explain why BART helps word ordering, we extend analysis with
probing and empirically identify that syntactic dependency knowledge in BART is
a reliable explanation. We also report performance gains with BART in the
related partial tree linearization task, which readily extends our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset. (arXiv:2205.09209v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09209">
<div class="article-summary-box-inner">
<span><p>As language models grow in popularity, it becomes increasingly important to
clearly measure all possible markers of demographic identity in order to avoid
perpetuating existing societal harms. Many datasets for measuring bias
currently exist, but they are restricted in their coverage of demographic axes
and are commonly used with preset bias tests that presuppose which types of
biases models can exhibit. In this work, we present a new, more inclusive bias
measurement dataset, HolisticBias, which includes nearly 600 descriptor terms
across 13 different demographic axes. HolisticBias was assembled in a
participatory process including experts and community members with lived
experience of these terms. These descriptors combine with a set of bias
measurement templates to produce over 450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms of bias in several generative
models. We demonstrate that HolisticBias is effective at measuring previously
undetectable biases in token likelihoods from language models, as well as in an
offensiveness classifier. We will invite additions and amendments to the
dataset, which we hope will serve as a basis for more easy-to-use and
standardized methods for evaluating bias in NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twist Decoding: Diverse Generators Guide Each Other. (arXiv:2205.09273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09273">
<div class="article-summary-box-inner">
<span><p>Many language generation models are now available for a wide range of
generation tasks, including machine translation and summarization. Combining
such diverse models may lead to further progress, but ensembling generation
models is challenging during inference: conventional ensembling methods (e.g.,
shallow fusion) require that the models share vocabulary/tokenization schemes.
We introduce Twist decoding, a simple and general text generation algorithm
that benefits from diverse models at inference time. Our method does not assume
the vocabulary, tokenization or even generation order is shared. Our extensive
evaluations on machine translation and scientific paper summarization
demonstrate that Twist decoding substantially outperforms each model decoded in
isolation over various scenarios, including cases where domain-specific and
general-purpose models are both available. Twist decoding also consistently
outperforms the popular reranking heuristic where output candidates from one
model are rescored by another. We hope that our work will encourage researchers
and practitioners to examine generation models collectively, not just
independently, and to seek out models with complementary strengths to the
currently available models. Our code is available at
https://github.com/jungokasai/twist_decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNaC: Coherence Error Detection for Narrative Summarization. (arXiv:2205.09641v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09641">
<div class="article-summary-box-inner">
<span><p>Progress in summarizing long texts is inhibited by the lack of appropriate
evaluation frameworks. When a long summary must be produced to appropriately
cover the facets of that text, that summary needs to present a coherent
narrative to be understandable by a reader, but current automatic and human
evaluation methods fail to identify gaps in coherence. In this work, we
introduce SNaC, a narrative coherence evaluation framework rooted in
fine-grained annotations for long summaries. We develop a taxonomy of coherence
errors in generated narrative summaries and collect span-level annotations for
6.6k sentences across 150 book and movie screenplay summaries. Our work
provides the first characterization of coherence errors generated by
state-of-the-art summarization models and a protocol for eliciting coherence
judgments from crowd annotators. Furthermore, we show that the collected
annotations allow us to train a strong classifier for automatically localizing
coherence errors in generated summaries as well as benchmarking past work in
coherence modeling. Finally, our SNaC framework can support future work in long
document summarization and coherence evaluation, including improved
summarization modeling and post-hoc summary correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating Hanja Historical Documents to Contemporary Korean and English. (arXiv:2205.10019v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10019">
<div class="article-summary-box-inner">
<span><p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of
Joseon, the 500-year kingdom preceding the modern nation of Korea. The Annals
were originally written in an archaic Korean writing system, `Hanja', and were
translated into Korean from 1968 to 1993. The resulting translation was however
too literal and contained many archaic Korean words; thus, a new expert
translation effort began in 2012. Since then, the records of only one king have
been completed in a decade. In parallel, expert translators are working on
English translation, also at a slow pace and produced only one king's records
in English so far. Thus, we propose H2KE, a neural machine translation model,
that translates historical documents in Hanja to more easily understandable
Korean and to English. Built on top of multilingual neural machine translation,
H2KE learns to translate a historical document written in Hanja, from both a
full dataset of outdated Korean translation and a small dataset of more
recently translated contemporary Korean and English. We compare our method
against two baselines: a recent model that simultaneously learns to restore and
translate Hanja historical document and a Transformer based model trained only
on newly translated corpora. The experiments reveal that our method
significantly outperforms the baselines in terms of BLEU scores for both
contemporary Korean and English translations. We further conduct extensive
human evaluation which shows that our translation is preferred over the
original expert translations by both experts and non-expert Korean speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics. (arXiv:2205.10646v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10646">
<div class="article-summary-box-inner">
<span><p>Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they do not take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility. As a
proof-of-concept, we provide a contextual version of the referenceless metric
CLIPScore which begins to address the disconnect to the BLV data. An accessible
HTML version of this paper is available at
https://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. (arXiv:2205.12206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12206">
<div class="article-summary-box-inner">
<span><p>Formal verse poetry imposes strict constraints on the meter and rhyme scheme
of poems. Most prior work on generating this type of poetry uses existing poems
for supervision, which are difficult to obtain for most languages and poetic
forms. In this work, we propose an unsupervised approach to generate poems
following any given meter and rhyme scheme, without requiring any poetic text
for training. Our method works by splitting a regular, non-poetic corpus into
phrases, prepending control codes that describe the length and end rhyme of
each phrase, and training a transformer language model in the augmented corpus.
During inference, we build control codes for the desired meter and rhyme
scheme, and condition our language model on them to generate formal verse
poetry. Experiments in Spanish and Basque show that our approach is able to
generate valid poems, which are often comparable in quality to those written by
humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your Transformer May Not be as Powerful as You Expect. (arXiv:2205.13401v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13401">
<div class="article-summary-box-inner">
<span><p>Relative Positional Encoding (RPE), which encodes the relative distance
between any pair of tokens, is one of the most successful modifications to the
original Transformer. As far as we know, theoretical understanding of the
RPE-based Transformers is largely unexplored. In this work, we mathematically
analyze the power of RPE-based Transformers regarding whether the model is
capable of approximating any continuous sequence-to-sequence functions. One may
naturally assume the answer is in the affirmative -- RPE-based Transformers are
universal function approximators. However, we present a negative result by
showing there exist continuous sequence-to-sequence functions that RPE-based
Transformers cannot approximate no matter how deep and wide the neural network
is. One key reason lies in that most RPEs are placed in the softmax attention
that always generates a right stochastic matrix. This restricts the network
from capturing positional information in the RPEs and limits its capacity. To
overcome the problem and make the model more powerful, we first present
sufficient conditions for RPE-based Transformers to achieve universal function
approximation. With the theoretical guidance, we develop a novel attention
module, called Universal RPE-based (URPE) Attention, which satisfies the
conditions. Therefore, the corresponding URPE-based Transformers become
universal function approximators. Extensive experiments covering typical
architectures and tasks demonstrate that our model is parameter-efficient and
can achieve superior performance to strong baselines in a wide range of
applications. The code will be made publicly available at
https://github.com/lsj2408/URPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features. (arXiv:2206.07023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07023">
<div class="article-summary-box-inner">
<span><p>Models based on large-pretrained language models, such as S(entence)BERT,
provide effective and efficient sentence embeddings that show high correlation
to human similarity ratings, but lack interpretability. On the other hand,
graph metrics for graph-based meaning representations (e.g., Abstract Meaning
Representation, AMR) can make explicit the semantic aspects in which two
sentences are similar. However, such metrics tend to be slow, rely on parsers,
and do not reach state-of-the-art performance when rating sentence similarity.
</p>
<p>In this work, we aim at the best of both worlds, by learning to induce
$S$emantically $S$tructured $S$entence BERT embeddings (S$^3$BERT). Our
S$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize
various semantic sentence features (e.g., semantic roles, negation, or
quantification). We show how to i) learn a decomposition of the sentence
embeddings into semantic features, through approximation of a suite of
interpretable AMR graph metrics, and how to ii) preserve the overall power of
the neural embeddings by controlling the decomposition learning process with a
second objective that enforces consistency with the similarity ratings of an
SBERT teacher model. In our experimental studies, we show that our approach
offers interpretability -- while fully preserving the effectiveness and
efficiency of the neural sentence embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models. (arXiv:2206.08325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08325">
<div class="article-summary-box-inner">
<span><p>Large language models produce human-like text that drive a growing number of
applications. However, recent literature and, increasingly, real world
observations, have demonstrated that these models can generate language that is
toxic, biased, untruthful or otherwise harmful. Though work to evaluate
language model harms is under way, translating foresight about which harms may
arise into rigorous benchmarks is not straightforward. To facilitate this
translation, we outline six ways of characterizing harmful text which merit
explicit consideration when designing new benchmarks. We then use these
characteristics as a lens to identify trends and gaps in existing benchmarks.
Finally, we apply them in a case study of the Perspective API, a toxicity
classifier that is widely used in harm benchmarks. Our characteristics provide
one piece of the bridge that translates between foresight and effective
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Active Learning for Pre-trained Transformer-based Models. (arXiv:2208.05379v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.05379">
<div class="article-summary-box-inner">
<span><p>Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Interpersonal Conflict Types and their Impact on Perception Classification. (arXiv:2208.08758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08758">
<div class="article-summary-box-inner">
<span><p>Studies on interpersonal conflict have a long history and contain many
suggestions for conflict typology. We use this as the basis of a novel
annotation scheme and release a new dataset of situations and conflict aspect
annotations. We then build a classifier to predict whether someone will
perceive the actions of one individual as right or wrong in a given situation.
Our analyses include conflict aspects, but also generated clusters, which are
human validated, and show differences in conflict content based on the
relationship of participants to the author. Our findings have important
implications for understanding conflict and social norms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media. (arXiv:2208.09021v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09021">
<div class="article-summary-box-inner">
<span><p>We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an
extension of the popular Vision-and-Language Transformer (ViLT), and improves
performance on vision-and-language (VL) tasks that involve more complex text
inputs than image captions while having minimal impact on training and
inference efficiency. ViLT, importantly, enables efficient training and
inference in VL tasks, achieved by encoding images using a linear projection of
patches instead of an object detector. However, it is pretrained on captioning
datasets, where the language input is simple, literal, and descriptive,
therefore lacking linguistic diversity. So, when working with multimedia data
in the wild, such as multimodal social media data, there is a notable shift
from captioning language data, as well as diversity of tasks. We indeed find
evidence that the language capacity of ViLT is lacking. The key insight of
VAuLT is to propagate the output representations of a large language model (LM)
like BERT to the language input of ViLT. We show that joint training of the LM
and ViLT in VAuLT can yield relative improvements up to 20% over ViLT on VL
tasks involving richer language inputs and affective constructs, such as for
Target-Oriented Sentiment Classification in TWITTER-2015 and TWITTER-2017, and
Sentiment Classification in MVSA-Single and MVSA-Multiple. Our code is
available at https://github.com/gchochla/VAuLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers. (arXiv:2209.06442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06442">
<div class="article-summary-box-inner">
<span><p>This paper aims to improve the performance of text-to-SQL parsing by
exploring the intrinsic uncertainties in the neural network based approaches
(called SUN). From the data uncertainty perspective, it is indisputable that a
single SQL can be learned from multiple semantically-equivalent
questions.Different from previous methods that are limited to one-to-one
mapping, we propose a data uncertainty constraint to explore the underlying
complementary semantic information among multiple semantically-equivalent
questions (many-to-one) and learn the robust feature representations with
reduced spurious associations. In this way, we can reduce the sensitivity of
the learned representations and improve the robustness of the parser. From the
model uncertainty perspective, there is often structural information
(dependence) among the weights of neural networks. To improve the
generalizability and stability of neural text-to-SQL parsers, we propose a
model uncertainty constraint to refine the query representations by enforcing
the output representations of different perturbed encoding networks to be
consistent with each other. Extensive experiments on five benchmark datasets
demonstrate that our method significantly outperforms strong competitors and
achieves new state-of-the-art results. For reproducibility, we release our code
and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05423">
<div class="article-summary-box-inner">
<span><p>We introduce a new task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed instructional videos using a natural language question. This task
requires a range of skills - the interaction between vision and language, video
retrieval, passage comprehension, and visual answer localization. In this
paper, we propose a cross-modal contrastive global-span (CCGS) method for the
VCVAL, jointly training the video corpus retrieval and visual answer
localization subtasks with the global-span matrix. We have reconstructed a
dataset named MedVidCQA, on which the VCVAL task is benchmarked. Experimental
results show that the proposed method outperforms other competitive methods
both in the video corpus retrieval and visual answer localization subtasks.
Most importantly, we perform detailed analyses on extensive experiments, paving
a new path for understanding the instructional videos, which ushers in further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing. (arXiv:2210.11888v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11888">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel SQL guided pre-training framework STAR for
context-dependent text-to-SQL parsing, which leverages contextual information
to enrich natural language (NL) utterance and table schema representations for
text-to-SQL conversations. Concretely, we propose two novel pre-training
objectives which respectively explore the context-dependent interactions of NL
utterances and SQL queries within each text-to-SQL conversation: (i) schema
state tracking (SST) objective that tracks and explores the schema states of
context-dependent SQL queries in the form of schema-states by predicting and
updating the value of each schema slot during interaction; (ii) utterance
dependency tracking (UDT) objective that employs weighted contrastive learning
to pull together two semantically similar NL utterances and push away the
representations of semantically dissimilar NL utterances within each
conversation. In addition, we construct a high-quality large-scale
context-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive
experiments show that STAR achieves new state-of-the-art performance on two
downstream benchmarks (SParC and CoSQL), significantly outperforming previous
pre-training methods and ranking first on the leaderboard. We believe the
release of the constructed corpus, codebase and pre-trained STAR checkpoints
would push forward the research in this area. For reproducibility, we release
our code and data at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/star.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation. (arXiv:2210.13304v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13304">
<div class="article-summary-box-inner">
<span><p>We study the text generation task under the approach of pre-trained language
models (PLMs). Typically, an auto-regressive (AR) method is adopted for
generating texts in a token-by-token manner. Despite many advantages of AR
generation, it usually suffers from inefficient inference. Therefore,
non-autoregressive (NAR) models are proposed to generate all target tokens
simultaneously. However, NAR models usually generate texts of lower quality due
to the absence of token dependency in the output text. In this paper, we
propose ELMER: an efficient and effective PLM for NAR text generation to
explicitly model the token dependency during NAR generation. By leveraging the
early exit technique, ELMER enables the token generations at different layers,
according to their prediction confidence (a more confident token will exit at a
lower layer). Besides, we propose a novel pre-training objective, Layer
Permutation Language Modeling, to pre-train ELMER by permuting the exit layer
for each token in sequences. Experiments on three text generation tasks show
that ELMER significantly outperforms NAR models and further narrows the
performance gap with AR PLMs (\eg ELMER (29.92) vs BART (30.61) ROUGE-L in
XSUM) while achieving over 10 times inference speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Knowledge Graph Construction and Event-centric Knowledge Infusion for Scientific NLI. (arXiv:2210.15248v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15248">
<div class="article-summary-box-inner">
<span><p>With the advance of natural language inference (NLI), a rising demand for NLI
is to handle scientific texts. Existing methods depend on pre-trained models
(PTM) which lack domain-specific knowledge. To tackle this drawback, we
introduce a scientific knowledge graph to generalize PTM to scientific domain.
However, existing knowledge graph construction approaches suffer from some
drawbacks, i.e., expensive labeled data, failure to apply in other domains,
long inference time and difficulty extending to large corpora. Therefore, we
propose an unsupervised knowledge graph construction method to build a
scientific knowledge graph (SKG) without any labeled data. Moreover, to
alleviate noise effect from SKG and complement knowledge in sentences better,
we propose an event-centric knowledge infusion method to integrate external
knowledge into each event that is a fine-grained semantic unit in sentences.
Experimental results show that our method achieves state-of-the-art performance
and the effectiveness and reliability of SKG.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-31 23:17:52.948999397 UTC">2022-10-31 23:17:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>