<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-04T01:30:00Z">04-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00008">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are revolutionizing several areas of Artificial
Intelligence. One of the most remarkable applications is creative writing,
e.g., poetry or storytelling: the generated outputs are often of astonishing
quality. However, a natural question arise: can LLMs really be considered
creative? In this article we firstly analyze the development of LLMs under the
lens of creativity theories, investigating the key open questions and
challenges. Then, we identify a set of "easy" and "hard" problems in machine
creativity, discussing them in relation to LLMs. Finally, we analyze the
societal impact of these technologies with a particular focus on the creative
industries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00020">
<div class="article-summary-box-inner">
<span><p>The prevalence of memes on social media has created the need to sentiment
analyze their underlying meanings for censoring harmful content. Meme censoring
systems by machine learning raise the need for a semi-supervised learning
solution to take advantage of the large number of unlabeled memes available on
the internet and make the annotation process less challenging. Moreover, the
approach needs to utilize multimodal data as memes' meanings usually come from
both images and texts. This research proposes a multimodal semi-supervised
learning approach that outperforms other multimodal semi-supervised learning
and supervised learning state-of-the-art models on two datasets, the Multimedia
Automatic Misogyny Identification and Hateful Memes dataset. Building on the
insights gained from Contrastive Language-Image Pre-training, which is an
effective multimodal learning technique, this research introduces SemiMemes, a
novel training method that combines auto-encoder and classification task to
make use of the resourceful unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case. (arXiv:2304.00025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00025">
<div class="article-summary-box-inner">
<span><p>After the pandemic, artificial intelligence (AI) powered support for mental
health care has become increasingly important. The breadth and complexity of
significant challenges required to provide adequate care involve: (a)
Personalized patient understanding, (b) Safety-constrained and medically
validated chatbot patient interactions, and (c) Support for continued
feedback-based refinements in design using chatbot-patient interactions. We
propose Alleviate, a chatbot designed to assist patients suffering from mental
health challenges with personalized care and assist clinicians with
understanding their patients better. Alleviate draws from an array of publicly
available clinically valid mental-health texts and databases, allowing
Alleviate to make medically sound and informed decisions. In addition,
Alleviate's modular design and explainable decision-making lends itself to
robust and continued feedback-based refinements to its design. In this paper,
we explain the different modules of Alleviate and submit a short video
demonstrating Alleviate's capabilities to help patients and clinicians
understand each other better to facilitate optimal care strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing. (arXiv:2304.00111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00111">
<div class="article-summary-box-inner">
<span><p>Delirium is an acute decline or fluctuation in attention, awareness, or other
cognitive function that can lead to serious adverse outcomes. Despite the
severe outcomes, delirium is frequently unrecognized and uncoded in patients'
electronic health records (EHRs) due to its transient and diverse nature.
Natural language processing (NLP), a key technology that extracts medical
concepts from clinical narratives, has shown great potential in studies of
delirium outcomes and symptoms. To assist in the diagnosis and phenotyping of
delirium, we formed an expert panel to categorize diverse delirium symptoms,
composed annotation guidelines, created a delirium corpus with diverse delirium
symptoms, and developed NLP methods to extract delirium symptoms from clinical
notes. We compared 5 state-of-the-art transformer models including 2 models
(BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC,
RoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the
best strict and lenient F1 scores of 0.8055 and 0.8759, respectively. We
conducted an error analysis to identify challenges in annotating delirium
symptoms and developing NLP systems. To the best of our knowledge, this is the
first large language model-based delirium symptom extraction system. Our study
lays the foundation for the future development of computable phenotypes and
diagnosis methods for delirium.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00114">
<div class="article-summary-box-inner">
<span><p>Vector-based retrieval systems have become a common staple for academic and
industrial search applications because they provide a simple and scalable way
of extending the search to leverage contextual representations for documents
and queries. As these vector-based systems rely on contextual language models,
their usage commonly requires GPUs, which can be expensive and difficult to
manage. Given recent advances in introducing sparsity into language models for
improved inference efficiency, in this paper, we study how sparse language
models can be used for dense retrieval to improve inference efficiency. Using
the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA
datasets, we find that sparse language models can be used as direct
replacements with little to no drop in accuracy and up to 4.3x improved
inference speeds
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods. (arXiv:2304.00115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00115">
<div class="article-summary-box-inner">
<span><p>The ultrasound characteristics of thyroid nodules guide the evaluation of
thyroid cancer in patients with thyroid nodules. However, the characteristics
of thyroid nodules are often documented in clinical narratives such as
ultrasound reports. Previous studies have examined natural language processing
(NLP) methods in extracting a limited number of characteristics (&lt;9) using
rule-based NLP systems. In this study, a multidisciplinary team of NLP experts
and thyroid specialists, identified thyroid nodule characteristics that are
important for clinical care, composed annotation guidelines, developed a
corpus, and compared 5 state-of-the-art transformer-based NLP methods,
including BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of
thyroid nodule characteristics from ultrasound reports. Our GatorTron model, a
transformer-based large language model trained using over 90 billion words of
text, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for
the extraction of a total number of 16 thyroid nodule characteristics, and
0.9321 for linking characteristics to nodules, outperforming other clinical
transformer models. To the best of our knowledge, this is the first study to
systematically categorize and apply transformer-based NLP models to extract a
large number of clinical relevant thyroid nodule characteristics from
ultrasound reports. This study lays ground for assessing the documentation
quality of thyroid ultrasound reports and examining outcomes of patients with
thyroid nodules using electronic health records.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Large Language Models with Climate Resources. (arXiv:2304.00116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00116">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have significantly transformed the landscape of
artificial intelligence by demonstrating their ability in generating human-like
text across diverse topics. However, despite their impressive capabilities,
LLMs lack recent information and often employ imprecise language, which can be
detrimental in domains where accuracy is crucial, such as climate change. In
this study, we make use of recent ideas to harness the potential of LLMs by
viewing them as agents that access multiple sources, including databases
containing recent and precise information about organizations, institutions,
and companies. We demonstrate the effectiveness of our method through a
prototype agent that retrieves emission data from ClimateWatch
(https://www.climatewatchdata.org/) and leverages general Google search. By
integrating these resources with LLMs, our approach overcomes the limitations
associated with imprecise language and delivers more reliable and accurate
information in the critical domain of climate change. This work paves the way
for future advancements in LLMs and their application in domains where
precision is of paramount importance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts. (arXiv:2304.00121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00121">
<div class="article-summary-box-inner">
<span><p>Scholarly writing presents a complex space that generally follows a
methodical procedure to plan and produce both rationally sound and creative
compositions. Recent works involving large language models (LLM) demonstrate
considerable success in text generation and revision tasks; however, LLMs still
struggle to provide structural and creative feedback on the document level that
is crucial to academic writing. In this paper, we introduce a novel taxonomy
that categorizes scholarly writing behaviors according to intention, writer
actions, and the information types of the written data. We also provide
ManuScript, an original dataset annotated with a simplified version of our
taxonomy to show writer actions and the intentions behind them. Motivated by
cognitive writing theory, our taxonomy for scientific papers includes three
levels of categorization in order to trace the general writing flow and
identify the distinct writer activities embedded within each higher-level
process. ManuScript intends to provide a complete picture of the scholarly
writing process by capturing the linearity and non-linearity of writing
trajectory, such that writing assistants can provide stronger feedback and
suggestions on an end-to-end level. The collected writing trajectories are
viewed at https://minnesotanlp.github.io/REWARD_demo/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00171">
<div class="article-summary-box-inner">
<span><p>Conformer models maintain a large number of internal states, the vast
majority of which are associated with self-attention layers. With limited
memory bandwidth, reading these from memory at each inference step can slow
down inference. In this paper, we design an optimized conformer that is small
enough to meet on-device restrictions and has fast inference on TPUs. We
explore various ideas to improve the execution speed, including replacing lower
conformer blocks with convolution-only blocks, strategically downsizing the
architecture, and utilizing an RNNAttention-Performer. Our optimized conformer
can be readily incorporated into a cascaded-encoder setting, allowing a
second-pass decoder to operate on its output and improve the accuracy whenever
more resources are available. Altogether, we find that these optimizations can
reduce latency by a factor of 6.8x, and come at a reasonable trade-off in
quality. With the cascaded second-pass, we show that the recognition accuracy
is completely recoverable. Thus, our proposed encoder can double as a strong
standalone encoder in on device, and as the first part of a high-performance
ASR pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00173">
<div class="article-summary-box-inner">
<span><p>In end-to-end (E2E) speech recognition models, a representational
tight-coupling inevitably emerges between the encoder and the decoder. We build
upon recent work that has begun to explore building encoders with modular
encoded representations, such that encoders and decoders from different models
can be stitched together in a zero-shot manner without further fine-tuning.
While previous research only addresses full-context speech models, we explore
the problem in a streaming setting as well. Our framework builds on top of
existing encoded representations, converting them to modular features, dubbed
as Lego-Features, without modifying the pre-trained model. The features remain
interchangeable when the model is retrained with distinct initializations.
Though sparse, we show that the Lego-Features are powerful when tested with
RNN-T or LAS decoders, maintaining high-quality downstream performance. They
are also rich enough to represent the first-pass prediction during two-pass
deliberation. In this scenario, they outperform the N-best hypotheses, since
they do not need to be supplemented with acoustic features to deliver the best
results. Moreover, generating the Lego-Features does not require beam search or
auto-regressive computation. Overall, they present a modular, powerful and
cheap alternative to the standard encoder output, as well as the N-best
hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00180">
<div class="article-summary-box-inner">
<span><p>Response ranking in dialogues plays a crucial role in retrieval-based
conversational systems. In a multi-turn dialogue, to capture the gist of a
conversation, contextual information serves as essential knowledge to achieve
this goal. In this paper, we present a flexible neural framework that can
integrate contextual information from multiple channels. Specifically for the
current task, our approach is to provide two information channels in parallel,
Fusing Conversation history and domain knowledge extracted from Candidate
provenance (FCC), where candidate responses are curated, as contextual
information to improve the performance of multi-turn dialogue response ranking.
The proposed approach can be generalized as a module to incorporate
miscellaneous contextual features for other context-oriented tasks. We evaluate
our model on the MSDialog dataset widely used for evaluating conversational
response ranking tasks. Our experimental results show that our framework
significantly outperforms the previous state-of-the-art models, improving
Recall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies to
evaluate the contributions of each information channel, and of the framework
components, to the overall ranking performance, providing additional insights
and directions for further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00215">
<div class="article-summary-box-inner">
<span><p>Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models can rate news outlet credibility. (arXiv:2304.00228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00228">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have shown exceptional performance in
various natural language processing tasks, they are prone to hallucinations.
State-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue
by gathering information directly from the internet to ground their answers. In
this setting, the capacity to distinguish trustworthy sources is critical for
providing appropriate accuracy contexts to users. Here we assess whether
ChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With
appropriate instructions, ChatGPT can provide ratings for a diverse set of news
outlets, including those in non-English languages and satirical sources, along
with contextual explanations. Our results show that these ratings correlate
with those from human experts (Spearmam's $\rho=0.54, p&lt;0.001$). These findings
suggest that LLMs could be an affordable reference for credibility ratings in
fact-checking applications. Future LLMs should enhance their alignment with
human expert judgments of source credibility to improve information accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Does the Indian Parliament Discuss? An Exploratory Analysis of the Question Hour in the Lok Sabha. (arXiv:2304.00235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00235">
<div class="article-summary-box-inner">
<span><p>The TCPD-IPD dataset is a collection of questions and answers discussed in
the Lower House of the Parliament of India during the Question Hour between
1999 and 2019. Although it is difficult to analyze such a huge collection
manually, modern text analysis tools can provide a powerful means to navigate
it. In this paper, we perform an exploratory analysis of the dataset. In
particular, we present insightful corpus-level statistics and a detailed
analysis of three subsets of the dataset. In the latter analysis, the focus is
on understanding the temporal evolution of topics using a dynamic topic model.
We observe that the parliamentary conversation indeed mirrors the political and
socio-economic tensions of each period.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus. (arXiv:2304.00350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00350">
<div class="article-summary-box-inner">
<span><p>Building a natural language dataset requires caution since word semantics is
vulnerable to subtle text change or the definition of the annotated concept.
Such a tendency can be seen in generative tasks like question-answering and
dialogue generation and also in tasks that create a categorization-based
corpus, like topic classification or sentiment analysis. Open-domain
conversations involve two or more crowdworkers freely conversing about any
topic, and collecting such data is particularly difficult for two reasons: 1)
the dataset should be ``crafted" rather than ``obtained" due to privacy
concerns, and 2) paid creation of such dialogues may differ from how
crowdworkers behave in real-world settings. In this study, we tackle these
issues when creating a large-scale open-domain persona dialogue corpus, where
persona implies that the conversation is performed by several actors with a
fixed persona and user-side workers from an unspecified crowd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Authorship Attribution in the Work of Tirso de Molina. (arXiv:2304.00363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00363">
<div class="article-summary-box-inner">
<span><p>Automatic Authorship Attribution (AAA) is the result of applying tools and
techniques from Digital Humanities to authorship attribution studies. Through a
quantitative and statistical approach this discipline can draw further
conclusions about renowned authorship issues which traditional critics have
been dealing with for centuries, opening a new door to style comparison. The
aim of this paper is to prove the potential of these tools and techniques by
testing the authorship of five comedies traditionally attributed to Spanish
playwright Tirso de Molina (1579-1648): La ninfa del cielo, El burlador de
Sevilla, Tan largo me lo fiais, La mujer por fuerza and El condenado por
desconfiado. To accomplish this purpose some experiments concerning clustering
analysis by Stylo package from R and four distance measures are carried out on
a corpus built with plays by Tirso, Andres de Claramonte (c. 1560-1626),
Antonio Mira de Amescua (1577-1644) and Luis Velez de Guevara (1579-1644). The
results obtained point to the denial of all the attributions to Tirso except
for the case of La mujer por fuerza.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00416">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs) have led to the development
of powerful AI chatbots capable of engaging in natural and human-like
conversations. However, these chatbots can be potentially harmful, exhibiting
manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to
be safe, trustworthy and ethical. To create healthy AI systems, we present the
SafeguardGPT framework that uses psychotherapy to correct for these harmful
behaviors in AI chatbots. The framework involves four types of AI agents: a
Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the
effectiveness of SafeguardGPT through a working example of simulating a social
conversation. Our results show that the framework can improve the quality of
conversations between AI chatbots and humans. Although there are still several
challenges and directions to be addressed in the future, SafeguardGPT provides
a promising approach to improving the alignment between AI chatbots and human
values. By incorporating psychotherapy and reinforcement learning techniques,
the framework enables AI chatbots to learn and adapt to human preferences and
values in a safe and ethical way, contributing to the development of a more
human-centric and responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural invariants and semantic fingerprints in the "ego network" of words. (arXiv:2203.00588v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00588">
<div class="article-summary-box-inner">
<span><p>Well-established cognitive models coming from anthropology have shown that,
due to the cognitive constraints that limit our "bandwidth" for social
interactions, humans organize their social relations according to a regular
structure. In this work, we postulate that similar regularities can be found in
other cognitive processes, such as those involving language production. In
order to investigate this claim, we analyse a dataset containing tweets of a
heterogeneous group of Twitter users (regular users and professional writers).
Leveraging a methodology similar to the one used to uncover the
well-established social cognitive constraints, we find regularities at both the
structural and semantic level. At the former, we find that a concentric layered
structure (which we call ego network of words, in analogy to the ego network of
social relationships) very well captures how individuals organise the words
they use. The size of the layers in this structure regularly grows
(approximately 2-3 times with respect to the previous one) when moving
outwards, and the two penultimate external layers consistently account for
approximately 60% and 30% of the used words, irrespective of the number of the
total number of layers of the user. For the semantic analysis, each ring of
each ego network is described by a semantic profile, which captures the topics
associated with the words in the ring. We find that ring #1 has a special role
in the model. It is semantically the most dissimilar and the most diverse among
the rings. We also show that the topics that are important in the innermost
ring also have the characteristic of being predominant in each of the other
rings, as well as in the entire ego network. In this respect, ring #1 can be
seen as the semantic fingerprint of the ego network of words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05711">
<div class="article-summary-box-inner">
<span><p>Despite recent advances of AI, story understanding remains an open and
under-investigated problem. We collect, preprocess, and publicly release a
video-language story dataset, Synopses of Movie Narratives (SYMON), containing
5,193 video summaries of popular movies and TV series. SYMON captures
naturalistic story-telling videos for human audience made by human creators. As
a prototypical and naturalistic story dataset, SYMON features high coverage of
multimodal story events, abundant mental-state descriptions, and large semantic
gaps between the visual and the textual modalities. We establish benchmarks on
video-text retrieval and zero-shot alignment on movie summary videos, which
showcase the importance of in-domain data in story understanding. With SYMON,
we hope to lay the groundwork for progress in multimodal story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07496">
<div class="article-summary-box-inner">
<span><p>We propose a simple and effective re-ranking method for improving passage
retrieval in open question answering. The re-ranker re-scores retrieved
passages with a zero-shot question generation model, which uses a pre-trained
language model to compute the probability of the input question conditioned on
a retrieved passage. This approach can be applied on top of any retrieval
method (e.g. neural or keyword-based), does not require any domain- or
task-specific training (and therefore is expected to generalize better to data
distribution shifts), and provides rich cross-attention between query and
passage (i.e. it must explain every token in the question). When evaluated on a
number of open-domain retrieval datasets, our re-ranker improves strong
unsupervised retrieval models by 6%-18% absolute and strong supervised models
by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new
state-of-the-art results on full open-domain question answering by simply
adding the new re-ranker to existing models with no further changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12191">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, when evaluated under
out-of-distribution (out-of-dataset) settings for VQA, we observe that these
models exhibit poor generalization. We comprehensively evaluate two pretrained
V&amp;L models under different settings (i.e. classification and open-ended text
generation) by conducting cross-dataset evaluations. We find that these models
tend to learn to solve the benchmark, rather than learning the high-level
skills required by the VQA task. We also find that in most cases generative
models are less susceptible to shifts in data distribution compared to
discriminative ones, and that multimodal pretraining is generally helpful for
OOD generalization. Finally, we revisit assumptions underlying the use of
automatic VQA evaluation metrics, and empirically show that their stringent
nature repeatedly penalizes models for correct responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12452">
<div class="article-summary-box-inner">
<span><p>Large Language Models have become the core architecture upon which most
modern natural language processing (NLP) systems build. These models can
consistently deliver impressive accuracy and robustness across tasks and
domains, but their high computational overhead can make inference difficult and
expensive. To make using these models less costly, recent work has explored
leveraging structured and unstructured pruning, quantization, and distillation
to improve inference speed and decrease size. This paper studies how models
pruned using Gradual Unstructured Magnitude Pruning can transfer between
domains and tasks. Our experimentation shows that models that are pruned during
pretraining using general domain masked language models can transfer to novel
domains and tasks without extensive hyperparameter exploration or specialized
approaches. We demonstrate that our general sparse model Sparse*BERT can become
SparseBioBERT simply by pretraining the compressed architecture on unstructured
biomedical text. Moreover, we show that SparseBioBERT can match the quality of
BioBERT with only 10\% of the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10658">
<div class="article-summary-box-inner">
<span><p>We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08562">
<div class="article-summary-box-inner">
<span><p>In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.02052">
<div class="article-summary-box-inner">
<span><p>We employ Natural Language Processing techniques to analyse 377808 English
song lyrics from the "Two Million Song Database" corpus, focusing on the
expression of sexism across five decades (1960-2010) and the measurement of
gender biases. Using a sexism classifier, we identify sexist lyrics at a larger
scale than previous studies using small samples of manually annotated popular
songs. Furthermore, we reveal gender biases by measuring associations in word
embeddings learned on song lyrics. We find sexist content to increase across
time, especially from male artists and for popular songs appearing in Billboard
charts. Songs are also shown to contain different language biases depending on
the gender of the performer, with male solo artist songs containing more and
stronger biases. This is the first large scale analysis of this type, giving
insights into language usage in such an influential part of popular culture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07316">
<div class="article-summary-box-inner">
<span><p>Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10341">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13312">
<div class="article-summary-box-inner">
<span><p>Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial. In this
work, we examine the open question of social intelligence and Theory of Mind in
modern NLP systems from an empirical and theory-based perspective. We show that
one of today's largest language models (GPT-3; Brown et al., 2020) lacks this
kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et
al., 2019), which measures models' ability to understand intents and reactions
of participants of social interactions, and ToMi (Le et al., 2019), which
measures whether models can infer mental states and realities of participants
of situations. Our results show that models struggle substantially at these
Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on
SocialIQa and ToMi, respectively. To conclude, we draw on theories from
pragmatics to contextualize this shortcoming of large language models, by
examining the limitations stemming from their data, neural architecture, and
training paradigms. Challenging the prevalent narrative that only scale is
needed, we posit that person-centric NLP approaches might be more effective
towards neural Theory of Mind.
</p>
<p>In our updated version, we also analyze newer instruction tuned and RLFH
models for neural ToM. We find that even ChatGPT and GPT-4 do not display
emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on
the ToMi questions related to mental states and realities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08142">
<div class="article-summary-box-inner">
<span><p>Mathematical notation makes up a large portion of STEM literature, yet,
finding semantic representations for formulae remains a challenging problem.
Because mathematical notation is precise, and its meaning changes significantly
with small character shifts, the methods that work for natural text do not
necessarily work well for mathematical expressions. In this work, we describe
an approach for representing mathematical expressions in a continuous vector
space. We use the encoder of a sequence-to-sequence architecture, trained on
visually different but mathematically equivalent expressions, to generate
vector representations (or embeddings). We compare this approach with an
autoencoder and show that the former is better at capturing mathematical
semantics. Finally, to expedite future research, we publish a corpus of
equivalent transcendental and algebraic expression pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model. (arXiv:2211.11152v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11152">
<div class="article-summary-box-inner">
<span><p>Large-scale Transformer models bring significant improvements for various
downstream vision language tasks with a unified architecture. The performance
improvements come with increasing model size, resulting in slow inference speed
and increased cost for severing. While some certain predictions benefit from
the full complexity of the large-scale model, not all of inputs need the same
amount of computation to conduct, potentially leading to computation resource
waste. To handle this challenge, early exiting is proposed to adaptively
allocate computational power in term of input complexity to improve inference
efficiency. The existing early exiting strategies usually adopt output
confidence based on intermediate layers as a proxy of input complexity to incur
the decision of skipping following layers. However, such strategies cannot
apply to encoder in the widely-used unified architecture with both encoder and
decoder due to difficulty of output confidence estimation in the encoder. It is
suboptimal in term of saving computation power to ignore the early exiting in
encoder component. To handle this challenge, we propose a novel early exiting
strategy for unified visual language models, which allows dynamically skip the
layers in encoder and decoder simultaneously in term of input layer-wise
similarities with multiple times of early exiting, namely \textbf{MuE}. By
decomposing the image and text modalities in the encoder, MuE is flexible and
can skip different layers in term of modalities, advancing the inference
efficiency while minimizing performance drop. Experiments on the SNLI-VE and MS
COCO datasets show that the proposed approach MuE can reduce expected inference
time by up to 50\% and 40\% while maintaining 99\% and 96\% performance
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13613">
<div class="article-summary-box-inner">
<span><p>Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Montague semantics and modifier consistency measurement in neural language models. (arXiv:2212.04310v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04310">
<div class="article-summary-box-inner">
<span><p>In recent years, distributional language representation models have
demonstrated great practical success. At the same time, the need for
interpretability has elicited questions on their intrinsic properties and
capabilities. Crucially, distributional models are often inconsistent when
dealing with compositional phenomena in natural language, which has significant
implications for their safety and fairness. Despite this, most current research
on compositionality is directed towards improving their performance on
similarity tasks only. This work takes a different approach, and proposes a
methodology for measuring compositional behavior in contemporary language
models. Specifically, we focus on adjectival modifier phenomena in
adjective-noun phrases. We introduce three novel tests of compositional
behavior inspired by Montague semantics. Our experimental results indicate that
current neural language models behave according to the expected linguistic
theories to a limited extent only. This raises the question of whether these
language models are not able to capture the semantic properties we evaluated,
or whether linguistic theories from Montagovian tradition would not match the
expected capabilities of distributional models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13201">
<div class="article-summary-box-inner">
<span><p>Operations research deals with modeling and solving real-world problems as
mathematical optimization problems. While solving mathematical systems is
accomplished by analytical software, formulating a problem as a set of
mathematical operations has been typically done manually by domain experts.
Recent machine learning methods have shown promise in converting textual
problem descriptions to corresponding mathematical formulations. This paper
presents an approach that converts linear programming word problems into
mathematical formulations. We leverage the named entities in the input and
augment the input to highlight these entities. Our approach achieves the
highest accuracy among all submissions to the NL4Opt Competition, securing
first place in the generation track.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology. (arXiv:2301.02228v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02228">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider enhancing medical visual-language pre-training
(VLP) with domain-specific knowledge, by exploiting the paired image-text
reports from the radiological daily practice. In particular, we make the
following contributions: First, unlike existing works that directly process the
raw reports, we adopt a novel triplet extraction module to extract the
medical-related information, avoiding unnecessary complexity from language
grammar and enhancing the supervision signals; Second, we propose a novel
triplet encoding module with entity translation by querying a knowledge base,
to exploit the rich domain knowledge in medical field, and implicitly build
relationships between medical entities in the language embedding space; Third,
we propose to use a Transformer-based fusion model for spatially aligning the
entity description with visual signals at the image patch level, enabling the
ability for medical diagnosis; Fourth, we conduct thorough experiments to
validate the effectiveness of our architecture, and benchmark on numerous
public benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,
COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning
settings, our model has demonstrated strong performance compared with the
former methods on disease classification and grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02748">
<div class="article-summary-box-inner">
<span><p>Protein language models (LMs) have been successful in sequence, structural
and functional predictions. However, currently, protein LMs are limited to
encoder- or decoder-only architectures for single sequences while many
biological contexts involve protein-protein interactions. Here, we introduce
pAbT5, which models antibody chain pairing as forward- and back-translations
using a T5-based architecture. We show that pAbT5 accurately reflects chain
pairing through sequence generation. Our protein LM generates variable-length
sequences and its next-word prediction probability agrees with
position-specific scoring matrix from sequence alignment. Like other works in
protein LM, pAbT5 performs state-of-the-art unsupervised prediction on
experimental measurements. To the best of our knowledge, pAbT5 is the first
generative encoder-decoder protein LM for protein-protein interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09767">
<div class="article-summary-box-inner">
<span><p>In this paper, a new perspective is suggested for unsupervised Ontology
Matching (OM) or Ontology Alignment (OA) by treating it as a translation task.
Ontologies are represented as graphs, and the translation is performed from a
node in the source ontology graph to a path in the target ontology graph. The
proposed framework, Truveta Mapper (TM), leverages a multi-task
sequence-to-sequence transformer model to perform alignment across multiple
ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables
the model to implicitly learn the relationship between different ontologies via
transfer-learning without requiring any explicit cross-ontology manually
labeled data. This also enables the formulated framework to outperform existing
solutions for both runtime latency and alignment quality. The model is
pre-trained and fine-tuned only on publicly available text corpus and
inner-ontologies data. The proposed solution outperforms state-of-the-art
approaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented
new OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers
log-linear complexity in contrast to quadratic in the existing end-to-end
methods, and overall makes the OM task efficient and more straightforward
without much post-processing involving mapping extension or mapping repair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08956">
<div class="article-summary-box-inner">
<span><p>Africa is home to over 2000 languages from over six language families and has
the highest linguistic diversity among all continents. This includes 75
languages with at least one million speakers each. Yet, there is little NLP
research conducted on African languages. Crucial in enabling such research is
the availability of high-quality annotated datasets. In this paper, we
introduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets
in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,
Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,
Tigrinya, Twi, Xitsonga, and Yor\`ub\'a) from four language families annotated
by native speakers. The data is used in SemEval 2023 Task 12, the first
Afro-centric SemEval shared task. We describe the data collection methodology,
annotation process, and related challenges when curating each of the datasets.
We conduct experiments with different sentiment classification baselines and
discuss their usefulness. We hope AfriSenti enables new work on
under-represented languages. The dataset is available at
https://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be
loaded as a huggingface datasets
(https://huggingface.co/datasets/shmuhammad/AfriSenti).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09184">
<div class="article-summary-box-inner">
<span><p>With the popularity of the recent Transformer-based models represented by
BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range
of natural language processing tasks. However, the massive computations, huge
memory footprint, and thus high latency of Transformer-based models is an
inevitable challenge for the cloud with high real-time requirement. To tackle
the issue, we propose BBCT, a method of block-wise bit-compression for
transformer without retraining. Our method achieves more fine-grained
compression of the whole transformer, including embedding, matrix
multiplication, GELU, softmax, layer normalization, and all the intermediate
results. As a case, we compress an efficient BERT with the method of BBCT. Our
benchmark test results on General Language Understanding Evaluation (GLUE) show
that BBCT can achieve less than 1% accuracy drop in most tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12528">
<div class="article-summary-box-inner">
<span><p>Generative AI models have impressive performance on many Natural Language
Processing tasks such as language understanding, reasoning and language
generation. One of the most important questions that is being asked by the AI
community today is about the capabilities and limits of these models, and it is
clear that evaluating generative AI is very challenging. Most studies on
generative Large Language Models (LLMs) are restricted to English and it is
unclear how capable these models are at understanding and generating other
languages. We present the first comprehensive benchmarking of generative LLMs -
MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse
tasks and 33 typologically diverse languages. We also compare the performance
of generative LLMs to State of the Art (SOTA) non-autoregressive models on
these tasks to determine how well generative models perform compared to the
previous generation of LLMs. We present a thorough analysis of the performance
of models across languages and discuss some of the reasons why generative LLMs
are currently not optimal for all languages. We create a framework for
evaluating generative LLMs in the multilingual setting and provide directions
for future progress in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12570">
<div class="article-summary-box-inner">
<span><p>The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model, which allows for the effective
utilization of repository-level information for code completion and grants the
ability to generate code at various levels of granularity. Furthermore,
RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges
the gap between retrieval context and the intended completion target. We also
propose a new benchmark RepoEval, which consists of the latest and high-quality
real-world repositories covering line, API invocation, and function body
completion scenarios. We test the performance of RepoCoder by using various
combinations of code retrievers and generators. Experimental results indicate
that RepoCoder significantly improves the zero-shot code completion baseline by
over 10% in all settings and consistently outperforms the vanilla
retrieval-augmented code completion approach. Furthermore, we validate the
effectiveness of RepoCoder through comprehensive analysis, providing valuable
insights for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14070">
<div class="article-summary-box-inner">
<span><p>Recent large language models (LLMs) in the general domain, such as ChatGPT,
have shown remarkable success in following instructions and producing
human-like responses. However, such language models have not been tailored to
the medical domain, resulting in poor answer accuracy and inability to give
plausible recommendations for medical diagnosis, medications, etc. To address
this issue, we collected more than 700 diseases and their corresponding
symptoms, required medical tests, and recommended medications, from which we
generated 5K doctor-patient conversations. In addition, we obtained 200K real
patient-doctor conversations from online Q\&amp;A medical consultation sites. By
fine-tuning LLMs using these 205k doctor-patient conversations, the resulting
models emerge with great potential to understand patients' needs, provide
informed advice, and offer valuable assistance in a variety of medical-related
fields. The integration of these advanced language models into healthcare can
revolutionize the way healthcare professionals and patients communicate,
ultimately improving the overall efficiency and quality of patient care and
outcomes. In addition, we made public all the source codes, datasets, and model
weights to facilitate the further development of dialogue models in the medical
field. The training data, codes, and weights of this project are available at:
The training data, codes, and weights of this project are available at:
https://github.com/Kent0n-Li/ChatDoctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Information Extraction Study: Take In Mind the Tokenization!. (arXiv:2303.15100v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15100">
<div class="article-summary-box-inner">
<span><p>Current research on the advantages and trade-offs of using characters,
instead of tokenized text, as input for deep learning models, has evolved
substantially. New token-free models remove the traditional tokenization step;
however, their efficiency remains unclear. Moreover, the effect of tokenization
is relatively unexplored in sequence tagging tasks. To this end, we investigate
the impact of tokenization when extracting information from documents and
present a comparative study and analysis of subword-based and character-based
models. Specifically, we study Information Extraction (IE) from biomedical
texts. The main outcome is twofold: tokenization patterns can introduce
inductive bias that results in state-of-the-art performance, and the
character-based models produce promising results; thus, transitioning to
token-free IE models is feasible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17580">
<div class="article-summary-box-inner">
<span><p>Solving complicated AI tasks with different domains and modalities is a key
step toward advanced artificial intelligence. While there are abundant AI
models available for different domains and modalities, they cannot handle
complicated AI tasks. Considering large language models (LLMs) have exhibited
exceptional ability in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks and language could be a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, a
framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in
machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT is able to cover numerous sophisticated AI tasks in different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards advanced
artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17649">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a methodology to align a medium-sized GPT model,
originally trained in English for an open domain, to a small closed domain in
Spanish. The application for which the model is finely tuned is the question
answering task. To achieve this we also needed to train and implement another
neural network (which we called the reward model) that could score and
determine whether an answer is appropriate for a given question. This component
served to improve the decoding and generation of the answers of the system.
Numerical metrics such as BLEU and perplexity were used to evaluate the model,
and human judgment was also used to compare the decoding technique with others.
Finally, the results favored the proposed method, and it was determined that it
is feasible to use a reward model to align the generation of responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Referring Image Segmentation with Global-Local Context Features. (arXiv:2303.17811v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17811">
<div class="article-summary-box-inner">
<span><p>Referring image segmentation (RIS) aims to find a segmentation mask given a
referring expression grounded to a region of the input image. Collecting
labelled datasets for this task, however, is notoriously costly and
labor-intensive. To overcome this issue, we propose a simple yet effective
zero-shot referring image segmentation method by leveraging the pre-trained
cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded
to the input text, we propose a mask-guided visual encoder that captures global
and local contextual information of an input image. By utilizing instance masks
obtained from off-the-shelf mask proposal techniques, our method is able to
segment fine-detailed Istance-level groundings. We also introduce a
global-local text encoder where the global feature captures complex
sentence-level semantics of the entire input expression while the local feature
focuses on the target noun phrase extracted by a dependency parser. In our
experiments, the proposed method outperforms several zero-shot baselines of the
task and even the weakly supervised referring expression segmentation method
with substantial margins. Our code is available at
https://github.com/Seonghoon-Yu/Zero-shot-RIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18149">
<div class="article-summary-box-inner">
<span><p>The engineering community has recently witnessed the emergence of chatbot
technology with the release of OpenAI ChatGPT-4 and Google Bard. While these
chatbots have been reported to perform well and even pass various standardized
tests, including medical and law exams, this forum paper explores whether these
chatbots can also pass the Fundamentals of Engineering (FE) and Principles and
Practice of Engineering (PE) exams. A diverse range of civil and environmental
engineering questions and scenarios are used to evaluate the chatbots'
performance, as commonly present in the FE and PE exams. The chatbots'
responses were analyzed based on their relevance, accuracy, and clarity and
then compared against the recommendations of the National Council of Examiners
for Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and
Bard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in
the PE exam. It is evident that the current version of ChatGPT-4 could
potentially pass the FE exam. While future editions are much more likely to
pass both exams, this study also highlights the potential of using chatbots as
teaching assistants and guiding engineers.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-04 23:11:18.706209695 UTC">2023-04-04 23:11:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>