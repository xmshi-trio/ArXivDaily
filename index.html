<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-14T01:30:00Z">12-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Mortality Prediction Models with Clinical Notes Using Sparse Attention at the Word and Sentence Levels. (arXiv:2212.06267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06267">
<div class="article-summary-box-inner">
<span><p>Intensive Care in-hospital mortality prediction has various clinical
applications. Neural prediction models, especially when capitalising on
clinical notes, have been put forward as improvement on currently existing
models. However, to be acceptable these models should be performant and
transparent. This work studies different attention mechanisms for clinical
neural prediction models in terms of their discrimination and calibration.
Specifically, we investigate sparse attention as an alternative to dense
attention weights in the task of in-hospital mortality prediction from clinical
notes. We evaluate the attention mechanisms based on: i) local self-attention
over words in a sentence, and ii) global self-attention with a transformer
architecture across sentences. We demonstrate that the sparse mechanism
approach outperforms the dense one for the local self-attention in terms of
predictive performance with a publicly available dataset, and puts higher
attention to prespecified relevant directive words. The performance at the
sentence level, however, deteriorates as sentences including the influential
directive words tend to be dropped all together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety. (arXiv:2212.06295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06295">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exploded in popularity in the past few
years and have achieved undeniably impressive results on benchmarks as varied
as question answering and text summarization. We provide a simple new prompting
strategy that leads to yet another supposedly "super-human" result, this time
outperforming humans at common sense ethical reasoning (as measured by accuracy
on a subset of the ETHICS dataset). Unfortunately, we find that relying on
average performance to judge capabilities can be highly misleading. LLM errors
differ systematically from human errors in ways that make it easy to craft
adversarial examples, or even perturb existing examples to flip the output
label. We also observe signs of inverse scaling with model size on some
examples, and show that prompting models to "explain their reasoning" often
leads to alarming justifications of unethical actions. Our results highlight
how human-like performance does not necessarily imply human-like understanding
or reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition. (arXiv:2212.06346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06346">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in Natural Language Understanding (NLU), the creation
of multilingual NLU systems remains a challenge. It is common to have NLU
systems limited to a subset of languages due to lack of available data. They
also often vary widely in performance. We launch a three-phase approach to
address the limitations in NLU and help propel NLU technology to new heights.
We release a 52 language dataset called the Multilingual Amazon SLU resource
package (SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation, or MASSIVE, in an effort to address parallel data availability for
voice assistants. We organize the Massively Multilingual NLU 2022 Challenge to
provide a competitive environment and push the state-of-the art in the
transferability of models into other languages. Finally, we host the first
Massively Multilingual NLU workshop which brings these components together. The
MMNLU workshop seeks to advance the science behind multilingual NLU by
providing a platform for the presentation of new research in the field and
connecting teams working on this research direction. This paper summarizes the
dataset, workshop and the competition and the findings of each phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model. (arXiv:2212.06369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06369">
<div class="article-summary-box-inner">
<span><p>Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06373">
<div class="article-summary-box-inner">
<span><p>Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through multi-head attention based intention fusion module to capture
the speaker's intention. Besides, we utilize previous utterances to predict the
last utterance, which simulates human's psychology to guess what the
interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a general purpose machine translation system for Sranantongo. (arXiv:2212.06383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06383">
<div class="article-summary-box-inner">
<span><p>Machine translation for Sranantongo (Sranan, srn), a low-resource Creole
language spoken predominantly in Surinam, is virgin territory. In this study we
create a general purpose machine translation system for srn. In order to
facilitate this research, we introduce the SRNcorpus, a collection of parallel
Dutch (nl) to srn and monolingual srn data. We experiment with a wide range of
proven machine translation methods. Our results demonstrate a strong baseline
machine translation system for srn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities. (arXiv:2212.06385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06385">
<div class="article-summary-box-inner">
<span><p>Recently, the success of pre-training in text domain has been fully extended
to vision, audio, and cross-modal scenarios. The proposed pre-training models
of different modalities are showing a rising trend of homogeneity in their
model structures, which brings the opportunity to implement different
pre-training models within a uniform framework. In this paper, we present
TencentPretrain, a toolkit supporting pre-training models of different
modalities. The core feature of TencentPretrain is the modular design. The
toolkit uniformly divides pre-training models into 5 components: embedding,
encoder, target embedding, decoder, and target. As almost all of common modules
are provided in each component, users can choose the desired modules from
different components to build a complete pre-training model. The modular design
enables users to efficiently reproduce existing pre-training models or build
brand-new one. We test the toolkit on text, vision, and audio benchmarks and
show that it can match the performance of the original implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and Speaker-wise Normalization in Speech Synthesis. (arXiv:2212.06397v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06397">
<div class="article-summary-box-inner">
<span><p>Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesised speech of a target speaker's timbre. Most
previous approaches rely on data with style labels, but manually-annotated
labels are expensive and not always reliable. In response to this problem, we
propose Style-Label-Free, a cross-speaker style transfer method, which can
realize the style transfer from source speaker to target speaker without style
labels. Firstly, a reference encoder structure based on quantized variational
autoencoder (Q-VAE) and style bottleneck is designed to extract discrete style
representations. Secondly, a speaker-wise batch normalization layer is proposed
to reduce the source speaker leakage. In order to improve the style extraction
ability of the reference encoder, a style invariant and contrastive data
augmentation method is proposed. Experimental results show that the method
outperforms the baseline. We provide a website with audio samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lisan: Yemenu, Irqi, Libyan, and Sudanese Arabic Dialect Copora with Morphological Annotations. (arXiv:2212.06468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06468">
<div class="article-summary-box-inner">
<span><p>This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and
Libyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.
We collected the content of the corpora from several social media platforms.
The Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.
The corpora of the other three dialects (~ 50K tokens each) came manually from
Facebook and YouTube posts and comments.
</p>
<p>Thirty five (35) annotators who are native speakers of the target dialects
carried out the annotations. The annotators segemented all words in the four
corpora into prefixes, stems and suffixes and labeled each with different
morphological features such as part of speech, lemma, and a gloss in English.
An Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the
annation. The annotators were trained on a set of guidelines and on how to use
ADAT. We developed ADAT to assist the annotators and to ensure compatibility
with SAMA and Curras tagsets. The tool is open source, and the four corpora are
also available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly-Supervised Named Entity Recognition with Adaptive Teacher Learning and Fine-grained Student Ensemble. (arXiv:2212.06522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06522">
<div class="article-summary-box-inner">
<span><p>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates
the data scarcity problem in NER by automatically generating training samples.
Unfortunately, the distant supervision may induce noisy labels, thus
undermining the robustness of the learned models and restricting the practical
application. To relieve this problem, recent works adopt self-training
teacher-student frameworks to gradually refine the training labels and improve
the generalization ability of NER models. However, we argue that the
performance of the current self-training frameworks for DS-NER is severely
underestimated by their plain designs, including both inadequate student
learning and coarse-grained teacher updating. Therefore, in this paper, we make
the first attempt to alleviate these issues by proposing: (1) adaptive teacher
learning comprised of joint training of two teacher-student networks and
considering both consistent and inconsistent predictions between two teachers,
thus promoting comprehensive student learning. (2) fine-grained student
ensemble that updates each fragment of the teacher model with a temporal moving
average of the corresponding fragment of the student, which enhances consistent
predictions on each model fragment against noise. To verify the effectiveness
of our proposed method, we conduct experiments on four DS-NER datasets. The
experimental results demonstrate that our method significantly surpasses
previous SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling Stance Detection as Textual Entailment Recognition and Leveraging Measurement Knowledge from Social Sciences. (arXiv:2212.06543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06543">
<div class="article-summary-box-inner">
<span><p>Stance detection (SD) can be considered a special case of textual entailment
recognition (TER), a generic natural language task. Modelling SD as TER may
offer benefits like more training data and a more general learning scheme. In
this paper, we present an initial empirical analysis of this approach. We apply
it to a difficult but relevant test case where no existing labelled SD dataset
is available, because this is where modelling SD as TER may be especially
helpful. We also leverage measurement knowledge from social sciences to improve
model performance. We discuss our findings and suggest future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized Latent Updates for Fine-Tuning Vision-Language Models. (arXiv:2212.06556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06556">
<div class="article-summary-box-inner">
<span><p>Although massive pre-trained vision-language models like CLIP show impressive
generalization capabilities for many tasks, still it often remains necessary to
fine-tune them for improved performance on specific datasets. When doing so, it
is desirable that updating the model is fast and that the model does not lose
its capabilities on data outside of the dataset, as is often the case with
classical fine-tuning approaches. In this work we suggest a lightweight
adapter, that only updates the models predictions close to seen datapoints. We
demonstrate the effectiveness and speed of this relatively simple approach in
the context of few-shot learning, where our results both on classes seen and
unseen during training are comparable with or improve on the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Fake News Detection with Heterogeneous Social Media Context Graphs. (arXiv:2212.06560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06560">
<div class="article-summary-box-inner">
<span><p>Fake news detection has become a research area that goes way beyond a purely
academic interest as it has direct implications on our society as a whole.
Recent advances have primarily focused on textbased approaches. However, it has
become clear that to be effective one needs to incorporate additional,
contextual information such as spreading behaviour of news articles and user
interaction patterns on social media. We propose to construct heterogeneous
social context graphs around news articles and reformulate the problem as a
graph classification task. Exploring the incorporation of different types of
information (to get an idea as to what level of social context is most
effective) and using different graph neural network architectures indicates
that this approach is highly effective with robust results on a common
benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category Theory for Quantum Natural Language Processing. (arXiv:2212.06615v1 [math.CT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06615">
<div class="article-summary-box-inner">
<span><p>This thesis introduces quantum natural language processing (QNLP) models
based on a simple yet powerful analogy between computational linguistics and
quantum mechanics: grammar as entanglement. The grammatical structure of text
and sentences connects the meaning of words in the same way that entanglement
structure connects the states of quantum systems. Category theory allows to
make this language-to-qubit analogy formal: it is a monoidal functor from
grammar to vector spaces. We turn this abstract analogy into a concrete
algorithm that translates the grammatical structure onto the architecture of
parameterised quantum circuits. We then use a hybrid classical-quantum
algorithm to train the model so that evaluating the circuits computes the
meaning of sentences in data-driven tasks.
</p>
<p>The implementation of QNLP models motivated the development of DisCoPy
(Distributional Compositional Python), the toolkit for applied category theory
of which the first chapter gives a comprehensive overview. String diagrams are
the core data structure of DisCoPy, they allow to reason about computation at a
high level of abstraction. We show how they can encode both grammatical
structures and quantum circuits, but also logical formulae, neural networks or
arbitrary Python code. Monoidal functors allow to translate these abstract
diagrams into concrete computation, interfacing with optimised task-specific
libraries.
</p>
<p>The second chapter uses DisCopy to implement QNLP models as parameterised
functors from grammar to quantum circuits. It gives a first proof-of-concept
for the more general concept of functorial learning: generalising machine
learning from functions to functors by learning from diagram-like data. In
order to learn optimal functor parameters via gradient descent, we introduce
the notion of diagrammatic differentiation: a graphical calculus for computing
the gradients of parameterised diagrams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorical Tools for Natural Language Processing. (arXiv:2212.06636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06636">
<div class="article-summary-box-inner">
<span><p>This thesis develops the translation between category theory and
computational linguistics as a foundation for natural language processing. The
three chapters deal with syntax, semantics and pragmatics. First, string
diagrams provide a unified model of syntactic structures in formal grammars.
Second, functors compute semantics by turning diagrams into logical, tensor,
neural or quantum computation. Third, the resulting functorial models can be
composed to form games where equilibria are the solutions of language
processing tasks. This framework is implemented as part of DisCoPy, the Python
library for computing with string diagrams. We describe the correspondence
between categorical, linguistic and computational structures, and demonstrate
their applications in compositional natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?. (arXiv:2212.06645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06645">
<div class="article-summary-box-inner">
<span><p>Traditional multi-task learning architectures train a single model across
multiple tasks through a shared encoder followed by task-specific decoders.
Learning these models often requires specialized training algorithms that
address task-conflict in the shared parameter updates, which otherwise can lead
to negative transfer. A new type of multi-task learning within NLP homogenizes
multi-task architectures as a shared encoder and language model decoder, which
does surprisingly well across a range of diverse tasks. Does this new
architecture suffer from task-conflicts that require specialized training
algorithms? We study how certain factors in the shift towards text-to-text
models affects multi-task conflict and negative transfer, finding that both
directional conflict and transfer are surprisingly constant across
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06711">
<div class="article-summary-box-inner">
<span><p>Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions towards more valid and
reliable TPC research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Prompting: Scaling In-Context Learning to 1,000 Examples. (arXiv:2212.06713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06713">
<div class="article-summary-box-inner">
<span><p>Large language models have exhibited intriguing in-context learning
capability, achieving promising zero- and few-shot performance without updating
the parameters. However, conventional in-context learning is usually restricted
by length constraints, rendering it ineffective to absorb supervision from a
large number of examples. In order to go beyond few shots, we introduce
structured prompting that breaks the length limit and scales in-context
learning to thousands of examples. Specifically, demonstration examples are
separately encoded with well-designed position embeddings, and then they are
jointly attended by the test example using a rescaled attention mechanism. So
we can scale the number of exemplars with linear complexity instead of
quadratic complexity with respect to length. Experimental results on a diverse
set of tasks show that our approach improves end-task performance and reduces
evaluation variance over conventional in-context learning as the number of
demonstration examples increases. Code has been released at
https://aka.ms/structured-prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Turing Deception. (arXiv:2212.06721v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06721">
<div class="article-summary-box-inner">
<span><p>This research revisits the classic Turing test and compares recent large
language models such as ChatGPT for their abilities to reproduce human-level
comprehension and compelling text generation. Two task challenges --
summarization, and question answering -- prompt ChatGPT to produce original
content (98-99%) from a single text entry and also sequential questions
originally posed by Turing in 1950. The question of a machine fooling a human
judge recedes in this work relative to the question of "how would one prove
it?" The original contribution of the work presents a metric and simple
grammatical set for understanding the writing mechanics of chatbots in
evaluating their readability and statistical clarity, engagement, delivery, and
overall quality. While Turing's original prose scores at least 14% below the
machine-generated output, the question of whether an algorithm displays hints
of Turing's truly original thoughts (the "Lovelace 2.0" test) remains
unanswered and potentially unanswerable for now.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06742">
<div class="article-summary-box-inner">
<span><p>Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
will make our code and pre-trained models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Earthquake Impact Analysis Based on Text Mining and Social Media Analytics. (arXiv:2212.06765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06765">
<div class="article-summary-box-inner">
<span><p>Earthquakes have a deep impact on wide areas, and emergency rescue operations
may benefit from social media information about the scope and extent of the
disaster. Therefore, this work presents a text miningbased approach to collect
and analyze social media data for early earthquake impact analysis. First,
disasterrelated microblogs are collected from the Sina microblog based on
crawler technology. Then, after data cleaning a series of analyses are
conducted including (1) the hot words analysis, (2) the trend of the number of
microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and
rule-based text classification for earthquake impact analysis. Finally, two
recent earthquakes with the same magnitude and focal depth in China are
analyzed to compare their impacts. The results show that the public opinion
trend analysis and the trend of public opinion sentiment can estimate the
earthquake's social impact at an early stage, which will be helpful to
decision-making and rescue management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06800">
<div class="article-summary-box-inner">
<span><p>In-context learning has shown great success in i.i.d semantic parsing splits,
where the training and test sets are drawn from the same distribution. In this
setup, models are typically prompted with demonstrations that are similar to
the input question. However, in the setup of compositional generalization,
where models are tested on outputs with structures that are absent from the
training set, selecting similar demonstrations is insufficient, as often no
example will be similar enough to the input. In this work, we propose a method
to select diverse demonstrations that aims to collectively cover all of the
structures required in the output program, in order to encourage the model to
generalize to new structures from these demonstrations. We empirically show
that combining diverse demonstrations with in-context learning substantially
improves performance across three compositional generalization semantic parsing
datasets in the pure in-context learning setup and when combined with
finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06801">
<div class="article-summary-box-inner">
<span><p>Pragmatics is an essential part of communication, but it remains unclear what
mechanisms underlie human pragmatic communication and whether NLP systems
capture pragmatic language understanding. To investigate both these questions,
we perform a fine-grained comparison of language models and humans on seven
pragmatic phenomena, using zero-shot prompting on an expert-curated set of
English materials. We ask whether models (1) select pragmatic interpretations
of speaker utterances, (2) make similar error patterns as humans, and (3) use
similar linguistic cues as humans to solve the tasks. We find that the largest
models achieve high accuracy and match human error patterns: within incorrect
responses, models favor the literal interpretation of an utterance over
heuristic-based distractors. We also find evidence that models and humans are
sensitive to similar linguistic cues. Our results suggest that even
paradigmatic pragmatic phenomena may be solved without explicit representations
of other agents' mental states, and that artificial models can be used to gain
mechanistic insights into human pragmatic processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06817">
<div class="article-summary-box-inner">
<span><p>By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06378">
<div class="article-summary-box-inner">
<span><p>The problem of answering questions using knowledge from pre-trained language
models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA
context (question and answer choice), methods need to (i) identify relevant
knowledge from large KGs, and (ii) perform joint reasoning over the QA context
and KG. In this work, we propose a new model, QA-GNN, which addresses the above
challenges through two key innovations: (i) relevance scoring, where we use LMs
to estimate the importance of KG nodes relative to the given QA context, and
(ii) joint reasoning, where we connect the QA context and KG to form a joint
graph, and mutually update their representations through graph neural networks.
We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,
OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing
LM and LM+KG models, and exhibits capabilities to perform interpretable and
structured reasoning, e.g., correctly handling negation in questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textless Speech Emotion Conversion using Discrete and Decomposed Representations. (arXiv:2111.07402v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07402">
<div class="article-summary-box-inner">
<span><p>Speech emotion conversion is the task of modifying the perceived emotion of a
speech utterance while preserving the lexical content and speaker identity. In
this study, we cast the problem of emotion conversion as a spoken language
translation task. We use a decomposition of the speech signal into discrete
learned representations, consisting of phonetic-content units, prosodic
features, speaker, and emotion. First, we modify the speech content by
translating the phonetic-content units to a target emotion, and then predict
the prosodic features based on these units. Finally, the speech waveform is
generated by feeding the predicted representations into a neural vocoder. Such
a paradigm allows us to go beyond spectral and parametric changes of the
signal, and model non-verbal vocalizations, such as laughter insertion, yawning
removal, etc. We demonstrate objectively and subjectively that the proposed
method is vastly superior to current approaches and even beats text-based
systems in terms of perceived emotion and audio quality. We rigorously evaluate
all components of such a complex system and conclude with an extensive model
analysis and ablation study to better emphasize the architectural choices,
strengths and weaknesses of the proposed method. Samples are available under
the following link: https://speechbot.github.io/emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Transformers: Vocabulary Transfer. (arXiv:2112.14569v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14569">
<div class="article-summary-box-inner">
<span><p>Transformers are responsible for the vast majority of recent advances in
natural language processing. The majority of practical natural language
processing applications of these models are typically enabled through transfer
learning. This paper studies if corpus-specific tokenization used for
fine-tuning improves the resulting performance of the model. Through a series
of experiments, we demonstrate that such tokenization combined with the
initialization and fine-tuning strategy for the vocabulary tokens speeds up the
transfer and boosts the performance of the fine-tuned model. We call this
aspect of transfer facilitation vocabulary transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HighMMT: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Many real-world problems are inherently multimodal, from the communicative
modalities humans use to express social and emotional states to the force,
proprioception, and visual sensors ubiquitous on robots. While there has been
an explosion of interest in multimodal representation learning, these methods
are still largely focused on a small set of modalities, primarily in the
language, vision, and audio space. In order to accelerate generalization
towards diverse and understudied modalities, this paper studies efficient
representation learning for high-modality scenarios. Since adding new models
for every new modality or task becomes prohibitively expensive, a critical
technical challenge is heterogeneity quantification: how can we measure which
modalities encode similar information and interactions in order to permit
parameter sharing with previous modalities? We propose two new
information-theoretic metrics for heterogeneity quantification: (1) modality
heterogeneity studies how similar 2 modalities $\{X_1,X_2\}$ are by measuring
how much information can be transferred from $X_1$ to $X_2$, while (2)
interaction heterogeneity studies how similarly pairs of modalities
$\{X_1,X_2\}, \{X_3,X_4\}$ interact by measuring how much interaction
information can be transferred from $\{X_1,X_2\}$ to $\{X_3,X_4\}$. We show the
importance of these proposed metrics in high-modality scenarios as a way to
automatically prioritize the fusion of modalities that contain unique
information or interactions. The result is a single model, HighMMT, that scales
up to $10$ modalities and $15$ tasks from $5$ different research areas. Not
only does HighMMT outperform prior methods on the tradeoff between performance
and efficiency, it also demonstrates a crucial scaling behavior: performance
continues to improve with each modality added, and transfers to entirely new
modalities and tasks during fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models. (arXiv:2205.10558v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10558">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) represents a large collection of tasks in
the field of NLP. While many of these tasks have been tackled well by the
cross-entropy (CE) loss, the task of dialog generation poses a few unique
challenges for this loss function. First, CE loss assumes that for any given
input, the only possible output is the one available as the ground truth in the
training dataset. In general, this is not true for any task, as there can be
multiple semantically equivalent sentences, each with a different surface form.
This problem gets exaggerated further for the dialog generation task, as there
can be multiple valid responses (for a given context) that not only have
different surface forms but are also not semantically equivalent. Second, CE
loss does not take the context into consideration while processing the response
and, hence, it treats all ground truths with equal importance irrespective of
the context. But, we may want our final agent to avoid certain classes of
responses (e.g. bland, non-informative or biased responses) and give relatively
higher weightage for more context-specific responses. To circumvent these
shortcomings of the CE loss, in this paper, we propose a novel loss function,
CORAL, that directly optimizes recently proposed estimates of human preference
for generated responses. Using CORAL, we can train dialog generation models
without assuming non-existence of response other than the ground-truth. Also,
the CORAL loss is computed based on both the context and the response.
Extensive comparisons on two benchmark datasets show that the proposed methods
outperform strong state-of-the-art baseline models of different sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Round-Trip Translation for Machine Translation Evaluation. (arXiv:2209.07351v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07351">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation on low-resource language translation suffers from a
deficiency of parallel corpora. Round-trip translation could be served as a
clever and straightforward technique to alleviate the requirement of the
parallel evaluation corpus. However, there was an observation of obscure
correlations between the evaluation scores by forward and round-trip
translations in the era of statistical machine translation (SMT). In this
paper, we report the surprising finding that round-trip translation can be used
for automatic evaluation without the references. Firstly, our revisit on the
round-trip translation in SMT evaluation unveils that its long-standing
misunderstanding is essentially caused by copying mechanism. After removing
copying mechanism in SMT, round-trip translation scores can appropriately
reflect the forward translation performance. Then, we demonstrate the
rectification is overdue as round-trip translation could benefit multiple
machine translation evaluation tasks. To be more specific, round-trip
translation could be used i) to predict corresponding forward translation
scores; ii) to improve the performance of the recently advanced quality
estimation model; and iii) to identify adversarial competitors in shared tasks
via cross-system verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models. (arXiv:2210.12360v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12360">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models show significant performance gains
for zero-shot cross-lingual model transfer on a wide range of natural language
understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,
pre-trained models are only fine-tuned on English data and tested on a variety
of target languages. In this paper, we do cross-lingual evaluation on various
NLU tasks (sentence classification, sequence labeling, question answering)
using prompt-tuning and compare it with fine-tuning. The results show that
prompt tuning achieves much better cross-lingual transfer than fine-tuning
across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we
demonstrate through the analysis that prompt tuning can have better
cross-lingual transferability of representations on downstream tasks with
better aligned decision boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reuse Distractors to support Multiple Choice Question Generation in Education. (arXiv:2210.13964v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13964">
<div class="article-summary-box-inner">
<span><p>Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects &amp; languages and a 77k multilingual pool of
distractor vocabulary for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Math Word Problem via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16257">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (PLMs) bring new opportunities to
challenge problems, especially those that need high-level intelligence, such as
the math word problem (MWPs). However, directly applying existing PLMs to MWPs
can fail as the generation process lacks sufficient supervision and thus lacks
fast adaptivity as humans. We notice that human reasoning has a dual reasoning
framework that consists of an immediate reaction system (system 1) and a
delicate reasoning system (system 2), where the entire reasoning is determined
by their interaction. This inspires us to develop a cooperative
reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),
resulting in a human-like reasoning architecture with system 1 as the generator
and system 2 as the verifier. In our approach, the generator is responsible for
generating reasoning paths, and the verifiers are used to supervise the
evaluation in order to obtain reliable feedback for the generator. We evaluate
our CoRe framework on several mathematical reasoning datasets and achieve
decent improvement over state-of-the-art methods, up to 9.8% increase over best
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Multiple Replies for Natural Language Generation Chatbots. (arXiv:2210.17209v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17209">
<div class="article-summary-box-inner">
<span><p>In this research, by responding to users' utterances with multiple replies to
create a group chat atmosphere, we alleviate the problem that Natural Language
Generation chatbots might reply with inappropriate content, thus causing a bad
user experience. Because according to our findings, users tend to pay attention
to appropriate replies and ignore inappropriate replies. We conducted a 2
(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)
repeated measures experiment to compare the chatting experience in different
conditions. The result shows that users will have a better chatting experience
when receiving multiple replies at once from the NLG model compared to the
single reply. Furthermore, according to the effect size of our result, to
improve the chatting experience for NLG chatbots which is single reply and
anonymous avatar, providing five replies will have more benefits than setting
an anime avatar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Building Text-To-Speech Systems for the Next Billion Users. (arXiv:2211.09536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09536">
<div class="article-summary-box-inner">
<span><p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09783">
<div class="article-summary-box-inner">
<span><p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection. (arXiv:2211.14477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14477">
<div class="article-summary-box-inner">
<span><p>Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts under the zero-shot setting, where the
relation sets at the training and testing stages are disjoint. Previous
state-of-the-art method handles this challenging task by leveraging pretrained
language models to generate data as additional training samples, which
increases the training cost and severely constrains the model performance. To
address the above issues, we propose a novel method named PCRED for ZeroRTE
with Potential Candidate Relation Selection and Entity Boundary Detection. The
remarkable characteristic of PCRED is that it does not rely on additional data
and still achieves promising performance. The model adopts a relation-first
paradigm, recognizing unseen relations through candidate relation selection.
With this approach, the semantics of relations are naturally infused in the
context. Entities are extracted based on the context and the semantics of
relations subsequently. We evaluate our model on two ZeroRTE datasets. The
experiment results show that our method consistently outperforms previous
works. Our code will be available at https://anonymous.4open.science/r/PCRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keywords Reinforcement LM: Improving End-to-End Response Generation in Task Oriented Dialog. (arXiv:2211.16773v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16773">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogs such as MultiWoZ (Budzianowski et al., 2018), an
informative and successful system response needs to include key information
such as the phone number of a hotel. Therefore, we hypothesize that by asking
the model to focus on generating more key quantities correctly, it can achieve
better overall performance. In this paper, we propose a new training algorithm,
Keywords Reinforcement Language Modeling (KRLM), that aims to use a
fine-grained reward function for each token and a new per-token Reinforcement
Learning procedure to help the model learn keywords generation more robustly
during inference. Empirical results show that our proposed KRLM training
algorithm can achieve state-of-the-art performance on the inform rate, success
rate, and combined score in the MultiWoZ benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for On-Device Speech Recognition using Disentangled Conformers. (arXiv:2212.01393v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01393">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition research focuses on training and evaluating on
static datasets. Yet, as speech models are increasingly deployed on personal
devices, such models encounter user-specific distributional shifts. To simulate
this real-world scenario, we introduce LibriContinual, a continual learning
benchmark for speaker-specific domain adaptation derived from LibriVox
audiobooks, with data corresponding to 118 individual speakers and 6 train
splits per speaker of different sizes. Additionally, current speech recognition
models and continual learning algorithms are not optimized to be
compute-efficient. We adapt a general-purpose training algorithm NetAug for ASR
and create a novel Conformer variant called the DisConformer (Disentangled
Conformer). This algorithm produces ASR models consisting of a frozen 'core'
network for general-purpose use and several tunable 'augment' networks for
speaker-specific tuning. Using such models, we propose a novel
compute-efficient continual learning algorithm called DisentangledCL. Our
experiments show that the DisConformer models significantly outperform
baselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On
speaker-specific LibriContinual they significantly outperform
trainable-parameter-matched baselines (by 20.65% rel. WER on test) and even
match fully finetuned baselines in some settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03760">
<div class="article-summary-box-inner">
<span><p>Recent studies have proposed unified user modeling frameworks that leverage
user behavior data from various applications. Many of them benefit from
utilizing users' behavior sequences as plain texts, representing rich
information in any domain or system without losing generality. Hence, a
question arises: Can language modeling for user history corpus help improve
recommender systems? While its versatile usability has been widely investigated
in many domains, its applications to recommender systems still remain
underexplored. We show that language modeling applied directly to task-specific
user histories achieves excellent results on diverse recommendation tasks.
Also, leveraging additional task-agnostic user histories delivers significant
performance benefits. We further demonstrate that our approach can provide
promising transfer learning capabilities for a broad spectrum of real-world
recommender systems, even on unseen domains and services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages. (arXiv:2212.05409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05409">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce IndicXTREME, a benchmark consisting of nine
diverse tasks covering 18 languages from the Indic sub-continent belonging to
four different families. Across languages and tasks, IndicXTREME contains a
total of 103 evaluation sets, of which 51 are new contributions to the
literature. To maintain high quality, we only use human annotators to curate or
translate our datasets. To the best of our knowledge, this is the first effort
toward creating a standard benchmark for Indic languages that aims to test the
zero-shot capabilities of pretrained language models. We also release IndicCorp
v2, an updated and much larger version of IndicCorp that contains 20.9 billion
tokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate
it on IndicXTREME to show that it outperforms existing multilingual language
models such as XLM-R and MuRIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images. (arXiv:2212.05525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05525">
<div class="article-summary-box-inner">
<span><p>Digitization of scanned receipts aims to extract text from receipt images and
save it into structured documents. This is usually split into two sub-tasks:
text localization and optical character recognition (OCR). Most existing OCR
models only focus on the cropped text instance images, which require the
bounding box information provided by a text region detection model. Introducing
an additional detector to identify the text instance images in advance is
inefficient, however instance-level OCR models have very low accuracy when
processing the whole image for the document-level OCR, such as receipt images
containing multiple text lines arranged in various layouts. To this end, we
propose a localization-free document-level OCR model for transcribing all the
characters in a receipt image into an ordered sequence end-to-end.
Specifically, we finetune the pretrained Transformer-based instance-level model
TrOCR with randomly cropped image chunks, and gradually increase the image
chunk size to generalize the recognition ability from instance images to
full-page images. In our experiments on the SROIE receipt OCR dataset, the
model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character
error rates (CER) on the word-level and character-level metrics, respectively,
which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The
best model, which splits the full image into 15 equally sized chunks, gives
87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of
the output. Moreover, the characters in the generated document-level sequences
are arranged in the reading order, which is practical for real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05767">
<div class="article-summary-box-inner">
<span><p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models. (arXiv:2212.05857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05857">
<div class="article-summary-box-inner">
<span><p>Background: Encouraged by the success of pretrained Transformer models in
many natural language processing tasks, their use for International
Classification of Diseases (ICD) coding tasks is now actively being explored.
In this study, we investigate three types of Transformer-based models, aiming
to address the extreme label set and long text classification challenges that
are posed by automated ICD coding tasks. Methods: The Transformer-based model
PLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD
coding benchmark dataset MIMIC-III. It was chosen as our baseline model to be
further optimised. XR-Transformer, the new SOTA model in the general extreme
multi-label text classification domain, and XR-LAT, a novel adaptation of the
XR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a
recursively trained model chain on a predefined hierarchical code tree with
label-wise attention, knowledge transferring and dynamic negative sampling
mechanisms. Results: Our optimised PLM-ICD model, which was trained with longer
total and chunk sequence lengths, significantly outperformed the current SOTA
PLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The
XR-Transformer model, although SOTA in the general domain, did not perform well
across all metrics. The best XR-LAT based model obtained results that were
competitive with the current SOTA PLM-ICD model, including improving the
macro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA
model for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT
model performs competitively with the previous SOTA PLM-ICD model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technological taxonomies for hypernym and hyponym retrieval in patent texts. (arXiv:2212.06039v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06039">
<div class="article-summary-box-inner">
<span><p>This paper presents an automatic approach to creating taxonomies of technical
terms based on the Cooperative Patent Classification (CPC). The resulting
taxonomy contains about 170k nodes in 9 separate technological branches and is
freely available. We also show that a Text-to-Text Transfer Transformer (T5)
model can be fine-tuned to generate hypernyms and hyponyms with relatively high
precision, confirming the manually assessed quality of the resource. The T5
model opens the taxonomy to any new technological terms for which a hypernym
can be generated, thus making the resource updateable with new terms, an
essential feature for the constantly evolving field of technological
terminology.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-14 23:13:05.484803640 UTC">2022-12-14 23:13:05 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>