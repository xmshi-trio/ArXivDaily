<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-17T01:30:00Z">11-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08447">
<div class="article-summary-box-inner">
<span><p>The goal of sexism detection is to mitigate negative online content targeting
certain gender groups of people. However, the limited availability of labeled
sexism-related datasets makes it problematic to identify online sexism for
low-resource languages. In this paper, we address the task of automatic sexism
detection in social media for one low-resource language -- Chinese. Rather than
collecting new sexism data or building cross-lingual transfer learning models,
we develop a cross-lingual domain-aware semantic specialisation system in order
to make the most of existing data. Semantic specialisation is a technique for
retrofitting pre-trained distributional word vectors by integrating external
linguistic knowledge (such as lexico-semantic relations) into the specialised
feature space. To do this, we leverage semantic resources for sexism from a
high-resource language (English) to specialise pre-trained word vectors in the
target language (Chinese) to inject domain knowledge. We demonstrate the
benefit of our sexist word embeddings (SexWEs) specialised by our framework via
intrinsic evaluation of word similarity and extrinsic evaluation of sexism
detection. Compared with other specialisation approaches and Chinese baseline
word vectors, our SexWEs shows an average score improvement of 0.033 and 0.064
in both intrinsic and extrinsic evaluations, respectively. The ablative results
and visualisation of SexWEs also prove the effectiveness of our framework on
retrofitting word vectors in low-resource languages. Our code and
sexism-related word vectors will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">kogito: A Commonsense Knowledge Inference Toolkit. (arXiv:2211.08451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08451">
<div class="article-summary-box-inner">
<span><p>In this paper, we present kogito, an open-source tool for generating
commonsense inferences about situations described in text. kogito provides an
intuitive and extensible interface to interact with natural language generation
models that can be used for hypothesizing commonsense knowledge inference from
a textual input. In particular, kogito offers several features for targeted,
multi-granularity knowledge generation. These include a standardized API for
training and evaluating knowledge models, and generating and filtering
inferences from them. We also include helper functions for converting natural
language texts into a format ingestible by knowledge models - intermediate
pipeline stages such as knowledge head extraction from text, heuristic and
model-based knowledge head-relation matching, and an ability to define and use
custom knowledge relations. We make the code for kogito available at
https://github.com/epfl-nlp/kogito along with thorough documentation at
https://kogito.readthedocs.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models. (arXiv:2211.08461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08461">
<div class="article-summary-box-inner">
<span><p>The awareness and mitigation of biases are of fundamental importance for the
fair and transparent use of contextual language models, yet they crucially
depend on the accurate detection of biases as a precursor. Consequently,
numerous bias detection methods have been proposed, which vary in their
approach, the considered type of bias, and the data used for evaluation.
However, while most detection methods are derived from the word embedding
association test for static word embeddings, the reported results are
heterogeneous, inconsistent, and ultimately inconclusive. To address this
issue, we conduct a rigorous analysis and comparison of bias detection methods
for contextual language models. Our results show that minor design and
implementation decisions (or errors) have a substantial and often significant
impact on the derived bias scores. Overall, we find the state of the field to
be both worse than previously acknowledged due to systematic and propagated
errors in implementations, yet better than anticipated since divergent results
in the literature homogenize after accounting for implementation errors. Based
on our findings, we conclude with a discussion of paths towards more robust and
consistent bias detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigating Connected Memories with a Task-oriented Dialog System. (arXiv:2211.08462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08462">
<div class="article-summary-box-inner">
<span><p>Recent years have seen an increasing trend in the volume of personal media
captured by users, thanks to the advent of smartphones and smart glasses,
resulting in large media collections. Despite conversation being an intuitive
human-computer interface, current efforts focus mostly on single-shot natural
language based media retrieval to aid users query their media and re-live their
memories. This severely limits the search functionality as users can neither
ask follow-up queries nor obtain information without first formulating a
single-turn query.
</p>
<p>In this work, we propose dialogs for connected memories as a powerful tool to
empower users to search their media collection through a multi-turn,
interactive conversation. Towards this, we collect a new task-oriented dialog
dataset COMET, which contains $11.5k$ user&lt;-&gt;assistant dialogs (totaling $103k$
utterances), grounded in simulated personal memory graphs. We employ a
resource-efficient, two-phase data collection pipeline that uses: (1) a novel
multimodal dialog simulator that generates synthetic dialog flows grounded in
memory graphs, and, (2) manual paraphrasing to obtain natural language
utterances. We analyze COMET, formulate four main tasks to benchmark meaningful
progress, and adopt state-of-the-art language models as strong baselines, in
order to highlight the multimodal challenges captured by our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ED-FAITH: Evaluating Dialogue Summarization on Faithfulness. (arXiv:2211.08464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08464">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization models typically generate content unfaithful to the
input, thus highlighting the significance of evaluating the faithfulness of
generated summaries. Most faithfulness metrics are only evaluated on news
domain, can they be transferred to other summarization tasks? In this work, we
first present a systematic study of faithfulness metrics for dialogue
summarization. We evaluate common faithfulness metrics on dialogue datasets and
observe that most metrics correlate poorly with human judgements despite
performing well on news datasets. Given these findings, to improve existing
metrics' performance on dialogue summarization, we first finetune on in-domain
dataset, then apply unlikelihood training on negative samples, and show that
they can successfully improve metric performance on dialogue data. Inspired by
the strong zero-shot performance of the T0 language model, we further propose
T0-Score -- a new metric for faithfulness evaluation, which shows consistent
improvement against baseline metrics across multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales. (arXiv:2211.08466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08466">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Generation is the task of generating questions which
require the reader to reason over and combine information spread across
multiple passages using several reasoning steps. Chain-of-thought rationale
generation has been shown to improve performance on multi-step reasoning tasks
and make model predictions more interpretable. However, few-shot performance
gains from including rationales have been largely observed only in +100B
language models, and otherwise require large scale manual rationale annotation.
In this work, we introduce a new framework for applying chain-of-thought
inspired structured rationale generation to multi-hop question generation under
a very low supervision regime (8- to 128-shot). We propose to annotate a small
number of examples following our proposed multi-step rationale schema, treating
each reasoning step as a separate task to be performed by a generative language
model. We show that our framework leads to improved control over the difficulty
of the generated questions and better performance compared to baselines trained
without rationales, both on automatic evaluation metrics and in human
evaluation. Importantly, we show that this is achievable with a modest model
size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Compositional Generalization Gap of In-Context Learning. (arXiv:2211.08473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08473">
<div class="article-summary-box-inner">
<span><p>Pretrained large generative language models have shown great performance on
many tasks, but exhibit low compositional generalization abilities. Scaling
such models has been shown to improve their performance on various NLP tasks
even just by conditioning them on a few examples to solve the task without any
fine-tuning (also known as in-context learning). In this work, we look at the
gap between the in-distribution (ID) and out-of-distribution (OOD) performance
of such models in semantic parsing tasks with in-context learning. In the ID
settings, the demonstrations are from the same split (test or train) that the
model is being evaluated on, and in the OOD settings, they are from the other
split. We look at how the relative generalization gap of in-context learning
evolves as models are scaled up. We evaluate four model families, OPT, BLOOM,
CodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery
with different number of exemplars, and observe a trend of decreasing relative
generalization gap as models are scaled up.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Carriers of the Diffuse Interstellar Bands Across Disciplines, using Natural Language Processing. (arXiv:2211.08513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08513">
<div class="article-summary-box-inner">
<span><p>The explosion of scientific publications overloads researchers with
information. This is even more dramatic for interdisciplinary studies, where
several fields need to be explored. A tool to help researchers overcome this is
Natural Language Processing (NLP): a machine-learning (ML) technique that
allows scientists to automatically synthesize information from many articles.
As a practical example, we have used NLP to conduct an interdisciplinary search
for compounds that could be carriers for Diffuse Interstellar Bands (DIBs), a
long-standing open question in astrophysics. We have trained a NLP model on a
corpus of 1.5 million cross-domain articles in open access, and fine-tuned this
model with a corpus of astrophysical publications about DIBs. Our analysis
points us toward several molecules, studied primarily in biology, having
transitions at the wavelengths of several DIBs and composed of abundant
interstellar atoms. Several of these molecules contain chromophores, small
molecular groups responsible for the molecule's colour, that could be promising
candidate carriers. Identifying viable carriers demonstrates the value of using
NLP to tackle open scientific questions, in an interdisciplinary manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MapQA: A Dataset for Question Answering on Choropleth Maps. (arXiv:2211.08545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08545">
<div class="article-summary-box-inner">
<span><p>Choropleth maps are a common visual representation for region-specific
tabular data and are used in a number of different venues (newspapers,
articles, etc). These maps are human-readable but are often challenging to deal
with when trying to extract data for screen readers, analyses, or other related
tasks. Recent research into Visual-Question Answering (VQA) has studied
question answering on human-generated charts (ChartQA), such as bar, line, and
pie charts. However, little work has paid attention to understanding maps;
general VQA models, and ChartQA models, suffer when asked to perform this task.
To facilitate and encourage research in this area, we present MapQA, a
large-scale dataset of ~800K question-answer pairs over ~60K map images. Our
task tests various levels of map understanding, from surface questions about
map styles to complex questions that require reasoning on the underlying data.
We present the unique challenges of MapQA that frustrate most strong baseline
algorithms designed for ChartQA and general VQA tasks. We also present a novel
algorithm, Visual Multi-Output Data Extraction based QA (V-MODEQA) for MapQA.
V-MODEQA extracts the underlying structured data from a map image with a
multi-output model and then performs reasoning on the extracted data. Our
experimental results show that V-MODEQA has better overall performance and
robustness on MapQA than the state-of-the-art ChartQA and VQA algorithms by
capturing the unique properties in map question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALIGN-MLM: Word Embedding Alignment is Crucial for Multilingual Pre-training. (arXiv:2211.08547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08547">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models exhibit zero-shot cross-lingual transfer,
where a model fine-tuned on a source language achieves surprisingly good
performance on a target language. While studies have attempted to understand
transfer, they focus only on MLM, and the large number of differences between
natural languages makes it hard to disentangle the importance of different
properties. In this work, we specifically highlight the importance of word
embedding alignment by proposing a pre-training objective (ALIGN-MLM) whose
auxiliary loss guides similar words in different languages to have similar word
embeddings. ALIGN-MLM either outperforms or matches three widely adopted
objectives (MLM, XLM, DICT-MLM) when we evaluate transfer between pairs of
natural languages and their counterparts created by systematically modifying
specific properties like the script. In particular, ALIGN-MLM outperforms XLM
and MLM by 35 and 30 F1 points on POS-tagging for transfer between languages
that differ both in their script and word order (left-to-right v.s.
right-to-left). We also show a strong correlation between alignment and
transfer for all objectives (e.g., rho=0.727 for XNLI), which together with
ALIGN-MLM's strong performance calls for explicitly aligning word embeddings
for multilingual models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08584">
<div class="article-summary-box-inner">
<span><p>Radiology report summarization is a growing area of research. Given the
Findings and/or Background sections of a radiology report, the goal is to
generate a summary (called an Impression section) that highlights the key
observations and conclusions of the radiology study. Recent efforts have
released systems that achieve promising performance as measured by widely used
summarization metrics such as BLEU and ROUGE. However, the research area of
radiology report summarization currently faces important limitations. First,
most of the results are reported on private datasets. This limitation prevents
the ability to reproduce results and fairly compare different systems and
solutions. Secondly, to the best of our knowledge, most research is carried out
on chest X-rays. Sometimes, studies even omit to mention the concerned modality
and anatomy in the radiology reports used for their experiments. To palliate
these limitations, we propose a new dataset of six different modalities and
anatomies based on the MIMIC-III database. We further release our results and
the data splits used to carry out our experiments. Finally, we propose a simple
report summarization system that outperforms the previous replicable research
on the existing dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering. (arXiv:2211.08588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08588">
<div class="article-summary-box-inner">
<span><p>Few-Shot Text Classification (FSTC) imitates humans to learn a new text
classifier efficiently with only few examples, by leveraging prior knowledge
from historical tasks. However, most prior works assume that all the tasks are
sampled from a single data source, which cannot adapt to real-world scenarios
where tasks are heterogeneous and lie in different distributions. As such,
existing methods may suffer from their globally knowledge-shared mechanisms to
handle the task heterogeneity. On the other hand, inherent task relation are
not explicitly captured, making task knowledge unorganized and hard to transfer
to new tasks. Thus, we explore a new FSTC setting where tasks can come from a
diverse range of data sources. To address the task heterogeneity, we propose a
self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only
customizes cluster-specific knowledge by dynamically organizing heterogeneous
tasks into different clusters in hierarchical levels but also disentangles
underlying relations between tasks to improve the interpretability. Extensive
experiments on five public FSTC benchmark datasets demonstrate the
effectiveness of SS-HTC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08633">
<div class="article-summary-box-inner">
<span><p>There have been several studies on the correlation between human ratings and
metrics such as BLEU, chrF2 and COMET in machine translation. Most, if not all
consider full-sentence translation. It is unclear whether human ratings of
simultaneous speech translation Continuous Rating (CR) correlate with these
metrics or not. Therefore, we conduct an extensive correlation analysis of CR
and the aforementioned automatic metrics on evaluations of candidate systems at
English-German simultaneous speech translation task at IWSLT 2022. Our studies
reveal that the offline MT metrics correlate with CR and can be reliably used
for evaluating machine translation in the simultaneous mode, with some
limitations on the test set size. This implies that automatic metrics can be
used as proxies for CR, thereby alleviating the need for human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#maskUp: Selective Attribute Encryption for Sensitive Vocalization for English language on Social Media Platforms. (arXiv:2211.08653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08653">
<div class="article-summary-box-inner">
<span><p>Social media has become a platform for people to stand up and raise their
voices against social and criminal acts. Vocalization of such information has
allowed the investigation and identification of criminals. However, revealing
such sensitive information may jeopardize the victim's safety. We propose
#maskUp, a safe method for information communication in a secure fashion to the
relevant authorities, discouraging potential bullying of the victim. This would
ensure security by conserving their privacy through natural language processing
supplemented with selective encryption for sensitive attribute masking. To our
knowledge, this is the first work that aims to protect the privacy of the
victims by masking their private details as well as emboldening them to come
forward to report crimes. The use of masking technology allows only binding
authorities to view/un-mask this data. We construct and evaluate the proposed
methodology on continual learning tasks, allowing practical implementation of
the same in a real-world scenario. #maskUp successfully demonstrates this
integration on sample datasets validating the presented objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Tuning on Layer Normalization for Pre-trained Language Models. (arXiv:2211.08682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08682">
<div class="article-summary-box-inner">
<span><p>Conventional fine-tuning encounters increasing difficulties given the size of
current Pre-trained Language Models, which makes parameter-efficient tuning
become the focal point of frontier research. Previous methods in this field add
tunable adapters into MHA or/and FFN of Transformer blocks to enable PLMs
achieve transferability. However, as an important part of Transformer
architecture, the power of layer normalization for parameter-efficent tuning is
ignored. In this paper, we first propose LN-tuning, by tuning the gain and bias
term of Layer Normalization module with only 0.03\% parameters, which is of
high time-efficency and significantly superior to baselines which are less than
0.1\% tunable parameters. Further, we study the unified framework of combining
LN-tuning with previous ones and we find that: (1) the unified framework of
combining prefix-tuning, the adapter-based method working on MHA, and LN-tuning
achieves SOTA performance. (2) unified framework which tunes MHA and LayerNorm
simultaneously can get performance improvement but those which tune FFN and
LayerNorm simultaneous will cause performance decrease. Ablation study
validates LN-tuning is of no abundant parameters and gives a further
understanding of it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Unsupervised Reconstruction of Protolanguage Word Forms. (arXiv:2211.08684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08684">
<div class="article-summary-box-inner">
<span><p>We present a state-of-the-art neural approach to the unsupervised
reconstruction of ancient word forms. Previous work in this domain used
expectation-maximization to predict simple phonological changes between ancient
word forms and their cognates in modern languages. We extend this work with
neural models that can capture more complicated phonological and morphological
changes. At the same time, we preserve the inductive biases from classical
methods by building monotonic alignment constraints into the model and
deliberately underfitting during the maximization step. We evaluate our
performance on the task of reconstructing Latin from a dataset of cognates
across five Romance languages, achieving a notable reduction in edit distance
from the target word forms compared to previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08714">
<div class="article-summary-box-inner">
<span><p>To align conditional text generation model outputs with desired behaviors,
there has been an increasing focus on training the model using reinforcement
learning (RL) with reward functions learned from human annotations. Under this
framework, we identify three common cases where high rewards are incorrectly
assigned to undesirable patterns: noise-induced spurious correlation, naturally
occurring spurious correlation, and covariate shift. We show that even though
learned metrics achieve high performance on the distribution of the data used
to train the reward function, the undesirable patterns may be amplified during
RL training of the text generation model. While there has been discussion about
reward gaming in the RL or safety community, in this short discussion piece, we
would like to highlight reward gaming in the NLG community using concrete
conditional text generation examples and discuss potential fixes and areas for
future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Pairing and Partial Supervision for Opinion Summarization. (arXiv:2211.08723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08723">
<div class="article-summary-box-inner">
<span><p>Current opinion summarization systems simply generate summaries reflecting
important opinions from customer reviews, but the generated summaries may not
attract the reader's attention. Although it is helpful to automatically
generate professional reviewer-like summaries from customer reviews, collecting
many training pairs of customer and professional reviews is generally tricky.
We propose a weakly supervised opinion summarization framework, Noisy Pairing
and Partial Supervision (NAPA) that can build a stylized opinion summarization
system with no customer-professional review pairs. Experimental results show
consistent improvements in automatic evaluation metrics, and qualitative
analysis shows that our weakly supervised opinion summarization system can
generate summaries that look more like those written by professional reviewers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08726">
<div class="article-summary-box-inner">
<span><p>Disfluency detection has mainly been solved in a pipeline approach, as
post-processing of speech recognition. In this study, we propose
Transformer-based encoder-decoder models that jointly solve speech recognition
and disfluency detection, which work in a streaming manner. Compared to
pipeline approaches, the joint models can leverage acoustic information that
makes disfluency detection robust to recognition errors and provide non-verbal
clues. Moreover, joint modeling results in low-latency and lightweight
inference. We investigate two joint model variants for streaming disfluency
detection: a transcript-enriched model and a multi-task model. The
transcript-enriched model is trained on text with special tags indicating the
starting and ending points of the disfluent part. However, it has problems with
latency and standard language model adaptation, which arise from the additional
disfluency tags. We propose a multi-task model to solve such problems, which
has two output layers at the Transformer decoder; one for speech recognition
and the other for disfluency detection. It is modeled to be conditioned on the
currently recognized token with an additional token-dependency mechanism. We
show that the proposed joint models outperformed a BERT-based pipeline approach
in both accuracy and latency, on both the Switchboard and the corpus of
spontaneous Japanese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lesion Guided Explainable Few Weak-shot Medical Report Generation. (arXiv:2211.08732v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08732">
<div class="article-summary-box-inner">
<span><p>Medical images are widely used in clinical practice for diagnosis.
Automatically generating interpretable medical reports can reduce radiologists'
burden and facilitate timely care. However, most existing approaches to
automatic report generation require sufficient labeled data for training. In
addition, the learned model can only generate reports for the training classes,
lacking the ability to adapt to previously unseen novel diseases. To this end,
we propose a lesion guided explainable few weak-shot medical report generation
framework that learns correlation between seen and novel classes through visual
and semantic feature alignment, aiming to generate medical reports for diseases
not observed in training. It integrates a lesion-centric feature extractor and
a Transformer-based report generation module. Concretely, the lesion-centric
feature extractor detects the abnormal regions and learns correlations between
seen and novel classes with multi-view (visual and lexical) embeddings. Then,
features of the detected regions and corresponding embeddings are concatenated
as multi-view input to the report generation module for explainable report
generation, including text descriptions and corresponding abnormal regions
detected in the images. We conduct experiments on FFA-IR, a dataset providing
explainable annotations, showing that our framework outperforms others on
report generation for novel diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN. (arXiv:2211.08742v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08742">
<div class="article-summary-box-inner">
<span><p>Auditing machine learning-based (ML) healthcare tools for bias is critical to
preventing patient harm, especially in communities that disproportionately face
health inequities. General frameworks are becoming increasingly available to
measure ML fairness gaps between groups. However, ML for health (ML4H) auditing
principles call for a contextual, patient-centered approach to model
assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H
auditing principles and (2) able to illuminate and characterize communities
vulnerable to the most harm. To address this gap, we propose supplementing ML4H
auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs
detectioN), an automatic tool for capturing local biases in a clinical
prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs
detectioN), by contextualizing group bias detection in patient illness severity
and past medical history. We investigate and compare SLOGAN's bias detection
capabilities to LOGAN and other clustering techniques across patient subgroups
in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness
disparities in over 75% of patient groups than LOGAN while maintaining
clustering quality. Furthermore, in a diabetes case study, health disparity
literature corroborates the characterizations of the most biased clusters
identified by SLOGAN. Our results contribute to the broader discussion of how
machine learning biases may perpetuate existing healthcare disparities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2211.08769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08769">
<div class="article-summary-box-inner">
<span><p>To better support retrieval applications such as web search and question
answering, growing effort is made to develop retrieval-oriented language
models. Most of the existing works focus on improving the semantic
representation capability for the contextualized embedding of [CLS] token.
However, recent study shows that the ordinary tokens besides [CLS] may provide
extra information, which helps to produce a better representation effect. As
such, it's necessary to extend the current methods where all contextualized
embeddings can be jointly pre-trained for the retrieval tasks.
</p>
<p>With this motivation, we propose a new pre-training method: duplex masked
auto-encoder, a.k.a. DupMAE, which targets on improving the semantic
representation capacity for the contextualized embeddings of both [CLS] and
ordinary tokens. It introduces two decoding tasks: one is to reconstruct the
original input sentence based on the [CLS] embedding, the other one is to
minimize the bag-of-words loss (BoW) about the input sentence based on the
entire ordinary tokens' embeddings. The two decoding losses are added up to
train a unified encoding model. The embeddings from [CLS] and ordinary tokens,
after dimension reduction and aggregation, are concatenated as one unified
semantic representation for the input. DupMAE is simple but empirically
competitive: with a small decoding cost, it substantially contributes to the
model's representation capability and transferability, where remarkable
improvements are achieved on MS MARCO and BEIR benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSCD-IME: Correcting Spelling Errors Generated by Pinyin IME. (arXiv:2211.08788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08788">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Correction (CSC) is a task to detect and correct spelling
mistakes in texts. In fact, most of Chinese input is based on pinyin input
method, so the study of spelling errors in this process is more practical and
valuable. However, there is still no research dedicated to this essential
scenario. In this paper, we first present a Chinese Spelling Correction Dataset
for errors generated by pinyin IME (CSCD-IME), including 40,000 annotated
sentences from real posts of official media on Sina Weibo. Furthermore, we
propose a novel method to automatically construct large-scale and high-quality
pseudo data by simulating the input through pinyin IME. A series of analyses
and experiments on CSCD-IME show that spelling errors produced by pinyin IME
hold a particular distribution at pinyin level and semantic level and are
challenging enough. Meanwhile, our proposed pseudo-data construction method can
better fit this error distribution and improve the performance of CSC systems.
Finally, we also provide a useful guide to using pseudo data, including the
data scale, the data source, and the training strategy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08794">
<div class="article-summary-box-inner">
<span><p>Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into a multi-view compressed representation before
feeding it into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognitive Simplification Operations Improve Text Simplification. (arXiv:2211.08825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08825">
<div class="article-summary-box-inner">
<span><p>Text Simplification (TS) is the task of converting a text into a form that is
easier to read while maintaining the meaning of the original text. A sub-task
of TS is Cognitive Simplification (CS), converting text to a form that is
readily understood by people with cognitive disabilities without rendering it
childish or simplistic. This sub-task has yet to be explored with neural
methods in NLP, and resources for it are scarcely available. In this paper, we
present a method for incorporating knowledge from the cognitive accessibility
domain into a TS model, by introducing an inductive bias regarding what
simplification operations to use. We show that by adding this inductive bias to
a TS-trained model, it is able to adapt better to CS without ever seeing CS
data, and outperform a baseline model on a traditional TS benchmark. In
addition, we provide a novel test dataset for CS, and analyze the differences
between CS corpora and existing TS corpora, in terms of how simplification
operations are applied.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT. (arXiv:2211.08842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08842">
<div class="article-summary-box-inner">
<span><p>As an application of Natural Language Processing (NLP) techniques, financial
sentiment analysis (FSA) has become an invaluable tool for investors. Its speed
and accuracy can significantly impact the returns of trading strategies.With
the development of deep learning and Transformer-based pre-trained models like
BERT, the accuracy of FSA has been much improved, but these time-consuming big
models will also slow down the computation. To boost the processing speed of
the FSA system and ensure high precision, we first propose an efficient and
lightweight BERT (ELBERT) along with a novel confidence-window-based (CWB)
early exit mechanism. Based on ELBERT, an innovative method to accelerate text
processing on the GPU platform is developed, solving the difficult problem of
making the early exit mechanism work more effectively with a large input batch
size. Afterward, a fast and high-accuracy FSA system is built. Experimental
results show that the proposed CWB early exit mechanism achieves significantly
higher accuracy than existing early exit methods on BERT under the same
computation cost. Besides, our FSA system can boost the processing speed to
over 1000 texts per second with sufficient accuracy by using this acceleration
method, which is nearly twice as fast as the FastBERT. Hence, this system can
enable modern trading systems to quickly and accurately process financial text
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L2 proficiency assessment using self-supervised speech representations. (arXiv:2211.08849v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08849">
<div class="article-summary-box-inner">
<span><p>There has been a growing demand for automated spoken language assessment
systems in recent years. A standard pipeline for this process is to start with
a speech recognition system and derive features, either hand-crafted or based
on deep-learning, that exploit the transcription and audio. Though these
approaches can yield high performance systems, they require speech recognition
systems that can be used for L2 speakers, and preferably tuned to the specific
form of test being deployed. Recently a self-supervised speech representation
based scheme, requiring no speech recognition, was proposed. This work extends
the initial analysis conducted on this approach to a large scale proficiency
test, Linguaskill, that comprises multiple parts, each designed to assess
different attributes of a candidate's speaking proficiency. The performance of
the self-supervised, wav2vec 2.0, system is compared to a high performance
hand-crafted assessment system and a BERT-based text system both of which use
speech transcriptions. Though the wav2vec 2.0 based system is found to be
sensitive to the nature of the response, it can be configured to yield
comparable performance to systems requiring a speech transcription, and yields
gains when appropriately combined with standard approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consecutive Question Generation via Dynamic Multitask Learning. (arXiv:2211.08850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08850">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the task of consecutive question generation (CQG),
which generates a set of logically related question-answer pairs to understand
a whole passage, with a comprehensive consideration of the aspects including
accuracy, coverage, and informativeness. To achieve this, we first examine the
four key elements of CQG, i.e., question, answer, rationale, and context
history, and propose a novel dynamic multitask framework with one main task
generating a question-answer pair, and four auxiliary tasks generating other
elements. It directly helps the model generate good questions through both
joint training and self-reranking. At the same time, to fully explore the
worth-asking information in a given passage, we make use of the reranking
losses to sample the rationales and search for the best question series
globally. Finally, we measure our strategy by QA data augmentation and manual
evaluation, as well as a novel application of generated question-answer pairs
on DocNLI. We prove that our strategy can improve question generation
significantly and benefit multiple related NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSMind: Alibaba and Soochow University's Submission to the WMT22 Translation Suggestion Task. (arXiv:2211.08987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08987">
<div class="article-summary-box-inner">
<span><p>This paper describes the joint submission of Alibaba and Soochow University,
TSMind, to the WMT 2022 Shared Task on Translation Suggestion (TS). We
participate in the English-German and English-Chinese tasks. Basically, we
utilize the model paradigm fine-tuning on the downstream tasks based on
large-scale pre-trained models, which has recently achieved great success. We
choose FAIR's WMT19 English-German news translation system and MBART50 for
English-Chinese as our pre-trained models. Considering the task's condition of
limited use of training data, we follow the data augmentation strategies
proposed by WeTS to boost our TS model performance. The difference is that we
further involve the dual conditional cross-entropy model and GPT-2 language
model to filter augmented data. The leader board finally shows that our
submissions are ranked first in three of four language directions in the Naive
TS task of the WMT22 Translation Suggestion task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Avoid Overthinking in Self-Supervised Models for Speech Recognition. (arXiv:2211.08989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08989">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) models reshaped our approach to speech,
language and vision. However their huge size and the opaque relations between
their layers and tasks result in slow inference and network overthinking, where
predictions made from the last layer of large models is worse than those made
from intermediate layers. Early exit (EE) strategies can solve both issues by
dynamically reducing computations at inference time for certain samples.
Although popular for classification tasks in vision and language, EE has seen
less use for sequence-to-sequence speech recognition (ASR) tasks where outputs
from early layers are often degenerate. This challenge is further compounded
when speech SSL models are applied on out-of-distribution (OOD) data. This
paper first shows that SSL models do overthinking in ASR. We then motivate
further research in EE by computing an optimal bound for performance versus
speed trade-offs. To approach this bound we propose two new strategies for ASR:
(1) we adapt the recently proposed patience strategy to ASR; and (2) we design
a new EE strategy specific to ASR that performs better than all strategies
previously introduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (arXiv:2211.09039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09039">
<div class="article-summary-box-inner">
<span><p>Relational triple extraction is challenging for its difficulty in capturing
rich correlations between entities and relations. Existing works suffer from 1)
heterogeneous representations of entities and relations, and 2) heterogeneous
modeling of entity-entity interactions and entity-relation interactions.
Therefore, the rich correlations are not fully exploited by existing works. In
this paper, we propose UniRel to address these challenges. Specifically, we
unify the representations of entities and relations by jointly encoding them
within a concatenated natural language sequence, and unify the modeling of
interactions with a proposed Interaction Map, which is built upon the
off-the-shelf self-attention mechanism within any Transformer block. With
comprehensive experiments on two popular relational triple extraction datasets,
we demonstrate that UniRel is more effective and computationally efficient. The
source code is available at https://github.com/wtangdev/UniRel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A generative grammar of cooking. (arXiv:2211.09059v1 [physics.soc-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09059">
<div class="article-summary-box-inner">
<span><p>Cooking is a uniquely human endeavor for transforming raw ingredients into
delicious dishes. Over centuries, cultures worldwide have evolved diverse
cooking practices ingrained in their culinary traditions. Recipes, thus, are
cultural capsules that capture culinary knowledge in elaborate cooking
protocols. While simple quantitative models have probed the patterns in recipe
composition and the process of cuisine evolution, unlike other cultural quirks
such as language, the principles of cooking remain hitherto unexplored. The
fundamental rules that drive the act of cooking, shaping recipe composition and
cuisine architecture, are unclear. Here we present a generative grammar of
cooking that captures the underlying culinary logic. By studying an extensive
repository of structured recipes, we identify core concepts and rules that
together forge a combinatorial system for culinary synthesis. Building on the
body of work done in the context of language, the demonstration of a logically
consistent generative framework offers profound insights into the act of
cooking. Given the central role of food in nutrition and lifestyle disorders,
culinary grammar provides leverage to improve public health through dietary
interventions beyond applications for creative pursuits such as novel recipe
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Algorithmic Reasoning via In-context Learning. (arXiv:2211.09066v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09066">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown increasing in-context learning
capabilities through scaling up model and data size. Despite this progress,
LLMs are still unable to solve algorithmic reasoning problems. While providing
a rationale with the final answer has led to further improvements in multi-step
reasoning problems, Anil et al. 2022 showed that even simple algorithmic
reasoning tasks such as parity are far from solved. In this work, we identify
and study four key stages for successfully teaching algorithmic reasoning to
LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills
simultaneously (skill accumulation), (3) teaching how to combine skills (skill
composition) and (4) teaching how to use skills as tools. We show that it is
possible to teach algorithmic reasoning to LLMs via in-context learning, which
we refer to as algorithmic prompting. We evaluate our approach on a variety of
arithmetic and quantitative reasoning tasks, and demonstrate significant boosts
in performance over existing prompting techniques. In particular, for long
parity, addition, multiplication and subtraction, we achieve an error reduction
of approximately 10x, 9x, 5x and 2x respectively compared to the best available
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Computationally Verifiable Semantic Grounding for Language Models. (arXiv:2211.09070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09070">
<div class="article-summary-box-inner">
<span><p>The paper presents an approach to semantic grounding of language models (LMs)
that conceptualizes the LM as a conditional model generating text given a
desired semantic message formalized as a set of entity-relationship triples. It
embeds the LM in an auto-encoder by feeding its output to a semantic parser
whose output is in the same representation domain as the input message.
Compared to a baseline that generates text using greedy search, we demonstrate
two techniques that improve the fluency and semantic accuracy of the generated
text: The first technique samples multiple candidate text sequences from which
the semantic parser chooses. The second trains the language model while keeping
the semantic parser frozen to improve the semantic accuracy of the
auto-encoder. We carry out experiments on the English WebNLG 3.0 data set,
using BLEU to measure the fluency of generated text and standard parsing
metrics to measure semantic accuracy. We show that our proposed approaches
significantly improve on the greedy search baseline. Human evaluation
corroborates the results of the automatic evaluation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Galactica: A Large Language Model for Science. (arXiv:2211.09085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09085">
<div class="article-summary-box-inner">
<span><p>Information overload is a major obstacle to scientific progress. The
explosive growth in scientific literature and data has made it ever harder to
discover useful insights in a large mass of information. Today scientific
knowledge is accessed through search engines, but they are unable to organize
scientific knowledge alone. In this paper we introduce Galactica: a large
language model that can store, combine and reason about scientific knowledge.
We train on a large scientific corpus of papers, reference material, knowledge
bases and many other sources. We outperform existing models on a range of
scientific tasks. On technical knowledge probes such as LaTeX equations,
Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also
performs well on reasoning, outperforming Chinchilla on mathematical MMLU by
41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It
also sets a new state-of-the-art on downstream tasks such as PubMedQA and
MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general
corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these
results demonstrate the potential for language models as a new interface for
science. We open source the model for the benefit of the scientific community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09102">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) that have been trained on multilingual but not
parallel text exhibit a remarkable ability to translate between languages. We
probe this ability in an in-depth study of the pathways language model (PaLM),
which has demonstrated the strongest machine translation (MT) performance among
similarly-trained LLMs to date. We investigate various strategies for choosing
translation examples for few-shot prompting, concluding that example quality is
the most important factor. Using optimized prompts, we revisit previous
assessments of PaLM's MT capabilities with more recent test sets, modern MT
metrics, and human evaluation, and find that its performance, while impressive,
still lags that of state-of-the-art supervised systems. We conclude by
providing an analysis of PaLM's MT output which reveals some interesting
properties and prospects for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Evaluation of Language Models. (arXiv:2211.09110v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09110">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are becoming the foundation for almost all major
language technologies, but their capabilities, limitations, and risks are not
well understood. We present Holistic Evaluation of Language Models (HELM) to
improve the transparency of language models. First, we taxonomize the vast
space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)
that are of interest for LMs. Then we select a broad subset based on coverage
and feasibility, noting what's missing or underrepresented (e.g. question
answering for neglected English dialects, metrics for trustworthiness). Second,
we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,
robustness, fairness, bias, toxicity, and efficiency) for each of 16 core
scenarios when possible (87.5% of the time). This ensures metrics beyond
accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We
also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze
specific aspects (e.g. reasoning, disinformation). Third, we conduct a
large-scale evaluation of 30 prominent language models (spanning open,
limited-access, and closed models) on all 42 scenarios, 21 of which were not
previously used in mainstream LM evaluation. Prior to HELM, models on average
were evaluated on just 17.9% of the core HELM scenarios, with some prominent
models not sharing a single scenario in common. We improve this to 96.0%: now
all 30 models have been densely benchmarked on the same core scenarios and
metrics under standardized conditions. Our evaluation surfaces 25 top-level
findings. For full transparency, we release all raw model prompts and
completions publicly for further analysis, as well as a general modular
toolkit. We intend for HELM to be a living benchmark for the community,
continuously updated with new scenarios, metrics, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Measuring the Intrinsic Few-Shot Hardness of Datasets. (arXiv:2211.09113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09113">
<div class="article-summary-box-inner">
<span><p>While advances in pre-training have led to dramatic improvements in few-shot
learning of NLP tasks, there is limited understanding of what drives successful
few-shot adaptation in datasets. In particular, given a new dataset and a
pre-trained model, what properties of the dataset make it \emph{few-shot
learnable} and are these properties independent of the specific adaptation
techniques used? We consider an extensive set of recent few-shot learning
methods, and show that their performance across a large number of datasets is
highly correlated, showing that few-shot hardness may be intrinsic to datasets,
for a given pre-trained model. To estimate intrinsic few-shot hardness, we then
propose a simple and lightweight metric called "Spread" that captures the
intuition that few-shot learning is made possible by exploiting feature-space
invariances between training and test samples. Our metric better accounts for
few-shot hardness compared to existing notions of hardness, and is ~8-100x
faster to compute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back Translation Survey for Improving Text Augmentation. (arXiv:2102.09708v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09708">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) relies heavily on training data.
Transformers, as they have gotten bigger, have required massive amounts of
training data. To satisfy this requirement, text augmentation should be looked
at as a way to expand your current dataset and to generalize your models. One
text augmentation we will look at is translation augmentation. We take an
English sentence and translate it to another language before translating it
back to English. In this paper, we look at the effect of 108 different language
back translations on various metrics and text embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actionable Entities Recognition Benchmark for Interactive Fiction. (arXiv:2109.13855v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13855">
<div class="article-summary-box-inner">
<span><p>This paper presents a new natural language processing task - Actionable
Entities Recognition (AER) - recognition of entities that protagonists could
interact with for further plot development. Though similar to classical Named
Entity Recognition (NER), it has profound differences. In particular, it is
crucial for interactive fiction, where the agent needs to detect entities that
might be useful in the future. We also discuss if AER might be further helpful
for the systems dealing with narrative processing since actionable entities
profoundly impact the causal relationship in a story. We validate the proposed
task on two previously available datasets and present a new benchmark dataset
for the AER task that includes 5550 descriptions with one or more actionable
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Literature-Augmented Clinical Outcome Prediction. (arXiv:2111.08374v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08374">
<div class="article-summary-box-inner">
<span><p>We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach
for clinical outcome prediction that retrieves patient-specific medical
literature and incorporates it into predictive models. Based on each individual
patient's clinical notes, we train language models (LMs) to find relevant
papers and fuse them with information from notes to predict outcomes such as
in-hospital mortality. We develop methods to retrieve literature based on
noisy, information-dense patient notes, and to augment existing outcome
prediction models with retrieved papers in a manner that maximizes predictive
accuracy. Our approach boosts predictive performance on three important
clinical tasks in comparison to strong recent LM baselines, increasing F1 by up
to 5 points and precision@Top-K by a large margin of over 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernelized Concept Erasure. (arXiv:2201.12191v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12191">
<div class="article-summary-box-inner">
<span><p>The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific nonlinear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00964">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (LMs) do not capture factual knowledge very well.
This has led to the development of a number of knowledge integration (KI)
methods which aim to incorporate external knowledge into pretrained LMs. Even
though KI methods show some performance gains over vanilla LMs, the
inner-workings of these methods are not well-understood. For instance, it is
unclear how and what kind of knowledge is effectively integrated into these
models and if such integration may lead to catastrophic forgetting of already
learned knowledge. This paper revisits the KI process in these models with an
information-theoretic view and shows that KI can be interpreted using a graph
convolution operation. We propose a probe model called \textit{Graph
Convolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and
exposing what kind of knowledge is integrated into these models. We conduct
experiments to verify that our GCS can indeed be used to correctly interpret
the KI process, and we use it to analyze two well-known knowledge-enhanced LMs:
ERNIE and K-Adapter, and find that only a small amount of factual knowledge is
integrated in them. We stratify knowledge in terms of various relation types
and find that ERNIE and K-Adapter integrate different kinds of knowledge to
different extent. Our analysis also shows that simply increasing the size of
the KI corpus may not lead to better KI; fundamental advances may be needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LISA: Learning Interpretable Skill Abstractions from Language. (arXiv:2203.00054v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00054">
<div class="article-summary-box-inner">
<span><p>Learning policies that effectively utilize language instructions in complex,
multi-task environments is an important problem in imitation learning. While it
is possible to condition on the entire language instruction directly, such an
approach could suffer from generalization issues. To encode complex
instructions into skills that can generalize to unseen instructions, we propose
Learning Interpretable Skill Abstractions (LISA), a hierarchical imitation
learning framework that can learn diverse, interpretable skills from
language-conditioned demonstrations. LISA uses vector quantization to learn
discrete skill codes that are highly correlated with language instructions and
the behavior of the learned policy. In navigation and robotic manipulation
environments, LISA outperforms a strong non-hierarchical baseline in the low
data regime and is able to compose learned skills to solve tasks containing
unseen long-range instructions. Our method demonstrates a more natural way to
condition on language in sequential decision-making problems and achieve
interpretable and controllable behavior with the learned skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language and Culture Internalisation for Human-Like Autotelic AI. (arXiv:2206.01134v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01134">
<div class="article-summary-box-inner">
<span><p>Building autonomous agents able to grow open-ended repertoires of skills
across their lives is a fundamental goal of artificial intelligence (AI). A
promising developmental approach recommends the design of intrinsically
motivated agents that learn new skills by generating and pursuing their own
goals - autotelic agents. But despite recent progress, existing algorithms
still show serious limitations in terms of goal diversity, exploration,
generalisation or skill composition. This perspective calls for the immersion
of autotelic agents into rich socio-cultural worlds, an immensely important
attribute of our environment that shapes human cognition but is mostly omitted
in modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian
autotelic agents - agents able to internalise their interactions with others
and turn them into cognitive tools. We focus on language and show how its
structure and informational content may support the development of new
cognitive functions in artificial agents as it does in humans. We justify the
approach by uncovering several examples of new artificial cognitive functions
emerging from interactions between language and embodiment in recent works at
the intersection of deep reinforcement learning and natural language
processing. Looking forward, we highlight future opportunities and challenges
for Vygotskian Autotelic AI research, including the use of language models as
cultural models supporting artificial cognitive development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11697">
<div class="article-summary-box-inner">
<span><p>Recently Convolution-augmented Transformer (Conformer) has shown promising
results in Automatic Speech Recognition (ASR), outperforming the previous best
published Transformer Transducer. In this work, we believe that the output
information of each block in the encoder and decoder is not completely
inclusive, in other words, their output information may be complementary. We
study how to take advantage of the complementary information of each block in a
parameter-efficient way, and it is expected that this may lead to more robust
performance. Therefore we propose the Block-augmented Transformer for speech
recognition, named Blockformer. We have implemented two block ensemble methods:
the base Weighted Sum of the Blocks Output (Base-WSBO), and the
Squeeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).
Experiments have proved that the Blockformer significantly outperforms the
state-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER
of 4.29\% without using a language model and 4.05\% with an external language
model on the testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v9 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14382">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that are self-supervised and can be adapted with fine
tuning to a wide range of natural language tasks, each of which previously
would have required a separate network model. This is one step closer to the
extraordinary versatility of human language. GPT-3 and more recently LaMDA can
carry on dialogs with humans on many topics after minimal priming with a few
examples. However, there has been a wide range of reactions and debate on
whether these LLMs understand what they are saying or exhibit signs of
intelligence. This high variance is exhibited in three interviews with LLMs
reaching wildly different conclusions. A new possibility was uncovered that
could explain this divergence. What appears to be intelligence in LLMs may in
fact be a mirror that reflects the intelligence of the interviewer, a
remarkable twist that could be considered a Reverse Turing Test. If so, then by
studying interviews we may be learning more about the intelligence and beliefs
of the interviewer than the intelligence of the LLMs. As LLMs become more
capable they may transform the way we interact with machines and how they
interact with each other. Increasingly, LLMs are being coupled with
sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A
road map for achieving artificial general autonomy is outlined with seven major
improvements inspired by brain systems. LLMs could be used to uncover new
insights into brain function by downloading brain data during natural
behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atlas: Few-shot Learning with Retrieval Augmented Language Models. (arXiv:2208.03299v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03299">
<div class="article-summary-box-inner">
<span><p>Large language models have shown impressive few-shot results on a wide range
of tasks. However, when knowledge is key for such results, as is the case for
tasks such as question answering and fact checking, massive parameter counts to
store knowledge seem to be needed. Retrieval augmented models are known to
excel at knowledge intensive tasks without the need for as many parameters, but
it is unclear whether they work in few-shot settings. In this work we present
Atlas, a carefully designed and pre-trained retrieval augmented language model
able to learn knowledge intensive tasks with very few training examples. We
perform evaluations on a wide range of tasks, including MMLU, KILT and
NaturalQuestions, and study the impact of the content of the document index,
showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy
on Natural Questions using only 64 examples, outperforming a 540B parameters
model by 3% despite having 50x fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Pretrained Text-to-Text Models for Long Text Sequences. (arXiv:2209.10052v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10052">
<div class="article-summary-box-inner">
<span><p>We present an empirical study of adapting an existing pretrained text-to-text
model for long-sequence inputs. Through a comprehensive study along three axes
of the pretraining pipeline -- model architecture, optimization objective, and
pretraining corpus, we propose an effective recipe to build long-context models
from existing short-context models. Specifically, we replace the full attention
in transformers with pooling-augmented blockwise attention, and pretrain the
model with a masked-span prediction task with spans of varying length. In terms
of the pretraining corpus, we find that using randomly concatenated
short-documents from a large open-domain corpus results in better performance
than using existing long document corpora which are typically limited in their
domain coverage. With these findings, we build a long-context model that
achieves competitive performance on long-text QA tasks and establishes the new
state of the art on five long-text summarization datasets, often outperforming
previous methods with larger model sizes. Our code has been released at
https://github.com/facebookresearch/bart_ls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emb-GAM: an Interpretable and Efficient Predictor using Pre-trained Language Models. (arXiv:2209.11799v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11799">
<div class="article-summary-box-inner">
<span><p>Deep learning models have achieved impressive prediction performance but
often sacrifice interpretability, a critical consideration in high-stakes
domains such as healthcare or policymaking. In contrast, generalized additive
models (GAMs) can maintain interpretability but often suffer from poor
prediction performance due to their inability to effectively capture feature
interactions. In this work, we aim to bridge this gap by using pre-trained
neural language models to extract embeddings for each input before learning a
linear model in the embedding space. The final model (which we call Emb-GAM) is
a transparent, linear function of its input features and feature interactions.
Leveraging the language model allows Emb-GAM to learn far fewer linear
coefficients, model larger interactions, and generalize well to novel inputs
(e.g. unseen ngrams in text). Across a variety of natural-language-processing
datasets, Emb-GAM achieves strong prediction performance without sacrificing
interpretability. All code is made available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision. (arXiv:2210.10031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10031">
<div class="article-summary-box-inner">
<span><p>In the age of social media, where billions of internet users share
information and opinions, the negative impact of pandemics is not limited to
the physical world. It provokes a surge of incomplete, biased, and incorrect
information, also known as an infodemic. This global infodemic jeopardizes
measures to control the pandemic by creating panic, vaccine hesitancy, and
fragmented social response. Platforms like Facebook allow advertisers to adapt
their messaging to target different demographics and help alleviate or
exacerbate the infodemic problem depending on their content. In this paper, we
propose a minimally supervised multi-task learning framework for understanding
messaging on Facebook related to the COVID vaccine by identifying ad themes and
moral foundations. Furthermore, we perform a more nuanced thematic analysis of
messaging tactics of vaccine campaigns on social media so that policymakers can
make better decisions on pandemic control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcending Scaling Laws with 0.1% Extra Compute. (arXiv:2210.11399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11399">
<div class="article-summary-box-inner">
<span><p>Scaling language models improves performance but comes with significant
computational costs. This paper proposes UL2R, a method that substantially
improves existing language models and their scaling curves with a relatively
tiny amount of extra compute. The key idea is to continue training a
state-of-the-art large language model (e.g., PaLM) on a few more steps with
UL2's mixture-of-denoiser objective. We show that, with almost negligible extra
computational costs and no new sources of data, we are able to substantially
improve the scaling properties of large language models on downstream metrics.
In this paper, we continue training PaLM with UL2R, introducing a new set of
models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B
scale, we show an approximately 2x computational savings rate where U-PaLM
achieves the same performance as the final PaLM 540B model at around half its
computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further
show that this improved scaling curve leads to 'emergent abilities' on
challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM
on some tasks or demonstrates better quality at much smaller scale (62B as
opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many
few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question
answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual
tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide
qualitative examples showing the new capabilities of U-PaLM for single and
multi-span infilling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11416">
<div class="article-summary-box-inner">
<span><p>Finetuning language models on a collection of datasets phrased as
instructions has been shown to improve model performance and generalization to
unseen tasks. In this paper we explore instruction finetuning with a particular
focus on (1) scaling the number of tasks, (2) scaling the model size, and (3)
finetuning on chain-of-thought data. We find that instruction finetuning with
the above aspects dramatically improves performance on a variety of model
classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and
evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For
instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM
540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
state-of-the-art performance on several benchmarks, such as 75.2% on five-shot
MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong
few-shot performance even compared to much larger models, such as PaLM 62B.
Overall, instruction finetuning is a general method for improving the
performance and usability of pretrained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12353">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) like GPT-3 have achieved impressive
results on multiple choice question answering (MCQA) tasks in the zero, one,
and few-shot settings, they generally lag behind the MCQA state of the art
(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.
An LLM is conditioned on a question (without the associated answer options) and
its chosen option is the one assigned the highest probability after
normalization (for length, etc.). A more natural prompting approach is to
present the question and answer options to the LLM jointly and have it output
the symbol (e.g., "A") associated with its chosen answer option. This approach
allows the model to explicitly compare answer options, reduces computational
costs, and mitigates the effects of tokenization scheme and answer option
representations on answer selection. For the natural approach to be effective
the LLM it is used with must be able to associate answer options with the
symbols that represent them. The LLM needs what we term multiple choice symbol
binding (MCSB) ability. This ability varies greatly by model. We show that a
model with high MCSB ability performs much better with the natural approach
than with the traditional approach across 20 diverse datasets and largely
closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been
previously underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition. (arXiv:2210.12391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12391">
<div class="article-summary-box-inner">
<span><p>African languages are spoken by over a billion people, but are
underrepresented in NLP research and development. The challenges impeding
progress include the limited availability of annotated datasets, as well as a
lack of understanding of the settings where current methods are effective. In
this paper, we make progress towards solutions for these challenges, focusing
on the task of named entity recognition (NER). We create the largest
human-annotated NER dataset for 20 African languages, and we study the behavior
of state-of-the-art cross-lingual transfer methods in an Africa-centric
setting, demonstrating that the choice of source language significantly affects
performance. We show that choosing the best transfer language improves
zero-shot F1 scores by an average of 14 points across 20 languages compared to
using English. Our results highlight the need for benchmark datasets and models
that cover typologically-diverse African languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13952">
<div class="article-summary-box-inner">
<span><p>We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies. (arXiv:2211.00201v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00201">
<div class="article-summary-box-inner">
<span><p>Clinical Cohort Studies (CCS), such as randomized clinical trials, are a
great source of documented clinical research. Ideally, a clinical expert
inspects these articles for exploratory analysis ranging from drug discovery
for evaluating the efficacy of existing drugs in tackling emerging diseases to
the first test of newly developed drugs. However, more than 100 articles are
published daily on a single prevalent disease like COVID-19 in PubMed. As a
result, it can take days for a physician to find articles and extract relevant
information. Can we develop a system to sift through the long list of these
articles faster and document the crucial takeaways from each of these articles?
In this work, we propose CCS Explorer, an end-to-end system for relevance
prediction of sentences, extractive summarization, and patient, outcome, and
intervention entity detection from CCS. CCS Explorer is packaged in a web-based
graphical user interface where the user can provide any disease name. CCS
Explorer then extracts and aggregates all relevant information from articles on
PubMed based on the results of an automatically generated query produced on the
back-end. For each task, CCS Explorer fine-tunes pre-trained language
representation models based on transformers with additional layers. The models
are evaluated using two publicly available datasets. CCS Explorer obtains a
recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence
relevance prediction using BioBERT and achieves an average Micro F1-Score of
77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus,
CCS Explorer can reliably extract relevant information to summarize articles,
saving time by $\sim \text{660}\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05166">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Correction (GEC) is the task of automatically detecting and
correcting errors in text. The task not only includes the correction of
grammatical errors, such as missing prepositions and mismatched subject-verb
agreement, but also orthographic and semantic errors, such as misspellings and
word choice errors respectively. The field has seen significant progress in the
last decade, motivated in part by a series of five shared tasks, which drove
the development of rule-based methods, statistical classifiers, statistical
machine translation, and finally neural machine translation systems which
represent the current dominant state of the art. In this survey paper, we
condense the field into a single article and first outline some of the
linguistic challenges of the task, introduce the most popular datasets that are
available to researchers (for both English and other languages), and summarise
the various methods and techniques that have been developed with a particular
focus on artificial error generation. We next describe the many different
approaches to evaluation as well as concerns surrounding metric reliability,
especially in relation to subjective human judgements, before concluding with
an overview of recent progress and suggestions for future work and remaining
challenges. We hope that this survey will serve as comprehensive resource for
researchers who are new to the field or who want to be kept apprised of recent
developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. (arXiv:2211.05719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05719">
<div class="article-summary-box-inner">
<span><p>Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 88x. Second, it contains massive topics to generalize
the open-domain. To build engaging dialogue system with this dataset, we
propose and normalize two response producing tasks based on retrieval and
generative scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction. (arXiv:2211.07047v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07047">
<div class="article-summary-box-inner">
<span><p>Traditional evaluation metrics for classification in natural language
processing such as accuracy and area under the curve fail to differentiate
between models with different predictive behaviors despite their similar
performance metrics. We introduce sensitivity score, a metric that scrutinizes
models' behaviors at the vocabulary level to provide insights into disparities
in their decision-making logic. We assess the sensitivity score on a set of
representative words in the test set using two classifiers trained for hospital
readmission classification with similar performance statistics. Our experiments
compare the decision-making logic of clinicians and classifiers based on rank
correlations of sensitivity scores. The results indicate that the language
model's sensitivity score aligns better with the professionals than the xgboost
classifier on tf-idf embeddings, which suggests that xgboost uses some spurious
features. Overall, this metric offers a novel perspective on assessing models'
robustness by quantifying their discrepancy with professional opinions. Our
code is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT. (arXiv:2211.07499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07499">
<div class="article-summary-box-inner">
<span><p>Keyword extraction has been an important topic for modern natural language
processing. With its applications ranging from ontology generation, fact
verification in summarized text, and recommendation systems. While it has had
significant data-intensive applications, it is often hampered when the data set
is small. Downstream training for keyword extractors is a lengthy process and
requires a significant amount of data. Recently, Few-shot Learning (FSL) and
Zero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,
we propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM
bases by incorporating the concept of regularized attention into a pre-training
phase for downstream domain adaptation. As we believe our work has implications
to be utilized in the pipeline of FSL/ZSL and keyword extraction, we
open-source our code as well as provide the fine-tuning library of the same
name AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilevel Transformer For Multimodal Emotion Recognition. (arXiv:2211.07711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07711">
<div class="article-summary-box-inner">
<span><p>Multimodal emotion recognition has attracted much attention recently. Fusing
multiple modalities effectively with limited labeled data is a challenging
task. Considering the success of pre-trained model and fine-grained nature of
emotion expression, it is reasonable to take these two aspects into
consideration. Unlike previous methods that mainly focus on one aspect, we
introduce a novel multi-granularity framework, which combines fine-grained
representation with pre-trained utterance-level representation. Inspired by
Transformer TTS, we propose a multilevel transformer model to perform
fine-grained multimodal emotion recognition. Specifically, we explore different
methods to incorporate phoneme-level embedding with word-level embedding. To
perform multi-granularity learning, we simply combine multilevel transformer
model with Albert. Extensive experimental results show that both our multilevel
transformer model and multi-granularity model outperform previous
state-of-the-art approaches on IEMOCAP dataset with text transcripts and speech
signal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Phrase-based Sequence-to-Sequence Learning. (arXiv:2211.07906v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07906">
<div class="article-summary-box-inner">
<span><p>We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search. (arXiv:2211.08237v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08237">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition (SER) classifies audio into emotion categories
such as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion
Recognition (SER) is a common application for popular languages, it continues
to be a problem for low-resourced languages, i.e., languages with no pretrained
speech-to-text recognition models. This paper firstly proposes a
language-specific model that extract emotional information from multiple
pre-trained speech models, and then designs a multi-domain model that
simultaneously performs SER for various languages. Our multidomain model
employs a multi-gating mechanism to generate unique weighted feature
combination for each language, and also searches for specific neural network
structure for each language through a neural architecture search module. In
addition, we introduce a contrastive auxiliary loss to build more separable
representations for audio data. Our experiments show that our model raises the
state-of-the-art accuracy by 3% for German and 14.3% for French.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-17 23:15:36.754708865 UTC">2022-11-17 23:15:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>