<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-02T01:30:00Z">03-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Design with Language Models. (arXiv:2303.00001v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00001">
<div class="article-summary-box-inner">
<span><p>Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus. (arXiv:2303.00069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00069">
<div class="article-summary-box-inner">
<span><p>At present, Text-to-speech (TTS) systems that are trained with high-quality
transcribed speech data using end-to-end neural models can generate speech that
is intelligible, natural, and closely resembles human speech. These models are
trained with relatively large single-speaker professionally recorded audio,
typically extracted from audiobooks. Meanwhile, due to the scarcity of freely
available speech corpora of this kind, a larger gap exists in Arabic TTS
research and development. Most of the existing freely available Arabic speech
corpora are not suitable for TTS training as they contain multi-speaker casual
speech with variations in recording conditions and quality, whereas the corpus
curated for speech synthesis are generally small in size and not suitable for
training state-of-the-art end-to-end models. In a move towards filling this gap
in resources, we present a speech corpus for Classical Arabic Text-to-Speech
(ClArTTS) to support the development of end-to-end TTS systems for Arabic. The
speech is extracted from a LibriVox audiobook, which is then processed,
segmented, and manually transcribed and annotated. The final ClArTTS corpus
contains about 12 hours of speech from a single male speaker sampled at 40100
kHz. In this paper, we describe the process of corpus creation and provide
details of corpus statistics and a comparison with existing resources.
Furthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and
illustrate the performance of the resulting systems via subjective and
objective evaluations. The corpus will be made publicly available at
www.clartts.com for research purposes, along with the baseline TTS systems
demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics. (arXiv:2303.00077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00077">
<div class="article-summary-box-inner">
<span><p>Large language models are not detailed models of human linguistic processing.
They are, however, extremely successful at their primary task: providing a
model for language. For this reason and because there are no animal models for
language, large language models are important in psycholinguistics: they are
useful as a practical tool, as an illustrative comparative, and
philosophically, as a basis for recasting the relationship between language and
thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model. (arXiv:2303.00091v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00091">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) is a technology that converts spoken words
into text, facilitating interaction between humans and machines. One of the
most common applications of ASR is Speech-To-Text (STT) technology, which
simplifies user workflows by transcribing spoken words into text. In the
medical field, STT has the potential to significantly reduce the workload of
clinicians who rely on typists to transcribe their voice recordings. However,
developing an STT model for the medical domain is challenging due to the lack
of sufficient speech and text datasets. To address this issue, we propose a
medical-domain text correction method that modifies the output text of a
general STT system using the Vision Language Pre-training (VLP) method. VLP
combines textual and visual information to correct text based on image
knowledge. Our extensive experiments demonstrate that the proposed method
offers quantitatively and clinically significant improvements in STT
performance in the medical field. We further show that multi-modal
understanding of image and text information outperforms single-modal
understanding using only text information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for COVID-19 topic modelling via Twitter: Alpha, Delta and Omicron. (arXiv:2303.00135v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00135">
<div class="article-summary-box-inner">
<span><p>Topic modelling with innovative deep learning methods has gained interest for
a wide range of applications that includes COVID-19. Topic modelling can
provide, psychological, social and cultural insights for understanding human
behaviour in extreme events such as the COVID-19 pandemic. In this paper, we
use prominent deep learning-based language models for COVID-19 topic modelling
taking into account data from emergence (Alpha) to the Omicron variant. We
apply topic modeling to review the public behaviour across the first, second
and third waves based on Twitter dataset from India. Our results show that the
topics extracted for the subsequent waves had certain overlapping themes such
as covers governance, vaccination, and pandemic management while novel issues
aroused in political, social and economic situation during COVID-19 pandemic.
We also found a strong correlation of the major topics qualitatively to news
media prevalent at the respective time period. Hence, our framework has the
potential to capture major issues arising during different phases of the
COVID-19 pandemic which can be extended to other countries and regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIFFQG: Generating Questions to Summarize Factual Changes. (arXiv:2303.00242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00242">
<div class="article-summary-box-inner">
<span><p>Identifying the difference between two versions of the same article is useful
to update knowledge bases and to understand how articles evolve. Paired texts
occur naturally in diverse situations: reporters write similar news stories and
maintainers of authoritative websites must keep their information up to date.
We propose representing factual changes between paired documents as
question-answer pairs, where the answer to the same question differs between
two versions. We find that question-answer pairs can flexibly and concisely
capture the updated contents. Provided with paired documents, annotators
identify questions that are answered by one passage but answered differently or
cannot be answered by the other. We release DIFFQG which consists of 759 QA
pairs and 1153 examples of paired passages with no factual change. These
questions are intended to be both unambiguous and information-seeking and
involve complex edits, pushing beyond the capabilities of current question
generation and factual change detection systems. Our dataset summarizes the
changes between two versions of the document as questions and answers, studying
automatic update summarization in a novel way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Markov Transformer for Simultaneous Machine Translation. (arXiv:2303.00257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00257">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) outputs the target sequence while
receiving the source sequence, and hence learning when to start translating
each target token is the core challenge for SiMT task. However, it is
non-trivial to learn the optimal moment among many possible moments of starting
translating, as the moments of starting translating always hide inside the
model and can only be supervised with the observed target sequence. In this
paper, we propose a Hidden Markov Transformer (HMT), which treats the moments
of starting translating as hidden events and the target sequence as the
corresponding observed events, thereby organizing them as a hidden Markov
model. HMT explicitly models multiple moments of starting translating as the
candidate hidden events, and then selects one to generate the target token.
During training, by maximizing the marginal likelihood of the target sequence
over multiple moments of starting translating, HMT learns to start translating
at the moments that target tokens can be generated more accurately. Experiments
on multiple SiMT benchmarks show that HMT outperforms strong baselines and
achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Covid-19 Segmentation via Vision-Language Alignment. (arXiv:2303.00279v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00279">
<div class="article-summary-box-inner">
<span><p>Segmentation of COVID-19 lesions can assist physicians in better diagnosis
and treatment of COVID-19. However, there are few relevant studies due to the
lack of detailed information and high-quality annotation in the COVID-19
dataset. To solve the above problem, we propose C2FVL, a Coarse-to-Fine
segmentation framework via Vision-Language alignment to merge text information
containing the number of lesions and specific locations of image information.
The introduction of text information allows the network to achieve better
prediction results on challenging datasets. We conduct extensive experiments on
two COVID-19 datasets including chest X-ray and CT, and the results demonstrate
that our proposed method outperforms other state-of-the-art segmentation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks. (arXiv:2303.00293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00293">
<div class="article-summary-box-inner">
<span><p>The GPT-3.5 models have demonstrated impressive performance in various
Natural Language Processing (NLP) tasks, showcasing their strong understanding
and reasoning capabilities. However, their robustness and abilities to handle
various complexities of the open world have yet to be explored, which is
especially crucial in assessing the stability of models and is a key aspect of
trustworthy AI. In this study, we perform a comprehensive experimental analysis
of GPT-3.5, exploring its robustness using 21 datasets (about 116K test
samples) with 66 text transformations from TextFlint that cover 9 popular
Natural Language Understanding (NLU) tasks. Our findings indicate that while
GPT-3.5 outperforms existing fine-tuned models on some tasks, it still
encounters significant robustness degradation, such as its average performance
dropping by up to 35.74\% and 43.59\% in natural language inference and
sentiment analysis tasks, respectively. We also show that GPT-3.5 faces some
specific robustness challenges, including robustness instability, prompt
sensitivity, and number sensitivity. These insights are valuable for
understanding its limitations and guiding future research in addressing these
challenges to enhance GPT-3.5's overall performance and generalization
abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Multiple User Interests using Hierarchical Knowledge for Conversational Recommender System. (arXiv:2303.00311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00311">
<div class="article-summary-box-inner">
<span><p>A conversational recommender system (CRS) is a practical application for item
recommendation through natural language conversation. Such a system estimates
user interests for appropriate personalized recommendations. Users sometimes
have various interests in different categories or genres, but existing studies
assume a unique user interest that can be covered by closely related items. In
this work, we propose to model such multiple user interests in CRS. We
investigated its effects in experiments using the ReDial dataset and found that
the proposed method can recommend a wider variety of items than that of the
baseline CR-Walker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-Based Analysis of Language Models. (arXiv:2303.00333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00333">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of large pretrained language models (LMs) on a
variety of prompting tasks, these models can be alarmingly brittle to small
changes in inputs or application contexts. To better understand such behavior
and motivate the design of more robust LMs, we propose a general experimental
framework, CALM (Competence-based Analysis of Language Models), where targeted
causal interventions are utilized to damage an LM's internal representation of
various linguistic properties in order to evaluate its use of each
representation in performing a given task. We implement these interventions as
gradient-based adversarial attacks, which (in contrast to prior causal probing
methodologies) are able to target arbitrarily-encoded representations of
relational properties, and carry out a case study of this approach to analyze
how BERT-like LMs use representations of several relational properties in
performing associated relation prompting tasks. We find that, while the
representations LMs leverage in performing each task are highly entangled, they
may be meaningfully interpreted in terms of the tasks where they are most
utilized; and more broadly, that CALM enables an expanded scope of inquiry in
LM analysis that may be useful in predicting and explaining weaknesses of
existing LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inline Citation Classification using Peripheral Context and Time-evolving Augmentation. (arXiv:2303.00344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00344">
<div class="article-summary-box-inner">
<span><p>Citation plays a pivotal role in determining the associations among research
articles. It portrays essential information in indicative, supportive, or
contrastive studies. The task of inline citation classification aids in
extrapolating these relationships; However, existing studies are still immature
and demand further scrutiny. Current datasets and methods used for inline
citation classification only use citation-marked sentences constraining the
model to turn a blind eye to domain knowledge and neighboring contextual
sentences. In this paper, we propose a new dataset, named 3Cext, which along
with the cited sentences, provides discourse information using the vicinal
sentences to analyze the contrasting and entailing relationships as well as
domain information. We propose PeriCite, a Transformer-based deep neural
network that fuses peripheral sentences and domain knowledge. Our model
achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best
baseline. We conduct extensive ablations to analyze the efficacy of the
proposed dataset and model fusion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Persian Benchmark for Joint Intent Detection and Slot Filling. (arXiv:2303.00408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00408">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) is important in today's technology as it
enables machines to comprehend and process human language, leading to improved
human-computer interactions and advancements in fields such as virtual
assistants, chatbots, and language-based AI systems. This paper highlights the
significance of advancing the field of NLU for low-resource languages. With
intent detection and slot filling being crucial tasks in NLU, the widely used
datasets ATIS and SNIPS have been utilized in the past. However, these datasets
only cater to the English language and do not support other languages. In this
work, we aim to address this gap by creating a Persian benchmark for joint
intent detection and slot filling based on the ATIS dataset. To evaluate the
effectiveness of our benchmark, we employ state-of-the-art methods for intent
detection and slot filling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00456">
<div class="article-summary-box-inner">
<span><p>Error correction models form an important part of Automatic Speech
Recognition (ASR) post-processing to improve the readability and quality of
transcriptions. Most prior works use the 1-best ASR hypothesis as input and
therefore can only perform correction by leveraging the context within one
sentence. In this work, we propose a novel N-best T5 model for this task, which
is fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By
transferring knowledge from the pre-trained language model and obtaining richer
information from the ASR decoding space, the proposed approach outperforms a
strong Conformer-Transducer baseline. Another issue with standard error
correction is that the generation process is not well-guided. To address this a
constrained decoding process, either based on the N-best list or an ASR
lattice, is used which allows additional information to be propagated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uzbek text summarization based on TF-IDF. (arXiv:2303.00461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00461">
<div class="article-summary-box-inner">
<span><p>The volume of information is increasing at an incredible rate with the rapid
development of the Internet and electronic information services. Due to time
constraints, we don't have the opportunity to read all this information. Even
the task of analyzing textual data related to one field requires a lot of work.
The text summarization task helps to solve these problems. This article
presents an experiment on summarization task for Uzbek language, the
methodology was based on text abstracting based on TF-IDF algorithm. Using this
density function, semantically important parts of the text are extracted. We
summarize the given text by applying the n-gram method to important parts of
the whole text. The authors used a specially handcrafted corpus called "School
corpus" to evaluate the performance of the proposed method. The results show
that the proposed approach is effective in extracting summaries from Uzbek
language text and can potentially be used in various applications such as
information retrieval and natural language processing. Overall, this research
contributes to the growing body of work on text summarization in
under-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uzbek text's correspondence with the educational potential of pupils: a case study of the School corpus. (arXiv:2303.00465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00465">
<div class="article-summary-box-inner">
<span><p>One of the major challenges of an educational system is choosing appropriate
content considering pupils' age and intellectual potential. In this article the
experiment of primary school grades (from 1st to 4th grades) is considered for
automatically determining the correspondence of an educational materials
recommended for pupils by using the School corpus where it includes the dataset
of 25 school textbooks confirmed by the Ministry of preschool and school
education of the Republic of Uzbekistan. In this case, TF-IDF scores of the
texts are determined, they are converted into a vector representation, and the
given educational materials are compared with the corresponding class of the
School corpus using the cosine similarity algorithm. Based on the results of
the calculation, it is determined whether the given educational material is
appropriate or not appropriate for the pupils' educational potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training. (arXiv:2303.00534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00534">
<div class="article-summary-box-inner">
<span><p>Vision-and-language multi-modal pretraining and fine-tuning have shown great
success in visual question answering (VQA). Compared to general domain VQA, the
performance of biomedical VQA suffers from limited data. In this paper, we
propose a retrieval-augmented pretrain-and-finetune paradigm named RAMM for
biomedical VQA to overcome the data limitation issue. Specifically, we collect
a new biomedical dataset named PMCPM which offers patient-based image-text
pairs containing diverse patient situations from PubMed. Then, we pretrain the
biomedical multi-modal model to learn visual and textual representation for
image-text pairs and align these representations with image-text contrastive
objective (ITC). Finally, we propose a retrieval-augmented method to better use
the limited data. We propose to retrieve similar image-text pairs based on ITC
from pretraining datasets and introduce a novel retrieval-attention module to
fuse the representation of the image and the question with the retrieved images
and texts. Experiments demonstrate that our retrieval-augmented
pretrain-and-finetune paradigm obtains state-of-the-art performance on
Med-VQA2019, Med-VQA2021, VQARAD, and SLAKE datasets. Further analysis shows
that the proposed RAMM and PMCPM can enhance biomedical VQA performance
compared with previous resources and methods. We will open-source our dataset,
codes, and pretrained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00595">
<div class="article-summary-box-inner">
<span><p>Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
</p>
<p>In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation. (arXiv:2303.00628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00628">
<div class="article-summary-box-inner">
<span><p>We introduce MuAViC, a multilingual audio-visual corpus for robust speech
recognition and robust speech-to-text translation providing 1200 hours of
audio-visual speech in 9 languages. It is fully transcribed and covers 6
English-to-X translation as well as 6 X-to-English translation directions. To
the best of our knowledge, this is the first open benchmark for audio-visual
speech-to-text translation and the largest open benchmark for multilingual
audio-visual speech recognition. Our baseline results show that MuAViC is
effective for building noise-robust speech recognition and translation models.
We make the corpus available at https://github.com/facebookresearch/muavic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation. (arXiv:2303.00722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00722">
<div class="article-summary-box-inner">
<span><p>The effectiveness of Neural Machine Translation (NMT) models largely depends
on the vocabulary used at training; small vocabularies can lead to
out-of-vocabulary problems -- large ones, to memory issues. Subword (SW)
tokenization has been successfully employed to mitigate these issues. The
choice of vocabulary and SW tokenization has a significant impact on both
training and fine-tuning an NMT model. Fine-tuning is a common practice in
optimizing an MT model with respect to new data. However, new data potentially
introduces new words (or tokens), which, if not taken into consideration, may
lead to suboptimal performance. In addition, the distribution of tokens in the
new data can differ from the distribution of the original data. As such, the
original SW tokenization model could be less suitable for the new data. Through
a systematic empirical evaluation, in this work we compare different strategies
for SW tokenization and vocabulary generation with the ultimate goal to uncover
an optimal setting for fine-tuning a domain-specific model. Furthermore, we
developed several (in-domain) models, the best of which achieves 6 BLEU points
improvement over the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks. (arXiv:2303.00733v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00733">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a technology that tunes a small set of parameters to steer a
pre-trained language model (LM) to directly generate the output for downstream
tasks. Recently, prompt tuning has demonstrated its storage and computation
efficiency in both natural language processing (NLP) and speech processing
fields. These advantages have also revealed prompt tuning as a candidate
approach to serving pre-trained LM for multiple tasks in a unified manner. For
speech processing, SpeechPrompt shows its high parameter efficiency and
competitive performance on a few speech classification tasks. However, whether
SpeechPrompt is capable of serving a large number of tasks is unanswered. In
this work, we propose SpeechPrompt v2, a prompt tuning framework capable of
performing a wide variety of speech classification tasks, covering multiple
languages and prosody-related tasks. The experiment result shows that
SpeechPrompt v2 achieves performance on par with prior works with less than
0.15M trainable parameters in a unified framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformers know symbolic rules, and would we know if they did?. (arXiv:2203.00162v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00162">
<div class="article-summary-box-inner">
<span><p>To improve the explainability of leading Transformer networks used in NLP, it
is important to tease apart genuine symbolic rules from merely associative
input-output patterns. However, we identify several inconsistencies in how
``symbolicity'' has been construed in recent NLP literature. To mitigate this
problem, we propose two criteria to be the most relevant, one pertaining to a
system's internal architecture and the other to the dissociation between
abstract rules and specific input identities. From this perspective, we
critically examine prior work on the symbolic capacities of Transformers, and
deem the results to be fundamentally inconclusive for reasons inherent in
experiment design. We further maintain that there is no simple fix to this
problem, since it arises -- to an extent -- in all end-to-end settings.
Nonetheless, we emphasize the need for more robust evaluation of whether
non-symbolic explanations exist for success in seemingly symbolic tasks. To
facilitate this, we experiment on four sequence modelling tasks on the T5
Transformer in two experiment settings: zero-shot generalization, and
generalization across class-specific vocabularies flipped between the training
and test set. We observe that T5's generalization is markedly stronger in
sequence-to-sequence tasks than in comparable classification tasks. Based on
this, we propose a thus far overlooked analysis, where the Transformer itself
does not need to be symbolic to be part of a symbolic architecture as the
processor, operating on the input and output as external memory components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes. (arXiv:2205.05656v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05656">
<div class="article-summary-box-inner">
<span><p>Computational text phenotyping is the practice of identifying patients with
certain disorders and traits from clinical notes. Rare diseases are challenging
to be identified due to few cases available for machine learning and the need
for data annotation from domain experts. We propose a method using ontologies
and weak supervision, with recent pre-trained contextual representations from
Bi-directional Transformers (e.g. BERT). The ontology-based framework includes
two steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking
mentions to concepts in Unified Medical Language System (UMLS), with a Named
Entity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with
customised rules and contextual mention representation; (ii) UMLS-to-ORDO,
matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology
(ORDO). The weakly supervised approach is proposed to learn a phenotype
confirmation model to improve Text-to-UMLS linking, without annotated data from
domain experts. We evaluated the approach on three clinical datasets, MIMIC-III
discharge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging
reports from two institutions in the US and the UK, with annotations. The
improvements in the precision were pronounced (by over 30% to 50% absolute
score for Text-to-UMLS linking), with almost no loss of recall compared to the
existing NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and
NHS Tayside were consistent with the discharge summaries. The overall pipeline
processing clinical notes can extract rare disease cases, mostly uncaptured in
structured data (manually assigned ICD codes). We discuss the usefulness of the
weak supervision approach and propose directions for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Similarity is More Valuable than Character Similarity: An Empirical Study for Chinese Spell Checking. (arXiv:2207.09217v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09217">
<div class="article-summary-box-inner">
<span><p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling
errors. Recently, related researches focus on introducing character similarity
from confusion set to enhance the CSC models, ignoring the context of
characters that contain richer information. To make better use of contextual
information, we propose a simple yet effective Curriculum Learning (CL)
framework for the CSC task. With the help of our model-agnostic CL framework,
existing CSC models will be trained from easy to difficult as humans learn
Chinese characters and achieve further performance improvements. Extensive
experiments and detailed analyses on widely used SIGHAN datasets show that our
method outperforms previous state-of-the-art methods. More instructively, our
study empirically suggests that contextual similarity is more valuable than
character similarity for the CSC task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. (arXiv:2208.04202v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.04202">
<div class="article-summary-box-inner">
<span><p>We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00312">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is fundamental to human cognition and holds an important
place in various fields. However, previous studies mainly focus on single-modal
analogical reasoning and ignore taking advantage of structure knowledge.
Notably, the research in cognitive psychology has demonstrated that information
from multimodal sources always brings more powerful cognitive transfer than
single modality sources. To this end, we introduce the new task of multimodal
analogical reasoning over knowledge graphs, which requires multimodal reasoning
ability with the help of background knowledge. Specifically, we construct a
Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph
MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained
Transformer baselines, illustrating the potential challenges of the proposed
task. We further propose a novel model-agnostic Multimodal analogical reasoning
framework with Transformer (MarT) motivated by the structure mapping theory,
which can obtain better performance. Code and datasets are available in
https://github.com/zjunlp/MKG_Analogy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. (arXiv:2210.01241v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01241">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of aligning pre-trained large language models (LMs)
with human preferences. If we view text generation as a sequential
decision-making problem, reinforcement learning (RL) appears to be a natural
conceptual framework. However, using RL for LM-based generation faces empirical
challenges, including training instability due to the combinatorial action
space, as well as a lack of open-source libraries and benchmarks customized for
LM alignment. Thus, a question rises in the research community: is RL a
practical paradigm for NLP?
</p>
<p>To help answer this, we first introduce an open-source modular library,
RL4LMs (Reinforcement Learning for Language Models), for optimizing language
generators with RL. The library consists of on-policy RL algorithms that can be
used to train any encoder or encoder-decoder LM in the HuggingFace library
(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE
(General Reinforced-language Understanding Evaluation) benchmark, a set of 6
language generation tasks which are supervised not by target strings, but by
reward functions which capture automated measures of human preference.GRUE is
the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,
we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language
Policy Optimization)} that learns to effectively reduce the combinatorial
action space in language generation. We show 1) that RL techniques are
generally better than supervised methods at aligning LMs to human preferences;
and 2) that NLPO exhibits greater stability and performance than previous
policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both
automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binding Language Models in Symbolic Languages. (arXiv:2210.02875v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02875">
<div class="article-summary-box-inner">
<span><p>Though end-to-end neural approaches have recently been dominating NLP tasks
in both performance and ease-of-use, they lack interpretability and robustness.
We propose Binder, a training-free neural-symbolic framework that maps the task
input to a program, which (1) allows binding a unified API of language model
(LM) functionalities to a programming language (e.g., SQL, Python) to extend
its grammar coverage and thus tackle more diverse questions, (2) adopts an LM
as both the program parser and the underlying model called by the API during
execution, and (3) requires only a few in-context exemplar annotations.
Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only
a few in-context exemplars, Codex is able to identify the part of the task
input that cannot be answerable by the original programming language, correctly
generate API calls to prompt Codex to solve the unanswerable part, and identify
where to place the API calls while being compatible with the original grammar.
In the execution stage, Codex can perform versatile functionalities (e.g.,
commonsense QA, information extraction) given proper prompts in the API calls.
Binder achieves state-of-the-art results on WikiTableQuestions and TabFact
datasets, with explicit output programs that benefit human debugging. Note that
previous best systems are all finetuned on tens of thousands of task-specific
samples, while Binder only uses dozens of annotations as in-context exemplars
without any training. Our code is available at https://github.com/HKUNLP/Binder .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning. (arXiv:2210.12587v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12587">
<div class="article-summary-box-inner">
<span><p>Prompt tuning approaches, which learn task-specific soft prompts for a
downstream task conditioning on frozen pre-trained models, have attracted
growing interest due to its parameter efficiency. With large language models
and sufficient training data, prompt tuning performs comparably to full-model
tuning. However, with limited training samples in few-shot settings, prompt
tuning fails to match the performance of full-model fine-tuning. In this work,
we focus on improving the few-shot performance of prompt tuning by transferring
knowledge from soft prompts of source tasks. Recognizing the good
generalization capabilities of ensemble methods in low-data regime, we first
experiment and show that a simple ensemble of model predictions based on
different source prompts, outperforms existing multi-prompt knowledge transfer
approaches such as source prompt fusion in the few-shot setting. Motivated by
this observation, we further investigate model ensembles and propose
Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the
contribution of each source model for each target sample separately when
ensembling source model outputs. Through this way, SESoM inherits the superior
generalization of model ensemble approaches and simultaneously captures the
sample-specific competence of each source prompt. We conduct experiments across
a diverse set of eight NLP tasks using models of different scales (T5-{base,
large, XL}) and find that SESoM consistently outperforms the existing models of
the same as well as larger parametric scale by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition. (arXiv:2211.04717v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04717">
<div class="article-summary-box-inner">
<span><p>Noisy Student Training (NST) has recently demonstrated extremely strong
performance in Automatic Speech Recognition(ASR). In this paper, we propose a
data selection strategy named LM Filter to improve the performance of NST on
non-target domain data in ASR tasks. Hypotheses with and without a Language
Model are generated and the CER differences between them are utilized as a
filter threshold. Results reveal that significant improvements of 10.4%
compared with no data filtering baselines. We can achieve 3.31% CER in
AISHELL-1 test set, which is best result from our knowledge without any other
supervised data. We also perform evaluations on the supervised 1000 hour
AISHELL-2 dataset and competitive results of 4.73% CER can be achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards continually learning new languages. (arXiv:2211.11703v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11703">
<div class="article-summary-box-inner">
<span><p>Multilingual speech recognition with neural networks is often implemented
with batch-learning, when all of the languages are available before training.
An ability to add new languages after the prior training sessions can be
economically beneficial, but the main challenge is catastrophic forgetting. In
this work, we combine the qualities of weight factorization and elastic weight
consolidation in order to counter catastrophic forgetting and facilitate
learning new languages quickly. Such combination allowed us to eliminate
catastrophic forgetting while still achieving performance for the new languages
comparable with having all languages at once, in experiments of learning from
an initial 10 languages to achieve 26 languages without catastrophic forgetting
and a reasonable performance compared to training all languages from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. (arXiv:2212.00959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00959">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it first retrieves a relatively small subgraph related to the question and then
performs the reasoning on the subgraph to find the answer entities accurately.
Although these two stages are highly related, previous work employs very
different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the directed edges on KGs. For parameter learning, we design a shared
pre-training task based on question-relation matching for both retrieval and
reasoning models, and then propose retrieval- and reasoning-oriented
fine-tuning strategies. Compared with previous studies, our approach is more
unified, tightly relating the retrieval and reasoning stages. Extensive
experiments on three benchmark datasets have demonstrated the effectiveness of
our method on the multi-hop KGQA task. Our codes and data are publicly
available at~\url{https://github.com/RUCAIBox/UniKGQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Avoiding spurious correlations via logit correction. (arXiv:2212.01433v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01433">
<div class="article-summary-box-inner">
<span><p>Empirical studies suggest that machine learning models trained with empirical
risk minimization (ERM) often rely on attributes that may be spuriously
correlated with the class labels. Such models typically lead to poor
performance during inference for data lacking such correlations. In this work,
we explicitly consider a situation where potential spurious correlations are
present in the majority of training data. In contrast with existing approaches,
which use the ERM model outputs to detect the samples without spurious
correlations and either heuristically upweight or upsample those samples, we
propose the logit correction (LC) loss, a simple yet effective improvement on
the softmax cross-entropy loss, to correct the sample logit. We demonstrate
that minimizing the LC loss is equivalent to maximizing the group-balanced
accuracy, so the proposed LC could mitigate the negative impacts of spurious
correlations. Our extensive experimental results further reveal that the
proposed LC loss outperforms state-of-the-art solutions on multiple popular
benchmarks by a large margin, an average 5.5\% absolute improvement, without
access to spurious attribute labels. LC is also competitive with oracle methods
that make use of the attribute labels. Code is available at
https://github.com/shengliu66/LC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. (arXiv:2212.05032v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05032">
<div class="article-summary-box-inner">
<span><p>Large-scale diffusion models have achieved state-of-the-art results on
text-to-image synthesis (T2I) tasks. Despite their ability to generate
high-quality yet creative images, we observe that attribution-binding and
compositional capabilities are still considered major challenging issues,
especially when involving multiple objects. In this work, we improve the
compositional skills of T2I models, specifically more accurate attribute
binding and better image compositions. To do this, we incorporate linguistic
structures with the diffusion guidance process based on the controllable
properties of manipulating cross-attention layers in diffusion-based T2I
models. We observe that keys and values in cross-attention layers have strong
semantic meanings associated with object layouts and content. Therefore, we can
better preserve the compositional semantics in the generated image by
manipulating the cross-attention representations based on linguistic insights.
Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention
design is efficient that requires no additional training samples. We achieve
better compositional skills in qualitative and quantitative results, leading to
a 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an
in-depth analysis to reveal potential causes of incorrect image compositions
and justify the properties of cross-attention layers in the generation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training language models to summarize narratives improves brain alignment. (arXiv:2212.10898v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10898">
<div class="article-summary-box-inner">
<span><p>Building systems that achieve a deeper understanding of language is one of
the central goals of natural language processing (NLP). Towards this goal,
recent works have begun to train language models on narrative datasets which
require extracting the most critical information by integrating across long
contexts. However, it is still an open question whether these models are
learning a deeper understanding of the text, or if the models are simply
learning a heuristic to complete the task. This work investigates this further
by turning to the one language processing system that truly understands complex
language: the human brain. We show that training language models for deeper
narrative understanding results in richer representations that have improved
alignment to human brain activity. We further find that the improvements in
brain alignment are larger for character names than for other discourse
features, which indicates that these models are learning important narrative
elements. Taken together, these results suggest that this type of training can
indeed lead to deeper language understanding. These findings have consequences
both for cognitive neuroscience by revealing some of the significant factors
behind brain-NLP alignment, and for NLP by highlighting that understanding of
long-range context can be improved beyond language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling. (arXiv:2301.00591v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00591">
<div class="article-summary-box-inner">
<span><p>This work profoundly analyzes discrete self-supervised speech representations
(units) through the eyes of Generative Spoken Language Modeling (GSLM).
Following the findings of such an analysis, we propose practical improvements
to the discrete unit for the GSLM. First, we start comprehending these units by
analyzing them in three axes: interpretation, visualization, and resynthesis.
Our analysis finds a high correlation between the speech units to phonemes and
phoneme families, while their correlation with speaker or gender is weaker.
Additionally, we found redundancies in the extracted units and claim that one
reason may be the units' context. Following this analysis, we propose a new,
unsupervised metric to measure unit redundancies. Finally, we use this metric
to develop new methods that improve the robustness of units' clustering and
show significant improvement considering zero-resource speech metrics such as
ABX. Code and analysis tools are available under the following link:
https://github.com/slp-rl/SLM-Discrete-Representations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08771">
<div class="article-summary-box-inner">
<span><p>Developing models to automatically score students' written responses to
science problems is critical for science education. However, collecting and
labeling sufficient student responses for training models is time and
cost-consuming. Recent studies suggest that pre-trained language models (PLMs)
can be adapted to downstream tasks without fine-tuning with prompts. However,
no research has employed such a prompt approach in science education. As
student responses are presented with natural language, aligning the scoring
procedure as the next sentence prediction task using prompts can skip the
costly fine-tuning stage. In this study, we developed a zero-shot approach to
automatically score student responses via Matching Exemplars as Next Sentence
Prediction (MeNSP). This approach employs no training samples. We first apply
MeNSP in scoring three assessment tasks of scientific argumentation and found
machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and
F1 score ranges from 0.54 to 0.81. To improve the performance, we extend our
research to the few-shots setting, either randomly selecting labeled student
responses or manually constructing responses to fine-tune the models. We find
that one task's performance is improved with more samples, Cohen's Kappa from
0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring
performance is not improved. We also find that randomly selected few-shots
perform better than the human expert-crafted approach. This study suggests that
MeNSP can yield referable automatic scoring for student responses while
significantly reducing the cost of model training. This method can benefit
low-stakes classroom assessment practices in science education. Future research
should further explore the applicability of the MeNSP in different types of
assessment tasks in science education and improve the model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UzbekTagger: The rule-based POS tagger for Uzbek language. (arXiv:2301.12711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12711">
<div class="article-summary-box-inner">
<span><p>This research paper presents a part-of-speech (POS) annotated dataset and
tagger tool for the low-resource Uzbek language. The dataset includes 12 tags,
which were used to develop a rule-based POS-tagger tool. The corpus text used
in the annotation process was made sure to be balanced over 20 different fields
in order to ensure its representativeness. Uzbek being an agglutinative
language so the most of the words in an Uzbek sentence are formed by adding
suffixes. This nature of it makes the POS-tagging task difficult to find the
stems of words and the right part-of-speech they belong to. The methodology
proposed in this research is the stemming of the words with an affix/suffix
stripping approach including database of the stem forms of the words in the
Uzbek language. The tagger tool was tested on the annotated dataset and showed
high accuracy in identifying and tagging parts of speech in Uzbek text. This
newly presented dataset and tagger tool can be used for a variety of natural
language processing tasks such as language modeling, machine translation, and
text-to-speech synthesis. The presented dataset is the first of its kind to be
made publicly available for Uzbek, and the POS-tagger tool created can also be
used as a pivot to use as a base for other closely-related Turkic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10186">
<div class="article-summary-box-inner">
<span><p>This paper reimagines some aspects of speech processing using speech
encoders, specifically about extracting entities directly from speech, with no
intermediate textual representation. In human-computer conversations,
extracting entities such as names, postal addresses and email addresses from
speech is a challenging task. In this paper, we study the impact of fine-tuning
pre-trained speech encoders on extracting spoken entities in human-readable
form directly from speech without the need for text transcription. We
illustrate that such a direct approach optimizes the encoder to transcribe only
the entity relevant portions of speech, ignoring the superfluous portions such
as carrier phrases and spellings of entities. In the context of dialogs from an
enterprise virtual agent, we demonstrate that the 1-step approach outperforms
the typical 2-step cascade of first generating lexical transcriptions followed
by text-based entity extraction for identifying spoken entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (arXiv:2302.12813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12813">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and inability to use external knowledge.This paper
proposes a LLM-Augmenter system, which augments a black-box LLM with a set of
plug-and-play modules. Our system makes the LLM generate responses grounded in
consolidated external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of mission-critical scenarios, task-oriented dialog and open-domain
question answering. LLM-Augmenter significantly reduces ChatGPT's
hallucinations without sacrificing the fluency and informativeness of its
responses. We make the source code and models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media. (arXiv:2302.12840v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12840">
<div class="article-summary-box-inner">
<span><p>This paper describes our participation in SemEval-2023 Task 10, whose goal is
the detection of sexism in social media. We explore some of the most popular
transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study
different data augmentation techniques to increase the training dataset. During
the development phase, our best results were obtained by using RoBERTa and data
augmentation for tasks B and C. However, the use of synthetic data does not
improve the results for task C. We participated in the three subtasks. Our
approach still has much room for improvement, especially in the two
fine-grained classifications. All our code is available in the repository
https://github.com/isegura/hulat_edos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Is Not All You Need: Aligning Perception with Language Models. (arXiv:2302.14045v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14045">
<div class="article-summary-box-inner">
<span><p>A big convergence of language, multimodal perception, action, and world
modeling is a key step toward artificial general intelligence. In this work, we
introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive
general modalities, learn in context (i.e., few-shot), and follow instructions
(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale
multimodal corpora, including arbitrarily interleaved text and images,
image-caption pairs, and text data. We evaluate various settings, including
zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range
of tasks without any gradient updates or finetuning. Experimental results show
that Kosmos-1 achieves impressive performance on (i) language understanding,
generation, and even OCR-free NLP (directly fed with document images), (ii)
perception-language tasks, including multimodal dialogue, image captioning,
visual question answering, and (iii) vision tasks, such as image recognition
with descriptions (specifying classification via text instructions). We also
show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge
from language to multimodal, and from multimodal to language. In addition, we
introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning
capability of MLLMs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-02 23:13:40.155362005 UTC">2023-03-02 23:13:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>