<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-10T01:30:00Z">02-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CRL+: A Novel Semi-Supervised Deep Active Contrastive Representation Learning-Based Text Classification Model for Insurance Data. (arXiv:2302.04343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04343">
<div class="article-summary-box-inner">
<span><p>Financial sector and especially the insurance industry collect vast volumes
of text on a daily basis and through multiple channels (their agents, customer
care centers, emails, social networks, and web in general). The information
collected includes policies, expert and health reports, claims and complaints,
results of surveys, and relevant social media posts. It is difficult to
effectively extract label, classify, and interpret the essential information
from such varied and unstructured material. Therefore, the Insurance Industry
is among the ones that can benefit from applying technologies for the
intelligent analysis of free text through Natural Language Processing (NLP).
</p>
<p>In this paper, CRL+, a novel text classification model combining Contrastive
Representation Learning (CRL) and Active Learning is proposed to handle the
challenge of using semi-supervised learning for text classification. In this
method, supervised (CRL) is used to train a RoBERTa transformer model to encode
the textual data into a contrastive representation space and then classify
using a classification layer. This (CRL)-based transformer model is used as the
base model in the proposed Active Learning mechanism to classify all the data
in an iterative manner. The proposed model is evaluated using unstructured
obituary data with objective to determine the cause of the death from the data.
This model is compared with the CRL model and an Active Learning model with the
RoBERTa base model. The experiment shows that the proposed method can
outperform both methods for this specific task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment analysis and opinion mining on educational data: A survey. (arXiv:2302.04359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04359">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis AKA opinion mining is one of the most widely used NLP
applications to identify human intentions from their reviews. In the education
sector, opinion mining is used to listen to student opinions and enhance their
learning-teaching practices pedagogically. With advancements in sentiment
annotation techniques and AI methodologies, student comments can be labelled
with their sentiment orientation without much human intervention. In this
review article, (1) we consider the role of emotional analysis in education
from four levels: document level, sentence level, entity level, and aspect
level, (2) sentiment annotation techniques including lexicon-based and
corpus-based approaches for unsupervised annotations are explored, (3) the role
of AI in sentiment analysis with methodologies like machine learning, deep
learning, and transformers are discussed, (4) the impact of sentiment analysis
on educational procedures to enhance pedagogy, decision-making, and evaluation
are presented. Educational institutions have been widely invested to build
sentiment analysis tools and process their student feedback to draw their
opinions and insights. Applications built on sentiment analysis of student
feedback are reviewed in this study. Challenges in sentiment analysis like
multi-polarity, polysemous, negation words, and opinion spam detection are
explored and their trends in the research space are discussed. The future
directions of sentiment analysis in education are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04391">
<div class="article-summary-box-inner">
<span><p>In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04415">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult
for PLMs to yield faithful text. Low-resource generation likewise faces unique
challenges in this domain. Inspired by how humans descript tabular data with
prior knowledge, we suggest a new framework: PromptMize, which targets
table-to-text generation under few-shot settings. The design of our framework
consists of two aspects: a prompt planner and a knowledge adapter. The prompt
planner aims to generate a prompt signal that provides instance guidance for
PLMs to bridge the topology gap between tabular data and text. Moreover, the
knowledge adapter memorizes domain-specific knowledge from the unlabelled
corpus to supply essential information during generation. Extensive experiments
and analyses are investigated on three open domain few-shot NLG datasets:
human, song, and book. Compared with previous state-of-the-art approaches, our
model achieves remarkable performance in generating quality as judged by human
and automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-based Response Evaluator for Open-Domain Spoken Conversation. (arXiv:2302.04424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04424">
<div class="article-summary-box-inner">
<span><p>Many open-domain dialogue systems rely on multiple response generators, any
of which can contribute a response to the dialogue in a particular context.
Thus the ability to compare potential responses and then select the best plays
an important role in ensuring a dialogue system is coherent and engaging.
Dialogue coherence goes beyond simply remaining on topic -- some trivia may be
on topic and engaging when mentioned out of the blue, but may not be coherent
and grounded in the context of the conversation. We carry out experiments on
response selection in the Athena system, an Alexa Prize SocialBot that has
dedicated content and multiple topic-specific response generators for a large
number of topics. First, we collect a corpus of Athena conversations with live
human traffic, where potential responses from all enabled response generators
are logged and subsequently annotated for response quality. We compare several
off-the-shelf response ranking methods for open-domain dialogue to
Athena-Heuristic, a heuristic response ranker that was field-tested in Athena
during the third Alexa Prize competition. We also compare these to a
transformer-based response ranker we call Athena-RR, that we train on our
Athena conversations. Athena-RR uses both the conversational context and the
dialogue state to rank the potential responses. We find that Athena-RR with a
Recall@1 of 70.79\% outperforms Athena-Heuristic and all of the off-the-shelf
rankers by a large margin. We then conduct a live A/B study comparing
Athena-Heuristic to Athena-RR in a 6,358 conversations with Alexa users. We
show that Athena-RR leads to significantly longer conversations that receive
significantly higher user ratings than the heuristic rule-based ranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow. (arXiv:2302.04434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04434">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that language models exploit `artifacts' in
benchmarks to solve tasks, rather than truly learning them, leading to inflated
model performance. In pursuit of creating better benchmarks, we propose VAIDA,
a novel benchmark creation paradigm for NLP, that focuses on guiding
crowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.
VAIDA facilitates sample correction by providing realtime visual feedback and
recommendations to improve sample quality. Our approach is domain, model, task,
and metric agnostic, and constitutes a paradigm shift for robust, validated,
and dynamic benchmark creation via human-and-metric-in-the-loop workflows. We
evaluate via expert review and a user study with NASA TLX. We find that VAIDA
decreases effort, frustration, mental, and temporal demands of crowdworkers and
analysts, simultaneously increasing the performance of both user groups with a
45.8% decrease in the level of artifacts in created samples. As a by product of
our user study, we observe that created samples are adversarial across models,
leading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)
in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing E-Commerce Recommendation using Pre-Trained Language Model and Fine-Tuning. (arXiv:2302.04443v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04443">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLM) have been greatly successful on a board
range of natural language processing (NLP) tasks. However, it has just started
being applied to the domain of recommendation systems. Traditional
recommendation algorithms failed to incorporate the rich textual information in
e-commerce datasets, which hinderss the performance of those models. We present
a thorough investigation on the effect of various strategy of incorporating
PLMs into traditional recommender algorithms on one of the e-commerce datasets,
and we compare the results with vanilla recommender baseline models. We show
that the application of PLMs and domain specific fine-tuning lead to an
increase on the predictive capability of combined models. These results
accentuate the importance of utilizing textual information in the context of
e-commerce, and provides insight on how to better apply PLMs alongside
traditional recommender system algorithms. The code used in this paper is
available on Github: https://github.com/NuofanXu/bert_retail_recommender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04449">
<div class="article-summary-box-inner">
<span><p>High sample complexity has long been a challenge for RL. On the other hand,
humans learn to perform tasks not only from interaction or demonstrations, but
also by reading unstructured text documents, e.g., instruction manuals.
Instruction manuals and wiki pages are among the most abundant data that could
inform agents of valuable features and policies or task-specific environmental
dynamics and reward structures. Therefore, we hypothesize that the ability to
utilize human-written instruction manuals to assist learning policies for
specific tasks should lead to a more efficient and better-performing agent.
</p>
<p>We propose the Read and Reward framework. Read and Reward speeds up RL
algorithms on Atari games by reading manuals released by the Atari game
developers. Our framework consists of a QA Extraction module that extracts and
summarizes relevant information from the manual and a Reasoning module that
evaluates object-agent interactions based on information from the manual.
Auxiliary reward is then provided to a standard A2C RL agent, when interaction
is detected. When assisted by our design, A2C improves on 4 games in the Atari
environment with sparse rewards, and requires 1000x less training frames
compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04456">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been an increased popularity in image and speech
generation using diffusion models. However, directly generating music waveforms
from free-form text prompts is still under-explored. In this paper, we propose
the first text-to-waveform music generation model that can receive arbitrary
texts using diffusion models. We incorporate the free-form textual prompt as
the condition to guide the waveform generation process of diffusion models. To
solve the problem of lacking such text-music parallel data, we collect a
dataset of text-music pairs from the Internet with weak supervision. Besides,
we compare the effect of two prompt formats of conditioning texts (music tags
and free-form texts) and prove the superior performance of our method in terms
of text-music relevance. We further demonstrate that our generated music in the
waveform domain outperforms previous works by a large margin in terms of
diversity, quality, and text-music relevance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Constraints with Prompting for Zero-Shot Event Argument Classification. (arXiv:2302.04459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04459">
<div class="article-summary-box-inner">
<span><p>Determining the role of event arguments is a crucial subtask of event
extraction. Most previous supervised models leverage costly annotations, which
is not practical for open-domain applications. In this work, we propose to use
global constraints with prompting to effectively tackles event argument
classification without any annotation and task-specific training. Specifically,
given an event and its associated passage, the model first creates several new
passages by prefix prompts and cloze prompts, where prefix prompts indicate
event type and trigger span, and cloze prompts connect each candidate role with
the target argument span. Then, a pre-trained language model scores the new
passages, making the initial prediction. Our novel prompt templates can easily
adapt to all events and argument types without manual effort. Next, the model
regularizes the prediction by global constraints exploiting cross-task,
cross-argument, and cross-event relations. Extensive experiments demonstrate
our model's effectiveness: it outperforms the best zero-shot baselines by 12.5%
and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1,
respectively, without given argument spans. We have made our code publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04460">
<div class="article-summary-box-inner">
<span><p>With the advance of language models, privacy protection is receiving more
attention. Training data extraction is therefore of great importance, as it can
serve as a potential tool to assess privacy leakage. However, due to the
difficulty of this task, most of the existing methods are proof-of-concept and
still not effective enough. In this paper, we investigate and benchmark tricks
for improving training data extraction using a publicly available dataset.
Because most existing extraction methods use a pipeline of
generating-then-ranking, i.e., generating text candidates as potential training
data and then ranking them based on specific criteria, our research focuses on
the tricks for both text generation (e.g., sampling strategy) and text ranking
(e.g., token-level criteria). The experimental results show that several
previously overlooked tricks can be crucial to the success of training data
extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks
outperform the baseline by a large margin in most cases, providing a much
stronger baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Contextomized Quotes in News Headlines by Contrastive Learning. (arXiv:2302.04465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04465">
<div class="article-summary-box-inner">
<span><p>Quotes are critical for establishing credibility in news articles. A direct
quote enclosed in quotation marks has a strong visual appeal and is a sign of a
reliable citation. Unfortunately, this journalistic practice is not strictly
followed, and a quote in the headline is often "contextomized." Such a quote
uses words out of context in a way that alters the speaker's intention so that
there is no semantically matching quote in the body text. We present QuoteCSE,
a contrastive learning framework that represents the embedding of news quotes
based on domain-driven positive and negative samples to identify such an
editorial strategy. The dataset and code are available at
https://github.com/ssu-humane/contextomized-quote-contrastive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination. (arXiv:2302.04511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04511">
<div class="article-summary-box-inner">
<span><p>The Covid-19 pandemic had an enormous effect on our lives, especially on
people's interactions. By introducing Covid-19 vaccines, both positive and
negative opinions were raised over the subject of taking vaccines or not. In
this paper, using data gathered from Twitter, including tweets and user
profiles, we offer a comprehensive analysis of public opinion in Iran about the
Coronavirus vaccines. For this purpose, we applied a search query technique
combined with a topic modeling approach to extract vaccine-related tweets. We
utilized transformer-based models to classify the content of the tweets and
extract themes revolving around vaccination. We also conducted an emotion
analysis to evaluate the public happiness and anger around this topic. Our
results demonstrate that Covid-19 vaccination has attracted considerable
attention from different angles, such as governmental issues, safety or
hesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like
public vaccination and the rate of infection deeply impacted public emotional
status and users' interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Attention via Control Variates. (arXiv:2302.04542v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04542">
<div class="article-summary-box-inner">
<span><p>Random-feature-based attention (RFA) is an efficient approximation of softmax
attention with linear runtime and space complexity. However, the approximation
gap between RFA and conventional softmax attention is not well studied. Built
upon previous progress of RFA, we characterize this gap through the lens of
control variates and show that RFA can be decomposed into a sum of multiple
control variate estimators for each element in the sequence. This new framework
reveals that exact softmax attention can be recovered from RFA by manipulating
each control variate. Besides, it allows us to develop a more flexible form of
control variates, resulting in a novel attention mechanism that significantly
reduces the approximation gap while maintaining linear complexity. Extensive
experiments demonstrate that our model outperforms state-of-the-art efficient
attention mechanisms on both vision and language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Robust Character Detection in Fantasy Novels. (arXiv:2302.04555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04555">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a low-level task often used as a foundation
for solving higher level NLP problems. In the context of character detection in
novels, NER false negatives can be an issue as they possibly imply missing
certain characters or relationships completely. In this article, we demonstrate
that applying a straightforward data augmentation technique allows training a
model achieving higher recall, at the cost of a certain amount of precision
regarding ambiguous entities. We show that this decrease in precision can be
mitigated by giving the model more local context, which resolves some of the
ambiguities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP-based Decision Support System for Examination of Eligibility Criteria from Securities Prospectuses at the German Central Bank. (arXiv:2302.04562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04562">
<div class="article-summary-box-inner">
<span><p>As part of its digitization initiative, the German Central Bank (Deutsche
Bundesbank) wants to examine the extent to which natural Language Processing
(NLP) can be used to make independent decisions upon the eligibility criteria
of securities prospectuses. Every month, the Directorate General Markets at the
German Central Bank receives hundreds of scanned prospectuses in PDF format,
which must be manually processed to decide upon their eligibility. We found
that this tedious and time-consuming process can be (semi-)automated by
employing modern NLP model architectures, which learn the linguistic feature
representation in text to identify the present eligible and ineligible
criteria. The proposed Decision Support System provides decisions of
document-level eligibility criteria accompanied by human-understandable
explanations of the decisions. The aim of this project is to model the
described use case and to evaluate the extent to which current research results
from the field of NLP can be applied to this problem. After creating a
heterogeneous domain-specific dataset containing annotations of eligible and
non-eligible mentions of relevant criteria, we were able to successfully build,
train and deploy a semi-automatic decider model. This model is based on
transformer-based language models and decision trees, which integrate the
established rule-based parts of the decision processes. Results suggest that it
is possible to efficiently model the problem and automate decision making to
more than 90% for many of the considered eligibility criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating a Structured Summary of Numerous Academic Papers: Dataset and Method. (arXiv:2302.04580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04580">
<div class="article-summary-box-inner">
<span><p>Writing a survey paper on one research topic usually needs to cover the
salient content from numerous related papers, which can be modeled as a
multi-document summarization (MDS) task. Existing MDS datasets usually focus on
producing the structureless summary covering a few input documents. Meanwhile,
previous structured summary generation works focus on summarizing a single
document into a multi-section summary. These existing datasets and methods
cannot meet the requirements of summarizing numerous academic papers into a
structured summary. To deal with the scarcity of available data, we propose
BigSurvey, the first large-scale dataset for generating comprehensive summaries
of numerous academic papers on each topic. We collect target summaries from
more than seven thousand survey papers and utilize their 430 thousand reference
papers' abstracts as input documents. To organize the diverse content from
dozens of input documents and ensure the efficiency of processing long text
sequences, we propose a summarization method named category-based alignment and
sparse transformer (CAST). The experimental results show that our CAST method
outperforms various advanced summarization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study. (arXiv:2302.04618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04618">
<div class="article-summary-box-inner">
<span><p>A deployed question answering (QA) model can easily fail when the test data
has a distribution shift compared to the training data. Robustness tuning (RT)
methods have been widely studied to enhance model robustness against
distribution shifts before model deployment. However, can we improve a model
after deployment? To answer this question, we evaluate test-time adaptation
(TTA) to improve a model after deployment. We first introduce COLDQA, a unified
evaluation benchmark for robust QA against text corruption and changes in
language and domain. We then evaluate previous TTA methods on COLDQA and
compare them to RT methods. We also propose a novel TTA method called online
imitation learning (OIL). Through extensive experiments, we find that TTA is
comparable to RT methods, and applying TTA after RT can significantly boost the
performance on COLDQA. Our proposed OIL improves TTA to be more robust to
variation in hyper-parameters and test distributions over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Approach for Auto-Formulation of Optimization Problems. (arXiv:2302.04643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04643">
<div class="article-summary-box-inner">
<span><p>In the Natural Language for Optimization (NL4Opt) NeurIPS 2022 competition,
competitors focus on improving the accessibility and usability of optimization
solvers, with the aim of subtask 1: recognizing the semantic entities that
correspond to the components of the optimization problem; subtask 2: generating
formulations for the optimization problem. In this paper, we present the
solution of our team. First, we treat subtask 1 as a named entity recognition
(NER) problem with the solution pipeline including pre-processing methods,
adversarial training, post-processing methods and ensemble learning. Besides,
we treat subtask 2 as a generation problem with the solution pipeline including
specially designed prompts, adversarial training, post-processing methods and
ensemble learning. Our proposed methods have achieved the F1-score of 0.931 in
subtask 1 and the accuracy of 0.867 in subtask 2, which won the fourth and
third places respectively in this competition. Our code is available at
https://github.com/bigdata-ustc/nl4opt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04662">
<div class="article-summary-box-inner">
<span><p>Large language models trained on code (LLMCs), such as Codex, hold great
promise in enhancing programming education by automatically generating feedback
for students. We investigate using LLMCs to generate feedback for fixing syntax
errors in Python programs, a key scenario in introductory programming. More
concretely, given a student's buggy program, our goal is to generate feedback
comprising a fixed program along with a natural language explanation describing
the errors/fixes, inspired by how a human tutor would give feedback. While
using LLMCs is promising, the critical challenge is to ensure high precision in
the generated feedback, which is imperative before deploying such technology in
classrooms. The main research question we study is: Can we develop LLMCs-based
feedback generation techniques with a tunable precision parameter, giving
educators quality control over the feedback that students receive? To this end,
we introduce PyFiXV, our technique to generate high-precision feedback powered
by Codex. The key idea behind PyFiXV is to use a novel run-time validation
mechanism to decide whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also provides a precision knob
to educators. We perform an extensive evaluation using two real-world datasets
of Python programs with syntax errors and show the efficacy of PyFiXV in
generating high-precision feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting NLP data to counter Annotation Artifacts for NLI Tasks. (arXiv:2302.04700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04700">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore Annotation Artifacts - the phenomena wherein large
pre-trained NLP models achieve high performance on benchmark datasets but do
not actually "solve" the underlying task and instead rely on some dataset
artifacts (same across train, validation, and test sets) to figure out the
right answer. We explore this phenomenon on the well-known Natural Language
Inference task by first using contrast and adversarial examples to understand
limitations to the model's performance and show one of the biases arising from
annotation artifacts (the way training data was constructed by the annotators).
We then propose a data augmentation technique to fix this bias and measure its
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Generalizability of Collaborative Dialogue Analysis with Multi-Feature Embeddings. (arXiv:2302.04716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04716">
<div class="article-summary-box-inner">
<span><p>Conflict prediction in communication is integral to the design of virtual
agents that support successful teamwork by providing timely assistance. The aim
of our research is to analyze discourse to predict collaboration success.
Unfortunately, resource scarcity is a problem that teamwork researchers
commonly face since it is hard to gather a large number of training examples.
To alleviate this problem, this paper introduces a multi-feature embedding
(MFeEmb) that improves the generalizability of conflict prediction models
trained on dialogue sequences. MFeEmb leverages textual, structural, and
semantic information from the dialogues by incorporating lexical, dialogue
acts, and sentiment features. The use of dialogue acts and sentiment features
reduces performance loss from natural distribution shifts caused mainly by
changes in vocabulary.
</p>
<p>This paper demonstrates the performance of MFeEmb on domain adaptation
problems in which the model is trained on discourse from one task domain and
applied to predict team performance in a different domain. The generalizability
of MFeEmb is quantified using the similarity measure proposed by Bontonou et
al. (2021). Our results show that MFeEmb serves as an excellent domain-agnostic
representation for meta-pretraining a few-shot model on collaborative
multiparty dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Transformers for Clinical Natural Language Processing. (arXiv:2302.04725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04725">
<div class="article-summary-box-inner">
<span><p>Specialised pre-trained language models are becoming more frequent in NLP
since they can potentially outperform models trained on generic texts. BioBERT
and BioClinicalBERT are two examples of such models that have shown promise in
medical NLP tasks. Many of these models are overparametrised and
resource-intensive, but thanks to techniques like Knowledge Distillation (KD),
it is possible to create smaller versions that perform almost as well as their
larger counterparts. In this work, we specifically focus on development of
compact language models for processing clinical texts (i.e. progress notes,
discharge summaries etc). We developed a number of efficient lightweight
clinical transformers using knowledge distillation and continual learning, with
the number of parameters ranging from 15 million to 65 million. These models
performed comparably to larger models such as BioBERT and ClinicalBioBERT and
significantly outperformed other compact models trained on general or
biomedical data. Our extensive evaluation was done across several standard
datasets and covered a wide range of clinical text-mining tasks, including
Natural Language Inference, Relation Extraction, Named Entity Recognition, and
Sequence Classification. To our knowledge, this is the first comprehensive
study specifically focused on creating efficient and compact transformers for
clinical NLP tasks. The models and code used in this study can be found on our
Huggingface profile at https://huggingface.co/nlpie and Github page at
https://github.com/nlpie-research/Lightweight-Clinical-Transformers,
respectively, promoting reproducibility of our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toolformer: Language Models Can Teach Themselves to Use Tools. (arXiv:2302.04761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04761">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&amp;A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource Indian Languages. (arXiv:2302.04790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04790">
<div class="article-summary-box-inner">
<span><p>Massive knowledge graphs like Wikidata attempt to capture world knowledge
about multiple entities. Recent approaches concentrate on automatically
enriching these KGs from text. However a lot of information present in the form
of natural text in low resource languages is often missed out. Cross Lingual
Information Extraction aims at extracting factual information in the form of
English triples from low resource Indian Language text. Despite its massive
potential, progress made on this task is lagging when compared to Monolingual
Information Extraction. In this paper, we propose the task of Cross Lingual
Fact Extraction(CLFE) from text and devise an end-to-end generative approach
for the same which achieves an overall F1 score of 77.46.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Multilingual Study of Visual Constraints on Linguistic Selection of Descriptions. (arXiv:2302.04811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04811">
<div class="article-summary-box-inner">
<span><p>We present a large, multilingual study into how vision constrains linguistic
choice, covering four languages and five linguistic properties, such as verb
transitivity or use of numerals. We propose a novel method that leverages
existing corpora of images with captions written by native speakers, and apply
it to nine corpora, comprising 600k images and 3M captions. We study the
relation between visual input and linguistic choices by training classifiers to
predict the probability of expressing a property from raw images, and find
evidence supporting the claim that linguistic properties are constrained by
visual context across languages. We complement this investigation with a corpus
study, taking the test case of numerals. Specifically, we use existing
annotations (number or type of objects) to investigate the effect of different
visual conditions on the use of numeral expressions in captions, and show that
similar patterns emerge across languages. Our methods and findings both confirm
and extend existing research in the cognitive literature. We additionally
discuss possible applications for language generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanation Selection Using Unlabeled Data for In-Context Learning. (arXiv:2302.04813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04813">
<div class="article-summary-box-inner">
<span><p>Recent work has addressed textual reasoning tasks by prompting large language
models with explanations via the chain-of-thought paradigm. However, subtly
different explanations can yield widely varying downstream task accuracy, so
explanations that have not been "tuned" for a task, such as off-the-shelf
explanations written by nonexperts, may lead to mediocre performance. This
paper tackles the problem of how to optimize explanation-infused prompts in a
black-box fashion. We first generate sets of candidate explanations for each
example in the prompt using a leave-one-out scheme. We then use a two-stage
framework where we first evaluate explanations for each in-context example in
isolation according to proxy metrics. Finally, we search over sets of
explanations to find a set which yields high performance against a
silver-labeled development set, drawing inspiration from recent work on
bootstrapping language models on unlabeled data. Across four textual reasoning
tasks spanning question answering, mathematical reasoning, and natural language
inference, results show that our proxy metrics correlate with ground truth
accuracy and our overall method can effectively improve prompts over
crowdworker annotations and naive search strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Cognitive Dynamics of Artificial Intelligence in the Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT. (arXiv:2302.04818v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04818">
<div class="article-summary-box-inner">
<span><p>The emergence of artificial intelligence has incited a paradigm shift across
the spectrum of human endeavors, with ChatGPT serving as a catalyst for the
transformation of various established domains, including but not limited to
education, journalism, security, and ethics. In the post-pandemic era, the
widespread adoption of remote work has prompted the educational sector to
reassess conventional pedagogical methods. This paper is to scrutinize the
underlying psychological principles of ChatGPT, delve into the factors that
captivate user attention, and implicate its ramifications on the future of
learning. The ultimate objective of this study is to instigate a scholarly
discourse on the interplay between technological advancements in education and
the evolution of human learning patterns, raising the question of whether
technology is driving human evolution or vice versa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning. (arXiv:2302.04834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04834">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04858">
<div class="article-summary-box-inner">
<span><p>Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained state-of-the-art results in image-to-text generation.
However, these models store all the knowledge within their parameters, thus
often requiring enormous model parameters to model the abundant visual concepts
and very rich textual descriptions. Additionally, they are inefficient in
incorporating new data, requiring a computational-expensive fine-tuning
process. In this work, we introduce a Retrieval-augmented Visual Language
Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant
knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04863">
<div class="article-summary-box-inner">
<span><p>Research on neural networks has largely focused on understanding a single
model trained on a single dataset. However, relatively little is known about
the relationships between different models, especially those trained or tested
on different datasets. We address this by studying how the weight space and
underlying loss landscape of different models are interconnected.
</p>
<p>Specifically, we demonstrate that fine-tuned models that were optimized for
high performance, reside in well-defined regions in weight space, and vice
versa -- that any model that resides anywhere in those regions also has high
performance. Specifically, we show that language models that have been
fine-tuned on the same dataset form a tight cluster in the weight space and
that models fine-tuned on different datasets from the same underlying task form
a looser cluster. Moreover, traversing around the region between the models
reaches new models that perform comparably or even better than models found via
fine-tuning, even on tasks that the original models were not fine-tuned on.
</p>
<p>Our findings provide insight into the relationships between models,
demonstrating that a model positioned between two similar models can acquire
the knowledge of both. We leverage this finding and design a method to pick a
better model for efficient fine-tuning. Specifically, we show that starting
from the center of the region is as good or better than the pre-trained model
in 11 of 12 datasets and improves accuracy by 3.06 on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning by Asking for Embodied Visual Navigation and Task Completion. (arXiv:2302.04865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04865">
<div class="article-summary-box-inner">
<span><p>The research community has shown increasing interest in designing intelligent
embodied agents that can assist humans in accomplishing tasks. Despite recent
progress on related vision-language benchmarks, most prior work has focused on
building agents that follow instructions rather than endowing agents the
ability to ask questions to actively resolve ambiguities arising naturally in
embodied environments. To empower embodied agents with the ability to interact
with humans, in this work, we propose an Embodied Learning-By-Asking (ELBA)
model that learns when and what questions to ask to dynamically acquire
additional information for completing the task. We evaluate our model on the
TEACH vision-dialog navigation and task completion dataset. Experimental
results show that ELBA achieves improved task performance compared to baseline
models without question-answering capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offsite-Tuning: Transfer Learning without Full Model. (arXiv:2302.04870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04870">
<div class="article-summary-box-inner">
<span><p>Transfer learning is important for foundation models to adapt to downstream
tasks. However, many foundation models are proprietary, so users must share
their data with model owners to fine-tune the models, which is costly and raise
privacy concerns. Moreover, fine-tuning large foundation models is
computation-intensive and impractical for most downstream users. In this paper,
we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning
framework that can adapt billion-parameter foundation models to downstream data
without access to the full model. In offsite-tuning, the model owner sends a
light-weight adapter and a lossy compressed emulator to the data owner, who
then fine-tunes the adapter on the downstream data with the emulator's
assistance. The fine-tuned adapter is then returned to the model owner, who
plugs it into the full model to create an adapted foundation model.
Offsite-tuning preserves both parties' privacy and is computationally more
efficient than the existing fine-tuning methods that require access to the full
model weights. We demonstrate the effectiveness of offsite-tuning on various
large language and vision foundation models. Offsite-tuning can achieve
comparable accuracy as full model fine-tuning while being privacy-preserving
and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is
available at https://github.com/mit-han-lab/offsite-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From How Humans Correct. (arXiv:2102.00225v13 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and re-label
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we re-label the
noisy data in our dataset for our industry application. The experiment result
shows that our method improve the classification accuracy from 91.7% to 92.5%.
The 91.7% accuracy is trained on the corrected dataset, which improve the
baseline from 83.3% to 91.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04845">
<div class="article-summary-box-inner">
<span><p>The degree of semantic relatedness of two units of language has long been
considered fundamental to understanding meaning. Additionally, automatically
determining relatedness has many applications such as question answering and
summarization. However, prior NLP work has largely focused on semantic
similarity, a subset of relatedness, because of a lack of relatedness datasets.
In this paper, we introduce a dataset for Semantic Textual Relatedness,
STR-2022, that has 5,500 English sentence pairs manually annotated using a
comparative annotation framework, resulting in fine-grained scores. We show
that human intuition regarding relatedness of sentence pairs is highly
reliable, with a repeat annotation correlation of 0.84. We use the dataset to
explore questions on what makes sentences semantically related. We also show
the utility of STR-2022 for evaluating automatic methods of sentence
representation and for various downstream NLP tasks.
</p>
<p>Our dataset, data statement, and annotation questionnaire can be found at:
https://doi.org/10.5281/zenodo.7599667
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07560">
<div class="article-summary-box-inner">
<span><p>Fine-tuning the entire set of parameters of a large pretrained model has
become the mainstream approach for transfer learning. To increase its
efficiency and prevent catastrophic forgetting and interference, techniques
like adapters and sparse fine-tuning have been developed. Adapters are modular,
as they can be combined to adapt a model towards different facets of knowledge
(e.g., dedicated language and/or task adapters). Sparse fine-tuning is
expressive, as it controls the behavior of all model components. In this work,
we introduce a new fine-tuning method with both these desirable properties. In
particular, we learn sparse, real-valued masks based on a simple variant of the
Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data
in a source language, and language-specific masks from masked language modeling
in a target language. Both these masks can then be composed with the pretrained
model. Unlike adapter-based fine-tuning, this method neither increases the
number of parameters at inference time nor alters the original model
architecture. Most importantly, it outperforms adapters in zero-shot
cross-lingual transfer by a large margin in a series of multilingual
benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI.
Based on an in-depth analysis, we additionally find that sparsity is crucial to
prevent both 1) interference between the fine-tunings to be composed and 2)
overfitting. We release the code and models at
https://github.com/cambridgeltl/composable-sft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Latent-Variable Model for Intrinsic Probing. (arXiv:2201.08214v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08214">
<div class="article-summary-box-inner">
<span><p>The success of pre-trained contextualized representations has prompted
researchers to analyze them for the presence of linguistic information. Indeed,
it is natural to assume that these pre-trained representations do encode some
level of linguistic knowledge as they have brought about large empirical
improvements on a wide variety of NLP tasks, which suggests they are learning
true linguistic generalization. In this work, we focus on intrinsic probing, an
analysis technique where the goal is not only to identify whether a
representation encodes a linguistic attribute but also to pinpoint where this
attribute is encoded. We propose a novel latent-variable formulation for
constructing intrinsic probes and derive a tractable variational approximation
to the log-likelihood. Our results show that our model is versatile and yields
tighter mutual information estimates than two intrinsic probes previously
proposed in the literature. Finally, we find empirical evidence that
pre-trained representations develop a cross-lingually entangled notion of
morphosyntax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Spatial Reasoning. (arXiv:2205.00363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00363">
<div class="article-summary-box-inner">
<span><p>Spatial relations are a basic part of human cognition. However, they are
expressed in natural language in a variety of ways, and previous work has
suggested that current vision-and-language models (VLMs) struggle to capture
relational information. In this paper, we present Visual Spatial Reasoning
(VSR), a dataset containing more than 10k natural text-image pairs with 65
types of spatial relations in English (such as: under, in front of, and
facing). While using a seemingly simple annotation format, we show how the
dataset includes challenging linguistic phenomena, such as varying reference
frames. We demonstrate a large gap between human and model performance: the
human ceiling is above 95%, while state-of-the-art models only achieve around
70%. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations concerning the orientations of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Performance Disparities Between English-Language Accents in Automatic Speech Recognition. (arXiv:2208.01157v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01157">
<div class="article-summary-box-inner">
<span><p>Past research has identified discriminatory automatic speech recognition
(ASR) performance as a function of the racial group and nationality of the
speaker. In this paper, we expand the discussion beyond bias as a function of
the individual national origin of the speaker to look for bias as a function of
the geopolitical orientation of their nation of origin. We audit some of the
most popular English language ASR services using a large and global data set of
speech from The Speech Accent Archive, which includes over 2,700 speakers of
English born in 171 different countries. We show that, even when controlling
for multiple linguistic covariates, ASR service performance has a statistically
significant relationship to the political alignment of the speaker's birth
country with respect to the United States' geopolitical power. This holds for
all ASR services tested. We discuss this bias in the context of the historical
use of language to maintain global and political power.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Approaches to Multilingual Information Retrieval. (arXiv:2209.01335v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.01335">
<div class="article-summary-box-inner">
<span><p>Providing access to information across languages has been a goal of
Information Retrieval (IR) for decades. While progress has been made on Cross
Language IR (CLIR) where queries are expressed in one language and documents in
another, the multilingual (MLIR) task to create a single ranked list of
documents across many languages is considerably more challenging. This paper
investigates whether advances in neural document translation and pretrained
multilingual neural language models enable improvements in the state of the art
over earlier MLIR techniques. The results show that although combining neural
document translation with neural ranking yields the best Mean Average Precision
(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing
time by using a pretrained XLM-R multilingual language model to index documents
in their native language, and that 2% difference in effectiveness is not
statistically significant. Key to achieving these results for MLIR is to
fine-tune XLM-R using mixed-language batches from neural translations of MS
MARCO passages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models. (arXiv:2210.01963v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01963">
<div class="article-summary-box-inner">
<span><p>A characteristic feature of human semantic cognition is its ability to not
only store and retrieve the properties of concepts observed through experience,
but to also facilitate the inheritance of properties (can breathe) from
superordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate
property inheritance. In this paper, we present COMPS, a collection of minimal
pair sentences that jointly tests pre-trained language models (PLMs) on their
ability to attribute properties to concepts and their ability to demonstrate
property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal
that they can easily distinguish between concepts on the basis of a property
when they are trivially different, but find it relatively difficult when
concepts are related on the basis of nuanced knowledge representations.
Furthermore, we find that PLMs can demonstrate behavior consistent with
property inheritance to a great extent, but fail in the presence of distracting
information, which decreases the performance of many models, sometimes even
below chance. This lack of robustness in demonstrating simple reasoning raises
important questions about PLMs' capacity to make correct inferences even when
they appear to possess the prerequisite knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Pre-Training by Reducing Representation Confusion. (arXiv:2210.04246v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04246">
<div class="article-summary-box-inner">
<span><p>In this work, we revisit the Transformer-based pre-trained language models
and identify two different types of information confusion in position encoding
and model representations, respectively. Firstly, we show that in the relative
position encoding, the joint modeling about relative distances and directions
brings confusion between two heterogeneous information. It may make the model
unable to capture the associative semantics of the same distance and the
opposite directions, which in turn affects the performance of downstream tasks.
Secondly, we notice the BERT with Mask Language Modeling (MLM) pre-training
objective outputs similar token representations (last hidden states of
different tokens) and head representations (attention weights of different
heads), which may make the diversity of information expressed by different
tokens and heads limited. Motivated by the above investigation, we propose two
novel techniques to improve pre-trained language models: Decoupled Directional
Relative Position (DDRP) encoding and MTH pre-training objective. DDRP
decouples the relative distance features and the directional features in
classical relative position encoding. MTH applies two novel auxiliary
regularizers besides MLM to enlarge the dissimilarities between (a) last hidden
states of different tokens, and (b) attention weights of different heads. These
designs allow the model to capture different categories of information more
clearly, as a way to alleviate information confusion in representation learning
for better optimization. Extensive experiments and ablation studies on GLUE
benchmark demonstrate the effectiveness of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best Practices in the Creation and Use of Emotion Lexicons. (arXiv:2210.07206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07206">
<div class="article-summary-box-inner">
<span><p>Words play a central role in how we express ourselves. Lexicons of
word-emotion associations are widely used in research and real-world
applications for sentiment analysis, tracking emotions associated with products
and policies, studying health disorders, tracking emotional arcs of stories,
and so on. However, inappropriate and incorrect use of these lexicons can lead
to not just sub-optimal results, but also inferences that are directly harmful
to people. This paper brings together ideas from Affective Computing and AI
Ethics to present, some of the practical and ethical considerations involved in
the creation and use of emotion lexicons -- best practices. The goal is to
provide a comprehensive set of relevant considerations, so that readers
(especially those new to work with emotions) can find relevant information in
one place. We hope this work will facilitate more thoughtfulness when one is
deciding on what emotions to work on, how to create an emotion lexicon, how to
use an emotion lexicon, how to draw meaningful inferences, and how to judge
success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPU-MLIR: A Compiler For TPU Using MLIR. (arXiv:2210.15016v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15016">
<div class="article-summary-box-inner">
<span><p>Multi-level intermediate representations (MLIR) show great promise for
reducing the cost of building domain-specific compilers by providing a reusable
and extensible compiler infrastructure. This work presents TPU-MLIR, an
end-to-end compiler based on MLIR that deploys pre-trained neural network (NN)
models to a custom ASIC called a Tensor Processing Unit (TPU). TPU-MLIR defines
two new dialects to implement its functionality: 1. a Tensor operation (TOP)
dialect that encodes the deep learning graph semantics and independent of the
deep learning framework and 2. a TPU kernel dialect to provide a standard
kernel computation on TPU. A NN model is translated to the TOP dialect and then
lowered to the TPU dialect for different TPUs according to the chip's
configuration. We demonstrate how to use the MLIR pass pipeline to organize and
perform optimization on TPU to generate machine code. The paper also presents a
verification procedure to ensure the correctness of each transform stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Average Token Delay: A Latency Metric for Simultaneous Translation. (arXiv:2211.13173v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13173">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation is a task in which translation begins before the
speaker has finished speaking. In its evaluation, we have to consider the
latency of the translation in addition to the quality. The latency is
preferably as small as possible for users to comprehend what the speaker says
with a small delay. Existing latency metrics focus on when the translation
starts but do not consider adequately when the translation ends. This means
such metrics do not penalize the latency caused by a long translation output,
which actually delays users' comprehension. In this work, we propose a novel
latency evaluation metric called Average Token Delay (ATD) that focuses on the
end timings of partial translations in simultaneous translation. We discuss the
advantage of ATD using simulated examples and also investigate the differences
between ATD and Average Lagging with simultaneous translation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement. (arXiv:2212.05970v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05970">
<div class="article-summary-box-inner">
<span><p>Can we take a recurrent neural network (RNN) trained to translate between
languages and augment it to support a new natural language without retraining
the model from scratch? Can we fix the faulty behavior of the RNN by replacing
portions associated with the faulty behavior? Recent works on decomposing a
fully connected neural network (FCNN) and convolutional neural network (CNN)
into modules have shown the value of engineering deep models in this manner,
which is standard in traditional SE but foreign for deep learning models.
However, prior works focus on the image-based multiclass classification
problems and cannot be applied to RNN due to (a) different layer structures,
(b) loop structures, (c) different types of input-output architectures, and (d)
usage of both nonlinear and logistic activation functions. In this work, we
propose the first approach to decompose an RNN into modules. We study different
types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN
modules can be reused and replaced in various scenarios. We evaluate our
approach against 5 canonical datasets (i.e., Math QA, Brown Corpus,
Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.
We found that decomposing a trained model has a small cost (Accuracy: -0.6%,
BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced
without needing to retrain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01015">
<div class="article-summary-box-inner">
<span><p>In this paper we explore the task of modeling (semi) structured object
sequences; in particular we focus our attention on the problem of developing a
structure-aware input representation for such sequences. In such sequences, we
assume that each structured object is represented by a set of key-value pairs
which encode the attributes of the structured object. Given a universe of keys,
a sequence of structured objects can then be viewed as an evolution of the
values for each key, over time. We encode and construct a sequential
representation using the values for a particular key (Temporal Value Modeling -
TVM) and then self-attend over the set of key-conditioned value sequences to a
create a representation of the structured object sequence (Key Aggregation -
KA). We pre-train and fine-tune the two components independently and present an
innovative training schedule that interleaves the training of both modules with
shared attention heads. We find that this iterative two part-training results
in better performance than a unified network with hierarchical encoding as well
as over, other methods that use a {\em record-view} representation of the
sequence \cite{de2021transformers4rec} or a simple {\em flattened}
representation of the sequence. We conduct experiments using real-world data to
demonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed
ablation studies motivating our modeling choices. We find that our approach
performs better than flattening sequence objects and also allows us to operate
on significantly larger sequences than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Questions for Zero-Shot Relation Extraction. (arXiv:2301.09640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09640">
<div class="article-summary-box-inner">
<span><p>Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where
the training and test sets have no shared relation types. This very challenging
domain is a good test of a model's ability to generalize. Previous approaches
to ZRE reframed relation extraction as Question Answering (QA), allowing for
the use of pre-trained QA models. However, this method required manually
creating gold question templates for each new relation. Here, we do away with
these gold templates and instead learn a model that can generate questions for
unseen relations. Our technique can successfully translate relation
descriptions into relevant questions, which are then leveraged to generate the
correct tail entity. On tail entity extraction, we outperform the previous
state-of-the-art by more than 16 F1 points without using gold question
templates. On the RE-QA dataset where no previous baseline for relation
extraction exists, our proposed algorithm comes within 0.7 F1 points of a
system that uses gold question templates. Our model also outperforms the
state-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that
QA models no longer need template questions to match the performance of models
specifically tailored to the ZRE task. Our implementation is available at
https://github.com/fyshelab/QA-ZRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Chain-of-Thought Reasoning in Language Models. (arXiv:2302.00923v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00923">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies are mostly isolated in the language modality with LLMs,
where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a
possible solution is to fine-tune small language models by fusing the vision
and language features to perform CoT reasoning. The key challenge is that those
language models tend to generate hallucinated reasoning chains that mislead the
answer inference. To mitigate the effect of such mistakes, we propose
Multimodal-CoT that incorporates vision features. The framework separates the
rationale generation and answer inference into two stages. By incorporating the
vision features in both stages, the model is able to generate effective
rationales that contribute to answer inference. With Multimodal-CoT, our model
under 1 billion parameters outperforms the previous state-of-the-art LLM
(GPT-3.5) by 16% (75.17%-&gt;91.68%) on the ScienceQA benchmark and even surpasses
human performance. Code is publicly available at
https://github.com/amazon-science/mm-cot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many and Which Training Points Would Need to be Removed to Flip this Prediction?. (arXiv:2302.02169v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02169">
<div class="article-summary-box-inner">
<span><p>We consider the problem of identifying a minimal subset of training data
$\mathcal{S}_t$ such that if the instances comprising $\mathcal{S}_t$ had been
removed prior to training, the categorization of a given test point $x_t$ would
have been different. Identifying such a set may be of interest for a few
reasons. First, the cardinality of $\mathcal{S}_t$ provides a measure of
robustness (if $|\mathcal{S}_t|$ is small for $x_t$, we might be less confident
in the corresponding prediction), which we show is correlated with but
complementary to predicted probabilities. Second, interrogation of
$\mathcal{S}_t$ may provide a novel mechanism for contesting a particular model
prediction: If one can make the case that the points in $\mathcal{S}_t$ are
wrongly labeled or irrelevant, this may argue for overturning the associated
prediction. Identifying $\mathcal{S}_t$ via brute-force is intractable. We
propose comparatively fast approximation methods to find $\mathcal{S}_t$ based
on influence functions, and find that -- for simple convex text classification
models -- these approaches can often successfully identify relatively small
sets of training examples which, if removed, would flip the prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Benefits of Training Expert Language Models over Instruction Tuning. (arXiv:2302.03202v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03202">
<div class="article-summary-box-inner">
<span><p>Recently, Language Models (LMs) instruction-tuned on multiple tasks, also
known as multitask-prompted fine-tuning (MT), have shown the capability to
generalize to unseen tasks. Previous work has shown that scaling the number of
training tasks is the key component in making stronger MT LMs. In this work, we
report an unexpected finding that an expert LM fine-tuned on just a single task
can outperform an MT LM trained with 300+ different tasks on 11 different
unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean
accuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the
previously held belief that simply scaling the number of tasks makes stronger
MT LMs. Leveraging this finding, we further show that this distributed approach
of training a separate expert LM per training task instead of a single MT LM
for zero-shot inference possesses many benefits including (1) avoiding negative
task transfer that often occurs during instruction tuning, (2) being able to
continually learn new tasks without having to re-train on previous tasks to
avoid catastrophic forgetting, and (3) showing compositional capabilities when
merging individual experts together. The code is available at
https://github.com/joeljang/ELM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Learning: An Adversarial Process of Two Pre-trained Models for Natural Language Generation. (arXiv:2302.03896v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03896">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have been used in many fields in recent years, ranging
from natural language understanding to computer vision and natural language
generation. Nowadays, the performance of these natural language generation
models is overly dependent on the model's scale and the dataset's size. While
the larger language model is excellent in some respects, it cannot learn
up-to-date knowledge and is relatively difficult to relearn. In this paper, a
new adversarial process learning method is called Auto-Learning, which can
improve the performance of any natural language generation model without the
help of additional datasets. Auto-Learning includes two models: $G$ is a text
generation model, and $D$ can test whether the data generated by G is
legitimate. Firstly, the fine-tuned $D$ model is used as the brain's knowledge
base before the process. Then the text generated by the $G$ model is used as
the input of $D$ to determine whether the text is legitimate. Finally, $G$ is
fine-tuned according to the output of $D$. This adversarial process is like a
self-escalation of the brain through some a priori knowledge. When this
adversarial system wants to learn something new, simply fine-tune the $D$
model. Our approach applies to Autoregressive Language Modeling for all
Transformer classes. Auto-Learning enables 8 models to achieve stable
improvement in 10 natural language processing tasks without any change in
structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEBERT: Efficient and robust binary ensemble BERT. (arXiv:2210.15976v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15976">
<div class="article-summary-box-inner">
<span><p>Pre-trained BERT models have achieved impressive accuracy on natural language
processing (NLP) tasks. However, their excessive amount of parameters hinders
them from efficient deployment on edge devices. Binarization of the BERT models
can significantly alleviate this issue but comes with a severe accuracy drop
compared with their full-precision counterparts. In this paper, we propose an
efficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.
To the best of our knowledge, this is the first work employing ensemble
techniques on binary BERTs, yielding BEBERT, which achieves superior accuracy
while retaining computational efficiency. Furthermore, we remove the knowledge
distillation procedures during ensemble to speed up the training process
without compromising accuracy. Experimental results on the GLUE benchmark show
that the proposed BEBERT significantly outperforms the existing binary BERT
models in accuracy and robustness with a 2x speedup on training time. Moreover,
our BEBERT has only a negligible accuracy loss of 0.3% compared to the
full-precision baseline while saving 15x and 13x in FLOPs and model size,
respectively. In addition, BEBERT also outperforms other compressed BERTs in
accuracy by up to 6.7%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-11 23:12:03.037682079 UTC">2023-02-11 23:12:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>