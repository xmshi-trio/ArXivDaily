<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-19T01:30:00Z">10-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11465">
<div class="article-summary-box-inner">
<span><p>This study presents a large multi-modal Bangla YouTube clickbait dataset
consisting of 253,070 data points collected through an automated process using
the YouTube API and Python web automation frameworks. The dataset contains 18
diverse features categorized into metadata, primary content, engagement
statistics, and labels for individual videos from 58 Bangla YouTube channels. A
rigorous preprocessing step has been applied to denoise, deduplicate, and
remove bias from the features, ensuring unbiased and reliable analysis. As the
largest and most robust clickbait corpus in Bangla to date, this dataset
provides significant value for natural language processing and data science
researchers seeking to advance modeling of clickbait phenomena in low-resource
languages. Its multi-modal nature allows for comprehensive analyses of
clickbait across content, user interactions, and linguistic dimensions to
develop more sophisticated detection methods with cross-linguistic
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. (arXiv:2310.11501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11501">
<div class="article-summary-box-inner">
<span><p>Recent work has aimed to capture nuances of human behavior by using LLMs to
simulate responses from particular demographics in settings like social science
experiments and public opinion surveys. However, there are currently no
established ways to discuss or evaluate the quality of such LLM simulations.
Moreover, there is growing concern that these LLM simulations are flattened
caricatures of the personas that they aim to simulate, failing to capture the
multidimensionality of people and perpetuating stereotypes. To bridge these
gaps, we present CoMPosT, a framework to characterize LLM simulations using
four dimensions: Context, Model, Persona, and Topic. We use this framework to
measure open-ended LLM simulations' susceptibility to caricature, defined via
two criteria: individuation and exaggeration. We evaluate the level of
caricature in scenarios from existing work on LLM simulations. We find that for
GPT-4, simulations of certain demographics (political and marginalized groups)
and topics (general, uncontroversial) are highly susceptible to caricature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11511">
<div class="article-summary-box-inner">
<span><p>Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11520">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing is booming with its applications in the real
world, one of which is Text Summarization for large texts including news
articles. This research paper provides an extensive comparative evaluation of
extractive and abstractive approaches for news text summarization, with an
emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail
dataset, which consists of news articles and human-generated reference
summaries. The evaluation employs ROUGE scores to assess the efficacy and
quality of generated summaries. After Evaluation, we integrate the
best-performing models on a web application to assess their real-world
capabilities and user experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11523">
<div class="article-summary-box-inner">
<span><p>Many applications of large language models (LLMs), ranging from chatbots to
creative writing, require nuanced subjective judgments that can differ
significantly across different groups. Existing alignment algorithms can be
expensive to align for each group, requiring prohibitive amounts of
group-specific preference data and computation for real-world use cases. We
introduce Group Preference Optimization (GPO), an alignment framework that
steers language models to preferences of individual groups in a few-shot
manner. In GPO, we augment the base LLM with an independent transformer module
trained to predict the preferences of a group for the LLM generations. For
few-shot learning, we parameterize this module as an in-context autoregressive
transformer and train it via meta-learning on several groups. We empirically
validate the efficacy of GPO through rigorous evaluations using LLMs with
varied sizes on three human opinion adaptation tasks. These tasks involve
adapting to the preferences of US demographic groups, global countries, and
individual users. Our results demonstrate that GPO not only aligns models more
accurately but also requires fewer group-specific preferences, and less
training and inference computing resources, outperforming existing strategies
such as in-context steering and fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11532">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the usage of large language models (LLMs) to
improve the performance of competitive speech recognition systems. Different
from traditional language models that focus on one single data domain, the rise
of LLMs brings us the opportunity to push the limit of state-of-the-art ASR
performance, and at the same time to achieve higher robustness and generalize
effectively across multiple domains. Motivated by this, we propose a novel
multi-stage approach to combine traditional language model re-scoring and LLM
prompting. Specifically, the proposed method has two stages: the first stage
uses a language model to re-score an N-best list of ASR hypotheses and run a
confidence check; The second stage uses prompts to a LLM to perform ASR error
correction on less confident results from the first stage. Our experimental
results demonstrate the effectiveness of the proposed method by showing a 10% ~
20% relative improvement in WER over a competitive ASR system -- across
multiple test domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11541">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a methodology for linguistic feature extraction,
focusing particularly on automatically syllabifying words in multiple
languages, with a design to be compatible with a forced-alignment tool, the
Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our
method focuses on the extraction of phonetic transcriptions from text, stress
marks, and a unified automatic syllabification (in text and phonetic domains).
The system was built with open-source components and resources. Through an
ablation study, we demonstrate the efficacy of our approach in automatically
syllabifying words from several languages (English, French and Spanish).
Additionally, we apply the technique to the transcriptions of the CMU ARCTIC
dataset, generating valuable annotations available
online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for
speech representation learning, speech unit discovery, and disentanglement of
speech factors in several speech-related fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11564">
<div class="article-summary-box-inner">
<span><p>While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11571">
<div class="article-summary-box-inner">
<span><p>Asking questions is an important element of real-life collaboration on
reasoning tasks like question answering. For example, a legal assistant chatbot
may be unable to make accurate recommendations without specific information on
the user's circumstances. However, large language models are usually deployed
to solve reasoning tasks directly without asking follow-up questions to the
user or third parties. We term this problem task-oriented asking (TOA).
Zero-shot chat models can perform TOA, but their training is primarily based on
next-token prediction rather than whether questions contribute to successful
collaboration. To enable the training and evaluation of TOA models, we present
a definition and framework for natural language task-oriented asking, the
problem of generating questions that result in answers useful for a reasoning
task. We also present fact-level masking (FLM), a procedure for converting
natural language datasets into self-supervised TOA datasets by omitting
particular critical facts. Finally, we generate a TOA dataset from the HotpotQA
dataset using FLM and evaluate several zero-shot language models on it. Our
experiments show that current zero-shot models struggle to ask questions that
retrieve useful information, as compared to human annotators. These results
demonstrate an opportunity to use FLM datasets and the TOA framework to train
and evaluate better TOA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages. (arXiv:2310.11584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11584">
<div class="article-summary-box-inner">
<span><p>Current research on automatic readability assessment (ARA) has focused on
improving the performance of models in high-resource languages such as English.
In this work, we introduce and release BasahaCorpus as part of an initiative
aimed at expanding available corpora and baseline models for readability
assessment in lower resource languages in the Philippines. We compiled a corpus
of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and
Rinconada -- languages belonging to the Central Philippine family tree subgroup
-- to train ARA models using surface-level, syllable-pattern, and n-gram
overlap features. We also propose a new hierarchical cross-lingual modeling
approach that takes advantage of a language's placement in the family tree to
increase the amount of available training data. Our study yields encouraging
results that support previous work showcasing the efficacy of cross-lingual
models in low-resource settings, as well as similarities in highly informative
linguistic features for mutually intelligible languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11589">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) can be directed to perform target tasks by using
labeled examples or natural language prompts. But selecting examples or writing
prompts for can be challenging--especially in tasks that involve unusual edge
cases, demand precise articulation of nebulous preferences, or require an
accurate mental model of LM behavior. We propose to use *LMs themselves* to
guide the task specification process. In this paper, we introduce **Generative
Active Task Elicitation (GATE)**: a learning framework in which models elicit
and infer intended behavior through free-form, language-based interaction with
users. We study GATE in three domains: email validation, content
recommendation, and moral reasoning. In preregistered experiments, we show that
LMs prompted to perform GATE (e.g., by generating open-ended questions or
synthesizing informative edge cases) elicit responses that are often more
informative than user-written prompts or labels. Users report that interactive
task elicitation requires less effort than prompting or example labeling and
surfaces novel considerations not initially anticipated by users. Our findings
suggest that LM-driven elicitation can be a powerful tool for aligning models
to complex human preferences and values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11593">
<div class="article-summary-box-inner">
<span><p>Personalized text generation presents a specialized mechanism for delivering
content that is specific to a user's personal context. While the research
progress in this area has been rapid, evaluation still presents a challenge.
Traditional automated metrics such as BLEU and ROUGE primarily measure lexical
similarity to human-written references, and are not able to distinguish
personalization from other subtle semantic aspects, thus falling short of
capturing the nuances of personalized generated content quality. On the other
hand, human judgments are costly to obtain, especially in the realm of
personalized evaluation. Inspired by these challenges, we explore the use of
large language models (LLMs) for evaluating personalized text generation, and
examine their ability to understand nuanced user context. We present AuPEL, a
novel evaluation method that distills three major semantic aspects of the
generated text: personalization, quality and relevance, and automatically
measures these aspects. To validate the effectiveness of AuPEL, we design
carefully controlled experiments and compare the accuracy of the evaluation
judgments made by LLMs versus that of judgements made by human annotators, and
conduct rigorous analyses of the consistency and sensitivity of the proposed
metric. We find that, compared to existing evaluation metrics, AuPEL not only
distinguishes and ranks models based on their personalization abilities more
accurately, but also presents commendable consistency and efficiency for this
task. Our work suggests that using LLMs as the evaluators of personalized text
generation is superior to traditional text similarity metrics, even though
interesting new challenges still remain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11604">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently shown promise as high-level
planners for robots when given access to a selection of low-level skills.
However, it is often assumed that LLMs do not possess sufficient knowledge to
be used for the low-level trajectories themselves. In this work, we address
this assumption thoroughly, and investigate if an LLM (GPT-4) can directly
predict a dense sequence of end-effector poses for manipulation skills, when
given access to only object detection and segmentation vision models. We study
how well a single task-agnostic prompt, without any in-context examples, motion
primitives, or external trajectory optimisers, can perform across 26 real-world
language-based tasks, such as "open the bottle cap" and "wipe the plate with
the sponge", and we investigate which design choices in this prompt are the
most effective. Our conclusions raise the assumed limit of LLMs for robotics,
and we reveal for the first time that LLMs do indeed possess an understanding
of low-level robot control sufficient for a range of common tasks, and that
they can additionally detect failures and then re-plan trajectories
accordingly. Videos, code, and prompts are available at:
https://www.robot-learning.uk/language-models-trajectory-generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11616">
<div class="article-summary-box-inner">
<span><p>This study uncovers the factor of general intelligence, or g, in language
models, extending the psychometric theory traditionally applied to humans and
certain animal species. Utilizing factor analysis on two extensive datasets -
Open LLM Leaderboard with 1,232 models and General Language Understanding
Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for
a unidimensional, highly stable g factor that accounts for 85% of the variance
in model performance. The study also finds a moderate correlation of .48
between model size and g. The discovery of g in language models offers a
unified metric for model evaluation and opens new avenues for more robust,
g-based model ability assessment. These findings lay the foundation for
understanding and future research on artificial general intelligence from a
psychometric perspective and have practical implications for model evaluation
and development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11628">
<div class="article-summary-box-inner">
<span><p>Language models typically tokenize text into subwords, using a deterministic,
hand-engineered heuristic of combining characters into longer surface-level
strings such as 'ing' or whole words. Recent literature has repeatedly shown
the limitations of such a tokenization strategy, particularly for documents not
written in English and for representing numbers. On the other extreme,
byte/character-level language models are much less restricted but suffer from
increased sequence description lengths and a subsequent quadratic expansion in
self-attention computation. Recent attempts to compress and limit these context
lengths with fixed size convolutions is helpful but completely ignores the word
boundary. This paper considers an alternative 'learn your tokens' scheme which
utilizes the word boundary to pool bytes/characters into word representations,
which are fed to the primary language model, before again decoding individual
characters/bytes per word in parallel. We find that our moderately expressive
and moderately fast end-to-end tokenizer outperform by over 300% both subwords
and byte/character models over the intrinsic language modeling metric of
next-word prediction across datasets. It particularly outshines on rare words,
outperforming by a factor of 30! We extensively study the language modeling
setup for all three categories of tokenizers and theoretically analyze how our
end-to-end models can also be a strong trade-off in efficiency and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations. (arXiv:2310.11634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11634">
<div class="article-summary-box-inner">
<span><p>Humans possess a remarkable ability to assign novel interpretations to
linguistic expressions, enabling them to learn new words and understand
community-specific connotations. However, Large Language Models (LLMs) have a
knowledge cutoff and are costly to finetune repeatedly. Therefore, it is
crucial for LLMs to learn novel interpretations in-context. In this paper, we
systematically analyse the ability of LLMs to acquire novel interpretations
using in-context learning. To facilitate our study, we introduce MAGNIFICo, an
evaluation suite implemented within a text-to-SQL semantic parsing framework
that incorporates diverse tokens and prompt settings to simulate real-world
complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a
surprisingly robust capacity for comprehending novel interpretations from
natural language descriptions as well as from discussions within long
conversations. Nevertheless, our findings also highlight the need for further
improvements, particularly when interpreting unfamiliar words or when composing
multiple novel interpretations simultaneously in the same example.
Additionally, our analysis uncovers the semantic predispositions in LLMs and
reveals the impact of recency bias for information presented in long contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11638">
<div class="article-summary-box-inner">
<span><p>Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. (arXiv:2310.11648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11648">
<div class="article-summary-box-inner">
<span><p>Despite tremendous improvements in natural language generation, summarization
models still suffer from the unfaithfulness issue. Previous work evaluates
faithfulness either using models trained on the other tasks or in-domain
synthetic data, or prompting a large model such as ChatGPT. This paper proposes
to do zero-shot faithfulness evaluation simply with a moderately-sized
foundation language model. We introduce a new metric FFLM, which is a
combination of probability changes based on the intuition that prefixing a
piece of text that is consistent with the output will increase the probability
of predicting the output. Experiments show that FFLM performs competitively
with or even outperforms ChatGPT on both inconsistency detection and
faithfulness rating with 24x fewer parameters. FFLM also achieves improvements
over other strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Field-testing items using artificial intelligence: Natural language processing with transformers. (arXiv:2310.11655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11655">
<div class="article-summary-box-inner">
<span><p>Five thousand variations of the RoBERTa model, an artificially intelligent
"transformer" that can understand text language, completed an English literacy
exam with 29 multiple-choice questions. Data were used to calculate the
psychometric properties of the items, which showed some degree of agreement to
those obtained from human examinee data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11667">
<div class="article-summary-box-inner">
<span><p>Humans are social beings; we pursue social goals in our daily interactions,
which is a crucial aspect of social intelligence. Yet, AI systems' abilities in
this realm remain elusive. We present SOTOPIA, an open-ended environment to
simulate complex social interactions between artificial agents and evaluate
their social intelligence. In our environment, agents role-play and interact
under a wide variety of scenarios; they coordinate, collaborate, exchange, and
compete with each other to achieve complex social goals. We simulate the
role-play interaction between LLM-based agents and humans within this task
space and evaluate their performance with a holistic evaluation framework
called SOTOPIA-Eval. With SOTOPIA, we find significant differences between
these models in terms of their social intelligence, and we identify a subset of
SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.
We find that on this subset, GPT-4 achieves a significantly lower goal
completion rate than humans and struggles to exhibit social commonsense
reasoning and strategic communication skills. These findings demonstrate
SOTOPIA's promise as a general platform for research on evaluating and
improving social intelligence in artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11670">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction. (arXiv:2310.11671v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11671">
<div class="article-summary-box-inner">
<span><p>Data Augmentation through generating pseudo data has been proven effective in
mitigating the challenge of data scarcity in the field of Grammatical Error
Correction (GEC). Various augmentation strategies have been widely explored,
most of which are motivated by two heuristics, i.e., increasing the
distribution similarity and diversity of pseudo data. However, the underlying
mechanism responsible for the effectiveness of these strategies remains poorly
understood. In this paper, we aim to clarify how data augmentation improves GEC
models. To this end, we introduce two interpretable and computationally
efficient measures: Affinity and Diversity. Our findings indicate that an
excellent GEC data augmentation strategy characterized by high Affinity and
appropriate Diversity can better improve the performance of GEC models. Based
on this observation, we propose MixEdit, a data augmentation approach that
strategically and dynamically augments realistic data, without requiring extra
monolingual corpora. To verify the correctness of our findings and the
effectiveness of the proposed MixEdit, we conduct experiments on mainstream
English and Chinese GEC datasets. The results show that MixEdit substantially
improves GEC models and is complementary to traditional data augmentation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11672">
<div class="article-summary-box-inner">
<span><p>Open-ended Commonsense Reasoning is defined as solving a commonsense question
without providing 1) a short list of answer candidates and 2) a pre-defined
answer scope. Conventional ways of formulating the commonsense question into a
question-answering form or utilizing external knowledge to learn
retrieval-based methods are less applicable in the open-ended setting due to an
inherent challenge. Without pre-defining an answer scope or a few candidates,
open-ended commonsense reasoning entails predicting answers by searching over
an extremely large searching space. Moreover, most questions require implicit
multi-hop reasoning, which presents even more challenges to our problem. In
this work, we leverage pre-trained language models to iteratively retrieve
reasoning paths on the external knowledge base, which does not require
task-specific supervision. The reasoning paths can help to identify the most
precise answer to the commonsense question. We conduct experiments on two
commonsense benchmark datasets. Compared to other approaches, our proposed
method achieves better performance both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Descriptive Knowledge Graph in Biomedical Domain. (arXiv:2310.11681v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11681">
<div class="article-summary-box-inner">
<span><p>We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11685">
<div class="article-summary-box-inner">
<span><p>Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
</p>
<p>Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
</p>
<p>In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11689">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently shown great advances in a variety
of tasks, including natural language understanding and generation. However,
their use in high-stakes decision-making scenarios is still limited due to the
potential for errors. Selective prediction is a technique that can be used to
improve the reliability of the LLMs by allowing them to abstain from making
predictions when they are unsure of the answer. In this work, we propose a
novel framework for adaptation with self-evaluation to improve the selective
prediction performance of LLMs. Our framework is based on the idea of using
parameter-efficient tuning to adapt the LLM to the specific task at hand while
improving its ability to perform self-evaluation. We evaluate our method on a
variety of question-answering (QA) datasets and show that it outperforms
state-of-the-art selective prediction methods. For example, on the CoQA
benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the
AUROC from 74.61% to 80.25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11699">
<div class="article-summary-box-inner">
<span><p>Augmented reality (AR) requires the seamless integration of visual, auditory,
and linguistic channels for optimized human-computer interaction. While
auditory and visual inputs facilitate real-time and contextual user guidance,
the potential of large language models (LLMs) in this landscape remains largely
untapped. Our study introduces an innovative method harnessing LLMs to
assimilate information from visual, auditory, and contextual modalities.
Focusing on the unique challenge of task performance quantification in AR, we
utilize egocentric video, speech, and context analysis. The integration of LLMs
facilitates enhanced state estimation, marking a step towards more adaptive AR
systems. Code, dataset, and demo will be available at
https://github.com/nguyennm1024/misar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11710">
<div class="article-summary-box-inner">
<span><p>Aphasia, a language disorder resulting from brain damage, requires accurate
identification of specific aphasia types, such as Broca's and Wernicke's
aphasia, for effective treatment. However, little attention has been paid to
developing methods to detect different types of aphasia. Recognizing the
importance of analyzing co-speech gestures for distinguish aphasia types, we
propose a multimodal graph neural network for aphasia type detection using
speech and corresponding gesture patterns. By learning the correlation between
the speech and gesture modalities for each aphasia type, our model can generate
textual representations sensitive to gesture information, leading to accurate
aphasia type detection. Extensive experiments demonstrate the superiority of
our approach over existing methods, achieving state-of-the-art results (F1
84.2\%). We also show that gesture features outperform acoustic features,
highlighting the significance of gesture expression in detecting aphasia types.
We provide the codes for reproducibility purposes\footnote{Code:
\url{https://github.com/DSAIL-SKKU/Multimodal-Aphasia-Type-Detection_EMNLP_2023}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11715">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) frequently suffers from the problem of
insufficient labeled data, particularly in fine-grained NER scenarios. Although
$K$-shot learning techniques can be applied, their performance tends to
saturate when the number of annotations exceeds several tens of labels. To
overcome this problem, we utilize existing coarse-grained datasets that offer a
large number of annotations. A straightforward approach to address this problem
is pre-finetuning, which employs coarse-grained data for representation
learning. However, it cannot directly utilize the relationships between
fine-grained and coarse-grained entities, although a fine-grained entity type
is likely to be a subcategory of a coarse-grained entity type. We propose a
fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage
the hierarchical structure explicitly. In addition, we present an inconsistency
filtering method to eliminate coarse-grained entities that are inconsistent
with fine-grained entity types to avoid performance degradation. Our
experimental results show that our method outperforms both $K$-shot learning
and supervised learning methods when dealing with a small number of
fine-grained annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11716">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models (LLMs) have expanded the
horizons of natural language understanding and generation. Notably, the output
control and alignment with the input of LLMs can be refined through instruction
tuning. However, as highlighted in several studies, low-quality data in the
training set are usually detrimental to instruction tuning, resulting in
inconsistent or even misleading LLM outputs. We propose a novel method, termed
"reflection-tuning," which addresses the problem by self-improvement and
judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle
the original training data by introspecting and enhancing the quality of
instructions and responses in the data. Extensive experiments on widely used
evaluation benchmarks show that LLMs trained with our recycled data outperform
those trained with existing datasets in various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11721">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) is a technique that guides Large Language Models
(LLMs) to decompose complex tasks into multi-step reasoning through
intermediate steps in natural language form. Briefly, CoT enables LLMs to think
step by step. However, although many Natural Language Understanding (NLU) tasks
also require thinking step by step, LLMs perform less well than small-scale
Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose
Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt
tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the
perspective of CoT, CoTT's two-step framework enables MLMs to implement task
decomposition; CoTT's prompt tuning allows intermediate steps to be used in
natural language form. Thereby, the success of CoT can be extended to NLU tasks
through MLMs. To verify the effectiveness of CoTT, we conduct experiments on
two NLU tasks: hierarchical classification and relation extraction, and the
results show that CoTT outperforms baselines and achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11722">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have the potential to revolutionize the way
users self-diagnose through search engines by offering direct and efficient
suggestions. Recent studies primarily focused on the quality of LLMs evaluated
by GPT-4 or their ability to pass medical exams, no studies have quantified the
extent of health-related atomic knowledge stored in LLMs' memory, which is the
basis of LLMs to provide more factual suggestions. In this paper, we first
constructed a benchmark, including the most common types of atomic knowledge in
user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces
of atomic knowledge. Then, we evaluated both generic and specialized LLMs on
the benchmark. The experimental results showcased that generic LLMs perform
better than specialized LLMs in terms of atomic knowledge and
instruction-following ability. Error analysis revealed that both generic and
specialized LLMs are sycophantic, e.g., always catering to users' claims when
it comes to unknown knowledge. Besides, generic LLMs showed stronger safety,
which can be learned by specialized LLMs through distilled data. We further
explored different types of data commonly adopted for fine-tuning specialized
LLMs, i.e., real-world, semi-distilled, and distilled data, and found that
distilled data can benefit LLMs most.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11732">
<div class="article-summary-box-inner">
<span><p>Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM's
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs' overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11753">
<div class="article-summary-box-inner">
<span><p>This technical report explores the ability of ChatGPT in recognizing emotions
from text, which can be the basis of various applications like interactive
chatbots, data annotation, and mental health analysis. While prior research has
shown ChatGPT's basic ability in sentiment analysis, its performance in more
nuanced emotion recognition is not yet explored. Here, we conducted experiments
to evaluate its performance of emotion recognition across different datasets
and emotion labels. Our findings indicate a reasonable level of reproducibility
in its performance, with noticeable improvement through fine-tuning. However,
the performance varies with different emotion labels and datasets, highlighting
an inherent instability and possible bias. The choice of dataset and emotion
labels significantly impacts ChatGPT's emotion recognition performance. This
paper sheds light on the importance of dataset and label selection, and the
potential of fine-tuning in enhancing ChatGPT's emotion recognition
capabilities, providing a groundwork for better integration of emotion analysis
in applications using ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11761">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated great potential for
domain-specific applications, such as the law domain. However, recent disputes
over GPT-4's law evaluation raise questions concerning their performance in
real-world legal tasks. To systematically investigate their competency in the
law, we design practical baseline solutions based on LLMs and test on the task
of legal judgment prediction. In our solutions, LLMs can work alone to answer
open questions or coordinate with an information retrieval (IR) system to learn
from similar cases or solve simplified multi-choice questions. We show that
similar cases and multi-choice options, namely label candidates, included in
prompts can help LLMs recall domain knowledge that is critical for expertise
legal reasoning. We additionally present an intriguing paradox wherein an IR
system surpasses the performance of LLM+IR due to limited gains acquired by
weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes
redundant. Our evaluation pipeline can be easily extended into other tasks to
facilitate evaluations in other domains. Code is available at
https://github.com/srhthu/LM-CompEval-Legal
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotated Job Ads with Named Entity Recognition. (arXiv:2310.11769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11769">
<div class="article-summary-box-inner">
<span><p>We have trained a named entity recognition (NER) model that screens Swedish
job ads for different kinds of useful information (e.g. skills required from a
job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we
faced was the creation of a labelled dataset, which required manual annotation.
This paper gives an overview of the methods we employed to make the annotation
process more efficient and to ensure high quality data. We also report on the
performance of the resulting model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11772">
<div class="article-summary-box-inner">
<span><p>Topic segmentation is critical for obtaining structured long documents and
improving downstream tasks like information retrieval. Due to its ability of
automatically exploring clues of topic shift from a large amount of labeled
data, recent supervised neural models have greatly promoted the development of
long document topic segmentation, but leaving the deeper relationship of
semantic coherence and topic segmentation underexplored. Therefore, this paper
enhances the supervised model's ability to capture coherence from both
structure and similarity perspectives to further improve the topic segmentation
performance, including the Topic-aware Sentence Structure Prediction (TSSP) and
Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is
proposed to force the model to comprehend structural information by learning
the original relations of adjacent sentences in a disarrayed document, which is
constructed by jointly disrupting the original document at the topic and
sentence levels. In addition, we utilize inter- and intra-topic information to
construct contrastive samples and design the CSSL objective to ensure that the
sentences representations in the same topic have higher semantic similarity,
while those in different topics are less similar. Extensive experiments show
that the Longformer with our approach significantly outperforms old
state-of-the-art (SOTA) methods. Our approach improves $F_{1}$ of old SOTA by
3.42 (73.74 -&gt; 77.16) and reduces $P_{k}$ by 1.11 points (15.0 -&gt; 13.89) on
WIKI-727K and achieves an average reduction of 0.83 points on $P_{k}$ on
WikiSection. The average $P_{k}$ drop of 2.82 points on the two out-of-domain
datasets also illustrates the robustness of our approach
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11778">
<div class="article-summary-box-inner">
<span><p>The recent surge in the research of diffusion models has accelerated the
adoption of text-to-image models in various Artificial Intelligence Generated
Content (AIGC) commercial products. While these exceptional AIGC products are
gaining increasing recognition and sparking enthusiasm among consumers, the
questions regarding whether, when, and how these models might unintentionally
reinforce existing societal stereotypes remain largely unaddressed. Motivated
by recent advancements in language agents, here we introduce a novel agent
architecture tailored for stereotype detection in text-to-image models. This
versatile agent architecture is capable of accommodating free-form detection
tasks and can autonomously invoke various tools to facilitate the entire
process, from generating corresponding instructions and images, to detecting
stereotypes. We build the stereotype-relevant benchmark based on multiple
open-text datasets, and apply this architecture to commercial products and
popular open source text-to-image models. We find that these models often
display serious stereotypes when it comes to certain prompts about personal
characteristics, social cultural context and crime-related aspects. In summary,
these empirical findings underscore the pervasive existence of stereotypes
across social dimensions, including gender, race, and religion, which not only
validate the effectiveness of our proposed approach, but also emphasize the
critical necessity of addressing potential ethical risks in the burgeoning
realm of AIGC. As AIGC continues its rapid expansion trajectory, with new
models and plugins emerging daily in staggering numbers, the challenge lies in
the timely detection and mitigation of potential biases within these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Annotation Handbook: A Practical Guide for Machine Learning Projects. (arXiv:2310.11780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11780">
<div class="article-summary-box-inner">
<span><p>This handbook is a hands-on guide on how to approach text annotation tasks.
It provides a gentle introduction to the topic, an overview of theoretical
concepts as well as practical advice. The topics covered are mostly technical,
but business, ethical and regulatory issues are also touched upon. The focus
lies on readability and conciseness rather than completeness and scientific
rigor. Experience with annotation and knowledge of machine learning are useful
but not required. The document may serve as a primer or reference book for a
wide range of professions such as team leaders, project managers, IT
architects, software developers and machine learning engineers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics. (arXiv:2310.11870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11870">
<div class="article-summary-box-inner">
<span><p>This paper presents "AI Nushu," an emerging language system inspired by Nushu
(women's scripts), the unique language created and used exclusively by ancient
Chinese women who were thought to be illiterate under a patriarchal society. In
this interactive installation, two artificial intelligence (AI) agents are
trained in the Chinese dictionary and the Nushu corpus. By continually
observing their environment and communicating, these agents collaborate towards
creating a standard writing system to encode Chinese. It offers an artistic
interpretation of the creation of a non-western script from a computational
linguistics perspective, integrating AI technology with Chinese cultural
heritage and a feminist viewpoint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11877">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been shown to possess impressive
capabilities, while also raising crucial concerns about the faithfulness of
their responses. A primary issue arising in this context is the management of
unanswerable queries by LLMs, which often results in hallucinatory behavior,
due to overconfidence. In this paper, we explore the behavior of LLMs when
presented with unanswerable queries. We ask: do models \textbf{represent} the
fact that the question is unanswerable when generating a hallucinatory answer?
Our results show strong indications that such models encode the answerability
of an input query, with the representation of the first decoded token often
being a strong indicator. These findings shed new light on the spatial
organization within the latent representations of LLMs, unveiling previously
unexplored facets of these models. Moreover, they pave the way for the
development of improved decoding techniques with better adherence to factual
generation, particularly in scenarios where query unanswerability is a concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11878">
<div class="article-summary-box-inner">
<span><p>In legal NLP, Case Outcome Classification (COC) must not only be accurate but
also trustworthy and explainable. Existing work in explainable COC has been
limited to annotations by a single expert. However, it is well-known that
lawyers may disagree in their assessment of case facts. We hence collect a
novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two
experts in the domain of international human rights law, for whom we observe
weak agreement. We study their disagreements and build a two-level
task-independent taxonomy, supplemented with COC-specific subcategories. To our
knowledge, this is the first work in the legal NLP that focuses on human label
variation. We quantitatively assess different taxonomy categories and find that
disagreements mainly stem from underspecification of the legal context, which
poses challenges given the typically limited granularity and noise in COC
metadata. We further assess the explainablility of SOTA COC models on RAVE and
observe limited agreement between models and experts. Overall, our case study
reveals hitherto underappreciated complexities in creating benchmark datasets
in legal NLP that revolve around identifying aspects of a case's facts
supposedly relevant to its outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11884">
<div class="article-summary-box-inner">
<span><p>In this paper, we review recent approaches for explaining concepts in neural
networks. Concepts can act as a natural link between learning and reasoning:
once the concepts are identified that a neural learning system uses, one can
integrate those concepts with a reasoning system for inference or use a
reasoning system to act upon them to improve or enhance the learning system. On
the other hand, knowledge can not only be extracted from neural networks but
concept knowledge can also be inserted into neural network architectures. Since
integrating learning and reasoning is at the core of neuro-symbolic AI, the
insights gained from this survey can serve as an important step towards
realizing neuro-symbolic AI based on explainable concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11906">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations, where one decision is explained in contrast to
another, are supposed to be closer to how humans explain a decision than
non-contrastive explanations, where the decision is not necessarily referenced
to an alternative. This claim has never been empirically validated. We analyze
four English text-classification datasets (SST2, DynaSent, BIOS and
DBpedia-Animals). We fine-tune and extract explanations from three different
models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three
post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore
collect and release human rationale annotations for a subset of 100 samples
from the BIOS dataset for contrastive and non-contrastive settings. A
cross-comparison between model-based rationales and human annotations, both in
contrastive and non-contrastive settings, yields a high agreement between the
two settings for models as well as for humans. Moreover, model-based
explanations computed in both settings align equally well with human
rationales. Thus, we empirically find that humans do not necessarily explain in
a contrastive manner.9 pages, long paper at ACL 2022 proceedings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11917">
<div class="article-summary-box-inner">
<span><p>Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of
predicting facts for new, previously unseen entities based on context
information. Although new entities can be integrated by retraining the model
from scratch in principle, such an approach is infeasible for large-scale KGs,
where retraining is expensive and new entities may arise frequently. In this
paper, we propose and describe a large-scale benchmark to evaluate
semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It
provides transductive, k-shot, and 0-shot LP tasks, each varying the available
information from (i) only KG structure, to (ii) including textual mentions, and
(iii) detailed descriptions of the entities. We report on a small study of
recent approaches and found that semi-inductive LP performance is far from
transductive performance on long-tail entities throughout all experiments. The
benchmark provides a test bed for further research into integrating context and
textual information in semi-inductive LP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing. (arXiv:2310.11923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11923">
<div class="article-summary-box-inner">
<span><p>The question of what kinds of linguistic information are encoded in different
layers of Transformer-based language models is of considerable interest for the
NLP community. Existing work, however, has overwhelmingly focused on word-level
representations and encoder-only language models with the masked-token training
objective. In this paper, we present experiments with semantic structural
probing, a method for studying sentence-level representations via finding a
subspace of the embedding space that provides suitable task-specific pairwise
distances between data-points. We apply our method to language models from
different families (encoder-only, decoder-only, encoder-decoder) and of
different sizes in the context of two tasks, semantic textual similarity and
natural-language inference. We find that model families differ substantially in
their performance and layer dynamics, but that the results are largely
model-size invariant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding. (arXiv:2310.11938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11938">
<div class="article-summary-box-inner">
<span><p>Grounding has been argued to be a crucial component towards the development
of more complete and truly semantically competent artificial intelligence
systems. Literature has divided into two camps: While some argue that grounding
allows for qualitatively different generalizations, others believe it can be
compensated by mono-modal data quantity. Limited empirical evidence has emerged
for or against either position, which we argue is due to the methodological
challenges that come with studying grounding and its effects on NLP systems.
</p>
<p>In this paper, we establish a methodological framework for studying what the
effects are - if any - of providing models with richer input sources than
text-only. The crux of it lies in the construction of comparable samples of
populations of models trained on different input modalities, so that we can
tease apart the qualitative effects of different input sources from
quantifiable model performances. Experiments using this framework reveal
qualitative differences in model behavior between cross-modally grounded,
cross-lingually grounded, and ungrounded models, which we measure both at a
global dataset level as well as for specific word representations, depending on
how concrete their semantics is.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models. (arXiv:2310.11954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11954">
<div class="article-summary-box-inner">
<span><p>AI-empowered music processing is a diverse field that encompasses dozens of
tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension
tasks (e.g., music classification). For developers and amateurs, it is very
difficult to grasp all of these task to satisfy their requirements in music
processing, especially considering the huge differences in the representations
of music data and the model applicability across platforms among various tasks.
Consequently, it is necessary to build a system to organize and integrate these
tasks, and thus help practitioners to automatically analyze their demand and
call suitable tools as solutions to fulfill their requirements. Inspired by the
recent success of large language models (LLMs) in task automation, we develop a
system, named MusicAgent, which integrates numerous music-related tools and an
autonomous workflow to address user requirements. More specifically, we build
1) toolset that collects tools from diverse sources, including Hugging Face,
GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,
ChatGPT) to organize these tools and automatically decompose user requests into
multiple sub-tasks and invoke corresponding music tools. The primary goal of
this system is to free users from the intricacies of AI-music tools, enabling
them to concentrate on the creative aspect. By granting users the freedom to
effortlessly combine tools, the system offers a seamless and enriching music
experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11958">
<div class="article-summary-box-inner">
<span><p>We call into question the recently popularized method of direct model editing
as a means of correcting factual errors in LLM generations. We contrast model
editing with three similar but distinct approaches that pursue better defined
objectives: (1) retrieval-based architectures, which decouple factual memory
from inference and linguistic capabilities embodied in LLMs; (2) concept
erasure methods, which aim at preventing systemic bias in generated text; and
(3) attribution methods, which aim at grounding generations into identified
textual sources. We argue that direct model editing cannot be trusted as a
systematic remedy for the disadvantages inherent to LLMs, and while it has
proven potential in improving model explainability, it opens risks by
reinforcing the notion that models can be trusted for factuality. We call for
cautious promotion and application of model editing as part of the LLM
deployment process, and for responsibly limiting the use cases of LLMs to those
not relying on editing as a critical component.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11960">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved state-of-the-art performance in many
areas. However, the quadratic complexity of self-attention with respect to the
input length hinders the applicability of Transformer-based models to long
sequences. To address this, we present Fast Multipole Attention, a new
attention mechanism that uses a divide-and-conquer strategy to reduce the time
and memory complexity of attention for sequences of length $n$ from
$\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a
global receptive field. The hierarchical approach groups queries, keys, and
values into $\mathcal{O}( \log n)$ levels of resolution, where groups at
greater distances are increasingly larger in size and the weights to compute
group quantities are learned. As such, the interaction between tokens far from
each other is considered in lower resolution in an efficient hierarchical
manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$
or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled
or not. This multi-level divide-and-conquer strategy is inspired by fast
summation methods from $n$-body physics and the Fast Multipole Method. We
perform evaluation on autoregressive and bidirectional language modeling tasks
and compare our Fast Multipole Attention model with other efficient attention
variants on medium-size datasets. We find empirically that the Fast Multipole
Transformer performs much better than other efficient transformers in terms of
memory size and accuracy. The Fast Multipole Attention mechanism has the
potential to empower large language models with much greater sequence lengths,
taking the full context into account in an efficient, naturally hierarchical
manner during training and when generating long sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11964">
<div class="article-summary-box-inner">
<span><p>Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks. (arXiv:2310.11965v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11965">
<div class="article-summary-box-inner">
<span><p>We introduce a novel and efficient method for Event Coreference Resolution
(ECR) applied to a lower-resourced language domain. By framing ECR as a graph
reconstruction task, we are able to combine deep semantic embeddings with
structural coreference chain knowledge to create a parameter-efficient family
of Graph Autoencoder models (GAE). Our method significantly outperforms
classical mention-pair methods on a large Dutch event coreference corpus in
terms of overall score, efficiency and training speed. Additionally, we show
that our models are consistently able to classify more difficult coreference
links and are far more robust in low-data settings when compared to
transformer-based mention-pair coreference algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11976">
<div class="article-summary-box-inner">
<span><p>Diffusion models have garnered considerable interest in the field of text
generation. Several studies have explored text diffusion models with different
structures and applied them to various tasks, including named entity
recognition and summarization. However, there exists a notable disparity
between the "easy-first" text generation process of current diffusion models
and the "keyword-first" natural text generation process of humans, which has
received limited attention. To bridge this gap, we propose InfoDiffusion, a
non-autoregressive text diffusion model. Our approach introduces a
"keyinfo-first" generation strategy and incorporates a noise schedule based on
the amount of text information. In addition, InfoDiffusion combines
self-conditioning with a newly proposed partially noising model structure.
Experimental results show that InfoDiffusion outperforms the baseline model in
terms of generation quality and diversity, as well as exhibiting higher
sampling efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11984">
<div class="article-summary-box-inner">
<span><p>Since its introduction, the transformer model has demonstrated outstanding
performance across various tasks. However, there are still unresolved issues
regarding length generalization, particularly in algorithmic tasks. In this
paper, we investigate the inherent capabilities of transformer models in
learning arithmetic algorithms, such as addition and multiplication. Through
experiments and attention analysis, we identify a number of crucial factors for
achieving optimal length generalization. We show that transformer models are
able to generalize to long lengths with the help of targeted attention biasing.
We then introduce Attention Bias Calibration (ABC), a calibration stage that
enables the model to automatically learn the proper attention biases, which we
link to mechanisms in relative position encoding. We demonstrate that using
ABC, the transformer model can achieve unprecedented perfect length
generalization on certain arithmetic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11986">
<div class="article-summary-box-inner">
<span><p>Generative AI systems produce a range of risks. To ensure the safety of
generative AI systems, these risks must be evaluated. In this paper, we make
two main contributions toward establishing such evaluations. First, we propose
a three-layered framework that takes a structured, sociotechnical approach to
evaluating these risks. This framework encompasses capability evaluations,
which are the main current approach to safety evaluation. It then reaches
further by building on system safety principles, particularly the insight that
context determines whether a given capability may cause harm. To account for
relevant context, our framework adds human interaction and systemic impacts as
additional layers of evaluation. Second, we survey the current state of safety
evaluation of generative AI systems and create a repository of existing
evaluations. Three salient evaluation gaps emerge from this analysis. We
propose ways forward to closing these gaps, outlining practical steps as well
as roles and responsibilities for different actors. Sociotechnical safety
evaluation is a tractable approach to the robust and comprehensive safety
evaluation of generative AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. (arXiv:2310.12008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12008">
<div class="article-summary-box-inner">
<span><p>Knowledge graph entity typing (KGET) aims at inferring plausible types of
entities in knowledge graphs. Existing approaches to KGET focus on how to
better encode the knowledge provided by the neighbors and types of an entity
into its representation. However, they ignore the semantic knowledge provided
by the way in which types can be clustered together. In this paper, we propose
a novel method called Multi-view Contrastive Learning for knowledge graph
Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge
provided by clusters into entity and type embeddings. MCLET is composed of
three modules: i) Multi-view Generation and Encoder module, which encodes
structured information from entity-type, entity-cluster and cluster-type views;
ii) Cross-view Contrastive Learning module, which encourages different views to
collaboratively improve view-specific representations of entities and types;
iii) Entity Typing Prediction module, which integrates multi-head attention and
a Mixture-of-Experts strategy to infer missing entity types. Extensive
experiments show the strong performance of MCLET compared to the
state-of-the-art
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection. (arXiv:2310.12011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12011">
<div class="article-summary-box-inner">
<span><p>Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12020">
<div class="article-summary-box-inner">
<span><p>The convergence of embodied agents and large language models (LLMs) has
brought significant advancements to embodied instruction following.
Particularly, the strong reasoning capabilities of LLMs make it possible for
robots to perform long-horizon tasks without expensive annotated
demonstrations. However, public benchmarks for testing the long-horizon
reasoning capabilities of language-conditioned robots in various scenarios are
still missing. To fill this gap, this work focuses on the tabletop manipulation
task and releases a simulation benchmark, \textit{LoHoRavens}, which covers
various long-horizon reasoning aspects spanning color, size, space, arithmetics
and reference. Furthermore, there is a key modality bridging problem for
long-horizon manipulation tasks with LLMs: how to incorporate the observation
feedback during robot execution for the LLM's closed-loop planning, which is
however less studied by prior work. We investigate two methods of bridging the
modality gap: caption generation and learnable interface for incorporating
explicit and implicit observation feedback to the LLM, respectively. These
methods serve as the two baselines for our proposed benchmark. Experiments show
that both methods struggle to solve some tasks, indicating long-horizon
manipulation tasks are still challenging for current popular models. We expect
the proposed public benchmark and baselines can help the community develop
better models for long-horizon tabletop manipulation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation. (arXiv:2310.12024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12024">
<div class="article-summary-box-inner">
<span><p>We introduce CORE, a dataset for few-shot relation classification (RC)
focused on company relations and business entities. CORE includes 4,708
instances of 12 relation types with corresponding textual evidence extracted
from company Wikipedia pages. Company names and business entities pose a
challenge for few-shot RC models due to the rich and diverse information
associated with them. For example, a company name may represent the legal
entity, products, people, or business divisions depending on the context.
Therefore, deriving the relation type between entities is highly dependent on
textual context. To evaluate the performance of state-of-the-art RC models on
the CORE dataset, we conduct experiments in the few-shot domain adaptation
setting. Our results reveal substantial performance gaps, confirming that
models trained on different domains struggle to adapt to CORE. Interestingly,
we find that models trained on CORE showcase improved out-of-domain
performance, which highlights the importance of high-quality data for robust
domain adaptation. Specifically, the information richness embedded in business
entities allows models to focus on contextual nuances, reducing their reliance
on superficial clues such as relation-specific verbs. In addition to the
dataset, we provide relevant code snippets to facilitate reproducibility and
encourage further research in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models. (arXiv:2310.12049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12049">
<div class="article-summary-box-inner">
<span><p>Existing text scaling methods often require a large corpus, struggle with
short texts, or require labeled data. We develop a text scaling method that
leverages the pattern recognition capabilities of generative large language
models (LLMs). Specifically, we propose concept-guided chain-of-thought
(CGCoT), which uses prompts designed to summarize ideas and identify target
parties in texts to generate concept-specific breakdowns, in many ways similar
to guidance for human coder content analysis. CGCoT effectively shifts pairwise
text comparisons from a reasoning problem to a pattern recognition problem. We
then pairwise compare concept-specific breakdowns using an LLM. We use the
results of these pairwise comparisons to estimate a scale using the
Bradley-Terry model. We use this approach to scale affective speech on Twitter.
Our measures correlate more strongly with human judgments than alternative
approaches like Wordfish. Besides a small set of pilot data to develop the
CGCoT prompts, our measures require no additional labeled data and produce
binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands
of human-labeled tweets. We demonstrate how combining substantive knowledge
with LLMs can create state-of-the-art measures of abstract concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12059">
<div class="article-summary-box-inner">
<span><p>In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles. (arXiv:2310.12064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12064">
<div class="article-summary-box-inner">
<span><p>This paper presents a scheme for annotating coreference across news articles,
extending beyond traditional identity relations by also considering
near-identity and bridging relations. It includes a precise description of how
to set up Inception, a respective annotation tool, how to annotate entities in
news articles, connect them with diverse coreferential relations, and link them
across documents to Wikidata's global knowledge graph. This multi-layered
annotation approach is discussed in the context of the problem of media bias.
Our main contribution lies in providing a methodology for creating a diverse
cross-document coreference corpus which can be applied to the analysis of media
bias by word-choice and labelling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12072">
<div class="article-summary-box-inner">
<span><p>Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12074">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new IncidentAI dataset for safety prevention.
Different from prior corpora that usually contain a single task, our dataset
comprises three tasks: named entity recognition, cause-effect extraction, and
information retrieval. The dataset is annotated by domain experts who have at
least six years of practical experience as high-pressure gas conservation
managers. We validate the contribution of the dataset in the scenario of safety
prevention. Preliminary results on the three tasks show that NLP techniques are
beneficial for analyzing incident reports to prevent future failures. The
dataset facilitates future research in NLP and incident management communities.
The access to the dataset is also provided (the IncidentAI dataset is available
at: https://github.com/Cinnamon/incident-ai-dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Benefit of Generative Foundation Models for Human Activity Recognition. (arXiv:2310.12085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12085">
<div class="article-summary-box-inner">
<span><p>In human activity recognition (HAR), the limited availability of annotated
data presents a significant challenge. Drawing inspiration from the latest
advancements in generative AI, including Large Language Models (LLMs) and
motion synthesis models, we believe that generative AI can address this data
scarcity by autonomously generating virtual IMU data from text descriptions.
Beyond this, we spotlight several promising research pathways that could
benefit from generative AI for the community, including the generating
benchmark datasets, the development of foundational models specific to HAR, the
exploration of hierarchical structures within HAR, breaking down complex
activities, and applications in health sensing and activity summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12086">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread
attention owing to their myriad of practical applications, yet their adoption
has been constrained by issues of fact-conflicting hallucinations across web
platforms. The assessment of factuality in text, produced by LLMs, remains
inadequately explored, extending not only to the judgment of vanilla facts but
also encompassing the evaluation of factual errors emerging in complex
inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a
fact-conflicting hallucination detection benchmark meticulously designed for
LLMs. Functioning as a pivotal tool in evaluating factuality within
"Query-Respons" contexts, our benchmark assimilates a large-scale dataset,
encapsulating a broad spectrum of factuality patterns, such as vanilla,
multi-hops, comparison, and set-operation patterns. A distinctive feature of
our benchmark is its incorporation of fact-based chains of evidence, thereby
facilitating comprehensive and conducive factual reasoning throughout the
assessment process. We evaluate multiple LLMs, demonstrating the effectiveness
of the benchmark and current methods fall short of faithfully detecting factual
errors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective
considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming
to yield more credible detection through the amalgamation of predictive results
and evidence. The benchmark dataset and source code will be made available in
https://github.com/zjunlp/FactCHD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12100">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) and vision language models (VLMs) demonstrate
excellent performance on a wide range of tasks by scaling up parameter counts
from O(10^9) to O(10^{12}) levels and further beyond. These large scales make
it impossible to adapt and deploy fully specialized models given a task of
interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising
direction to tackle the adaptation and serving challenges for such large
models. We categorize PEFT techniques into two types: intrusive and
non-intrusive. Intrusive PEFT techniques directly change a model's internal
architecture. Though more flexible, they introduce significant complexities for
training and serving. Non-intrusive PEFT techniques leave the internal
architecture unchanged and only adapt model-external parameters, such as
embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT
technique that achieves competitive performance compared to SoTA intrusive PEFT
(LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both
text-only and multimodal tasks, with experiments that account for both
parameter-count scaling and training regime (with and without instruction
tuning).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers. (arXiv:2310.12118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12118">
<div class="article-summary-box-inner">
<span><p>Neural networks have revolutionized language modeling and excelled in various
downstream tasks. However, the extent to which these models achieve
compositional generalization comparable to human cognitive abilities remains a
topic of debate. While existing approaches in the field have mainly focused on
novel architectures and alternative learning paradigms, we introduce a
pioneering method harnessing the power of dataset cartography (Swayamdipta et
al., 2020). By strategically identifying a subset of compositional
generalization data using this approach, we achieve a remarkable improvement in
model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets.
Notably, our technique incorporates dataset cartography as a curriculum
learning criterion, eliminating the need for hyperparameter tuning while
consistently achieving superior performance. Our findings highlight the
untapped potential of dataset cartography in unleashing the full capabilities
of compositional generalization within Transformer models. Our code is
available at https://github.com/cyberiada/cartography-for-compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12126">
<div class="article-summary-box-inner">
<span><p>We introduce SHARCS for adaptive inference that takes into account the
hardness of input samples. SHARCS can train a router on any transformer
network, enabling the model to direct different samples to sub-networks with
varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or
complements existing per-sample adaptive inference methods across various
classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes
across different architectures and can be even applied to compressed and
efficient transformer encoders to further improve their efficiency; (3) SHARCS
can provide a 2 times inference speed up at an insignificant drop in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12127">
<div class="article-summary-box-inner">
<span><p>Recent instruction fine-tuned models can solve multiple NLP tasks when
prompted to do so, with machine translation (MT) being a prominent use case.
However, current research often focuses on standard performance benchmarks,
leaving compelling fairness and ethical considerations behind. In MT, this
might lead to misgendered translations, resulting, among other harms, in the
perpetuation of stereotypes and prejudices. In this work, we address this gap
by investigating whether and to what extent such models exhibit gender bias in
machine translation and how we can mitigate it. Concretely, we compute
established gender bias metrics on the WinoMT corpus from English to German and
Spanish. We discover that IFT models default to male-inflected translations,
even disregarding female occupational stereotypes. Next, using interpretability
methods, we unveil that models systematically overlook the pronoun indicating
the gender of a target occupation in misgendered translations. Finally, based
on this finding, we propose an easy-to-implement and effective bias mitigation
solution based on few-shot learning that leads to significantly fairer
translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12128">
<div class="article-summary-box-inner">
<span><p>Text-to-image (T2I) generation has seen significant growth over the past few
years. Despite this, there has been little work on generating diagrams with T2I
models. A diagram is a symbolic/schematic representation that explains
information using structurally rich and spatially complex visualizations (e.g.,
a dense combination of related objects, text labels, directional arrows,
connection lines, etc.). Existing state-of-the-art T2I models often fail at
diagram generation because they lack fine-grained object layout control when
many objects are densely connected via complex relations such as arrows/lines
and also often fail to render comprehensible text labels. To address this gap,
we present DiagrammerGPT, a novel two-stage text-to-diagram generation
framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)
to generate more accurate open-domain, open-platform diagrams. In the first
stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a
planner-auditor feedback loop) which describe all the entities (objects and
text labels), their relationships (arrows or lines), and their bounding box
layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a
text label rendering module to generate diagrams following the diagram plans.
To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a
densely annotated diagram dataset built on top of the AI2D dataset. We show
quantitatively and qualitatively that our DiagrammerGPT framework produces more
accurate diagrams, outperforming existing T2I models. We also provide
comprehensive analysis including open-domain diagram generation, vector graphic
diagram generation in different platforms, human-in-the-loop diagram plan
editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our
work can inspire further research on diagram generation via T2I models and
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudointelligence: A Unifying Framework for Language Model Evaluation. (arXiv:2310.12135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12135">
<div class="article-summary-box-inner">
<span><p>With large language models surpassing human performance on an increasing
number of benchmarks, we must take a principled approach for targeted
evaluation of model capabilities. Inspired by pseudorandomness, we propose
pseudointelligence, which captures the maxim that "(perceived) intelligence
lies in the eye of the beholder". That is, that claims of intelligence are
meaningful only when their evaluator is taken into account. Concretely, we
propose a complexity-theoretic framework of model evaluation cast as a dynamic
interaction between a model and a learned evaluator. We demonstrate that this
framework can be used to reason about two case studies in language model
evaluation, as well as analyze existing evaluation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12143">
<div class="article-summary-box-inner">
<span><p>Deep networks typically learn concepts via classifiers, which involves
setting up a model and training it via gradient descent to fit the
concept-labeled data. We will argue instead that learning a concept could be
done by looking at its moment statistics matrix to generate a concrete
representation or signature of that concept. These signatures can be used to
discover structure across the set of concepts and could recursively produce
higher-level concepts by learning this structure from those signatures. When
the concepts are `intersected', signatures of the concepts can be used to find
a common theme across a number of related `intersected' concepts. This process
could be used to keep a dictionary of concepts so that inputs could correctly
identify and be routed to the set of concepts involved in the (latent)
generation of the input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12150">
<div class="article-summary-box-inner">
<span><p>We present a study of retrieval-augmented language models (LMs) on long-form
question answering. We analyze how retrieval augmentation impacts different
LMs, by comparing answers generated from models while using the same evidence
documents, and how differing quality of retrieval document set impacts the
answers generated from the same LM. We study various attributes of generated
answers (e.g., fluency, length, variance) with an emphasis on the attribution
of generated long-form answers to in-context evidence documents. We collect
human annotations of answer attribution and evaluate methods for automatically
judging attribution. Our study provides new insights on how retrieval
augmentation impacts long, knowledge-rich text generation of LMs. We further
identify attribution patterns for long text generation and analyze the main
culprits of attribution errors. Together, our analysis reveals how retrieval
augmentation impacts long knowledge-rich text generation and provide directions
for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13877">
<div class="article-summary-box-inner">
<span><p>We introduce YATO, an open-source, easy-to-use toolkit for text analysis with
deep learning. Different from existing heavily engineered toolkits and
platforms, YATO is lightweight and user-friendly for researchers from
cross-disciplinary areas. Designed in a hierarchical structure, YATO supports
free combinations of three types of widely used features including 1)
traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models
(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a
simple configurable file. Benefiting from the advantages of flexibility and
ease of use, YATO can facilitate fast reproduction and refinement of
state-of-the-art NLP models, and promote the cross-disciplinary applications of
NLP techniques. The code, examples, and documentation are publicly available at
https://github.com/jiesutd/YATO. A demo video is also available at
https://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Narrowing the Compositionality Gap in Language Models. (arXiv:2210.03350v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03350">
<div class="article-summary-box-inner">
<span><p>We investigate the ability of language models to perform compositional
reasoning tasks where the overall solution depends on correctly composing the
answers to sub-problems. We measure how often models can correctly answer all
sub-problems but not generate the overall solution, a ratio we call the
compositionality gap. We evaluate this ratio by asking multi-hop questions with
answers that require composing multiple facts unlikely to have been observed
together during pretraining. In the GPT-3 family of models, as model size
increases we show that the single-hop question answering performance improves
faster than the multi-hop performance does, therefore the compositionality gap
does not decrease. This surprising result suggests that while more powerful
models memorize and recall more factual knowledge, they show no corresponding
improvement in their ability to perform this kind of compositional reasoning.
</p>
<p>We then demonstrate how elicitive prompting (such as chain of thought)
narrows the compositionality gap by reasoning explicitly. We present a new
method, self-ask, that further improves on chain of thought. In our method, the
model explicitly asks itself (and answers) follow-up questions before answering
the initial question. We finally show that self-ask's structured prompting lets
us easily plug in a search engine to answer the follow-up questions, which
additionally improves accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15237">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a realistic semantic network called seq2seq-SC,
designed to be compatible with 5G NR and capable of working with generalized
text datasets using a pre-trained language model. The goal is to achieve
unprecedented communication efficiency by focusing on the meaning of messages
in semantic communication. We employ a performance metric called semantic
similarity, measured by BLEU for lexical similarity and SBERT for semantic
similarity. Our findings demonstrate that seq2seq-SC outperforms previous
models in extracting semantically meaningful information while maintaining
superior performance. This study paves the way for continued advancements in
semantic communication and its prospective incorporation with future wireless
systems in 6G networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10764">
<div class="article-summary-box-inner">
<span><p>Domain adaptation aims to transfer the knowledge learned on (data-rich)
source domains to (low-resource) target domains, and a popular method is
invariant representation learning, which matches and aligns the data
distributions on the feature space. Although this method is studied extensively
and applied on classification and regression problems, its adoption on ranking
problems is sporadic, and the few existing implementations lack theoretical
justifications. This paper revisits invariant representation learning for
ranking. Upon reviewing prior work, we found that they implement what we call
item-level alignment, which aligns the distributions of the items being ranked
from all lists in aggregate but ignores their list structure. However, the list
structure should be leveraged, because it is intrinsic to ranking problems
where the data and the metrics are defined and computed on lists, not the items
by themselves. To close this discrepancy, we propose list-level alignment --
learning domain-invariant representations at the higher level of lists. The
benefits are twofold: it leads to the first domain adaptation generalization
bound for ranking, in turn providing theoretical support for the proposed
method, and it achieves better empirical transfer performance for unsupervised
domain adaptation on ranking tasks, including passage reranking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v8 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02676">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is important for language models to match
human needs and to align with human and social values. Prior works have
achieved remarkable successes by learning from human feedback to understand and
follow instructions. Nonetheless, these methods are either founded on
hand-picked model generations that are favored by human annotators, rendering
them inefficient in terms of data utilization and challenging to apply in
general, or they depend on reinforcement learning, which often suffers from
imperfect reward functions and relies on extremely challenging optimizations.
In this work, we propose a novel technique, Chain of Hindsight, that is easy to
optimize and can learn from any form of feedback, regardless of its polarity.
Our idea is inspired by how humans learn from extensive feedback presented in
the form of languages. We convert all types of feedback into sequences of
sentences, which are then used to fine-tune the model, allowing us to take
advantage of the language comprehension capabilities of language models. We
condition the model on a sequence of model generations paired with feedback. By
doing so, the model is trained to generate outputs based on feedback, while
learning to identify and correct negative attributes or errors. Applying our
method to large language models, we observed that Chain of Hindsight
significantly surpasses previous methods in aligning language models with human
preferences. We report significant improvements on summarization and dialogue
benchmarks, with our approach markedly preferred in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting. (arXiv:2302.04813v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04813">
<div class="article-summary-box-inner">
<span><p>Recent work has shown how to prompt large language models with explanations
to obtain strong performance on textual reasoning tasks, i.e., the
chain-of-thought paradigm. However, subtly different explanations can yield
widely varying downstream task accuracy. Explanations that have not been
"tuned" for a task, such as off-the-shelf explanations written by nonexperts,
may lead to mediocre performance. This paper tackles the problem of how to
optimize explanation-infused prompts in a blackbox fashion. We first generate
sets of candidate explanations for each example in the prompt using a
leave-one-out scheme, then find an effective combination of these explanations
with a two-stage framework. We first evaluate explanations for each in-context
example in isolation according to two proxy metrics, log likelihood and
accuracy on new examples. Then, we search over combinations of explanations to
find one that yields high performance against a silver-labeled development set.
Across four textual reasoning tasks spanning question answering, mathematical
reasoning, and natural language inference, results show that our proxy metrics
correlate with ground truth accuracy and our overall method can effectively
improve prompts over crowdworker annotations and naive search strategies
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09473">
<div class="article-summary-box-inner">
<span><p>While recent progress in video-text retrieval has been advanced by the
exploration of better representation learning, in this paper, we present a
novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse
space shared between the video and the text for video-text retrieval. The
shared sparse space is initialized with a finite number of sparse concepts,
each of which refers to a number of words. With the text data at hand, we learn
and update the shared sparse space in a supervised manner using the proposed
similarity and alignment losses. Moreover, to enable multi-grained alignment,
we incorporate frame representations for better modeling the video modality and
calculating fine-grained and coarse-grained similarities. Benefiting from the
learned shared sparse space and multi-grained similarities, extensive
experiments on several video-text retrieval benchmarks demonstrate the
superiority of S3MA over existing methods. Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments. (arXiv:2302.11649v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11649">
<div class="article-summary-box-inner">
<span><p>Grounding navigational commands to linear temporal logic (LTL) leverages its
unambiguous semantics for reasoning about long-horizon tasks and verifying the
satisfaction of temporal constraints. Existing approaches require training data
from the specific environment and landmarks that will be used in natural
language to understand commands in those environments. We propose Lang2LTL, a
modular system and a software package that leverages large language models
(LLMs) to ground temporal navigational commands to LTL specifications in
environments without prior language data. We comprehensively evaluate Lang2LTL
for five well-defined generalization behaviors. Lang2LTL demonstrates the
state-of-the-art ability of a single model to ground navigational commands to
diverse temporal specifications in 21 city-scaled environments. Finally, we
demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse
navigational commands in two indoor environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01580">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) effectively generate fluent text when the target
output follows natural language patterns. However, structured prediction tasks
confine the output format to a limited ontology, causing even very large models
to struggle since they were never trained with such restrictions in mind. The
difficulty of using LLMs for direct prediction is exacerbated in few-shot
learning scenarios, which commonly arise due to domain shift and resource
limitations. We flip the problem on its head by leveraging the LLM as a tool
for data augmentation rather than direct prediction. Our proposed Mixture of
Soft Prompts (MSP) serves as a parameter-efficient procedure for generating
data in a controlled manner. Denoising mechanisms are further applied to
improve the quality of synthesized data. Automatic metrics show our method is
capable of producing diverse and natural text, while preserving label
semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks
when compared against strong baselines. Our method offers an alternate
data-centric approach for applying LLMs to complex prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13408">
<div class="article-summary-box-inner">
<span><p>The rise in malicious usage of large language models, such as fake content
creation and academic plagiarism, has motivated the development of approaches
that identify AI-generated text, including those based on watermarking or
outlier detection. However, the robustness of these detection algorithms to
paraphrases of AI-generated text remains unclear. To stress test these
detectors, we build a 11B parameter paraphrase generation model (DIPPER) that
can paraphrase paragraphs, condition on surrounding context, and control
lexical diversity and content reordering. Using DIPPER to paraphrase text
generated by three large language models (including GPT3.5-davinci-003)
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection
accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of
1%), without appreciably modifying the input semantics.
</p>
<p>To increase the robustness of AI-generated text detection to paraphrase
attacks, we introduce a simple defense that relies on retrieving
semantically-similar generations and must be maintained by a language model API
provider. Given a candidate text, our algorithm searches a database of
sequences previously generated by the API, looking for sequences that match the
candidate text within a certain threshold. We empirically verify our defense
using a database of 15M generations from a fine-tuned T5-XXL model and find
that it can detect 80% to 97% of paraphrased generations across different
settings while only classifying 1% of human-written sequences as AI-generated.
We open-source our models, code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11060">
<div class="article-summary-box-inner">
<span><p>We present SkillGPT, a tool for skill extraction and standardization (SES)
from free-style job descriptions and user profiles with an open-source Large
Language Model (LLM) as backbone. Most previous methods for similar tasks
either need supervision or rely on heavy data-preprocessing and feature
engineering. Directly prompting the latest conversational LLM for standard
skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes
a LLM to perform its tasks in steps via summarization and vector similarity
search, to balance speed with precision. The backbone LLM of SkillGPT is based
on Llama, free for academic use and thus useful for exploratory research and
prototype development. Hence, our cost-free SkillGPT gives users the
convenience of conversational SES, efficiently and reliably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13007">
<div class="article-summary-box-inner">
<span><p>Modern systems for multi-hop question answering (QA) typically break
questions into a sequence of reasoning steps, termed chain-of-thought (CoT),
before arriving at a final answer. Often, multiple chains are sampled and
aggregated through a voting mechanism over the final answers, but the
intermediate steps themselves are discarded. While such approaches improve
performance, they do not consider the relations between intermediate steps
across chains and do not provide a unified explanation for the predicted
answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts
large language models to meta-reason over multiple chains of thought, rather
than aggregating their answers. MCR examines different reasoning chains, mixes
information between them and selects the most relevant facts in generating an
explanation and predicting the answer. MCR outperforms strong baselines on 7
multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations
exhibit high quality, enabling humans to verify its answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14770">
<div class="article-summary-box-inner">
<span><p>Universal Information Extraction (UIE) is an area of interest due to the
challenges posed by varying targets, heterogeneous structures, and
demand-specific schemas. However, previous works have only achieved limited
success by unifying a few tasks, such as Named Entity Recognition (NER) and
Relation Extraction (RE), which fall short of being authentic UIE models
particularly when extracting other general schemas such as quadruples and
quintuples. Additionally, these models used an implicit structural schema
instructor, which could lead to incorrect links between types, hindering the
model's generalization and performance in low-resource scenarios. In this
paper, we redefine the authentic UIE with a formal formulation that encompasses
almost all extraction schemas. To the best of our knowledge, we are the first
to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which
is a Recursive Method with Explicit Schema Instructor for UIE. To avoid
interference between different types, we reset the position ids and attention
mask matrices. RexUIE shows strong performance under both full-shot and
few-shot settings and achieves State-of-the-Art results on the tasks of
extracting complex schemas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization. (arXiv:2305.01951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01951">
<div class="article-summary-box-inner">
<span><p>Recent pre-trained language models (PLMs) achieve promising results in
existing abstractive summarization datasets. However, existing summarization
benchmarks overlap in time with the standard pre-training corpora and
finetuning datasets. Hence, the strong performance of PLMs may rely on the
parametric knowledge that is memorized during pre-training and fine-tuning.
Moreover, the knowledge memorized by PLMs may quickly become outdated, which
affects the generalization performance of PLMs on future data. In this work, we
propose TempoSum, a novel benchmark that contains data samples from 2010 to
2022, to understand the temporal generalization ability of abstractive
summarization models. Through extensive human evaluation, we show that
parametric knowledge stored in summarization models significantly affects the
faithfulness of the generated summaries on future data. Moreover, existing
faithfulness enhancement methods cannot reliably improve the faithfulness of
summarization models on future data. Finally, we discuss several
recommendations to the research community on how to evaluate and improve the
temporal generalization capability of text summarization models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03695">
<div class="article-summary-box-inner">
<span><p>Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10160">
<div class="article-summary-box-inner">
<span><p>Data contamination has become prevalent and challenging with the rise of
models pretrained on large automatically-crawled corpora. For closed models,
the training data becomes a trade secret, and even for open models, it is not
trivial to detect contamination. Strategies such as leaderboards with hidden
answers, or using test data which is guaranteed to be unseen, are expensive and
become fragile with time. Assuming that all relevant actors value clean test
data and will cooperate to mitigate data contamination, what can be done? We
propose three strategies that can make a difference: (1) Test data made public
should be encrypted with a public key and licensed to disallow derivative
distribution; (2) demand training exclusion controls from closed API holders,
and protect your test data by refusing to evaluate without them; (3) avoid data
which appears with its solution on the internet, and release the web-page
context of internet-derived data along with the data. These strategies are
practical and can be effective in preventing data contamination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11096">
<div class="article-summary-box-inner">
<span><p>End-to-end sign language translation (SLT) aims to convert sign language
videos into spoken language texts directly without intermediate
representations. It has been a challenging task due to the modality gap between
sign videos and texts and the data scarcity of labeled data. To tackle these
challenges, we propose a novel Cross-modality Data Augmentation (XmDA)
framework to transfer the powerful gloss-to-text translation capabilities to
end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo
gloss-text pairs from the sign gloss translation model. Specifically, XmDA
consists of two key components, namely, cross-modality mix-up and
cross-modality knowledge distillation. The former explicitly encourages the
alignment between sign video features and gloss embeddings to bridge the
modality gap. The latter utilizes the generation knowledge from gloss-to-text
teacher models to guide the spoken language text generation. Experimental
results on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,
demonstrate that the proposed XmDA framework significantly and consistently
outperforms the baseline models. Extensive analyses confirm our claim that XmDA
enhances spoken language text generation by reducing the representation
distance between videos and texts, as well as improving the processing of
low-frequency words and long sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning. (arXiv:2305.11508v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11508">
<div class="article-summary-box-inner">
<span><p>The patient-centered medical dialogue systems strive to offer diagnostic
interpretation services to users who are less knowledgeable about medical
knowledge, through emphasizing the importance of providing responses specific
to the patients. It is difficult for the large language models (LLMs) to
guarantee the specificity of responses in spite of its promising performance
even in some tasks in medical field. Inspired by in-context learning, we
propose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this
challenge. PlugMed is equipped with two modules, the prompt generation (PG)
module and the response ranking (RR) module, to enhances LLMs' dialogue
strategies for improving the specificity of the dialogue. The PG module is
designed to stimulate the imitative ability of LLMs by providing them with real
dialogues from similar patients as prompts. The RR module incorporates
fine-tuned small model as response filter to enable the selection of
appropriate responses generated by LLMs. Furthermore, we introduce a new
evaluation method based on matching both user's intent and high-frequency
medical term to effectively assess the specificity of the responses. We conduct
experimental evaluations on three medical dialogue datasets, and the results,
including both automatic and human evaluation, demonstrate the effectiveness of
our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate. (arXiv:2305.11595v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11595">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown impressive capabilities in various
applications, but they still face various inconsistency issues. Existing works
primarily focus on the inconsistency issues within a single LLM, while we
complementarily explore the inter-consistency among multiple LLMs for
collaboration. To examine whether LLMs can collaborate effectively to achieve a
consensus for a shared goal, we focus on commonsense reasoning, and introduce a
formal debate framework (FORD) to conduct a three-stage debate among LLMs with
real-world scenarios alignment: fair debate, mismatched debate, and roundtable
debate. Through extensive experiments on various datasets, LLMs can effectively
collaborate to reach a consensus despite noticeable inter-inconsistencies, but
imbalances in their abilities can lead to domination by superior LLMs.
Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost
collaboration performance. Our work contributes to understanding the
inter-consistency among LLMs and lays the foundation for developing future
collaboration methods. Codes and data are available at
https://github.com/Waste-Wood/FORD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11828">
<div class="article-summary-box-inner">
<span><p>Medical systematic reviews play a vital role in healthcare decision making
and policy. However, their production is time-consuming, limiting the
availability of high-quality and up-to-date evidence summaries. Recent
advancements in large language models (LLMs) offer the potential to
automatically generate literature reviews on demand, addressing this issue.
However, LLMs sometimes generate inaccurate (and potentially misleading) texts
by hallucination or omission. In healthcare, this can make LLMs unusable at
best and dangerous at worst. We conducted 16 interviews with international
systematic review experts to characterize the perceived utility and risks of
LLMs in the specific context of medical evidence reviews. Experts indicated
that LLMs can assist in the writing process by drafting summaries, generating
templates, distilling information, and crosschecking information. They also
raised concerns regarding confidently composed but inaccurate LLM outputs and
other potential downstream harms, including decreased accountability and
proliferation of low-quality reviews. Informed by this qualitative analysis, we
identify criteria for rigorous evaluation of biomedical LLMs aligned with
domain expert views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12084">
<div class="article-summary-box-inner">
<span><p>The uniform information density (UID) hypothesis states that humans tend to
distribute information roughly evenly across an utterance or discourse. Early
evidence in support of the UID hypothesis came from Genzel &amp; Charniak (2002),
which proposed an entropy rate constancy principle based on the probability of
English text under n-gram language models. We re-evaluate the claims of Genzel
&amp; Charniak (2002) with neural language models, failing to find clear evidence
in support of entropy rate constancy. We conduct a range of experiments across
datasets, model sizes, and languages and discuss implications for the uniform
information density hypothesis and linguistic theories of efficient
communication more broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization. (arXiv:2305.12169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12169">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that sequence-to-sequence (seq2seq) models struggle
with compositional generalization (CG), i.e., the ability to systematically
generalize to unseen compositions of seen components. There is mounting
evidence that one of the reasons hindering CG is the representation of the
encoder uppermost layer is entangled, i.e., the syntactic and semantic
representations of sequences are entangled. However, we consider that the
previously identified representation entanglement problem is not comprehensive
enough. Additionally, we hypothesize that the source keys and values
representations passing into different decoder layers are also entangled.
Starting from this intuition, we propose \textsc{CompoSition} (\textbf{Compo}se
\textbf{S}yntactic and Semant\textbf{i}c Representa\textbf{tion}s), an
extension to seq2seq models which learns to compose representations of
different encoder layers dynamically for different tasks, since recent studies
reveal that the bottom layers of the Transformer encoder contain more syntactic
information and the top ones contain more semantic information. Specifically,
we introduce a \textit{composed layer} between the encoder and decoder to
compose different encoder layers' representations to generate specific keys and
values passing into different decoder layers. \textsc{CompoSition} achieves
competitive results on two comprehensive and realistic benchmarks, which
empirically demonstrates the effectiveness of our proposal. Codes are available
at~\url{https://github.com/thinkaboutzero/COMPOSITION}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge. (arXiv:2305.12212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12212">
<div class="article-summary-box-inner">
<span><p>Multimodal Named Entity Recognition (MNER) on social media aims to enhance
textual entity prediction by incorporating image-based clues. Existing studies
mainly focus on maximizing the utilization of pertinent image information or
incorporating external knowledge from explicit knowledge bases. However, these
methods either neglect the necessity of providing the model with external
knowledge, or encounter issues of high redundancy in the retrieved knowledge.
In this paper, we present PGIM -- a two-stage framework that aims to leverage
ChatGPT as an implicit knowledge base and enable it to heuristically generate
auxiliary knowledge for more efficient entity prediction. Specifically, PGIM
contains a Multimodal Similar Example Awareness module that selects suitable
examples from a small number of predefined artificial samples. These examples
are then integrated into a formatted prompt template tailored to the MNER and
guide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired
knowledge is integrated with the original text and fed into a downstream model
for further processing. Extensive experiments show that PGIM outperforms
state-of-the-art methods on two classic MNER datasets and exhibits a stronger
robustness and generalization capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Simplification of Medical Texts. (arXiv:2305.12532v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12532">
<div class="article-summary-box-inner">
<span><p>Automated text simplification aims to produce simple versions of complex
texts. This task is especially useful in the medical domain, where the latest
medical findings are typically communicated via complex and technical articles.
This creates barriers for laypeople seeking access to up-to-date medical
findings, consequently impeding progress on health literacy. Most existing work
on medical text simplification has focused on monolingual settings, with the
result that such evidence would be available only in just one language (most
often, English). This work addresses this limitation via multilingual
simplification, i.e., directly simplifying complex texts into simplified texts
in multiple languages. We introduce MultiCochrane, the first sentence-aligned
multilingual text simplification dataset for the medical domain in four
languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and
zero-shot models across these languages, with extensive human assessments and
analyses. Although models can now generate viable simplified texts, we identify
outstanding challenges that this dataset might be used to address.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation. (arXiv:2305.12733v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12733">
<div class="article-summary-box-inner">
<span><p>Modeling multi-party conversations (MPCs) with graph neural networks has been
proven effective at capturing complicated and graphical information flows.
However, existing methods rely heavily on the necessary addressee labels and
can only be applied to an ideal setting where each utterance must be tagged
with an addressee label. To study the scarcity of addressee labels which is a
common issue in MPCs, we propose MADNet that maximizes addressee deduction
expectation in heterogeneous graph neural networks for MPC generation. Given an
MPC with a few addressee labels missing, existing methods fail to build a
consecutively connected conversation graph, but only a few separate
conversation fragments instead. To ensure message passing between these
conversation fragments, four additional types of latent edges are designed to
complete a fully-connected graph. Besides, to optimize the edge-type-dependent
message passing for those utterances without addressee labels, an
Expectation-Maximization-based method that iteratively generates silver
addressee labels (E step), and optimizes the quality of generated responses (M
step), is designed. Experimental results on two Ubuntu IRC channel benchmarks
show that MADNet outperforms various baseline models on the task of MPC
generation, especially under the more common and challenging setting where part
of addressee labels are missing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14124">
<div class="article-summary-box-inner">
<span><p>Multilingual machine translation (MMT), trained on a mixture of parallel and
monolingual data, is key for improving translation in low-resource language
pairs. However, the literature offers conflicting results on the performance of
different methods of including monolingual data. To resolve this, we examine
how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under
different data conditions and model scales. Unlike prior studies, we use a
realistic dataset of 100 translation directions and consider many domain
combinations of monolingual and test data. We find that monolingual data
generally helps MMT, but models are surprisingly brittle to domain mismatches,
especially at smaller model scales. BT is beneficial when the parallel,
monolingual, and test data sources are similar but can be detrimental
otherwise, while DAE is less effective than previously reported. Next, we
analyze the impact of scale (from 90M to 1.6B parameters) and find it is
important for both methods, particularly DAE. As scale increases, DAE
transitions from underperforming the parallel-only baseline at 90M to
converging with BT performance at 1.6B, and even surpassing it in low-resource.
These results offer new insights into how to best use monolingual data in MMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14221">
<div class="article-summary-box-inner">
<span><p>Question answering plays a pivotal role in human daily life because it
involves our acquisition of knowledge about the world. However, due to the
dynamic and ever-changing nature of real-world facts, the answer can be
completely different when the time constraint in the question changes.
Recently, Large Language Models (LLMs) have shown remarkable intelligence in
question answering, while our experiments reveal that the aforementioned
problems still pose a significant challenge to existing LLMs. This can be
attributed to the LLMs' inability to perform rigorous reasoning based on
surface-level text semantics. To overcome this limitation, rather than
requiring LLMs to directly answer the question, we propose a novel approach
where we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task
$\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, by
leveraging modern LLMs' superior capability in understanding both natural
language and programming language, we endeavor to harness LLMs to represent
diversely expressed text as well-structured code and select the best matching
answer from multiple candidates through programming. We evaluate our QAaP
framework on several time-sensitive question answering datasets and achieve
decent improvement, up to $14.5$% over strong baselines. Our codes and data are
available at https://github.com/TianHongZXY/qaap
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14577">
<div class="article-summary-box-inner">
<span><p>The self-supervised objective of masking-and-predicting has led to promising
performance gains on a variety of downstream tasks. However, while most
approaches randomly mask tokens, there is strong intuition that deciding what
to mask can substantially improve learning outcomes. We investigate this in
continued pretraining setting in which pretrained models continue to pretrain
on domain-specific data before performing some downstream task. We introduce
Difference-Masking, a masking strategy that automatically chooses what to mask
during continued pretraining by considering what makes a task domain different
from the pretraining domain. Empirically, we find that Difference-Masking
outperforms baselines on continued pretraining settings across four diverse
language-only and multimodal video tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey. (arXiv:2305.18703v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18703">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04528">
<div class="article-summary-box-inner">
<span><p>The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs' resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. The adversarial
prompts, crafted to mimic plausible user errors like typos or synonyms, aim to
evaluate how slight deviations can affect LLM outcomes while maintaining
semantic integrity. These prompts are then employed in diverse tasks, such as
sentiment analysis, natural language inference, reading comprehension, machine
translation, and math problem-solving. Our study generates 4788 adversarial
prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings
demonstrate that contemporary LLMs are not robust to adversarial prompts.
Furthermore, we present comprehensive analysis to understand the mystery behind
prompt robustness and its transferability. We then offer insightful robustness
analysis and pragmatic recommendations for prompt composition, beneficial to
both researchers and everyday users. Code is available at:
https://github.com/microsoft/promptbench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15895">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been recently leveraged as training data
generators for various natural language processing (NLP) tasks. While previous
research has explored different approaches to training models using generated
data, they generally rely on simple class-conditional prompts, which may limit
the diversity of the generated data and inherit systematic biases of LLM. Thus,
we investigate training data generation with diversely attributed prompts
(e.g., specifying attributes like length and style), which have the potential
to yield diverse and attributed generated data. Our investigation focuses on
datasets with high cardinality and diverse domains, wherein we demonstrate that
attributed prompts outperform simple class-conditional prompts in terms of the
resulting model's performance. Additionally, we present a comprehensive
empirical study on data generation encompassing vital aspects like bias,
diversity, and efficiency, and highlight three key observations: firstly,
synthetic datasets generated by simple prompts exhibit significant biases, such
as regional bias; secondly, attribute diversity plays a pivotal role in
enhancing model performance; lastly, attributed prompts achieve the performance
of simple class-conditional prompts while utilizing only 5\% of the querying
cost of ChatGPT associated with the latter. The data and code are available on
\url{https://github.com/yueyu1030/AttrPrompt}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07280">
<div class="article-summary-box-inner">
<span><p>While Automatic Speech Recognition (ASR) models have shown significant
advances with the introduction of unsupervised or self-supervised training
techniques, these improvements are still only limited to a subsection of
languages and speakers. Transfer learning enables the adaptation of large-scale
multilingual models to not only low-resource languages but also to more
specific speaker groups. However, fine-tuning on data from new domains is
usually accompanied by a decrease in performance on the original domain.
Therefore, in our experiments, we examine how well the performance of
large-scale ASR models can be approximated for smaller domains, with our own
dataset of German Senior Voice Commands (SVC-de), and how much of the general
speech recognition performance can be preserved by selectively freezing parts
of the model during training. To further increase the robustness of the ASR
model to vocabulary and speakers outside of the fine-tuned domain, we apply
Experience Replay for continual learning. By adding only a fraction of data
from the original domain, we are able to reach Word-Error-Rates (WERs) below
5\% on the new domain, while stabilizing performance for general speech
recognition at acceptable WERs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08813">
<div class="article-summary-box-inner">
<span><p>Understanding protein interactions and pathway knowledge is crucial for
unraveling the complexities of living systems and investigating the underlying
mechanisms of biological functions and complex diseases. While existing
databases provide curated biological data from literature and other sources,
they are often incomplete and their maintenance is labor-intensive,
necessitating alternative approaches. In this study, we propose to harness the
capabilities of large language models to address these issues by automatically
extracting such knowledge from the relevant scientific literature. Toward this
goal, in this work, we investigate the effectiveness of different large
language models in tasks that involve recognizing protein interactions,
identifying genes associated with pathways affected by low-dose radiation, and
gene regulatory relations. We thoroughly evaluate the performance of various
models, highlight the significant findings, and discuss both the future
opportunities and the remaining challenges associated with this approach. The
code and data are available at: https://github.com/boxorange/BioIE-LLM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12444">
<div class="article-summary-box-inner">
<span><p>Generative Artificial Intelligence is set to revolutionize healthcare
delivery by transforming traditional patient care into a more personalized,
efficient, and proactive process. Chatbots, serving as interactive
conversational models, will probably drive this patient-centered transformation
in healthcare. Through the provision of various services, including diagnosis,
personalized lifestyle recommendations, and mental health support, the
objective is to substantially augment patient health outcomes, all the while
mitigating the workload burden on healthcare providers. The life-critical
nature of healthcare applications necessitates establishing a unified and
comprehensive set of evaluation metrics for conversational models. Existing
evaluation metrics proposed for various generic large language models (LLMs)
demonstrate a lack of comprehension regarding medical and health concepts and
their significance in promoting patients' well-being. Moreover, these metrics
neglect pivotal user-centered aspects, including trust-building, ethics,
personalization, empathy, user comprehension, and emotional support. The
purpose of this paper is to explore state-of-the-art LLM-based evaluation
metrics that are specifically applicable to the assessment of interactive
conversational models in healthcare. Subsequently, we present an comprehensive
set of evaluation metrics designed to thoroughly assess the performance of
healthcare chatbots from an end-user perspective. These metrics encompass an
evaluation of language processing abilities, impact on real-world clinical
tasks, and effectiveness in user-interactive conversations. Finally, we engage
in a discussion concerning the challenges associated with defining and
implementing these metrics, with particular emphasis on confounding factors
such as the target audience, evaluation methods, and prompt techniques involved
in the evaluation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't throw away your value model! Making PPO even better via Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15028">
<div class="article-summary-box-inner">
<span><p>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may
seem unnecessary when generating natural language text based on
state-of-the-art reinforcement learning such as Proximal Policy Optimization
(PPO). In this paper, we demonstrate that it is possible to get extra mileage
out of PPO by integrating MCTS on top. The key idea is not to throw out the
value network, a byproduct of PPO training for evaluating partial output
sequences, when decoding text out of the policy network. More concretely, we
present a novel value-guided decoding algorithm called PPO-MCTS, which can
integrate the value network from PPO to work closely with the policy network
during inference-time generation. Compared to prior approaches based on MCTS
for controlled text generation, the key strength of our approach is to reduce
the fundamental mismatch of the scoring mechanisms of the partial outputs
between training and test. Evaluation on four text generation tasks demonstrate
that PPO-MCTS greatly improves the preferability of generated text compared to
the standard practice of using only the PPO policy. Our results demonstrate the
promise of search algorithms even on top of the aligned language models from
PPO, and the under-explored benefit of the value network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00313">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) exhibit remarkable performance improvement
through in-context learning (ICL) by leveraging task-specific examples in the
input. However, the mechanisms behind this improvement remain elusive. In this
work, we investigate embeddings and attention representations in Llama-2 70B
and Vicuna 13B. Specifically, we study how embeddings and attention change
after in-context-learning, and how these changes mediate improvement in
behavior. We employ neuroscience-inspired techniques, such as representational
similarity analysis (RSA), and propose novel methods for parameterized probing
and attention ratio analysis (ARA, measuring the ratio of attention to relevant
vs. irrelevant information). We designed three tasks with a priori
relationships among their conditions: reading comprehension, linear regression,
and adversarial prompt injection. We formed hypotheses about expected
similarities in task representations to investigate latent changes in
embeddings and attention. Our analyses revealed a meaningful correlation
between changes in both embeddings and attention representations with
improvements in behavioral performance after ICL. This empirical framework
empowers a nuanced understanding of how latent representations affect LLM
behavior with and without ICL, offering valuable tools and insights for future
research and practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04921">
<div class="article-summary-box-inner">
<span><p>Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
"chain-of-thought" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Storytelling with Question-Answer Plans. (arXiv:2310.05295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05295">
<div class="article-summary-box-inner">
<span><p>Visual storytelling aims to generate compelling narratives from image
sequences. Existing models often focus on enhancing the representation of the
image sequence, e.g., with external knowledge sources or advanced graph
structures. Despite recent progress, the stories are often repetitive,
illogical, and lacking in detail. To mitigate these issues, we present a novel
framework which integrates visual representations with pretrained language
models and planning. Our model translates the image sequence into a visual
prefix, a sequence of continuous embeddings which language models can
interpret. It also leverages a sequence of question-answer pairs as a blueprint
plan for selecting salient visual concepts and determining how they should be
assembled into a narrative. Automatic and human evaluation on the VIST
benchmark (Huang et al., 2016) demonstrates that blueprint-based models
generate stories that are more coherent, interesting, and natural compared to
competitive baselines and state-of-the-art systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Speculative Decoding. (arXiv:2310.07177v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07177">
<div class="article-summary-box-inner">
<span><p>Speculative decoding is a pivotal technique to accelerate the inference of
large language models (LLMs) by employing a smaller draft model to predict the
target model's outputs. However, its efficacy can be limited due to the low
predictive accuracy of the draft model, particularly when faced with diverse
text inputs and a significant capability gap between the draft and target
models. We introduce online speculative decoding (OSD) to address this
challenge. The main idea is to continually update (multiple) draft model(s) on
observed user query data using the abundant excess computational power in an
LLM serving cluster. Given that LLM inference is memory-bounded, the surplus
computational power in a typical LLM serving cluster can be repurposed for
online retraining of draft models, thereby making the training cost-neutral.
Since the query distribution of an LLM service is relatively simple, retraining
on query distribution enables the draft model to more accurately predict the
target model's outputs, particularly on data originating from query
distributions. As the draft model evolves online, it aligns with the query
distribution in real time, mitigating distribution shifts. We develop a
prototype of online speculative decoding based on online knowledge distillation
and evaluate it using both synthetic and real query data on several popular
LLMs. The results show a substantial increase in the token acceptance rate by
0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07487">
<div class="article-summary-box-inner">
<span><p>Phonological reconstruction is one of the central problems in historical
linguistics where a proto-word of an ancestral language is determined from the
observed cognate words of daughter languages. Computational approaches to
historical linguistics attempt to automate the task by learning models on
available linguistic data. Several ideas and techniques drawn from
computational biology have been successfully applied in the area of
computational historical linguistics. Following these lines, we adapt MSA
Transformer, a protein language model, to the problem of automated phonological
reconstruction. MSA Transformer trains on multiple sequence alignments as input
and is, thus, apt for application on aligned cognate words. We, hence, name our
model as Cognate Transformer. We also apply the model on another associated
task, namely, cognate reflex prediction, where a reflex word in a daughter
language is predicted based on cognate words from other daughter languages. We
show that our model outperforms the existing models on both tasks, especially
when it is pre-trained on masked word prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07521">
<div class="article-summary-box-inner">
<span><p>This survey addresses the crucial issue of factuality in Large Language
Models (LLMs). As LLMs find applications across diverse domains, the
reliability and accuracy of their outputs become vital. We define the
Factuality Issue as the probability of LLMs to produce content inconsistent
with established facts. We first delve into the implications of these
inaccuracies, highlighting the potential consequences and challenges posed by
factual errors in LLM outputs. Subsequently, we analyze the mechanisms through
which LLMs store and process facts, seeking the primary causes of factual
errors. Our discussion then transitions to methodologies for evaluating LLM
factuality, emphasizing key metrics, benchmarks, and studies. We further
explore strategies for enhancing LLM factuality, including approaches tailored
for specific domains. We focus two primary LLM configurations standalone LLMs
and Retrieval-Augmented LLMs that utilizes external data, we detail their
unique challenges and potential enhancements. Our survey offers a structured
guide for researchers aiming to fortify the factual reliability of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Expressive Power of Transformers with Chain of Thought. (arXiv:2310.07923v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07923">
<div class="article-summary-box-inner">
<span><p>Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a "chain of
thought" or "scratchpad", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps adds a
clear new ability (under standard complexity conjectures): recognizing all
regular languages. Our results also imply that linear steps keep transformer
decoders within context-sensitive languages, and polynomial steps make them
recognize exactly the class of polynomial-time solvable problems -- the first
exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach. (arXiv:2310.08172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08172">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have not only exhibited exceptional performance
across various tasks, but also demonstrated sparks of intelligence. Recent
studies have focused on assessing their capabilities on human exams and
revealed their impressive competence in different domains. However, cognitive
research on the overall knowledge structure of LLMs is still lacking. In this
paper, based on educational diagnostic assessment method, we conduct an
evaluation using MoocRadar, a meticulously annotated human test dataset based
on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain
insights of their cognitive capabilities. This research emphasizes the
significance of investigating LLMs' knowledge and understanding the disparate
cognitive patterns of LLMs. By shedding light on models' knowledge, researchers
can advance development and utilization of LLMs in a more informed and
effective manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09166">
<div class="article-summary-box-inner">
<span><p>Media bias has been extensively studied by both social and computational
sciences. However, current work still has a large reliance on human input and
subjective assessment to label biases. This is especially true for cable news
research. To address these issues, we develop an unsupervised machine learning
method to characterize the bias of cable news programs without any human input.
This method relies on the analysis of what topics are mentioned through Named
Entity Recognition and how those topics are discussed through Stance Analysis
in order to cluster programs with similar biases together. Applying our method
to 2020 cable news transcripts, we find that program clusters are consistent
over time and roughly correspond to the cable news network of the program. This
method reveals the potential for future tools to objectively assess media bias
and characterize unfamiliar media environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09238">
<div class="article-summary-box-inner">
<span><p>Bangla is the 7th most widely spoken language globally, with a staggering 234
million native speakers primarily hailing from India and Bangladesh. This
morphologically rich language boasts a rich literary tradition, encompassing
diverse dialects and language-specific challenges. Despite its linguistic
richness and history, Bangla remains categorized as a low-resource language
within the natural language processing (NLP) and speech community. This paper
presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media
Posts) of the BLP Workshop. We experiment with various Transformer-based
architectures to solve this task. Our quantitative results show that transfer
learning really helps in better learning of the models in this low-resource
language scenario. This becomes evident when we further finetune a model which
has already been finetuned on twitter data for sentiment analysis task and that
finetuned model performs the best among all other models. We also perform a
detailed error analysis where we find some instances where ground truth labels
need to be relooked at. We obtain a micro-F1 of 67.02\% on the test set and our
performance in this shared task is ranked at 21 in the leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09619">
<div class="article-summary-box-inner">
<span><p>Generating mathematical equations from natural language requires an accurate
understanding of the relations among math expressions. Existing approaches can
be broadly categorized into token-level and expression-level generation. The
former treats equations as a mathematical language, sequentially generating
math tokens. Expression-level methods generate each expression one by one.
However, each expression represents a solving step, and there naturally exist
parallel or dependent relations between these steps, which are ignored by
current sequential methods. Therefore, we integrate tree structure into the
expression-level generation and advocate an expression tree decoding strategy.
To generate a tree with expression as its node, we employ a layer-wise parallel
decoding strategy: we decode multiple independent expressions (leaf nodes) in
parallel at each layer and repeat parallel decoding layer by layer to
sequentially generate these parent node expressions that depend on others.
Besides, a bipartite matching algorithm is adopted to align multiple
predictions with annotations for each layer. Experiments show our method
outperforms other baselines, especially for these equations with complex
structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09716">
<div class="article-summary-box-inner">
<span><p>Query rewriting plays a vital role in enhancing conversational search by
transforming context-dependent user queries into standalone forms. Existing
approaches primarily leverage human-rewritten queries as labels to train query
rewriting models. However, human rewrites may lack sufficient information for
optimal retrieval performance. To overcome this limitation, we propose
utilizing large language models (LLMs) as query rewriters, enabling the
generation of informative query rewrites through well-designed instructions. We
define four essential properties for well-formed rewrites and incorporate all
of them into the instruction. In addition, we introduce the role of rewrite
editors for LLMs when initial query rewrites are available, forming a
"rewrite-then-edit" process. Furthermore, we propose distilling the rewriting
capabilities of LLMs into smaller models to reduce rewriting latency. Our
experimental evaluation on the QReCC dataset demonstrates that informative
query rewrites can yield substantially improved retrieval performance compared
to human rewrites, especially with sparse retrievers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10378">
<div class="article-summary-box-inner">
<span><p>Multilingual large-scale Pretrained Language Models (PLMs) have been shown to
store considerable amounts of factual knowledge, but large variations are
observed across languages. With the ultimate goal of ensuring that users with
different language backgrounds obtain consistent feedback from the same model,
we study the cross-lingual consistency (CLC) of factual knowledge in various
multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)
metric to evaluate knowledge consistency across languages independently from
accuracy. Using this metric, we conduct an in-depth analysis of the determining
factors for CLC, both at model level and at language-pair level. Among other
results, we find that increasing model size leads to higher factual probing
accuracy in most languages, but does not improve cross-lingual consistency.
Finally, we conduct a case study on CLC when new factual associations are
inserted in the PLMs via model editing. Results on a small sample of facts
inserted in English reveal a clear pattern whereby the new piece of knowledge
transfers only to languages with which English has a high RankC score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10449">
<div class="article-summary-box-inner">
<span><p>Text summarization is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation.
Leveraging Large Language Models (LLMs) has shown remarkable promise in
enhancing summarization techniques. This paper embarks on an exploration of
text summarization with a diverse set of LLMs, including MPT-7b-instruct,
falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment
was performed with different hyperparameters and evaluated the generated
summaries using widely accepted metrics such as the Bilingual Evaluation
Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) Score, and Bidirectional Encoder Representations from Transformers
(BERT) Score. According to the experiment, text-davinci-003 outperformed the
others. This investigation involved two distinct datasets: CNN Daily Mail and
XSum. Its primary objective was to provide a comprehensive understanding of the
performance of Large Language Models (LLMs) when applied to different datasets.
The assessment of these models' effectiveness contributes valuable insights to
researchers and practitioners within the NLP domain. This work serves as a
resource for those interested in harnessing the potential of LLMs for text
summarization and lays the foundation for the development of advanced
Generative AI applications aimed at addressing a wide spectrum of business
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10520">
<div class="article-summary-box-inner">
<span><p>Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10706">
<div class="article-summary-box-inner">
<span><p>To explore how humans can best leverage LLMs for writing and how interacting
with these models affects feelings of ownership and trust in the writing
process, we compared common human-AI interaction types (e.g., guiding system,
selecting from system outputs, post-editing outputs) in the context of
LLM-assisted news headline generation. While LLMs alone can generate
satisfactory news headlines, on average, human control is needed to fix
undesirable model outputs. Of the interaction methods, guiding and selecting
model output added the most benefit with the lowest cost (in time and effort).
Further, AI assistance did not harm participants' perception of control
compared to freeform editing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-19 23:11:18.334071416 UTC">2023-10-19 23:11:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>