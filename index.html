<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-26T01:30:00Z">05-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15425">
<div class="article-summary-box-inner">
<span><p>Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, concerns have been raised
about the quality of their outputs across different languages. In this paper,
we show how disparity in the treatment of different languages arises at the
tokenization stage, well before a model is even invoked. The same text
translated into different languages can have drastically different tokenization
lengths, with differences up to 15 times in some cases. These disparities
persist across the 17 tokenizers we evaluate, even if they are intentionally
trained for multilingual support. Character-level and byte-level models also
exhibit over 4 times the difference in the encoding length for some language
pairs. This induces unfair treatment for some language communities in regard to
the cost of accessing commercial language services, the processing time and
latency, as well as the amount of content that can be provided as context to
the models. Therefore, we make the case that we should train future language
models using multilingually fair tokenizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15444">
<div class="article-summary-box-inner">
<span><p>In a surprising turn, Large Language Models (LLMs) together with a growing
arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches
providing few-shot solutions to myriad classic NLP problems. However, despite
promising early results, these LLM-based few-shot methods remain far from the
state of the art in Named Entity Recognition (NER), where prevailing methods
include learning representations via end-to-end structural understanding and
fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,
a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to
any new NER task PromptNER requires a set of entity definitions in addition to
the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to
produce a list of potential entities along with corresponding explanations
justifying their compatibility with the provided entity type definitions.
Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,
achieving an 11% (absolute) improvement in F1 score on the ConLL dataset, and a
10% (absolute) improvement on the FewNERD dataset. PromptNER also moves the
state of the art on Cross Domain NER, outperforming all prior methods
(including those not limited to the few-shot setting), setting a new mark on
all 5 CrossNER target domains, with an average F1 gain of 9%, despite using
less than 2% of the available data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for User Interest Journeys. (arXiv:2305.15498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15498">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown impressive capabilities in natural
language understanding and generation. Their potential for deeper user
understanding and improved personalized user experience on recommendation
platforms is, however, largely untapped. This paper aims to address this gap.
Recommender systems today capture users' interests through encoding their
historical activities on the platforms. The generated user representations are
hard to examine or interpret. On the other hand, if we were to ask people about
interests they pursue in their life, they might talk about their hobbies, like
I just started learning the ukulele, or their relaxation routines, e.g., I like
to watch Saturday Night Live, or I want to plant a vertical garden. We argue,
and demonstrate through extensive experiments, that LLMs as foundation models
can reason through user activities, and describe their interests in nuanced and
interesting ways, similar to how a human would.
</p>
<p>We define interest journeys as the persistent and overarching user interests,
in other words, the non-transient ones. These are the interests that we believe
will benefit most from the nuanced and personalized descriptions. We introduce
a framework in which we first perform personalized extraction of interest
journeys, and then summarize the extracted journeys via LLMs, using techniques
like few-shot prompting, prompt-tuning and fine-tuning. Together, our results
in prompting LLMs to name extracted user journeys in a large-scale industrial
platform demonstrate great potential of these models in providing deeper, more
interpretable, and controllable user understanding. We believe LLM powered user
understanding can be a stepping stone to entirely new user experiences on
recommendation platforms that are journey-aware, assistive, and enabling
frictionless conversation down the line.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deriving Language Models from Masked Language Models. (arXiv:2305.15501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15501">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLM) do not explicitly define a distribution over
language, i.e., they are not language models per se. However, recent work has
implicitly treated them as such for the purposes of generation and scoring.
This paper studies methods for deriving explicit joint distributions from MLMs,
focusing on distributions over two tokens, which makes it possible to calculate
exact distributional properties. We find that an approach based on identifying
joints whose conditionals are closest to those of the MLM works well and
outperforms existing Markov random field-based approaches. We further find that
this derived model's conditionals can even occasionally outperform the original
MLM's conditionals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15507">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have successfully been applied to code
generation tasks, raising the question of how well these models understand
programming. Typical programming languages have invariances and equivariances
in their semantics that human programmers intuitively understand and exploit,
such as the (near) invariance to the renaming of identifiers. We show that LLMs
not only fail to properly generate correct Python code when default function
names are swapped, but some of them even become more confident in their
incorrect predictions as the model size increases, an instance of the recently
discovered phenomenon of Inverse Scaling, which runs contrary to the commonly
observed trend of increasing prediction quality with increasing model size. Our
findings indicate that, despite their astonishing typical-case performance,
LLMs still lack a deep, abstract understanding of the content they manipulate,
making them unsuitable for tasks that statistically deviate from their training
data, and that mere scaling is not enough to achieve such capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Free Lunch for Efficient Textual Commonsense Integration in Language Models. (arXiv:2305.15516v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15516">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the emergence of textual commonsense knowledge
bases, aimed at providing more nuanced and context-rich knowledge. The
integration of external commonsense into language models has been shown to be a
key enabler in advancing the state-of-the-art for a wide range of NLP tasks.
However, incorporating textual commonsense descriptions is computationally
expensive, as compared to encoding conventional symbolic knowledge. In this
paper, we propose a method to improve its efficiency without modifying the
model. We group training samples with similar commonsense descriptions into a
single batch, thus reusing the encoded description across multiple samples. One
key observation is that the upper bound of batch partitioning can be reduced to
the classic {\it graph k-cut problem}. Consequently, we propose a spectral
clustering-based algorithm to solve this problem. Extensive experiments
illustrate that the proposed batch partitioning approach effectively reduces
the computational cost while preserving performance. The efficiency improvement
is more pronounced on larger datasets and on devices with more memory capacity,
attesting to its practical utility for large-scale applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction. (arXiv:2305.15520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15520">
<div class="article-summary-box-inner">
<span><p>Previous research has demonstrated that natural language explanations provide
valuable inductive biases that guide models, thereby improving the
generalization ability and data efficiency. In this paper, we undertake a
systematic examination of the effectiveness of these explanations. Remarkably,
we find that corrupted explanations with diminished inductive biases can
achieve competitive or superior performance compared to the original
explanations. Our findings furnish novel insights into the characteristics of
natural language explanations in the following ways: (1) the impact of
explanations varies across different training styles and datasets, with
previously believed improvements primarily observed in frozen language models.
(2) While previous research has attributed the effect of explanations solely to
their inductive biases, our study shows that the effect persists even when the
explanations are completely corrupted. We propose that the main effect is due
to the provision of additional context space. (3) Utilizing the proposed
automatic perturbed context, we were able to attain comparable results to
annotated explanations, but with a significant increase in computational
efficiency, 20-30 times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Few-Shot Health Learners. (arXiv:2305.15525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15525">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can capture rich representations of concepts
that are useful for real-world tasks. However, language alone is limited. While
existing LLMs excel at text-based inferences, health applications require that
models be grounded in numerical data (e.g., vital signs, laboratory values in
clinical domains; steps, movement in the wellness domain) that is not easily or
readily expressed as text in existing training corpus. We demonstrate that with
only few-shot tuning, a large language model is capable of grounding various
physiological and behavioral time-series data and making meaningful inferences
on numerous health tasks for both clinical and wellness contexts. Using data
from wearable and medical sensor recordings, we evaluate these capabilities on
the tasks of cardiac signal analysis, physical activity recognition, metabolic
calculation (e.g., calories burned), and estimation of stress reports and
mental health screeners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Refugee Case Analysis: An NLP Pipeline for Supporting Legal Practitioners. (arXiv:2305.15533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15533">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce an end-to-end pipeline for retrieving,
processing, and extracting targeted information from legal cases. We
investigate an under-studied legal domain with a case study on refugee law in
Canada. Searching case law for past similar cases is a key part of legal work
for both lawyers and judges, the potential end-users of our prototype. While
traditional named-entity recognition labels such as dates provide meaningful
information in legal work, we propose to extend existing models and retrieve a
total of 19 useful categories of items from refugee cases. After creating a
novel data set of cases, we perform information extraction based on
state-of-the-art neural named-entity recognition (NER). We test different
architectures including two transformer models, using contextual and
non-contextual embeddings, and compare general purpose versus domain-specific
pre-training. The results demonstrate that models pre-trained on legal data
perform best despite their smaller size, suggesting that domain matching had a
larger effect than network architecture. We achieve a F1 score above 90% on
five of the targeted categories and over 80% on four further categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation. (arXiv:2305.15541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15541">
<div class="article-summary-box-inner">
<span><p>Translating natural language sentences to first-order logic (NL-FOL
translation) is a longstanding challenge in the NLP and formal logic
literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for
NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of
directly translating natural language into FOL rules, which outperforms
GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,
and can achieve similar performance as GPT-4 with a fraction of the cost. This
correction ability was achieved by a novel supervised fine-tuning (SFT) +
reinforcement learning with human feedback (RLHF) framework, which initially
trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought
reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier
as the reward model.
</p>
<p>To train LogicLLaMA, we present MALLS (large language $\textbf{M}$odel
gener$\textbf{A}$ted N$\textbf{L}$-FO$\textbf{L}$ pair$\textbf{S}$), a dataset
of 34K high-quality and diverse sentence-level NL-FOL pairs collected from
GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4
for pairs, and dynamically adjusts the prompts to ensure the collection of
pairs with rich and diverse contexts at different levels of complexity, and
verifies the validity of the generated FOL rules. Codes, weights, and data are
available at $\href{https://github.com/gblackout/LogicLLaMA}{{\small
\text{https://github.com/gblackout/LogicLLaMA}}}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15542">
<div class="article-summary-box-inner">
<span><p>Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we emphasize the importance of
refocusing the attention in transfer learning. We introduce Top-Down Attention
Steering (TOAST), a novel transfer learning algorithm that keeps the
pre-trained backbone frozen, while selecting the task-relevant elements in the
output and feeding them back to the model to steer its attention to the
task-specific features. By refocusing the attention only, TOAST achieves
state-of-the-art results on a number of transfer learning benchmarks, while
having a small portion of tunable parameters. Compared to fully fine-tuning,
LoRA, and prompt tuning, TOAST substantially improves performance across a
range of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% on
FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer. (arXiv:2305.15582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15582">
<div class="article-summary-box-inner">
<span><p>Text style transfer is an exciting task within the field of natural language
generation that is often plagued by the need for high-quality paired datasets.
Furthermore, training a model for multi-attribute text style transfer requires
datasets with sufficient support across all combinations of the considered
stylistic attributes, adding to the challenges of training a style transfer
model. This paper explores the impact of training data input diversity on the
quality of the generated text from the multi-style transfer model. We construct
a pseudo-parallel dataset by devising heuristics to adjust the style
distribution in the training samples. We balance our training dataset using
marginal and joint distributions to train our style transfer models. We observe
that a balanced dataset produces more effective control effects over multiple
styles than an imbalanced or skewed one. Through quantitative analysis, we
explore the impact of multiple style distributions in training data on
style-transferred output. These findings will better inform the design of
style-transfer datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15587">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) models based on Machine Learning (ML) are
susceptible to adversarial attacks -- malicious algorithms that imperceptibly
modify input text to force models into making incorrect predictions. However,
evaluations of these attacks ignore the property of imperceptibility or study
it under limited settings. This entails that adversarial perturbations would
not pass any human quality gate and do not represent real threats to
human-checked NLP systems. To bypass this limitation and enable proper
assessment (and later, improvement) of NLP model robustness, we have surveyed
378 human participants about the perceptibility of text adversarial examples
produced by state-of-the-art methods. Our results underline that existing text
attacks are impractical in real-world scenarios where humans are involved. This
contrasts with previous smaller-scale human studies, which reported overly
optimistic conclusions regarding attack success. Through our work, we hope to
position human perceptibility as a first-class success criterion for text
attacks, and provide guidance for research to build effective attack algorithms
and, in turn, design appropriate defence mechanisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15594">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are excellent in-context learners. However, the
sensitivity of data contained in prompts raises privacy concerns. Our work
first shows that these concerns are valid: we instantiate a simple but highly
effective membership inference attack against the data used to prompt LLMs. To
address this vulnerability, one could forego prompting and resort to
fine-tuning LLMs with known algorithms for private gradient descent. However,
this comes at the expense of the practicality and efficiency offered by
prompting. Therefore, we propose to privately learn to prompt. We first show
that soft prompts can be obtained privately through gradient descent on
downstream data. However, this is not the case for discrete prompts. Thus, we
orchestrate a noisy vote among an ensemble of LLMs presented with different
prompts, i.e., a flock of stochastic parrots. The vote privately transfers the
flock's knowledge into a single public prompt. We show that LLMs prompted with
our private algorithms closely match the non-private baselines. For example,
using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the
sst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs.
95.2% for the non-private baseline. Through our experiments, we also show that
our prompt-based approach is easily deployed with existing commercial APIs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models. (arXiv:2305.15597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15597">
<div class="article-summary-box-inner">
<span><p>The mission of open knowledge graph (KG) completion is to draw new findings
from known facts. Existing works that augment KG completion require either (1)
factual triples to enlarge the graph reasoning space or (2) manually designed
prompts to extract knowledge from a pre-trained language model (PLM),
exhibiting limited performance and requiring expensive efforts from experts. To
this end, we propose TAGREAL that automatically generates quality query prompts
and retrieves support information from large text corpora to probe knowledge
from PLM for KG completion. The results show that TAGREAL achieves
state-of-the-art performance on two benchmark datasets. We find that TAGREAL
has superb performance even with limited training data, outperforming existing
embedding-based, graph-based, and PLM-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Sentence Union Generation as a Testbed for Text Consolidation. (arXiv:2305.15605v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15605">
<div class="article-summary-box-inner">
<span><p>Tasks involving text generation based on multiple input texts, such as
multi-document summarization, long-form question answering and contemporary
dialogue applications, challenge models for their ability to properly
consolidate partly-overlapping multi-text information. However, these tasks
entangle the consolidation phase with the often subjective and ill-defined
content selection requirement, impeding proper assessment of models'
consolidation capabilities. In this paper, we suggest revisiting the sentence
union generation task as an effective well-defined testbed for assessing text
consolidation capabilities, decoupling the consolidation challenge from
subjective content selection. To support research on this task, we present
refined annotation methodology and tools for crowdsourcing sentence union,
create the largest union dataset to date and provide an analysis of its rich
coverage of various consolidation aspects. We then propose a comprehensive
evaluation protocol for union generation, including both human and automatic
evaluation. Finally, as baselines, we evaluate state-of-the-art language models
on the task, along with a detailed analysis of their capacity to address
multi-text consolidation challenges and their limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphological Inflection: A Reality Check. (arXiv:2305.15637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15637">
<div class="article-summary-box-inner">
<span><p>Morphological inflection is a popular task in sub-word NLP with both
practical and cognitive applications. For years now, state-of-the-art systems
have reported high, but also highly variable, performance across data sets and
languages. We investigate the causes of this high performance and high
variability; we find several aspects of data set creation and evaluation which
systematically inflate performance and obfuscate differences between languages.
To improve generalizability and reliability of results, we propose new data
sampling and evaluation strategies that better reflect likely use-cases. Using
these new strategies, we make new observations on the generalization abilities
of current inflection systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15645">
<div class="article-summary-box-inner">
<span><p>In conversational search, the user's real search intent for the current turn
is dependent on the previous conversation history. It is challenging to
determine a good search query from the whole conversation context. To avoid the
expensive re-training of the query encoder, most existing methods try to learn
a rewriting model to de-contextualize the current query by mimicking the manual
query rewriting. However, manually rewritten queries are not always the best
search queries. Training a rewriting model on them would limit the model's
ability to produce good search queries. Another useful hint is the potential
answer to the question. In this paper, we propose ConvGQR, a new framework to
reformulate conversational queries based on generative pre-trained language
models (PLMs), one for query rewriting and another for generating potential
answers. By combining both, ConvGQR can produce better search queries. In
addition, to relate query reformulation to retrieval performance, we propose a
knowledge infusion mechanism to optimize both query reformulation and
retrieval. Extensive experiments on four conversational search datasets
demonstrate the effectiveness of ConvGQR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture-of-Expert Conformer for Streaming Multilingual ASR. (arXiv:2305.15663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15663">
<div class="article-summary-box-inner">
<span><p>End-to-end models with large capacity have significantly improved
multilingual automatic speech recognition, but their computation cost poses
challenges for on-device applications. We propose a streaming truly
multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn
to only activate a subset of parameters in training and inference. The MoE
layer consists of a softmax gate which chooses the best two experts among many
in forward propagation. The proposed MoE layer offers efficient inference by
activating a fixed number of parameters as the number of experts increases. We
evaluate the proposed model on a set of 12 languages, and achieve an average
11.9% relative improvement in WER over the baseline. Compared to an adapter
model using ground truth information, our MoE model achieves similar WER and
activates similar number of parameters but without any language information. We
further show around 3% relative WER improvement by multilingual shallow fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model. (arXiv:2305.15673v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15673">
<div class="article-summary-box-inner">
<span><p>With the continuous development and change exhibited by large language model
(LLM) technology, represented by generative pretrained transformers (GPTs),
many classic scenarios in various fields have re-emerged with new
opportunities. This paper takes ChatGPT as the modeling object, incorporates
LLM technology into the typical book resource understanding and recommendation
scenario for the first time, and puts it into practice. By building a
ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,
this paper attempts to apply ChatGPT to recommendation modeling for three
typical tasks, book rating recommendation, user rating recommendation, and book
summary recommendation, and explores the feasibility of LLM technology in book
recommendation scenarios. At the same time, based on different evaluation
schemes for book recommendation tasks and the existing classic recommendation
models, this paper discusses the advantages and disadvantages of the BookGPT in
book recommendation scenarios and analyzes the opportunities and improvement
directions for subsequent LLMs in these scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Grammatical Error Correction Systems with Explanations. (arXiv:2305.15676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15676">
<div class="article-summary-box-inner">
<span><p>Grammatical error correction systems improve written communication by
detecting and correcting language mistakes. To help language learners better
understand why the GEC system makes a certain correction, the causes of errors
(evidence words) and the corresponding error types are two key factors. To
enhance GEC systems with explanations, we introduce EXPECT, a large dataset
annotated with evidence words and grammatical error types. We propose several
baselines and anlysis to understand this task. Furthermore, human evaluation
verifies our explainable GEC system's explanations can assist second-language
learners in determining whether to accept a correction suggestion and in
understanding the associated grammar rule.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting non-English Text Simplification: A Unified Multilingual Benchmark. (arXiv:2305.15678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15678">
<div class="article-summary-box-inner">
<span><p>Recent advancements in high-quality, large-scale English resources have
pushed the frontier of English Automatic Text Simplification (ATS) research.
However, less work has been done on multilingual text simplification due to the
lack of a diverse evaluation benchmark that covers complex-simple sentence
pairs in many languages. This paper introduces the MultiSim benchmark, a
collection of 27 resources in 12 distinct languages containing over 1.7 million
complex-simple sentence pairs. This benchmark will encourage research in
developing more effective multilingual text simplification models and
evaluation metrics. Our experiments using MultiSim with pre-trained
multilingual language models reveal exciting performance improvements from
multilingual training in non-English settings. We observe strong performance
from Russian in zero-shot cross-lingual transfer to low-resource languages. We
further show that few-shot prompting with BLOOM-176b achieves comparable
quality to reference simplifications outperforming fine-tuned models in most
languages. We validate these findings through human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbation-based Self-supervised Attention for Attention Bias in Text Classification. (arXiv:2305.15684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15684">
<div class="article-summary-box-inner">
<span><p>In text classification, the traditional attention mechanisms usually focus
too much on frequent words, and need extensive labeled data in order to learn.
This paper proposes a perturbation-based self-supervised attention approach to
guide attention learning without any annotation overhead. Specifically, we add
as much noise as possible to all the words in the sentence without changing
their semantics and predictions. We hypothesize that words that tolerate more
noise are less significant, and we can use this information to refine the
attention distribution. Experimental results on three text classification tasks
show that our approach can significantly improve the performance of current
attention-based models, and is more effective than existing self-supervised
methods. We also provide a visualization analysis to verify the effectiveness
of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15685">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities in long-form text generation tasks expressed through natural
language instructions. However, user expectations for long-form text rewriting
is high, and unintended rewrites (''hallucinations'') produced by the model can
negatively impact its overall performance. Existing evaluation benchmarks
primarily focus on limited rewriting styles and sentence-level rewriting rather
than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel
benchmark that covers a wide variety of rewriting types expressed through
natural language instructions. It is specifically designed to facilitate the
evaluation of open-ended rewriting of long-form texts. In addition, we propose
a strong baseline model, RewriteLM, an instruction-tuned large language model
for long-form text rewriting. We develop new strategies that facilitate the
generation of diverse instructions and preference data with minimal human
intervention. We conduct empirical experiments and demonstrate that our model
outperforms the current state-of-the-art LLMs in text rewriting. Specifically,
it excels in preserving the essential content and meaning of the source text,
minimizing the generation of ''hallucinated'' content, while showcasing the
ability to generate rewrites with diverse wording and structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15689">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that natural-language prompts can help to
leverage the knowledge learned by pre-trained language models for the binary
sentence-level sentiment classification task. Specifically, these methods
utilize few-shot learning settings to fine-tune the sentiment classification
model using manual or automatically generated prompts. However, the performance
of these methods is sensitive to the perturbations of the utilized prompts.
Furthermore, these methods depend on a few labeled instances for automatic
prompt generation and prompt ranking. This study aims to find high-quality
prompts for the given task in a zero-shot setting. Given a base prompt, our
proposed approach automatically generates multiple prompts similar to the base
prompt employing positional, reasoning, and paraphrasing techniques and then
ranks the prompts using a novel metric. We empirically demonstrate that the
top-ranked prompts are high-quality and significantly outperform the base
prompt and the prompts generated using few-shot learning for the binary
sentence-level sentiment classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The False Promise of Imitating Proprietary LLMs. (arXiv:2305.15717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15717">
<div class="article-summary-box-inner">
<span><p>An emerging method to cheaply improve a weaker language model is to finetune
it on outputs from a stronger model, such as a proprietary system like ChatGPT
(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply
imitate the proprietary model's capabilities using a weaker open-source model.
In this work, we critically analyze this approach. We first finetune a series
of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data
sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the
models using crowd raters and canonical NLP benchmarks. Initially, we were
surprised by the output quality of our imitation models -- they appear far
better at following instructions, and crowd workers rate their outputs as
competitive with ChatGPT. However, when conducting more targeted automatic
evaluations, we find that imitation models close little to none of the gap from
the base LM to ChatGPT on tasks that are not heavily supported in the imitation
data. We show that these performance discrepancies may slip past human raters
because imitation models are adept at mimicking ChatGPT's style but not its
factuality. Overall, we conclude that model imitation is a false promise: there
exists a substantial capabilities gap between open and closed LMs that, with
current methods, can only be bridged using an unwieldy amount of imitation data
or by using more capable base LMs. In turn, we argue that the highest leverage
action for improving open-source models is to tackle the difficult challenge of
developing better base LMs, rather than taking the shortcut of imitating
proprietary systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Higher Pareto Frontier in Multilingual Machine Translation. (arXiv:2305.15718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15718">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation has witnessed remarkable progress in
recent years. However, the long-tailed distribution of multilingual corpora
poses a challenge of Pareto optimization, i.e., optimizing for some languages
may come at the cost of degrading the performance of others. Existing balancing
training strategies are equivalent to a series of Pareto optimal solutions,
which trade off on a Pareto frontier. In this work, we propose a new training
framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto
frontier outwards rather than making trade-offs. Specifically, Pareto-MD
collaboratively trains two Pareto optimal solutions that favor different
languages and allows them to learn from the strengths of each other via
knowledge distillation. Furthermore, we introduce a novel strategy to enable
stronger communication between Pareto optimal solutions and broaden the
applicability of our approach. Experimental results on the widely-used WMT and
TED datasets show that our method significantly pushes the Pareto frontier and
outperforms baselines by up to +2.46 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15722">
<div class="article-summary-box-inner">
<span><p>The term "Code Mixed" refers to the use of more than one language in the same
text. This phenomenon is predominantly observed on social media platforms, with
an increasing amount of adaptation as time goes on. It is critical to detect
foreign elements in a language and process them correctly, as a considerable
number of individuals are using code-mixed languages that could not be
comprehended by understanding one of those languages. In this work, we focus on
low-resource Hindi-English code-mixed language and enhancing the performance of
different code-mixed natural language processing tasks such as sentiment
analysis, emotion recognition, and hate speech identification. We perform a
comparative analysis of different Transformer-based language Models pre-trained
using unsupervised approaches. We have included the code-mixed models like
HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like
AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-English
downstream tasks. We report state-of-the-art results on respective datasets
using HingBERT-based models which are specifically pre-trained on real
code-mixed text. Our HingBERT-based models provide significant improvements
thus highlighting the poor performance of vanilla BERT models on code-mixed
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Not Link: Exploring NIL Prediction in Entity Linking. (arXiv:2305.15725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15725">
<div class="article-summary-box-inner">
<span><p>Entity linking models have achieved significant success via utilizing
pretrained language models to capture semantic features. However, the NIL
prediction problem, which aims to identify mentions without a corresponding
entity in the knowledge base, has received insufficient attention. We
categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,
and propose an entity linking dataset NEL that focuses on the NIL prediction
problem. NEL takes ambiguous entities as seeds, collects relevant mention
context in the Wikipedia corpus, and ensures the presence of mentions linking
to NIL by human annotation and entity masking. We conduct a series of
experiments with the widely used bi-encoder and cross-encoder entity linking
models, results show that both types of NIL mentions in training data have a
significant influence on the accuracy of NIL prediction. Our code and dataset
can be accessed at https://github.com/solitaryzero/NIL_EL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration. (arXiv:2305.15749v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15749">
<div class="article-summary-box-inner">
<span><p>This work aims to build a multilingual text-to-speech (TTS) synthesis system
for ten lower-resourced Turkic languages: Azerbaijani, Bashkir, Kazakh, Kyrgyz,
Sakha, Tatar, Turkish, Turkmen, Uyghur, and Uzbek. We specifically target the
zero-shot learning scenario, where a TTS model trained using the data of one
language is applied to synthesise speech for other, unseen languages. An
end-to-end TTS system based on the Tacotron 2 architecture was trained using
only the available data of the Kazakh language. To generate speech for the
other Turkic languages, we first mapped the letters of the Turkic alphabets
onto the symbols of the International Phonetic Alphabet (IPA), which were then
converted to the Kazakh alphabet letters. To demonstrate the feasibility of the
proposed approach, we evaluated the multilingual Turkic TTS model subjectively
and obtained promising results. To enable replication of the experiments, we
make our code and dataset publicly available in our GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. (arXiv:2305.15756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15756">
<div class="article-summary-box-inner">
<span><p>Prior study has shown that pretrained language models (PLM) can boost the
performance of text-based recommendation. In contrast to previous works that
either use PLM to encode user history as a whole input text, or impose an
additional aggregation network to fuse multi-turn history representations, we
propose a unified local- and global-attention Transformer encoder to better
model two-level contexts of user history. Moreover, conditioned on user history
encoded by Transformer encoders, our framework leverages Transformer decoders
to estimate the language perplexity of candidate text items, which can serve as
a straightforward yet significant contrastive signal for user-item text
matching. Based on this, our framework, UniTRec, unifies the contrastive
objectives of discriminative matching scores and candidate text perplexity to
jointly enhance text-based recommendation. Extensive evaluation shows that
UniTRec delivers SOTA performance on three text-based recommendation tasks.
Code is available at https://github.com/Veason-silverbullet/UniTRec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Healing Unsafe Dialogue Responses with Weak Supervision Signals. (arXiv:2305.15757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15757">
<div class="article-summary-box-inner">
<span><p>Recent years have seen increasing concerns about the unsafe response
generation of large-scale dialogue systems, where agents will learn offensive
or biased behaviors from the real-world corpus. Some methods are proposed to
address the above issue by detecting and replacing unsafe training examples in
a pipeline style. Though effective, they suffer from a high annotation cost and
adapt poorly to unseen scenarios as well as adversarial attacks. Besides, the
neglect of providing safe responses (e.g. simply replacing with templates) will
cause the information-missing problem of dialogues. To address these issues, we
propose an unsupervised pseudo-label sampling method, TEMP, that can
automatically assign potential safe responses. Specifically, our TEMP method
groups responses into several clusters and samples multiple labels with an
adaptively sharpened sampling strategy, inspired by the observation that unsafe
samples in the clusters are usually few and distribute in the tail. Extensive
experiments in chitchat and task-oriented dialogues show that our TEMP
outperforms state-of-the-art models with weak supervision signals and obtains
comparable results under unsupervised learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Svarah: Evaluating English ASR Systems on Indian Accents. (arXiv:2305.15760v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15760">
<div class="article-summary-box-inner">
<span><p>India is the second largest English-speaking country in the world with a
speaker base of roughly 130 million. Thus, it is imperative that automatic
speech recognition (ASR) systems for English should be evaluated on Indian
accents. Unfortunately, Indian speakers find a very poor representation in
existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent
Archive, etc. In this work, we address this gap by creating Svarah, a benchmark
that contains 9.6 hours of transcribed English audio from 117 speakers across
65 geographic locations throughout India, resulting in a diverse range of
accents. Svarah comprises both read speech and spontaneous conversational data,
covering various domains, such as history, culture, tourism, etc., ensuring a
diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR
systems on Svarah and show that there is clear scope for improvement on Indian
accents. Svarah as well as all our code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15769">
<div class="article-summary-box-inner">
<span><p>Recent years have seen increasing concerns about the private inference of NLP
services and Transformer models. However, existing two-party privacy-preserving
methods solely consider NLU scenarios, while the private inference of text
generation such as translation, dialogue, and code completion remains unsolved.
Besides, while migrated to NLG models, existing privacy-preserving methods
perform poorly in terms of inference speed, and suffer from the convergence
problem during the training stage. To address these issues, we propose MERGE, a
fast private text generation framework for Transformer-based language models.
Specifically, MERGE reuse the output hidden state as the word embedding to
bypass the embedding computation, and reorganize the linear operations in the
Transformer module to accelerate the forward procedure. Based on these two
optimizations, extensive experiments show that MERGE can achieve a 26.5x
speedup under the sequence length 512, and reduce 80\% communication bytes,
with an up to 10x speedup to existing state-of-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15805">
<div class="article-summary-box-inner">
<span><p>Autoregressive Transformers adopted in Large Language Models (LLMs) are hard
to scale to long sequences. Despite several works trying to reduce their
computational cost, most of LLMs still adopt attention layers between all pairs
of tokens in the sequence, thus incurring a quadratic cost. In this study, we
present a novel approach that dynamically prunes contextual information while
preserving the model's expressiveness, resulting in reduced memory and
computational requirements during inference. Our method employs a learnable
mechanism that determines which uninformative tokens can be dropped from the
context at any point across the generation process. By doing so, our approach
not only addresses performance concerns but also enhances interpretability,
providing valuable insight into the model's decision-making process. Our
technique can be applied to existing pre-trained models through a
straightforward fine-tuning process, and the pruning strength can be specified
by a sparsity parameter. Notably, our empirical findings demonstrate that we
can effectively prune up to 80\% of the context without significant performance
degradation on downstream tasks, offering a valuable tool for mitigating
inference costs. Our reference implementation achieves up to $2\times$ increase
in inference throughput and even greater memory savings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15814">
<div class="article-summary-box-inner">
<span><p>We create publicly available language identification (LID) datasets and
models in all 22 Indian languages listed in the Indian constitution in both
native-script and romanized text. First, we create Bhasha-Abhijnaanam, a
language identification test set for native-script as well as romanized text
which spans all 22 Indic languages. We also train IndicLID, a language
identifier for all the above-mentioned languages in both native and romanized
script. For native-script text, it has better language coverage than existing
LIDs and is competitive or better than other LIDs. IndicLID is the first LID
for romanized text in Indian languages. Two major challenges for romanized text
LID are the lack of training data and low-LID performance when languages are
similar. We provide simple and effective solutions to these problems. In
general, there has been limited work on romanized text in any language, and our
findings are relevant to other languages that need romanized language
identification. Our models are publicly available at
https://github.com/AI4Bharat/IndicLID under open-source licenses. Our training
and test sets are also publicly available at
https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam under open-source
licenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15852">
<div class="article-summary-box-inner">
<span><p>Large language models (large LMs) are susceptible to producing text with
hallucinated content. Self-contradiction, where the LM generates two
contradictory sentences within the same context, is an important form of
hallucination. In this work, we present a comprehensive analysis on
self-contradiction for state-of-the-art, instruction-tuned LMs, including
evaluation, detection, and mitigation. To effectively trigger
self-contradictions, we design a framework that constrains LMs to generate
appropriate sentence pairs. Our evaluation on these sentence pairs reveals that
self-contradictions occur frequently across different LMs for both famous and
lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our
results indicate that ChatGPT and GPT-4 are able to accurately identify
self-contradictions, while Vicuna-13B struggles to do so. For example, with our
best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the
sentence pairs generated by itself. To automatically mitigate
self-contradictions, we develop an iterative algorithm that prompts the LMs to
remove the detected self-contradictions from the generated text. Our algorithm
successfully revises the text such that self-contradictions are significantly
reduced, while maintaining its fluency and informativeness. Importantly, our
entire pipeline of triggering, detecting, and mitigating self-contradictions is
applicable to black-box LMs and does not require any external grounded
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Integrated Gradients: a simple but effective method for explaining language models. (arXiv:2305.15853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15853">
<div class="article-summary-box-inner">
<span><p>Several explanation methods such as Integrated Gradients (IG) can be
characterised as path-based methods, as they rely on a straight line between
the data and an uninformative baseline. However, when applied to language
models, these methods produce a path for each word of a sentence
simultaneously, which could lead to creating sentences from interpolated words
either having no clear meaning, or having a significantly different meaning
compared to the original sentence. In order to keep the meaning of these
sentences as close as possible to the original one, we propose Sequential
Integrated Gradients (SIG), which computes the importance of each word in a
sentence by keeping fixed every other words, only creating interpolations
between the baseline and the word of interest. Moreover, inspired by the
training procedure of several language models, we also propose to replace the
baseline token "pad" with the trained token "mask". While being a simple
improvement over the original IG method, we show on various models and datasets
that SIG proves to be a very effective method for explaining language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Text Representations for Terms and Phrases in Technical Domains. (arXiv:2305.15867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15867">
<div class="article-summary-box-inner">
<span><p>Extracting dense representations for terms and phrases is a task of great
importance for knowledge discovery platforms targeting highly-technical fields.
Dense representations are used as features for downstream components and have
multiple applications ranging from ranking results in search to summarization.
Common approaches to create dense representations include training
domain-specific embeddings with self-supervised setups or using sentence
encoder models trained over similarity tasks. In contrast to static embeddings,
sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but
impose significant computational costs. In this paper, we propose a fully
unsupervised approach to text encoding that consists of training small
character-based models with the objective of reconstructing large pre-trained
embedding matrices. Models trained with this approach can not only match the
quality of sentence encoders in technical domains, but are 5 times smaller and
up to 10 times faster, even on high-end GPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation. (arXiv:2305.15872v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15872">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning has been an important approach to address challenges
in extracting entities and relations from limited data. However, current
semi-supervised works handle the two tasks (i.e., Named Entity Recognition and
Relation Extraction) separately and ignore the cross-correlation of entity and
relation instances as well as the existence of similar instances across
unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous
Graph-based Propagation framework for joint semi-supervised entity and relation
extraction, which captures the global structure information between individual
tasks and exploits interactions within unlabeled data. Specifically, we
construct a unified span-based heterogeneous graph from entity and relation
candidates and propagate class labels based on confidence scores. We then
employ a propagation learning scheme to leverage the affinities between
labelled and unlabeled samples. Experiments on benchmark datasets show that our
framework outperforms the state-of-the-art semi-supervised approaches on NER
and RE tasks. We show that the joint semi-supervised learning of the two tasks
benefits from their codependency and validates the importance of utilizing the
shared information between unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15875">
<div class="article-summary-box-inner">
<span><p>We investigate the phenomenon of an LLM's untruthful response using a large
set of 220 handcrafted linguistic features. We focus on GPT-3 models and find
that the linguistic profiles of responses are similar across model sizes. That
is, how varying-sized LLMs respond to given prompts stays similar on the
linguistic properties level. We expand upon this finding by training support
vector machines that rely only upon the stylistic components of model responses
to classify the truthfulness of statements. Though the dataset size limits our
current findings, we present promising evidence that truthfulness detection is
possible without evaluating the content itself.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15878">
<div class="article-summary-box-inner">
<span><p>Past research has identified a rich set of handcrafted linguistic features
that can potentially assist various tasks. However, their extensive number
makes it difficult to effectively select and utilize existing handcrafted
features. Coupled with the problem of inconsistent implementation across
research works, there has been no categorization scheme or generally-accepted
feature names. This creates unwanted confusion. Also, most existing handcrafted
feature extraction libraries are not open-source or not actively maintained. As
a result, a researcher often has to build such an extraction system from the
ground up.
</p>
<p>We collect and categorize more than 220 popular handcrafted features grounded
on past literature. Then, we conduct a correlation analysis study on several
task-specific datasets and report the potential use cases of each feature.
Lastly, we devise a multilingual handcrafted linguistic feature extraction
system in a systematically expandable manner. We open-source our system for
public access to a rich set of pre-implemented handcrafted features. Our system
is coined LFTK and is the largest of its kind. Find it at
github.com/brucewlee/lftk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset. (arXiv:2305.15891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15891">
<div class="article-summary-box-inner">
<span><p>The cross-domain text-to-SQL task aims to build a system that can parse user
questions into SQL on complete unseen databases, and the single-domain
text-to-SQL task evaluates the performance on identical databases. Both of
these setups confront unavoidable difficulties in real-world applications. To
this end, we introduce the cross-schema text-to-SQL task, where the databases
of evaluation data are different from that in the training data but come from
the same domain. Furthermore, we present CSS, a large-scale CrosS-Schema
Chinese text-to-SQL dataset, to carry on corresponding studies. CSS originally
consisted of 4,340 question/SQL pairs across 2 databases. In order to
generalize models to different medical systems, we extend CSS and create 19 new
databases along with 29,280 corresponding dataset examples. Moreover, CSS is
also a large corpus for single-domain Chinese text-to-SQL studies. We present
the data collection approach and a series of analyses of the data statistics.
To show the potential and usefulness of CSS, benchmarking baselines have been
conducted and reported. Our dataset is publicly available at
\url{https://huggingface.co/datasets/zhanghanchong/css}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Private Meeting Summarization Without Performance Loss. (arXiv:2305.15894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15894">
<div class="article-summary-box-inner">
<span><p>Meeting summarization has an enormous business potential, but in addition to
being a hard problem, roll-out is challenged by privacy concerns. We explore
the problem of meeting summarization under differential privacy constraints and
find, to our surprise, that while differential privacy leads to slightly lower
performance on in-sample data, differential privacy improves performance when
evaluated on unseen meeting types. Since meeting summarization systems will
encounter a great variety of meeting types in practical employment scenarios,
this observation makes safe meeting summarization seem much more feasible. We
perform extensive error analysis and identify potential risks in meeting
summarization under differential privacy, including a faithfulness analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collective Knowledge Graph Completion with Mutual Knowledge Distillation. (arXiv:2305.15895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15895">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion (KGC), the task of predicting missing information
based on the existing relational data inside a knowledge graph (KG), has drawn
significant attention in recent years. However, the predictive power of KGC
methods is often limited by the completeness of the existing knowledge graphs
from different sources and languages. In monolingual and multilingual settings,
KGs are potentially complementary to each other. In this paper, we study the
problem of multi-KG completion, where we focus on maximizing the collective
knowledge from different KGs to alleviate the incompleteness of individual KGs.
Specifically, we propose a novel method called CKGC-CKD that uses
relation-aware graph convolutional network encoder models on both individual
KGs and a large fused KG in which seed alignments between KGs are regarded as
edges for message propagation. An additional mutual knowledge distillation
mechanism is also employed to maximize the knowledge transfer between the
models of "global" fused KG and the "local" individual KGs. Experimental
results on multilingual datasets have shown that our method outperforms all
state-of-the-art models in the KGC task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15904">
<div class="article-summary-box-inner">
<span><p>Efficient utilisation of both intra- and extra-textual context remains one of
the critical gaps between machine and human translation. Existing research has
primarily focused on providing individual, well-defined types of context in
translation, such as the surrounding text or discrete external variables like
the speaker's gender. This work introduces MTCue, a novel neural machine
translation (NMT) framework that interprets all context (including discrete
variables) as text. MTCue learns an abstract representation of context,
enabling transferability across different data settings and leveraging similar
attributes in low-resource scenarios. With a focus on a dialogue domain with
access to document and metadata context, we extensively evaluate MTCue in four
language pairs in both translation directions. Our framework demonstrates
significant improvements in translation quality over a parameter-matched
non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).
Moreover, MTCue significantly outperforms a "tagging" baseline at translating
English text. Analysis reveals that the context encoder of MTCue learns a
representation space that organises context based on specific attributes, such
as formality, enabling effective zero-shot control. Pre-training on context
embeddings also improves MTCue's few-shot performance compared to the "tagging"
baseline. Finally, an ablation study conducted on model components and
contextual variables further supports the robustness of MTCue for context-based
NMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?. (arXiv:2305.15908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15908">
<div class="article-summary-box-inner">
<span><p>Longitudinal Dialogues (LD) are the most challenging type of conversation for
human-machine dialogue systems. LDs include the recollections of events,
personal thoughts, and emotions specific to each individual in a sparse
sequence of dialogue sessions. Dialogue systems designed for LDs should
uniquely interact with the users over multiple sessions and long periods of
time (e.g. weeks), and engage them in personal dialogues to elaborate on their
feelings, thoughts, and real-life events. In this paper, we study the task of
response generation in LDs. We evaluate whether general-purpose Pre-trained
Language Models (PLM) are appropriate for this purpose. We fine-tune two PLMs,
GePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different
representations of the personal knowledge extracted from LDs for grounded
response generation, including the graph representation of the mentioned events
and participants. We evaluate the performance of the models via automatic
metrics and the contribution of the knowledge via the Integrated Gradients
technique. We categorize the natural language generation errors via human
evaluations of contextualization, appropriateness and engagement of the user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization. (arXiv:2305.15913v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15913">
<div class="article-summary-box-inner">
<span><p>Memes are a powerful tool for communication over social media. Their affinity
for evolving across politics, history, and sociocultural phenomena makes them
an ideal communication vehicle. To comprehend the subtle message conveyed
within a meme, one must understand the background that facilitates its holistic
assimilation. Besides digital archiving of memes and their metadata by a few
websites like knowyourmeme.com, currently, there is no efficient way to deduce
a meme's context dynamically. In this work, we propose a novel task, MEMEX -
given a meme and a related document, the aim is to mine the context that
succinctly explains the background of the meme. At first, we develop MCC (Meme
Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we
propose MIME (MultImodal Meme Explainer), a multimodal neural framework that
uses common sense enriched meme representation and a layered approach to
capture the cross-modal semantic dependencies between the meme and the context.
MIME surpasses several unimodal and multimodal systems and yields an absolute
improvement of ~ 4% F1-score over the best baseline. Lastly, we conduct
detailed analyses of MIME's performance, highlighting the aspects that could
lead to optimal modeling of cross-modal contextual associations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable identification of selection mechanisms in language change. (arXiv:2305.15914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15914">
<div class="article-summary-box-inner">
<span><p>Language change is a cultural evolutionary process in which variants of
linguistic variables change in frequency through processes analogous to
mutation, selection and genetic drift. In this work, we apply a
recently-introduced method to corpus data to quantify the strength of selection
in specific instances of historical language change. We first demonstrate, in
the context of English irregular verbs, that this method is more reliable and
interpretable than similar methods that have previously been applied. We
further extend this study to demonstrate that a bias towards phonological
simplicity overrides that favouring grammatical simplicity when these are in
conflict. Finally, with reference to Spanish spelling reforms, we show that the
method can also detect points in time at which selection strengths change, a
feature that is generically expected for socially-motivated language change.
Together, these results indicate how hypotheses for mechanisms of language
change can be tested quantitatively using historical corpus data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of a phonological bias in ChatGPT. (arXiv:2305.15929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15929">
<div class="article-summary-box-inner">
<span><p>Current large language models, such as OpenAI's ChatGPT, have captured the
public's attention because how remarkable they are in the use of language.
Here, I demonstrate that ChatGPT displays phonological biases that are a
hallmark of human language processing. More concretely, just like humans,
ChatGPT has a consonant bias. That is, the chatbot has a tendency to use
consonants over vowels to identify words. This is observed across languages
that differ in their relative distribution of consonants and vowels such as
English and Spanish. Despite the differences in how current artificial
intelligence language models are trained to process linguistic stimuli and how
human infants acquire language, such training seems to be enough for the
emergence of a phonological bias in ChatGPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15932">
<div class="article-summary-box-inner">
<span><p>Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as
the construction of commonsense reasoning datasets is expensive, and they are
inevitably limited in their scope. A popular approach to UCR is to fine-tune
language models with external knowledge (e.g., knowledge graphs), but this
usually requires a large number of training examples. In this paper, we propose
to transform the downstream multiple choice question answering task into a
simpler binary classification task by ranking all candidate answers according
to their reasonableness. To this end, for training the model, we convert the
knowledge graph triples into reasonable and unreasonable texts. Extensive
experimental results show the effectiveness of our approach on various multiple
choice question answering benchmarks. Furthermore, compared with existing UCR
approaches using KGs, ours is less data hungry. Our code is available at
https://github.com/probe2/BUCA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15937">
<div class="article-summary-box-inner">
<span><p>We propose a visually grounded speech model that acquires new words and their
visual depictions from just a few word-image example pairs. Given a set of test
images and a spoken query, we ask the model which image depicts the query word.
Previous work has simplified this problem by either using an artificial setting
with digit word-image pairs or by using a large number of examples per class.
We propose an approach that can work on natural word-image pairs but with less
examples, i.e. fewer shots. Our approach involves using the given word-image
example pairs to mine new unsupervised word-image training pairs from large
collections of unlabelled speech and images. Additionally, we use a
word-to-image attention mechanism to determine word-image similarity. With this
new model, we achieve better performance with fewer shots than any existing
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04350">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models based on transformer architecture have
drastically changed the natural language processing (NLP) landscape. However,
deploying those models for on-device applications in constrained devices such
as smart watches is completely impractical due to their size and inference
cost. As an alternative to transformer-based architectures, recent work on
efficient NLP has shown that weight-efficient models can attain competitive
performance for simple tasks, such as slot filling and intent classification,
with model sizes in the order of the megabyte. This work introduces the
pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP
that achieves high weight-efficiency thanks to a novel projection layer. We
evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual
semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves
99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using
170x fewer parameters. Our model consistently beats the state-of-the-art of
tiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03691">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures are the model of choice for natural language
understanding, but they come at a significant cost, as they have quadratic
complexity in the input length, require a lot of training data, and can be
difficult to tune. In the pursuit of lower costs, we investigate simple
MLP-based architectures. We find that existing architectures such as MLPMixer,
which achieves token mixing through a static MLP applied to each feature
independently, are too detached from the inductive biases required for natural
language understanding. In this paper, we propose a simple variant, HyperMixer,
which forms the token mixing MLP dynamically using hypernetworks. Empirically,
we demonstrate that our model performs better than alternative MLP-based
models, and on par with Transformers. In contrast to Transformers, HyperMixer
achieves these results at substantially lower costs in terms of processing
time, training data, and hyperparameter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07648">
<div class="article-summary-box-inner">
<span><p>Recent progress in representation and contrastive learning in NLP has not
widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning
in interaction within different language communities). To bridge this gap, we
propose a novel framework for learning task-agnostic representations
transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate
speech, humor, sarcasm). Our framework outperforms other contrastive learning
frameworks for both in-domain and out-of-domain data, across both the general
and few-shot settings. For example, compared to two popular pre-trained
language models, our method obtains an improvement of $11.66$ average $F_1$ on
$16$ datasets when fine-tuned on only $20$ training samples per dataset.Our
code is available at: https://github.com/UBC-NLP/infodcl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05953">
<div class="article-summary-box-inner">
<span><p>Sign language recognition and translation first uses a recognition module to
generate glosses from sign language videos and then employs a translation
module to translate glosses into spoken sentences. Most existing works focus on
the recognition step, while paying less attention to sign language translation.
In this work, we propose a task-aware instruction network, namely TIN-SLT, for
sign language translation, by introducing the instruction module and the
learning-based feature fuse strategy into a Transformer network. In this way,
the pre-trained model's language ability can be well explored and utilized to
further boost the translation performance. Moreover, by exploring the
representation space of sign language glosses and target spoken language, we
propose a multi-level data augmentation scheme to adjust the data distribution
of the training set. We conduct extensive experiments on two challenging
benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method
outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code
is published at https://github.com/yongcaoplus/TIN-SLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Inflection for Scientific Discovery. (arXiv:2205.02007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02007">
<div class="article-summary-box-inner">
<span><p>We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and collaboration and
communication platforms. The transition has led to the creation and growth of a
tremendous amount of information -- much of which is available for public
access -- opening exciting opportunities for computational models and systems
that analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in artificial intelligence, including
large neural language models capable of learning powerful representations from
unstructured text. Dramatic changes in scientific communication -- such as the
advent of the first scientific journal in the 17th century -- have historically
catalyzed revolutions in scientific thought. The confluence of societal and
computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12505">
<div class="article-summary-box-inner">
<span><p>Recent works in Event Argument Extraction (EAE) have focused on improving
model generalizability to cater to new events and domains. However, standard
benchmarking datasets like ACE and ERE cover less than 40 event types and 25
entity-centric argument roles. Limited diversity and coverage hinder these
datasets from adequately evaluating the generalizability of EAE models. In this
paper, we first contribute by creating a large and diverse EAE ontology. This
ontology is created by transforming FrameNet, a comprehensive semantic role
labeling (SRL) dataset for EAE, by exploiting the similarity between these two
tasks. Then, exhaustive human expert annotations are collected to build the
ontology, concluding with 115 events and 220 argument roles, with a significant
portion of roles not being entities. We utilize this ontology to further
introduce GENEVA, a diverse generalizability benchmarking dataset comprising
four test suites, aimed at evaluating models' ability to handle limited data
and unseen event type generalization. We benchmark six EAE models from various
families. The results show that owing to non-entity argument roles, even the
best-performing model can only achieve 39% F1 score, indicating how GENEVA
provides new challenges for generalization in EAE. Overall, our large and
diverse EAE ontology can aid in creating more comprehensive future resources,
while GENEVA is a challenging benchmarking dataset encouraging further research
for improving generalizability in EAE. The code and data can be found at
https://github.com/PlusLabNLP/GENEVA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14116">
<div class="article-summary-box-inner">
<span><p>We present Claim-Dissector: a novel latent variable model for fact-checking
and analysis, which given a claim and a set of retrieved evidences jointly
learns to identify: (i) the relevant evidences to the given claim, (ii) the
veracity of the claim. We propose to disentangle the per-evidence relevance
probability and its contribution to the final veracity probability in an
interpretable way -- the final veracity probability is proportional to a linear
ensemble of per-evidence relevance probabilities. In this way, the individual
contributions of evidences towards the final predicted probability can be
identified. In per-evidence relevance probability, our model can further
distinguish whether each relevant evidence is supporting (S) or refuting (R)
the claim. This allows to quantify how much the S/R probability contributes to
the final verdict or to detect disagreeing evidence.
</p>
<p>Despite its interpretable nature, our system achieves results competitive
with state-of-the-art on the FEVER dataset, as compared to typical two-stage
system pipelines, while using significantly fewer parameters. It also sets new
state-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows
that our model can learn fine-grained relevance cues while using coarse-grained
supervision, and we demonstrate it in 2 ways. (i) We show that our model can
achieve competitive sentence recall while using only paragraph-level relevance
supervision. (ii) Traversing towards the finest granularity of relevance, we
show that our model is capable of identifying relevance at the token level. To
do this, we present a new benchmark TLR-FEVER focusing on token-level
interpretability -- humans annotate tokens in relevant evidences they
considered essential when making their judgment. Then we measure how similar
are these annotations to the tokens our model is focusing on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICE: Data-Efficient Clinical Event Extraction with Generative Models. (arXiv:2208.07989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07989">
<div class="article-summary-box-inner">
<span><p>Event extraction for the clinical domain is an under-explored research area.
The lack of training data along with the high volume of domain-specific
terminologies with vague entity boundaries makes the task especially
challenging. In this paper, we introduce DICE, a robust and data-efficient
generative model for clinical event extraction. DICE frames event extraction as
a conditional generation problem and introduces a contrastive learning
objective to accurately decide the boundaries of biomedical mentions. DICE also
trains an auxiliary mention identification task jointly with event extraction
tasks to better identify entity mention boundaries, and further introduces
special markers to incorporate identified entity mentions as trigger and
argument candidates for their respective tasks. To benchmark clinical event
extraction, we compose MACCROBAT-EE, the first clinical event extraction
dataset with argument annotation, based on an existing clinical information
extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art
performances of DICE for clinical and news domain event extraction, especially
under low data settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Better Masking for Better Language Model Pre-training. (arXiv:2208.10806v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.10806">
<div class="article-summary-box-inner">
<span><p>Masked Language Modeling (MLM) has been widely used as the denoising
objective in pre-training language models (PrLMs). Existing PrLMs commonly
adopt a Random-Token Masking strategy where a fixed masking ratio is applied
and different contents are masked by an equal probability throughout the entire
training. However, the model may receive complicated impact from pre-training
status, which changes accordingly as training time goes on. In this paper, we
show that such time-invariant MLM settings on masking ratio and masked content
are unlikely to deliver an optimal outcome, which motivates us to explore the
influence of time-variant MLM settings. We propose two scheduled masking
approaches that adaptively tune the masking ratio and masked content in
different training stages, which improves the pre-training efficiency and
effectiveness verified on the downstream tasks. Our work is a pioneer study on
time-variant masking strategy on ratio and content and gives a better
understanding of how masking ratio and masked content influence the MLM
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding. (arXiv:2209.13359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13359">
<div class="article-summary-box-inner">
<span><p>This paper explores the task of Temporal Video Grounding (TVG) where, given
an untrimmed video and a natural language sentence query, the goal is to
recognize and determine temporal boundaries of action instances in the video
described by the query. Recent works tackled this task by improving query
inputs with large pre-trained language models (PLM) at the cost of more
expensive training. However, the effects of this integration are unclear, as
these works also propose improvements in the visual inputs. Therefore, this
paper studies the effects of PLMs in TVG and assesses the applicability of
parameter-efficient training with NLP adapters. We couple popular PLMs with a
selection of existing approaches and test different adapters to reduce the
impact of the additional parameters. Our results on three challenging datasets
show that, without changing the visual inputs, TVG models greatly benefited
from the PLM integration and fine-tuning, stressing the importance of sentence
query representation in this task. Furthermore, NLP adapters were an effective
alternative to full fine-tuning, even though they were not tailored to our
task, allowing PLM integration in larger TVG models and delivering results
comparable to SOTA models. Finally, our results shed light on which adapters
work best in different scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code4Struct: Code Generation for Few-Shot Event Structure Prediction. (arXiv:2210.12810v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12810">
<div class="article-summary-box-inner">
<span><p>Large Language Model (LLM) trained on a mixture of text and code has
demonstrated impressive capability in translating natural language (NL) into
structured code. We observe that semantic structures can be conveniently
translated into code and propose Code4Struct to leverage such text-to-structure
translation capability to tackle structured prediction tasks. As a case study,
we formulate Event Argument Extraction (EAE) as converting text into
event-argument structures that can be represented as a class object using code.
This alignment between structures and code enables us to take advantage of
Programming Language (PL) features such as inheritance and type annotation to
introduce external knowledge or add constraints. We show that, with sufficient
in-context examples, formulating EAE as a code generation problem is
advantageous over using variants of text-based prompts. Despite only using 20
training event instances for each event type, Code4Struct is comparable to
supervised models trained on 4,202 instances and outperforms current
state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1.
Code4Struct can use 10-shot training data from a sibling event type to predict
arguments for zero-resource event types and outperforms the zero-shot baseline
by 12% absolute F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition. (arXiv:2210.15876v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15876">
<div class="article-summary-box-inner">
<span><p>One of limitations in end-to-end automatic speech recognition (ASR) framework
is its performance would be compromised if train-test utterance lengths are
mismatched. In this paper, we propose an on-the-fly random utterance
concatenation (RUC) based data augmentation method to alleviate train-test
utterance length mismatch issue for short-video ASR task. Specifically, we are
motivated by observations that our human-transcribed training utterances tend
to be much shorter for short-video spontaneous speech (~3 seconds on average),
while our test utterance generated from voice activity detection front-end is
much longer (~10 seconds on average). Such a mismatch can lead to suboptimal
performance. Empirically, it's observed the proposed RUC method significantly
improves long utterance recognition without performance drop on short one.
Overall, it achieves 5.72% word error rate reduction on average for 15
languages and improved robustness to various utterance length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual information integration for stance detection via cross-attention. (arXiv:2211.01874v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01874">
<div class="article-summary-box-inner">
<span><p>Stance detection deals with identifying an author's stance towards a target.
Most existing stance detection models are limited because they do not consider
relevant contextual information which allows for inferring the stance
correctly. Complementary context can be found in knowledge bases but
integrating the context into pretrained language models is non-trivial due to
the graph structure of standard knowledge bases. To overcome this, we explore
an approach to integrate contextual information as text which allows for
integrating contextual information from heterogeneous sources, such as
structured knowledge sources and by prompting large language models. Our
approach can outperform competitive baselines on a large and diverse stance
detection benchmark in a cross-target setup, i.e. for targets unseen during
training. We demonstrate that it is more robust to noisy context and can
regularize for unwanted correlations between labels and target-specific
vocabulary. Finally, it is independent of the pretrained language model in use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Classification by Logical Reasoning on Natural Language Explanations. (arXiv:2211.03252v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03252">
<div class="article-summary-box-inner">
<span><p>Humans can classify data of an unseen category by reasoning on its language
explanations. This ability is owing to the compositional nature of language: we
can combine previously seen attributes to describe the new category. For
example, we might describe a sage thrasher as "it has a slim straight
relatively short bill, yellow eyes and a long tail", so that others can use
their knowledge of attributes "slim straight relatively short bill", "yellow
eyes" and "long tail" to recognize a sage thrasher. Inspired by this
observation, in this work we tackle zero-shot classification task by logically
parsing and reasoning on natural language expla-nations. To this end, we
propose the framework CLORE (Classification by LOgical Reasoning on
Explanations). While previous methods usually regard textual information as
implicit features, CLORE parses explanations into logical structures and then
explicitly reasons along thess structures on the input to produce a
classification score. Experimental results on explanation-based zero-shot
classification benchmarks demonstrate that CLORE is superior to baselines,
which we further show mainly comes from higher scores on tasks requiring more
logical reasoning. We also demonstrate that our framework can be extended to
zero-shot classification on visual modality. Alongside classification
decisions, CLORE can provide the logical parsing and reasoning process as a
clear form of rationale. Through empirical analysis we demonstrate that CLORE
is also less affected by linguistic biases than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05523">
<div class="article-summary-box-inner">
<span><p>Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to 'more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05985">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation is a prominent problem in today's society, and
many researchers in academia and industry are trying to combat it. Due to the
vast amount of misinformation that is created every day, it is unrealistic to
leave this task to human fact-checkers. Data scientists and researchers have
been working on automated misinformation detection for years, and it is still a
challenging problem today. The goal of our research is to add a new level to
automated misinformation detection; classifying segments of text with
persuasive writing techniques in order to produce interpretable reasoning for
why an article can be marked as misinformation. To accomplish this, we present
a novel annotation scheme containing many common persuasive writing tactics,
along with a dataset with human annotations accordingly. For this task, we make
use of a RoBERTa model for text classification, due to its high performance in
NLP. We develop several language model-based baselines and present the results
of our persuasive strategy label predictions as well as the improvements these
intermediate labels make in detecting misinformation and producing
interpretable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts. (arXiv:2211.11772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11772">
<div class="article-summary-box-inner">
<span><p>Classroom discourse is a core medium of instruction - analyzing it can
provide a window into teaching and learning as well as driving the development
of new tools for improving instruction. We introduce the largest dataset of
mathematics classroom transcripts available to researchers, and demonstrate how
this data can help improve instruction. The dataset consists of 1,660 45-60
minute long 4th and 5th grade elementary mathematics observations collected by
the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The
anonymized transcripts represent data from 317 teachers across 4 school
districts that serve largely historically marginalized students. The
transcripts come with rich metadata, including turn-level annotations for
dialogic discourse moves, classroom observation scores, demographic
information, survey responses and student test scores. We demonstrate that our
natural language processing model, trained on our turn-level annotations, can
learn to identify dialogic discourse moves and these moves are correlated with
better classroom observation scores and learning outcomes. This dataset opens
up several possibilities for researchers, educators and policymakers to learn
about and improve K-12 instruction. The dataset can be found at
https://github.com/ddemszky/classroom-transcript-analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13873">
<div class="article-summary-box-inner">
<span><p>Due to the absence of explicit connectives, implicit discourse relation
recognition (IDRR) remains a challenging task in discourse analysis. The
critical step for IDRR is to learn high-quality discourse relation
representations between two arguments. Recent methods tend to integrate the
whole hierarchical information of senses into discourse relation
representations for multi-level sense recognition. Nevertheless, they
insufficiently incorporate the static hierarchical structure containing all
senses (defined as global hierarchy), and ignore the hierarchical sense label
sequence corresponding to each instance (defined as local hierarchy). For the
purpose of sufficiently exploiting global and local hierarchies of senses to
learn better discourse relation representations, we propose a novel GlObal and
Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of
hierarchies with the aid of multi-task learning and contrastive learning.
Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our
method remarkably outperforms current state-of-the-art models at all
hierarchical levels. Our code is publicly available at
https://github.com/YJiangcm/GOLF_for_IDRR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and reducing the distance between synthetic and real speech distributions. (arXiv:2211.16049v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16049">
<div class="article-summary-box-inner">
<span><p>While modern Text-to-Speech (TTS) systems can produce natural-sounding
speech, they remain unable to reproduce the full diversity found in natural
speech data. We consider the distribution of all possible real speech samples
that could be generated by these speakers alongside the distribution of all
synthetic samples that could be generated for the same set of speakers, using a
particular TTS system. We set out to quantify the distance between real and
synthetic speech via a range of utterance-level statistics related to
properties of the speaker, speech prosody and acoustic environment. Differences
in the distribution of these statistics are evaluated using the Wasserstein
distance. We reduce these distances by providing ground-truth values at
generation time, and quantify the improvements to the overall distribution
distance, approximated using an automatic speech recognition system. Our best
system achieves a 10\% reduction in distribution distance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00196">
<div class="article-summary-box-inner">
<span><p>Obtaining labeled data to train a model for a task of interest is often
expensive. Prior work shows training models on multitask data augmented with
task descriptions (prompts) effectively transfers knowledge to new tasks.
Towards efficiently building task-specific models, we assume access to a small
number (32-1000) of unlabeled target-task examples and use those to retrieve
the most similar labeled examples from a large pool of multitask data augmented
with prompts. Compared to the current practice of finetuning models on
uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of
finetuning on cross-task nearest neighbors is significantly more
data-efficient. Using only 2% of the data from the P3 pool without any labeled
target-task data, our models outperform strong baselines trained on all
available data by 3-30% on 12 out of 14 datasets representing held-out tasks
including legal and scientific document QA. Similarly, models trained on
cross-task nearest neighbors from SuperNaturalInstructions, representing about
5% of the pool, obtain comparable performance to state-of-the-art models on 12
held-out tasks from that pool. Moreover, the models produced by our approach
also provide a better initialization than single multitask finetuned models for
few-shot finetuning on target-task data, as shown by a 2-23% relative
improvement over few-shot finetuned T0-3B models on 8 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04325">
<div class="article-summary-box-inner">
<span><p>Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
models, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09535">
<div class="article-summary-box-inner">
<span><p>The BLOOM model is a large publicly available multilingual language model,
but its pretraining was limited to 46 languages. To extend the benefits of
BLOOM to other languages without incurring prohibitively large costs, it is
desirable to adapt BLOOM to new languages not seen during pretraining. In this
work, we apply existing language adaptation strategies to BLOOM and benchmark
its zero-shot prompting performance on eight new languages in a
resource-constrained setting. We find language adaptation to be effective at
improving zero-shot performance in new languages. Surprisingly, we find that
adapter-based finetuning is more effective than continued pretraining for large
models. In addition, we discover that prompting performance is not
significantly affected by language specifics, such as the writing system. It is
primarily determined by the size of the language adaptation data. We also add
new languages to BLOOMZ, which is a multitask finetuned version of BLOOM
capable of following task instructions zero-shot. We find including a new
language in the multitask fine-tuning mixture to be the most effective method
to teach BLOOMZ a new language. We conclude that with sufficient training data
language adaptation can generalize well to diverse languages. Our code is
available at https://github.com/bigscience-workshop/multilingual-modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges. (arXiv:2212.09660v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09660">
<div class="article-summary-box-inner">
<span><p>Code-Switching, a common phenomenon in written text and conversation, has
been studied over decades by the natural language processing (NLP) research
community. Initially, code-switching is intensively explored by leveraging
linguistic theories and, currently, more machine-learning oriented approaches
to develop models. We introduce a comprehensive systematic survey on
code-switching research in natural language processing to understand the
progress of the past decades and conceptualize the challenges and tasks on the
code-switching topic. Finally, we summarize the trends and findings and
conclude with a discussion for future direction and open questions for further
investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09683">
<div class="article-summary-box-inner">
<span><p>We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. We make our data and
detailed annotation guidelines available to support the evaluation of
human-in-the-loop systems that identify novel misinformation directly from raw
user-generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10029">
<div class="article-summary-box-inner">
<span><p>When people think of everyday things like an "egg," they typically have a
mental image associated with it. This commonsense knowledge helps us understand
how these everyday things work and how to interact with them. For example, when
someone tries to make a fried egg, they know that it has a shell and that it
can be cracked open to reveal the egg white and yolk inside. However, if a
system does not have a coherent picture of such everyday things, thinking that
the egg yolk surrounds the shell, then it might have to resort to ridiculous
approaches such as trying to scrape the egg yolk off the shell into the pan. Do
language models have a coherent picture of such everyday things? To investigate
this, we propose a benchmark dataset consisting of 100 everyday things, their
parts, and the relationships between these parts. We observe that
state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have
fragments of knowledge about these entities, but they fail to produce
consistent parts mental models. We propose a simple extension to these LMs
where we apply a constraint satisfaction layer on top of raw predictions from
LMs to produce more consistent and accurate parts mental models of everyday
things.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot Generalisation. (arXiv:2212.10315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10315">
<div class="article-summary-box-inner">
<span><p>Recent NLP models have shown the remarkable ability to effectively generalise
`zero-shot' to new tasks using only natural language instructions as guidance.
However, many of these approaches suffer from high computational costs due to
their reliance on concatenating lengthy instructions with every input example,
resulting in costly reprocessing of the instruction. To avoid this, we
introduce Hypernetworks for INstruction Tuning (HINT), which convert task
instructions and examples into parameter-efficient modules inserted into an
underlying model using a pretrained text encoder, eliminating the need to
include instructions in the model input. The hypernetwork in HINT also produces
an encoded instruction, which we concatenate with encoded inputs during
decoding to further improve performance. HINT models outperform strong
state-of-the-art baselines by over 10% when controlling for compute (measured
in FLOPs). By converting instructions into modules, HINT models can effectively
disregard the length of instructions and few-shot example inputs in terms of
compute usage. As a result, HINT can enhance its performance by up to 25% by
incorporating additional few-shot data, while utilizing only up to 5% more
compute. This combines the strengths of parameter-efficient fine-tuning and
in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Curation Alone Can Stabilize In-context Learning. (arXiv:2212.10378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10378">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) enables large language models (LLMs) to perform new
tasks by prompting them with a sequence of training examples. However, it is
known that ICL is very sensitive to the choice of training examples: randomly
sampling examples from a training set leads to high variance in performance. In
this paper, we show that carefully curating a subset of training data greatly
stabilizes ICL performance without any other changes to the ICL algorithm
(e.g., prompt retrieval or calibration). We introduce two methods to choose
training subsets -- both score training examples individually, then select the
highest-scoring ones. CondAcc scores a training example by its average dev-set
ICL accuracy when combined with random training examples, while Datamodels
learns linear regressors that estimate how the presence of each training
example influences LLM outputs. Across five tasks and two LLMs, sampling from
stable subsets selected by CondAcc and Datamodels improves average accuracy
over sampling from the entire training set by 7.7% and 6.3%, respectively.
Surprisingly, the stable subset examples are not especially diverse in content
or low in perplexity, in contrast with other work suggesting that diversity and
perplexity are important when prompting LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition. (arXiv:2212.10750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10750">
<div class="article-summary-box-inner">
<span><p>The widely studied task of Natural Language Inference (NLI) requires a system
to recognize whether one piece of text is textually entailed by another, i.e.
whether the entirety of its meaning can be inferred from the other. In current
NLI datasets and models, textual entailment relations are typically defined on
the sentence- or paragraph-level. However, even a simple sentence often
contains multiple propositions, i.e. distinct units of meaning conveyed by the
sentence. As these propositions can carry different truth values in the context
of a given premise, we argue for the need to recognize the textual entailment
relation of each proposition in a sentence individually.
</p>
<p>We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert
human raters. Our dataset structure resembles the tasks of (1) segmenting
sentences within a document to the set of propositions, and (2) classifying the
entailment relation of each proposition with respect to a different yet
topically-aligned document, i.e. documents describing the same event or entity.
We establish strong baselines for the segmentation and entailment tasks.
Through case studies on summary hallucination detection and document-level NLI,
we demonstrate that our conceptual framework is potentially useful for
understanding and explaining the compositionality of NLI labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09656">
<div class="article-summary-box-inner">
<span><p>While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises. (arXiv:2302.07324v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07324">
<div class="article-summary-box-inner">
<span><p>For many real-world applications, the user-generated inputs usually contain
various noises due to speech recognition errors caused by linguistic
variations1 or typographical errors (typos). Thus, it is crucial to test model
performance on data with realistic input noises to ensure robustness and
fairness. However, little study has been done to construct such benchmarks for
Chinese, where various language-specific input noises happen in the real world.
In order to fill this important gap, we construct READIN: a Chinese multi-task
benchmark with REalistic And Diverse Input Noises. READIN contains four diverse
tasks and requests annotators to re-enter the original test data with two
commonly used Chinese input methods: Pinyin input and speech input. We designed
our annotation pipeline to maximize diversity, for example by instructing the
annotators to use diverse input method editors (IMEs) for keyboard noises and
recruiting speakers from diverse dialectical groups for speech noises. We
experiment with a series of strong pretrained language models as well as robust
training methods, we find that these models often suffer significant
performance drops on READIN even with robustness methods like data
augmentation. As the first large-scale attempt in creating a benchmark with
noises geared towards user-generated inputs, we believe that READIN serves as
an important complement to existing Chinese NLP benchmarks. The source code and
dataset can be obtained from https://github.com/thunlp/READIN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer models: an introduction and catalog. (arXiv:2302.07730v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07730">
<div class="article-summary-box-inner">
<span><p>In the past few years we have seen the meteoric appearance of dozens of
foundation models of the Transformer family, all of which have memorable and
sometimes funny, but not self-explanatory, names. The goal of this paper is to
offer a somewhat comprehensive but simple catalog and classification of the
most popular Transformer models. The paper also includes an introduction to the
most important aspects and innovations in Transformer models. Our catalog will
include models that are trained using self-supervised learning (e.g., BERT or
GPT3) as well as those that are further trained using a human-in-the-loop (e.g.
the InstructGPT model used by ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07748">
<div class="article-summary-box-inner">
<span><p>Narratives include a rich source of events unfolding over time and context.
Automatic understanding of these events provides a summarised comprehension of
the narrative for further computation (such as reasoning). In this paper, we
study the Information Status (IS) of the events and propose a novel challenging
task: the automatic identification of \textit{new} events in a narrative. We
define an event as a triplet of subject, predicate, and object. The event is
categorized as new with respect to the discourse context and whether it can be
inferred through commonsense reasoning. We annotated a publicly available
corpus of narratives with the new events at sentence level using human
annotators. We present the annotation protocol and study the quality of the
annotation and the difficulty of the task. We publish the annotated dataset,
annotation materials, and machine learning baseline models for the task of new
event extraction for narrative understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08624">
<div class="article-summary-box-inner">
<span><p>In this paper, we present InstructABSA, Aspect Based Sentiment Analysis
(ABSA) using the instruction learning paradigm for the ABSA subtasks: Aspect
Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint
Task modeling. Our method introduces positive, negative, and neutral examples
to each training sample, and instruction tunes the model (Tk-Instruct) the ABSA
subtasks, yielding significant performance improvements. Experimental results
on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA
outperforms the previous state-of-the-art (SOTA) approaches on the three ABSA
subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x
larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE
subtask by 5.69% points, Rest15 ATSC subtask by 9.59% points, and on the Lapt14
Joint Task by 3.37% points. Our results also suggest a strong generalization
ability to new domains across all three subtasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17580">
<div class="article-summary-box-inner">
<span><p>Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are abundant AI models
available for different domains and modalities, they cannot handle complicated
AI tasks. Considering large language models (LLMs) have exhibited exceptional
ability in language understanding, generation, interaction, and reasoning, we
advocate that LLMs could act as a controller to manage existing AI models to
solve complicated AI tasks and language could be a generic interface to empower
this. Based on this philosophy, we present HuggingGPT, a framework that
leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning
communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use
ChatGPT to conduct task planning when receiving a user request, select models
according to their function descriptions available in Hugging Face, execute
each subtask with the selected AI model, and summarize the response according
to the execution results. By leveraging the strong language capability of
ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover
numerous sophisticated AI tasks in different modalities and domains and achieve
impressive results in language, vision, speech, and other challenging tasks,
which paves a new way towards artificial general intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00457">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized natural language processing
and demonstrated impressive capabilities in various tasks. Unfortunately, they
are prone to hallucinations, where the model exposes incorrect or false
information in its responses, which renders diligent evaluation approaches
mandatory. While LLM performance in specific knowledge fields is often
evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually
report only a single accuracy number for the entire field, a procedure which is
problematic with respect to transparency and model improvement. A stratified
evaluation could instead reveal subfields, where hallucinations are more likely
to occur and thus help to better assess LLMs' risks and guide their further
development. To support such stratified evaluations, we propose LLMMaps as a
novel visualization technique that enables users to evaluate LLMs' performance
with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs'
knowledge capabilities in different subfields, by transforming Q&amp;A datasets as
well as LLM responses into our internal knowledge structure. An extension for
comparative visualization furthermore, allows for the detailed comparison of
multiple LLMs. To assess LLMMaps we use them to conduct a comparative analysis
of several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and
LLaMa-13B, as well as two qualitative user evaluations. All necessary source
code and data for generating LLMMaps to be used in scientific publications and
elsewhere will be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05783">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been known to perpetuate biases from the
underlying datasets to downstream tasks. However, these findings are
predominantly based on monolingual language models for English, whereas there
are few investigative studies of biases encoded in language models for
languages beyond English. In this paper, we fill this gap by analysing gender
bias in West Slavic language models. We introduce the first template-based
dataset in Czech, Polish, and Slovak for measuring gender bias towards male,
female and non-binary subjects. We complete the sentences using both mono- and
multilingual language models and assess their suitability for the masked
language modelling objective. Next, we measure gender bias encoded in West
Slavic language models by quantifying the toxicity and genderness of the
generated words. We find that these language models produce hurtful completions
that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and
Polish language models produce more hurtful completions with men as subjects,
which, upon inspection, we find is due to completions being related to
violence, death, and sickness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06767">
<div class="article-summary-box-inner">
<span><p>Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially
significant repercussions. Consequently, aligning these models with human
ethics and preferences is an essential step toward ensuring their responsible
and effective deployment in real-world applications. Prior research has
primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means
of addressing this problem, wherein generative models are fine-tuned using RL
algorithms guided by a human-feedback-informed reward model. However, the
inefficiencies and instabilities associated with RL algorithms frequently
present substantial obstacles to the successful alignment of generative models,
necessitating the development of a more robust and streamlined approach. To
this end, we introduce a new framework, Reward rAnked FineTuning (RAFT),
designed to align generative models more effectively. Utilizing a reward model
and a sufficient number of samples, our approach selects the high-quality
samples, discarding those that exhibit undesired behavior, and subsequently
assembles a streaming dataset. This dataset serves as the basis for aligning
the generative model and can be employed under both offline and online
settings. Notably, the sample generation process within RAFT is gradient-free,
rendering it compatible with black-box generators. Through extensive
experiments, we demonstrate that our proposed algorithm exhibits strong
performance in the context of both large language models and diffusion models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01901">
<div class="article-summary-box-inner">
<span><p>Few-shot event detection (ED) has been widely studied, while this brings
noticeable discrepancies, e.g., various motivations, tasks, and experimental
settings, that hinder the understanding of models for future progress.This
paper presents a thorough empirical study, a unified view of ED models, and a
better unified baseline. For fair evaluation, we compare 12 representative
methods on three datasets, which are roughly grouped into prompt-based and
prototype-based models for detailed analysis. Experiments consistently
demonstrate that prompt-based methods, including ChatGPT, still significantly
trail prototype-based methods in terms of overall performance. To investigate
their superior performance, we break down their design elements along several
dimensions and build a unified framework on prototype-based methods. Under such
unified view, each prototype-method can be viewed a combination of different
modules from these design elements. We further combine all advantageous modules
and propose a simple yet effective baseline, which outperforms existing methods
by a large margin (e.g., 2.7% F1 gains under low-resource setting).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay More Attention to Relation Exploration for Knowledge Base Question Answering. (arXiv:2305.02118v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02118">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) is a challenging task that aims to
retrieve correct answers from large-scale knowledge bases. Existing attempts
primarily focus on entity representation and final answer reasoning, which
results in limited supervision for this task. Moreover, the relations, which
empirically determine the reasoning path selection, are not fully considered in
recent advancements. In this study, we propose a novel framework, RE-KBQA, that
utilizes relations in the knowledge base to enhance entity representation and
introduce additional supervision. We explore guidance from relations in three
aspects, including (1) distinguishing similar entities by employing a
variational graph auto-encoder to learn relation importance; (2) exploring
extra supervision by predicting relation distributions as soft labels with a
multi-task scheme; (3) designing a relation-guided re-ranking algorithm for
post-processing. Experimental results on two benchmark datasets demonstrate the
effectiveness and superiority of our framework, improving the F1 score by 5.7%
from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par
with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation. (arXiv:2305.04720v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04720">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in open-domain dialogue systems, building a
reliable evaluation metric is still a challenging problem. Recent studies
proposed learnable metrics based on classification models trained to
distinguish the correct response. However, neural classifiers are known to make
overly confident predictions for examples from unseen distributions. We propose
DEnsity, which evaluates a response by utilizing density estimation on the
feature space derived from a neural classifier. Our metric measures how likely
a response would appear in the distribution of human conversations. Moreover,
to improve the performance of DEnsity, we utilize contrastive learning to
further compress the feature space. Experiments on multiple response evaluation
datasets show that DEnsity correlates better with human evaluations than the
existing metrics. Our code is available at https://github.com/ddehun/DEnsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Drop of Ink Makes a Million Think: The Spread of False Information in Large Language Models. (arXiv:2305.04812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04812">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have gained increasing prominence in artificial
intelligence, making a profound impact on society and various industries like
business and science. However, the presence of false information on the
internet and in text corpus poses a significant risk to the reliability and
safety of LLMs, underscoring the urgent need to understand the mechanisms of
how false information influences the behaviors of LLMs. In this paper, we dive
into this problem and investigate how false information spreads in LLMs and
affects related responses. Specifically, in our series of experiments, we
investigate different factors that can influence the spread of information in
LLMs by comparing three degrees of information relevance (direct, indirect, and
peripheral), four information source styles (Twitter, web blogs, news reports,
and research papers) and two common knowledge injection paradigms (in-context
injection and learning-based injection). The experimental results show that
(1)False information will spread and contaminate related memories in LLMs via a
semantic diffusion process, i.e., false information has global detrimental
effects beyond its direct impact. (2)Current LLMs are susceptible to authority
bias, i.e., LLMs are more likely to follow false information presented in
trustworthy styles such as news reports and research papers, which usually
cause deeper and wider pollution of information. (3)Current LLMs are more
sensitive to false information through in-context injection than through
learning-based injection, which severely challenges the reliability and safety
of LLMs even when all training data are trusty and correct. The above findings
raise the need for new false information defense algorithms to address the
global impact of false information, and new alignment algorithms to unbiasedly
lead LLMs to follow essential human values rather than superficial patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04835">
<div class="article-summary-box-inner">
<span><p>Compositional generalization--understanding unseen combinations of seen
primitives--is an essential reasoning capability in human intelligence. The AI
community mainly studies this capability by fine-tuning neural networks on lots
of training samples, while it is still unclear whether and how in-context
learning--the prevailing few-shot paradigm based on large language
models--exhibits compositional generalization. In this paper, we present CoFe,
a test suite to investigate in-context compositional generalization. We find
that the compositional generalization performance can be easily affected by the
selection of in-context examples, thus raising the research question what the
key factors are to make good in-context examples for compositional
generalization. We study three potential factors: similarity, diversity and
complexity. Our systematic experiments indicate that in-context examples should
be structurally similar to the test case, diverse from each other, and
individually simple. Furthermore, two strong limitations are observed:
in-context compositional generalization on fictional words is much weaker than
that on commonly used ones; it is still critical that the in-context examples
should cover required linguistic structures, even though the backbone model has
been pre-trained on large corpus. We hope our analysis would facilitate the
understanding and utilization of in-context learning paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06586">
<div class="article-summary-box-inner">
<span><p>We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task
focused on methods to identify complex fine-grained named entities (like
WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and
multilingual scenarios, as well as noisy settings. The task used the MultiCoNER
V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and
Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It
attracted 842 submissions from 47 teams, and 34 teams submitted system papers.
Results showed that complex entity types such as media titles and product names
were the most challenging. Methods fusing external knowledge into transformer
models achieved the best performance, and the largest gains were on the
Creative Work and Group classes, which are still challenging even with external
knowledge. Some fine-grained classes proved to be more challenging than others,
such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data
has a significant impact on model performance, with an average drop of 10% on
the noisy subset. The task highlights the need for future research on improving
NER robustness on noisy data containing complex entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07677">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) have proven to be effective for second-pass
rescoring in Automatic Speech Recognition (ASR) systems. In this work, we
propose Masked Audio Text Encoder (MATE), a multi-modal masked language model
rescorer which incorporates acoustic representations into the input space of
MLM. We adopt contrastive learning for effectively aligning the modalities by
learning shared representations. We show that using a multi-modal rescorer is
beneficial for domain generalization of the ASR system when target domain data
is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and
3%-7% on out-of-domain datasets, over the text-only baseline. Additionally,
with very limited amount of training data (0.8 hours), MATE achieves a WER
reduction of 8%-23% over the first-pass baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07759">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are powerful tools for natural language processing, but
they often struggle to produce coherent and fluent text when they are small.
Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can
rarely generate coherent and consistent English text beyond a few words even
after extensive training. This raises the question of whether the emergence of
the ability to produce coherent English text only occurs at larger scales (with
hundreds of millions of parameters or more) and complex architectures (with
many layers of global attention).
</p>
<p>In this work, we introduce TinyStories, a synthetic dataset of short stories
that only contain words that a typical 3 to 4-year-olds usually understand,
generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train
and evaluate LMs that are much smaller than the state-of-the-art models (below
10 million total parameters), or have much simpler architectures (with only one
transformer block), yet still produce fluent and consistent stories with
several paragraphs that are diverse and have almost perfect grammar, and
demonstrate reasoning capabilities.
</p>
<p>We also introduce a new paradigm for the evaluation of language models: We
suggest a framework which uses GPT-4 to grade the content generated by these
models as if those were stories written by students and graded by a (human)
teacher. This new paradigm overcomes the flaws of standard benchmarks which
often requires the model's output to be very structures, and moreover provides
a multidimensional score for the model, providing scores for different
capabilities such as grammar, creativity and consistency.
</p>
<p>We hope that TinyStories can facilitate the development, analysis and
research of LMs, especially for low-resource or specialized domains, and shed
light on the emergence of language capabilities in LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08339">
<div class="article-summary-box-inner">
<span><p>Chatbots based on Large Language Models (LLMs) have shown strong capabilities
in language understanding. In this study, we explore the potential of LLMs in
assisting corpus-based linguistic studies through automatic annotation of texts
with specific categories of linguistic information. Specifically, we examined
to what extent LLMs understand the functional elements constituting the speech
act of apology from a local grammar perspective, by comparing the performance
of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a
human coder in the annotation task. The results demonstrate that the Bing
chatbot significantly outperformed ChatGPT in the task. Compared to human
annotator, the overall performance of the Bing chatbot was slightly less
satisfactory. However, it already achieved high F1 scores: 99.95% for the tag
of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for
APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to
use LLM-assisted annotation for local grammar analysis, together with human
intervention on tags that are less accurately recognized by machine. We
strongly advocate conducting future studies to evaluate the performance of LLMs
in annotating other linguistic phenomena. These studies have the potential to
offer valuable insights into the advancement of theories developed in corpus
linguistics, as well into the linguistic capabilities of LLMs..
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08711">
<div class="article-summary-box-inner">
<span><p>We present $\text{sustain.AI}$, an intelligent, context-aware recommender
system that assists auditors and financial investors as well as the general
public to efficiently analyze companies' sustainability reports. The tool
leverages an end-to-end trainable architecture that couples a BERT-based
encoding module with a multi-label classification head to match relevant text
passages from sustainability reports to their respective law regulations from
the Global Reporting Initiative (GRI) standards. We evaluate our model on two
novel German sustainability reporting data sets and consistently achieve a
significantly higher recommendation performance compared to multiple strong
baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone
at https://sustain.ki.nrw/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10568">
<div class="article-summary-box-inner">
<span><p>Noun compound interpretation is the task of expressing a noun compound (e.g.
chocolate bunny) in a free-text paraphrase that makes the relationship between
the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose
modifications to the data and evaluation setup of the standard task (Hendrickx
et al., 2013), and show that GPT-3 solves it almost perfectly. We then
investigate the task of noun compound conceptualization, i.e. paraphrasing a
novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped
chocolate. This task requires creativity, commonsense, and the ability to
generalize knowledge about similar concepts. While GPT-3's performance is not
perfect, it is better than that of humans -- likely thanks to its access to
vast amounts of knowledge, and because conceptual processing is effortful for
people (Connell and Lynott, 2012). Finally, we estimate the extent to which
GPT-3 is reasoning about the world vs. parroting its training data. We find
that the outputs from GPT-3 often have significant overlap with a large web
corpus, but that the parroting strategy is less beneficial for novel noun
compounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10680">
<div class="article-summary-box-inner">
<span><p>Estimating confidence scores for recognition results is a classic task in ASR
field and of vital importance for kinds of downstream tasks and training
strategies. Previous end-to-end~(E2E) based confidence estimation models (CEM)
predict score sequences of equal length with input transcriptions, leading to
unreliable estimation when deletion and insertion errors occur. In this paper
we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve
accurate and reliable confidence estimation based on novel non-autoregressive
E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of
continuous integrate-and-fire (CIF) mechanism to generate token-synchronous
acoustic embedding, which solves the estimation failure issue above. We measure
the quality of estimation with AUC and RMSE in token level and ECE-U - a
proposed metrics in utterance level. CA-CEM gains 24% and 19% relative
reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore,
we conduct analysis to explore the potential of CEM for different ASR related
usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11255">
<div class="article-summary-box-inner">
<span><p>While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is at
https://github.com/scofield7419/THOR-ISA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11442">
<div class="article-summary-box-inner">
<span><p>Existing solutions to zero-shot text classification either conduct prompting
with pre-trained language models, which is sensitive to the choices of
templates, or rely on large-scale annotated data of relevant tasks for
meta-tuning. In this work, we propose a new paradigm based on self-supervised
learning to solve zero-shot text classification tasks by tuning the language
models with unlabeled data, called self-supervised tuning. By exploring the
inherent structure of free texts, we propose a new learning objective called
first sentence prediction to bridge the gap between unlabeled data and text
classification tasks. After tuning the model to learn to predict the first
sentence in a paragraph based on the rest, the model is able to conduct
zero-shot inference on unseen tasks such as topic classification and sentiment
analysis. Experimental results show that our model outperforms the
state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals
that our model is less sensitive to the prompt design. Our code and pre-trained
models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11719">
<div class="article-summary-box-inner">
<span><p>Existing research on multimodal relation extraction (MRE) faces two
co-existing challenges, internal-information over-utilization and
external-information under-exploitation. To combat that, we propose a novel
framework that simultaneously implements the idea of internal-information
screening and external-information exploiting. First, we represent the
fine-grained semantic structures of the input image and text with the visual
and textual scene graphs, which are further fused into a unified cross-modal
graph (CMG). Based on CMG, we perform structure refinement with the guidance of
the graph information bottleneck principle, actively denoising the
less-informative features. Next, we perform topic modeling over the input image
and text, incorporating latent multimodal topic features to enrich the
contexts. On the benchmark MRE dataset, our system outperforms the current best
model significantly. With further in-depth analyses, we reveal the great
potential of our method for the MRE task. Our codes are open at
https://github.com/ChocoWu/MRE-ISE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11768">
<div class="article-summary-box-inner">
<span><p>Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12086">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning aims to mitigate the large memory requirements of
adapting pretrained language models for downstream tasks. For example, one
popular method, prefix-tuning, prepends trainable tokens to sequences while
freezing the rest of the model's parameters. Although such models attain
comparable performance with fine-tuning when applied to sequences with short to
moderate lengths, we show their inferior performance when modelling long
sequences. To bridge this gap, we propose prefix-propagation, a simple but
effective approach that conditions prefixes on previous hidden states. We
empirically demonstrate that prefix-propagation outperforms prefix-tuning
across long-document tasks, while using 50% fewer parameters. To further
investigate the proposed architecture, we also show its advantage in
calibration, and perform additional study on its relationship with kernel
attention. To the best of our knowledge, this work is the first to focus on
parameter-efficient learning for long-sequence language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Development of NLP models. (arXiv:2305.12219v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12219">
<div class="article-summary-box-inner">
<span><p>Despite substantial advancements, Natural Language Processing (NLP) models
often require post-training adjustments to enforce business rules, rectify
undesired behavior, and align with user values. These adjustments involve
operationalizing "concepts"--dictating desired model responses to certain
inputs. However, it's difficult for a single entity to enumerate and define all
possible concepts, indicating a need for a multi-user, collaborative model
alignment framework. Moreover, the exhaustive delineation of a concept is
challenging, and an improper approach can create shortcuts or interfere with
original data or other concepts.
</p>
<p>To address these challenges, we introduce CoDev, a framework that enables
multi-user interaction with the model, thereby mitigating individual
limitations. CoDev aids users in operationalizing their concepts using Large
Language Models, and relying on the principle that NLP models exhibit simpler
behaviors in local regions. Our main insight is learning a \emph{local} model
for each concept, and a \emph{global} model to integrate the original data with
all concepts. We then steer a large language model to generate instances within
concept boundaries where local and global disagree. Our experiments show CoDev
is effective at helping multiple users operationalize concepts and avoid
interference for a variety of scenarios, tasks, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination. (arXiv:2305.12256v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12256">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate a more realistic unsupervised multimodal machine
translation (UMMT) setup, inference-time image-free UMMT, where the model is
trained with source-text image pairs, and tested with only source-text inputs.
First, we represent the input images and texts with the visual and language
scene graphs (SG), where such fine-grained vision-language features ensure a
holistic understanding of the semantics. To enable pure-text input during
inference, we devise a visual scene hallucination mechanism that dynamically
generates pseudo visual SG from the given textual SG. Several SG-pivoting based
learning objectives are introduced for unsupervised translation training. On
the benchmark Multi30K data, our SG-based method outperforms the
best-performing baseline by significant BLEU scores on the task and setup,
helping yield translations with better completeness, relevance and fluency
without relying on paired images. Further in-depth analyses reveal how our
model advances in the task setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction. (arXiv:2305.12258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12258">
<div class="article-summary-box-inner">
<span><p>Latest efforts on cross-lingual relation extraction (XRE) aggressively
leverage the language-consistent structural features from the universal
dependency (UD) resource, while they may largely suffer from biased transfer
(e.g., either target-biased or source-biased) due to the inevitable linguistic
disparity between languages. In this work, we investigate an unbiased UD-based
XRE transfer by constructing a type of code-mixed UD forest. We first translate
the sentence of the source language to the parallel target-side language, for
both of which we parse the UD tree respectively. Then, we merge the
source-/target-side UD structures as a unified code-mixed UD forest. With such
forest features, the gaps of UD-based XRE between the training and predicting
phases can be effectively closed. We conduct experiments on the ACE XRE
benchmark datasets, where the results demonstrate that the proposed code-mixed
UD forests help unbiased UD-based XRE transfer, with which we achieve
significant XRE performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment. (arXiv:2305.12260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12260">
<div class="article-summary-box-inner">
<span><p>Unpaired cross-lingual image captioning has long suffered from irrelevancy
and disfluency issues, due to the inconsistencies of the semantic scene and
syntax attributes during transfer. In this work, we propose to address the
above problems by incorporating the scene graph (SG) structures and the
syntactic constituency (SC) trees. Our captioner contains the semantic
structure-guided image-to-pivot captioning and the syntactic structure-guided
pivot-to-target translation, two of which are joined via pivot language. We
then take the SG and SC structures as pivoting, performing cross-modal semantic
structure alignment and cross-lingual syntactic structure alignment learning.
We further introduce cross-lingual&amp;cross-modal back-translation training to
fully align the captioning and translation stages. Experiments on
English-Chinese transfers show that our model shows great superiority in
improving captioning relevancy and fluency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction. (arXiv:2305.12678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12678">
<div class="article-summary-box-inner">
<span><p>Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews
based on predicted helpfulness scores and has been widely applied in e-commerce
via presenting customers with useful reviews. Previous studies commonly employ
fully-connected neural networks (FCNNs) as the final score predictor and
pairwise loss as the training objective. However, FCNNs have been shown to
perform inefficient splitting for review features, making the model difficult
to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise
objective, which works on review pairs, may not completely capture the MRHP
goal to produce the ranking for the entire review list, and possibly induces
low generalization during testing. To address these issues, we propose a
listwise attention network that clearly captures the MRHP ranking context and a
listwise optimization objective that enhances model generalization. We further
propose gradient-boosted decision tree as the score predictor to efficaciously
partition product reviews' representations. Extensive experiments demonstrate
that our method achieves state-of-the-art results and polished generalization
performance on two large-scale MRHP benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIT: Far-reaching Interleaved Transformers. (arXiv:2305.12689v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12689">
<div class="article-summary-box-inner">
<span><p>We present FIT: a transformer-based architecture with efficient
self-attention and adaptive computation. Unlike original transformers, which
operate on a single sequence of data tokens, we divide the data tokens into
groups, with each group being a shorter sequence of tokens. We employ two types
of transformer layers: local layers operate on data tokens within each group,
while global layers operate on a smaller set of introduced latent tokens. These
layers, comprising the same set of self-attention and feed-forward layers as
standard transformers, are interleaved, and cross-attention is used to
facilitate information exchange between data and latent tokens within the same
group. The attention complexity is $O(n^2)$ locally within each group of size
$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The
efficiency can be further enhanced by relying more on global layers that
perform adaptive computation using a smaller set of latent tokens. FIT is a
versatile architecture and can function as an encoder, diffusion decoder, or
autoregressive decoder. We provide initial evidence demonstrating its
effectiveness in high-resolution image understanding and generation tasks.
Notably, FIT exhibits potential in performing end-to-end training on
gigabit-scale data, such as 6400$\times$6400 images, or 160K tokens (after
patch tokenization), within a memory capacity of 16GB, without requiring
specific optimizations or model parallelism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optimal Policy for Simultaneous Machine Translation via Binary Search. (arXiv:2305.12774v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12774">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) starts to output translation while
reading the source sentence and needs a precise policy to decide when to output
the generated translation. Therefore, the policy determines the number of
source tokens read during the translation of each target token. However, it is
difficult to learn a precise translation policy to achieve good latency-quality
trade-offs, because there is no golden policy corresponding to parallel
sentences as explicit supervision. In this paper, we present a new method for
constructing the optimal policy online via binary search. By employing explicit
supervision, our approach enables the SiMT model to learn the optimal policy,
which can guide the model in completing the translation during inference.
Experiments on four translation tasks show that our method can exceed strong
baselines across all latency scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels. (arXiv:2305.13137v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13137">
<div class="article-summary-box-inner">
<span><p>The increasing adoption of text-to-speech technologies has led to a growing
demand for natural and emotive voices that adapt to a conversation's context
and emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is a
unique speech dataset created to enhance conversations' expressiveness and
emotive quality in interactive narrative-driven systems. The corpus consists of
a 2.3-hour recording featuring a female speaker delivering labelled utterances.
It encompasses eight acted emotional states, evenly distributed with a variance
of 0.68%, along with expressiveness levels and natural language descriptions
with word emphasis labels. The evaluation of audio samples from different
datasets revealed that the EMNS corpus achieved the highest average scores in
accurately conveying emotions and demonstrating expressiveness. It outperformed
other datasets in conveying shared emotions and achieved comparable levels of
genuineness. A classification task confirmed the accurate representation of
intended emotions in the corpus, with participants recognising the recordings
as genuine and expressive. Additionally, the availability of the dataset
collection tool under the Apache 2.0 License simplifies remote speech data
collection for researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design. (arXiv:2305.13297v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13297">
<div class="article-summary-box-inner">
<span><p>This paper investigates the key role of Feed-Forward Networks (FFNs) in
transformer models by utilizing the Parallel Attention and Feed-Forward Net
Design (PAF) architecture, and comparing it to their Series Attention and
Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF
are two main assumptions regarding the FFN block and the attention block within
a layer: 1) the primary function of the FFN block is to maintain isotropy among
token embeddings and prevent their degeneration, and 2) the residual norm
computed in the attention block is substantially smaller than the input token
embedding norm. To empirically validate these assumptions, we train PAF
variants of two large language models (RoBERTa-large and bert-large-uncased).
Our results demonstrate that both assumptions hold true in the PAF design. This
study contributes to a deeper understanding of the roles and interactions
between FFNs and self-attention mechanisms in transformer architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Readability Assessment for Closely Related Languages. (arXiv:2305.13478v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13478">
<div class="article-summary-box-inner">
<span><p>In recent years, the main focus of research on automatic readability
assessment (ARA) has shifted towards using expensive deep learning-based
methods with the primary goal of increasing models' accuracy. This, however, is
rarely applicable for low-resource languages where traditional handcrafted
features are still widely used due to the lack of existing NLP tools to extract
deeper linguistic representations. In this work, we take a step back from the
technical component and focus on how linguistic aspects such as mutual
intelligibility or degree of language relatedness can improve ARA in a
low-resource setting. We collect short stories written in three languages in
the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment
models and explore the interaction of data and features in various
cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel
specialized feature exploiting n-gram overlap applied to languages with high
mutual intelligibility, significantly improves the performance of ARA models
compared to the use of off-the-shelf large multilingual language models alone.
Consequently, when both linguistic representations are combined, we achieve
state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA
in Bikol.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14635">
<div class="article-summary-box-inner">
<span><p>End-to-end speech translation (ST) is the task of translating speech signals
in the source language into text in the target language. As a cross-modal task,
end-to-end ST is difficult to train with limited data. Existing methods often
try to transfer knowledge from machine translation (MT), but their performances
are restricted by the modality gap between speech and text. In this paper, we
propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality
gap. We find the alignment between speech and text sequences via optimal
transport and then mix up the sequences from different modalities at a token
level using the alignment. Experiments on the MuST-C ST benchmark demonstrate
that CMOT achieves an average BLEU of 30.0 in 8 translation directions,
outperforming previous methods. Further analysis shows CMOT can adaptively find
the alignment between modalities, which helps alleviate the modality gap
between speech and text. Code is publicly available at
https://github.com/ictnlp/CMOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14652">
<div class="article-summary-box-inner">
<span><p>Video multimodal fusion aims to integrate multimodal signals in videos, such
as visual, audio and text, to make a complementary prediction with multiple
modalities contents. However, unlike other image-text multimodal tasks, video
has longer multimodal sequences with more redundancy and noise in both visual
and audio modalities. Prior denoising methods like forget gate are coarse in
the granularity of noise filtering. They often suppress the redundant and noisy
information at the risk of losing critical information. Therefore, we propose a
denoising bottleneck fusion (DBF) model for fine-grained video multimodal
fusion. On the one hand, we employ a bottleneck mechanism to filter out noise
and redundancy with a restrained receptive field. On the other hand, we use a
mutual information maximization module to regulate the filter-out module to
preserve key information within different modalities. Our DBF model achieves
significant improvement over current state-of-the-art baselines on multiple
benchmarks covering multimodal sentiment analysis and multimodal summarization
tasks. It proves that our model can effectively capture salient features from
noisy and redundant video, audio, and text inputs. The code for this paper is
publicly available at https://github.com/WSXRHFG/DBF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utility-Probability Duality of Neural Networks. (arXiv:2305.14859v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14859">
<div class="article-summary-box-inner">
<span><p>It is typically understood that the training of modern neural networks is a
process of fitting the probability distribution of desired output. However,
recent paradoxical observations in a number of language generation tasks let
one wonder if this canonical probability-based explanation can really account
for the empirical success of deep learning. To resolve this issue, we propose
an alternative utility-based explanation to the standard supervised learning
procedure in deep learning. The basic idea is to interpret the learned neural
network not as a probability model but as an ordinal utility function that
encodes the preference revealed in training data. In this perspective, training
of the neural network corresponds to a utility learning process. Specifically,
we show that for all neural networks with softmax outputs, the SGD learning
dynamic of maximum likelihood estimation (MLE) can be seen as an iteration
process that optimizes the neural network toward an optimal utility function.
This utility-based interpretation can explain several otherwise-paradoxical
observations about the neural networks thus trained. Moreover, our
utility-based theory also entails an equation that can transform the learned
utility values back to a new kind of probability estimation with which
probability-compatible decision rules enjoy dramatic (double-digits)
performance improvements. These evidences collectively reveal a phenomenon of
utility-probability duality in terms of what modern neural networks are (truly)
modeling: We thought they are one thing (probabilities), until the
unexplainable showed up; changing mindset and treating them as another thing
(utility values) largely reconcile the theory, despite remaining subtleties
regarding its original (probabilistic) identity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RefGPT: Reference -> Truthful & Customized Dialogues Generation by GPTs and for GPTs. (arXiv:2305.14994v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14994">
<div class="article-summary-box-inner">
<span><p>General chat models, like ChatGPT, have attained impressive capability to
resolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with
high-quality instruction data. However, collecting human-written high-quality
data, especially multi-turn dialogues, is expensive and unattainable for most
people. Though previous studies have used powerful LLMs to generate the
dialogues automatically, but they all suffer from generating untruthful
dialogues because of the LLMs hallucination. Therefore, we propose a method
called RefGPT to generate enormous truthful and customized dialogues without
worrying about factual errors caused by the model hallucination. RefGPT solves
the model hallucination in dialogue generation by restricting the LLMs to
leverage the given reference instead of reciting their own knowledge to
generate dialogues. Additionally, RefGPT adds detailed controls on every
utterances to enable highly customization capability, which previous studies
have ignored. On the basis of RefGPT, we also propose two high-quality dialogue
datasets generated by GPT-4, namely RefGPT-Fact and RefGPT-Code. RefGPT-Fact is
100k multi-turn dialogue datasets based on factual knowledge and RefGPT-Code is
76k multi-turn dialogue dataset covering a wide range of coding scenarios. Our
code and datasets are released in https://github.com/ziliwangnlp/RefGPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11300">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning has proved to be a valuable component
for out-of-distribution (OoD) detection with only the texts of in-distribution
(ID) examples. These approaches either train a language model from scratch or
fine-tune a pre-trained language model using ID examples, and then take the
perplexity output by the language model as OoD scores. In this paper, we
analyze the complementary characteristics of both OoD detection methods and
propose a multi-level knowledge distillation approach that integrates their
strengths while mitigating their limitations. Specifically, we use a fine-tuned
model as the teacher to teach a randomly initialized student model on the ID
examples. Besides the prediction layer distillation, we present a
similarity-based intermediate layer distillation method to thoroughly explore
the representation space of the teacher model. In this way, the learned student
can better represent the ID data manifold while gaining a stronger ability to
map OoD examples outside the ID data manifold with the regularization inherited
from pre-training. Besides, the student model sees only ID examples during
parameter learning, further promoting more distinguishable features for OoD
detection. We conduct extensive experiments over multiple benchmark datasets,
i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the
proposed method yields new state-of-the-art performance. We also explore its
application as an AIGC detector to distinguish between answers generated by
ChatGPT and human experts. It is observed that our model exceeds human
evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10823">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the relation embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 5.8% on the two datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-26 23:11:18.527920258 UTC">2023-05-26 23:11:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>