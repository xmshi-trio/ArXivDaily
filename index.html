<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-07T01:30:00Z">11-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">LMentry: A Language Model Benchmark of Elementary Language Tasks. (arXiv:2211.02069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02069">
<div class="article-summary-box-inner">
<span><p>As the performance of large language models rapidly improves, benchmarks are
getting larger and more complex as well. We present LMentry, a benchmark that
avoids this "arms race" by focusing on a compact set of tasks that are trivial
to humans, e.g. writing a sentence containing a specific word, identifying
which words in a list belong to a specific category, or choosing which of two
words is longer. LMentry is specifically designed to provide quick and
interpretable insights into the capabilities and robustness of large language
models. Our experiments reveal a wide variety of failure cases that, while
immediately obvious to humans, pose a considerable challenge for large language
models, including OpenAI's latest 175B-parameter instruction-tuned model,
TextDavinci002. LMentry complements contemporary evaluation approaches of large
language models, providing a quick, automatic, and easy-to-run "unit test",
without resorting to large benchmark suites of complex tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic. (arXiv:2211.02098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02098">
<div class="article-summary-box-inner">
<span><p>Through their transfer learning abilities, highly-parameterized large
pre-trained language models have dominated the NLP landscape for a multitude of
downstream language tasks. Though linguistically proficient, the inability of
these models to incorporate the learning of non-linguistic entities (numerals
and arithmetic reasoning) limits their usage for tasks that require numeric
comprehension or strict mathematical reasoning. However, as we illustrate in
this paper, building a general purpose language model that also happens to be
proficient in mathematical reasoning is not as straight-forward as training it
on a numeric dataset. In this work, we develop a novel framework that enables
language models to be mathematically proficient while retaining their
linguistic prowess. Specifically, we offer information-theoretic interventions
to overcome the catastrophic forgetting of linguistic skills that occurs while
injecting non-linguistic skills into language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logographic Information Aids Learning Better Representations for Natural Language Inference. (arXiv:2211.02136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02136">
<div class="article-summary-box-inner">
<span><p>Statistical language models conventionally implement representation learning
based on the contextual distribution of words or other formal units, whereas
any information related to the logographic features of written text are often
ignored, assuming they should be retrieved relying on the cooccurence
statistics. On the other hand, as language models become larger and require
more data to learn reliable representations, such assumptions may start to fall
back, especially under conditions of data sparsity. Many languages, including
Chinese and Vietnamese, use logographic writing systems where surface forms are
represented as a visual organization of smaller graphemic units, which often
contain many semantic cues. In this paper, we present a novel study which
explores the benefits of providing language models with logographic information
in learning better semantic representations. We test our hypothesis in the
natural language inference (NLI) task by evaluating the benefit of computing
multi-modal representations that combine contextual information with glyph
information. Our evaluation results in six languages with different typology
and writing systems suggest significant benefits of using multi-modal
embeddings in languages with logograhic systems, especially for words with less
occurence statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-aware Prompting for Text Generation. (arXiv:2211.02162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02162">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the effects of incorporating timestamps, such as
document creation dates, into generation systems. Two types of time-aware
prompts are investigated: (1) textual prompts that encode document timestamps
in natural language sentences; and (2) linear prompts that convert timestamps
into continuous vectors. To explore extrapolation to future data points, we
further introduce a new data-to-text generation dataset, TempWikiBio,
containing more than 4 millions of chronologically ordered revisions of
biographical articles from English Wikipedia, each paired with structured
personal profiles. Through data-to-text generation on TempWikiBio, text-to-text
generation on the content transfer dataset, and summarization on XSum, we show
that linear prompts on encoder and textual prompts improve the generation
quality on all datasets. Despite having less performance drop when testing on
data drawn from a later time, linear prompts focus more on non-temporal
information and are less sensitive to the given timestamps, according to human
evaluations and sensitivity analyses. Meanwhile, textual prompts establish the
association between the given timestamps and the output dates, yielding more
factual temporal information in the output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Video Moment Retrieval With Off-the-Shelf Models. (arXiv:2211.02178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02178">
<div class="article-summary-box-inner">
<span><p>For the majority of the machine learning community, the expensive nature of
collecting high-quality human-annotated data and the inability to efficiently
finetune very large state-of-the-art pretrained models on limited compute are
major bottlenecks for building models for new tasks. We propose a zero-shot
simple approach for one such task, Video Moment Retrieval (VMR), that does not
perform any additional finetuning and simply repurposes off-the-shelf models
trained on other tasks. Our three-step approach consists of moment proposal,
moment-query matching and postprocessing, all using only off-the-shelf models.
On the QVHighlights benchmark for VMR, we vastly improve performance of
previous zero-shot approaches by at least 2.5x on all metrics and reduce the
gap between zero-shot and state-of-the-art supervised by over 74%. Further, we
also show that our zero-shot approach beats non-pretrained supervised models on
the Recall metrics and comes very close on mAP metrics; and that it also
performs better than the best pretrained supervised model on shorter moments.
Finally, we ablate and analyze our results and propose interesting future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC 2022. (arXiv:2211.02200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02200">
<div class="article-summary-box-inner">
<span><p>We introduce efficient deep learning-based methods for legal document
processing including Legal Document Retrieval and Legal Question Answering
tasks in the Automated Legal Question Answering Competition (ALQAC 2022). In
this competition, we achieve 1\textsuperscript{st} place in the first task and
3\textsuperscript{rd} place in the second task. Our method is based on the
XLM-RoBERTa model that is pre-trained from a large amount of unlabeled corpus
before fine-tuning to the specific tasks. The experimental results showed that
our method works well in legal retrieval information tasks with limited labeled
data. Besides, this method can be applied to other information retrieval tasks
in low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Regularization: an Inductive Bias for Sequence Modeling. (arXiv:2211.02255v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02255">
<div class="article-summary-box-inner">
<span><p>Various forms of regularization in learning tasks strive for different
notions of simplicity. This paper presents a spectral regularization technique,
which attaches a unique inductive bias to sequence modeling based on an
intuitive concept of simplicity defined in the Chomsky hierarchy. From
fundamental connections between Hankel matrices and regular grammars, we
propose to use the trace norm of the Hankel matrix, the tightest convex
relaxation of its rank, as the spectral regularizer. To cope with the fact that
the Hankel matrix is bi-infinite, we propose an unbiased stochastic estimator
for its trace norm. Ultimately, we demonstrate experimental results on Tomita
grammars, which exhibit the potential benefits of spectral regularization and
validate the proposed stochastic estimator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis. (arXiv:2211.02269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02269">
<div class="article-summary-box-inner">
<span><p>Prior work on ideology prediction has largely focused on single modalities,
i.e., text or images. In this work, we introduce the task of multimodal
ideology prediction, where a model predicts binary or five-point scale
ideological leanings, given a text-image pair with political content. We first
collect five new large-scale datasets with English documents and images along
with their ideological leanings, covering news articles from a wide range of US
mainstream media and social media posts from Reddit and Twitter. We conduct
in-depth analyses of news articles and reveal differences in image content and
usage across the political spectrum. Furthermore, we perform extensive
experiments and ablation studies, demonstrating the effectiveness of targeted
pretraining objectives on different model components. Our best-performing
model, a late-fusion architecture pretrained with a triplet objective over
multimodal content, outperforms the state-of-the-art text-only model by almost
4% and a strong multimodal baseline with no pretraining by over 3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiWOZ-DF -- A Dataflow implementation of the MultiWOZ dataset. (arXiv:2211.02303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02303">
<div class="article-summary-box-inner">
<span><p>Semantic Machines (SM) have introduced the use of the dataflow (DF) paradigm
to dialogue modelling, using computational graphs to hierarchically represent
user requests, data, and the dialogue history [Semantic Machines et al. 2020].
Although the main focus of that paper was the SMCalFlow dataset (to date, the
only dataset with "native" DF annotations), they also reported some results of
an experiment using a transformed version of the commonly used MultiWOZ dataset
[Budzianowski et al. 2018] into a DF format. In this paper, we expand the
experiments using DF for the MultiWOZ dataset, exploring some additional
experimental set-ups. The code and instructions to reproduce the experiments
reported here have been released. The contributions of this paper are: 1.) A DF
implementation capable of executing MultiWOZ dialogues; 2.) Several versions of
conversion of MultiWOZ into a DF format are presented; 3.) Experimental results
on state match and translation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02332">
<div class="article-summary-box-inner">
<span><p>The sequence length along the time axis is often the dominant factor of the
computational cost of self-supervised speech models. Works have been proposed
to reduce the sequence length for lowering the computational cost. However,
different downstream tasks have different tolerance of sequence compressing, so
a model that produces a fixed compressing rate may not fit all tasks. In this
work, we introduce a once-for-all (OFA) sequence compression framework for
self-supervised speech models that supports a continuous range of compressing
rates. The framework is evaluated on various tasks, showing marginal
degradation compared to the fixed compressing rate variants with a smooth
performance-efficiency trade-off. We further explore adaptive compressing rate
learning, demonstrating the ability to select task-specific preferred frame
periods without needing a grid search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Latency Training of Sequence Transducers for Streaming End-to-End Speech Recognition. (arXiv:2211.02333v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02333">
<div class="article-summary-box-inner">
<span><p>Sequence transducers, such as the RNN-T and the Conformer-T, are one of the
most promising models of end-to-end speech recognition, especially in streaming
scenarios where both latency and accuracy are important. Although various
methods, such as alignment-restricted training and FastEmit, have been studied
to reduce the latency, latency reduction is often accompanied with a
significant degradation in accuracy. We argue that this suboptimal performance
might be caused because none of the prior methods explicitly model and reduce
the latency. In this paper, we propose a new training method to explicitly
model and reduce the latency of sequence transducer models. First, we define
the expected latency at each diagonal line on the lattice, and show that its
gradient can be computed efficiently within the forward-backward algorithm.
Then we augment the transducer loss with this expected latency, so that an
optimal trade-off between latency and accuracy is achieved. Experimental
results on the WSJ dataset show that the proposed minimum latency training
reduces the latency of causal Conformer-T from 220 ms to 27 ms within a WER
degradation of 0.7%, and outperforms conventional alignment-restricted training
(110 ms) and FastEmit (67 ms) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Name Entity Recognition and Intent Classification Employing Deep Learning Architectures. (arXiv:2211.02415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02415">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition and Intent Classification are among the most
important subfields of the field of Natural Language Processing. Recent
research has lead to the development of faster, more sophisticated and
efficient models to tackle the problems posed by those two tasks. In this work
we explore the effectiveness of two separate families of Deep Learning networks
for those tasks: Bidirectional Long Short-Term networks and Transformer-based
networks. The models were trained and tested on the ATIS benchmark dataset for
both English and Greek languages. The purpose of this paper is to present a
comparative study of the two groups of networks for both languages and showcase
the results of our experiments. The models, being the current state-of-the-art,
yielded impressive results and achieved high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLSE: Corpus of Linguistically Significant Entities. (arXiv:2211.02423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02423">
<div class="article-summary-box-inner">
<span><p>One of the biggest challenges of natural language generation (NLG) is the
proper handling of named entities. Named entities are a common source of
grammar mistakes such as wrong prepositions, wrong article handling, or
incorrect entity inflection. Without factoring linguistic representation, such
errors are often underrepresented when evaluating on a small set of arbitrarily
picked argument values, or when translating a dataset from a linguistically
simpler language, like English, to a linguistically complex language, like
Russian. However, for some applications, broadly precise grammatical
correctness is critical -- native speakers may find entity-related grammar
errors silly, jarring, or even offensive.
</p>
<p>To enable the creation of more linguistically diverse NLG datasets, we
release a Corpus of Linguistically Significant Entities (CLSE) annotated by
linguist experts. The corpus includes 34 languages and covers 74 different
semantic types to support various applications from airline ticketing to video
games. To demonstrate one possible use of CLSE, we produce an augmented version
of the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and a
small number of human translations, we create a linguistically representative
NLG evaluation benchmark in three languages: French (high-resource), Marathi
(low-resource), and Russian (highly inflected language). We establish quality
baselines for neural, template-based, and hybrid NLG systems and discuss the
strengths and weaknesses of each approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dealing with Abbreviations in the Slovenian Biographical Lexicon. (arXiv:2211.02429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02429">
<div class="article-summary-box-inner">
<span><p>Abbreviations present a significant challenge for NLP systems because they
cause tokenization and out-of-vocabulary errors. They can also make the text
less readable, especially in reference printed books, where they are
extensively used. Abbreviations are especially problematic in low-resource
settings, where systems are less robust to begin with. In this paper, we
propose a new method for addressing the problems caused by a high density of
domain-specific abbreviations in a text. We apply this method to the case of a
Slovenian biographical lexicon and evaluate it on a newly developed
gold-standard dataset of 51 Slovenian biographies. Our abbreviation
identification method performs significantly better than commonly used ad-hoc
solutions, especially at identifying unseen abbreviations. We also propose and
present the results of a method for expanding the identified abbreviations in
context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMAuC -- The Scientific Multi-Authorship Corpus. (arXiv:2211.02477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02477">
<div class="article-summary-box-inner">
<span><p>With an ever-growing number of new publications each day, scientific writing
poses an interesting domain for authorship analysis of both single-author and
multi-author documents. Unfortunately, most existing corpora lack either
material from the science domain or the required metadata. Hence, we present
SMAuC, a new metadata-rich corpus designed specifically for authorship analysis
in scientific writing. With more than three million publications from various
scientific disciplines, SMAuC is the largest openly available corpus for
authorship analysis to date. It combines a wide and diverse range of scientific
texts from the humanities and natural sciences with rich and curated metadata,
including unique and carefully disambiguated author IDs. We hope SMAuC will
contribute significantly to advancing the field of authorship analysis in the
science domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing. (arXiv:2211.02483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02483">
<div class="article-summary-box-inner">
<span><p>The explosion of e-commerce has caused the need for processing and analysis
of product titles, like entity typing in product titles. However, the rapid
activity in e-commerce has led to the rapid emergence of new entities, which is
difficult to be solved by general entity typing. Besides, product titles in
e-commerce have very different language styles from text data in general
domain. In order to handle new entities in product titles and address the
special language styles problem of product titles in e-commerce domain, we
propose our textual entailment model with continuous prompt tuning based
hypotheses and fusion embeddings for e-commerce entity typing. First, we
reformulate the entity typing task into a textual entailment problem to handle
new entities that are not present during training. Second, we design a model to
automatically generate textual entailment hypotheses using a continuous prompt
tuning method, which can generate better textual entailment hypotheses without
manual design. Third, we utilize the fusion embeddings of BERT embedding and
CharacterBERT embedding with a two-layer MLP classifier to solve the problem
that the language styles of product titles in e-commerce are different from
that of general domain. To analyze the effect of each contribution, we compare
the performance of entity typing and textual entailment model, and conduct
ablation studies on continuous prompt tuning and fusion embeddings. We also
evaluate the impact of different prompt template initialization for the
continuous prompt tuning. We show our proposed model improves the average F1
score by around 2% compared to the baseline BERT entity typing model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02499">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce our work of building a Streaming Multilingual
Speech Model (SM2), which can transcribe or translate multiple spoken languages
into texts of the target language. The backbone of SM2 is Transformer
Transducer, which has high streaming capability. Instead of human labeled
speech translation (ST) data, SM2 models are trained using weakly supervised
data generated by converting the transcriptions in speech recognition corpora
with a machine translation service. With 351 thousand hours of anonymized
speech training data from 25 languages, SM2 models achieve comparable or even
better ST quality than some recent popular large-scale non-streaming speech
models. More importantly, we show that SM2 has the truly zero-shot capability
when expanding to new target languages, yielding high quality ST results for
{source-speech, target-text} pairs that are not seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT for Long Documents: A Case Study of Automated ICD Coding. (arXiv:2211.02519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02519">
<div class="article-summary-box-inner">
<span><p>Transformer models have achieved great success across many NLP problems.
However, previous studies in automated ICD coding concluded that these models
fail to outperform some of the earlier solutions such as CNN-based models. In
this paper we challenge this conclusion. We present a simple and scalable
method to process long text with the existing transformer models such as BERT.
We show that this method significantly improves the previous results reported
for transformer models in ICD coding, and is able to outperform one of the
prominent CNN-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02533">
<div class="article-summary-box-inner">
<span><p>The substitute-based recommendation is widely used in E-commerce to provide
better alternatives to customers. However, existing research typically uses the
customer behavior signals like co-view and view-but-purchase-another to capture
the substitute relationship. Despite its intuitive soundness, we find that such
an approach might ignore the functionality and characteristics of products. In
this paper, we adapt substitute recommendation into language matching problem
by taking product title description as model input to consider product
functionality. We design a new transformation method to de-noise the signals
derived from production data. In addition, we consider multilingual support
from the engineering point of view. Our proposed end-to-end transformer-based
model achieves both successes from offline and online experiments. The proposed
model has been deployed in a large-scale E-commerce website for 11 marketplaces
in 6 languages. Our proposed model is demonstrated to increase revenue by 19%
based on an online A/B experiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biased Self-supervised learning for ASR. (arXiv:2211.02536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02536">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning via masked prediction pre-training (MPPT) has shown
impressive performance on a range of speech-processing tasks. This paper
proposes a method to bias self-supervised learning towards a specific task. The
core idea is to slightly finetune the model that is used to obtain the target
sequence. This leads to better performance and a substantial increase in
training speed. Furthermore, this paper proposes a variant of MPPT that allows
low-footprint streaming models to be trained effectively by computing the MPPT
loss on masked and unmasked frames. These approaches are evaluated for
automatic speech recognition on the Librispeech corpus, where 100 hours of data
served as the labelled data and 860 hours as the unlabelled data. The biased
training outperforms the unbiased training by 15.5% after 250k updates and
23.8% after 100k updates on test-other. For the streaming models, the
pre-training approach yields a reduction in word error rate of 44.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Chinese classical poetry based on pre-trained model. (arXiv:2211.02541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02541">
<div class="article-summary-box-inner">
<span><p>In order to test whether artificial intelligence can create qualified
classical poetry like humans, the author proposes a study of Chinese classical
poetry generation based on a pre-trained model. This paper mainly tries to use
BART and other pre training models, proposes FS2TEXT and RR2TEXT to generate
metrical poetry text and even specific style poetry text, and solves the
problem that the user's writing intention gradually reduces the relevance of
the generated poetry text.
</p>
<p>In order to test the model's results, the authors selected ancient poets, by
combining it with BART's poetic model work, developed a set of AI poetry Turing
problems, it was reviewed by a group of poets and poetry writing researchers.
There were more than 600 participants, and the final results showed that,
high-level poetry lovers can't distinguish between AI activity and human
activity, this indicates that the author's working methods are not
significantly different from human activities. The model of poetry generation
studied by the author generalizes works that cannot be distinguished from those
of advanced scholars.
</p>
<p>The number of modern Chinese poets has reached 5 million. However, many
modern Chinese poets lack language ability and skills as a result of their
childhood learning. However, many modern poets have no creative inspiration,
and the author's model can help them. They can look at this model when they
choose words and phrases and they can write works based on the poems they
already have, and they can write their own poems. The importance of poetry lies
in the author's thoughts and reflections. It doesn't matter how good AI poetry
is. The only thing that matters is for people to see and inspire them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Speech Enhancement through Synthesis. (arXiv:2211.02542v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02542">
<div class="article-summary-box-inner">
<span><p>Modern speech enhancement (SE) networks typically implement noise suppression
through time-frequency masking, latent representation masking, or
discriminative signal prediction. In contrast, some recent works explore SE via
generative speech synthesis, where the system's output is synthesized by a
neural vocoder after an inherently lossy feature-denoising step. In this paper,
we propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy
representations and learns to directly synthesize clean speech. We leverage
rich representations from self-supervised learning (SSL) speech models to
discover relevant features. We conduct a candidate search across 15 potential
SSL front-ends and subsequently train our vocoder adversarially with the best
SSL configuration. Additionally, we demonstrate a causal version capable of
running on streaming audio with 10ms latency and minimal performance
degradation. Finally, we conduct both objective evaluations and subjective
listening studies to show our system improves objective metrics and outperforms
an existing state-of-the-art SE model subjectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of SVM against Pre-trained Language Models (PLMs) for Text Classification Tasks. (arXiv:2211.02563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02563">
<div class="article-summary-box-inner">
<span><p>The emergence of pre-trained language models (PLMs) has shown great success
in many Natural Language Processing (NLP) tasks including text classification.
Due to the minimal to no feature engineering required when using these models,
PLMs are becoming the de facto choice for any NLP task. However, for
domain-specific corpora (e.g., financial, legal, and industrial), fine-tuning a
pre-trained model for a specific task has shown to provide a performance
improvement. In this paper, we compare the performance of four different PLMs
on three public domain-free datasets and a real-world dataset containing
domain-specific words, against a simple SVM linear classifier with TFIDF
vectorized text. The experimental results on the four datasets show that using
PLMs, even fine-tuned, do not provide significant gain over the linear SVM
classifier. Hence, we recommend that for text classification tasks, traditional
SVM along with careful feature engineering can pro-vide a cheaper and superior
performance than PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 'Problem' of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation. (arXiv:2211.02570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02570">
<div class="article-summary-box-inner">
<span><p>Human variation in labeling is often considered noise. Annotation projects
for machine learning (ML) aim at minimizing human label variation, with the
assumption to maximize data quality and in turn optimize and maximize machine
learning metrics. However, this conventional practice assumes that there exists
a ground truth, and neglects that there exists genuine human variation in
labeling due to disagreement, subjectivity in annotation or multiple plausible
answers. In this position paper, we argue that this big open problem of human
label variation persists and critically needs more attention to move our field
forward. This is because human label variation impacts all stages of the ML
pipeline: data, modeling and evaluation. However, few works consider all of
these dimensions jointly; and existing research is fragmented. We reconcile
different previously proposed notions of human label variation, provide a
repository of publicly-available datasets with un-aggregated labels, depict
approaches proposed so far, identify gaps and suggest ways forward. As datasets
are becoming increasingly available, we hope that this synthesized view on the
'problem' will lead to an open discussion on possible strategies to devise
fundamentally new directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and Improving Factuality in Multimodal Abstractive Summarization. (arXiv:2211.02580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02580">
<div class="article-summary-box-inner">
<span><p>Current metrics for evaluating factuality for abstractive document
summarization have achieved high correlations with human judgment, but they do
not account for the vision modality and thus are not adequate for
vision-and-language summarization. We propose CLIPBERTScore, a simple weighted
combination of CLIPScore and BERTScore to leverage the robustness and strong
factuality detection performance between image-summary and document-summary,
respectively. Next, due to the lack of meta-evaluation benchmarks to evaluate
the quality of multimodal factuality metrics, we collect human judgments of
factuality with respect to documents and images. We show that this simple
combination of two metrics in the zero-shot setting achieves higher
correlations than existing factuality metrics for document summarization,
outperforms an existing multimodal summarization metric, and performs
competitively with strong multimodal factuality metrics specifically fine-tuned
for the task. Our thorough analysis demonstrates the robustness and high
correlation of CLIPBERTScore and its components on four factuality
metric-evaluation benchmarks. Finally, we demonstrate two practical downstream
applications of our CLIPBERTScore metric: for selecting important images to
focus on during training, and as a reward for reinforcement learning to improve
factuality of multimodal summary generation w.r.t automatic and human
evaluation. Our data and code are publicly available at
https://github.com/meetdavidwan/faithful-multimodal-summ
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer Architecture for Online Gesture Recognition of Mathematical Expressions. (arXiv:2211.02643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02643">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture is shown to provide a powerful framework as an
end-to-end model for building expression trees from online handwritten gestures
corresponding to glyph strokes. In particular, the attention mechanism was
successfully used to encode, learn and enforce the underlying syntax of
expressions creating latent representations that are correctly decoded to the
exact mathematical expression tree, providing robustness to ablated inputs and
unseen glyphs. For the first time, the encoder is fed with spatio-temporal data
tokens potentially forming an infinitely large vocabulary, which finds
applications beyond that of online gesture recognition. A new supervised
dataset of online handwriting gestures is provided for training models on
generic handwriting recognition tasks and a new metric is proposed for the
evaluation of the syntactic correctness of the output expression trees. A small
Transformer model suitable for edge inference was successfully trained to an
average normalised Levenshtein accuracy of 94%, resulting in valid postfix RPN
tree representation for 94% of predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Anti-Vaccine Users on Twitter. (arXiv:2110.11333v3 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11333">
<div class="article-summary-box-inner">
<span><p>Vaccine hesitancy, which has recently been driven by online narratives,
significantly degrades the efficacy of vaccination strategies, such as those
for COVID-19. Despite broad agreement in the medical community about the safety
and efficacy of available vaccines, a large number of social media users
continue to be inundated with false information about vaccines and are
indecisive or unwilling to be vaccinated. The goal of this study is to better
understand anti-vaccine sentiment by developing a system capable of
automatically identifying the users responsible for spreading anti-vaccine
narratives. We introduce a publicly available Python package capable of
analyzing Twitter profiles to assess how likely that profile is to share
anti-vaccine sentiment in the future. The software package is built using text
embedding methods, neural networks, and automated dataset generation and is
trained on several million tweets. We find this model can accurately detect
anti-vaccine users up to a year before they tweet anti-vaccine hashtags or
keywords. We also show examples of how text analysis helps us understand
anti-vaccine discussions by detecting moral and emotional differences between
anti-vaccine spreaders on Twitter and regular users. Our results will help
researchers and policy-makers understand how users become anti-vaccine and what
they discuss on Twitter. Policy-makers can utilize this information for better
targeted campaigns that debunk harmful anti-vaccination myths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning. (arXiv:2111.00107v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00107">
<div class="article-summary-box-inner">
<span><p>In this paper we present a natural language programming framework to consider
how the fairness of acts can be measured. For the purposes of the paper, a fair
act is defined as one that one would be accepting of if it were done to
oneself. The approach is based on an implementation of the golden rule (GR) in
the digital domain. Despite the GRs prevalence as an axiom throughout history,
no transfer of this moral philosophy into computational systems exists. In this
paper we consider how to algorithmically operationalise this rule so that it
may be used to measure sentences such as: the boy harmed the girl, and
categorise them as fair or unfair. A review and reply to criticisms of the GR
is made. A suggestion of how the technology may be implemented to avoid unfair
biases in word embeddings is made - given that individuals would typically not
wish to be on the receiving end of an unfair act, such as racism, irrespective
of whether the corpus being used deems such discrimination as praiseworthy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics. (arXiv:2112.08321v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08321">
<div class="article-summary-box-inner">
<span><p>Recent works that revealed the vulnerability of dialogue state tracking (DST)
models to distributional shifts have made holistic comparisons on robustness
and qualitative analyses increasingly important for understanding their
relative performance. We present our findings from standardized and
comprehensive DST diagnoses, which have previously been sparse and
uncoordinated, using our toolkit, CheckDST, a collection of robustness tests
and failure mode analytics. We discover that different classes of DST models
have clear strengths and weaknesses, where generation models are more promising
for handling language variety while span-based classification models are more
robust to unseen entities. Prompted by this discovery, we also compare
checkpoints from the same model and find that the standard practice of
selecting checkpoints using validation loss/accuracy is prone to overfitting
and each model class has distinct patterns of failure. Lastly, we demonstrate
how our diagnoses motivate a pre-finetuning procedure with non-dialogue data
that offers comprehensive improvements to generation models by alleviating the
impact of distributional shifts through transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition (ASR) system would not help since
they are pre-trained on voices that differ from children's in terms of
frequency and amplitude. Because most of these are pre-trained with data in a
specific range of amplitude, their objectives do not make them ready for voices
in different amplitudes. To overcome this issue, we added a new objective to
the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch
(RFP). In addition, we used our newly introduced dataset to fine-tune our model
for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using
masking in concatenation with RFP outperforms the masking objective of Wav2Vec
2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER
of 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our
novel methodology produces positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v6 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13366">
<div class="article-summary-box-inner">
<span><p>For a long time, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language can describe almost anything and language grounding
is a powerful medium to represent various problems or tasks, we present a
flexible and unified text-to-text paradigm called "Pretrain, Personalized
Prompt, and Predict Paradigm" (P5) for recommendation, which unifies various
recommendation tasks in a shared framework. In P5, all data such as user-item
interactions, user descriptions, item metadata, and user reviews are converted
to a common format -- natural language sequences. The rich information from
natural language assists P5 to capture deeper semantics for personalization and
recommendation. Specifically, P5 learns different tasks with the same language
modeling objective during pretraining. Thus, it serves as the foundation model
for various downstream recommendation tasks, allows easy integration with other
modalities, and enables instruction-based recommendation based on prompts. P5
advances recommender systems from shallow model to deep model to big model, and
will revolutionize the technical form of recommender systems towards universal
recommendation engine. With adaptive personalized prompt for different users,
P5 is able to make predictions in a zero-shot or few-shot manner and largely
reduces the necessity for extensive fine-tuning. On several recommendation
benchmarks, we conduct experiments to show the effectiveness of P5. We release
the source code at https://github.com/jeykigung/P5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation between Molecules and Natural Language. (arXiv:2204.11817v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11817">
<div class="article-summary-box-inner">
<span><p>We present $\textbf{MolT5}$ $-$ a self-supervised learning framework for
pretraining models on a vast amount of unlabeled natural language text and
molecule strings. $\textbf{MolT5}$ allows for new, useful, and challenging
analogs of traditional vision-language tasks, such as molecule captioning and
text-based de novo molecule generation (altogether: translation between
molecules and language), which we explore for the first time. Since
$\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the
chemistry domain shortcoming of data scarcity. Furthermore, we consider several
metrics, including a new cross-modal embedding-based metric, to evaluate the
tasks of molecule captioning and text-based molecule generation. Our results
show that $\textbf{MolT5}$-based models are able to generate outputs, both
molecules and captions, which in many cases are high quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyglot Prompt: Multilingual Multitask PrompTraining. (arXiv:2204.14264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14264">
<div class="article-summary-box-inner">
<span><p>This paper aims for a potential architectural improvement for multilingual
learning and asks: Can different tasks from different languages be modeled in a
monolithic framework, i.e. without any task/language-specific module? The
benefit of achieving this could open new doors for future multilingual
research, including allowing systems trained on low resources to be further
assisted by other languages as well as other tasks. We approach this goal by
developing a learning framework named Polyglot Prompting to exploit prompting
methods for learning a unified semantic space for different languages and tasks
with multilingual prompt engineering. We performed a comprehensive evaluation
of 6 tasks, namely topic classification, sentiment classification, named entity
recognition, question answering, natural language inference, and summarization,
covering 24 datasets and 49 languages. The experimental results demonstrated
the efficacy of multilingual multitask prompt-based learning and led to
inspiring observations. We also present an interpretable multilingual
evaluation methodology and show how the proposed framework, multilingual
multitask prompt training, works. We release all datasets prompted in the best
setting and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11211">
<div class="article-summary-box-inner">
<span><p>End-to-End Speech Translation (E2E-ST) has received increasing attention due
to the potential of its less error propagation, lower latency, and fewer
parameters. However, the effectiveness of neural-based approaches to this task
is severely limited by the available training corpus, especially for domain
adaptation where in-domain triplet training data is scarce or nonexistent. In
this paper, we propose a novel non-parametric method that leverages
domain-specific text translation corpus to achieve domain adaptation for the
E2E-ST system. To this end, we first incorporate an additional encoder into the
pre-trained E2E-ST model to realize text translation modelling, and then unify
the decoder's output representation for text and speech translation tasks by
reducing the correspondent representation mismatch in available triplet
training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier
is introduced to produce the final translation distribution using the external
datastore built by the domain-specific text translation corpus, while the
universal output representation is adopted to perform a similarity search.
Experiments on the Europarl-ST benchmark demonstrate that when in-domain text
translation data is involved only, our proposed approach significantly improves
baseline by 12.82 BLEU on average in all translation directions, even
outperforming the strong in-domain fine-tuning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning. (arXiv:2205.12410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12410">
<div class="article-summary-box-inner">
<span><p>Standard fine-tuning of large pre-trained language models (PLMs) for
downstream tasks requires updating hundreds of millions to billions of
parameters, and storing a large copy of the PLM weights for every task
resulting in increased cost for storing, sharing and serving the models. To
address this, parameter-efficient fine-tuning (PEFT) techniques were introduced
where small trainable components are injected in the PLM and updated during
fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of
adaptation modules -- given the underlying PEFT method of choice -- introduced
in each Transformer layer while keeping most of the PLM weights frozen. For
instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture
of low rank decomposition matrices like LoRA to improve downstream task
performance over the corresponding PEFT methods for fully supervised and
few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the
same computational cost and the number of tunable parameters as the underlying
PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix
outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for
both NLU and NLG tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts. (arXiv:2205.12496v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12496">
<div class="article-summary-box-inner">
<span><p>Question-answering datasets require a broad set of reasoning skills. We show
how to use question decompositions to teach language models these broad
reasoning skills in a robust fashion. Specifically, we use widely available
QDMR representations to programmatically create hard-to-cheat synthetic
contexts for real questions in six multi-step reasoning datasets. These
contexts are carefully designed to avoid reasoning shortcuts prevalent in real
contexts that prevent models from learning the right skills. This results in a
pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with
associated formal programs) covering about 900 reasoning patterns. We show that
pretraining standard language models (LMs) on TeaBReaC before fine-tuning them
on target datasets improves their performance by up to 13 F1 points across 4
multi-step QA datasets, with up to 21 point gain on more complex questions. The
resulting models also demonstrate higher robustness, with a 5-8 F1 point
improvement on two contrast sets. Furthermore, TeaBReaC pretraining
substantially improves model performance and robustness even when starting with
numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work
thus shows how to effectively use decomposition-guided contexts to robustly
teach multi-step reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memorization in NLP Fine-tuning Methods. (arXiv:2205.12506v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12506">
<div class="article-summary-box-inner">
<span><p>Large language models are shown to present privacy risks through memorization
of training data, and several recent works have studied such risks for the
pre-training phase. Little attention, however, has been given to the
fine-tuning phase and it is not well understood how different fine-tuning
methods (such as fine-tuning the full model, the model head, and adapter)
compare in terms of memorization risk. This presents increasing concern as the
"pre-train and fine-tune" paradigm proliferates. In this paper, we empirically
study memorization of fine-tuning methods using membership inference and
extraction attacks, and show that their susceptibility to attacks is very
different. We observe that fine-tuning the head of the model has the highest
susceptibility to attacks, whereas fine-tuning smaller adapters appears to be
less vulnerable to known extraction attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook. (arXiv:2206.07304v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07304">
<div class="article-summary-box-inner">
<span><p>Knowledge management systems (KMS) are in high demand for industrial
researchers, chemical or research enterprises, or evidence-based decision
making. However, existing systems have limitations in categorizing and
organizing paper insights or relationships. Traditional databases are usually
disjoint with logging systems, which limit its utility in generating concise,
collated overviews. In this work, we briefly survey existing approaches of this
problem space and propose a unified framework that utilizes relational
databases to log hierarchical information to facilitate the research and
writing process, or generate useful knowledge from references or insights from
connected concepts. Our framework of bidirectional knowledge management system
(BKMS) enables novel functionalities encompassing improved hierarchical
note-taking, AI-assisted brainstorming, and multi-directional relationships.
Potential applications include managing inventories and changes for manufacture
or research enterprises, or generating analytic reports with evidence-based
decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts. (arXiv:2210.03422v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03422">
<div class="article-summary-box-inner">
<span><p>We present SpaceQA, to the best of our knowledge the first open-domain QA
system in Space mission design. SpaceQA is part of an initiative by the
European Space Agency (ESA) to facilitate the access, sharing and reuse of
information about Space mission design within the agency and with the public.
We adopt a state-of-the-art architecture consisting of a dense retriever and a
neural reader and opt for an approach based on transfer learning rather than
fine-tuning due to the lack of domain-specific annotated data. Our evaluation
on a test set produced by ESA is largely consistent with the results originally
reported by the evaluated retrievers and confirms the need of fine tuning for
reading comprehension. As of writing this paper, ESA is piloting SpaceQA
internally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering. (arXiv:2210.03427v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03427">
<div class="article-summary-box-inner">
<span><p>Quality management and assurance is key for space agencies to guarantee the
success of space missions, which are high-risk and extremely costly. In this
paper, we present a system to generate quizzes, a common resource to evaluate
the effectiveness of training sessions, from documents about quality assurance
procedures in the Space domain. Our system leverages state of the art
auto-regressive models like T5 and BART to generate questions, and a RoBERTa
model to extract answers for such questions, thus verifying their suitability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalization of CTC Speech Recognition Models using Contextual Adapters and Adaptive Boosting. (arXiv:2210.09510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09510">
<div class="article-summary-box-inner">
<span><p>End-to-end speech recognition models trained using joint Connectionist
Temporal Classification (CTC)-Attention loss have gained popularity recently.
In these models, a non-autoregressive CTC decoder is often used at inference
time due to its speed and simplicity. However, such models are hard to
personalize because of their conditional independence assumption that prevents
output tokens from previous time steps to influence future predictions. To
tackle this, we propose a novel two-way approach that first biases the encoder
with attention over a predefined list of rare long-tail and out-of-vocabulary
(OOV) words and then uses dynamic boosting and phone alignment network during
decoding to further bias the subword predictions. We evaluate our approach on
open-source VoxPopuli and in-house medical datasets to showcase a 60%
improvement in F1 score on domain-specific rare words over a strong CTC
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Legal Domain Adaptation. (arXiv:2210.13712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13712">
<div class="article-summary-box-inner">
<span><p>Seeking legal advice is often expensive. Recent advancements in machine
learning for solving complex problems can be leveraged to help make legal
services more accessible to the public. However, real-life applications
encounter significant challenges. State-of-the-art language models are growing
increasingly large, making parameter-efficient learning increasingly important.
Unfortunately, parameter-efficient methods perform poorly with small amounts of
data, which are common in the legal domain (where data labelling costs are
high). To address these challenges, we propose parameter-efficient legal domain
adaptation, which uses vast unsupervised legal data from public legal forums to
perform legal pre-training. This method exceeds or matches the fewshot
performance of existing models such as LEGAL-BERT on various legal tasks while
tuning only approximately 0.1% of model parameters. Additionally, we show that
our method can achieve calibration comparable to existing methods across
several tasks. To the best of our knowledge, this work is among the first to
explore parameter-efficient methods of tuning language models in the legal
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyHope: Two-Level Hope Speech Detection from Tweets. (arXiv:2210.14136v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14136">
<div class="article-summary-box-inner">
<span><p>Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into "Hope" and "Not Hope",
then into three fine-grained hope categories: "Generalized Hope", "Realistic
Hope", and "Unrealistic Hope" (along with "Not Hope"). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGReE: A system for generating Automated Grammar Reading Exercises. (arXiv:2210.16302v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16302">
<div class="article-summary-box-inner">
<span><p>We describe the AGReE system, which takes user-submitted passages as input
and automatically generates grammar practice exercises that can be completed
while reading. Multiple-choice practice items are generated for a variety of
different grammar constructs: punctuation, articles, conjunctions, pronouns,
prepositions, verbs, and nouns. We also conducted a large-scale human
evaluation with around 4,500 multiple-choice practice items. We notice for 95%
of items, a majority of raters out of five were able to identify the correct
answer and for 85% of cases, raters agree that there is only one correct answer
among the choices. Finally, the error analysis shows that raters made the most
mistakes for punctuation and conjunctions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning. (arXiv:2210.16952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16952">
<div class="article-summary-box-inner">
<span><p>Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00768">
<div class="article-summary-box-inner">
<span><p>Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., "there is a mug in some grass" vs.
"there is some grass in a mug"). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Intrinsic Compositionality in Transformers with Tree Projections. (arXiv:2211.01288v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01288">
<div class="article-summary-box-inner">
<span><p>When trained on language data, do transformers learn some arbitrary
computation that utilizes the full capacity of the architecture or do they
learn a simpler, tree-like computation, hypothesized to underlie compositional
meaning systems like human languages? There is an apparent tension between
compositional accounts of human language understanding, which are based on a
restricted bottom-up computational process, and the enormous success of neural
models like transformers, which can route information arbitrarily between
different parts of their input. One possibility is that these models, while
extremely flexible in principle, in practice learn to interpret language
hierarchically, ultimately building sentence representations close to those
predictable by a bottom-up, tree-structured model. To evaluate this
possibility, we describe an unsupervised and parameter-free method to
\emph{functionally project} the behavior of any transformer into the space of
tree-structured networks. Given an input sentence, we produce a binary tree
that approximates the transformer's representation-building process and a score
that captures how "tree-like" the transformer's behavior is on the input. While
calculation of this score does not require training any additional models, it
provably upper-bounds the fit between a transformer and any tree-structured
approximation. Using this method, we show that transformers for three different
tasks become more tree-like over the course of training, in some cases
unsupervisedly recovering the same trees as supervised parsers. These trees, in
turn, are predictive of model behavior, with more tree-like models generalizing
better on tests of compositional generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions. (arXiv:2211.01542v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01542">
<div class="article-summary-box-inner">
<span><p>This paper considers continual learning of large-scale pretrained neural
machine translation model without accessing the previous training data or
introducing model separation. We argue that the widely used
regularization-based methods, which perform multi-objective learning with an
auxiliary loss, suffer from the misestimate problem and cannot always achieve a
good balance between the previous and new tasks. To solve the problem, we
propose a two-stage training method based on the local features of the real
loss. We first search low forgetting risk regions, where the model can retain
the performance on the previous task as the parameters are updated, to avoid
the catastrophic forgetting problem. Then we can continually train the model
within this region only with the new training data to fit the new task.
Specifically, we propose two methods to search the low forgetting risk regions,
which are based on the curvature of loss and the impacts of the parameters on
the model output, respectively. We conduct experiments on domain adaptation and
more challenging language adaptation tasks, and the experimental results show
that our method can achieve significant improvements compared with several
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-07 23:18:08.064448331 UTC">2022-11-07 23:18:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>