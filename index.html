<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-08T01:30:00Z">05-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation. (arXiv:2305.03088v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03088">
<div class="article-summary-box-inner">
<span><p>Conversational Question Generation (CQG) is a critical task for machines to
assist humans in fulfilling their information needs through conversations. The
task is generally cast into two different settings: answer-aware and
answer-unaware. While the former facilitates the models by exposing the
expected answer, the latter is more realistic and receiving growing attentions
recently. What-to-ask and how-to-ask are the two main challenges in the
answer-unaware setting. To address the first challenge, existing methods mainly
select sequential sentences in context as the rationales. We argue that the
conversation generated using such naive heuristics may not be natural enough as
in reality, the interlocutors often talk about the relevant contents that are
not necessarily sequential in context. Additionally, previous methods decide
the type of question to be generated (boolean/span-based) implicitly. Modeling
the question type explicitly is crucial as the answer, which hints the models
to generate a boolean or span-based question, is unavailable. To this end, we
present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a
sentence is selected as the rationale from a semantic graph that we construct,
and extract the answer span from it. For the how-to-ask stage, a classifier
determines the target answer type of the question via two explicit control
signals before generating and filtering. In addition, we propose Conv-Distinct,
a novel evaluation metric for CQG, to evaluate the diversity of the generated
conversation from a context. Compared with the existing answer-unaware CQG
models, the proposed SG-CQG achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03092">
<div class="article-summary-box-inner">
<span><p>Well curated, large-scale corpora of social media posts containing broad
public opinion offer an alternative data source to complement traditional
surveys. While surveys are effective at collecting representative samples and
are capable of achieving high accuracy, they can be both expensive to run and
lag public opinion by days or weeks. Both of these drawbacks could be overcome
with a real-time, high volume data stream and fast analysis pipeline. A central
challenge in orchestrating such a data pipeline is devising an effective method
for rapidly selecting the best corpus of relevant documents for analysis.
Querying with keywords alone often includes irrelevant documents that are not
easily disambiguated with bag-of-words natural language processing methods.
Here, we explore methods of corpus curation to filter irrelevant tweets using
pre-trained transformer-based models, fine-tuned for our binary classification
task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.
The low cost and high performance of fine-tuning such a model suggests that our
approach could be of broad benefit as a pre-processing step for social media
datasets with uncertain corpus boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks. (arXiv:2305.03101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03101">
<div class="article-summary-box-inner">
<span><p>Transducer and Attention based Encoder-Decoder (AED) are two widely used
frameworks for speech-to-text tasks. They are designed for different purposes
and each has its own benefits and drawbacks for speech-to-text tasks. In order
to leverage strengths of both modeling methods, we propose a solution by
combining Transducer and Attention based Encoder-Decoder (TAED) for
speech-to-text tasks. The new method leverages AED's strength in non-monotonic
sequence to sequence learning while retaining Transducer's streaming property.
In the proposed framework, Transducer and AED share the same speech encoder.
The predictor in Transducer is replaced by the decoder in the AED model, and
the outputs of the decoder are conditioned on the speech inputs instead of
outputs from an unconditioned language model. The proposed solution ensures
that the model is optimized by covering all possible read/write scenarios and
creates a matched environment for streaming applications. We evaluate the
proposed approach on the \textsc{MuST-C} dataset and the findings demonstrate
that TAED performs significantly better than Transducer for offline automatic
speech recognition (ASR) and speech-to-text translation (ST) tasks. In the
streaming case, TAED outperforms Transducer in the ASR task and one ST
direction while comparable results are achieved in another translation
direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03111">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL parsing, which aims at converting natural language instructions
into executable SQLs, has gained increasing attention in recent years. In
particular, Codex and ChatGPT have shown impressive results in this task.
However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on
database schema with few rows of database contents leaving the gap between
academic study and real-world applications. To mitigate this gap, we present
Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks,
containing 12,751 pairs of text-to-SQL data and 95 databases with a total size
of 33.4 GB, spanning 37 professional domains. Our emphasis on database values
highlights the new challenges of dirty database contents, external knowledge
between NL questions and database contents, and SQL efficiency, particularly in
the context of massive databases. To solve these problems, text-to-SQL models
must feature database value comprehension in addition to semantic parsing. The
experimental results demonstrate the significance of database values in
generating accurate text-to-SQLs for big databases. Furthermore, even the most
effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution
accuracy, which is still far from the human result of 92.96%, proving that
challenges still stand. Besides, we also provide an efficiency analysis to
offer insights into generating text-to-efficient-SQLs that are beneficial to
industries. We believe that BIRD will contribute to advancing real-world
applications of text-to-SQL research. The leaderboard and source code are
available: https://bird-bench.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations. (arXiv:2305.03117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03117">
<div class="article-summary-box-inner">
<span><p>Human-annotated labels and explanations are critical for training explainable
NLP models. However, unlike human-annotated labels whose quality is easier to
calibrate (e.g., with a majority vote), human-crafted free-form explanations
can be quite subjective, as some recent works have discussed. Before blindly
using them as ground truth to train ML models, a vital question needs to be
asked: How do we evaluate a human-annotated explanation's quality? In this
paper, we build on the view that the quality of a human-annotated explanation
can be measured based on its helpfulness (or impairment) to the ML models'
performance for the desired NLP tasks for which the annotations were collected.
In comparison to the commonly used Simulatability score, we define a new metric
that can take into consideration the helpfulness of an explanation for model
performance at both fine-tuning and inference. With the help of a unified
dataset format, we evaluated the proposed metric on five datasets (e.g.,
e-SNLI) against two model architectures (T5 and BART), and the results show
that our proposed metric can objectively evaluate the quality of
human-annotated explanations, while Simulatability falls short.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03123">
<div class="article-summary-box-inner">
<span><p>ChatGPT is another large language model (LLM) inline but due to its
performance and ability to converse effectively, it has gained a huge
popularity amongst research as well as industrial community. Recently, many
studies have been published to show the effectiveness, efficiency, integration,
and sentiments of chatGPT and other LLMs. In contrast, this study focuses on
the important aspects that are mostly overlooked, i.e. sustainability, privacy,
digital divide, and ethics and suggests that not only chatGPT but every
subsequent entry in the category of conversational bots should undergo
Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This
paper discusses in detail about the issues and concerns raised over chatGPT in
line with aforementioned characteristics. We support our hypothesis by some
preliminary data collection and visualizations along with hypothesized facts.
We also suggest mitigations and recommendations for each of the concerns.
Furthermore, we also suggest some policies and recommendations for AI policy
act, if designed by the governments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03130">
<div class="article-summary-box-inner">
<span><p>The retrieval model is an indispensable component for real-world
knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As
separate retrieval skills are annotated for different datasets, recent work
focuses on customized methods, limiting the model transferability and
scalability. In this work, we propose a modular retriever where individual
modules correspond to key skills that can be reused across datasets. Our
approach supports flexible skill configurations based on the target domain to
boost performance. To mitigate task interference, we design a novel
modularization parameterization inspired by sparse Transformer. We demonstrate
that our model can benefit from self-supervised pretraining on Wikipedia and
fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our
approach outperforms recent self-supervised retrievers in zero-shot evaluations
and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA
and OTT-QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03132">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformer-based models have recently shown great performance
when applied to Named Entity Recognition (NER). As the complexity of their
self-attention mechanism prevents them from processing long documents at once,
these models are usually applied in a sequential fashion. Such an approach
unfortunately only incorporates local context and prevents leveraging global
document context in long documents such as novels, which might hinder
performance. In this article, we explore the impact of global document context,
and its relationships with local context. We find that correctly retrieving
global document context has a greater impact on performance than only
leveraging local context, prompting for further research on how to better
retrieve that context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03144">
<div class="article-summary-box-inner">
<span><p>With the advent of e-commerce platforms, reviews are crucial for customers to
assess the credibility of a product. The star ratings do not always match the
review text written by the customer. For example, a three star rating (out of
five) may be incongruous with the review text, which may be more suitable for a
five star review. A clustering approach can be used to relabel the correct star
ratings by grouping the text reviews into individual groups. In this work, we
explore the task of choosing different text embeddings to represent these
reviews and also explore the impact the embedding choice has on the performance
of various classes of clustering algorithms. We use contextual (BERT) and
non-contextual (Word2Vec) text embeddings to represent the text and measure
their impact of three classes on clustering algorithms - partitioning based
(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN
and HDBSCAN), each with various experimental settings. We use the silhouette
score, adjusted rand index score, and cluster purity score metrics to evaluate
the performance of the algorithms and discuss the impact of different
embeddings on the clustering performance. Our results indicate that the type of
embedding chosen drastically affects the performance of the algorithm, the
performance varies greatly across different types of clustering algorithms, no
embedding type is better than the other, and DBSCAN outperforms KMeans and
single linkage agglomerative clustering but also labels more data points as
outliers. We provide a thorough comparison of the performances of different
algorithms and provide numerous ideas to foster further research in the domain
of text clustering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03169">
<div class="article-summary-box-inner">
<span><p>In the era of big data, there is an increasing need for healthcare providers,
communities, and researchers to share data and collaborate to improve health
outcomes, generate valuable insights, and advance research. The Health
Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law
designed to protect sensitive health information by defining regulations for
protected health information (PHI). However, it does not provide efficient
tools for detecting or removing PHI before data sharing. One of the challenges
in this area of research is the heterogeneous nature of PHI fields in data
across different parties. This variability makes rule-based sensitive variable
identification systems that work on one database fail on another. To address
this issue, our paper explores the use of machine learning algorithms to
identify sensitive variables in structured data, thus facilitating the
de-identification process. We made a key observation that the distributions of
metadata of PHI fields and non-PHI fields are very different. Based on this
novel finding, we engineered over 30 features from the metadata of the original
features and used machine learning to build classification models to
automatically identify PHI fields in structured Electronic Health Record (EHR)
data. We trained the model on a variety of large EHR databases from different
data sources and found that our algorithm achieves 99% accuracy when detecting
PHI-related fields for unseen datasets. The implications of our study are
significant and can benefit industries that handle sensitive data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03195">
<div class="article-summary-box-inner">
<span><p>Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation
language model in the GPT series, developed by OpenAI, which promises
significant advancements in the field of natural language processing (NLP). In
this research article, we have discussed the features of GPT-4, its potential
applications, and the challenges that it might face. We have also compared
GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one
trillion), better multilingual capabilities, improved contextual understanding,
and reasoning capabilities than GPT-3. Some of the potential applications of
GPT-4 include chatbots, personal assistants, language translation, text
summarization, and question-answering. However, GPT-4 poses several challenges
and limitations such as computational requirements, data requirements, and
ethical concerns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Employing Hybrid Deep Neural Networks on Dari Speech. (arXiv:2305.03200v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03200">
<div class="article-summary-box-inner">
<span><p>This paper is an extension of our previous conference paper. In recent years,
there has been a growing interest among researchers in developing and improving
speech recognition systems to facilitate and enhance human-computer
interaction. Today, Automatic Speech Recognition (ASR) systems have become
ubiquitous, used in everything from games to translation systems, robots, and
more. However, much research is still needed on speech recognition systems for
low-resource languages. This article focuses on the recognition of individual
words in the Dari language using the Mel-frequency cepstral coefficients
(MFCCs) feature extraction method and three different deep neural network
models: Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and
Multilayer Perceptron (MLP), as well as two hybrid models combining CNN and
RNN. We evaluate these models using an isolated Dari word corpus that we have
created, consisting of 1000 utterances for 20 short Dari terms. Our study
achieved an impressive average accuracy of 98.365%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis. (arXiv:2305.03201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03201">
<div class="article-summary-box-inner">
<span><p>Text classification has become a crucial task in various fields, leading to a
significant amount of research on developing automated text classification
systems for national and international languages. However, there is a growing
need for automated text classification systems that can handle local languages.
This study aims to establish an automated classification system for Pashto
text. To achieve this goal, we constructed a dataset of Pashto documents and
applied various models, including statistical and neural machine learning
models such as DistilBERT-base-multilingual-cased, Multilayer Perceptron,
Support Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\"ive
Bayes, multinomial na\"ive Bayes, random forest, and logistic regression, to
identify the most effective approach. We also evaluated two different feature
extraction methods, bag of words and Term Frequency Inverse Document Frequency.
The study achieved an average testing accuracy rate of 94% using the MLP
classification algorithm and TFIDF feature extraction method in single-label
multiclass classification. Similarly, MLP+TFIDF yielded the best results, with
an F1-measure of 0.81. Furthermore, the use of pre-trained language
representation models, such as DistilBERT, showed promising results for Pashto
text classification; however, the study highlights the importance of developing
a specific tokenizer for a particular language to achieve reasonable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03204">
<div class="article-summary-box-inner">
<span><p>We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03207">
<div class="article-summary-box-inner">
<span><p>Multilingual language models have shown impressive cross-lingual transfer
ability across a diverse set of languages and tasks. To improve the
cross-lingual ability of these models, some strategies include transliteration
and finer-grained segmentation into characters as opposed to subwords. In this
work, we investigate lexical sharing in multilingual machine translation (MT)
from Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist
in translation performance between data sampling and vocabulary size, and we
explore whether transliteration is useful in encouraging cross-script
generalisation. We also verify how the different settings generalise to unseen
languages (Marathi and Bengali). We find that transliteration does not give
pronounced improvements and our analysis suggests that our multilingual MT
models trained on original scripts seem to already be robust to cross-script
differences even for relatively low-resource languages
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03210">
<div class="article-summary-box-inner">
<span><p>Transformer models are revolutionizing machine learning, but their inner
workings remain mysterious. In this work, we present a new visualization
technique designed to help researchers understand the self-attention mechanism
in transformers that allows these models to learn rich, contextual
relationships between elements of a sequence. The main idea behind our method
is to visualize a joint embedding of the query and key vectors used by
transformer models to compute attention. Unlike previous attention
visualization techniques, our approach enables the analysis of global patterns
across multiple input sequences. We create an interactive visualization tool,
AttentionViz, based on these joint query-key embeddings, and use it to study
attention mechanisms in both language and vision transformers. We demonstrate
the utility of our approach in improving model understanding and offering new
insights about query-key interactions through several application scenarios and
expert feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuromodulation Gated Transformer. (arXiv:2305.03232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03232">
<div class="article-summary-box-inner">
<span><p>We introduce a novel architecture, the Neuromodulation Gated Transformer
(NGT), which is a simple implementation of neuromodulation in transformers via
a multiplicative effect. We compare it to baselines and show that it results in
the best average performance on the SuperGLUE benchmark validation sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03236">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is essential for the reliable and safe
deployment of machine learning systems in the real world. Great progress has
been made over the past years. This paper presents the first review of recent
advances in OOD detection with a particular focus on natural language
processing approaches. First, we provide a formal definition of OOD detection
and discuss several related fields. We then categorize recent algorithms into
three classes according to the data they used: (1) OOD data available, (2) OOD
data unavailable + in-distribution (ID) label available, and (3) OOD data
unavailable + ID label unavailable. Third, we introduce datasets, applications,
and metrics. Finally, we summarize existing work and present potential future
research topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03237">
<div class="article-summary-box-inner">
<span><p>Out-of-Domain (OOD) intent detection is vital for practical dialogue systems,
and it usually requires considering multi-turn dialogue contexts. However, most
previous OOD intent detection approaches are limited to single dialogue turns.
In this paper, we introduce a context-aware OOD intent detection (Caro)
framework to model multi-turn contexts in OOD intent detection tasks.
Specifically, we follow the information bottleneck principle to extract robust
representations from multi-turn dialogue contexts. Two different views are
constructed for each input sample and the superfluous information not related
to intent detection is removed using a multi-view information bottleneck loss.
Moreover, we also explore utilizing unlabeled data in Caro. A two-stage
training process is introduced to mine OOD samples from these unlabeled data,
and these OOD samples are used to train the resulting model with a
bootstrapping approach. Comprehensive experiments demonstrate that Caro
establishes state-of-the-art performances on multi-turn OOD detection tasks by
improving the F1-OOD score of over $29\%$ compared to the previous best method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna. (arXiv:2305.03253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03253">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain. (arXiv:2305.03256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03256">
<div class="article-summary-box-inner">
<span><p>Existing data-to-text generation efforts mainly focus on generating a
coherent text from non-linguistic input data, such as tables and
attribute-value pairs, but overlook that different application scenarios may
require texts of different styles. Inspired by this, we define a new task,
namely stylized data-to-text generation, whose aim is to generate coherent text
for the given non-linguistic data according to a specific style. This task is
non-trivial, due to three challenges: the logic of the generated text,
unstructured style reference, and biased training samples. To address these
challenges, we propose a novel stylized data-to-text generation model, named
StyleD2T, comprising three components: logic planning-enhanced data embedding,
mask-based style embedding, and unbiased stylized text generation. In the first
component, we introduce a graph-guided logic planner for attribute organization
to ensure the logic of generated text. In the second component, we devise
feature-level mask-based style embedding to extract the essential style signal
from the given unstructured style reference. In the last one, pseudo triplet
augmentation is utilized to achieve unbiased text generation, and a
multi-condition based confidence assignment function is designed to ensure the
quality of pseudo samples. Extensive experiments on a newly collected dataset
from Taobao have been conducted, and the results show the superiority of our
model over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization. (arXiv:2305.03262v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03262">
<div class="article-summary-box-inner">
<span><p>Training a dialogue policy using deep reinforcement learning requires a lot
of exploration of the environment. The amount of wasted invalid exploration
makes their learning inefficient. In this paper, we find and define an
important reason for the invalid exploration: dead-ends. When a conversation
enters a dead-end state, regardless of the actions taken afterward, it will
continue in a dead-end trajectory until the agent reaches a termination state
or maximum turn. We propose a dead-end resurrection (DDR) algorithm that
detects the initial dead-end state in a timely and efficient manner and
provides a rescue action to guide and correct the exploration direction. To
prevent dialogue policies from repeatedly making the same mistake, DDR also
performs dialogue data augmentation by adding relevant experiences containing
dead-end states. We first validate the dead-end detection reliability and then
demonstrate the effectiveness and generality of the method by reporting
experimental results on several dialogue datasets from different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. (arXiv:2305.03268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03268">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) have become the norm in NLP, demonstrating
good performance in generation and reasoning tasks, one of its most fatal
disadvantages is the lack of factual correctness. Generating unfactual texts
not only leads to lower performances but also degrades the trust and validity
of their applications. Chain-of-Thought (CoT) prompting improves trust and
model performance on complex reasoning tasks by generating interpretable
reasoning chains, but still suffers from factuality concerns in
knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit
framework for CoT prompting, which seeks to increase prediction factuality by
post-editing reasoning chains according to external knowledge. Building on top
of GPT-3, our framework lead to accuracy improvements in multiple open-domain
question-answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03276">
<div class="article-summary-box-inner">
<span><p>Expository documents are vital resources for conveying complex information to
readers. Despite their usefulness, writing expository documents by hand is a
time-consuming and labor-intensive process that requires knowledge of the
domain of interest, careful content planning, and the ability to synthesize
information from multiple sources. To ease these burdens, we introduce the task
of expository text generation, which seeks to automatically generate an
accurate and informative expository document from a knowledge source. We solve
our task by developing IRP, an iterative framework that overcomes the
limitations of language models and separately tackles the steps of content
planning, fact selection, and rephrasing. Through experiments on three diverse
datasets, we demonstrate that IRP produces high-quality expository documents
that accurately inform readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03287">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally
requires large numbers of annotated data to achieve state-of-the-art
performance on a range of NLP tasks in the scientific domain. However,
obtaining the fine-tune data for scientific NLP task is still challenging and
expensive. Inspired by recent advancement in prompt learning, in this paper, we
propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to
alleviate the dependence on annotated data and improve the performance of
multi-granularity academic function recognition tasks with a small number of
labeled examples. Specifically, the proposed method provides multi-perspective
representations by combining manual prompt templates with automatically learned
continuous prompt templates to help the given academic function recognition
task take full advantage of knowledge in PLMs. Based on these prompt templates
and the fine-tuned PLM, a large number of pseudo labels are assigned to the
unlabeled examples. Finally, we fine-tune the PLM using the pseudo training
set. We evaluate our method on three academic function recognition tasks of
different granularity including the citation function, the abstract sentence
function, and the keyword function, with datasets from computer science domain
and biomedical domain. Extensive experiments demonstrate the effectiveness of
our method and statistically significant improvements against strong baselines.
In particular, it achieves an average increase of 5% in Macro-F1 score compared
with fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised
method under low-resource settings. In addition, MPT is a general method that
can be easily applied to other low-resource scientific classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition. (arXiv:2305.03296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03296">
<div class="article-summary-box-inner">
<span><p>Emotion Support Conversation (ESC) is an emerging and challenging task with
the goal of reducing the emotional distress of people. Previous attempts fail
to maintain smooth transitions between utterances in ESC because they ignore to
grasp the fine-grained transition information at each dialogue turn. To solve
this problem, we propose to take into account turn-level state
\textbf{Trans}itions of \textbf{ESC} (\textbf{TransESC}) from three
perspectives, including semantics transition, strategy transition and emotion
transition, to drive the conversation in a smooth and natural way.
Specifically, we construct the state transition graph with a two-step way,
named transit-then-interact, to grasp such three types of turn-level transition
information. Finally, they are injected into the transition-aware decoder to
generate more engaging responses. Both automatic and human evaluations on the
benchmark dataset demonstrate the superiority of TransESC to generate more
smooth and effective supportive responses. Our source code is available at
\url{https://github.com/circle-hit/TransESC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Information Extraction via Chunks. (arXiv:2305.03299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03299">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OIE) aims to extract relational tuples from
open-domain sentences. Existing OIE systems split a sentence into tokens and
recognize token spans as tuple relations and arguments. We instead propose
Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations
and arguments. We argue that SaC has better quantitative and qualitative
properties for OIE than sentence as token sequence, and evaluate four choices
of chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from
SpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based
model for sentence chunking, and propose Chunk-OIE for tuple extraction on top
of SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets,
showing that SaC benefits OIE task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa. (arXiv:2305.03300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03300">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition(NER) is a task of recognizing entities at a token
level in a sentence. This paper focuses on solving NER tasks in a multilingual
setting for complex named entities. Our team, LLM-RM participated in the
recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual
Complex Named Entity Recognition. We approach the problem by leveraging
cross-lingual representation provided by fine-tuning XLM-Roberta base model on
datasets of all of the 12 languages provided -- Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and
Ukrainian
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell Checking. (arXiv:2305.03314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03314">
<div class="article-summary-box-inner">
<span><p>Recently, Chinese Spell Checking(CSC), a task to detect erroneous characters
in a sentence and correct them, has attracted extensive interest because of its
wide applications in various NLP tasks. Most of the existing methods have
utilized BERT to extract semantic information for CSC task. However, these
methods directly take sentences with only a few errors as inputs, where the
correct characters may leak answers to the model and dampen its ability to
capture distant context; while the erroneous characters may disturb the
semantic encoding process and result in poor representations. Based on such
observations, this paper proposes an n-gram masking layer that masks current
and/or surrounding tokens to avoid label leakage and error disturbance.
Moreover, considering that the mask strategy may ignore multi-modal information
indicated by errors, a novel dot-product gating mechanism is proposed to
integrate the phonological and morphological information with semantic
representation. Extensive experiments on SIGHAN datasets have demonstrated that
the pluggable n-gram masking mechanism can improve the performance of prevalent
CSC models and the proposed methods in this paper outperform multiple powerful
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03319">
<div class="article-summary-box-inner">
<span><p>Encoding long sequences in Natural Language Processing (NLP) is a challenging
problem. Though recent pretraining language models achieve satisfying
performances in many NLP tasks, they are still restricted by a pre-defined
maximum length, making them challenging to be extended to longer sequences. So
some recent works utilize hierarchies to model long sequences. However, most of
them apply sequential models for upper hierarchies, suffering from long
dependency issues. In this paper, we alleviate these issues through a
graph-based method. We first chunk the sequence with a fixed length to model
the sentence-level information. We then leverage graphs to model intra- and
cross-sentence correlations with a new attention mechanism. Additionally, due
to limited standard benchmarks for long document classification (LDC), we
propose a new challenging benchmark, totaling six datasets with up to 53k
samples and 4034 average tokens' length. Evaluation shows our model surpasses
competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence
dataset. Our method is shown to outperform hierarchical sequential models with
better performance and scalability, especially for longer sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models. (arXiv:2305.03336v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03336">
<div class="article-summary-box-inner">
<span><p>Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03353">
<div class="article-summary-box-inner">
<span><p>Theory of Mind (ToM) is a critical component of intelligence, yet accurately
measuring it continues to be a subject of debate. Prior research has attempted
to apply human ToM assessments to natural language processing models using
either human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
In this study, we utilize dynamic epistemic logic, which has established
overlaps with ToM, to generate more intricate problems. We also introduce novel
verbalization techniques to express these problems using natural language. Our
findings indicate that certain language model scaling (from 70M to 6B and 350M
to 174B) does not consistently yield results better than random chance. While
GPT-4 demonstrates improved epistemic reasoning capabilities, there is still
room for enhancement. Our code and datasets are publicly available
https://github.com/antoinelrnld/modlog
https://huggingface.co/datasets/sileod/mindgames
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03356">
<div class="article-summary-box-inner">
<span><p>Parsing questions into executable logical forms has showed impressive results
for knowledge-base question answering (KBQA). However, complex KBQA is a more
challenging task that requires to perform complex multi-step reasoning.
Recently, a new semantic parser called KoPL has been proposed to explicitly
model the reasoning processes, which achieved the state-of-the-art on complex
KBQA. In this paper, we further explore how to unlock the reasoning ability of
semantic parsers by a simple proposed parse-execute-refine paradigm. We refine
and improve the KoPL parser by demonstrating the executed intermediate
reasoning steps to the KBQA model. We show that such simple strategy can
significantly improve the ability of complex reasoning. Specifically, we
propose three components: a parsing stage, an execution stage and a refinement
stage, to enhance the ability of complex reasoning. The parser uses the KoPL to
generate the transparent logical forms. Then, the execution stage aligns and
executes the logical forms over knowledge base to obtain intermediate reasoning
processes. Finally, the intermediate step-by-step reasoning processes are
demonstrated to the KBQA model in the refinement stage. With the explicit
reasoning processes, it is much easier to answer the complex questions.
Experiments on benchmark dataset shows that the proposed PER-KBQA performs
significantly better than the stage-of-the-art baselines on the complex KBQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation. (arXiv:2305.03369v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03369">
<div class="article-summary-box-inner">
<span><p>The MuSe 2023 is a set of shared tasks addressing three different
contemporary multimodal affect and sentiment analysis problems: In the Mimicked
Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous
emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset
comprising of user-generated videos. For the Cross-Cultural Humour Detection
Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football
Coach Humour (Passau-SFCH) dataset is provided. Participants predict the
presence of spontaneous humour in a cross-cultural setting. The Personalisation
Sub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress
Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed
situation. Here, arousal and valence signals are to be predicted, whereas parts
of the test labels are made available in order to facilitate personalisation.
MuSe 2023 seeks to bring together a broad audience from different research
communities such as audio-visual emotion recognition, natural language
processing, signal processing, and health informatics. In this baseline paper,
we introduce the datasets, sub-challenges, and provided feature sets. As a
competitive baseline system, a Gated Recurrent Unit (GRU)-Recurrent Neural
Network (RNN) is employed. On the respective sub-challenges' test datasets, it
achieves a mean (across three continuous intensity targets) Pearson's
Correlation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC)
value of .8310 for MuSe-Humor and Concordance Correlation Coefficient (CCC)
values of .7482 for arousal and .7827 for valence in the MuSe-Personalisation
sub-challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Gesture Recognition using Transformer and Natural Language Processing. (arXiv:2305.03407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03407">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture is shown to provide a powerful machine
transduction framework for online handwritten gestures corresponding to glyph
strokes of natural language sentences. The attention mechanism is successfully
used to create latent representations of an end-to-end encoder-decoder model,
solving multi-level segmentation while also learning some language features and
syntax rules. The additional use of a large decoding space with some learned
Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and
syntax rules. The encoder stack was directly fed with spatio-temporal data
tokens potentially forming an infinitely large input vocabulary, an approach
that finds applications beyond that of this work. Encoder transfer learning
capabilities is also demonstrated on several languages resulting in faster
optimisation and shared parameters. A new supervised dataset of online
handwriting gestures suitable for generic handwriting recognition tasks was
used to successfully train a small transformer model to an average normalised
Levenshtein accuracy of 96% on English or German sentences and 94% in French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using ChatGPT for Entity Matching. (arXiv:2305.03423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03423">
<div class="article-summary-box-inner">
<span><p>Entity Matching is the task of deciding if two entity descriptions refer to
the same real-world entity. State-of-the-art entity matching methods often rely
on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks
of using these models for entity matching are that (i) the models require
significant amounts of fine-tuning data for reaching a good performance and
(ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using ChatGPT for entity matching as a
more robust, training data-efficient alternative to traditional Transformer
models. We perform experiments along three dimensions: (i) general prompt
design, (ii) in-context learning, and (iii) provision of higher-level matching
knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,
reaching an average zero-shot performance of 83% F1 on a challenging matching
task on which RoBERTa requires 2000 training examples for reaching a similar
performance. Adding in-context demonstrations to the prompts further improves
the F1 by up to 5% even using only a small set of 20 handpicked examples.
Finally, we show that guiding the zero-shot model by stating higher-level
matching rules leads to similar gains as providing in-context examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulating H.P. Lovecraft horror literature with the ChatGPT large language model. (arXiv:2305.03429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03429">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel approach to simulating H.P. Lovecraft's
horror literature using the ChatGPT large language model, specifically the
GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's
unique writing style and themes, while also examining the effectiveness of
prompt engineering techniques in guiding the model's output. To achieve this,
we curated a prompt containing several specialized literature references and
employed advanced prompt engineering methods. We conducted an empirical
evaluation of the generated text by administering a survey to a sample of
undergraduate students. Utilizing statistical hypothesis testing, we assessed
the students ability to distinguish between genuine Lovecraft works and those
generated by our model. Our findings demonstrate that the participants were
unable to reliably differentiate between the two, indicating the effectiveness
of the GPT-4 model and our prompt engineering techniques in emulating
Lovecraft's literary style. In addition to presenting the GPT model's
capabilities, this paper provides a comprehensive description of its underlying
architecture and offers a comparative analysis with related work that simulates
other notable authors and philosophers, such as Dennett. By exploring the
potential of large language models in the context of literary emulation, our
study contributes to the body of research on the applications and limitations
of these models in various creative domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03445">
<div class="article-summary-box-inner">
<span><p>Figurative language is a challenge for language models since its
interpretation is based on the use of words in a way that deviates from their
conventional order and meaning. Yet, humans can easily understand and interpret
metaphors, similes or idioms as they can be derived from embodied metaphors.
Language is a proxy for embodiment and if a metaphor is conventional and
lexicalised, it becomes easier for a system without a body to make sense of
embodied concepts. Yet, the intricate relation between embodiment and features
such as concreteness or age of acquisition has not been studied in the context
of figurative language interpretation concerning language models. Hence, the
presented study shows how larger language models perform better at interpreting
metaphoric sentences when the action of the metaphorical sentence is more
embodied. The analysis rules out multicollinearity with other features (e.g.
word length or concreteness) and provides initial evidence that larger language
models conceptualise embodied concepts to a degree that facilitates figurative
language understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03453">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently demonstrated exceptional
performance in various Natural Language Processing (NLP) tasks. They have also
shown the ability to perform chain-of-thought (CoT) reasoning to solve complex
problems. Recent studies have explored CoT reasoning in complex multimodal
scenarios, such as the science question answering task, by fine-tuning
multimodal models with high-quality human-annotated CoT rationales. However,
collecting high-quality COT rationales is usually time-consuming and costly.
Besides, the annotated rationales are hardly accurate due to the redundant
information involved or the essential information missed. To address these
issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching
science question answering with LLM signals. The T-SciQ approach generates
high-quality CoT rationales as teaching signals and is advanced to train much
smaller models to perform CoT reasoning in complex modalities. Additionally, we
introduce a novel data mixing strategy to produce more effective teaching data
samples for simple and complex science question answer problems. Extensive
experimental results show that our T-SciQ method achieves a new
state-of-the-art performance on the ScienceQA benchmark, with an accuracy of
96.18%. Moreover, our approach outperforms the most powerful fine-tuned
baseline by 4.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question. (arXiv:2305.03458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03458">
<div class="article-summary-box-inner">
<span><p>Hybrid question answering (HybridQA) over the financial report contains both
textual and tabular data, and requires the model to select the appropriate
evidence for the numerical reasoning task. Existing methods based on
encoder-decoder framework employ a expression tree-based decoder to solve
numerical reasoning problems. However, encoders rely more on Machine Reading
Comprehension (MRC) methods, which take table serialization and text splicing
as input, damaging the granularity relationship between table and text as well
as the spatial structure information of table itself. In order to solve these
problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the
relations among the granularity into account and capture the relations from
multiple view. By utilizing MVGE as a module, we constuct Tabular View,
Relation View and Numerical View which aim to retain the original
characteristics of the hybrid data. We validate our model on the publicly
available table-text hybrid QA benchmark (TAT-QA) and outperform the
state-of-the-art model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Acquisition of Fine-grained Visual Concepts by Exploiting Semantics of Generic Characterizations in Discourse. (arXiv:2305.03461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03461">
<div class="article-summary-box-inner">
<span><p>Interactive Task Learning (ITL) concerns learning about unforeseen domain
concepts via natural interactions with human users. The learner faces a number
of significant constraints: learning should be online, incremental and
few-shot, as it is expected to perform tangible belief updates right after
novel words denoting unforeseen concepts are introduced. In this work, we
explore a challenging symbol grounding task--discriminating among object
classes that look very similar--within the constraints imposed by ITL. We
demonstrate empirically that more data-efficient grounding results from
exploiting the truth-conditions of the teacher's generic statements (e.g., "Xs
have attribute Z.") and their implicatures in context (e.g., as an answer to
"How are Xs and Ys different?", one infers Y lacks attribute Z).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03495">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown impressive performance as general
purpose agents, but their abilities remain highly dependent on prompts which
are hand written with onerous trial-and-error effort. We propose a simple and
nonparametric solution to this problem, Automatic Prompt Optimization (APO),
which is inspired by numerical gradient descent to automatically improve
prompts, assuming access to training data and an LLM API. The algorithm uses
minibatches of data to form natural language ``gradients'' that criticize the
current prompt. The gradients are then ``propagated'' into the prompt by
editing the prompt in the opposite semantic direction of the gradient. These
gradient descent steps are guided by a beam search and bandit selection
procedure which significantly improves algorithmic efficiency. Preliminary
results across three benchmark NLP tasks and the novel problem of LLM jailbreak
detection suggest that Automatic Prompt Optimization can outperform prior
prompt editing techniques and improve an initial prompt's performance by up to
31\%, by using data to rewrite vague task descriptions into more precise
annotation instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy. (arXiv:2305.03497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03497">
<div class="article-summary-box-inner">
<span><p>With the increasing use of cloud-based services for training and deploying
machine learning models, data privacy has become a major concern. This is
particularly important for natural language processing (NLP) models, which
often process sensitive information such as personal communications and
confidential documents. In this study, we propose a method for training NLP
models on encrypted text data to mitigate data privacy concerns while
maintaining similar performance to models trained on non-encrypted data. We
demonstrate our method using two different architectures, namely
Doc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups
dataset. Our results indicate that both encrypted and non-encrypted models
achieve comparable performance, suggesting that our encryption method is
effective in preserving data privacy without sacrificing model accuracy. In
order to replicate our experiments, we have provided a Colab notebook at the
following address: https://t.ly/lR-TP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers. (arXiv:2305.03501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03501">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach to accurately classify the hallmarks of
cancer, which is a crucial task in cancer research. Our proposed method
utilizes the Bidirectional Encoder Representations from Transformers (BERT)
architecture, which has shown exceptional performance in various downstream
applications. By applying transfer learning, we fine-tuned the pre-trained BERT
model on a small corpus of biomedical text documents related to cancer. The
outcomes of our experimental investigations demonstrate that our approach
attains a noteworthy accuracy of 94.45%, surpassing almost all prior findings
with a substantial increase of at least 8.04% as reported in the literature.
These findings highlight the effectiveness of our proposed model in accurately
classifying and comprehending text documents for cancer research, thus
contributing significantly to the field. As cancer remains one of the top ten
leading causes of death globally, our approach holds great promise in advancing
cancer research and improving patient outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore the difficulty of words and its influential attributes based on the Wordle game. (arXiv:2305.03502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03502">
<div class="article-summary-box-inner">
<span><p>We adopt the distribution and expectation of guessing times in game Wordle as
metrics to predict the difficulty of words and explore their influence factors.
In order to predictthe difficulty distribution, we use Monte Carlo to simulate
the guessing process of players and then narrow the gap between raw and actual
distribution of guessing times for each word with Markov which generates the
associativity of words. Afterwards, we take advantage of lasso regression to
predict the deviation of guessing times expectation and quadratic programming
to obtain the correction of the original distribution.To predict the difficulty
levels, we first use hierarchical clustering to classify the difficulty levels
based on the expectation of guessing times. Afterwards we downscale the
variables of lexical attributes based on factor analysis. Significant factors
include the number of neighboring words, letter similarity, sub-string
similarity, and word frequency. Finally, we build the relationship between
lexical attributes and difficulty levels through ordered logistic regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction. (arXiv:2305.03503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03503">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) aims to extract potential relations according to the
context of two entities, thus, deriving rational contexts from sentences plays
an important role. Previous works either focus on how to leverage the entity
information (e.g., entity types, entity verbalization) to inference relations,
but ignore context-focused content, or use counterfactual thinking to remove
the model's bias of potential relations in entities, but the relation reasoning
process will still be hindered by irrelevant content. Therefore, how to
preserve relevant content and remove noisy segments from sentences is a crucial
task. In addition, retained content needs to be fluent enough to maintain
semantic coherence and interpretability. In this work, we propose a novel
rationale extraction framework named RE2, which leverages two continuity and
sparsity factors to obtain relevant and coherent rationales from sentences. To
solve the problem that the gold rationales are not labeled, RE2 applies an
optimizable binary mask to each token in the sentence, and adjust the
rationales that need to be selected according to the relation label.
Experiments on four datasets show that RE2 surpasses baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03506">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation~(ERC) across modalities is of vital
importance for a variety of applications, including intelligent healthcare,
artificial intelligence for conversation, and opinion mining over chat history.
The crux of ERC is to model both cross-modality and cross-time interactions
throughout the conversation. Previous methods have made progress in learning
the time series information of conversation while lacking the ability to trace
down the different emotional states of each speaker in a conversation. In this
paper, we propose a recurrent structure called Speaker Information Enhanced
Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states
of the distinct speaker can be tracked in a sequential way to enhance the
learning of the emotion in conversation. Further, to improve the learning of
multimodal features in ERC, we utilize a cross-modal attention component to
fuse the features between different modalities and model the interaction of the
important information from different modalities. Experimental results on two
benchmark datasets demonstrate the superiority of the proposed SI-LSTM against
the state-of-the-art baseline methods in the ERC task on multimodal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence. (arXiv:2305.03507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03507">
<div class="article-summary-box-inner">
<span><p>Real-world fact verification task aims to verify the factuality of a claim by
retrieving evidence from the source document. The quality of the retrieved
evidence plays an important role in claim verification. Ideally, the retrieved
evidence should be faithful (reflecting the model's decision-making process in
claim verification) and plausible (convincing to humans), and can improve the
accuracy of verification task. Although existing approaches leverage the
similarity measure of semantic or surface form between claims and documents to
retrieve evidence, they all rely on certain heuristics that prevent them from
satisfying all three requirements. In light of this, we propose a fact
verification model named ReRead to retrieve evidence and verify claim that: (1)
Train the evidence retriever to obtain interpretable evidence (i.e.,
faithfulness and plausibility criteria); (2) Train the claim verifier to
revisit the evidence retrieved by the optimized evidence retriever to improve
the accuracy. The proposed system is able to achieve significant improvements
upon best-reported models under different settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing. (arXiv:2305.03508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03508">
<div class="article-summary-box-inner">
<span><p>In legal document writing, one of the key elements is properly citing the
case laws and other sources to substantiate claims and arguments. Understanding
the legal domain and identifying appropriate citation context or cite-worthy
sentences are challenging tasks that demand expensive manual annotation. The
presence of jargon, language semantics, and high domain specificity makes legal
language complex, making any associated legal task hard for automation. The
current work focuses on the problem of citation-worthiness identification. It
is designed as the initial step in today's citation recommendation systems to
lighten the burden of extracting an adequate set of citation contexts. To
accomplish this, we introduce a labeled dataset of 178M sentences for
citation-worthiness detection in the legal domain from the Caselaw Access
Project (CAP). The performance of various deep learning models was examined on
this novel dataset. The domain-specific pre-trained model tends to outperform
other models, with an 88% F1-score for the citation-worthiness detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03509">
<div class="article-summary-box-inner">
<span><p>Diffusion-based generative models' impressive ability to create convincing
images has captured global attention. However, their complex internal
structures and operations often make them difficult for non-experts to
understand. We present Diffusion Explainer, the first interactive visualization
tool that explains how Stable Diffusion transforms text prompts into images.
Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's
complex components with detailed explanations of their underlying operations,
enabling users to fluidly transition between multiple levels of abstraction
through animations and interactive elements. By comparing the evolutions of
image representations guided by two related text prompts over refinement
timesteps, users can discover the impact of prompts on image generation.
Diffusion Explainer runs locally in users' web browsers without the need for
installation or specialized hardware, broadening the public's education access
to modern AI techniques. Our open-sourced tool is available at:
https://poloclub.github.io/diffusion-explainer/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03510">
<div class="article-summary-box-inner">
<span><p>Pre-trained vision and language models such as CLIP have witnessed remarkable
success in connecting images and texts with a primary focus on English texts.
Despite recent efforts to extend CLIP to support other languages, disparities
in performance among different languages have been observed due to uneven
resource availability. Additionally, current cross-lingual transfer methods of
those pre-trained models would consume excessive resources for a large number
of languages. Therefore, we propose a new parameter-efficient cross-lingual
transfer learning framework that utilizes a translation-based alignment method
to mitigate multilingual disparities and explores parameter-efficient
fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive
experiments on XTD and Multi30K datasets, covering 11 languages under
zero-shot, few-shot, and full-dataset learning scenarios, show that our
framework significantly reduces the multilingual disparities among languages
and improves cross-lingual transfer results, especially in low-resource
scenarios, while only keeping and fine-tuning an extremely small number of
parameters compared to the full model (e.g., Our framework only requires 0.16\%
additional parameters of a full-model for each language in the few-shot
learning scenario).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation. (arXiv:2305.03511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03511">
<div class="article-summary-box-inner">
<span><p>Latent variable modeling in non-autoregressive neural machine translation
(NAT) is a promising approach to mitigate the multimodality problem. In the
previous works, they added an auxiliary model to estimate the posterior
distribution of the latent variable conditioned on the source and target
sentences. However, it causes several disadvantages, such as redundant
information extraction in the latent variable, increasing parameters, and a
tendency to ignore a part of the information from the inputs. In this paper, we
propose a new latent variable modeling that is based on a dual reconstruction
perspective and an advanced hierarchical latent modeling approach. Our proposed
method, {\em LadderNMT}, shares a latent space across both languages so that it
hypothetically alleviates or solves the above disadvantages. Experimental
results quantitatively and qualitatively demonstrate that our proposed latent
variable modeling learns an advantageous latent space and significantly
improves translation quality in WMT translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Multimodal AI Chatbots. (arXiv:2305.03512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03512">
<div class="article-summary-box-inner">
<span><p>This work aims to create a multimodal AI system that chats with humans and
shares relevant photos. While earlier works were limited to dialogues about
specific objects or scenes within images, recent works have incorporated images
into open-domain dialogues. However, their response generators are unimodal,
accepting text input but no image input, thus prone to generating responses
contradictory to the images shared in the dialogue. Therefore, this work
proposes a complete chatbot system using two multimodal deep learning models:
an image retriever that understands texts and a response generator that
understands images. The image retriever, implemented by ViT and BERT, selects
the most relevant image given the dialogue history and a database of images.
The response generator, implemented by ViT and GPT-2/DialoGPT, generates an
appropriate response given the dialogue history and the most recently retrieved
image. The two models are trained and evaluated on PhotoChat, an open-domain
dialogue dataset in which a photo is shared in each session. In automatic
evaluation, the proposed image retriever outperforms existing baselines VSE++
and SCAN with Recall@1/5/10 of 0.1/0.3/0.4 and MRR of 0.2 when ranking 1,000
images. The proposed response generator also surpasses the baseline Divter with
PPL of 16.9, BLEU-1/2 of 0.13/0.03, and Distinct-1/2 of 0.97/0.86, showing a
significant improvement in PPL by -42.8 and BLEU-1/2 by +0.07/0.02. In human
evaluation with a Likert scale of 1-5, the complete multimodal chatbot system
receives higher image-groundedness of 4.3 and engagingness of 4.3, along with
competitive fluency of 4.1, coherence of 3.9, and humanness of 3.1, when
compared to other chatbot variants. The source code is available at:
https://github.com/minniie/multimodal_chat.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03513">
<div class="article-summary-box-inner">
<span><p>ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03514">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) like ChatGPT are capable of successfully
performing many language processing tasks zero-shot (without the need for
training data). If this capacity also applies to the coding of social phenomena
like persuasiveness and political ideology, then LLMs could effectively
transform Computational Social Science (CSS). This work provides a road map for
using LLMs as CSS tools. Towards this end, we contribute a set of prompting
best practices and an extensive evaluation pipeline to measure the zero-shot
performance of 13 language models on 24 representative CSS benchmarks. On
taxonomic labeling tasks (classification), LLMs fail to outperform the best
fine-tuned models but still achieve fair levels of agreement with humans. On
free-form coding tasks (generation), LLMs produce explanations that often
exceed the quality of crowdworkers' gold references. We conclude that today's
LLMs can radically augment the CSS research pipeline in two ways: (1) serving
as zero-shot data annotators on human annotation teams, and (2) bootstrapping
challenging creative generation tasks (e.g., explaining the hidden meaning
behind text). In summary, LLMs can significantly reduce costs and increase
efficiency of social science analysis in partnership with humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03517">
<div class="article-summary-box-inner">
<span><p>Incorporating auxiliary modalities such as images into event detection models
has attracted increasing interest over the last few years. The complexity of
natural language in describing situations has motivated researchers to leverage
the related visual context to improve event detection performance. However,
current approaches in this area suffer from data scarcity, where a large amount
of labelled text-image pairs are required for model training. Furthermore,
limited access to the visual context at inference time negatively impacts the
performance of such models, which makes them practically ineffective in
real-world scenarios. In this paper, we present a novel domain-adaptive
visually-fused event detection approach that can be trained on a few labelled
image-text paired data points. Specifically, we introduce a visual imaginator
method that synthesises images from text in the absence of visual context.
Moreover, the imaginator can be customised to a specific domain. In doing so,
our model can leverage the capabilities of pre-trained vision-language models
and can be trained in a few-shot setting. This also allows for effective
inference where only single-modality data (i.e. text) is available. The
experimental evaluation on the benchmark M2E2 dataset shows that our model
outperforms existing state-of-the-art models, by up to 11 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03518">
<div class="article-summary-box-inner">
<span><p>Black-box prompt tuning uses derivative-free optimization algorithms to learn
prompts in low-dimensional subspaces instead of back-propagating through the
network of Large Language Models (LLMs). Recent studies have found that
black-box prompt tuning lacks versatility across tasks and LLMs, which we
believe is related to the inappropriate choice of subspaces. In this paper, we
propose Black-box prompt tuning with Subspace Learning (BSL) to improve the
versatility of black-box prompt tuning. Based on the assumption that nearly
optimal prompts for similar tasks exist in a common subspace, we propose
identifying such subspaces by meta-learning on a set of similar source tasks.
Therefore, for a target task that shares similarities with source tasks, we
guarantee that optimizing in the subspace can find a prompt that performs well
on the target task. Experiments confirm that our BSL framework consistently
achieves competitive performance regardless of downstream tasks and LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging BERT Language Model for Arabic Long Document Classification. (arXiv:2305.03519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03519">
<div class="article-summary-box-inner">
<span><p>Given the number of Arabic speakers worldwide and the notably large amount of
content in the web today in some fields such as law, medicine, or even news,
documents of considerable length are produced regularly. Classifying those
documents using traditional learning models is often impractical since extended
length of the documents increases computational requirements to an
unsustainable level. Thus, it is necessary to customize these models
specifically for long textual documents. In this paper we propose two simple
but effective models to classify long length Arabic documents. We also
fine-tune two different models-namely, Longformer and RoBERT, for the same task
and compare their results to our models. Both of our models outperform the
Longformer and RoBERT in this task over two different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03520">
<div class="article-summary-box-inner">
<span><p>The issue of word sense ambiguity poses a significant challenge in natural
language processing due to the scarcity of annotated data to feed machine
learning models to face the challenge. Therefore, unsupervised word sense
disambiguation methods have been developed to overcome that challenge without
relying on annotated data. This research proposes a new context-aware approach
to unsupervised word sense disambiguation, which provides a flexible mechanism
for incorporating contextual information into the similarity measurement
process. We experiment with a popular benchmark dataset to evaluate the
proposed strategy and compare its performance with state-of-the-art
unsupervised word sense disambiguation techniques. The experimental results
indicate that our approach substantially enhances disambiguation accuracy and
surpasses the performance of several existing techniques. Our findings
underscore the significance of integrating contextual information in semantic
similarity measurements to manage word sense ambiguity in unsupervised
scenarios effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models. (arXiv:2305.03573v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03573">
<div class="article-summary-box-inner">
<span><p>The phenomena of in-context learning has typically been thought of as
"learning from examples". In this work which focuses on Machine Translation, we
present a perspective of in-context learning as the desired generation task
maintaining coherency with its context, i.e., the prompt examples. We first
investigate randomly sampled prompts across 4 domains, and find that
translation performance improves when shown in-domain prompts. Next, we
investigate coherency for the in-domain setting, which uses prompt examples
from a moving window. We study this with respect to other factors that have
previously been identified in the literature such as length, surface similarity
and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,
Bloom3B, XGLM2.9B), and three translation directions
(\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-term
coherency of the prompts and the test sentence is a good indicator of
downstream translation performance. In doing so, we demonstrate the efficacy of
In-context Machine Translation for on-the-fly adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03584">
<div class="article-summary-box-inner">
<span><p>In recent years, Federated Learning (FL) has shown significant advancements
in its ability to perform various natural language processing (NLP) tasks. This
work focuses on applying personalized FL for on-device language modeling. Due
to limitations of memory and latency, these models cannot support the
complexity of sub-word tokenization or beam search decoding, resulting in the
decision to deploy a closed-vocabulary language model. However,
closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words
belonging to specific users. To address this issue, We propose a novel
technique called "OOV expansion" that improves OOV coverage and increases model
accuracy while minimizing the impact on memory and latency. This method
introduces a personalized "OOV adapter" that effectively transfers knowledge
from a central model and learns word embedding for personalized vocabulary. OOV
expansion significantly outperforms standard FL personalization methods on a
set of common FL benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03598">
<div class="article-summary-box-inner">
<span><p>How can we interpret and retrieve medical evidence to support clinical
decisions? Clinical trial reports (CTR) amassed over the years contain
indispensable information for the development of personalized medicine.
However, it is practically infeasible to manually inspect over 400,000+
clinical trial reports in order to find the best evidence for experimental
treatments. Natural Language Inference (NLI) offers a potential solution to
this problem, by allowing the scalable computation of textual entailment.
However, existing NLI models perform poorly on biomedical corpora, and
previously published datasets fail to capture the full complexity of inference
over CTRs. In this work, we present a novel resource to advance research on NLI
for reasoning on CTRs. The resource includes two main tasks. Firstly, to
determine the inference relation between a natural language statement, and a
CTR. Secondly, to retrieve supporting facts to justify the predicted relation.
We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these
tasks. Baselines on this corpus expose the limitations of existing NLI models,
with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To
the best of our knowledge, we are the first to design a task that covers the
interpretation of full CTRs. To encourage further work on this challenging
dataset, we make the corpus, competition leaderboard, website and code to
replicate the baseline experiments available at:
https://github.com/ai-systems/nli4ct
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03602">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) is a realistic but challenging task that
requires an agent to locate the target region using verbal and visual cues.
While significant advancements have been achieved recently, there are still two
broad limitations: (1) The explicit information mining for significant guiding
semantics concealed in both vision and language is still under-explored; (2)
The previously structured map method provides the average historical appearance
of visited nodes, while it ignores distinctive contributions of various images
and potent information retention in the reasoning process. This work proposes a
dual semantic-aware recurrent global-adaptive network (DSRG) to address the
above problems. First, DSRG proposes an instruction-guidance linguistic module
(IGL) and an appearance-semantics visual module (ASV) for boosting vision and
language semantic learning respectively. For the memory mechanism, a global
adaptive aggregation module (GAA) is devised for explicit panoramic observation
fusion, and a recurrent memory fusion module (RMF) is introduced to supply
implicit temporal hidden states. Extensive experimental results on the R2R and
REVERIE datasets demonstrate that our method achieves better performance than
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03610">
<div class="article-summary-box-inner">
<span><p>Recent advances in image captioning are mainly driven by large-scale
vision-language pretraining, relying heavily on computational resources and
increasingly large multimodal datasets. Instead of scaling up pretraining data,
we ask whether it is possible to improve performance by improving the quality
of the samples in existing datasets. We pursue this question through two
approaches to data curation: one that assumes that some examples should be
avoided due to mismatches between the image and caption, and one that assumes
that the mismatch can be addressed by replacing the image, for which we use the
state-of-the-art Stable Diffusion model. These approaches are evaluated using
the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot
learning settings. Our simple yet effective approaches consistently outperform
baselines, indicating that better image captioning models can be trained by
curating existing resources. Finally, we conduct a human study to understand
the errors made by the Stable Diffusion model and highlight directions for
future work in text-to-image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03642">
<div class="article-summary-box-inner">
<span><p>Results from Randomized Controlled Trials (RCTs) establish the comparative
effectiveness of interventions, and are in turn critical inputs for
evidence-based care. However, results from RCTs are presented in (often
unstructured) natural language articles describing the design, execution, and
outcomes of trials; clinicians must manually extract findings pertaining to
interventions and outcomes of interest from such articles. This onerous manual
process has motivated work on (semi-)automating extraction of structured
evidence from trial reports. In this work we propose and evaluate a
text-to-text model built on instruction-tuned Large Language Models (LLMs) to
jointly extract Interventions, Outcomes, and Comparators (ICO elements) from
clinical abstracts, and infer the associated results reported. Manual (expert)
and automated evaluations indicate that framing evidence extraction as a
conditional generation task and fine-tuning LLMs for this purpose realizes
considerable ($\sim$20 point absolute F1 score) gains over the previous SOTA.
We perform ablations and error analyses to assess aspects that contribute to
model performance, and to highlight potential directions for further
improvements. We apply our model to a collection of published RCTs through
mid-2022, and release a searchable database of structured findings (anonymously
for now): bit.ly/joint-relations-extraction-mlhc
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03655">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformers are popular in state-of-the-art dialogue generation
(DG) systems. Such language models are, however, vulnerable to various
adversarial samples as studied in traditional tasks such as text
classification, which inspires our curiosity about their robustness in DG
systems. One main challenge of attacking DG models is that perturbations on the
current sentence can hardly degrade the response accuracy because the unchanged
chat histories are also considered for decision-making. Instead of merely
pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that
crafting adversarial samples to force longer generation outputs benefits attack
effectiveness -- the generated responses are typically irrelevant, lengthy, and
repetitive. To this end, we propose a white-box multi-objective attack method
called DGSlow. Specifically, DGSlow balances two objectives -- generation
accuracy and length, via a gradient-based multi-objective optimizer and applies
an adaptive searching mechanism to iteratively craft adversarial samples with
only a few modifications. Comprehensive experiments on four benchmark datasets
demonstrate that DGSlow could significantly degrade state-of-the-art DG models
with a higher success rate than traditional accuracy-based methods. Besides,
our crafted sentences also exhibit strong transferability in attacking other
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03660">
<div class="article-summary-box-inner">
<span><p>We propose Retrieval Augmented Generation (RAG) as an approach for automated
radiology report writing that leverages multimodally aligned embeddings from a
contrastively pretrained vision language model for retrieval of relevant
candidate radiology text for an input radiology image and a general domain
generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for
report generation using the relevant radiology text retrieved. This approach
keeps hallucinated generations under check and provides capabilities to
generate report content in the format we desire leveraging the instruction
following capabilities of these generative models. Our approach achieves better
clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score
of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different
clinical settings as it allows to augment the automated radiology report
generation process with content relevant for that setting while also having the
ability to inject user intents and requirements in the prompts as part of the
report generation process to modulate the content and format of the generated
reports as applicable for that clinical setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03661">
<div class="article-summary-box-inner">
<span><p>In this paper we present a novel approach to risk assessment for patients
hospitalized with pneumonia or COVID-19 based on their admission reports. We
applied a Longformer neural network to admission reports and other textual data
available shortly after admission to compute risk scores for the patients. We
used patient data of multiple European hospitals to demonstrate that our
approach outperforms the Transformer baselines. Our experiments show that the
proposed model generalises across institutions and diagnoses. Also, our method
has several other advantages described in the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03668">
<div class="article-summary-box-inner">
<span><p>Webpages have been a rich, scalable resource for vision-language and language
only tasks. Yet only pieces of webpages are kept: image-caption pairs, long
text articles, or raw HTML, never all in one place. Webpage tasks have
resultingly received little attention and structured image-text data left
underused. To study multimodal webpage understanding, we introduce the
Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three
generative tasks: page description generation, section summarization, and
contextual image captioning. We design a novel attention mechanism Prefix
Global, which selects the most relevant image and text content as global tokens
to attend to the rest of the webpage for context. By using page structure to
separate such tokens, it performs better than full attention with lower
computational complexity. Experiments show that the new annotations from
WikiWeb2M improve task performance compared to data from prior work. We also
include ablations on sequence length, input features, and model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03688">
<div class="article-summary-box-inner">
<span><p>The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity
recognition (NER) in fine-grained and noisy scenarios, and it inherits the
semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To
cope with these problems, the previous top systems in the MultiCoNER \RNum{1}
either incorporate the knowledge bases or gazetteers. However, they still
suffer from insufficient knowledge, limited context length, single retrieval
strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified
retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We
perform error analysis on the previous top systems and reveal that their
performance bottleneck lies in insufficient knowledge. Also, we discover that
the limited context length causes the retrieval knowledge to be invisible to
the model. To enhance the retrieval context, we incorporate the entity-centric
Wikidata knowledge base, while utilizing the infusion approach to broaden the
contextual scope of the model. Also, we explore various search strategies and
refine the quality of retrieval knowledge. Our system\footnote{We will release
the dataset, code, and scripts of our system at {\small
\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins
9 out of 13 tracks in the MultiCoNER \RNum{2} shared task. Additionally, we
compared our system with ChatGPT, one of the large language models which have
unlocked strong capabilities on many tasks. The results show that there is
still much room for improvement for ChatGPT on the extraction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03695">
<div class="article-summary-box-inner">
<span><p>Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management. (arXiv:2305.03715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03715">
<div class="article-summary-box-inner">
<span><p>This study investigates the potential of an ambulatory device that
incorporates Large Language Models (LLMs) in cadence with other specialized ML
models to assess anemia severity in sickle cell patients in real time. The
device would rely on sensor data that measures angiogenic material levels to
assess anemia severity, providing real-time information to patients and
clinicians to reduce the frequency of vaso-occlusive crises because of the
early detection of anemia severity, allowing for timely interventions and
potentially reducing the likelihood of serious complications. The main
challenges in developing such a device are the creation of a reliable
non-invasive tool for angiogenic level assessment, a biophysics model and the
practical consideration of an LLM communicating with emergency personnel on
behalf of an incapacitated patient. A possible system is proposed, and the
limitations of this approach are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Otter: A Multi-Modal Model with In-Context Instruction Tuning. (arXiv:2305.03726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03726">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated significant universal
capabilities as few/zero-shot learners in various tasks due to their
pre-training on vast amounts of text data, as exemplified by GPT-3, which
boosted to InstrctGPT and ChatGPT, effectively following natural language
instructions to accomplish real-world tasks. In this paper, we propose to
introduce instruction tuning into multi-modal models, motivated by the Flamingo
model's upstream interleaved format pretraining dataset. We adopt a similar
approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)
dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo
(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and
showcasing improved instruction-following ability and in-context learning. We
also optimize OpenFlamingo's implementation for researchers, democratizing the
required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs,
and integrate both OpenFlamingo and Otter into Huggingface Transformers for
more researchers to incorporate the models into their customized training and
inference pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVATAR: A Parallel Corpus for Java-Python Program Translation. (arXiv:2108.11590v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11590">
<div class="article-summary-box-inner">
<span><p>Program translation refers to migrating source code from one programming
language to another. It has tremendous practical value in software development,
as porting software across languages is time-consuming and costly. Automating
program translation is of paramount importance in software migration, and
recently researchers explored unsupervised approaches due to the unavailability
of parallel corpora. However, the availability of pre-trained language models
for programming languages enables supervised fine-tuning with a small number of
labeled examples. Therefore, we present AVATAR, a collection of 9,515
programming problems and their solutions written in two popular languages, Java
and Python. AVATAR is collected from competitive programming sites, online
platforms, and open-source repositories. Furthermore, AVATAR includes unit
tests for 250 examples to facilitate functional correctness evaluation. We
benchmark several pre-trained language models fine-tuned on AVATAR. Experiment
results show that the models lack in generating functionally accurate code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01404">
<div class="article-summary-box-inner">
<span><p>Natural language generation from structured data mainly focuses on
surface-level descriptions, suffering from uncontrollable content selection and
low fidelity. Previous works leverage logical forms to facilitate logical
knowledge-conditioned text generation. Though achieving remarkable progress,
they are data-hungry, which makes the adoption for real-world applications
challenging with limited data. To this end, this paper proposes a unified
framework for logical knowledge-conditioned text generation in the few-shot
setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach
leverages self-training and samples pseudo logical forms based on content and
structure consistency. Experimental results demonstrate that our approach can
obtain better few-shot performance than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05823">
<div class="article-summary-box-inner">
<span><p>Open intent detection is a significant problem in natural language
understanding, which aims to identify the unseen open intent while ensuring
known intent identification performance. However, current methods face two
major challenges. Firstly, they struggle to learn friendly representations to
detect the open intent with prior knowledge of only known intents. Secondly,
there is a lack of an effective approach to obtaining specific and compact
decision boundaries for known intents. To address these issues, this paper
presents an original framework called DA-ADB, which successively learns
distance-aware intent representations and adaptive decision boundaries for open
intent detection. Specifically, we first leverage distance information to
enhance the distinguishing capability of the intent representations. Then, we
design a novel loss function to obtain appropriate decision boundaries by
balancing both empirical and open space risks. Extensive experiments
demonstrate the effectiveness of the proposed distance-aware and boundary
learning strategies. Compared to state-of-the-art methods, our framework
achieves substantial improvements on three benchmark datasets. Furthermore, it
yields robust performance with varying proportions of labeled data and known
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03954">
<div class="article-summary-box-inner">
<span><p>The popularity of graph neural networks has triggered a resurgence of
graph-based methods for single-label and multi-label text classification.
However, it is unclear whether these graph-based methods are beneficial
compared to standard machine learning methods and modern pretrained language
models. We compare a rich selection of bag-of-words, sequence-based,
graph-based, and hierarchical methods for text classification. We aggregate
results from the literature over 5 single-label and 7 multi-label datasets and
run our own experiments. Our findings unambiguously demonstrate that for
single-label and multi-label classification tasks, the graph-based methods fail
to outperform fine-tuned language models and sometimes even perform worse than
standard machine learning methods like multilayer perceptron (MLP) on a
bag-of-words. This questions the enormous amount of effort put into the
development of new graph-based methods in the last years and the promises they
make for text classification. Given our extensive experiments, we confirm that
pretrained language models remain state-of-the-art in text classification
despite all recent specialized advances. We argue that future work in text
classification should thoroughly test against strong baselines like MLPs to
properly assess the true scientific progress.
</p>
<p>The source code is available: https://github.com/drndr/multilabel-text-clf
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness. (arXiv:2210.03884v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03884">
<div class="article-summary-box-inner">
<span><p>As a critical step to achieve human-like chatbots, empathetic response
generation has attained increasing interests. Previous attempts are incomplete
and not sufficient enough to elicit empathy because they only focus on the
initial aspect of empathy to automatically mimic the feelings and thoughts of
the user via other-awareness. However, they ignore to maintain and take the own
views of the system into account, which is a crucial process to achieve the
empathy called self-other awareness. To this end, we propose to generate
Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,
three stages, self-other differentiation, self-other modulation and self-other
generation, are devised to clearly maintain, regulate and inject the self-other
aware information into the process of empathetic response generation. Both
automatic and human evaluations on the benchmark dataset demonstrate the
superiority of EmpSOA to generate more empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05257">
<div class="article-summary-box-inner">
<span><p>For monitoring crises, political events are extracted from the news. The
large amount of unstructured full-text event descriptions makes a case-by-case
analysis unmanageable, particularly for low-resource humanitarian aid
organizations. This creates a demand to classify events into event types, a
task referred to as event coding. Typically, domain experts craft an event type
ontology, annotators label a large dataset and technical experts develop a
supervised coding system. In this work, we propose PR-ENT, a new event coding
approach that is more flexible and resource-efficient, while maintaining
competitive accuracy: first, we extend an event description such as "Military
injured two civilians'' by a template, e.g. "People were [Z]" and prompt a
pre-trained (cloze) language model to fill the slot Z. Second, we select answer
candidates Z* = {"injured'', "hurt"...} by treating the event description as
premise and the filled templates as hypothesis in a textual entailment task.
This allows domain experts to draft the codebook directly as labeled prompts
and interpretable answer candidates. This human-in-the-loop process is guided
by our interactive codebook design tool. We evaluate PR-ENT in several
robustness checks: perturbing the event description and prompt template,
restricting the vocabulary and removing contextual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00735">
<div class="article-summary-box-inner">
<span><p>In the field of antibody engineering, an essential task is to design a novel
antibody whose paratopes bind to a specific antigen with correct epitopes.
Understanding antibody structure and its paratope can facilitate a mechanistic
understanding of its function. Therefore, antibody structure prediction from
its sequence alone has always been a highly valuable problem for de novo
antibody design. AlphaFold2, a breakthrough in the field of structural biology,
provides a solution to predict protein structure based on protein sequences and
computationally expensive coevolutionary multiple sequence alignments (MSAs).
However, the computational efficiency and undesirable prediction accuracy of
antibodies, especially on the complementarity-determining regions (CDRs) of
antibodies limit their applications in the industrially high-throughput drug
design. To learn an informative representation of antibodies, we employed a
deep antibody language model (ALM) on curated sequences from the observed
antibody space database via a transformer model. We also developed a novel
model named xTrimoABFold to predict antibody structure from antibody sequence
based on the pretrained ALM as well as efficient evoformers and structural
modules. The model was trained end-to-end on the antibody structures in PDB by
minimizing the ensemble loss of domain-specific focal loss on CDR and the
frame-aligned point loss. xTrimoABFold outperforms AlphaFold2 and other protein
language model based SOTAs, e.g., OmegaFold, HelixFold-Single, and IgFold with
a large significant margin (30+\% improvement on RMSD) while performing 151
times faster than AlphaFold2. To the best of our knowledge, xTrimoABFold
achieved state-of-the-art antibody structure prediction. Its improvement in
both accuracy and efficiency makes it a valuable tool for de novo antibody
design and could make further improvements in immuno-theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01692">
<div class="article-summary-box-inner">
<span><p>Large language models show an emergent ability to learn a new task from a
small number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of finding new associations in the input.
However, the commonly-used few-shot evaluation settings using a random
selection of in-context demonstrations can not disentangle models' ability to
learn a new skill from demonstrations, as most of the randomly-selected
demonstrations do not present relations informative for prediction beyond
exposing the new task distribution.
</p>
<p>To disentangle models' in-context learning ability independent of models'
memory, we introduce a Conceptual few-shot learning method selecting the
demonstrations sharing a possibly-informative concept with the predicted
sample. We extract a set of such concepts from annotated explanations and
measure how much can models benefit from presenting these concepts in few-shot
demonstrations.
</p>
<p>We find that smaller models are more sensitive to the presented concepts.
While some of the models are able to benefit from concept-presenting
demonstrations for each assessed concept, we find that none of the assessed
in-context learners can benefit from all presented reasoning concepts
consistently, leaving the in-context concept learning an open challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09086">
<div class="article-summary-box-inner">
<span><p>We investigate response generation for multi-turn dialogue in
generative-based chatbots. Existing generative models based on RNNs (Recurrent
Neural Networks) usually employ the last hidden state to summarize the
sequences, which makes models unable to capture the subtle variability observed
in different dialogues and cannot distinguish the differences between dialogues
that are similar in composition. In this paper, we propose a Pseudo-Variational
Gated Recurrent Unit (PVGRU) component without posterior knowledge through
introducing a recurrent summarizing variable into the GRU, which can aggregate
the accumulated distribution variations of subsequences. PVGRU can perceive the
subtle semantic variability through summarizing variables that are optimized by
the devised distribution consistency and reconstruction objectives. In
addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model
based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve
the diversity and relevance of responses on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10020">
<div class="article-summary-box-inner">
<span><p>In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore is confused by truncation errors in summarization, and
MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or
middle of generations. Further, we investigate the reasons behind these blind
spots and suggest practical workarounds for a more reliable evaluation of text
generation. We have released our code and data at
https://github.com/cloudygoose/blindspot_nlg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. (arXiv:2302.12173v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12173">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are increasingly being integrated into various
applications. The functionalities of recent LLMs can be flexibly modulated via
natural language prompts. This renders them susceptible to targeted adversarial
prompting, e.g., Prompt Injection (PI) attacks enable attackers to override
original instructions and employed controls. So far, it was assumed that the
user is directly prompting the LLM. But, what if it is not the user prompting?
We argue that LLM-Integrated Applications blur the line between data and
instructions. We reveal new attack vectors, using Indirect Prompt Injection,
that enable adversaries to remotely (without a direct interface) exploit
LLM-integrated applications by strategically injecting prompts into data likely
to be retrieved. We derive a comprehensive taxonomy from a computer security
perspective to systematically investigate impacts and vulnerabilities,
including data theft, worming, information ecosystem contamination, and other
novel security risks. We demonstrate our attacks' practical viability against
both real-world systems, such as Bing's GPT-4 powered Chat and code-completion
engines, and synthetic applications built on GPT-4. We show how processing
retrieved prompts can act as arbitrary code execution, manipulate the
application's functionality, and control how and if other APIs are called.
Despite the increasing integration and reliance on LLMs, effective mitigations
of these emerging threats are currently lacking. By raising awareness of these
vulnerabilities and providing key insights into their implications, we aim to
promote the safe and responsible deployment of these powerful models and the
development of robust defenses that protect users and systems from potential
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12692">
<div class="article-summary-box-inner">
<span><p>Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. (arXiv:2304.00958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00958">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained language models (PLMs) achieve the best
performance on a wide range of natural language processing (NLP) tasks. While
the first models were trained on general domain data, specialized ones have
emerged to more effectively treat specific domains. In this paper, we propose
an original study of PLMs in the medical domain on French language. We compare,
for the first time, the performance of PLMs trained on both public data from
the web and private data from healthcare establishments. We also evaluate
different learning strategies on a set of biomedical tasks. In particular, we
show that we can take advantage of already existing biomedical PLMs in a
foreign language by further pre-train it on our targeted data. Finally, we
release the first specialized PLMs for the biomedical field in French, called
DrBERT, as well as the largest corpus of medical data under free license on
which these models are trained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03439">
<div class="article-summary-box-inner">
<span><p>Harnessing logical reasoning ability is a comprehensive natural language
understanding endeavor. With the release of Generative Pretrained Transformer 4
(GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn
the GPT-4 performance on various logical reasoning tasks. This report analyses
multiple logical reasoning datasets, with popular benchmarks like LogiQA and
ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice
reading comprehension and natural language inference tasks with benchmarks
requiring logical reasoning. We further construct a logical reasoning
out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.
We also make a performance comparison between ChatGPT and GPT-4. Experiment
results show that ChatGPT performs significantly better than the RoBERTa
fine-tuning method on most logical reasoning benchmarks. With early access to
the GPT-4 API we are able to conduct intense experiments on the GPT-4 model.
The results show GPT-4 yields even higher performance on most logical reasoning
datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known
datasets like LogiQA and ReClor. However, the performance drops significantly
when handling newly released and out-of-distribution datasets. Logical
reasoning remains challenging for ChatGPT and GPT-4, especially on
out-of-distribution and natural language inference datasets. We release the
prompt-style logical reasoning datasets as a benchmark suite and name it
LogiEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09797">
<div class="article-summary-box-inner">
<span><p>The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted an extensive and comprehensive evaluation to demonstrate the
effectiveness of the proposed method. Our experimental results on six
benchmarks show that combining CoT and self-consistency with PHP significantly
improves accuracy while remaining highly efficient. For instance, with
text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding
compared to Complex CoT, and a 46.17% reduction in sample paths with
self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances
on SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH
(50.2% -&gt; 53.9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>With the wide application of Large Language Models (LLMs) such as ChatGPT,
how to make the contents generated by LLM accurate and credible becomes very
important, especially in complex knowledge-intensive tasks. In this paper, we
propose a novel framework called Search-in-the-Chain (SearChain) to improve the
accuracy, credibility and traceability of LLM-generated content for multi-hop
question answering, which is a typical complex knowledge-intensive task.
SearChain is a framework that deeply integrates LLM and information retrieval
(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition
of the multi-hop question. Each node of the chain is a query-answer pair
consisting of an IR-oriented query and the answer generated by LLM for this
query. IR verifies, completes, and traces the information of each node of the
chain, so as to guide LLM to construct the correct chain-of-query, and finally
answer the multi-hop question. SearChain makes LLM change from trying to give a
answer to trying to construct the chain-of-query when faced with the multi-hop
question, which can stimulate the knowledge-reasoning ability and provides the
interface for IR to be deeply involved in reasoning process of LLM. IR
interacts with each node of chain-of-query of LLM. It verifies the information
of the node and provides the unknown knowledge to LLM, which ensures the
accuracy of the whole chain in the process of LLM generating the answer.
Besides, the contents returned by LLM to the user include not only the final
answer but also the reasoning process for the question, that is, the
chain-of-query and the supporting documents retrieved by IR for each node of
the chain, which improves the credibility and traceability of the contents
generated by LLM. Experimental results show SearChain outperforms related
baselines on four multi-hop question-answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01624">
<div class="article-summary-box-inner">
<span><p>Recent research demonstrates that external knowledge injection can advance
pre-trained language models (PLMs) in a variety of downstream NLP tasks.
However, existing knowledge injection methods are either applicable to
structured knowledge or unstructured knowledge, lacking a unified usage. In
this paper, we propose a UNified knowledge inTERface, UNTER, to provide a
unified perspective to exploit both structured knowledge and unstructured
knowledge. In UNTER, we adopt the decoder as a unified knowledge interface,
aligning span representations obtained from the encoder with their
corresponding knowledge. This approach enables the encoder to uniformly invoke
span-related knowledge from its parameters for downstream applications.
Experimental results show that, with both forms of knowledge injected, UNTER
gains continuous improvements on a series of knowledge-driven NLP tasks,
including entity typing, named entity recognition and relation extraction,
especially in low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02265">
<div class="article-summary-box-inner">
<span><p>Pretrained Vision-Language Models (VLMs) have achieved remarkable performance
in image retrieval from text. However, their performance drops drastically when
confronted with linguistically complex texts that they struggle to comprehend.
Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this
paper, we regard linguistically complex texts as compound proposition texts
composed of multiple simple proposition sentences and propose an end-to-end
Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three
main components: 1) Divide: a proposition generator divides the compound
proposition text into simple proposition sentences and produces their
corresponding representations, 2) Conquer: a pretrained VLMs-based
visual-linguistic interactor achieves the interaction between decomposed
proposition sentences and images, 3) Combine: a neural-symbolic reasoner
combines the above reasoning states to obtain the final solution via a neural
logic reasoning approach. According to the dual-process theory, the
visual-linguistic interactor and neural-symbolic reasoner could be regarded as
analogical reasoning System 1 and logical reasoning System 2. We conduct
extensive experiments on a challenging image retrieval from contextual
descriptions data set. Experimental results and analyses indicate NDCR
significantly improves performance in the complex image-text reasoning problem.
Code link: https://github.com/YunxinLi/NDCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02459">
<div class="article-summary-box-inner">
<span><p>While transformer-based systems have enabled greater accuracies with fewer
training examples, data acquisition obstacles still persist for rare-class
tasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active
learning has in general been proposed to alleviate such challenges, but choice
of selection strategy, the criteria by which rare-class examples are chosen,
has not been systematically evaluated. Further, transformers enable iterative
transfer-learning approaches. We propose and investigate transfer- and active
learning solutions to the rare class problem of dissonance detection through
utilizing models trained on closely related tasks and the evaluation of
acquisition strategies, including a proposed probability-of-rare-class (PRC)
approach. We perform these experiments for a specific rare class problem:
collecting language samples of cognitive dissonance from social media. We find
that PRC is a simple and effective strategy to guide annotations and ultimately
improve model accuracy while transfer-learning in a specific order can improve
the cold-start performance of the learner but does not benefit iterations of
active learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02783">
<div class="article-summary-box-inner">
<span><p>The recent improvement in code generation capabilities due to the use of
large language models has mainly benefited general purpose programming
languages. Domain specific languages, such as the ones used for IT Automation,
have received far less attention, despite involving many active developers and
being an essential component of modern cloud platforms. This work focuses on
the generation of Ansible-YAML, a widely used markup language for IT
Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code
generation tool, aimed at improving IT automation productivity. Ansible Wisdom
is a transformer-based model, extended by training with a new dataset
containing Ansible-YAML. We also develop two novel performance metrics for YAML
and Ansible to capture the specific characteristics of this domain. Results
show that Ansible Wisdom can accurately generate Ansible script from natural
language prompts with performance comparable or better than existing state of
the art code generation models.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-08 23:12:17.487684764 UTC">2023-05-08 23:12:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>