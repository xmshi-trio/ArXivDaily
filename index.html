<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-30T01:30:00Z">10-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17680">
<div class="article-summary-box-inner">
<span><p>Imagine a developer who can only change their last line of code, how often
would they have to start writing a function from scratch before it is correct?
Auto-regressive models for code generation from natural language have a similar
limitation: they do not easily allow reconsidering earlier tokens generated. We
introduce CodeFusion, a pre-trained diffusion code generation model that
addresses this limitation by iteratively denoising a complete program
conditioned on the encoded natural language. We evaluate CodeFusion on the task
of natural language to code generation for Bash, Python, and Microsoft Excel
conditional formatting (CF) rules. Experiments show that CodeFusion (75M
parameters) performs on par with state-of-the-art auto-regressive systems
(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and
top-5 accuracy due to its better balance in diversity versus quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-contrastive sentence representations via self-supervision. (arXiv:2310.17690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17690">
<div class="article-summary-box-inner">
<span><p>Sample contrastive methods, typically referred to simply as contrastive are
the foundation of most unsupervised methods to learn text and sentence
embeddings. On the other hand, a different class of self-supervised loss
functions and methods have been considered in the computer vision community and
referred to as dimension contrastive. In this paper, we thoroughly compare this
class of methods with the standard baseline for contrastive sentence
embeddings, SimCSE. We find that self-supervised embeddings trained using
dimension contrastive objectives can outperform SimCSE on downstream tasks
without needing auxiliary loss functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The impact of using an AI chatbot to respond to patient messages. (arXiv:2310.17703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17703">
<div class="article-summary-box-inner">
<span><p>Documentation burden is a major contributor to clinician burnout, which is
rising nationally and is an urgent threat to our ability to care for patients.
Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician
burden by assisting with documentation. Although many hospitals are actively
integrating such systems into electronic medical record systems, AI chatbots
utility and impact on clinical decision-making have not been studied for this
intended use. We are the first to examine the utility of large language models
in assisting clinicians draft responses to patient questions. In our two-stage
cross-sectional study, 6 oncologists responded to 100 realistic synthetic
cancer patient scenarios and portal messages developed to reflect common
medical situations, first manually, then with AI assistance.
</p>
<p>We find AI-assisted responses were longer, less readable, but provided
acceptable drafts without edits 58% of time. AI assistance improved efficiency
77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses
could severely harm. In 31% cases, physicians thought AI drafts were
human-written. AI assistance led to more patient education recommendations,
fewer clinical actions than manual responses. Results show promise for AI to
improve clinician efficiency and patient care through assisting documentation,
if used judiciously. Monitoring model outputs and human-AI interaction remains
crucial for safe implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term. (arXiv:2310.17711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17711">
<div class="article-summary-box-inner">
<span><p>With advancements in natural language processing (NLP) models, automatic
explanation generation has been proposed to mitigate misinformation on social
media platforms in addition to adding warning labels to identified fake news.
While many researchers have focused on generating good explanations, how these
explanations can really help humans combat fake news is under-explored. In this
study, we compare the effectiveness of a warning label and the state-of-the-art
counterfactual explanations generated by GPT-4 in debunking misinformation. In
a two-wave, online human-subject study, participants (N = 215) were randomly
assigned to a control group in which false contents are shown without any
intervention, a warning tag group in which the false claims were labeled, or an
explanation group in which the false contents were accompanied by GPT-4
generated explanations. Our results show that both interventions significantly
decrease participants' self-reported belief in fake claims in an equivalent
manner for the short-term and long-term. We discuss the implications of our
findings and directions for future NLP-based misinformation debunking
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents. (arXiv:2310.17714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17714">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) has achieved remarkable progress with the help of
pre-trained language models. However, existing RE models are usually incapable
of handling two situations: implicit expressions and long-tail relation
classes, caused by language complexity and data sparsity. Further, these
approaches and models are largely inaccessible to users who don't have direct
access to large language models (LLMs) and/or infrastructure for supervised
training or fine-tuning. Rule-based systems also struggle with implicit
expressions. Apart from this, Real world financial documents such as various
10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose
another challenge to rule-based systems in terms of longer and complex
sentences. In this paper, we introduce a simple approach that consults training
relations at test time through a nearest-neighbor search over dense vectors of
lexico-syntactic patterns and provides a simple yet effective means to tackle
the above issues. We evaluate our approach on REFinD and show that our method
achieves state-of-the-art performance. We further show that it can provide a
good start for human in the loop setup when a small number of annotations are
available and it is also beneficial when domain experts can provide high
quality patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17715">
<div class="article-summary-box-inner">
<span><p>Representations from large language models (LLMs) are known to be dominated
by a small subset of dimensions with exceedingly high variance. Previous works
have argued that although ablating these outlier dimensions in LLM
representations hurts downstream performance, outlier dimensions are
detrimental to the representational quality of embeddings. In this study, we
investigate how fine-tuning impacts outlier dimensions and show that 1) outlier
dimensions that occur in pre-training persist in fine-tuned models and 2) a
single outlier dimension can complete downstream tasks with a minimal error
rate. Our results suggest that outlier dimensions can encode crucial
task-specific knowledge and that the value of a representation in a single
outlier dimension drives downstream model decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI. (arXiv:2310.17721v1 [econ.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17721">
<div class="article-summary-box-inner">
<span><p>We explore the value of generative AI tools, such as ChatGPT, in helping
investors uncover dimensions of corporate risk. We develop and validate
firm-level measures of risk exposure to political, climate, and AI-related
risks. Using the GPT 3.5 model to generate risk summaries and assessments from
the context provided by earnings call transcripts, we show that GPT-based
measures possess significant information content and outperform the existing
risk measures in predicting (abnormal) firm-level volatility and firms' choices
such as investment and innovation. Importantly, information in risk assessments
dominates that in risk summaries, establishing the value of general AI
knowledge. We also find that generative AI is effective at detecting emerging
risks, such as AI risk, which has soared in recent quarters. Our measures
perform well both within and outside the GPT's training window and are priced
in equity markets. Taken together, an AI-based approach to risk measurement
provides useful insights to users of corporate disclosures at a low cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17722">
<div class="article-summary-box-inner">
<span><p>We show that large language models (LLMs) can be adapted to be generalizable
policies for embodied visual tasks. Our approach, called Large LAnguage model
Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take
as input text instructions and visual egocentric observations and output
actions directly in the environment. Using reinforcement learning, we train
LLaRP to see and act solely through environmental interactions. We show that
LLaRP is robust to complex paraphrasings of task instructions and can
generalize to new tasks that require novel optimal behavior. In particular, on
1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other
common learned baselines or zero-shot applications of LLMs. Finally, to aid the
community in studying language conditioned, massively multi-task, embodied AI
problems we release a novel benchmark, Language Rearrangement, consisting of
150,000 training and 1,000 testing tasks for language-conditioned
rearrangement. Video examples of LLaRP in unseen Language Rearrangement
instructions are at https://llm-rl.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17723">
<div class="article-summary-box-inner">
<span><p>Quantization techniques are pivotal in reducing the memory and computational
demands of deep neural network inference. Existing solutions, such as
ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook
crucial memory-bounded operators and the complexities of per-token
quantization. Addressing these gaps, we present a novel, fully
hardware-enhanced robust optimized post-training W8A8 quantization framework,
ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and
compute-intensive operators, aiming for optimal hardware performance.
Additionally, it offers flexibility by allowing specific INT8 modules to switch
to FP16/BF16 mode, enhancing accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Multilingual Coreference Resolution by Universal Annotations. (arXiv:2310.17734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17734">
<div class="article-summary-box-inner">
<span><p>Multilingual coreference resolution (MCR) has been a long-standing and
challenging task. With the newly proposed multilingual coreference dataset,
CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by
using its harmonized universal morphosyntactic and coreference annotations.
First, we study coreference by examining the ground truth data at different
linguistic levels, namely mention, entity and document levels, and across
different genres, to gain insights into the characteristics of coreference
across multiple languages. Second, we perform an error analysis of the most
challenging cases that the SotA system fails to resolve in the CRAC 2022 shared
task using the universal annotations. Last, based on this analysis, we extract
features from universal morphosyntactic annotations and integrate these
features into a baseline system to assess their potential benefits for the MCR
task. Our results show that our best configuration of features improves the
baseline by 0.9% F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages. (arXiv:2310.17737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17737">
<div class="article-summary-box-inner">
<span><p>Building multi-modal language models has been a trend in the recent years,
where additional modalities such as image, video, speech, etc. are jointly
learned along with natural languages (i.e., textual information). Despite the
success of these multi-modal language models with different modalities, there
is no existing solution for neural network architectures and natural languages.
Providing neural architectural information as a new modality allows us to
provide fast architecture-2-text and text-2-architecture retrieval/generation
services on the cloud with a single inference. Such solution is valuable in
terms of helping beginner and intermediate ML users to come up with better
neural architectures or AutoML approaches with a simple text query. In this
paper, we propose ArchBERT, a bi-modal model for joint learning and
understanding of neural architectures and natural languages, which opens up new
avenues for research in this area. We also introduce a pre-training strategy
named Masked Architecture Modeling (MAM) for a more generalized joint learning.
Moreover, we introduce and publicly release two new bi-modal datasets for
training and validating our methods. The ArchBERT's performance is verified
through a set of numerical experiments on different downstream tasks such as
architecture-oriented reasoning, question answering, and captioning
(summarization). Datasets, codes, and demos are available supplementary
materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17743">
<div class="article-summary-box-inner">
<span><p>Stylistic headline generation is the task to generate a headline that not
only summarizes the content of an article, but also reflects a desired style
that attracts users. As style-specific article-headline pairs are scarce,
previous researches focus on unsupervised approaches with a standard headline
generation dataset and mono-style corpora. In this work, we follow this line
and propose StyleBART, an unsupervised approach for stylistic headline
generation. Our method decorates the pretrained BART model with adapters that
are responsible for different styles and allows the generation of headlines
with diverse styles by simply switching the adapters. Different from previous
works, StyleBART separates the task of style learning and headline generation,
making it possible to freely combine the base model and the style adapters
during inference. We further propose an inverse paraphrasing task to enhance
the style adapters. Extensive automatic and human evaluations show that
StyleBART achieves new state-of-the-art performance in the unsupervised
stylistic headline generation task, producing high-quality headlines with the
desired style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems. (arXiv:2310.17749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17749">
<div class="article-summary-box-inner">
<span><p>Making big purchases requires consumers to research or consult a salesperson
to gain domain expertise. However, existing conversational recommender systems
(CRS) often overlook users' lack of background knowledge, focusing solely on
gathering preferences. In this work, we define a new problem space for
conversational agents that aim to provide both product recommendations and
educational value through mixed-type mixed-initiative dialog. We introduce
SalesOps, a framework that facilitates the simulation and evaluation of such
systems by leveraging recent advancements in large language models (LLMs). We
build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate
either side of the framework. A comprehensive human study compares SalesBot
against professional salespeople, revealing that although SalesBot approaches
professional performance in terms of fluency and informativeness, it lags
behind in recommendation quality. We emphasize the distinct limitations both
face in providing truthful information, highlighting the challenges of ensuring
faithfulness in the CRS context. We release our code and make all data
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications. (arXiv:2310.17750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17750">
<div class="article-summary-box-inner">
<span><p>We present a framework for the automated measurement of responsible AI (RAI)
metrics for large language models (LLMs) and associated products and services.
Our framework for automatically measuring harms from LLMs builds on existing
technical and sociotechnical expertise and leverages the capabilities of
state-of-the-art LLMs, such as GPT-4. We use this framework to run through
several case studies investigating how different LLMs may violate a range of
RAI-related principles. The framework may be employed alongside domain-specific
sociotechnical expertise to create measurements for new harm areas in the
future. By implementing this framework, we aim to enable more advanced harm
measurement efforts and further the responsible use of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17769">
<div class="article-summary-box-inner">
<span><p>We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17770">
<div class="article-summary-box-inner">
<span><p>A proper evaluation of stories generated for a sequence of images -- the task
commonly referred to as visual storytelling -- must consider multiple aspects,
such as coherence, grammatical correctness, and visual grounding. In this work,
we focus on evaluating the degree of grounding, that is, the extent to which a
story is about the entities shown in the images. We analyze current metrics,
both designed for this purpose and for general vision-text alignment. Given
their observed shortcomings, we propose a novel evaluation tool, GROOViST, that
accounts for cross-modal dependencies, temporal misalignments (the fact that
the order in which entities appear in the story and the image sequence may not
match), and human intuitions on visual grounding. An additional advantage of
GROOViST is its modular design, where the contribution of each component can be
assessed and interpreted individually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?. (arXiv:2310.17774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17774">
<div class="article-summary-box-inner">
<span><p>An important assumption that comes with using LLMs on psycholinguistic data
has gone unverified. LLM-based predictions are based on subword tokenization,
not decomposition of words into morphemes. Does that matter? We carefully test
this by comparing surprisal estimates using orthographic, morphological, and
BPE tokenization against reading time data. Our results replicate previous
findings and provide evidence that in the aggregate, predictions using BPE
tokenization do not suffer relative to morphological and orthographic
segmentation. However, a finer-grained analysis points to potential issues with
relying on BPE-based tokenization, as well as providing promising results
involving morphologically-aware surprisal estimates and suggesting a new method
for evaluating morphological prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17784">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) show promise for natural language tasks but
struggle when applied directly to complex domains like finance. LLMs have
difficulty reasoning about and integrating all relevant information. We propose
a data-centric approach to enable LLMs to better handle financial tasks. Our
key insight is that rather than overloading the LLM with everything at once, it
is more effective to preprocess and pre-understand the data. We create a
financial LLM (FLLM) using multitask prompt-based finetuning to achieve data
pre-processing and pre-understanding. However, labeled data is scarce for each
task. To overcome manual annotation costs, we employ abductive augmentation
reasoning (AAR) to automatically generate training data by modifying the pseudo
labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR
substantially outperforms baseline financial LLMs designed for raw text,
achieving state-of-the-art on financial analysis and interpretation tasks. We
also open source a new benchmark for financial analysis and interpretation. Our
methodology provides a promising path to unlock LLMs' potential for complex
real-world domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17787">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are typically evaluated on the basis of
task-based benchmarks such as MMLU. Such benchmarks do not examine responsible
behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+
context where social stereotypes may result in variation in LGBTI+ terminology.
Therefore, domain-specific lexicons or dictionaries may be useful as a
representative list of words against which the LLM's behaviour needs to be
evaluated. This paper presents a methodology for evaluation of LLMs using an
LGBTI+ lexicon in Indian languages. The methodology consists of four steps:
formulating NLP tasks relevant to the expected behaviour, creating prompts that
test LLMs, using the LLMs to obtain the output and, finally, manually
evaluating the results. Our qualitative analysis shows that the three LLMs we
experiment on are unable to detect underlying hateful content. Similarly, we
observe limitations in using machine translation as means to evaluate natural
language understanding in languages other than English. The methodology
presented in this paper can be useful for LGBTI+ lexicons in other languages as
well as other domain-specific lexicons. The work done in this paper opens
avenues for responsible behaviour of LLMs, as demonstrated in the context of
prevalent social perception of the LGBTI+ community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Language Models for Energy Load Forecasting. (arXiv:2310.17788v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17788">
<div class="article-summary-box-inner">
<span><p>Energy load forecasting plays a crucial role in optimizing resource
allocation and managing energy consumption in buildings and cities. In this
paper, we propose a novel approach that leverages language models for energy
load forecasting. We employ prompting techniques to convert energy consumption
data into descriptive sentences, enabling fine-tuning of language models. By
adopting an autoregressive generating approach, our proposed method enables
predictions of various horizons of future energy load consumption. Through
extensive experiments on real-world datasets, we demonstrate the effectiveness
and accuracy of our proposed method. Our results indicate that utilizing
language models for energy load forecasting holds promise for enhancing energy
efficiency and facilitating intelligent decision-making in energy systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17793">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) show amazing proficiency and fluency in the use
of language. Does this mean that they have also acquired insightful linguistic
knowledge about the language, to an extent that they can serve as an "expert
linguistic annotator"? In this paper, we examine the successes and limitations
of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning
structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et
al. 2013) parsing formalism, which provides rich graphical representations of
sentence meaning structure while abstracting away from surface forms. We
compare models' analysis of this semantic structure across two settings: 1)
direct production of AMR parses based on zero- and few-shot prompts, and 2)
indirect partial reconstruction of AMR via metalinguistic natural language
queries (e.g., "Identify the primary event of this sentence, and the predicate
corresponding to that event."). Across these settings, we find that models can
reliably reproduce the basic format of AMR, and can often capture core event,
argument, and modifier structure -- however, model outputs are prone to
frequent and major errors, and holistic analysis of parse acceptability shows
that even with few-shot demonstrations, models have virtually 0% success in
producing fully accurate parses. Eliciting natural language responses produces
similar patterns of errors. Overall, our findings indicate that these models
out-of-the-box can capture aspects of semantic structure, but there remain key
limitations in their ability to support fully accurate semantic analyses or
parses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles. (arXiv:2310.17802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17802">
<div class="article-summary-box-inner">
<span><p>Temporal relation extraction models have thus far been hindered by a number
of issues in existing temporal relation-annotated news datasets, including: (1)
low inter-annotator agreement due to the lack of specificity of their
annotation guidelines in terms of what counts as a temporal relation; (2) the
exclusion of long-distance relations within a given document (those spanning
across different paragraphs); and (3) the exclusion of events that are not
centred on verbs. This paper aims to alleviate these issues by presenting a new
annotation scheme that clearly defines the criteria based on which temporal
relations should be annotated. Additionally, the scheme includes events even if
they are not expressed as verbs (e.g., nominalised events). Furthermore, we
propose a method for annotating all temporal relations -- including
long-distance ones -- which automates the process, hence reducing time and
manual effort on the part of annotators. The result is a new dataset, the
TIMELINE corpus, in which improved inter-annotator agreement was obtained, in
comparison with previously reported temporal relation datasets. We report the
results of training and evaluating baseline temporal relation extraction models
on the new corpus, and compare them with results obtained on the widely used
MATRES corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17811">
<div class="article-summary-box-inner">
<span><p>Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models. (arXiv:2310.17857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17857">
<div class="article-summary-box-inner">
<span><p>Being able to predict people's opinions on issues and behaviors in realistic
scenarios can be helpful in various domains, such as politics and marketing.
However, conducting large-scale surveys like the European Social Survey to
solicit people's opinions on individual issues can incur prohibitive costs.
Leveraging prior research showing influence of core human values on individual
decisions and actions, we propose to use value-injected large language models
(LLM) to predict opinions and behaviors. To this end, we present Value
Injection Method (VIM), a collection of two methods -- argument generation and
question answering -- designed to inject targeted value distributions into LLMs
via fine-tuning. We then conduct a series of experiments on four tasks to test
the effectiveness of VIM and the possibility of using value-injected LLMs to
predict opinions and behaviors of people. We find that LLMs value-injected with
variations of VIM substantially outperform the baselines. Also, the results
suggest that opinions and behaviors can be better predicted using
value-injected LLMs than the baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17876">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of large language models (LLMs) has sparked interest in
data synthesis techniques, aiming to generate diverse and high-quality
synthetic datasets. However, these synthetic datasets often suffer from a lack
of diversity and added noise. In this paper, we present TarGEN, a multi-step
prompting strategy for generating high-quality synthetic datasets utilizing a
LLM. An advantage of TarGEN is its seedless nature; it does not require
specific task instances, broadening its applicability beyond task replication.
We augment TarGEN with a method known as self-correction empowering LLMs to
rectify inaccurately labeled instances during dataset creation, ensuring
reliable labels. To assess our technique's effectiveness, we emulate 8 tasks
from the SuperGLUE benchmark and finetune various language models, including
encoder-only, encoder-decoder, and decoder-only models on both synthetic and
original training sets. Evaluation on the original test set reveals that models
trained on datasets generated by TarGEN perform approximately 1-2% points
better than those trained on original datasets (82.84% via syn. vs. 81.12% on
og. using Flan-T5). When incorporating instruction tuning, the performance
increases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A
comprehensive analysis of the synthetic dataset compared to the original
dataset reveals that the synthetic dataset demonstrates similar or higher
levels of dataset complexity and diversity. Furthermore, the synthetic dataset
displays a bias level that aligns closely with the original dataset. Finally,
when pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive
results on the OpenLLM leaderboard, surpassing the model trained on the
Self-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for
quality data generation and reducing the human efforts to create complex
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17877">
<div class="article-summary-box-inner">
<span><p>We present ASPIRO, an approach for structured data verbalisation into short
template sentences in zero to few-shot settings. Unlike previous methods, our
approach prompts large language models (LLMs) to directly produce
entity-agnostic templates, rather than relying on LLMs to faithfully copy the
given example entities, or validating/crafting the templates manually. We
incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well
as the PARENT metric induced consistency validation to identify and rectify
template generation problems in real-time. ASPIRO, compared to direct LLM
output, averages 66\% parsing error rate reduction in generated verbalisations
of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup,
scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and
PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent
fine-tuned pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17884">
<div class="article-summary-box-inner">
<span><p>The interactive use of large language models (LLMs) in AI assistants (at
work, home, etc.) introduces a new set of inference-time privacy risks: LLMs
are fed different types of information from multiple sources in their inputs
and are expected to reason about what to share in their outputs, for what
purpose and with whom, within a given context. In this work, we draw attention
to the highly critical yet overlooked notion of contextual privacy by proposing
ConfAIde, a benchmark designed to identify critical weaknesses in the privacy
reasoning capabilities of instruction-tuned LLMs. Our experiments show that
even the most capable models such as GPT-4 and ChatGPT reveal private
information in contexts that humans would not, 39% and 57% of the time,
respectively. This leakage persists even when we employ privacy-inducing
prompts or chain-of-thought reasoning. Our work underscores the immediate need
to explore novel inference-time privacy-preserving approaches, based on
reasoning and theory of mind.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17894">
<div class="article-summary-box-inner">
<span><p>The emergence of natural language processing has revolutionized the way users
interact with tabular data, enabling a shift from traditional query languages
and manual plotting to more intuitive, language-based interfaces. The rise of
large language models (LLMs) such as ChatGPT and its successors has further
advanced this field, opening new avenues for natural language processing
techniques. This survey presents a comprehensive overview of natural language
interfaces for tabular data querying and visualization, which allow users to
interact with data using natural language queries. We introduce the fundamental
concepts and techniques underlying these interfaces with a particular emphasis
on semantic parsing, the key technology facilitating the translation from
natural language to SQL queries or data visualization commands. We then delve
into the recent advancements in Text-to-SQL and Text-to-Vis problems from the
perspectives of datasets, methodologies, metrics, and system designs. This
includes a deep dive into the influence of LLMs, highlighting their strengths,
limitations, and potential for future improvements. Through this survey, we aim
to provide a roadmap for researchers and practitioners interested in developing
and applying natural language interfaces for data interaction in the era of
large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Aware Visual Question Answering about Parts, Poses and Occlusions. (arXiv:2310.17914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17914">
<div class="article-summary-box-inner">
<span><p>Despite rapid progress in Visual question answering (VQA), existing datasets
and models mainly focus on testing reasoning in 2D. However, it is important
that VQA models also understand the 3D structure of visual scenes, for example
to support tasks like navigation or manipulation. This includes an
understanding of the 3D object pose, their parts and occlusions. In this work,
we introduce the task of 3D-aware VQA, which focuses on challenging questions
that require a compositional reasoning over the 3D structure of visual scenes.
We address 3D-aware VQA from both the dataset and the model perspective. First,
we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains
questions about object parts, their 3D poses, and occlusions. Second, we
propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas:
probabilistic neural symbolic program execution for reasoning and deep neural
networks with 3D generative representations of objects for robust visual
recognition. Our experimental results show our model PO3D-VQA outperforms
existing methods significantly, but we still observe a significant performance
gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an
important open research area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17918">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOUL: Towards Sentiment and Opinion Understanding of Language. (arXiv:2310.17924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17924">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a well-established natural language processing task,
with sentiment polarity classification being one of its most popular and
representative tasks. However, despite the success of pre-trained language
models in this area, they often fall short of capturing the broader
complexities of sentiment analysis. To address this issue, we propose a new
task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims
to evaluate sentiment understanding through two subtasks: Review Comprehension
(RC) and Justification Generation (JG). RC seeks to validate statements that
focus on subjective information based on a review text, while JG requires
models to provide explanations for their sentiment predictions. To enable
comprehensive evaluation, we annotate a new dataset comprising 15,028
statements from 3,638 reviews. Experimental results indicate that SOUL is a
challenging task for both small and large language models, with a performance
gap of up to 27% when compared to human performance. Furthermore, evaluations
conducted with both human experts and GPT-4 highlight the limitations of the
small language model in generating reasoning-based justifications. These
findings underscore the challenging nature of the SOUL task for existing
models, emphasizing the need for further advancements in sentiment analysis to
address its complexities. The new dataset and code are available at
https://github.com/DAMO-NLP-SG/SOUL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17936">
<div class="article-summary-box-inner">
<span><p>We argue that Transformers are essentially graph-to-graph models, with
sequences just being a special case. Attention weights are functionally
equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes
this ability explicit, by inputting graph edges into the attention weight
computations and predicting graph edges with attention-like functions, thereby
integrating explicit graphs into the latent graphs learned by pretrained
Transformers. Adding iterative graph refinement provides a joint embedding of
input, output, and latent graphs, allowing non-autoregressive graph prediction
to optimise the complete graph without any bespoke pipeline or decoding
strategy. Empirical results show that this architecture achieves
state-of-the-art accuracies for modelling a variety of linguistic structures,
integrating very effectively with the latent linguistic representations learned
by pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17940">
<div class="article-summary-box-inner">
<span><p>Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model's capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17953">
<div class="article-summary-box-inner">
<span><p>Recently Whisper has approached human-level robustness and accuracy in
English automatic speech recognition (ASR), while in minor language and mixed
language speech recognition, there remains a compelling need for further
improvement. In this work, we present the impressive results of Whisper-MCE,
our finetuned Whisper model, which was trained using our self-collected
dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile,
considering word error rate (WER) poses challenges when it comes to evaluating
its effectiveness in minor language and mixed-language contexts, we present a
novel rating mechanism. By comparing our model to the baseline whisper-large-v2
model, we demonstrate its superior ability to accurately capture the content of
the original audio, achieve higher recognition accuracy, and exhibit faster
recognition speed. Notably, our model outperforms other existing models in the
specific task of recognizing mixed language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17956">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have introduced a new era of proficiency in
comprehending complex healthcare and biomedical topics. However, there is a
noticeable lack of models in languages other than English and models that can
interpret multi-modal input, which is crucial for global healthcare
accessibility. In response, this study introduces Qilin-Med-VL, the first
Chinese large vision-language model designed to integrate the analysis of
textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer
(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum
training process that includes feature alignment and instruction tuning. This
method enhances the model's ability to generate medical captions and answer
complex medical queries. We also release ChiMed-VL, a dataset consisting of
more than 1M image-text pairs. This dataset has been carefully curated to
enable detailed and comprehensive interpretation of medical data using various
types of images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17976">
<div class="article-summary-box-inner">
<span><p>The emergence of large-scale pretrained language models has revolutionized
the capabilities of new AI application, especially in the realm of crafting
chatbots with distinct personas. Given the "stimulus-response" nature of
chatbots, this paper unveils an innovative open-ended interview-style approach
for personality assessment on role-playing chatbots, which offers a richer
comprehension of their intrinsic personalities. We conduct personality
assessments on 32 role-playing chatbots created by the ChatHaruhi library,
across both the Big Five and MBTI dimensions, and measure their alignment with
human perception. Evaluation results underscore that modern role-playing
chatbots based on LLMs can effectively portray personality traits of
corresponding characters, with an alignment rate of 82.8% compared with
human-perceived personalities. Besides, we also suggest potential strategies
for shaping chatbots' personalities. Hence, this paper serves as a cornerstone
study for role-playing chatbots that intersects computational linguistics and
psychology. Our resources are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. (arXiv:2310.18018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18018">
<div class="article-summary-box-inner">
<span><p>In this position paper, we argue that the classical evaluation on Natural
Language Processing (NLP) tasks using annotated benchmarks is in trouble. The
worst kind of data contamination happens when a Large Language Model (LLM) is
trained on the test split of a benchmark, and then evaluated in the same
benchmark. The extent of the problem is unknown, as it is not straightforward
to measure. Contamination causes an overestimation of the performance of a
contaminated model in a target benchmark and associated task with respect to
their non-contaminated counterparts. The consequences can be very harmful, with
wrong scientific conclusions being published while other correct ones are
discarded. This position paper defines different levels of data contamination
and argues for a community effort, including the development of automatic and
semi-automatic measures to detect when data from a benchmark was exposed to a
model, and suggestions for flagging papers with conclusions that are
compromised by data contamination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18023">
<div class="article-summary-box-inner">
<span><p>Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several datasets have been build with
the goal of training computational models for code-mixing. Although it is very
common to observe code-mixing with multiple languages, most datasets available
contain code-mixed between only two languages. In this paper, we introduce
SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data
between three languages Bangla, English, and Hindi. We carry out a
comprehensive evaluation using SentMix-3L. We show that zero-shot prompting
with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models for aspect-based sentiment analysis. (arXiv:2310.18025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18025">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) offer unprecedented text completion
capabilities. As general models, they can fulfill a wide range of roles,
including those of more specialized models. We assess the performance of GPT-4
and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based
sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art
F1 score of 83.8 on the joint aspect term extraction and polarity
classification task of the SemEval-2014 Task 4, improving upon InstructABSA
[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000
times more model parameters and thus increased inference cost. We discuss the
the cost-performance trade-offs of different models, and analyze the typical
errors that they make. Our results also indicate that detailed prompts improve
performance in zero-shot and few-shot settings but are not necessary for
fine-tuned models. This evidence is relevant for practioners that are faced
with the choice of prompt engineering versus fine-tuning when using LLMs for
ABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On General Language Understanding. (arXiv:2310.18038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18038">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing prides itself to be an empirically-minded, if not
outright empiricist field, and yet lately it seems to get itself into
essentialist debates on issues of meaning and measurement ("Do Large Language
Models Understand Language, And If So, How Much?"). This is not by accident:
Here, as everywhere, the evidence underspecifies the understanding. As a
remedy, this paper sketches the outlines of a model of understanding, which can
ground questions of the adequacy of current methods of measurement of model
quality. The paper makes three claims: A) That different language use situation
types have different characteristics, B) That language understanding is a
multifaceted phenomenon, bringing together individualistic and social
processes, and C) That the choice of Understanding Indicator marks the limits
of benchmarking, and the beginnings of considerations of the ethics of NLP use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese. (arXiv:2310.18046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18046">
<div class="article-summary-box-inner">
<span><p>In recent years, Visual Question Answering (VQA) has gained significant
attention for its diverse applications, including intelligent car assistance,
aiding visually impaired individuals, and document image information retrieval
using natural language queries. VQA requires effective integration of
information from questions and images to generate accurate answers. Neural
models for VQA have made remarkable progress on large-scale datasets, with a
primary focus on resource-rich languages like English. To address this, we
introduce the ViCLEVR dataset, a pioneering collection for evaluating various
visual reasoning capabilities in Vietnamese while mitigating biases. The
dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs),
each question annotated to specify the type of reasoning involved. Leveraging
this dataset, we conduct a comprehensive analysis of contemporary visual
reasoning systems, offering valuable insights into their strengths and
limitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion
that identifies objects in images based on questions. The architecture
effectively employs transformers to enable simultaneous reasoning over textual
and visual data, merging both modalities at an early model stage. The
experimental findings demonstrate that our proposed model achieves
state-of-the-art performance across four evaluation metrics. The accompanying
code and dataset have been made publicly accessible at
\url{https://github.com/kvt0012/ViCLEVR}. This provision seeks to stimulate
advancements within the research community, fostering the development of more
multimodal fusion algorithms, specifically tailored to address the nuances of
low-resource languages, exemplified by Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation. (arXiv:2310.18063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18063">
<div class="article-summary-box-inner">
<span><p>The ubiquity of complex machine learning has raised the importance of
model-agnostic explanation algorithms. These methods create artificial
instances by slightly perturbing real instances, capturing shifts in model
decisions. However, such methods rely on initial data and only provide
explanations of the decision for these. To tackle these problems, we propose
Therapy, the first global and model-agnostic explanation method adapted to text
which requires no input dataset. Therapy generates texts following the
distribution learned by a classifier through cooperative generation. Because it
does not rely on initial samples, it allows to generate explanations even when
data is absent (e.g., for confidentiality reasons). Moreover, conversely to
existing methods that combine multiple local explanations into a global one,
Therapy offers a global overview of the model behavior on the input space. Our
experiments show that although using no input data to generate samples, Therapy
provides insightful information about features used by the classifier that is
competitive with the ones from methods relying on input samples and outperforms
them when input samples are not specific to the studied model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-grained Evidence Inference for Multi-choice Reading Comprehension. (arXiv:2310.18070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18070">
<div class="article-summary-box-inner">
<span><p>Multi-choice Machine Reading Comprehension (MRC) is a major and challenging
task for machines to answer questions according to provided options. Answers in
multi-choice MRC cannot be directly extracted in the given passages, and
essentially require machines capable of reasoning from accurate extracted
evidence. However, the critical evidence may be as simple as just one word or
phrase, while it is hidden in the given redundant, noisy passage with multiple
linguistic hierarchies from phrase, fragment, sentence until the entire
passage. We thus propose a novel general-purpose model enhancement which
integrates multi-grained evidence comprehensively, named Multi-grained evidence
inferencer (Mugen), to make up for the inability. Mugen extracts three
different granularities of evidence: coarse-, middle- and fine-grained
evidence, and integrates evidence with the original passages, achieving
significant and consistent performance improvement on four multi-choice MRC
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports. (arXiv:2310.18073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18073">
<div class="article-summary-box-inner">
<span><p>Table of contents (ToC) extraction centres on structuring documents in a
hierarchical manner. In this paper, we propose a new dataset, ESGDoc,
comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to
2022. These reports pose significant challenges due to their diverse structures
and extensive length. To address these challenges, we propose a new framework
for Toc extraction, consisting of three steps: (1) Constructing an initial tree
of text blocks based on reading order and font sizes; (2) Modelling each tree
node (or text block) independently by considering its contextual information
captured in node-centric subtree; (3) Modifying the original tree by taking
appropriate action on each tree node (Keep, Delete, or Move). This
construction-modelling-modification (CMM) process offers several benefits. It
eliminates the need for pairwise modelling of section headings as in previous
approaches, making document segmentation practically feasible. By incorporating
structured information, each section heading can leverage both local and
long-distance context relevant to itself. Experimental results show that our
approach outperforms the previous state-of-the-art baseline with a fraction of
running time. Our framework proves its scalability by effectively handling
documents of any length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18075">
<div class="article-summary-box-inner">
<span><p>Inspired by the dual-process theory of human cognition, we introduce DUMA, a
novel conversational agent framework that embodies a dual-mind mechanism
through the utilization of two generative Large Language Models (LLMs)
dedicated to fast and slow thinking respectively. The fast thinking model
serves as the primary interface for external interactions and initial response
generation, evaluating the necessity for engaging the slow thinking model based
on the complexity of the complete response. When invoked, the slow thinking
model takes over the conversation, engaging in meticulous planning, reasoning,
and tool utilization to provide a well-analyzed response. This dual-mind
configuration allows for a seamless transition between intuitive responses and
deliberate problem-solving processes based on the situation. We have
constructed a conversational agent to handle online inquiries in the real
estate industry. The experiment proves that our method balances effectiveness
and efficiency, and has a significant improvement compared to the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18076">
<div class="article-summary-box-inner">
<span><p>Recent works in open-domain question answering (QA) have explored generating
context passages from large language models (LLMs), replacing the traditional
retrieval step in the QA pipeline. However, it is not well understood why
generated passages can be more effective than retrieved ones. This study
revisits the conventional formulation of QA and introduces the concept of
knowledge corpus error. This error arises when the knowledge corpus used for
retrieval is only a subset of the entire string space, potentially excluding
more helpful passages that exist outside the corpus. LLMs may mitigate this
shortcoming by generating passages in a larger space. We come up with an
experiment of paraphrasing human-annotated gold context using LLMs to observe
knowledge corpus error empirically. Our results across three QA benchmarks
reveal an increased performance (10% - 13%) when using paraphrased passage,
indicating a signal for the existence of knowledge corpus error. Our code is
available at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detrimental Contexts in Open-Domain Question Answering. (arXiv:2310.18077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18077">
<div class="article-summary-box-inner">
<span><p>For knowledge intensive NLP tasks, it has been widely accepted that accessing
more information is a contributing factor to improvements in the model's
end-to-end performance. However, counter-intuitively, too much context can have
a negative impact on the model when evaluated on common question answering (QA)
datasets. In this paper, we analyze how passages can have a detrimental effect
on retrieve-then-read architectures used in question answering. Our empirical
evidence indicates that the current read architecture does not fully leverage
the retrieved passages and significantly degrades its performance when using
the whole passages compared to utilizing subsets of them. Our findings
demonstrate that model accuracy can be improved by 10% on two popular QA
datasets by filtering out detrimental passages. Additionally, these outcomes
are attained by utilizing existing retrieval methods without further training
or data. We further highlight the challenges associated with identifying the
detrimental passages. First, even with the correct context, the model can make
an incorrect prediction, posing a challenge in determining which passages are
most influential. Second, evaluation typically considers lexical matching,
which is not robust to variations of correct answers. Despite these
limitations, our experimental results underscore the pivotal role of
identifying and removing these detrimental passages for the context-efficient
retrieve-then-read pipeline. Code and data are available at
https://github.com/xfactlab/emnlp2023-damaging-retrieval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in Translation -- Multilingual Misinformation and its Evolution. (arXiv:2310.18089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18089">
<div class="article-summary-box-inner">
<span><p>Misinformation and disinformation are growing threats in the digital age,
spreading rapidly across languages and borders. This paper investigates the
prevalence and dynamics of multilingual misinformation through an analysis of
over 250,000 unique fact-checks spanning 95 languages. First, we find that
while the majority of misinformation claims are only fact-checked once, 11.7%,
corresponding to more than 21,000 claims, are checked multiple times. Using
fact-checks as a proxy for the spread of misinformation, we find 33% of
repeated claims cross linguistic boundaries, suggesting that some
misinformation permeates language barriers. However, spreading patterns exhibit
strong homophily, with misinformation more likely to spread within the same
language. To study the evolution of claims over time and mutations across
languages, we represent fact-checks with multilingual sentence embeddings and
cluster semantically similar claims. We analyze the connected components and
shortest paths connecting different versions of a claim finding that claims
gradually drift over time and undergo greater alteration when traversing
languages. Overall, this novel investigation of multilingual misinformation
provides key insights. It quantifies redundant fact-checking efforts,
establishes that some claims diffuse across languages, measures linguistic
homophily, and models the temporal and cross-lingual evolution of claims. The
findings advocate for expanded information sharing between fact-checkers
globally while underscoring the importance of localized verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments. (arXiv:2310.18098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18098">
<div class="article-summary-box-inner">
<span><p>Writing strong arguments can be challenging for learners. It requires to
select and arrange multiple argumentative discourse units (ADUs) in a logical
and coherent way as well as to decide which ADUs to leave implicit, so called
enthymemes. However, when important ADUs are missing, readers might not be able
to follow the reasoning or understand the argument's main point. This paper
introduces two new tasks for learner arguments: to identify gaps in arguments
(enthymeme detection) and to fill such gaps (enthymeme reconstruction).
Approaches to both tasks may help learners improve their argument quality. We
study how corpora for these tasks can be created automatically by deleting ADUs
from an argumentative text that are central to the argument and its quality,
while maintaining the text's naturalness. Based on the ICLEv3 corpus of
argumentative learner essays, we create 40,089 argument instances for enthymeme
detection and reconstruction. Through manual studies, we provide evidence that
the proposed corpus creation process leads to the desired quality reduction,
and results in arguments that are similarly natural to those written by
learners. Finally, first baseline approaches to enthymeme detection and
reconstruction demonstrate the corpus' usefulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18119">
<div class="article-summary-box-inner">
<span><p>In Conversational Recommendation System (CRS), an agent is asked to recommend
a set of items to users within natural language conversations. To address the
need for both conversational capability and personalized recommendations, prior
works have utilized separate recommendation and dialogue modules. However, such
approach inevitably results in a discrepancy between recommendation results and
generated responses. To bridge the gap, we propose a multi-task learning for a
unified CRS, where a single model jointly learns both tasks via Contextualized
Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate
and soft gate. The former selectively gates between two task-specific teachers,
while the latter integrates knowledge from both teachers. Our gates are
computed on-the-fly in a context-specific manner, facilitating flexible
integration of relevant knowledge. Extensive experiments demonstrate that our
single model significantly improves recommendation performance while enhancing
fluency, and achieves comparable results in terms of diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18122">
<div class="article-summary-box-inner">
<span><p>Opinion summarization sets itself apart from other types of summarization
tasks due to its distinctive focus on aspects and sentiments. Although certain
automated evaluation methods like ROUGE have gained popularity, we have found
them to be unreliable measures for assessing the quality of opinion summaries.
In this paper, we present OpinSummEval, a dataset comprising human judgments
and outputs from 14 opinion summarization models. We further explore the
correlation between 24 automatic metrics and human ratings across four
dimensions. Our findings indicate that metrics based on neural networks
generally outperform non-neural ones. However, even metrics built on powerful
backbones, such as BART and GPT-3/3.5, do not consistently correlate well
across all dimensions, highlighting the need for advancements in automated
evaluation methods for opinion summarization. The code and data are publicly
available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18127">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) demonstrate their promise in tackling
complicated practical challenges by combining action-based policies with chain
of thought (CoT) reasoning. Having high-quality prompts on hand, however, is
vital to the framework's effectiveness. Currently, these prompts are
handcrafted utilizing extensive human labor, resulting in CoT policies that
frequently fail to generalize. Human intervention is also required in order to
develop grounding functions that ensure low-level controllers appropriately
process CoT reasoning. In this paper, we take the first step towards a fully
integrated end-to-end framework for task-solving in real settings employing
complicated reasoning. To that purpose, we offer a new leader-follower bilevel
framework capable of learning to ask relevant questions (prompts) and
subsequently undertaking reasoning to guide the learning of actions to be
performed in an environment. A good prompt should make introspective revisions
based on historical findings, leading the CoT to consider the anticipated
goals. A prompt-generator policy has its own aim in our system, allowing it to
adapt to the action policy and automatically root the CoT process towards
outputs that lead to decisive, high-performing actions. Meanwhile, the action
policy is learning how to use the CoT outputs to take specific actions. Our
empirical data reveal that our system outperforms leading methods in agent
learning benchmarks such as Overcooked and FourRoom.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues. (arXiv:2310.18130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18130">
<div class="article-summary-box-inner">
<span><p>Controversy is a reflection of our zeitgeist, and an important aspect to any
discourse. The rise of large language models (LLMs) as conversational systems
has increased public reliance on these systems for answers to their various
questions. Consequently, it is crucial to systematically examine how these
models respond to questions that pertaining to ongoing debates. However, few
such datasets exist in providing human-annotated labels reflecting the
contemporary discussions. To foster research in this area, we propose a novel
construction of a controversial questions dataset, expanding upon the publicly
released Quora Question Pairs Dataset. This dataset presents challenges
concerning knowledge recency, safety, fairness, and bias. We evaluate different
LLMs using a subset of this dataset, illuminating how they handle controversial
issues and the stances they adopt. This research ultimately contributes to our
understanding of LLMs' interaction with controversial issues, paving the way
for improvements in their comprehension and handling of complex societal
debates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18152">
<div class="article-summary-box-inner">
<span><p>Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs
such as citation networks, e-commerce networks and social networks has
attracted considerable attention in the web community. Recently, large language
models (LLMs) have demonstrated exceptional capabilities across a wide range of
tasks. However, the existing works focus on harnessing the potential of LLMs
solely relying on prompts to convey graph structure information to LLMs, thus
suffering from insufficient understanding of the complex structural
relationships within TAGs. To address this problem, in this paper we present
the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the
reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model
incorporates graph structure information through tailored disentangled graph
neural network (GNN) layers, enabling LLMs to capture the intricate
relationships hidden in text-attributed graphs from multiple structural
factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing
computational costs and allowing much more flexibility in combining with
different LLM models. Experimental evaluations demonstrate the effectiveness of
the proposed DGTL model on achieving superior or comparable performance over
state-of-the-art baselines. Additionally, we also demonstrate that our DGTL
model can offer natural language explanations for predictions, thereby
significantly enhancing model interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elevating Code-mixed Text Handling through Auditory Information of Words. (arXiv:2310.18155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18155">
<div class="article-summary-box-inner">
<span><p>With the growing popularity of code-mixed data, there is an increasing need
for better handling of this type of data, which poses a number of challenges,
such as dealing with spelling variations, multiple languages, different
scripts, and a lack of resources. Current language models face difficulty in
effectively handling code-mixed data as they primarily focus on the semantic
representation of words and ignore the auditory phonetic features. This leads
to difficulties in handling spelling variations in code-mixed text. In this
paper, we propose an effective approach for creating language models for
handling code-mixed textual data using auditory information of words from
SOUNDEX. Our approach includes a pre-training step based on
masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a
new method of providing input data to the pre-trained model. Through
experimentation on various code-mixed datasets (of different languages) for
sentiment, offensive and aggression classification tasks, we establish that our
novel language modeling approach (SAMLM) results in improved robustness towards
adversarial attacks on code-mixed classification tasks. Additionally, our SAMLM
based approach also results in better classification results over the popular
baselines for code-mixed tasks. We use the explainability technique, SHAP
(SHapley Additive exPlanations) to explain how the auditory features
incorporated through SAMLM assist the model to handle the code-mixed text
effectively and increase robustness against adversarial attacks
\footnote{Source code has been made available on
\url{https://github.com/20118/DefenseWithPhonetics},
\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html\#Phonetics}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension. (arXiv:2310.18167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18167">
<div class="article-summary-box-inner">
<span><p>The large language models have achieved superior performance on various
natural language tasks. One major drawback of such approaches is they are
resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a
resource-efficient solution to fine-tune the pre-trained language models (PLMs)
while keeping their weight frozen. Existing soft prompt methods mainly focus on
designing the input-independent prompts that steer the model to fit the domain
of the new dataset. Those methods often ignore the fine-grained information
about the task and context of the text. In this paper, we propose a multi-level
prompt tuning (MPrompt) method for machine reading comprehension. It utilizes
prompts at task-specific, domain-specific, and context-specific levels to
enhance the comprehension of input semantics at different granularities. We
also propose an independence constraint to steer each domain-specific prompt to
focus on information within its domain to avoid redundancy. Moreover, we
present a prompt generator that incorporates context-related knowledge in the
prompt generation to enhance contextual relevancy. We conducted extensive
experiments on 12 benchmarks of various QA formats and achieved an average
improvement of 1.94\% over the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18168">
<div class="article-summary-box-inner">
<span><p>Large Language Models are trained on vast amounts of text from the internet,
which contains both factual and misleading information about the world. Can
language models discern truth from falsehood in this contradicting data?
Expanding on the view that LLMs can model different agents producing the
corpora, we hypothesize that they can cluster truthful text by modeling a
truthful persona: a group of agents that are likely to produce truthful text
and share similar features. For example, trustworthy sources like Wikipedia and
Science usually use formal writing styles and make consistent claims. By
modeling this persona, LLMs can generalize truthfulness beyond the specific
contexts in which each agent generated the training text. For example, the
model can infer that the agent "Wikipedia" will behave truthfully on topics
that were only generated by "Science" because they share a persona. We first
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model's answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN. (arXiv:2310.18169v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18169">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS)
to generate the corresponding high-fidelity speech based on the style
description and content text as an input to generate speech samples within only
4 denoising steps. It leverages the novel conditional prosodic layer
normalization to incorporate the style embeddings into the multi head attention
based phoneme encoder and mel spectrogram decoder based generator architecture
to generate the speech. The style embedding is generated by fine tuning the
pretrained BERT model on auxiliary tasks such as pitch, speaking speed,
emotion,gender classifications. We demonstrate the efficacy of our proposed
architecture on multi-speaker LibriTTS and PromptSpeech datasets, using
multiple quantitative metrics that measure generated accuracy and MOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media. (arXiv:2310.18205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18205">
<div class="article-summary-box-inner">
<span><p>Claim span identification (CSI) is an important step in fact-checking
pipelines, aiming to identify text segments that contain a checkworthy claim or
assertion in a social media post. Despite its importance to journalists and
human fact-checkers, it remains a severely understudied problem, and the scarce
research on this topic so far has only focused on English. Here we aim to
bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K
real-world claims collected from numerous social media platforms in five Indian
languages and English. We report strong baselines with state-of-the-art
encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of
training on multiple languages over alternative cross-lingual transfer methods
such as zero-shot transfer, or training on translated data, from a
high-resource language such as English. We evaluate generative large language
models from the GPT series using prompting methods on the X-CLAIM dataset and
we find that they underperform the smaller encoder-only language models for
low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System. (arXiv:2310.18207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18207">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel negotiation dialogue agent designed for the
online marketplace. Our agent is integrative in nature i.e, it possesses the
capability to negotiate on price as well as other factors, such as the addition
or removal of items from a deal bundle, thereby offering a more flexible and
comprehensive negotiation experience. We create a new dataset called
Integrative Negotiation Dataset (IND) to enable this functionality. For this
dataset creation, we introduce a new semi-automated data creation method, which
combines defining negotiation intents, actions, and intent-action simulation
between users and the agent to generate potential dialogue flows. Finally, the
prompting of GPT-J, a state-of-the-art language model, is done to generate
dialogues for a given intent, with a human-in-the-loop process for post-editing
and refining minor errors to ensure high data quality. We employ a set of novel
rewards, specifically tailored for the negotiation task to train our
Negotiation Agent, termed as the Integrative Negotiation Agent (INA). These
rewards incentivize the chatbot to learn effective negotiation strategies that
can adapt to various contextual requirements and price proposals. By leveraging
the IND, we train our model and conduct experiments to evaluate the
effectiveness of our reward-based dialogue system for negotiation. Our results
demonstrate that the proposed approach and reward system significantly enhance
the agent's negotiation capabilities. The INA successfully engages in
integrative negotiations, displaying the ability to dynamically adjust prices
and negotiate the inclusion or exclusion of items in a bundle deal
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18208">
<div class="article-summary-box-inner">
<span><p>Existing deep-learning approaches to semantic column type annotation (CTA)
have important shortcomings: they rely on semantic types which are fixed at
training time; require a large number of training samples per type and incur
large run-time inference costs; and their performance can degrade when
evaluated on novel datasets, even when types remain constant. Large language
models have exhibited strong zero-shot classification performance on a wide
range of tasks and in this paper we explore their use for CTA. We introduce
ArcheType, a simple, practical method for context sampling, prompt
serialization, model querying, and label remapping, which enables large
language models to solve column type annotation problems in a fully zero-shot
manner. We ablate each component of our method separately, and establish that
improvements to context sampling and label remapping provide the most
consistent gains. ArcheType establishes new state-of-the-art performance on
both zero-shot and fine-tuned CTA, including three new domain-specific
benchmarks, which we release, along with the code to reproduce our results at
https://github.com/penfever/ArcheType.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing. (arXiv:2310.18229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18229">
<div class="article-summary-box-inner">
<span><p>In NLP, incremental processors produce output in instalments, based on
incoming prefixes of the linguistic input. Some tokens trigger revisions,
causing edits to the output hypothesis, but little is known about why models
revise when they revise. A policy that detects the time steps where revisions
should happen can improve efficiency. Still, retrieving a suitable signal to
train a revision policy is an open problem, since it is not naturally available
in datasets. In this work, we investigate the appropriateness of regressions
and skips in human reading eye-tracking data as signals to inform revision
policies in incremental sequence labelling. Using generalised mixed-effects
models, we find that the probability of regressions and skips by humans can
potentially serve as useful predictors for revisions in BiLSTMs and Transformer
models, with consistent results for various languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18235">
<div class="article-summary-box-inner">
<span><p>Evaluating text-to-image models is notoriously difficult. A strong recent
approach for assessing text-image faithfulness is based on QG/A (question
generation and answering), which uses pre-trained foundational models to
automatically generate a set of questions and answers from the prompt, and
output images are scored based on whether these answers extracted with a visual
question answering model are consistent with the prompt-based answers. This
kind of evaluation is naturally dependent on the quality of the underlying QG
and QA models. We identify and address several reliability challenges in
existing QG/A work: (a) QG questions should respect the prompt (avoiding
hallucinations, duplications, and omissions) and (b) VQA answers should be
consistent (not asserting that there is no motorcycle in an image while also
claiming the motorcycle is blue). We address these issues with Davidsonian
Scene Graph (DSG), an empirically grounded evaluation framework inspired by
formal semantics. DSG is an automatic, graph-based QG/A that is modularly
implemented to be adaptable to any QG/A module. DSG produces atomic and unique
questions organized in dependency graphs, which (i) ensure appropriate semantic
coverage and (ii) sidestep inconsistent answers. With extensive experimentation
and human evaluation on a range of model configurations (LLM, VQA, and T2I), we
empirically demonstrate that DSG addresses the challenges noted above. Finally,
we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060
prompts, covering a wide range of fine-grained semantic categories with a
balanced distribution. We will release the DSG-1k prompts and the corresponding
DSG questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Language Models Using Formal Methods Feedback. (arXiv:2310.18239v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18239">
<div class="article-summary-box-inner">
<span><p>Although pre-trained language models encode generic knowledge beneficial for
planning and control, they may fail to generate appropriate control policies
for domain-specific tasks. Existing fine-tuning methods use human feedback to
address this limitation, however, sourcing human feedback is labor intensive
and costly. We present a fully automated approach to fine-tune pre-trained
language models for applications in autonomous systems, bridging the gap
between generic knowledge and domain-specific requirements while reducing cost.
The method synthesizes automaton-based controllers from pre-trained models
guided by natural language task descriptions. These controllers are verifiable
against independently provided specifications within a world model, which can
be abstract or obtained from a high-fidelity simulator. Controllers with high
compliance with the desired specifications receive higher ranks, guiding the
iterative fine-tuning process. We provide quantitative evidences, primarily in
autonomous driving, to demonstrate the method's effectiveness across multiple
tasks. The results indicate an improvement in percentage of specifications
satisfied by the controller from 60% to 90%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16. (arXiv:2310.18263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18263">
<div class="article-summary-box-inner">
<span><p>The amount of news being consumed online has substantially expanded in recent
years. Fake news has become increasingly common, especially in regional
languages like Malayalam, due to the rapid publication and lack of editorial
standards on some online sites. Fake news may have a terrible effect on
society, causing people to make bad judgments, lose faith in authorities, and
even engage in violent behavior. When we take into the context of India, there
are many regional languages, and fake news is spreading in every language.
Therefore, providing efficient techniques for identifying false information in
regional tongues is crucial. Until now, little to no work has been done in
Malayalam, extracting features from multiple modalities to classify fake news.
Multimodal approaches are more accurate in detecting fake news, as features
from multiple modalities are extracted to build the deep learning
classification model. As far as we know, this is the first piece of work in
Malayalam that uses multimodal deep learning to tackle false information.
Models trained with more than one modality typically outperform models taught
with only one modality. Our study in the Malayalam language utilizing
multimodal deep learning is a significant step toward more effective
misinformation detection and mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Automatically generating Riddles aiding Concept Attainment. (arXiv:2310.18290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18290">
<div class="article-summary-box-inner">
<span><p>One of the primary challenges in online learning environments, is to retain
learner engagement. Several different instructional strategies are proposed
both in online and offline environments to enhance learner engagement. The
Concept Attainment Model is one such instructional strategy that focuses on
learners acquiring a deeper understanding of a concept rather than just its
dictionary definition. This is done by searching and listing the properties
used to distinguish examples from non-examples of various concepts. Our work
attempts to apply the Concept Attainment Model to build conceptual riddles, to
deploy over online learning environments. The approach involves creating
factual triples from learning resources, classifying them based on their
uniqueness to a concept into `Topic Markers' and `Common', followed by
generating riddles based on the Concept Attainment Model's format and capturing
all possible solutions to those riddles. The results obtained from the human
evaluation of riddles prove encouraging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18313">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore FP8 low-bit data formats for efficient training of
large language models (LLMs). Our key insight is that most variables, such as
gradients and optimizer states, in LLM training can employ low-precision data
formats without compromising model accuracy and requiring no changes to
hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision
framework for training LLMs. This framework offers three levels of FP8
utilization to streamline mixed-precision and distributed parallel training for
LLMs. It gradually incorporates 8-bit gradients, optimizer states, and
distributed learning in an incremental manner. Experiment results show that,
during the training of GPT-175B model on H100 GPU platform, our FP8
mixed-precision training framework not only achieved a remarkable 42% reduction
in real memory usage but also ran 64% faster than the widely adopted BF16
framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer
Engine by 17%. This largely reduces the training costs for large foundation
models. Furthermore, our FP8 mixed-precision training methodology is generic.
It can be seamlessly applied to other tasks such as LLM instruction tuning and
reinforcement learning with human feedback, offering savings in fine-tuning
expenses. Our FP8 low-precision training framework is open-sourced at
{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07051">
<div class="article-summary-box-inner">
<span><p>Abstract reasoning is a key ability for an intelligent system. Large language
models (LMs) achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect. For example, human reasoning is affected by our real-world knowledge
and beliefs, and shows notable "content effects"; humans reason more reliably
when the semantic content of a problem supports the correct logical inferences.
These content-entangled reasoning patterns play a central role in debates about
the fundamental nature of human intelligence. Here, we investigate whether
language models $\unicode{x2014}$ whose prior expectations capture some aspects
of human knowledge $\unicode{x2014}$ similarly mix content into their answers
to logical problems. We explored this question across three logical reasoning
tasks: natural language inference, judging the logical validity of syllogisms,
and the Wason selection task. We evaluate state of the art large language
models, as well as humans, and find that the language models reflect many of
the same patterns observed in humans across these tasks $\unicode{x2014}$ like
humans, models answer more accurately when the semantic content of a task
supports the logical inferences. These parallels are reflected both in answer
patterns, and in lower-level features like the relationship between model
answer distributions and human response times. Our findings have implications
for understanding both these cognitive effects in humans, and the factors that
contribute to language model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LR-Sum: Summarization for Less-Resourced Languages. (arXiv:2212.09674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09674">
<div class="article-summary-box-inner">
<span><p>This preprint describes work in progress on LR-Sum, a new
permissively-licensed dataset created with the goal of enabling further
research in automatic summarization for less-resourced languages. LR-Sum
contains human-written summaries for 40 languages, many of which are
less-resourced. We describe our process for extracting and filtering the
dataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022). The
source data is public domain newswire collected from from Voice of America
websites, and LR-Sum is released under a Creative Commons license (CC BY 4.0),
making it one of the most openly-licensed multilingual summarization datasets.
We describe how we plan to use the data for modeling experiments and discuss
limitations of the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10015">
<div class="article-summary-box-inner">
<span><p>Spatial understanding is a fundamental aspect of computer vision and integral
for human-level reasoning about images, making it an important component for
grounded language understanding. While recent text-to-image synthesis (T2I)
models have shown unprecedented improvements in photorealism, it is unclear
whether they have reliable spatial understanding capabilities. We investigate
the ability of T2I models to generate correct spatial relationships among
objects and present VISOR, an evaluation metric that captures how accurately
the spatial relationship described in text is generated in the image. To
benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that
contains sentences describing two or more objects and the spatial relationships
between them. We construct an automated evaluation pipeline to recognize
objects and their spatial relationships, and employ it in a large-scale
evaluation of T2I models. Our experiments reveal a surprising finding that,
although state-of-the-art T2I models exhibit high image quality, they are
severely limited in their ability to generate multiple objects or the specified
spatial relations between them. Our analyses demonstrate several biases and
artifacts of T2I models such as the difficulty with generating multiple
objects, a bias towards generating the first object mentioned, spatially
inconsistent outputs for equivalent relationships, and a correlation between
object co-occurrence and spatial understanding capabilities. We conduct a human
study that shows the alignment between VISOR and human judgement about spatial
understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to
the community in support of T2I reasoning research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10767">
<div class="article-summary-box-inner">
<span><p>Sequence labeling is a core task in text understanding for IE/IR systems.
Text generation models have increasingly become the go-to solution for such
tasks (e.g., entity extraction and dialog slot filling). While most research
has focused on the labeling accuracy, a key aspect -- of vital practical
importance -- has slipped through the cracks: understanding model confidence.
More specifically, we lack a principled understanding of how to reliably gauge
the confidence of a model in its predictions for each labeled span. This paper
aims to provide some empirical insights on estimating model confidence for
generative sequence labeling. Most notably, we find that simply using the
decoder's output probabilities \textbf{is not} the best in realizing
well-calibrated confidence estimates. As verified over six public datasets of
different tasks, we show that our proposed approach -- which leverages
statistics from top-$k$ predictions by a beam search -- significantly reduces
calibration errors of the predictions of a generative sequence labeling model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12534">
<div class="article-summary-box-inner">
<span><p>Offensive speech detection is a key component of content moderation. However,
what is offensive can be highly subjective. This paper investigates how machine
and human moderators disagree on what is offensive when it comes to real-world
social web political discourse. We show that (1) there is extensive
disagreement among the moderators (humans and machines); and (2) human and
large-language-model classifiers are unable to predict how other human raters
will respond, based on their political leanings. For (1), we conduct a noise
audit at an unprecedented scale that combines both machine and human responses.
For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our
noise audit reveals that moderation outcomes vary wildly across different
machine moderators. Our experiments with human moderators suggest that
political leanings combined with sensitive issues affect both first-person and
vicarious offense. The dataset is available through
https://github.com/Homan-Lab/voiced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">90% F1 Score in Relational Triple Extraction: Is it Real ?. (arXiv:2302.09887v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09887">
<div class="article-summary-box-inner">
<span><p>Extracting relational triples from text is a crucial task for constructing
knowledge bases. Recent advancements in joint entity and relation extraction
models have demonstrated remarkable F1 scores ($\ge 90\%$) in accurately
extracting relational triples from free text. However, these models have been
evaluated under restrictive experimental settings and unrealistic datasets.
They overlook sentences with zero triples (zero-cardinality), thereby
simplifying the task. In this paper, we present a benchmark study of
state-of-the-art joint entity and relation extraction models under a more
realistic setting. We include sentences that lack any triples in our
experiments, providing a comprehensive evaluation. Our findings reveal a
significant decline (approximately 10-15\% in one dataset and 6-14\% in another
dataset) in the models' F1 scores within this realistic experimental setup.
Furthermore, we propose a two-step modeling approach that utilizes a simple
BERT-based classifier. This approach leads to overall performance improvement
in these models within the realistic experimental setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to reason over visual objects. (arXiv:2303.02260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02260">
<div class="article-summary-box-inner">
<span><p>A core component of human intelligence is the ability to identify abstract
patterns inherent in complex, high-dimensional perceptual data, as exemplified
by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated
by the goal of designing AI systems with this capacity, recent work has focused
on evaluating whether neural networks can learn to solve RPM-like problems.
Previous work has generally found that strong performance on these problems
requires the incorporation of inductive biases that are specific to the RPM
problem format, raising the question of whether such models might be more
broadly useful. Here, we investigated the extent to which a general-purpose
mechanism for processing visual scenes in terms of objects might help promote
abstract visual reasoning. We found that a simple model, consisting only of an
object-centric encoder and a transformer reasoning module, achieved
state-of-the-art results on both of two challenging RPM-like benchmarks (PGM
and I-RAVEN), as well as a novel benchmark with greater visual complexity
(CLEVR-Matrices). These results suggest that an inductive bias for
object-centric processing may be a key component of abstract visual reasoning,
obviating the need for problem-specific inductive biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11403">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have so far impressed the world, with
unprecedented capabilities that emerge in models at large scales. On the vision
side, transformer models (i.e., ViT) are following the same trend, achieving
the best performance on challenging benchmarks. With the abundance of such
unimodal models, a natural question arises; do we need also to follow this
trend to tackle multimodal tasks? In this work, we propose to rather direct
effort to efficient adaptations of existing models, and propose to augment
Language Models with perception. Existing approaches for adapting pretrained
models for vision-language tasks still rely on several key components that
hinder their efficiency. In particular, they still train a large number of
parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)
trained on huge image-text datasets, and add significant inference overhead. In
addition, most of these approaches have focused on Zero-Shot and In Context
Learning, with little to no effort on direct finetuning. We investigate the
minimal computational effort needed to adapt unimodal models for multimodal
tasks and propose a new challenging setup, alongside different approaches, that
efficiently adapts unimodal pretrained models. We show that by freezing more
than 99% of total parameters, training only one linear projection layer, and
prepending only one trainable token, our approach (dubbed eP-ALM) significantly
outperforms other baselines on VQA and Captioning across Image, Video, and
Audio modalities, following the proposed setup. The code is available here:
https://github.com/mshukor/eP-ALM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02247">
<div class="article-summary-box-inner">
<span><p>We address an important gap in detecting political bias in news articles.
Previous works that perform document classification can be influenced by the
writing style of each news outlet, leading to overfitting and limited
generalizability. Our approach overcomes this limitation by considering both
the sentence-level semantics and the document-level rhetorical structure,
resulting in a more robust and style-agnostic approach to detecting political
bias in news articles. We introduce a novel multi-head hierarchical attention
model that effectively encodes the structure of long documents through a
diverse ensemble of attention heads. While journalism follows a formalized
rhetorical structure, the writing style may vary by news outlet. We demonstrate
that our method overcomes this domain dependency and outperforms previous
approaches for robustness and accuracy. Further analysis and human evaluation
demonstrate the ability of our model to capture common discourse structures in
journalism. Our code is available at:
https://github.com/xfactlab/emnlp2023-Document-Hierarchy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. (arXiv:2304.09542v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09542">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable zero-shot
generalization across various language-related tasks, including search engines.
However, existing work utilizes the generative ability of LLMs for Information
Retrieval (IR) rather than direct passage ranking. The discrepancy between the
pre-training objectives of LLMs and the ranking objective poses another
challenge. In this paper, we first investigate generative LLMs such as ChatGPT
and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal
that properly instructed LLMs can deliver competitive, even superior results to
state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to
address concerns about data contamination of LLMs, we collect a new test set
called NovelEval, based on the latest knowledge and aiming to verify the
model's ability to rank unknown knowledge. Finally, to improve efficiency in
real-world applications, we delve into the potential for distilling the ranking
capabilities of ChatGPT into small specialized models using a permutation
distillation scheme. Our evaluation results turn out that a distilled 440M
model outperforms a 3B supervised model on the BEIR benchmark. The code to
reproduce our results is available at www.github.com/sunnweiwei/RankGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Company classification using zero-shot learning. (arXiv:2305.01028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01028">
<div class="article-summary-box-inner">
<span><p>In recent years, natural language processing (NLP) has become increasingly
important in a variety of business applications, including sentiment analysis,
text classification, and named entity recognition. In this paper, we propose an
approach for company classification using NLP and zero-shot learning. Our
method utilizes pre-trained transformer models to extract features from company
descriptions, and then applies zero-shot learning to classify companies into
relevant categories without the need for specific training data for each
category. We evaluate our approach on a dataset obtained through the Wharton
Research Data Services (WRDS), which comprises textual descriptions of publicly
traded companies. We demonstrate that the approach can streamline the process
of company classification, thereby reducing the time and resources required in
traditional approaches such as the Global Industry Classification Standard
(GICS). The results show that this method has potential for automation of
company classification, making it a promising avenue for future research in
this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of sub-word segmentation on performance of transformer language models. (arXiv:2305.05480v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05480">
<div class="article-summary-box-inner">
<span><p>Language modeling is a fundamental task in natural language processing, which
has been thoroughly explored with various architectures and hyperparameters.
However, few studies focus on the effect of sub-word segmentation on the
performance of language models (LMs). In this paper, we compare GPT and BERT
models trained with the statistical segmentation algorithm BPE vs. two
unsupervised algorithms for morphological segmentation -- Morfessor and
StateMorph. We train the models for several languages -- including ones with
very rich morphology -- and compare their performance with different
segmentation algorithms, vocabulary sizes, and model sizes. The results show
that training with morphological segmentation allows the LMs to: 1. achieve
lower perplexity, 2. converge more efficiently in terms of training time, and
3. achieve equivalent or better evaluation scores on downstream tasks. Lastly,
we show 4. that LMs of smaller size using morphological segmentation can
perform comparably to models of larger size trained with BPE -- both in terms
of (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact
on sustainability of LMs, since they reduce the model cost: size and
computation time. While (2) reduces cost only in the training phase, (4) does
so also in the inference phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v6 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09770">
<div class="article-summary-box-inner">
<span><p>Despite a surge collection of XAI methods, users still struggle to obtain
required AI explanations. Previous research suggests chatbots as dynamic
solutions, but the effective design of conversational XAI agents for practical
human needs remains under-explored. This paper focuses on Conversational XAI
for AI-assisted scientific writing tasks. Drawing from human linguistic
theories and formative studies, we identify four design rationales:
"multifaceted", "controllability", "mix-initiative", "context-aware
drill-down". We incorporate them into an interactive prototype, ConvXAI, which
facilitates heterogeneous AI explanations for scientific writing through
dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based
baseline on improving human-perceived understanding and writing improvement.
The paper further discusses the practical human usage patterns in interacting
with ConvXAI for scientific co-writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences. (arXiv:2305.11129v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11129">
<div class="article-summary-box-inner">
<span><p>We present our work on developing a multilingual, efficient text-to-text
transformer that is suitable for handling long inputs. This model, called
mLongT5, builds upon the architecture of LongT5, while leveraging the
multilingual datasets used for pretraining mT5 and the pretraining tasks of
UL2. We evaluate this model on a variety of multilingual summarization and
question-answering tasks, and the results show stronger performance for mLongT5
when compared to existing multilingual models such as mBART or M-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12029">
<div class="article-summary-box-inner">
<span><p>Current disfluency detection models focus on individual utterances each from
a single speaker. However, numerous discontinuity phenomena in spoken
conversational transcripts occur across multiple turns, hampering human
readability and the performance of downstream NLP tasks. This study addresses
these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken
conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We
design a data labeling schema to collect the high-quality dataset and provide
extensive data analysis. Furthermore, we leverage two modeling approaches for
experimental evaluation as benchmarks for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13455">
<div class="article-summary-box-inner">
<span><p>Recent work has proposed a methodology for the systematic evaluation of
"Situated Language Understanding Agents"-agents that operate in rich linguistic
and non-linguistic contexts-through testing them in carefully constructed
interactive settings. Other recent work has argued that Large Language Models
(LLMs), if suitably set up, can be understood as (simulators of) such agents. A
connection suggests itself, which this paper explores: Can LLMs be evaluated
meaningfully by exposing them to constrained game-like settings that are built
to challenge specific capabilities? As a proof of concept, this paper
investigates five interaction settings, showing that current chat-optimised
LLMs are, to an extent, capable to follow game-play instructions. Both this
capability and the quality of the game play, measured by how well the
objectives of the different games are met, follows the development cycle, with
newer models performing better. The metrics even for the comparatively simple
example games are far from being saturated, suggesting that the proposed
instrument will remain to have diagnostic value. Our general framework for
implementing and evaluating games with LLMs is available at
https://github.com/clp-research/clembench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13661">
<div class="article-summary-box-inner">
<span><p>In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Capture Dissenting Human Voices?. (arXiv:2305.13788v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13788">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown impressive achievements in solving a
broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been
shown to generalize in zero-shot settings as well. However, whether LLMs
closely align with the human disagreement distribution has not been
well-studied, especially within the scope of natural language inference (NLI).
In this paper, we evaluate the performance and alignment of LLM distribution
with humans using two different techniques to estimate the multinomial
distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation
(LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks
and simultaneously fail to capture human disagreement distribution. The
inference and human alignment performances plunge even further on data samples
with high human disagreement levels, raising concerns about their natural
language understanding (NLU) ability and their representativeness to a larger
human population. The source code for the experiments is available at
https://github.com/xfactlab/emnlp2023-LLM-Disagreement
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13850">
<div class="article-summary-box-inner">
<span><p>Visual Relation Extraction (VRE) is a powerful means of discovering
relationships between entities within visually-rich documents. Existing methods
often focus on manipulating entity features to find pairwise relations, yet
neglect the more fundamental structural information that links disparate entity
pairs together. The absence of global structure information may make the model
struggle to learn long-range relations and easily predict conflicted results.
To alleviate such limitations, we propose a GlObal Structure knowledge-guided
relation Extraction (GOSE) framework. GOSE initiates by generating preliminary
relation predictions on entity pairs extracted from a scanned image of the
document. Subsequently, global structural knowledge is captured from the
preceding iterative predictions, which are then incorporated into the
representations of the entities. This "generate-capture-incorporate" cycle is
repeated multiple times, allowing entity representations and global structure
knowledge to be mutually reinforced. Extensive experiments validate that GOSE
not only outperforms existing methods in the standard fine-tuning setting but
also reveals superior cross-lingual learning capabilities; indeed, even yields
stronger data-efficient performance in the low-resource setting. The code for
GOSE will be available at https://github.com/chenxn2020/GOSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Chain-of-Thought Style Prompting for Text-to-SQL. (arXiv:2305.14215v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14215">
<div class="article-summary-box-inner">
<span><p>In-context learning with large language models (LLMs) has recently caught
increasing attention due to its superior few-shot performance on various tasks.
However, its performance on text-to-SQL parsing still has much room for
improvement. In this paper, we hypothesize that a crucial aspect of LLMs to
improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we
systematically study how to enhance LLMs' reasoning ability through chain of
thought (CoT) style prompting, including the original chain-of-thought
prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023).
Our experiments demonstrate that iterative prompting as in Zhou et al. (2023)
may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps
tends to have more error propagation issues. Based on these findings, we
propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2
and 6.5 point absolute gains on the Spider development set and the Spider
Realistic set, respectively, compared to the standard prompting method without
reasoning steps; 2.4 and 1.5 point absolute gains, compared to the
least-to-most prompting method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14282">
<div class="article-summary-box-inner">
<span><p>Automatically evaluating the quality of language generation is critical.
Although recent learned metrics show high correlation with human judgement,
these metrics can not explain their verdict or associate the scores with
defects in generated text. To address this limitation, we present
InstructScore, an explainable evaluation metric for text generation. By
harnessing both explicit human instruction and the implicit knowledge of GPT-4,
we fine-tune a text evaluation metric based on LLaMA, producing both a score
for generated text and a human readable diagnostic report. We evaluate
InstructScore on a variety of generation tasks, including translation,
captioning, data-to-text and commonsense generation. Experiments show that our
7B model surpasses all other unsupervised metrics, including those based on
175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct
supervision from human-rated data, achieves performance levels on par with
state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14695">
<div class="article-summary-box-inner">
<span><p>Entity bias widely affects pretrained (large) language models, causing them
to rely on (biased) parametric knowledge to make unfaithful predictions.
Although causality-inspired methods have shown great potential to mitigate
entity bias, it is hard to precisely estimate the parameters of underlying
causal models in practice. The rise of black-box LLMs also makes the situation
even worse, because of their inaccessible parameters and uncalibrated logits.
To address these problems, we propose a specific structured causal model (SCM)
whose parameters are comparatively easier to estimate. Building upon this SCM,
we propose causal intervention techniques to mitigate entity bias for both
white-box and black-box settings. The proposed causal intervention perturbs the
original entity with neighboring entities. This intervention reduces specific
biasing information pertaining to the original entity while still preserving
sufficient semantic information from similar entities. Under the white-box
setting, our training-time intervention improves OOD performance of PLMs on
relation extraction (RE) and machine reading comprehension (MRC) by 5.7 points
and by 9.1 points, respectively. Under the black-box setting, our in-context
intervention effectively reduces the entity-based knowledge conflicts of
GPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on
MRC and up to 17.6 points of reduction in memorization ratio on RE. Our code is
available at https://github.com/luka-group/Causal-View-of-Entity-Bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14815">
<div class="article-summary-box-inner">
<span><p>We present an accurate and interpretable method for answer extraction in
machine reading comprehension that is reminiscent of case-based reasoning (CBR)
from classical AI. Our method (CBR-MRC) builds upon the hypothesis that
contextualized answers to similar questions share semantic similarities with
each other. Given a test question, CBR-MRC first retrieves a set of similar
cases from a non-parametric memory and then predicts an answer by selecting the
span in the test context that is most similar to the contextualized
representations of answers in the retrieved cases. The semi-parametric nature
of our approach allows it to attribute a prediction to the specific set of
evidence cases, making it a desirable choice for building reliable and
debuggable QA systems. We show that CBR-MRC provides high accuracy comparable
with large reader models and outperforms baselines by 11.5 and 8.4 EM on
NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability
of CBR-MRC in identifying not just the correct answer tokens but also the span
with the most relevant supporting evidence. Lastly, we observe that contexts
for certain question types show higher lexical diversity than others and find
that CBR-MRC is robust to these variations while performance using
fully-parametric methods drops.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15328">
<div class="article-summary-box-inner">
<span><p>As large language models have demonstrated impressive performance in many
domains, recent works have adopted language models (LMs) as controllers of
visual modules for vision-and-language tasks. While existing work focuses on
equipping LMs with visual understanding, we propose two novel
interpretable/explainable visual programming frameworks for text-to-image (T2I)
generation and evaluation. First, we introduce VPGen, an interpretable
step-by-step T2I generation framework that decomposes T2I generation into three
steps: object/count generation, layout generation, and image generation. We
employ an LM to handle the first two steps (object/count generation and layout
generation), by finetuning it on text-layout pairs. Our step-by-step T2I
generation framework provides stronger spatial control than end-to-end models,
the dominant approach for this task. Furthermore, we leverage the world
knowledge of pretrained LMs, overcoming the limitation of previous
layout-guided T2I works that can only handle predefined object classes. We
demonstrate that our VPGen has improved control in counts/spatial
relations/scales of objects than state-of-the-art T2I generation models.
Second, we introduce VPEval, an interpretable and explainable evaluation
framework for T2I generation based on visual programming. Unlike previous T2I
evaluations with a single scoring model that is accurate in some skills but
unreliable in others, VPEval produces evaluation programs that invoke a set of
visual modules that are experts in different skills, and also provides
visual+textual explanations of the evaluation results. Our analysis shows that
VPEval provides a more human-correlated evaluation for skill-specific and
open-ended prompts than widely used single model-based evaluation. We hope that
our work encourages future progress on interpretable/explainable generation and
evaluation for T2I models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints. (arXiv:2305.19068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19068">
<div class="article-summary-box-inner">
<span><p>Querying knowledge graphs (KGs) using deep learning approaches can naturally
leverage the reasoning and generalization ability to learn to infer better
answers. Traditional neural complex query answering (CQA) approaches mostly
work on entity-centric KGs. However, in the real world, we also need to make
logical inferences about events, states, and activities (i.e., eventualities or
situations) to push learning systems from System I to System II, as proposed by
Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can
naturally provide references to such kind of intuitive and logical inference.
Thus, in this paper, we propose a new framework to leverage neural methods to
answer complex logical queries based on an EVKG, which can satisfy not only
traditional first-order logic constraints but also implicit logical constraints
over eventualities concerning their occurrences and orders. For instance, if we
know that "Food is bad" happens before "PersonX adds soy sauce", then "PersonX
adds soy sauce" is unlikely to be the cause of "Food is bad" due to implicit
temporal constraint. To facilitate consistent reasoning on EVKGs, we propose
Complex Eventuality Query Answering (CEQA), a more rigorous definition of CQA
that considers the implicit logical constraints governing the temporal order
and occurrence of eventualities. In this manner, we propose to leverage theorem
provers for constructing benchmark datasets to ensure the answers satisfy
implicit logical constraints. We also propose a Memory-Enhanced Query Encoding
(MEQE) approach to significantly improve the performance of state-of-the-art
neural query encoders on the CEQA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIES-Merging: Resolving Interference When Merging Models. (arXiv:2306.01708v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01708">
<div class="article-summary-box-inner">
<span><p>Transfer learning - i.e., further fine-tuning a pre-trained model on a
downstream task - can confer significant advantages, including improved
downstream performance, faster convergence, and better sample efficiency. These
advantages have led to a proliferation of task-specific fine-tuned models,
which typically can only perform a single task and do not benefit from one
another. Recently, model merging techniques have emerged as a solution to
combine multiple task-specific models into a single multitask model without
performing additional training. However, existing merging methods often ignore
the interference between parameters of different models, resulting in large
performance drops when merging multiple models. In this paper, we demonstrate
that prior merging techniques inadvertently lose valuable information due to
two major sources of interference: (a) interference due to redundant parameter
values and (b) disagreement on the sign of a given parameter's values across
models. To address this, we propose our method, TRIM, ELECT SIGN &amp; MERGE
(TIES-Merging), which introduces three novel steps when merging models: (1)
resetting parameters that only changed a small amount during fine-tuning, (2)
resolving sign conflicts, and (3) merging only the parameters that are in
alignment with the final agreed-upon sign. We find that TIES-Merging
outperforms several existing methods in diverse settings covering a range of
modalities, domains, number of tasks, model sizes, architectures, and
fine-tuning settings. We further analyze the impact of different types of
interference on model parameters, and highlight the importance of resolving
sign interference. Our code is available at
https://github.com/prateeky2806/ties-merging
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02531">
<div class="article-summary-box-inner">
<span><p>Autoregressive models for text sometimes generate repetitive and low-quality
output because errors accumulate during the steps of generation. This issue is
often attributed to exposure bias - the difference between how a model is
trained, and how it is used during inference. Denoising diffusion models
provide an alternative approach in which a model can revisit and revise its
output. However, they can be computationally expensive and prior efforts on
text have led to models that produce less fluent output compared to
autoregressive models, especially for longer text and paragraphs. In this
paper, we propose PLANNER, a model that combines latent semantic diffusion with
autoregressive generation, to generate fluent text while exercising global
control over paragraphs. The model achieves this by combining an autoregressive
"decoding" module with a "planning" module that uses latent diffusion to
generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed
method is evaluated on various conditional generation tasks, and results on
semantic generation, text completion and summarization show its effectiveness
in generating high-quality long-form text in an efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic HELM: A Human-Readable Memory for Reinforcement Learning. (arXiv:2306.09312v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09312">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning agents deployed in the real world often have to cope
with partially observable environments. Therefore, most agents employ memory
mechanisms to approximate the state of the environment. Recently, there have
been impressive success stories in mastering partially observable environments,
mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft.
However, existing methods lack interpretability in the sense that it is not
comprehensible for humans what the agent stores in its memory. In this regard,
we propose a novel memory mechanism that represents past events in human
language. Our method uses CLIP to associate visual inputs with language tokens.
Then we feed these tokens to a pretrained language model that serves the agent
as memory and provides it with a coherent and human-readable representation of
the past. We train our memory mechanism on a set of partially observable
environments and find that it excels on tasks that require a memory component,
while mostly attaining performance on-par with strong baselines on tasks that
do not. On a challenging continuous recognition task, where memorizing the past
is crucial, our memory mechanism converges two orders of magnitude faster than
prior methods. Since our memory mechanism is human-readable, we can peek at an
agent's memory and check whether crucial pieces of information have been
stored. This significantly enhances troubleshooting and paves the way toward
more interpretable agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-State Transformers. (arXiv:2306.09539v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09539">
<div class="article-summary-box-inner">
<span><p>State space models (SSMs) have shown impressive results on tasks that require
modeling long-range dependencies and efficiently scale to long sequences owing
to their subquadratic runtime complexity. Originally designed for continuous
signals, SSMs have shown superior performance on a plethora of tasks, in vision
and audio; however, SSMs still lag Transformer performance in Language Modeling
tasks. In this work, we propose a hybrid layer named Block-State Transformer
(BST), that internally combines an SSM sublayer for long-range
contextualization, and a Block Transformer sublayer for short-term
representation of sequences. We study three different, and completely
parallelizable, variants that integrate SSMs and block-wise attention. We show
that our model outperforms similar Transformer-based architectures on language
modeling perplexity and generalizes to longer sequences. In addition, the
Block-State Transformer demonstrates more than tenfold increase in speed at the
layer level compared to the Block-Recurrent Transformer when model
parallelization is employed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus. (arXiv:2306.15162v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15162">
<div class="article-summary-box-inner">
<span><p>Machine learning for sign languages is bottlenecked by data. In this paper,
we present YouTube-ASL, a large-scale, open-domain corpus of American Sign
Language (ASL) videos and accompanying English captions drawn from YouTube.
With ~1000 hours of videos and &gt;2500 unique signers, YouTube-ASL is ~3x as
large and has ~10x as many unique signers as the largest prior ASL dataset. We
train baseline models for ASL to English translation on YouTube-ASL and
evaluate them on How2Sign, where we achieve a new finetuned state of the art of
12.39 BLEU and, for the first time, report zero-shot results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16564">
<div class="article-summary-box-inner">
<span><p>Generative Large language models (LLMs) have demonstrated remarkable
capabilities for a wide range of applications, but reducing ungrounded or
erroneous responses remains a major growth area. Unlike task-specific models,
there lack an effective method to calibrate the confidence level of LLM
responses to indicate potential errors and facilitate human-in-the-loop
verification. An important source of calibration stems from expert-stipulated
programmatic supervision, which is often available at low cost but has its own
limitations such as noise and coverage. In this paper, we introduce a Pareto
optimal self-supervision framework that can leverage available programmatic
supervision to systematically calibrate LLM responses by producing a risk score
for every LLM response, without any additional manual efforts. This is
accomplished by learning a harmonizer model to align with LLM output as well as
other weak supervision sources. The model assigns higher risk scores to more
uncertain LLM responses and facilitate error correction. Experiments on
standard relation extraction and classification tasks in biomedical and general
domains demonstrate that the proposed risk score is highly correlated with the
actual LLM error rate. By using a dynamic prompting strategy based on the risk
score, we observed significant accuracy improvement for off-the-shelf LLMs,
boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model
and GPT-4 results past SOTA supervised results on challenging evaluation
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.04090">
<div class="article-summary-box-inner">
<span><p>Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://huggingface.co/spaces/Hellisotherpeople/DebateKG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08623">
<div class="article-summary-box-inner">
<span><p>Language models pretrained on large collections of tabular data have
demonstrated their effectiveness in several downstream tasks. However, many of
these models do not take into account the row/column permutation invariances,
hierarchical structure, etc. that exist in tabular data. To alleviate these
limitations, we propose HYTREL, a tabular language model, that captures the
permutation invariances and three more structural properties of tabular data by
using hypergraphs - where the table cells make up the nodes and the cells
occurring jointly together in each row, column, and the entire table are used
to form three different types of hyperedges. We show that HYTREL is maximally
invariant under certain conditions for tabular data, i.e., two tables obtain
the same representations via HYTREL iff the two tables are identical up to
permutations. Our empirical results demonstrate that HYTREL consistently
outperforms other competitive baselines on four downstream tasks with minimal
pretraining, illustrating the advantages of incorporating the inductive biases
associated with tabular data into the representations. Finally, our qualitative
analyses showcase that HYTREL can assimilate the table structures to generate
robust representations for the cells, rows, columns, and the entire table.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10088">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in device-control systems that can interpret
human natural language instructions and execute them on a digital device by
directly controlling its user interface. We present a dataset for
device-control research, Android in the Wild (AITW), which is orders of
magnitude larger than current datasets. The dataset contains human
demonstrations of device interactions, including the screens and actions, and
corresponding natural language instructions. It consists of 715k episodes
spanning 30k unique instructions, four versions of Android (v10-13),and eight
device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It
contains multi-step tasks that require semantic understanding of language and
visual context. This dataset poses a new challenge: actions available through
the user interface must be inferred from their visual appearance. And, instead
of simple UI element-based actions, the action space consists of precise
gestures (e.g., horizontal scrolls to operate carousel widgets). We organize
our dataset to encourage robustness analysis of device-control systems, i.e.,
how well a system performs in the presence of new task descriptions, new
applications, or new platform versions. We develop two agents and report
performance across the dataset. The dataset is available at
https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02122">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks have emerged as a prominent threat to natural language
processing (NLP) models, where the presence of specific triggers in the input
can lead poisoned models to misclassify these inputs to predetermined target
classes. Current detection mechanisms are limited by their inability to address
more covert backdoor strategies, such as style-based attacks. In this work, we
propose an innovative test-time poisoned sample detection framework that hinges
on the interpretability of model predictions, grounded in the semantic meaning
of inputs. We contend that triggers (e.g., infrequent words) are not supposed
to fundamentally alter the underlying semantic meanings of poisoned samples as
they want to stay stealthy. Based on this observation, we hypothesize that
while the model's predictions for paraphrased clean samples should remain
stable, predictions for poisoned samples should revert to their true labels
upon the mutations applied to triggers during the paraphrasing process. We
employ ChatGPT, a state-of-the-art large language model, as our paraphraser and
formulate the trigger-removal task as a prompt engineering problem. We adopt
fuzzing, a technique commonly used for unearthing software vulnerabilities, to
discover optimal paraphrase prompts that can effectively eliminate triggers
while concurrently maintaining input semantics. Experiments on 4 types of
backdoor attacks, including the subtle style backdoors, and 4 distinct datasets
demonstrate that our approach surpasses baseline methods, including STRIP, RAP,
and ONION, in precision and recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10397">
<div class="article-summary-box-inner">
<span><p>Detecting stereotypes and biases in Large Language Models (LLMs) can enhance
fairness and reduce adverse impacts on individuals or groups when these LLMs
are applied. However, the majority of existing methods focus on measuring the
model's preference towards sentences containing biases and stereotypes within
datasets, which lacks interpretability and cannot detect implicit biases and
stereotypes in the real world. To address this gap, this paper introduces a
four-stage framework to directly evaluate stereotypes and biases in the
generated content of LLMs, including direct inquiry testing, serial or adapted
story testing, implicit association testing, and unknown situation testing.
Additionally, the paper proposes multi-dimensional evaluation metrics and
explainable zero-shot prompts for automated evaluation. Using the education
sector as a case study, we constructed the Edu-FairMonitor based on the
four-stage framework, which encompasses 12,632 open-ended questions covering
nine sensitive factors and 26 educational scenarios. Experimental results
reveal varying degrees of stereotypes and biases in five LLMs evaluated on
Edu-FairMonitor. Moreover, the results of our proposed automated evaluation
method have shown a high correlation with human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04679">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models underpin a large portion of modern
NLP tools outside of English. A strong baseline for specializing these models
for specific languages is Language-Adaptive Pre-Training (LAPT). However,
retaining a large cross-lingual vocabulary and embedding matrix comes at
considerable excess computational cost during adaptation. In this study, we
propose several simple techniques to replace a cross-lingual vocabulary with a
compact, language-specific one. Namely, we address strategies for
re-initializing the token embedding matrix after vocabulary specialization. We
then provide a systematic experimental comparison of our techniques, in
addition to the recently-proposed Focus method. We demonstrate that: 1)
Embedding-replacement techniques in the monolingual transfer literature are
inadequate for adapting multilingual models. 2) Replacing cross-lingual
vocabularies with smaller specialized ones provides an efficient method to
improve performance in low-resource languages. 3) Simple embedding
re-initialization techniques based on script-wise sub-distributions rival
techniques such as Focus, which rely on similarity scores obtained from an
auxiliary model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06364">
<div class="article-summary-box-inner">
<span><p>Today, using Large-scale generative Language Models (LLMs) it is possible to
simulate free responses to interview questions like those traditionally
analyzed using qualitative research methods. Qualitative methodology
encompasses a broad family of techniques involving manual analysis of
open-ended interviews or conversations conducted freely in natural language.
Here we consider whether artificial "silicon participants" generated by LLMs
may be productively studied using qualitative methods aiming to produce
insights that could generalize to real human populations. The key concept in
our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)
capturing the degree to which LLM-generated outputs mirror human
sub-populations' beliefs and attitudes. By definition, high algorithmic
fidelity suggests latent beliefs elicited from LLMs may generalize to real
humans, whereas low algorithmic fidelity renders such research invalid. Here we
used an LLM to generate interviews with silicon participants matching specific
demographic characteristics one-for-one with a set of human participants. Using
framework-based qualitative analysis, we showed the key themes obtained from
both human and silicon participants were strikingly similar. However, when we
analyzed the structure and tone of the interviews we found even more striking
differences. We also found evidence of the hyper-accuracy distortion described
by Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not
have sufficient algorithmic fidelity to expect research on it to generalize to
human populations. However, the rapid pace of LLM research makes it plausible
this could change in the future. Thus we stress the need to establish epistemic
norms now around how to assess validity of LLM-based qualitative research,
especially concerning the need to ensure representation of heterogeneous lived
experiences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06520">
<div class="article-summary-box-inner">
<span><p>For sequence-to-sequence tasks it is challenging to combine individual system
outputs. Further, there is also often a mismatch between the decoding criterion
and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used
to combine system outputs in a manner that encourages better alignment with the
final assessment criterion. This paper examines MBR decoding for Grammatical
Error Correction (GEC) systems, where performance is usually evaluated in terms
of edits and an associated F-score. Hence, we propose a novel MBR loss function
directly linked to this form of criterion. Furthermore, an approach to expand
the possible set of candidate sentences is described. This builds on a current
max-voting combination scheme, as well as individual edit-level selection.
Experiments on three popular GEC datasets and with state-of-the-art GEC systems
demonstrate the efficacy of the proposed MBR approach. Additionally, the paper
highlights how varying reward metrics within the MBR decoding framework can
provide control over precision, recall, and the F-score in combined GEC
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04691">
<div class="article-summary-box-inner">
<span><p>Neural language models are probabilistic models of human text. They are
predominantly trained using maximum likelihood estimation (MLE), which is
equivalent to minimizing the forward cross-entropy between the empirical data
distribution and the model distribution. However, various degeneration
phenomena are still widely observed when decoding from the distributions
learned by such models. We establish that the forward cross-entropy is
suboptimal as a distance metric for aligning human and model distribution due
to its (1) recall-prioritization (2) negative diversity ignorance and (3)
train-test mismatch. In this paper, we propose Earth Mover Distance
Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on
the inherent properties of earth mover distance to address the aforementioned
challenges. Due to the high complexity of direct computation, we further
introduce a feasible upper bound for EMO to ease end-to-end training. Upon
extensive evaluation of language models trained using EMO and MLE. We find that
EMO demonstrates a consistently better language modeling performance than MLE
across domains. Moreover, EMO demonstrates noteworthy enhancements in
downstream performance with minimal fine-tuning on merely 25,000 sentences.
This highlights the tremendous potential of EMO as a lightweight calibration
method for enhancing large-scale pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09520">
<div class="article-summary-box-inner">
<span><p>While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11069">
<div class="article-summary-box-inner">
<span><p>Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11672">
<div class="article-summary-box-inner">
<span><p>Open-ended Commonsense Reasoning is defined as solving a commonsense question
without providing 1) a short list of answer candidates and 2) a pre-defined
answer scope. Conventional ways of formulating the commonsense question into a
question-answering form or utilizing external knowledge to learn
retrieval-based methods are less applicable in the open-ended setting due to an
inherent challenge. Without pre-defining an answer scope or a few candidates,
open-ended commonsense reasoning entails predicting answers by searching over
an extremely large searching space. Moreover, most questions require implicit
multi-hop reasoning, which presents even more challenges to our problem. In
this work, we leverage pre-trained language models to iteratively retrieve
reasoning paths on the external knowledge base, which does not require
task-specific supervision. The reasoning paths can help to identify the most
precise answer to the commonsense question. We conduct experiments on two
commonsense benchmark datasets. Compared to other approaches, our proposed
method achieves better performance both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13548">
<div class="article-summary-box-inner">
<span><p>Human feedback is commonly utilized to finetune AI assistants. But human
feedback may also encourage model responses that match user beliefs over
truthful ones, a behaviour known as sycophancy. We investigate the prevalence
of sycophancy in models whose finetuning procedure made use of human feedback,
and the potential role of human preference judgments in such behavior. We first
demonstrate that five state-of-the-art AI assistants consistently exhibit
sycophancy across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior, we analyze existing
human preference data. We find that when a response matches a user's views, it
is more likely to be preferred. Moreover, both humans and preference models
(PMs) prefer convincingly-written sycophantic responses over correct ones a
non-negligible fraction of the time. Optimizing model outputs against PMs also
sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results
indicate that sycophancy is a general behavior of state-of-the-art AI
assistants, likely driven in part by human preference judgments favoring
sycophantic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14088">
<div class="article-summary-box-inner">
<span><p>Curated datasets for healthcare are often limited due to the need of human
annotations from experts. In this paper, we present MedEval, a multi-level,
multi-task, and multi-domain medical benchmark to facilitate the development of
language models for healthcare. MedEval is comprehensive and consists of data
from several healthcare systems and spans 35 human body regions from 8
examination modalities. With 22,779 collected sentences and 21,228 reports, we
provide expert annotations at multiple levels, offering a granular potential
usage of the data and supporting a wide range of tasks. Moreover, we
systematically evaluated 10 generic and domain-specific language models under
zero-shot and finetuning settings, from domain-adapted baselines in healthcare
to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our
evaluations reveal varying effectiveness of the two categories of language
models across different tasks, from which we notice the importance of
instruction tuning for few-shot usage of large language models. Our
investigation paves the way toward benchmarking language models for healthcare
and provides valuable insights into the strengths and limitations of adopting
large language models in medical domains, informing their practical
applications and future advancements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14735">
<div class="article-summary-box-inner">
<span><p>This paper delves into the pivotal role of prompt engineering in unleashing
the capabilities of Large Language Models (LLMs). Prompt engineering is the
process of structuring input text for LLMs and is a technique integral to
optimizing the efficacy of LLMs. This survey elucidates foundational principles
of prompt engineering, such as role-prompting, one-shot, and few-shot
prompting, as well as more advanced methodologies such as the chain-of-thought
and tree-of-thoughts prompting. The paper sheds light on how external
assistance in the form of plugins can assist in this task, and reduce machine
hallucination by retrieving external knowledge. We subsequently delineate
prospective directions in prompt engineering research, emphasizing the need for
a deeper understanding of structures and the role of agents in Artificial
Intelligence-Generated Content (AIGC) tools. We discuss how to assess the
efficacy of prompt methods from different perspectives and using different
methods. Finally, we gather information about the application of prompt
engineering in such fields as education and programming, showing its
transformative potential. This comprehensive survey aims to serve as a friendly
guide for anyone venturing through the big world of LLMs and prompt
engineering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15970">
<div class="article-summary-box-inner">
<span><p>Speech accents pose a significant challenge to state-of-the-art automatic
speech recognition (ASR) systems. Degradation in performance across
underrepresented accents is a severe deterrent to the inclusive adoption of
ASR. In this work, we propose a novel accent adaptation approach for end-to-end
ASR systems using cross-attention with a trainable set of codebooks. These
learnable codebooks capture accent-specific information and are integrated
within the ASR encoder layers. The model is trained on accented English speech,
while the test data also contained accents which were not seen during training.
On the Mozilla Common Voice multi-accented dataset, we show that our proposed
approach yields significant performance gains not only on the seen English
accents (up to $37\%$ relative improvement in word error rate) but also on the
unseen accents (up to $5\%$ relative improvement in WER). Further, we
illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We
also compare the performance with other approaches based on accent adversarial
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Emotion Detection in Small Imbalanced Text Data. (arXiv:2310.17015v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17015">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in text, the task of identifying emotions such as joy or
anger, is a challenging problem in NLP with many applications. One of the
challenges is the shortage of available datasets that have been annotated with
emotions. Certain existing datasets are small, follow different emotion
taxonomies and display imbalance in their emotion distribution. In this work,
we studied the impact of data augmentation techniques precisely when applied to
small imbalanced datasets, for which current state-of-the-art models (such as
RoBERTa) under-perform. Specifically, we utilized four data augmentation
methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and
ProtAugment) on three datasets that come from different sources and vary in
size, emotion categories and distributions. Our experimental results show that
using the augmented data when training the classifier model leads to
significant improvements. Finally, we conducted two case studies: a) directly
using the popular chat-GPT API to paraphrase text using different prompts, and
b) using external data to augment the training set. Results show the promising
potential of these methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v2 [cs.MS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17408">
<div class="article-summary-box-inner">
<span><p>The optimization of the matrix multiplication (or GEMM) has been a need
during the last decades. This operation is considered the flagship of current
linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its
widespread use in a large variety of scientific applications. The GEMM is
usually implemented following the GotoBLAS philosophy, which tiles the GEMM
operands and uses a series of nested loops for performance improvement. These
approaches extract the maximum computational power of the architectures through
small pieces of hardware-oriented, high-performance code called micro-kernel.
However, this approach forces developers to generate, with a non-negligible
effort, a dedicated micro-kernel for each new hardware.
</p>
<p>In this work, we present a step-by-step procedure for generating
micro-kernels with the Exo compiler that performs close to (or even better
than) manually developed microkernels written with intrinsic functions or
assembly language. Our solution also improves the portability of the generated
code, since a hardware target is fully specified by a concise library-based
description of its instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17513">
<div class="article-summary-box-inner">
<span><p>Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17526">
<div class="article-summary-box-inner">
<span><p>Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01468">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This entity-deducing game can serve as an evaluation
framework to probe the conversational reasoning and planning capabilities of
language models. We systematically evaluate various LLMs and discover
significant differences in their performance on this task. We find that strong
LLMs like GPT-4 outperform human players by a large margin. We further employ
Behavior Cloning (BC) to examine whether a weaker model is capable of imitating
a stronger model and generalizing to data or domains, using only the
demonstrations from a stronger model. We finally propose to use Reinforcement
Learning to enhance reasoning and planning capacity of Vicuna models through
episodes of game playing, which lead to significant performance improvement. We
hope that this problem offers insights into how autonomous agents could be
trained to behave more intelligently in ambiguous circumstances.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-30 23:10:55.508069609 UTC">2023-10-30 23:10:55 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>