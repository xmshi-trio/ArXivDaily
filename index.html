<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-06T01:30:00Z">06-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01739">
<div class="article-summary-box-inner">
<span><p>This work involves the usage of various NLP models to predict the winner of a
particular judgment by the means of text extraction and summarization from a
judgment document. These documents are useful when it comes to legal
proceedings. One such advantage is that these can be used for citations and
precedence reference in Lawsuits and cases which makes a strong argument for
their case by the ones using it. When it comes to precedence, it is necessary
to refer to an ample number of documents in order to collect legal points with
respect to the case. However, reviewing these documents takes a long time to
analyze due to the complex word structure and the size of the document. This
work involves the comparative study of 6 different self-attention-based
transformer models and how they perform when they are being tweaked in 4
different activation functions. These models which are trained with 200
judgement contexts and their results are being judged based on different
benchmark parameters. These models finally have a confidence level up to 99%
while predicting the judgment. This can be used to get a particular judgment
document without spending too much time searching relevant cases and reading
them completely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection. (arXiv:2306.01742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01742">
<div class="article-summary-box-inner">
<span><p>Health experts assert that hope plays a crucial role in enhancing
individuals' physical and mental well-being, facilitating their recovery, and
promoting restoration. Hope speech refers to comments, posts and other social
media messages that offer support, reassurance, suggestions, inspiration, and
insight. The detection of hope speech involves the analysis of such textual
content, with the aim of identifying messages that invoke positive emotions in
people. Our study aims to find computationally efficient yet
comparable/superior methods for hope speech detection. We also make our
codebase public at https://github.com/aflah02/Hope_Speech_Detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abugida Normalizer and Parser for Unicode texts. (arXiv:2306.01743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01743">
<div class="article-summary-box-inner">
<span><p>This paper proposes two libraries to address common and uncommon issues with
Unicode-based writing schemes for Indic languages. The first is a normalizer
that corrects inconsistencies caused by the encoding scheme
https://pypi.org/project/bnunicodenormalizer/ . The second is a grapheme parser
for Abugida text https://pypi.org/project/indicparser/ . Both tools are more
efficient and effective than previously used tools. We report 400% increase in
speed and ensure significantly better performance for different language model
based downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preconditioned Visual Language Inference with Weak Supervision. (arXiv:2306.01753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01753">
<div class="article-summary-box-inner">
<span><p>Humans can infer the affordance of objects by extracting related contextual
preconditions for each scenario. For example, upon seeing an image of a broken
cup, we can infer that this precondition prevents the cup from being used for
drinking. Reasoning with preconditions of commonsense is studied in NLP where
the model explicitly gets the contextual precondition. However, it is unclear
if SOTA visual language models (VLMs) can extract such preconditions and infer
the affordance of objects with them. In this work, we introduce the task of
preconditioned visual language inference and rationalization (PVLIR). We
propose a learning resource based on three strategies to retrieve weak
supervision signals for the task and develop a human-verified test set for
evaluation. Our results reveal the shortcomings of SOTA VLM models in the task
and draw a road map to address the challenges ahead in improving them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01755">
<div class="article-summary-box-inner">
<span><p>Text-to-image models can often generate some relations, i.e., "astronaut
riding horse", but fail to generate other relations composed of the same basic
parts, i.e., "horse riding astronaut". These failures are often taken as
evidence that the models rely on training priors rather than constructing novel
images compositionally. This paper tests this intuition directly on the
stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object
(SVO) triads that form the backbone of these prompts (e.g., "astronaut",
"ride", "horse"), we find that the more often an SVO triad appears in the
training data, the better the model can generate an image aligned with that
triad. Here, by aligned we mean that each of the terms appears in the generated
image in the proper relation to each other. However, this increased frequency
also diminishes how well the model can generate an image aligned with the
flipped triad. For example, if "astronaut riding horse" appears frequently in
the training data, the image for "horse riding astronaut" will tend to be
poorly aligned. We also find that models often struggle to generate terms in
atypical roles, e.g., if "horse" is more often the semantic patient (object),
the model might struggle to visualize it as a semantic agent (subject). Our
results thus show that current models are biased to generate images aligned
with relations seen in training and provide important new data in the ongoing
debate on whether these text-to-image models employ abstract compositional
structure in a traditional sense, or rather, interpolate between relations
explicitly seen in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01761">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a conversational artificial intelligence that is a member of the
generative pre-trained transformer of the large language model family. This
text generative model was fine-tuned by both supervised learning and
reinforcement learning so that it can produce text documents that seem to be
written by natural intelligence. Although there are numerous advantages of this
generative model, it comes with some reasonable concerns as well. This paper
presents a machine learning-based solution that can identify the ChatGPT
delivered text from the human written text along with the comparative analysis
of a total of 11 machine learning and deep learning algorithms in the
classification process. We have tested the proposed model on a Kaggle dataset
consisting of 10,000 texts out of which 5,204 texts were written by humans and
collected from news and social media. On the corpus generated by GPT-3.5, the
proposed algorithm presents an accuracy of 77%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01768">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are being scaled and becoming powerful. Improving their
efficiency is one of the core research topics in neural information processing
systems. Tay et al. (2022) provided a comprehensive overview of efficient
Transformers that have become an indispensable staple in the field of NLP.
However, in the section of "On Evaluation", they left an open question "which
fundamental efficient Transformer one should consider," answered by "still a
mystery" because "many research papers select their own benchmarks."
Unfortunately, there was not quantitative analysis about the performances of
Transformers on any benchmarks. Moreover, state space models (SSMs) have
demonstrated their abilities of modeling long-range sequences with
non-attention mechanisms, which were not discussed in the prior review. This
article makes a meta analysis on the results from a set of papers on efficient
Transformers as well as those on SSMs. It provides a quantitative review on LM
efficiency research and gives suggestions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptual Design Generation Using Large Language Models. (arXiv:2306.01779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01779">
<div class="article-summary-box-inner">
<span><p>Concept generation is a creative step in the conceptual design phase, where
designers often turn to brainstorming, mindmapping, or crowdsourcing design
ideas to complement their own knowledge of the domain. Recent advances in
natural language processing (NLP) and machine learning (ML) have led to the
rise of Large Language Models (LLMs) capable of generating seemingly creative
outputs from textual prompts. The success of these models has led to their
integration and application across a variety of domains, including art,
entertainment, and other creative work. In this paper, we leverage LLMs to
generate solutions for a set of 12 design problems and compare them to a
baseline of crowdsourced solutions. We evaluate the differences between
generated and crowdsourced design solutions through multiple perspectives,
including human expert evaluations and computational metrics. Expert
evaluations indicate that the LLM-generated solutions have higher average
feasibility and usefulness while the crowdsourced solutions have more novelty.
We experiment with prompt engineering and find that leveraging few-shot
learning can lead to the generation of solutions that are more similar to the
crowdsourced solutions. These findings provide insight into the quality of
design solutions generated with LLMs and begins to evaluate prompt engineering
techniques that could be leveraged by practitioners to generate higher-quality
design solutions synergistically with LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01789">
<div class="article-summary-box-inner">
<span><p>RNN-T is currently considered the industry standard in ASR due to its
exceptional WERs in various benchmark tests and its ability to support seamless
streaming and longform transcription. However, its biggest drawback lies in the
significant discrepancy between its training and inference objectives. During
training, RNN-T maximizes all alignment probabilities by teacher forcing, while
during inference, it uses beam search which may not necessarily find the
maximum probable alignment. Additionally, RNN-T's inability to experience
mistakes during teacher forcing training makes it more problematic when a
mistake occurs in inference. To address this issue, this paper proposes a
Reinforcement Learning method that minimizes the gap between training and
inference time. Our Edit Distance based RL (EDRL) approach computes rewards
based on the edit distance, and trains the network at every action level. The
proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. (arXiv:2306.01805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01805">
<div class="article-summary-box-inner">
<span><p>As people become more aware of their food choices, food computation models
have become increasingly popular in assisting people in maintaining healthy
eating habits. For example, food recommendation systems analyze recipe
instructions to assess nutritional contents and provide recipe recommendations.
The recent and remarkable successes of generative AI methods, such as
auto-regressive large language models, can lead to robust methods for a more
comprehensive understanding of recipes for healthy food recommendations beyond
surface-level nutrition content assessments. In this study, we explore the use
of generative AI methods to extend current food computation models, primarily
involving the analysis of nutrition and ingredients, to also incorporate
cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).
Cooking actions are notoriously hard to model using statistical learning
methods due to irregular data patterns - significantly varying natural language
descriptions for the same action (e.g., marinate the meat vs. marinate the meat
and leave overnight) and infrequently occurring patterns (e.g., add salt occurs
far more frequently than marinating the meat). The prototypical approach to
handling irregular data patterns is to increase the volume of data that the
model ingests by orders of magnitude. Unfortunately, in the cooking domain,
these problems are further compounded with larger data volumes presenting a
unique challenge that is not easily handled by simply scaling up. In this work,
we propose novel aggregation-based generative AI methods, Cook-Gen, that
reliably generate cooking actions from recipes, despite difficulties with
irregular data patterns, while also outperforming Large Language Models and
other strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01807">
<div class="article-summary-box-inner">
<span><p>Applications of Natural Language Processing (NLP) are plentiful, from
sentiment analysis to text classification. Practitioners rely on static word
embeddings (e.g. Word2Vec or GloVe) or static word representation from
contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These
widely available word embeddings are built from large amount of text, so they
are likely to have captured most of the vocabulary in different context.
However, how well would they capture domain-specific semantics and word
relatedness? This paper explores this idea by creating a bank-specific word
embeddings and evaluates them against other sources of word embeddings such as
GloVe and BERT. Not surprising that embeddings built from bank-specific corpora
does a better job of capturing the bank-specific semantics and word
relatedness. This finding suggests that bank-specific word embeddings could be
a good stand-alone source or a complement to other widely available embeddings
when performing NLP tasks specific to the banking industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01818">
<div class="article-summary-box-inner">
<span><p>Thalassemia is a group of inherited blood disorders that happen when
hemoglobin, the protein in red blood cells that carries oxygen, is not made
enough. It is found all over the body and is needed for survival. If both
parents have thalassemia, a child's chance of getting it increases. Genetic
counselling and early diagnosis are essential for treating thalassemia and
stopping it from being passed on to future generations. It may be hard for
healthcare professionals to differentiate between people with thalassemia
carriers and those without. The current blood tests for beta thalassemia
carriers are too expensive, take too long, and require too much screening
equipment. The World Health Organization says there is a high death rate for
people with thalassemia. Therefore, it is essential to find thalassemia
carriers to act quickly. High-performance liquid chromatography (HPLC), the
standard test method, has problems such as cost, time, and equipment needs. So,
there must be a quick and cheap way to find people carrying the thalassemia
gene. Using federated learning (FL) techniques, this study shows a new way to
find people with the beta-thalassemia gene. FL allows data to be collected and
processed on-site while following privacy rules, making it an excellent choice
for sensitive health data. Researchers used FL to train a model for
beta-thalassemia carriers by looking at the complete blood count results and
red blood cell indices. The model was 92.38 % accurate at telling the
difference between beta-thalassemia carriers and people who did not have the
disease. The proposed FL model is better than other published methods in terms
of how well it works, how reliable it is, and how private it is. This research
shows a promising, quick, accurate, and low-cost way to find thalassemia
carriers and opens the door for screening them on a large scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binary and Ternary Natural Language Generation. (arXiv:2306.01841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01841">
<div class="article-summary-box-inner">
<span><p>Ternary and binary neural networks enable multiplication-free computation and
promise multiple orders of magnitude efficiency gains over full-precision
networks if implemented on specialized hardware. However, since both the
parameter and the output space are highly discretized, such networks have
proven very difficult to optimize. The difficulties are compounded for the
class of transformer text generation models due to the sensitivity of the
attention operation to quantization and the noise-compounding effects of
autoregressive decoding in the high-cardinality output space. We approach the
problem with a mix of statistics-based quantization for the weights and elastic
quantization of the activations and demonstrate the first ternary and binary
transformer models on the downstream tasks of summarization and machine
translation. Our ternary BART base achieves an R1 score of 41 on the
CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while
being 16x more efficient. Our binary model, while less accurate, achieves a
highly non-trivial score of 35.6. For machine translation, we achieved BLEU
scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full
precision mBART model score of 26.8. We also compare our approach in the 8-bit
activation setting, where our ternary and even binary weight models can match
or outperform the best existing 8-bit weight models in the literature. Our code
and models are available at:
https://github.com/facebookresearch/Ternary_Binary_Transformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01855">
<div class="article-summary-box-inner">
<span><p>Providing voice assistants the ability to navigate multi-turn conversations
is a challenging problem. Handling multi-turn interactions requires the system
to understand various conversational use-cases, such as steering, intent
carryover, disfluencies, entity carryover, and repair. The complexity of this
problem is compounded by the fact that these use-cases mix with each other,
often appearing simultaneously in natural language. This work proposes a
non-autoregressive query rewriting architecture that can handle not only the
five aforementioned tasks, but also complex compositions of these use-cases. We
show that our proposed model has competitive single task performance compared
to the baseline approach, and even outperforms a fine-tuned T5 model in
use-case compositions, despite being 15 times smaller in parameters and 25
times faster in latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge of cultural moral norms in large language models. (arXiv:2306.01857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01857">
<div class="article-summary-box-inner">
<span><p>Moral norms vary across cultures. A recent line of work suggests that English
large language models contain human-like moral biases, but these studies
typically do not examine moral variation in a diverse cultural setting. We
investigate the extent to which monolingual English language models contain
knowledge about moral norms in different countries. We consider two levels of
analysis: 1) whether language models capture fine-grained moral variation
across countries over a variety of topics such as ``homosexuality'' and
``divorce''; 2) whether language models capture cultural diversity and shared
tendencies in which topics people around the globe tend to diverge or agree on
in their moral judgment. We perform our analyses with two public datasets from
the World Values Survey (across 55 countries) and PEW global surveys (across 40
countries) on morality. We find that pre-trained English language models
predict empirical moral norms across countries worse than the English moral
norms reported previously. However, fine-tuning language models on the survey
data improves inference across countries at the expense of a less accurate
estimate of the English moral norms. We discuss the relevance and challenges of
incorporating cultural knowledge into the automated inference of moral norms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01879">
<div class="article-summary-box-inner">
<span><p>Vision-language models (VLMs) discriminatively pre-trained with contrastive
image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$
have been criticized for lacking compositional understanding. This means they
might output similar scores even if the original caption is rearranged into a
different semantic statement. To address this, we propose to use the ${\bf
V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf
VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal
generative}$ score that captures the likelihood of a text caption conditioned
on an image using an image-conditioned language model. Contrary to the belief
that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore
demonstrates top-tier performance on recently proposed image-text retrieval
benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore,
we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text)
and the $\textit{Pointwise Mutual Information}$ (PMI). This helps to (a)
diagnose datasets with strong language bias, and (b) debias results on other
benchmarks like Winoground using an information-theoretic framework.
VisualGPTScore provides valuable insights and serves as a strong baseline for
future evaluation of visio-linguistic compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v19 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrastic Representations at Scale. (arXiv:2104.15114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.15114">
<div class="article-summary-box-inner">
<span><p>We present a system that allows users to train their own state-of-the-art
paraphrastic sentence representations in a variety of languages. We also
release trained models for English, Arabic, German, French, Spanish, Russian,
Turkish, and Chinese. We train these models on large amounts of data, achieving
significantly improved performance from the original papers proposing the
methods on a suite of monolingual semantic similarity, cross-lingual semantic
similarity, and bitext mining tasks. Moreover, the resulting models surpass all
prior work on unsupervised semantic textual similarity, significantly
outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych,
2019). Additionally, our models are orders of magnitude faster than prior work
and can be used on CPU with little difference in inference speed (even improved
speed over GPU when using more CPU cores), making these models an attractive
choice for users without access to GPUs or for use on embedded devices.
Finally, we add significantly increased functionality to the code bases for
training paraphrastic sentence models, easing their use for both inference and
for training them for any desired language with parallel data. We also include
code to automatically download and preprocess training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02842">
<div class="article-summary-box-inner">
<span><p>Selecting suitable architecture parameters and training hyperparameters is
essential for enhancing machine learning (ML) model performance. Several recent
empirical studies conduct large-scale correlational analysis on neural networks
(NNs) to search for effective \emph{generalization metrics} that can guide this
type of model selection. Effective metrics are typically expected to correlate
strongly with test performance. In this paper, we expand on prior analyses by
examining generalization-metric-based model selection with the following
objectives: (i) focusing on natural language processing (NLP) tasks, as prior
work primarily concentrates on computer vision (CV) tasks; (ii) considering
metrics that directly predict \emph{test error} instead of the
\emph{generalization gap}; (iii) exploring metrics that do not need access to
data to compute. From these objectives, we are able to provide the first model
selection results on large pretrained Transformers from Huggingface using
generalization metrics. Our analyses consider (I) hundreds of Transformers
trained in different settings, in which we systematically vary the amount of
data, the model size and the optimization hyperparameters, (II) a total of 51
pretrained Transformers from eight families of Huggingface NLP models,
including GPT2, BERT, etc., and (III) a total of 28 existing and novel
generalization metrics. Despite their niche status, we find that metrics
derived from the heavy-tail (HT) perspective are particularly useful in NLP
tasks, exhibiting stronger correlations than other, more popular metrics. To
further examine these metrics, we extend prior formulations relying on power
law (PL) spectral distributions to exponential (EXP) and
exponentially-truncated power law (E-TPL) families.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMER: Multimodal Multi-task Learning for Speech Emotion Recognition. (arXiv:2203.16794v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16794">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose MMER, a novel Multimodal Multi-task learning
approach for Speech Emotion Recognition. MMER leverages a novel multimodal
network based on early-fusion and cross-modal self-attention between text and
acoustic modalities and solves three novel auxiliary tasks for learning emotion
recognition from spoken utterances. In practice, MMER outperforms all our
baselines and achieves state-of-the-art performance on the IEMOCAP benchmark.
Additionally, we conduct extensive ablation studies and results analysis to
prove the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are We Really Making Much Progress in Text Classification? A Comparative Review. (arXiv:2204.03954v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03954">
<div class="article-summary-box-inner">
<span><p>This study reviews and compares methods for single-label and multi-label text
classification, categorized into bag-of-words, sequence-based, graph-based, and
hierarchical methods. The comparison aggregates results from the literature
over five single-label and seven multi-label datasets and complements them with
new experiments. The findings reveal that all recently proposed graph-based and
hierarchy-based methods fail to outperform pre-trained language models and
sometimes perform worse than standard machine learning methods like a
multilayer perceptron on a bag-of-words. To assess the true scientific progress
in text classification, future work should thoroughly test against strong
bag-of-words baselines and state-of-the-art pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05428">
<div class="article-summary-box-inner">
<span><p>Most evaluations of attribution methods focus on the English language. In
this work, we present a multilingual approach for evaluating attribution
methods for the Natural Language Inference (NLI) task in terms of faithfulness
and plausibility. First, we introduce a novel cross-lingual strategy to measure
faithfulness based on word alignments, which eliminates the drawbacks of
erasure-based evaluations.We then perform a comprehensive evaluation of
attribution methods, considering different output mechanisms and aggregation
methods. Finally, we augment the XNLI dataset with highlight-based
explanations, providing a multilingual NLI dataset with highlights, to support
future exNLP studies. Our results show that attribution methods performing best
for plausibility and faithfulness are different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10282">
<div class="article-summary-box-inner">
<span><p>Representation learning on networks aims to derive a meaningful vector
representation for each node, thereby facilitating downstream tasks such as
link prediction, node classification, and node clustering. In heterogeneous
text-rich networks, this task is more challenging due to (1) presence or
absence of text: Some nodes are associated with rich textual information, while
others are not; (2) diversity of types: Nodes and edges of multiple types form
a heterogeneous network structure. As pretrained language models (PLMs) have
demonstrated their effectiveness in obtaining widely generalizable text
representations, a substantial amount of effort has been made to incorporate
PLMs into representation learning on text-rich networks. However, few of them
can jointly consider heterogeneous structure (network) information as well as
rich textual semantic information of each node effectively. In this paper, we
propose Heterformer, a Heterogeneous Network-Empowered Transformer that
performs contextualized text encoding and heterogeneous structure encoding in a
unified model. Specifically, we inject heterogeneous structure information into
each Transformer layer when encoding node texts. Meanwhile, Heterformer is
capable of characterizing node/edge type heterogeneity and encoding nodes with
or without texts. We conduct comprehensive experiments on three tasks (i.e.,
link prediction, node classification, and node clustering) on three large-scale
datasets from different domains, where Heterformer outperforms competitive
baselines significantly and consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifieR: A Unified Retriever for Large-Scale Retrieval. (arXiv:2205.11194v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11194">
<div class="article-summary-box-inner">
<span><p>Large-scale retrieval is to recall relevant documents from a huge collection
given a query. It relies on representation learning to embed documents and
queries into a common semantic encoding space. According to the encoding space,
recent retrieval methods based on pre-trained language models (PLM) can be
coarsely categorized into either dense-vector or lexicon-based paradigms. These
two paradigms unveil the PLMs' representation capability in different
granularities, i.e., global sequence-level compression and local word-level
contexts, respectively. Inspired by their complementary global-local
contextualization and distinct representing views, we propose a new learning
framework, UnifieR which unifies dense-vector and lexicon-based retrieval in
one model with a dual-representing capability. Experiments on passage retrieval
benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme
is further presented with even better retrieval quality. We lastly evaluate the
model on BEIR benchmark to verify its transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12677">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained language models can learn task-specific abilities or
memorize facts across multiple languages but inevitably make undesired
predictions with specific inputs. Under similar observation, model editing aims
to post-hoc calibrate a model targeted to specific inputs with keeping the
model's raw behavior. However, existing work only studies the monolingual
scenario, which lacks the cross-lingual transferability to perform editing
simultaneously across languages. In this work, we focus on cross-lingual model
editing. Firstly, we define the cross-lingual model editing task and
corresponding metrics, where an edit in one language propagates to the others.
Next, we propose a framework to naturally adapt monolingual model editing
approaches to the cross-lingual scenario using parallel corpus. Further, we
propose language anisotropic editing to improve cross-lingual editing by
amplifying different subsets of parameters for each language. On the newly
defined cross-lingual model editing task, we empirically demonstrate the
failure of monolingual baselines in propagating the edit to multiple languages
and the effectiveness of the proposed language anisotropic model editing. Our
code is publicly available at https://github.com/franklear/LiME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15171">
<div class="article-summary-box-inner">
<span><p>Societal biases are reflected in large pre-trained language models and their
fine-tuned versions on downstream tasks. Common in-processing bias mitigation
approaches, such as adversarial training and mutual information removal,
introduce additional optimization criteria, and update the model to reach a new
debiased state. However, in practice, end-users and practitioners might prefer
to switch back to the original model, or apply debiasing only on a specific
subset of protected attributes. To enable this, we propose a novel modular bias
mitigation approach, consisting of stand-alone highly sparse debiasing
subnetworks, where each debiasing module can be integrated into the core model
on-demand at inference time. Our approach draws from the concept of \emph{diff}
pruning, and proposes a novel training regime adaptable to various
representation disentanglement optimizations. We conduct experiments on three
classification tasks with gender, race, and age as protected attributes. The
results show that our modular approach, while maintaining task performance,
improves (or at least remains on-par with) the effectiveness of bias mitigation
in comparison with baseline finetuning. Particularly on a two-attribute
dataset, our approach with separately learned debiasing subnetworks shows
effective utilization of either or both the subnetworks for selective bias
mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03382">
<div class="article-summary-box-inner">
<span><p>Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep
learning models to trillion-plus parameters with fixed computational cost. The
algorithmic performance of MoE relies on its token routing mechanism that
forwards each input token to the right sub-models or experts. While token
routing dynamically determines the amount of expert workload at runtime,
existing systems suffer inefficient computation due to their static execution,
namely static parallelism and pipelining, which does not adapt to the dynamic
workload. We present Flex, a highly scalable stack design and implementation
for MoE with dynamically adaptive parallelism and pipelining. Flex designs an
identical layout for distributing MoE model parameters and input data, which
can be leveraged by all possible parallelism or pipelining methods without any
mathematical inequivalence or tensor migration overhead. This enables adaptive
parallelism/pipelining optimization at zero cost during runtime. Based on this
key design, Flex also implements various MoE acceleration techniques.
Aggregating all techniques, Flex finally delivers huge speedup at any scale --
4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs,
respectively, over the previous state-of-the-art. Our evaluation shows that
Flex efficiently and effectively runs a real-world MoE-based model named
SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision
architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x
and 2.11x speedup in training and inference over Fairseq, respectively. On
effectiveness, the SwinV2-MoE model achieves superior accuracy in both
pre-training and down-stream computer vision tasks such as COCO object
detection than the counterpart dense model, indicating the readiness of Flex
for end-to-end real-world model training and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14578">
<div class="article-summary-box-inner">
<span><p>Generative language models are promising for assisting human writing in
various domains. This manuscript aims to build generative language models in
the patent domain and evaluate model performance from a human-centric
perspective. The perspective is to measure the ratio of keystrokes that can be
saved by autocompletion based on generative patent language models. A higher
ratio means a more effective model which can save more keystrokes. This metric
can be used to benchmark model performance. The metric is different from
conventional machine-centric metrics that are token-based instead of
keystroke-based. In terms of model size, the largest model built in this
manuscript is 6B, which is state-of-the-art in the patent domain. Based on the
metric, it is found that the largest model is not necessarily the best for the
human-centric metric. The finding means that keeping increasing model sizes in
the patent domain might be unnecessary if the purpose is to assist human
writing with autocompletion. Several patent language models are pre-trained
from scratch in this research. The pre-trained models are released for future
researchers. Several visualization tools are also provided. The importance of
building a generative language model in the patent domain is the potential to
facilitate creativity and innovations in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05135">
<div class="article-summary-box-inner">
<span><p>Learning fine-grained movements is a challenging topic in robotics,
particularly in the context of robotic hands. One specific instance of this
challenge is the acquisition of fingerspelling sign language in robots. In this
paper, we propose an approach for learning dexterous motor imitation from video
examples without additional information. To achieve this, we first build a URDF
model of a robotic hand with a single actuator for each joint. We then leverage
pre-trained deep vision models to extract the 3D pose of the hand from RGB
videos. Next, using state-of-the-art reinforcement learning algorithms for
motion imitation (namely, proximal policy optimization and soft actor-critic),
we train a policy to reproduce the movement extracted from the demonstrations.
We identify the optimal set of hyperparameters for imitation based on a
reference motion. Finally, we demonstrate the generalizability of our approach
by testing it on six different tasks, corresponding to fingerspelled letters.
Our results show that our approach is able to successfully imitate these
fine-grained movements without additional information, highlighting its
potential for real-world applications in robotics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Generative & Dense Retrieval for Query Rewriting in Sponsored Search. (arXiv:2209.05861v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05861">
<div class="article-summary-box-inner">
<span><p>Sponsored search is a key revenue source for search engines, where
advertisers bid on keywords to target users or search queries of interest.
However, finding relevant keywords for a given query is challenging due to the
large and dynamic keyword space, ambiguous user/advertiser intents, and diverse
possible topics and languages. In this work, we present a comprehensive
comparison between two paradigms for online query rewriting: Generative (NLG)
and Dense Retrieval (DR) methods. We observe that both methods offer
complementary benefits that are additive. As a result, we show that around 40%
of the high-quality keywords retrieved by the two approaches are unique and not
retrieved by the other. To leverage the strengths of both methods, we propose
CLOVER-Unity, a novel approach that unifies generative and dense retrieval
methods in one single model. Through offline experiments, we show that the NLG
and DR components of CLOVER-Unity consistently outperform individually trained
NLG and DR models on public and internal benchmarks. Furthermore, we show that
CLOVER-Unity achieves 9.8% higher good keyword density than the ensemble of two
separate DR and NLG models while reducing computational costs by almost half.
We conduct extensive online A/B experiments on Microsoft Bing in 140+ countries
and achieve improved user engagement, with an average increase in total clicks
by 0.89% and increased revenue by 1.27%. We also share our practical lessons
and optimization tricks for deploying such unified models in production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06794">
<div class="article-summary-box-inner">
<span><p>Effective scaling and a flexible task interface enable large language models
to excel at many tasks. We present PaLI (Pathways Language and Image model), a
model that extends this approach to the joint modeling of language and vision.
PaLI generates text based on visual and textual inputs, and with this interface
performs many vision, language, and multimodal tasks, in many languages. To
train PaLI, we make use of large pre-trained encoder-decoder language models
and Vision Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the
benefits from even larger-capacity vision models. To train PaLI, we create a
large multilingual mix of pretraining tasks, based on a new image-text training
set containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation. (arXiv:2209.13885v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13885">
<div class="article-summary-box-inner">
<span><p>Personalized natural language generation for explainable recommendations
plays a key role in justifying why a recommendation might match a user's
interests. Existing models usually control the generation process by aspect
planning. While promising, these aspect-planning methods struggle to generate
specific information correctly, which prevents generated explanations from
being convincing. In this paper, we claim that introducing lexical constraints
can alleviate the above issues. We propose a model, UCEpic, that generates
high-quality personalized explanations for recommendation results by unifying
aspect planning and lexical constraints in an insertion-based generation
manner.
</p>
<p>Methodologically, to ensure text generation quality and robustness to various
lexical constraints, we pre-train a non-personalized text generator via our
proposed robust insertion process. Then, to obtain personalized explanations
under this framework of insertion-based generation, we design a method of
incorporating aspect planning and personalized references into the insertion
process. Hence, UCEpic unifies aspect planning and lexical constraints into one
framework and generates explanations for recommendations under different
settings. Compared to previous recommendation explanation generators controlled
by only aspects, UCEpic incorporates specific information from keyphrases and
then largely improves the diversity and informativeness of generated
explanations for recommendations on datasets such as RateBeer and Yelp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04183">
<div class="article-summary-box-inner">
<span><p>Multimodal representation learning has shown promising improvements on
various vision-language tasks. Most existing methods excel at building
global-level alignment between vision and language while lacking effective
fine-grained image-text interaction. In this paper, we propose a jointly masked
multimodal modeling method to learn fine-grained multimodal representations.
Our method performs joint masking on image-text input and integrates both
implicit and explicit targets for the masked signals to recover. The implicit
target provides a unified and debiased objective for vision and language, where
the model predicts latent multimodal representations of the unmasked input. The
explicit target further enriches the multimodal representations by recovering
high-level and semantically meaningful information: momentum visual features of
image patches and concepts of word tokens. Through such a masked modeling
process, our model not only learns fine-grained multimodal interaction, but
also avoids the semantic gap between high-level representations and low- or
mid-level prediction targets (e.g. image pixels), thus producing semantically
rich multimodal representations that perform well on both zero-shot and
fine-tuned settings. Our pre-trained model (named MAMO) achieves
state-of-the-art performance on various downstream vision-language tasks,
including image-text retrieval, visual question answering, visual reasoning,
and weakly-supervised visual grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06068">
<div class="article-summary-box-inner">
<span><p>Massively multilingual pre-trained language models (MMPLMs) are developed in
recent years demonstrating superpowers and the pre-knowledge they acquire for
downstream tasks. This work investigates whether MMPLMs can be applied to
clinical domain machine translation (MT) towards entirely unseen languages via
transfer learning. We carry out an experimental investigation using Meta-AI's
MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained
on 7 language pairs and 14 translation directions including English to Czech,
German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite
direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language
pair which \textit{did not exist at all} in their original pre-trained corpora
both implicitly and explicitly. We prepare carefully aligned \textit{clinical}
domain data for this fine-tuning, which is different from their original mixed
domain knowledge. Our experimental result shows that the fine-tuning is very
successful using just 250k well-aligned in-domain EN-ES segments for three
sub-task translation testings: clinical cases, clinical terms, and ontology
concepts. It achieves very close evaluation scores to another MMPLM NLLB from
Meta-AI, which included Spanish as a high-resource setting in the pre-training.
To the best of our knowledge, this is the first work on using MMPLMs towards
\textit{clinical domain transfer-learning NMT} successfully for totally unseen
languages during pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks. (arXiv:2212.05251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05251">
<div class="article-summary-box-inner">
<span><p>By focusing the pre-training process on domain-specific corpora, some
domain-specific pre-trained language models (PLMs) have achieved
state-of-the-art results. However, it is under-investigated to design a unified
paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose
KnowledgeDA, a unified domain language model development service to enhance the
task-specific training procedure with domain knowledge graphs. Given
domain-specific task texts input, KnowledgeDA can automatically generate a
domain-specific language model following three steps: (i) localize domain
knowledge entities in texts via an embedding-similarity approach; (ii) generate
augmented samples by retrieving replaceable domain entity pairs from two views
of both knowledge graph and training data; (iii) select high-quality augmented
samples for fine-tuning via confidence-based assessment. We implement a
prototype of KnowledgeDA to learn language models for two domains, healthcare
and software development. Experiments on domain-specific text classification
and QA tasks verify the effectiveness and generalizability of KnowledgeDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. (arXiv:2212.08061v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08061">
<div class="article-summary-box-inner">
<span><p>Generating a Chain of Thought (CoT) has been shown to consistently improve
large language model (LLM) performance on a wide range of NLP tasks. However,
prior work has mainly focused on logical reasoning tasks (e.g. arithmetic,
commonsense QA); it remains unclear whether improvements hold for more diverse
types of reasoning, especially in socially situated contexts. Concretely, we
perform a controlled evaluation of zero-shot CoT across two socially sensitive
domains: harmful questions and stereotype benchmarks. We find that zero-shot
CoT reasoning in sensitive domains significantly increases a model's likelihood
to produce harmful or undesirable output, with trends holding across different
prompt formats and model variants. Furthermore, we show that harmful CoTs
increase with model size, but decrease with improved instruction following. Our
work suggests that zero-shot CoT should be used with caution on socially
important tasks, especially when marginalized groups or sensitive topics are
involved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08153">
<div class="article-summary-box-inner">
<span><p>Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09282">
<div class="article-summary-box-inner">
<span><p>Logical reasoning of text is an important ability that requires understanding
the information present in the text, their interconnections, and then reasoning
through them to infer new conclusions. Prior works on improving the logical
reasoning ability of language models require complex processing of training
data (e.g., aligning symbolic knowledge to text), yielding task-specific data
augmentation solutions that restrict the learning of general logical reasoning
skills. In this work, we propose APOLLO, an adaptively pretrained language
model that has improved logical reasoning abilities. We select a subset of
Wikipedia, based on a set of logical inference keywords, for continued
pretraining of a language model. We use two self-supervised loss functions: a
modified masked language modeling loss where only specific parts-of-speech
words, that would likely require more reasoning than basic language
understanding, are masked, and a sentence-level classification loss that
teaches the model to distinguish between entailment and contradiction types of
sentences. The proposed training paradigm is both simple and independent of
task formats. We demonstrate the effectiveness of APOLLO by comparing it with
prior baselines on two logical reasoning datasets. APOLLO performs comparably
on ReClor and outperforms baselines on LogiQA. The code base has been made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09648">
<div class="article-summary-box-inner">
<span><p>We present NusaCrowd, a collaborative initiative to collect and unify
existing resources for Indonesian languages, including opening access to
previously non-public resources. Through this initiative, we have brought
together 137 datasets and 118 standardized data loaders. The quality of the
datasets has been assessed manually and automatically, and their value is
demonstrated through multiple experiments. NusaCrowd's data collection enables
the creation of the first zero-shot benchmarks for natural language
understanding and generation in Indonesian and the local languages of
Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual
automatic speech recognition benchmark in Indonesian and the local languages of
Indonesia. Our work strives to advance natural language processing (NLP)
research for languages that are under-represented despite being widely spoken.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09849">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations. (arXiv:2212.09865v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09865">
<div class="article-summary-box-inner">
<span><p>Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09955">
<div class="article-summary-box-inner">
<span><p>The proliferation of automatic faithfulness metrics for summarization has
produced a need for benchmarks to evaluate them. While existing benchmarks
measure the correlation with human judgements of faithfulness on
model-generated summaries, they are insufficient for diagnosing whether metrics
are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced
into a summary, 2) effective on human-written texts, and 3) sensitive to
different error types (as summaries can contain multiple errors). To address
these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a
dataset of 889 human-written, minimally different summary pairs, where a single
error is introduced to a summary from the CNN/DailyMail dataset to produce an
unfaithful summary. We find BUMP complements existing benchmarks in a number of
ways: 1) the summaries in BUMP are harder to discriminate and less probable
under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be
used to measure the consistency of metrics, and reveals that the most
discriminative metrics tend not to be the most consistent, and 3) unlike
datasets containing generated summaries with multiple errors, BUMP enables the
measurement of metrics' performance on individual error types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10726">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been successfully used for retrieval of semantically
aligned sentences, but it often requires large batch sizes or careful
engineering to work well. In this paper, we instead propose a generative model
for learning multilingual text embeddings which can be used to retrieve or
score sentence pairs. Our model operates on parallel data in $N$ languages and,
through an approximation we introduce, efficiently encourages source separation
in this multilingual setting, separating semantic information that is shared
between translations from stylistic or language-specific variation. We show
careful large-scale comparisons between contrastive and generation-based
approaches for learning multilingual text embeddings, a comparison that has not
been done to the best of our knowledge despite the popularity of these
approaches. We evaluate this method on a suite of tasks including semantic
similarity, bitext mining, and cross-lingual question retrieval -- the last of
which we introduce in this paper. Overall, our Variational Multilingual
Source-Separation Transformer (VMSST) model outperforms both a strong
contrastive and generative baseline on these tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10786">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10448">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented language models such as Fusion-in-Decoder are powerful,
setting the state of the art on a variety of knowledge-intensive tasks.
However, they are also expensive, due to the need to encode a large number of
retrieved passages. Some work avoids this cost by pre-encoding a text corpus
into a memory and retrieving dense representations directly. However,
pre-encoding memory incurs a severe quality penalty as the memory
representations are not conditioned on the current input. We propose LUMEN, a
hybrid between these two extremes, pre-computing the majority of the retrieval
representation and completing the encoding on the fly using a live encoder that
is conditioned on the question and fine-tuned for the task. We show that LUMEN
significantly outperforms pure memory on multiple question-answering tasks
while being much cheaper than FiD, and outperforms both for any given compute
budget. Moreover, the advantage of LUMEN over FiD increases with model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10521">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that inducing a large language model (LLM) to generate
explanations prior to outputting an answer is an effective strategy to improve
performance on a wide range of reasoning tasks. In this work, we show that
neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to
augment retrieval datasets with explanations and train a sequence-to-sequence
ranking model to output a relevance label and an explanation for a given
query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand
examples with synthetic explanations performs on par with models finetuned on
3x more examples without explanations. Furthermore, the ExaRanker model incurs
no additional computational cost during ranking and allows explanations to be
requested on demand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11716">
<div class="article-summary-box-inner">
<span><p>The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed to reduce
this gap, but most of them require architectural changes in ST training. In
this work, we propose to mitigate this issue at the pre-training stage,
requiring no change in the ST model. First, we show that the connectionist
temporal classification (CTC) loss can reduce the modality gap by design. We
provide a quantitative comparison with the more common cross-entropy loss,
showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models. Code and pre-trained models are available at
https://github.com/formiel/fairseq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11042">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) is a powerful paradigm emerged from large language
models (LLMs). Despite its promises, ICL performance is known to be highly
sensitive to input examples. In this work, we use $\textit{in-context
influences}$ to analyze few-shot ICL performance directly from the in-context
examples. Our proposed influence-based example selection method can identify
both positive and negative examples, outperforming several baselines when
evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$
performance gap between using the most negative in-context examples compared to
the most positive. In a case study, we apply our influence-based framework to
quantify the phenomena of recency bias in example ordering for few-shot ICL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07205">
<div class="article-summary-box-inner">
<span><p>The emergence of large language models (LLMs) has resulted in the production
of LLM-generated texts that is highly sophisticated and almost
indistinguishable from texts written by humans. However, this has also sparked
concerns about the potential misuse of such texts, such as spreading
misinformation and causing disruptions in the education system. Although many
detection approaches have been proposed, a comprehensive understanding of the
achievements and challenges is still lacking. This survey aims to provide an
overview of existing LLM-generated text detection techniques and enhance the
control and regulation of language generation models. Furthermore, we emphasize
crucial considerations for future research, including the development of
comprehensive evaluation metrics and the threat posed by open-source LLMs, to
drive progress in the area of LLM-generated text detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10330">
<div class="article-summary-box-inner">
<span><p>Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead. Codes are released at
https://github.com/Yuanhy1997/PartialKB-EL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02721">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence language models can be used to produce abstractive
summaries which are coherent, relevant, and concise. Still, model sizes can
make deployment in latency-sensitive or web-scale implementations difficult.
This paper studies the relationship between model size, structured pruning,
inference efficiency, and summarization accuracy on widely used summarization
datasets. We show that model accuracy is tied to the encoder size while
inference efficiency is connected to the decoder. Using asymmetric pruning can
lead to nearly 3x improvement in inference latency with ~1 point loss in
Rouge-2. Moreover, we find both the average degradation and the role of
asymmetry to be consistent across model sizes and variations in datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05534">
<div class="article-summary-box-inner">
<span><p>In the first half of 2023, text-generative artificial intelligence (AI),
including ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted
considerable attention worldwide. In this study, first, we compared Japanese
stylometric features of texts generated by GPT (-3.5 and -4) and those written
by humans. In this work, we performed multi-dimensional scaling (MDS) to
confirm the distributions of 216 texts of three classes (72 academic papers
written by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts
generated by GPT-4 on the basis of the titles of the aforementioned papers)
focusing on the following stylometric features: (1) bigrams of parts-of-speech,
(2) bigram of postpositional particle words, (3) positioning of commas, and (4)
rate of function words. MDS revealed distinct distributions at each stylometric
feature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than
GPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions
are likely to overlap. These results indicate that although the number of
parameters may increase in the future, GPT-generated texts may not be close to
that written by humans in terms of stylometric features. Second, we verified
the classification performance of random forest (RF) for two classes (GPT and
human) focusing on Japanese stylometric features. This study revealed the high
performance of RF in each stylometric feature: The RF classifier focusing on
the rate of function words achieved 98.1% accuracy. Furthermore the RF
classifier focusing on all stylometric features reached 100% in terms of all
performance indexes (accuracy, recall, precision, and F1 score). This study
concluded that at this stage we human discriminate ChatGPT from human limited
to Japanese language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations. (arXiv:2304.08216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08216">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversations (ERC) has been gaining increasing
importance as conversational agents become more and more common. Recognizing
emotions is key for effective communication, being a crucial component in the
development of effective and empathetic conversational agents. Knowledge and
understanding of the conversational context are extremely valuable for
identifying the emotions of the interlocutor. We thus approach Emotion
Recognition in Conversations leveraging the conversational context, i.e.,
taking into attention previous conversational turns. The usual approach to
model the conversational context has been to produce context-independent
representations of each utterance and subsequently perform contextual modeling
of these. Here we propose context-dependent embedding representations of each
utterance by leveraging the contextual representational power of pre-trained
transformer language models. In our approach, we feed the conversational
context appended to the utterance to be classified as input to the RoBERTa
encoder, to which we append a simple classification module, thus discarding the
need to deal with context after obtaining the embeddings since these constitute
already an efficient representation of such context. We also investigate how
the number of introduced conversational turns influences our model performance.
The effectiveness of our approach is validated on the open-domain DailyDialog
dataset and on the task-oriented EmoWOZ dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Inter-Bilingual Semantic Parsing for Indian Languages. (arXiv:2304.13005v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13005">
<div class="article-summary-box-inner">
<span><p>Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report on Token Position Bias in Transformers. (arXiv:2304.13567v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13567">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, specifically in terms of the ratio of positive to negative
examples, and class imbalance. In this paper, we investigate an additional
specific issue for language models, namely the position bias of positive
examples in token classification tasks. Therefore, we conduct an in-depth
evaluation of the impact of position bias on the performance of LMs when
fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and
OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We
propose an evaluation approach to investigate position bias in Transformer
models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as
GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in
their performance. To mitigate this effect, we propose two methods: Random
Position Shifting and Context Perturbation, that we apply on batches during the
training process. The results show an improvement of $\approx$ 2\% in the
performance of the model on CoNLL03, UD_en, and TweeBank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models. (arXiv:2305.02220v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02220">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03506">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation~(ERC) across modalities is of vital
importance for a variety of applications, including intelligent healthcare,
artificial intelligence for conversation, and opinion mining over chat history.
The crux of ERC is to model both cross-modality and cross-time interactions
throughout the conversation. Previous methods have made progress in learning
the time series information of conversation while lacking the ability to trace
down the different emotional states of each speaker in a conversation. In this
paper, we propose a recurrent structure called Speaker Information Enhanced
Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states
of the distinct speaker can be tracked in a sequential way to enhance the
learning of the emotion in conversation. Further, to improve the learning of
multimodal features in ERC, we utilize a cross-modal attention component to
fuse the features between different modalities and model the interaction of the
important information from different modalities. Experimental results on two
benchmark datasets demonstrate the superiority of the proposed SI-LSTM against
the state-of-the-art baseline methods in the ERC task on multimodal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03517">
<div class="article-summary-box-inner">
<span><p>Incorporating auxiliary modalities such as images into event detection models
has attracted increasing interest over the last few years. The complexity of
natural language in describing situations has motivated researchers to leverage
the related visual context to improve event detection performance. However,
current approaches in this area suffer from data scarcity, where a large amount
of labelled text-image pairs are required for model training. Furthermore,
limited access to the visual context at inference time negatively impacts the
performance of such models, which makes them practically ineffective in
real-world scenarios. In this paper, we present a novel domain-adaptive
visually-fused event detection approach that can be trained on a few labelled
image-text paired data points. Specifically, we introduce a visual imaginator
method that synthesises images from text in the absence of visual context.
Moreover, the imaginator can be customised to a specific domain. In doing so,
our model can leverage the capabilities of pre-trained vision-language models
and can be trained in a few-shot setting. This also allows for effective
inference where only single-modality data (i.e. text) is available. The
experimental evaluation on the benchmark M2E2 dataset shows that our model
outperforms existing state-of-the-art models, by up to 11 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04087">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04492">
<div class="article-summary-box-inner">
<span><p>Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-Side Augmentation for Document-Level Machine Translation. (arXiv:2305.04505v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04505">
<div class="article-summary-box-inner">
<span><p>Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Radiology Report Generation by Infusing Comparison Prior. (arXiv:2305.04561v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04561">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based models have made significant strides in generating
radiology reports from chest X-ray images. However, a prominent challenge
remains: these models often lack prior knowledge, resulting in the generation
of synthetic reports that mistakenly reference non-existent prior exams. This
discrepancy can be attributed to a knowledge gap between radiologists and the
generation models. While radiologists possess patient-specific prior
information, the models solely receive X-ray images at a specific time point.
To tackle this issue, we propose a novel approach that leverages a rule-based
labeler to extract comparison prior information from radiology reports. This
extracted comparison prior is then seamlessly integrated into state-of-the-art
transformer-based models, enabling them to produce more realistic and
comprehensive reports. Our method is evaluated on English report datasets, such
as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses
baseline models in terms of natural language generation metrics. Notably, our
model generates reports that are free from false references to non-existent
prior exams, setting it apart from previous models. By addressing this
limitation, our approach represents a significant step towards bridging the gap
between radiologists and generation models in the domain of medical report
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06566">
<div class="article-summary-box-inner">
<span><p>Personalized news recommendation systems have become essential tools for
users to navigate the vast amount of online news content, yet existing news
recommenders face significant challenges such as the cold-start problem, user
profile modeling, and news content understanding. Previous works have typically
followed an inflexible routine to address a particular challenge through model
design, but are limited in their ability to understand news content and capture
user interests. In this paper, we introduce GENRE, an LLM-powered generative
news recommendation framework, which leverages pretrained semantic knowledge
from large language models to enrich news data. Our aim is to provide a
flexible and unified solution for news recommendation by moving from model
design to prompt design. We showcase the use of GENRE for personalized news
generation, user profiling, and news summarization. Extensive experiments with
various popular recommendation models demonstrate the effectiveness of GENRE.
We will publish our code and data for other researchers to reproduce our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08195">
<div class="article-summary-box-inner">
<span><p>Interactive semantic parsing based on natural language (NL) feedback, where
users provide feedback to correct the parser mistakes, has emerged as a more
practical scenario than the traditional one-shot semantic parsing. However,
prior work has heavily relied on human-annotated feedback data to train the
interactive semantic parser, which is prohibitively expensive and not scalable.
In this work, we propose a new task of simulating NL feedback for interactive
semantic parsing. We accompany the task with a novel feedback evaluator. The
evaluator is specifically designed to assess the quality of the simulated
feedback, based on which we decide the best feedback simulator from our
proposed variants. On a text-to-SQL dataset, we show that our feedback
simulator can generate high-quality NL feedback to boost the error correction
ability of a specific parser. In low-data settings, our feedback simulator can
help achieve comparable error correction performance as trained using the
costly, full set of human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10614">
<div class="article-summary-box-inner">
<span><p>While there is much recent interest in studying why Transformer-based large
language models make predictions the way they do, the complex computations
performed within each layer have made their behavior somewhat opaque. To
mitigate this opacity, this work presents a linear decomposition of final
hidden states from autoregressive language models based on each initial input
token, which is exact for virtually all contemporary Transformer architectures.
This decomposition allows the definition of probability distributions that
ablate the contribution of specific input tokens, which can be used to analyze
their influence on model probabilities over a sequence of upcoming words with
only one forward pass from the model. Using the change in next-word probability
as a measure of importance, this work first examines which context words make
the biggest contribution to language model predictions. Regression experiments
suggest that Transformer-based language models rely primarily on collocational
associations, followed by linguistic factors such as syntactic dependencies and
coreference relationships in making next-word predictions. Additionally,
analyses using these measures to predict syntactic dependencies and coreferent
mention spans show that collocational association and repetitions of the same
token largely explain the language models' predictions on these tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10847">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11255">
<div class="article-summary-box-inner">
<span><p>While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is at
https://github.com/scofield7419/THOR-ISA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction. (arXiv:2305.12258v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12258">
<div class="article-summary-box-inner">
<span><p>Latest efforts on cross-lingual relation extraction (XRE) aggressively
leverage the language-consistent structural features from the universal
dependency (UD) resource, while they may largely suffer from biased transfer
(e.g., either target-biased or source-biased) due to the inevitable linguistic
disparity between languages. In this work, we investigate an unbiased UD-based
XRE transfer by constructing a type of code-mixed UD forest. We first translate
the sentence of the source language to the parallel target-side language, for
both of which we parse the UD tree respectively. Then, we merge the
source-/target-side UD structures as a unified code-mixed UD forest. With such
forest features, the gaps of UD-based XRE between the training and predicting
phases can be effectively closed. We conduct experiments on the ACE XRE
benchmark datasets, where the results demonstrate that the proposed code-mixed
UD forests help unbiased UD-based XRE transfer, with which we achieve
significant XRE performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13628">
<div class="article-summary-box-inner">
<span><p>In cross-lingual named entity recognition (NER), self-training is commonly
used to bridge the linguistic gap by training on pseudo-labeled target-language
data. However, due to sub-optimal performance on target languages, the pseudo
labels are often noisy and limit the overall performance. In this work, we aim
to improve self-training for cross-lingual NER by combining representation
learning and pseudo label refinement in one coherent framework. Our proposed
method, namely ContProto mainly comprises two components: (1) contrastive
self-training and (2) prototype-based pseudo-labeling. Our contrastive
self-training facilitates span classification by separating clusters of
different classes, and enhances cross-lingual transferability by producing
closely-aligned representations between the source and target language.
Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of
pseudo labels during training. We evaluate ContProto on multiple transfer
pairs, and experimental results show our method brings in substantial
improvements over current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16380">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias. (arXiv:2305.16409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16409">
<div class="article-summary-box-inner">
<span><p>While existing work on studying bias in NLP focues on negative or pejorative
language use, Govindarajan et al. (2023) offer a revised framing of bias in
terms of intergroup social context, and its effects on language behavior. In
this paper, we investigate if two pragmatic features (specificity and affect)
systematically vary in different intergroup contexts -- thus connecting this
new framing of bias to language output. Preliminary analysis finds modest
correlations between specificity and affect of tweets with supervised
intergroup relationship (IGR) labels. Counterfactual probing further reveals
that while neural models finetuned for predicting IGR labels reliably use
affect in classification, the model's usage of specificity is inconclusive.
Code and data can be found at: https://github.com/venkatasg/intergroup-probing
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16579">
<div class="article-summary-box-inner">
<span><p>As natural language processing (NLP) has recently seen an unprecedented level
of excitement, and more people are eager to enter the field, it is unclear
whether current research reproducibility efforts are sufficient for this group
of beginners to apply the latest developments. To understand their needs, we
conducted a study with 93 students in an introductory NLP course, where
students reproduced the results of recent NLP papers. Surprisingly, we find
that their programming skill and comprehension of research papers have a
limited impact on their effort spent completing the exercise. Instead, we find
accessibility efforts by research authors to be the key to success, including
complete documentation, better coding practice, and easier access to data
files. Going forward, we recommend that NLP researchers pay close attention to
these simple aspects of open-sourcing their work, and use insights from
beginners' feedback to provide actionable ideas on how to better support them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Imagine: Visually-Augmented Natural Language Generation. (arXiv:2305.16944v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16944">
<div class="article-summary-box-inner">
<span><p>People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17182">
<div class="article-summary-box-inner">
<span><p>Although unsupervised neural machine translation (UNMT) has achieved success
in many language pairs, the copying problem, i.e., directly copying some parts
of the input sentence as the translation, is common among distant language
pairs, especially when low-resource languages are involved. We find this issue
is closely related to an unexpected copying behavior during online
back-translation (BT). In this work, we propose a simple but effective training
schedule that incorporates a language discriminator loss. The loss imposes
constraints on the intermediate translation so that the translation is in the
desired language. By conducting extensive experiments on different language
pairs, including similar and distant, high and low-resource languages, we find
that our method alleviates the copying problem, thus improving the translation
performance on low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Unanswered Questions through Semantic Reformulations in Spoken QA. (arXiv:2305.17393v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17393">
<div class="article-summary-box-inner">
<span><p>Spoken Question Answering (QA) is a key feature of voice assistants, usually
backed by multiple QA systems. Users ask questions via spontaneous speech which
can contain disfluencies, errors, and informal syntax or phrasing. This is a
major challenge in QA, causing unanswered questions or irrelevant answers, and
leading to bad user experiences. We analyze failed QA requests to identify core
challenges: lexical gaps, proposition types, complex syntactic structure, and
high specificity. We propose a Semantic Question Reformulation (SURF) model
offering three linguistically-grounded operations (repair, syntactic reshaping,
generalization) to rewrite questions to facilitate answering. Offline
evaluation on 1M unanswered questions from a leading voice assistant shows that
SURF significantly improves answer rates: up to 24% of previously unanswered
questions obtain relevant answers (75%). Live deployment shows positive impact
for millions of customers with unanswered questions; explicit relevance
feedback shows high user satisfaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17553">
<div class="article-summary-box-inner">
<span><p>Recent model editing techniques promise to mitigate the problem of memorizing
false or outdated associations during LLM training. However, we show that these
techniques can introduce large unwanted side effects which are not detected by
existing specificity benchmarks. We extend the existing CounterFact benchmark
to include a dynamic component and dub our benchmark CounterFact+.
Additionally, we extend the metrics used for measuring specificity by a
principled KL divergence-based metric. We use this improved benchmark to
evaluate recent model editing techniques and find that they suffer from low
specificity. Our findings highlight the need for improved specificity
benchmarks that identify and prevent unwanted side effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17626">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is a fundamental capacity of human cognition that allows
us to reason abstractly about novel situations by relating them to past
experiences. While it is thought to be essential for robust reasoning in AI
systems, conventional approaches require significant training and/or
hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by
cognitive science research that has found connections between human language
and analogy-making, we explore the use of intuitive language-based abstractions
to support analogy in AI systems. Specifically, we apply large pre-trained
language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common
relational reasoning test. By simply encoding the perceptual features of the
problem into language form, we find that PLMs exhibit a striking capacity for
zero-shot relational reasoning, exceeding human performance and nearing
supervised vision-based methods. We explore different encodings that vary the
level of abstraction over task features, finding that higher-level abstractions
further strengthen PLMs' analogical reasoning. Our detailed analysis reveals
insights on the role of model complexity, in-context learning, and prior
knowledge in solving RPM tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18486">
<div class="article-summary-box-inner">
<span><p>The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00418">
<div class="article-summary-box-inner">
<span><p>Recently, aspect sentiment quad prediction has received widespread attention
in the field of aspect-based sentiment analysis. Existing studies extract
quadruplets via pre-trained generative language models to paraphrase the
original sentence into a templated target sequence. However, previous works
only focus on what to generate but ignore what not to generate. We argue that
considering the negative samples also leads to potential benefits. In this
work, we propose a template-agnostic method to control the token-level
generation, which boosts original learning and reduces mistakes simultaneously.
Specifically, we introduce Monte Carlo dropout to understand the built-in
uncertainty of pre-trained language models, acquiring the noises and errors. We
further propose marginalized unlikelihood learning to suppress the
uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to
balance the effects of marginalized unlikelihood learning. Extensive
experiments on four public datasets demonstrate the effectiveness of our
approach on various generation templates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01393">
<div class="article-summary-box-inner">
<span><p>Subword tokenization is the de facto standard for tokenization in neural
language models and machine translation systems. Three advantages are
frequently cited in favor of subwords: shorter encoding of frequent tokens,
compositionality of subwords, and ability to deal with unknown words. As their
relative importance is not entirely clear yet, we propose a tokenization
approach that enables us to separate frequency (the first advantage) from
compositionality. The approach uses Huffman coding to tokenize words, by order
of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR
and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores
reached by BPE, hence compositionality has less importance than previously
thought.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-06 23:11:06.541003760 UTC">2023-06-06 23:11:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>