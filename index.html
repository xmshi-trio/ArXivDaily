<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-07T01:30:00Z">02-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CAB: Empathetic Dialogue Generation with Cognition, Affection and Behavior. (arXiv:2302.01935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01935">
<div class="article-summary-box-inner">
<span><p>Empathy is an important characteristic to be considered when building a more
intelligent and humanized dialogue agent. However, existing methods did not
fully comprehend empathy as a complex process involving three aspects:
cognition, affection and behavior. In this paper, we propose CAB, a novel
framework that takes a comprehensive perspective of cognition, affection and
behavior to generate empathetic responses. For cognition, we build paths
between critical keywords in the dialogue by leveraging external knowledge.
This is because keywords in a dialogue are the core of sentences. Building the
logic relationship between keywords, which is overlooked by the majority of
existing works, can improve the understanding of keywords and contextual logic,
thus enhance the cognitive ability. For affection, we capture the emotional
dependencies with dual latent variables that contain both interlocutors'
emotions. The reason is that considering both interlocutors' emotions
simultaneously helps to learn the emotional dependencies. For behavior, we use
appropriate dialogue acts to guide the dialogue generation to enhance the
empathy expression. Extensive experiments demonstrate that our
multi-perspective model outperforms the state-of-the-art models in both
automatic and manual evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01973">
<div class="article-summary-box-inner">
<span><p>Current benchmarks for evaluating neural code models focus on only a small
subset of programming languages, excluding many popular languages such as Go or
Rust. To ameliorate this issue, we present the BabelCode framework for
execution-based evaluation of any benchmark in any language. BabelCode enables
new investigations into the qualitative performance of models' memory, runtime,
and individual test case results. Additionally, we present a new code
translation dataset called Translating Python Programming Puzzles (TP3) from
the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves
translating expert-level python functions to any language. With both BabelCode
and the TP3 benchmark, we investigate if balancing the distributions of 14
languages in a training dataset improves a large language model's performance
on low-resource languages. Training a model on a balanced corpus results in, on
average, 12.34% higher $pass@k$ across all tasks and languages compared to the
baseline. We find that this strategy achieves 66.48% better $pass@k$ on
low-resource languages at the cost of only a 12.94% decrease to high-resource
languages. In our three translation tasks, this strategy yields, on average,
30.77% better low-resource $pass@k$ while having 19.58% worse high-resource
$pass@k$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSST! Prosodic Speech Segmentation with Transformers. (arXiv:2302.01984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01984">
<div class="article-summary-box-inner">
<span><p>Self-attention mechanisms have enabled transformers to achieve
superhuman-level performance on many speech-to-text (STT) tasks, yet the
challenge of automatic prosodic segmentation has remained unsolved. In this
paper we finetune Whisper, a pretrained STT model, to annotate intonation unit
(IU) boundaries by repurposing low-frequency tokens. Our approach achieves an
accuracy of 95.8%, outperforming previous methods without the need for
large-scale labeled data or enterprise grade compute resources. We also
diminish input signals by applying a series of filters, finding that low pass
filters at a 3.2 kHz level improve segmentation performance in out of sample
and out of distribution contexts. We release our model as both a transcription
tool and a baseline for further improvements in prosodic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Witscript: A System for Generating Improvised Jokes in a Conversation. (arXiv:2302.02008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02008">
<div class="article-summary-box-inner">
<span><p>A chatbot is perceived as more humanlike and likeable if it includes some
jokes in its output. But most existing joke generators were not designed to be
integrated into chatbots. This paper presents Witscript, a novel joke
generation system that can improvise original, contextually relevant jokes,
such as humorous responses during a conversation. The system is based on joke
writing algorithms created by an expert comedy writer. Witscript employs
well-known tools of natural language processing to extract keywords from a
topic sentence and, using wordplay, to link those keywords and related words to
create a punch line. Then a pretrained neural network language model that has
been fine-tuned on a dataset of TV show monologue jokes is used to complete the
joke response by filling the gap between the topic sentence and the punch line.
A method of internal scoring filters out jokes that don't meet a preset
standard of quality. Human evaluators judged Witscript's responses to input
sentences to be jokes more than 40% of the time. This is evidence that
Witscript represents an important next step toward giving a chatbot a humanlike
sense of humor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Interpretability via Explicit Word Interaction Graph Layer. (arXiv:2302.02016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02016">
<div class="article-summary-box-inner">
<span><p>Recent NLP literature has seen growing interest in improving model
interpretability. Along this direction, we propose a trainable neural network
layer that learns a global interaction graph between words and then selects
more informative words using the learned word interactions. Our layer, we call
WIGRAPH, can plug into any neural network-based NLP text classifiers right
after its word embedding layer. Across multiple SOTA NLP models and various NLP
datasets, we demonstrate that adding the WIGRAPH layer substantially improves
NLP models' interpretability and enhances models' prediction performance at the
same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextShield: Beyond Successfully Detecting Adversarial Sentences in Text Classification. (arXiv:2302.02023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02023">
<div class="article-summary-box-inner">
<span><p>Adversarial attack serves as a major challenge for neural network models in
NLP, which precludes the model's deployment in safety-critical applications. A
recent line of work, detection-based defense, aims to distinguish adversarial
sentences from benign ones. However, {the core limitation of previous detection
methods is being incapable of giving correct predictions on adversarial
sentences unlike defense methods from other paradigms.} To solve this issue,
this paper proposes TextShield: (1) we discover a link between text attack and
saliency information, and then we propose a saliency-based detector, which can
effectively detect whether an input sentence is adversarial or not. (2) We
design a saliency-based corrector, which converts the detected adversary
sentences to benign ones. By combining the saliency-based detector and
corrector, TextShield extends the detection-only paradigm to a
detection-correction paradigm, thus filling the gap in the existing
detection-based defense. Comprehensive experiments show that (a) TextShield
consistently achieves higher or comparable performance than state-of-the-art
defense methods across various attacks on different benchmarks. (b) our
saliency-based detector outperforms existing detectors for detecting
adversarial sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Few-Shot Identification of Morality Frames using In-Context Learning. (arXiv:2302.02029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02029">
<div class="article-summary-box-inner">
<span><p>Data scarcity is a common problem in NLP, especially when the annotation
pertains to nuanced socio-linguistic concepts that require specialized
knowledge. As a result, few-shot identification of these concepts is desirable.
Few-shot in-context learning using pre-trained Large Language Models (LLMs) has
been recently applied successfully in many NLP tasks. In this paper, we study
few-shot identification of a psycho-linguistic concept, Morality Frames (Roy et
al., 2021), using LLMs. Morality frames are a representation framework that
provides a holistic view of the moral sentiment expressed in text, identifying
the relevant moral foundation (Haidt and Graham, 2007) and at a finer level of
granularity, the moral sentiment expressed towards the entities mentioned in
the text. Previous studies relied on human annotation to identify morality
frames in text which is expensive. In this paper, we propose prompting-based
approaches using pretrained Large Language Models for identification of
morality frames, relying only on few-shot exemplars. We compare our models'
performance with few-shot RoBERTa and found promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Deficiency in Masked Language Modeling. (arXiv:2302.02060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02060">
<div class="article-summary-box-inner">
<span><p>Masked Language Modeling (MLM) has been one of the most prominent approaches
for pretraining bidirectional text encoders due to its simplicity and
effectiveness. One notable concern about MLM is that the special
$\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and
downstream data as it is present only in pretraining but not in fine-tuning. In
this work, we offer a new perspective on the consequence of such a discrepancy:
We demonstrate empirically and theoretically that MLM pretraining allocates
some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens,
resulting in a representation deficiency for real tokens and limiting the
pretrained model's expressiveness when it is adapted to downstream data without
$\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM,
which pretrains the Masked Autoencoder architecture with MLM where
$\texttt{[MASK]}$ tokens are excluded from the encoder. Empirically, we show
that MAE-LM improves the utilization of model dimensions for real token
representations, and MAE-LM consistently outperforms MLM-pretrained models
across different pretraining settings and model sizes when fine-tuned on the
GLUE and SQuAD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lived Experience Matters: Automatic Detection of Stigma toward People Who Use Substances on Social Media. (arXiv:2302.02064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02064">
<div class="article-summary-box-inner">
<span><p>Stigma toward people who use substances (PWUS) is a leading barrier to
seeking treatment. Further, those in treatment are more likely to drop out if
they experience higher levels of stigmatization. While related concepts of hate
speech and toxicity, including those targeted toward vulnerable populations,
have been the focus of automatic content moderation research, stigma and, in
particular, people who use substances have not. This paper explores stigma
toward PWUS using a data set of roughly 5,000 public Reddit posts. We performed
a crowd-sourced annotation task where workers are asked to annotate each post
for the presence of stigma toward PWUS and answer a series of questions related
to their experiences with substance use. Results show that workers who use
substances or know someone with a substance use disorder are more likely to
rate a post as stigmatizing. Building on this, we use a supervised machine
learning framework that centers workers with lived substance use experience to
label each Reddit post as stigmatizing. Modeling person-level demographics in
addition to comment-level language results in a classification accuracy (as
measured by AUC) of 0.69 -- a 17% increase over modeling language alone.
Finally, we explore the linguist cues which distinguish stigmatizing content:
PWUS substances and those who don't agree that language around othering
("people", "they") and terms like "addict" are stigmatizing, while PWUS (as
opposed to those who do not) find discussions around specific substances more
stigmatizing. Our findings offer insights into the nature of perceived stigma
in substance use. Additionally, these results further establish the subjective
nature of such machine learning tasks, highlighting the need for understanding
their social contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning. (arXiv:2302.02069v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02069">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) recently emerges as a paradigm to train a global
machine learning model across distributed clients without sharing raw data.
Knowledge Graph (KG) embedding represents KGs in a continuous vector space,
serving as the backbone of many knowledge-driven applications. As a promising
combination, federated KG embedding can fully take advantage of knowledge
learned from different clients while preserving the privacy of local data.
However, realistic problems such as data heterogeneity and knowledge forgetting
still remain to be concerned. In this paper, we propose FedLU, a novel FL
framework for heterogeneous KG embedding learning and unlearning. To cope with
the drift between local optimization and global convergence caused by data
heterogeneity, we propose mutual knowledge distillation to transfer local
knowledge to global, and absorb global knowledge back. Moreover, we present an
unlearning method based on cognitive neuroscience, which combines retroactive
interference and passive decay to erase specific knowledge from local clients
and propagate to the global model by reusing knowledge distillation. We
construct new datasets for assessing realistic performance of the
state-of-the-arts. Extensive experiments show that FedLU achieves superior
results in both link prediction and knowledge forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information. (arXiv:2302.02078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02078">
<div class="article-summary-box-inner">
<span><p>The main purpose of relation extraction is to extract the semantic
relationships between tagged pairs of entities in a sentence, which plays an
important role in the semantic understanding of sentences and the construction
of knowledge graphs. In this paper, we propose that the key semantic
information within a sentence plays a key role in the relationship extraction
of entities. We propose the hypothesis that the key semantic information inside
the sentence plays a key role in entity relationship extraction. And based on
this hypothesis, we split the sentence into three segments according to the
location of the entity from the inside of the sentence, and find the
fine-grained semantic features inside the sentence through the intra-sentence
attention mechanism to reduce the interference of irrelevant noise information.
The proposed relational extraction model can make full use of the available
positive semantic information. The experimental results show that the proposed
relation extraction model improves the accuracy-recall curves and P@N values
compared with existing methods, which proves the effectiveness of this model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Prediction Backward-Compatiblility in NLP Model Upgrade with Gated Fusion. (arXiv:2302.02080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02080">
<div class="article-summary-box-inner">
<span><p>When upgrading neural models to a newer version, new errors that were not
encountered in the legacy version can be introduced, known as regression
errors. This inconsistent behavior during model upgrade often outweighs the
benefits of accuracy gain and hinders the adoption of new models. To mitigate
regression errors from model upgrade, distillation and ensemble have proven to
be viable solutions without significant compromise in performance. Despite the
progress, these approaches attained an incremental reduction in regression
which is still far from achieving backward-compatible model upgrade. In this
work, we propose a novel method, Gated Fusion, that promotes backward
compatibility via learning to mix predictions between old and new models.
Empirical results on two distinct model upgrade scenarios show that our method
reduces the number of regression errors by 62% on average, outperforming the
strongest baseline by an average of 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02083">
<div class="article-summary-box-inner">
<span><p>Theory of mind (ToM), or the ability to impute unobservable mental states to
others, is central to human social interactions, communication, empathy,
self-consciousness, and morality. We administer classic false-belief tasks,
widely used to test ToM in humans, to several language models, without any
examples or pre-training. Our results show that models published before 2022
show virtually no ability to solve ToM tasks. Yet, the January 2022 version of
GPT-3 (davinci-002) solved 70% of ToM tasks, a performance comparable with that
of seven-year-old children. Moreover, its November 2022 version (davinci-003),
solved 93% of ToM tasks, a performance comparable with that of nine-year-old
children. These findings suggest that ToM-like ability (thus far considered to
be uniquely human) may have spontaneously emerged as a byproduct of language
models' improving language skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Completion Method Combined With Adaptive Enhanced Semantic Information. (arXiv:2302.02116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02116">
<div class="article-summary-box-inner">
<span><p>Translation models tend to ignore the rich semantic information in triads in
the process of knowledge graph complementation. To remedy this shortcoming,
this paper constructs a knowledge graph complementation method that
incorporates adaptively enhanced semantic information. The hidden semantic
information inherent in the triad is obtained by fine-tuning the BERT model,
and the attention feature embedding method is used to calculate the semantic
attention scores between relations and entities in positive and negative triads
and incorporate them into the structural information to form a soft constraint
rule for semantic information. The rule is added to the original translation
model to realize the adaptive enhancement of semantic information. In addition,
the method takes into account the effect of high-dimensional vectors on the
effect, and uses the BERT-whitening method to reduce the dimensionality and
generate a more efficient semantic vector representation. After experimental
comparison, the proposed method performs better on both FB15K and WIN18
datasets, with a numerical improvement of about 2.6% compared with the original
translation model, which verifies the reasonableness and effectiveness of the
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New cross-domain strategy based XAI models for fake news detection. (arXiv:2302.02122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02122">
<div class="article-summary-box-inner">
<span><p>In this study, we presented a four-level cross-domain strategy for fake news
detection on pre-trained models. Cross-domain text classification is a task of
a model adopting a target domain by using the knowledge of the source domain.
Explainability is crucial in understanding the behaviour of these complex
models. A fine-tune BERT model is used to. perform cross-domain classification
with several experiments using datasets from different domains. Explanatory
models like Anchor, ELI5, LIME and SHAP are used to design a novel explainable
approach to cross-domain levels. The experimental analysis has given an ideal
pair of XAI models on different levels of cross-domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight, Is Attention All We Need? AEIUOrder: Greedy Ordering of Layer Weight Matrices in Transformer Improves Translation. (arXiv:2302.02123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02123">
<div class="article-summary-box-inner">
<span><p>Prior work has attempted to understand the internal structures and
functionalities of Transformer-based encoder-decoder architectures on the level
of multi-head attention and feed-forward sublayers. Interpretations have
focused on the encoder and decoder, along with the combinatorial possibilities
of the self-attention, cross-attention, and feed-forward sublayers. Could we
improve the quality of translation by diving into the Transformer sublayer
abstractions and permuting its layer weight matrices? We propose AEIUOrder to
greedily reorder layer weight matrices in the encoder by their
well-trainedness, as measured by Random Matrix Theory (RMT) metrics, and
reverse the ordering scheme for the encoder. The objective is to maximize Total
well-trainedness in the encoder while the decoder structure serves to represent
the reverse process of encoding. On the standard Transformer (6 layers, model
dimension 512), AEIUOrder achieves a BLEU score of 34.62 (baseline 34.31) on
the IWSLT 2016 German-to-English translation task, and 27.95 BLEU on the WMT
2014 English-to-German translation task (baseline 27.91). AEIUOrder is also
realized on Transformers with various depths and embedding dimensions, showing
significant improvements on deeper, wider models than on their shallower,
slimmer counterparts. For instance, the 8-layer, 768-dimension and the 4-layer,
1024-dimension Transformers achieve respective 29.1 and 29.31 BLEU scores on
the IWSLT 2016 English-to-German translation task (28.53 and 28.97 on
respective baselines). Our results suggest that the RMT-motivated approach to
maximize \textit{Total well-trainedness}, by greedily reordering its layer
weight matrices, facilitates the model to learn representations and generate
translations more effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interaction Order Prediction for Temporal Graphs. (arXiv:2302.02128v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02128">
<div class="article-summary-box-inner">
<span><p>Link prediction in graphs is a task that has been widely investigated. It has
been applied in various domains such as knowledge graph completion,
content/item recommendation, social network recommendations and so on. The
initial focus of most research was on link prediction in static graphs.
However, there has recently been abundant work on modeling temporal graphs, and
consequently one of the tasks that has been researched is link prediction in
temporal graphs. However, most of the existing work does not focus on the order
of link formation, and only predicts the existence of links. In this study, we
aim to predict the order of node interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers. (arXiv:2302.02141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02141">
<div class="article-summary-box-inner">
<span><p>Lipreading refers to understanding and further translating the speech of a
speaker in the video into natural language. State-of-the-art lipreading methods
excel in interpreting overlap speakers, i.e., speakers appear in both training
and inference sets. However, generalizing these methods to unseen speakers
incurs catastrophic performance degradation due to the limited number of
speakers in training bank and the evident visual variations caused by the
shape/color of lips for different speakers. Therefore, merely depending on the
visible changes of lips tends to cause model overfitting. To address this
problem, we propose to use multi-modal features across visual and landmarks,
which can describe the lip motion irrespective to the speaker identities. Then,
we develop a sentence-level lipreading framework based on visual-landmark
transformers, namely LipFormer. Specifically, LipFormer consists of a lip
motion stream, a facial landmark stream, and a cross-modal fusion. The
embeddings from the two streams are produced by self-attention, which are fed
to the cross-attention module to achieve the alignment between visuals and
landmarks. Finally, the resulting fused features can be decoded to output texts
by a cascade seq2seq model. Experiments demonstrate that our method can
effectively enhance the model generalization to unseen speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariants for neural automata. (arXiv:2302.02149v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02149">
<div class="article-summary-box-inner">
<span><p>Computational modeling of neurodynamical systems often deploys neural
networks and symbolic dynamics. A particular way for combining these approaches
within a framework called vector symbolic architectures leads to neural
automata. An interesting research direction we have pursued under this
framework has been to consider mapping symbolic dynamics onto neurodynamics,
represented as neural automata. This representation theory, enables us to ask
questions, such as, how does the brain implement Turing computations.
Specifically, in this representation theory, neural automata result from the
assignment of symbols and symbol strings to numbers, known as G\"odel encoding.
Under this assignment symbolic computation becomes represented by trajectories
of state vectors in a real phase space, that allows for statistical correlation
analyses with real-world measurements and experimental data. However, these
assignments are usually completely arbitrary. Hence, it makes sense to address
the problem question of, which aspects of the dynamics observed under such a
representation is intrinsic to the dynamics and which are not. In this study,
we develop a formally rigorous mathematical framework for the investigation of
symmetries and invariants of neural automata under different encodings. As a
central concept we define patterns of equality for such systems. We consider
different macroscopic observables, such as the mean activation level of the
neural network, and ask for their invariance properties. Our main result shows
that only step functions that are defined over those patterns of equality are
invariant under recodings, while the mean activation is not. Our work could be
of substantial importance for related regression studies of real-world
measurements with neurosymbolic processors for avoiding confounding results
that are dependant on a particular encoding and not intrinsic to the dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many and Which Training Points Would Need to be Removed to Flip this Prediction?. (arXiv:2302.02169v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02169">
<div class="article-summary-box-inner">
<span><p>We consider the problem of identifying a minimal subset of training data
$\mathcal{S}_t$ such that if the instances comprising $\mathcal{S}_t$ had been
removed prior to training, the categorization of a given test point $x_t$ would
have been different. Identifying such a set may be of interest for a few
reasons. First, the cardinality of $\mathcal{S}_t$ provides a measure of
robustness (if $|\mathcal{S}_t|$ is small for $x_t$, we might be less confident
in the corresponding prediction), which we show is correlated with but
complementary to predicted probabilities. Second, interrogation of
$\mathcal{S}_t$ may provide a novel mechanism for contesting a particular model
prediction: If one can make the case that the points in $\mathcal{S}_t$ are
wrongly labeled or irrelevant, this may argue for overturning the associated
prediction. Identifying $\mathcal{S}_t$ via brute-force is intractable. We
propose comparatively fast approximation methods to find $\mathcal{S}_t$ based
on influence functions, and find that -- for simple convex text classification
models -- these approaches can often successfully identify relatively small
sets of training examples which, if removed, would flip the prediction. To our
knowledge, this is the first work in to investigate the problem of identifying
a minimal training set necessary to flip a given prediction in the context of
machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction Grammar Provides Unique Insight into Neural Language Models. (arXiv:2302.02178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02178">
<div class="article-summary-box-inner">
<span><p>Construction Grammar (CxG) has recently been used as the basis for probing
studies that have investigated the performance of large pretrained language
models (PLMs) with respect to the structure and meaning of constructions. In
this position paper, we make suggestions for the continuation and augmentation
of this line of research. We look at probing methodology that was not designed
with CxG in mind, as well as probing methodology that was designed for specific
constructions. We analyse selected previous work in detail, and provide our
view of the most important challenges and research questions that this
promising new field faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark and Scoring Algorithm for Enriching Arabic Synonyms. (arXiv:2302.02232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02232">
<div class="article-summary-box-inner">
<span><p>This paper addresses the task of extending a given synset with additional
synonyms taking into account synonymy strength as a fuzzy value. Given a
mono/multilingual synset and a threshold (a fuzzy value [0-1]), our goal is to
extract new synonyms above this threshold from existing lexicons. We present
twofold contributions: an algorithm and a benchmark dataset. The dataset
consists of 3K candidate synonyms for 500 synsets. Each candidate synonym is
annotated with a fuzzy value by four linguists. The dataset is important for
(i) understanding how much linguists (dis/)agree on synonymy, in addition to
(ii) using the dataset as a baseline to evaluate our algorithm. Our proposed
algorithm extracts synonyms from existing lexicons and computes a fuzzy value
for each candidate. Our evaluations show that the algorithm behaves like a
linguist and its fuzzy values are close to those proposed by linguists (using
RMSE and MAE). The dataset and a demo page are publicly available at
https://portal.sina.birzeit.edu/synonyms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing. (arXiv:2302.02275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02275">
<div class="article-summary-box-inner">
<span><p>Sequence-to-Sequence (S2S) models have achieved remarkable success on various
text generation tasks. However, learning complex structures with S2S models
remains challenging as external neural modules and additional lexicons are
often supplemented to predict non-textual outputs. We present a systematic
study of S2S modeling using contained decoding on four core tasks:
part-of-speech tagging, named entity recognition, constituency and dependency
parsing, to develop efficient exploitation methods costing zero extra
parameters. In particular, 3 lexically diverse linearization schemas and
corresponding constrained decoding methods are designed and evaluated.
Experiments show that although more lexicalized schemas yield longer output
sequences that require heavier training, their sequences being closer to
natural language makes them easier to learn. Moreover, S2S models using our
constrained decoding outperform other S2S approaches using external resources.
Our best models perform better than or comparably to the state-of-the-art for
all 4 tasks, lighting a promise for S2S models to generate non-sequential
structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing. (arXiv:2302.02291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02291">
<div class="article-summary-box-inner">
<span><p>This study aims to demonstrate the methods for detecting negations in a
sentence by uniquely evaluating the lexical structure of the text via word
sense disambiguation. Additionally, the proposed method examined all the unique
features of the related expressions within a text to resolve the contextual
usage of the sentence and the effect of negation on sentiment analysis. The
application of popular expression detectors skips this important step, thereby
neglecting the root words caught in the web of negation, and making text
classification difficult for machine learning and sentiment analysis. This
study adopts the Natural Language Processing (NLP) approach to discover and
antonimize words that were negated for better accuracy in text classification.
This method acts as a lens that reads through a given word sequence using a
knowledge base provided by an NLP library called WordHoard in order to detect
negation signals. Early results show that our initial analysis improved
traditional sentiment analysis that sometimes neglects word negations or
assigns an inverse polarity score. The SentiWordNet analyzer was improved by
35%, the Vader analyzer by 20% and the TextBlob analyzer by 6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v16 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. (arXiv:2106.05970v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05970">
<div class="article-summary-box-inner">
<span><p>Automatic evaluations for natural language generation (NLG) conventionally
rely on token-level or embedding-level comparisons with text references. This
differs from human language processing, for which visual imagination often
improves comprehension. In this work, we propose ImaginE, an imagination-based
automatic evaluation metric for natural language generation. With the help of
StableDiffusion, a state-of-the-art text-to-image generator, we automatically
generate an image as the embodied imagination for the text snippet and compute
the imagination similarity using contextual embeddings. Experiments spanning
several text generation tasks demonstrate that adding machine-generated images
with our ImaginE displays great potential in introducing multi-modal
information into NLG evaluation, and improves existing automatic metrics'
correlations with human similarity judgments in both reference-based and
reference-free evaluation scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPSL: End-to-end perception and reasoning. (arXiv:2109.13662v4 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13662">
<div class="article-summary-box-inner">
<span><p>We introduce DeepPSL a variant of probabilistic soft logic (PSL) to produce
an end-to-end trainable system that integrates reasoning and perception. PSL
represents first-order logic in terms of a convex graphical model -- hinge-loss
Markov random fields (HL-MRFs). PSL stands out among probabilistic logic
frameworks due to its tractability having been applied to systems of more than
1 billion ground rules. The key to our approach is to represent predicates in
first-order logic using deep neural networks and then to approximately
back-propagate through the HL-MRF and thus train every aspect of the
first-order system being represented. We believe that this approach represents
an interesting direction for the integration of deep learning and reasoning
techniques with applications to knowledge base learning, multi-task learning,
and explainability. Evaluation on three different tasks demonstrates that
DeepPSL significantly outperforms state-of-the-art neuro-symbolic methods on
scalability while achieving comparable or better accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07002">
<div class="article-summary-box-inner">
<span><p>Text autoencoders are often used for unsupervised conditional text generation
by applying mappings in the latent space to change attributes to the desired
values. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these
mappings in the embedding space of an autoencoder. However, their method is
restricted to autoencoders with a single-vector embedding, which limits how
much information can be retained. We address this issue by extending their
method to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a
variable-size bag of vectors that grows with the size of the text, as in
attention-based models. This allows to encode and reconstruct much longer texts
than standard autoencoders. Analogous to conventional autoencoders, we propose
regularization techniques that facilitate learning meaningful operations in the
latent space. Finally, we adapt Emb2Emb for a training scheme that learns to
map an input bag to an output bag, including a novel loss function and neural
architecture. Our empirical evaluations on unsupervised sentiment transfer show
that our method performs substantially better than a standard autoencoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05575">
<div class="article-summary-box-inner">
<span><p>Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11176">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a growing interest in designing text generation
systems from a discourse coherence perspective, e.g., modeling the
interdependence between sentences. Still, recent BERT-based evaluation metrics
are weak in recognizing coherence, and thus are not reliable in a way to spot
the discourse-level improvements of those text generation systems. In this
work, we introduce DiscoScore, a parametrized discourse metric, which uses BERT
to model discourse coherence from different perspectives, driven by Centering
theory. Our experiments encompass 16 non-discourse and discourse metrics,
including DiscoScore and popular coherence models, evaluated on summarization
and document-level machine translation (MT). We find that (i) the majority of
BERT-based metrics correlate much worse with human rated coherence than early
discourse metrics, invented a decade ago; (ii) the recent state-of-the-art
BARTScore is weak when operated at system level -- which is particularly
problematic as systems are typically compared in this manner. DiscoScore, in
contrast, achieves strong system-level correlation with human ratings, not only
in coherence but also in factual consistency and other aspects, and surpasses
BARTScore by over 10 correlation points on average. Further, aiming to
understand DiscoScore, we provide justifications to the importance of discourse
coherence for evaluation metrics, and explain the superiority of one variant
over another. Our code is available at
\url{https://github.com/AIPHES/DiscoScore}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Typical Sampling. (arXiv:2202.00666v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00666">
<div class="article-summary-box-inner">
<span><p>Today's probabilistic language generators fall short when it comes to
producing coherent and fluent text despite the fact that the underlying models
perform well under standard metrics, e.g., perplexity. This discrepancy has
puzzled the language generation community for the last few years. In this work,
we posit that the abstraction of natural language generation as a discrete
stochastic process--which allows for an information-theoretic analysis--can
provide new insights into the behavior of probabilistic language generators,
e.g., why high-probability texts can be dull or repetitive. Humans use language
as a means of communicating information, aiming to do so in a simultaneously
efficient and error-minimizing manner; in fact, psycholinguistics research
suggests humans choose each word in a string with this subconscious goal in
mind. We formally define the set of strings that meet this criterion: those for
which each word has an information content close to the expected information
content, i.e., the conditional entropy of our model. We then propose a simple
and efficient procedure for enforcing this criterion when generating from
probabilistic models, which we call locally typical sampling. Automatic and
human evaluations show that, in comparison to nucleus and top-k sampling,
locally typical sampling offers competitive performance (in both abstractive
summarization and story generation) in terms of quality while consistently
reducing degenerate repetitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction of English Resume Corpus and Test with Pre-trained Language Models. (arXiv:2208.03219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03219">
<div class="article-summary-box-inner">
<span><p>Information extraction(IE) has always been one of the essential tasks of NLP.
Moreover, one of the most critical application scenarios of information
extraction is the information extraction of resumes. Constructed text is
obtained by classifying each part of the resume. It is convenient to store
these texts for later search and analysis. Furthermore, the constructed resume
data can also be used in the AI resume screening system. Significantly reduce
the labor cost of HR. This study aims to transform the information extraction
task of resumes into a simple sentence classification task. Based on the
English resume dataset produced by the prior study. The classification rules
are improved to create a larger and more fine-grained classification dataset of
resumes. This corpus is also used to test some current mainstream Pre-training
language models (PLMs) performance.Furthermore, in order to explore the
relationship between the number of training samples and the correctness rate of
the resume dataset, we also performed comparison experiments with training sets
of different train set sizes.The final multiple experimental results show that
the resume dataset with improved annotation rules and increased sample size of
the dataset improves the accuracy of the original resume dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wolfies at SemEval-2022 Task 8: Feature extraction pipeline with transformers for Multi-lingual news article similarity. (arXiv:2208.09715v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09715">
<div class="article-summary-box-inner">
<span><p>This work is about finding the similarity between a pair of news articles.
There are seven different objective similarity metrics provided in the dataset
for each pair and the news articles are in multiple different languages. On top
of the pre-trained embedding model, we calculated cosine similarity for
baseline results and feed-forward neural network was then trained on top of it
to improve the results. We also built separate pipelines for each similarity
metric for feature extraction. We could see significant improvement from
baseline results using feature extraction and feed-forward neural network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Perturbation-Based Gradient Estimation for Discrete Latent Variable Models. (arXiv:2209.04862v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04862">
<div class="article-summary-box-inner">
<span><p>The integration of discrete algorithmic components in deep learning
architectures has numerous applications. Recently, Implicit Maximum Likelihood
Estimation (IMLE, Niepert, Minervini, and Franceschi 2021), a class of gradient
estimators for discrete exponential family distributions, was proposed by
combining implicit differentiation through perturbation with the path-wise
gradient estimator. However, due to the finite difference approximation of the
gradients, it is especially sensitive to the choice of the finite difference
step size, which needs to be specified by the user. In this work, we present
Adaptive IMLE (AIMLE), the first adaptive gradient estimator for complex
discrete distributions: it adaptively identifies the target distribution for
IMLE by trading off the density of gradient information with the degree of bias
in the gradient estimates. We empirically evaluate our estimator on synthetic
examples, as well as on Learning to Explain, Discrete Variational
Auto-Encoders, and Neural Relational Inference tasks. In our experiments, we
show that our adaptive gradient estimator can produce faithful estimates while
requiring orders of magnitude fewer samples than other gradient estimators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bidirectional Language Models Are Also Few-shot Learners. (arXiv:2209.14500v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14500">
<div class="article-summary-box-inner">
<span><p>Large language models such as GPT-3 (Brown et al., 2020) can perform
arbitrary tasks without undergoing fine-tuning after being prompted with only a
few labeled examples. An arbitrary task can be reformulated as a natural
language prompt, and a language model can be asked to generate the completion,
indirectly performing the task in a paradigm known as prompt-based learning. To
date, emergent prompt-based learning capabilities have mainly been demonstrated
for unidirectional language models. However, bidirectional language models
pre-trained on denoising objectives such as masked language modeling produce
stronger learned representations for transfer learning. This motivates the
possibility of prompting bidirectional models, but their pre-training
objectives have made them largely incompatible with the existing prompting
paradigm. We present SAP (Sequential Autoregressive Prompting), a technique
that enables the prompting of bidirectional models. Utilizing the machine
translation task as a case study, we prompt the bidirectional mT5 model (Xue et
al., 2021) with SAP and demonstrate its few-shot and zero-shot translations
outperform the few-shot translations of unidirectional models like GPT-3 and
XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We
further show SAP is effective on question answering and summarization. For the
first time, our results demonstrate prompt-based learning is an emergent
property of a broader class of language models, rather than only unidirectional
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Non-monotonic Self-terminating Language Model. (arXiv:2210.00660v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00660">
<div class="article-summary-box-inner">
<span><p>Recent large-scale neural autoregressive sequence models have shown
impressive performances on a variety of natural language generation tasks.
However, their generated sequences often exhibit degenerate properties such as
non-termination, undesirable repetition, and premature termination, when
generated with decoding algorithms such as greedy search, beam search, top-$k$
sampling, and nucleus sampling. In this paper, we focus on the problem of
non-terminating sequences resulting from an incomplete decoding algorithm. We
first define an incomplete probable decoding algorithm which includes greedy
search, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding
algorithm originally put forward by Welleck et al. (2020). We then propose a
non-monotonic self-terminating language model, which significantly relaxes the
constraint of monotonically increasing termination probability in the
originally proposed self-terminating language model by Welleck et al. (2020),
to address the issue of non-terminating sequences when using incomplete
probable decoding algorithms. We prove that our proposed model prevents
non-terminating sequences when using not only incomplete probable decoding
algorithms but also beam search. We empirically validate our model on sequence
completion tasks with various architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing. (arXiv:2210.04878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04878">
<div class="article-summary-box-inner">
<span><p>Prior work in semantic parsing has shown that conventional seq2seq models
fail at compositional generalization tasks. This limitation led to a resurgence
of methods that model alignments between sentences and their corresponding
meaning representations, either implicitly through latent variables or
explicitly by taking advantage of alignment annotations. We take the second
direction and propose TPOL, a two-step approach that first translates input
sentences monotonically and then reorders them to obtain the correct output.
This is achieved with a modular framework comprising a Translator and a
Reorderer component. We test our approach on two popular semantic parsing
datasets. Our experiments show that by means of the monotonic translations,
TPOL can learn reliable lexico-logical patterns from aligned data,
significantly improving compositional generalization both over conventional
seq2seq models, as well as over other approaches that exploit gold alignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media. (arXiv:2210.06331v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06331">
<div class="article-summary-box-inner">
<span><p>We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly
annotated social media posts from Reddit spanning 24 health conditions.
Annotations include demarcations of spans corresponding to medical claims,
personal experiences, and questions. We collect additional granular annotations
on identified claims. Specifically, we mark snippets that describe patient
Populations, Interventions, and Outcomes (PIO elements) within these. Using
this corpus, we introduce the task of retrieving trustworthy evidence relevant
to a given claim made on social media. We propose a new method to automatically
derive (noisy) supervision for this task which we use to train a dense
retrieval model; this outperforms baseline models. Manual evaluation of
retrieval results performed by medical doctors indicate that while our system
performance is promising, there is considerable room for improvement. Collected
annotations (and scripts to assemble the dataset), are available at
https://github.com/sominw/redhot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models. (arXiv:2210.06475v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06475">
<div class="article-summary-box-inner">
<span><p>We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog. (arXiv:2210.07295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07295">
<div class="article-summary-box-inner">
<span><p>Traditional systems designed for task oriented dialog utilize knowledge
present only in structured knowledge sources to generate responses. However,
relevant information required to generate responses may also reside in
unstructured sources, such as documents. Recent state of the art models such as
HyKnow and SeKnow aimed at overcoming these challenges make limiting
assumptions about the knowledge sources. For instance, these systems assume
that certain types of information, such as a phone number, is always present in
a structured knowledge base (KB) while information about aspects such as
entrance ticket prices, would always be available in documents.
</p>
<p>In this paper, we create a modified version of the MutliWOZ-based dataset
prepared by SeKnow to demonstrate how current methods have significant
degradation in performance when strict assumptions about the source of
information are removed. Then, in line with recent work exploiting pre-trained
language models, we fine-tune a BART based model using prompts for the tasks of
querying knowledge sources, as well as, for response generation, without making
assumptions about the information present in each knowledge source. Through a
series of experiments, we demonstrate that our model is robust to perturbations
to knowledge modality (source of information), and that it can fuse information
from structured as well as unstructured knowledge to generate responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07316">
<div class="article-summary-box-inner">
<span><p>Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Automated Essay Scoring using Transformer Models. (arXiv:2210.12809v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12809">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring is one of the most important problem in Natural
Language Processing. It has been explored for a number of years, and it remains
partially solved. In addition to its economic and educational usefulness, it
presents research problems. Transfer learning has proved to be beneficial in
NLP. Data augmentation techniques have also helped build state-of-the-art
models for automated essay scoring. Many works in the past have attempted to
solve this problem by using RNNs, LSTMs, etc. This work examines the
transformer models like BERT, RoBERTa, etc. We empirically demonstrate the
effectiveness of transformer models and data augmentation for automated essay
grading across many topics using a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse scaling can become U-shaped. (arXiv:2211.02011v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02011">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has been empirically shown to improve performance
and unlock emergent abilities. Conversely, observing worse performance as a
function of scale ("inverse scaling") would indicate that scaling encourages
behaviors that are misaligned with human preferences. The Inverse Scaling Prize
identified eleven such inverse scaling tasks, evaluated on models of up to 280B
parameters and up to 500 zettaFLOPs of training compute.
</p>
<p>This paper takes a closer look at these inverse scaling tasks. We evaluate
models of up to 540B parameters, trained on five times more compute than those
evaluated in the Inverse Scaling Prize. With this increased range of model
sizes and training compute, ten out of the eleven tasks exhibit what we call
"U-shaped scaling" -- performance decreases up to a certain model size, and
then increases again up to the largest model evaluated. U-shaped scaling can be
seen as emergent ability unlocked by scaling and implies that inverse scaling
may not hold for larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiFSMNv2: Pushing Binary Neural Networks for Keyword Spotting to Real-Network Performance. (arXiv:2211.06987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06987">
<div class="article-summary-box-inner">
<span><p>Deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications while suffering expensive computation and
storage. Therefore, network compression technologies like binarization are
studied to deploy KWS models on edge. In this paper, we present a strong yet
efficient binary neural network for KWS, namely BiFSMNv2, pushing it to the
real-network accuracy performance. First, we present a Dual-scale Thinnable
1-bit-Architecture to recover the representation capability of the binarized
computation units by dual-scale activation binarization and liberate the
speedup potential from an overall architecture perspective. Second, we also
construct a Frequency Independent Distillation scheme for KWS
binarization-aware training, which distills the high and low-frequency
components independently to mitigate the information mismatch between
full-precision and binarized representations. Moreover, we propose the Learning
Propagation Binarizer, a general and efficient binarizer that enables the
forward and backward propagation of binary KWS networks to be continuously
improved through learning. We implement and deploy the BiFSMNv2 on ARMv8
real-world hardware with a novel Fast Bitwise Computation Kernel, which is
proposed to fully utilize registers and increase instruction throughput.
Comprehensive experiments show our BiFSMNv2 outperforms existing binary
networks for KWS by convincing margins across different datasets and achieves
comparable accuracy with the full-precision networks (only a tiny 1.51% drop on
Speech Commands V1-12). We highlight that benefiting from the compact
architecture and optimized hardware kernel, BiFSMNv2 can achieve an impressive
25.1x speedup and 20.2x storage-saving on edge hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models. (arXiv:2212.01944v2 [cs.FL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01944">
<div class="article-summary-box-inner">
<span><p>Automata-based representations play an important role in control and planning
in sequential decision-making, but obtaining high-level task knowledge for
building automata is often difficult. Although large-scale generative language
models (GLMs) can help automatically distill task knowledge, the textual
outputs from GLMs are not amenable for formal verification or use in sequential
decision-making. We propose a novel algorithm named GLM2FSA, which obtains
high-level task knowledge represented in a finite state automaton (FSA) from a
given brief description of the task goal. GLM2FSA sends queries to a GLM for
task knowledge in textual form and then builds an FSA to represent the textual
knowledge. It fills the gap between text and automata-based representations,
and the constructed FSA can be directly utilized in formal verification. We
provide an algorithm for iteratively refining the queries to the GLM based on
the outcomes, e.g., counter-examples, from verification. We demonstrate the
algorithm on examples that range from everyday tasks, e.g., crossing a road and
making coffee, to security applications to laboratory safety protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00503">
<div class="article-summary-box-inner">
<span><p>This paper illustrates the technologies of user next intent prediction with a
concept knowledge graph. The system has been deployed on the Web at Alipay,
serving more than 100 million daily active users. To explicitly characterize
user intent, we propose \textbf{AlipayKG}, which is an offline concept
knowledge graph in the Life-Service domain modeling the historical behaviors of
users, the rich content interacted by users and the relations between them. We
further introduce a Transformer-based model which integrates expert rules from
the knowledge graph to infer the online user's next intent. Experimental
results demonstrate that the proposed system can effectively enhance the
performance of the downstream tasks while retaining explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogical Inference Enhanced Knowledge Graph Embedding. (arXiv:2301.00982v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00982">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE), which maps entities and relations in a
knowledge graph into continuous vector spaces, has achieved great success in
predicting missing links in knowledge graphs. However, knowledge graphs often
contain incomplete triples that are difficult to inductively infer by KGEs. To
address this challenge, we resort to analogical inference and propose a novel
and general self-supervised framework AnKGE to enhance KGE models with
analogical inference capability. We propose an analogical object retriever that
retrieves appropriate analogical objects from entity-level, relation-level, and
triple-level. And in AnKGE, we train an analogy function for each level of
analogical inference with the original element embedding from a well-trained
KGE model as input, which outputs the analogical object embedding. In order to
combine inductive inference capability from the original KGE model and
analogical inference capability enhanced by AnKGE, we interpolate the analogy
score with the base model score and introduce the adaptive weights in the score
function for prediction. Through extensive experiments on FB15k-237 and WN18RR
datasets, we show that AnKGE achieves competitive results on link prediction
task and well performs analogical inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03029">
<div class="article-summary-box-inner">
<span><p>Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at Factify 2: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture. (arXiv:2301.03127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03127">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the Logically submissions to De-Factify 2 challenge
(DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our
submissions to this challenge including explored evidence retrieval and
selection techniques, pre-trained cross-modal and unimodal models, and a
cross-modal veracity model based on the well established Transformer Encoder
(TE) architecture which is heavily relies on the concept of self-attention.
Exploratory analysis is also conducted on this Factify 2 data set that uncovers
the salient multi-modal patterns and hypothesis motivating the architecture
proposed in this work. A series of preliminary experiments were done to
investigate and benchmarking different pre-trained embedding models, evidence
retrieval settings and thresholds. The final system, a standard two-stage
evidence based veracity detection system, yields weighted avg. 0.79 on both val
set and final blind test set on the task 1, which achieves 3rd place with a
small margin to the top performing system on the leaderboard among 9
participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers. (arXiv:2301.05453v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05453">
<div class="article-summary-box-inner">
<span><p>Depression detection from user-generated content on the internet has been a
long-lasting topic of interest in the research community, providing valuable
screening tools for psychologists. The ubiquitous use of social media platforms
lays out the perfect avenue for exploring mental health manifestations in posts
and interactions with other users. Current methods for depression detection
from social media mainly focus on text processing, and only a few also utilize
images posted by users. In this work, we propose a flexible time-enriched
multimodal transformer architecture for detecting depression from social media
posts, using pretrained models for extracting image and text embeddings. Our
model operates directly at the user-level, and we enrich it with the relative
time between posts by using time2vec positional embeddings. Moreover, we
propose another model variant, which can operate on randomly sampled and
unordered sets of posts to be more robust to dataset noise. We show that our
method, using EmoBERTa and CLIP embeddings, surpasses other methods on two
multimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a
popular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal
Reddit dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers as Algorithms: Generalization and Stability in In-context Learning. (arXiv:2301.07067v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.07067">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) is a type of prompting where a transformer model
operates on a sequence of (input, output) examples and performs inference
on-the-fly. In this work, we formalize in-context learning as an algorithm
learning problem where a transformer model implicitly constructs a hypothesis
function at inference-time. We first explore the statistical aspects of this
abstraction through the lens of multitask learning: We obtain generalization
bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label)
pairs or (2) a trajectory arising from a dynamical system. The crux of our
analysis is relating the excess risk to the stability of the algorithm
implemented by the transformer. We characterize when transformer/attention
architecture provably obeys the stability condition and also provide empirical
verification. For generalization on unseen tasks, we identify an inductive bias
phenomenon in which the transfer learning risk is governed by the task
complexity and the number of MTL tasks in a highly predictable manner. Finally,
we provide numerical evaluations that (1) demonstrate transformers can indeed
implement near-optimal algorithms on classical regression problems with i.i.d.
and dynamic data, (2) provide insights on stability, and (3) verify our
theoretical predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.07695">
<div class="article-summary-box-inner">
<span><p>We present a new text-to-SQL dataset for electronic health records (EHRs).
The utterances were collected from 222 hospital staff, including physicians,
nurses, insurance review and health records teams, and more. To construct the
QA dataset on structured EHR data, we conducted a poll at a university hospital
and templatized the responses to create seed questions. Then, we manually
linked them to two open-source EHR databases, MIMIC-III and eICU, and included
them with various time expressions and held-out unanswerable questions in the
dataset, which were all collected from the poll. Our dataset poses a unique set
of challenges: the model needs to 1) generate SQL queries that reflect a wide
range of needs in the hospital, including simple retrieval and complex
operations such as calculating survival rate, 2) understand various time
expressions to answer time-sensitive questions in healthcare, and 3)
distinguish whether a given question is answerable or unanswerable based on the
prediction confidence. We believe our dataset, EHRSQL, could serve as a
practical benchmark to develop and assess QA models on structured EHR data and
take one step further towards bridging the gap between text-to-SQL research and
its real-life deployment in healthcare. EHRSQL is available at
https://github.com/glee4810/EHRSQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?. (arXiv:2301.09017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09017">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models (LLMs) have drawn increasing
attention since the learned embeddings pretrained on large-scale datasets have
shown powerful ability in various downstream applications. However, whether the
learned knowledge by LLMs can be transferred to clinical cardiology remains
unknown. In this work, we aim to bridge this gap by transferring the knowledge
of LLMs to clinical Electrocardiography (ECG). We propose an approach for
cardiovascular disease diagnosis and automatic ECG diagnosis report generation.
We also introduce an additional loss function by Optimal Transport (OT) to
align the distribution between ECG and language embedding. The learned
embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis
report generation, and (2) zero-shot cardiovascular disease detection. Our
approach is able to generate high-quality cardiac diagnosis reports and also
achieves competitive zero-shot classification performance even compared with
supervised baselines, which proves the feasibility of transferring knowledge
from LLMs to the cardiac domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series. (arXiv:2301.09279v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09279">
<div class="article-summary-box-inner">
<span><p>There has been growing interest in applying NLP techniques in the financial
domain, however, resources are extremely limited. This paper introduces
StockEmotions, a new dataset for detecting emotions in the stock market that
consists of 10,000 English comments collected from StockTwits, a financial
social media platform. Inspired by behavioral finance, it proposes 12
fine-grained emotion classes that span the roller coaster of investor emotion.
Unlike existing financial sentiment datasets, StockEmotions presents granular
features such as investor sentiment classes, fine-grained emotions, emojis, and
time series data. To demonstrate the usability of the dataset, we perform a
dataset analysis and conduct experimental downstream tasks. For financial
sentiment/emotion classification tasks, DistilBERT outperforms other baselines,
and for multivariate time series forecasting, a Temporal Attention LSTM model
combining price index, text, and emotion features achieves the best performance
than using a single feature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10405">
<div class="article-summary-box-inner">
<span><p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, which are challenging to
modify without re-training after deployment. To address this issue, we propose
a new task of editing language model-based KG embeddings in this paper. The
proposed task aims to enable data-efficient and fast updates to KG embeddings
without damaging the performance of the rest. We build four new datasets:
E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge
editing baselines demonstrating the limited ability of previous models to
handle the proposed challenging task. We further propose a simple yet strong
baseline dubbed KGEditor, which utilizes additional parametric layers of the
hyper network to edit/add facts. Comprehensive experimental results demonstrate
that KGEditor can perform better when updating specific facts while not
affecting the rest with low training resources. Code and datasets will be
available in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewShotTextGCN: K-hop neighborhood regularization for few-shot learning on graphs. (arXiv:2301.10481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10481">
<div class="article-summary-box-inner">
<span><p>We present FewShotTextGCN, a novel method designed to effectively utilize the
properties of word-document graphs for improved learning in low-resource
settings. We introduce K-hop Neighbourhood Regularization, a regularizer for
heterogeneous graphs, and show that it stabilizes and improves learning when
only a few training samples are available. We furthermore propose a
simplification in the graph-construction method, which results in a graph that
is $\sim$7 times less dense and yields better performance in little-resource
settings while performing on par with the state of the art in high-resource
settings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling
tailored for word-document graphs. When using as little as 20 samples for
training, we outperform a strong TextGCN baseline with 17% in absolute accuracy
on average over eight languages. We demonstrate that our method can be applied
to document classification without any language model pretraining on a wide
range of typologically diverse languages while performing on par with large
pretrained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction. (arXiv:2301.12564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12564">
<div class="article-summary-box-inner">
<span><p>Knowledge of syntax includes knowledge of rare, idiosyncratic constructions.
LLMs must overcome frequency biases in order to master such constructions. In
this study, I prompt GPT-3 to give acceptability judgments on the
English-language Article + Adjective + Numeral + Noun construction (e.g., "a
lovely five days"). I validate the prompt using the CoLA corpus of
acceptability judgments and then zero in on the AANN construction. I compare
GPT- 3's judgments to crowdsourced human judgments on a subset of sentences.
GPT-3's judgments are broadly similar to human judgments and generally align
with proposed constraints in the literature but, in some cases, GPT-3's
judgments and human judgments diverge from the literature and from each other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining. (arXiv:2301.12596v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12596">
<div class="article-summary-box-inner">
<span><p>While neural text-to-speech (TTS) has achieved human-like natural synthetic
speech, multilingual TTS systems are limited to resource-rich languages due to
the need for paired text and studio-quality audio data. This paper proposes a
method for zero-shot multilingual TTS using text-only data for the target
language. The use of text-only data allows the development of TTS systems for
low-resource languages for which only textual resources are available, making
TTS accessible to thousands of languages. Inspired by the strong cross-lingual
transferability of multilingual language models, our framework first performs
masked language model pretraining with multilingual text-only data. Then we
train this model with a paired data in a supervised manner, while freezing a
language-aware embedding layer. This allows inference even for languages not
included in the paired data but present in the text-only data. Evaluation
results demonstrate highly intelligible zero-shot TTS with a character error
rate of less than 12% for an unseen language. All experiments were conducted
using public datasets and the implementation will be made available for
reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12609">
<div class="article-summary-box-inner">
<span><p>Contrary to its original interpretation as a facilitator of knowledge
transfer from one model to another, some recent studies have suggested that
knowledge distillation (KD) is instead a form of regularization. Perhaps the
strongest support of all for this claim is found in its apparent similarities
with label smoothing (LS). This paper investigates the stated equivalence of
these two methods by examining the predictive uncertainties of the models they
train. Experiments on four text classification tasks involving teachers and
students of different capacities show that: (a) In most settings, KD and LS
drive model uncertainty (entropy) in completely opposite directions, and (b) In
KD, the student's predictive uncertainty is a direct function of that of its
teacher, reinforcing the knowledge transfer view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. (arXiv:2301.12868v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12868">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is a technique aimed at constructing a structured
representation of the meaning of a natural-language question. Recent
advancements in few-shot language models trained on code have demonstrated
superior performance in generating these representations compared to
traditional unimodal language models, which are trained on downstream tasks.
Despite these advancements, existing fine-tuned neural semantic parsers are
susceptible to adversarial attacks on natural-language inputs. While it has
been established that the robustness of smaller semantic parsers can be
enhanced through adversarial training, this approach is not feasible for large
language models in real-world scenarios, as it requires both substantial
computational resources and expensive human annotation on in-domain semantic
parsing data. This paper presents the first empirical study on the adversarial
robustness of a large prompt-based language model of code, \codex. Our results
demonstrate that the state-of-the-art (SOTA) code-language models are
vulnerable to carefully crafted adversarial examples. To address this
challenge, we propose methods for improving robustness without the need for
significant amounts of labeled data or heavy computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12920">
<div class="article-summary-box-inner">
<span><p>Current multilingual semantic parsing (MSP) datasets are almost all collected
by translating the utterances in the existing datasets from the resource-rich
language to the target language. However, manual translation is costly. To
reduce the translation effort, this paper proposes the first active learning
procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing
datasets to be translated. We also propose a novel selection method that
prioritizes the examples diversifying the logical form structures with more
lexical choices, and a novel hyperparameter tuning method that needs no extra
annotation cost. Our experiments show that AL-MSP significantly reduces
translation costs with ideal selection methods. Our selection method with
proper hyperparameters yields better parsing performance than the other
baselines on two multilingual datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis. (arXiv:2301.13819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13819">
<div class="article-summary-box-inner">
<span><p>ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00674">
<div class="article-summary-box-inner">
<span><p>Few-shot learning involves learning an effective model from only a few
labeled datapoints. The use of a small training set makes it difficult to avoid
overfitting but also makes few-shot learning applicable to many important
real-world settings. In this work, we focus on Few-shot Learning with Auxiliary
Data (FLAD), a training paradigm that assumes access to auxiliary data during
few-shot learning in hopes of improving generalization. Introducing auxiliary
data during few-shot learning leads to essential design choices where
hand-designed heuristics can lead to sub-optimal performance. In this work, we
focus on automated sampling strategies for FLAD and relate them to the
explore-exploit dilemma that is central in multi-armed bandit settings. Based
on this connection we propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and
compare them with methods that either explore or exploit, finding that the
combination of exploration and exploitation is crucial. Using our proposed
algorithms to train T5 yields a 9% absolute improvement over the explicitly
multi-task pre-trained T0 model across 11 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction. (arXiv:2302.01676v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01676">
<div class="article-summary-box-inner">
<span><p>Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain
technologies and smart contracts, of unique crypto assets on digital art forms
(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,
NFTs have attracted the attention of crypto enthusiasts and investors intent on
placing promising investments in this profitable market. However, the NFT
financial performance prediction has not been widely explored to date.
</p>
<p>In this work, we address the above problem based on the hypothesis that NFT
images and their textual descriptions are essential proxies to predict the NFT
selling prices. To this purpose, we propose MERLIN, a novel multimodal deep
learning framework designed to train Transformer-based language and visual
models, along with graph neural network models, on collections of NFTs' images
and texts. A key aspect in MERLIN is its independence on financial features, as
it exploits only the primary data a user interested in NFT trading would like
to deal with, i.e., NFT images and textual descriptions. By learning dense
representations of such data, a price-category classification task is performed
by MERLIN models, which can also be tuned according to user preferences in the
inference phase to mimic different risk-return investment profiles.
Experimental evaluation on a publicly available dataset has shown that MERLIN
models achieve significant performances according to several financial
assessment criteria, fostering profitable investments, and also beating
baseline machine-learning classifiers based on financial features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIQUID: A Framework for List Question Answering Dataset Generation. (arXiv:2302.01691v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01691">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models often rely on large-scale training datasets,
which necessitates the development of a data generation framework to reduce the
cost of manual annotations. Although several recent studies have aimed to
generate synthetic questions with single-span answers, no study has been
conducted on the creation of list questions with multiple, non-contiguous spans
as answers. To address this gap, we propose LIQUID, an automated framework for
generating list QA datasets from unlabeled corpora. We first convert a passage
from Wikipedia or PubMed into a summary and extract named entities from the
summarized text as candidate answers. This allows us to select answers that are
semantically correlated in context and is, therefore, suitable for constructing
list questions. We then create questions using an off-the-shelf question
generator with the extracted entities and original passage. Finally, iterative
filtering and answer expansion are performed to ensure the accuracy and
completeness of the answers. Using our synthetic data, we significantly improve
the performance of the previous best list QA models by exact-match F1 scores of
5.0 on MultiSpanQA, 1.9 on Quoref, and 2.8 averaged across three BioASQ
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-07 23:12:30.086778128 UTC">2023-02-07 23:12:30 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>