<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-16T01:30:00Z">11-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Alzheimer's Diagnosis and Generation-Based Chatbot Using Hierarchical Attention and Transformer. (arXiv:2211.07703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07703">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a natural language processing architecture that can
handle tasks that previously required two models as one model. With a single
model, we analyze the language patterns and conversational context of
Alzheimer's patients and derive answers from two results: patient
classification and chatbot. If the patient's language characteristics are
identified by chatbots in daily life, doctors can plan more precise diagnosis
and treatment for early diagnosis. The proposed model is used to develop
chatbots that replace questionnaires that required experts. There are two
natural language processing tasks performed by the model. The first is a
'natural language classification' that indicates with probability whether the
patient has an illness, and the second is to generate the next 'answer' of the
chatbot to the patient's answer. In the first half, a context vector, which is
a characteristic of patient utterance, is extracted through a self-attention
neural network. This context vector and chatbot (expert, moderator) questions
are entered together into the encoder to obtain a matrix containing the
characteristics of the interaction between the questioner and the patient. The
vectorized matrix becomes a probability value for classification of patients.
Enter the matrix into the decoder with the next answer from the chatbot (the
moderator) to generate the next utterance. As a result of learning this
structure with DmentiaBank's cookie theft description corpus, it was confirmed
that the value of the loss function of the encoder and decoder was
significantly reduced and converged. This shows that capturing the speech
language pattern of Alzheimer's disease patients can contribute to early
diagnosis and longitudinal studies of the disease in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Approach to Classifying Construction Cost Documents into the International Construction Measurement Standard. (arXiv:2211.07705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07705">
<div class="article-summary-box-inner">
<span><p>We introduce the first automated models for classifying natural language
descriptions provided in cost documents called "Bills of Quantities" (BoQs)
popular in the infrastructure construction industry, into the International
Construction Measurement Standard (ICMS). The models we deployed and
systematically evaluated for multi-class text classification are learnt from a
dataset of more than 50 thousand descriptions of items retrieved from 24 large
infrastructure construction projects across the United Kingdom. We describe our
approach to language representation and subsequent modelling to examine the
strength of contextual semantics and temporal dependency of language used in
construction project documentation. To do that we evaluate two experimental
pipelines to inferring ICMS codes from text, on the basis of two different
language representation models and a range of state-of-the-art sequence-based
classification methods, including recurrent and convolutional neural network
architectures. The findings indicate a highly effective and accurate ICMS
automation model is within reach, with reported accuracy results above 90% F1
score on average, on 32 ICMS categories. Furthermore, due to the specific
nature of language use in the BoQs text; short, largely descriptive and
technical, we find that simpler models compare favourably to achieving higher
accuracy results. Our analysis suggest that information is more likely embedded
in local key features in the descriptive text, which explains why a simpler
generic temporal convolutional network (TCN) exhibits comparable memory to
recurrent architectures with the same capacity, and subsequently outperforms
these at this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incongruity Detection between Bangla News Headline and Body Content through Graph Neural Network. (arXiv:2211.07709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07709">
<div class="article-summary-box-inner">
<span><p>Incongruity between news headlines and the body content is a common method of
deception used to attract readers. Profitable headlines pique readers' interest
and encourage them to visit a specific website. This is usually done by adding
an element of dishonesty, using enticements that do not precisely reflect the
content being delivered. As a result, automatic detection of incongruent news
between headline and body content using language analysis has gained the
research community's attention. However, various solutions are primarily being
developed for English to address this problem, leaving low-resource languages
out of the picture. Bangla is ranked 7th among the top 100 most widely spoken
languages, which motivates us to pay special attention to the Bangla language.
Furthermore, Bangla has a more complex syntactic structure and fewer natural
language processing resources, so it becomes challenging to perform NLP tasks
like incongruity detection and stance detection. To tackle this problem, for
the Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)
model that learns the content similarity and contradiction between Bangla news
headlines and content paragraphs effectively. The experimental results show
that the proposed Bangla graph-based neural network model achieves above 90%
accuracy on various Bangla news datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English. (arXiv:2211.07710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07710">
<div class="article-summary-box-inner">
<span><p>Automation of on-call customer support relies heavily on accurate and
efficient speech-to-intent (S2I) systems. Building such systems using
multi-component pipelines can pose various challenges because they require
large annotated datasets, have higher latency, and have complex deployment.
These pipelines are also prone to compounding errors. To overcome these
challenges, we discuss an end-to-end (E2E) S2I model for customer support
voicebot task in a bilingual setting. We show how we can solve E2E intent
classification by leveraging a pre-trained automatic speech recognition (ASR)
model with slight modification and fine-tuning on small annotated datasets.
Experimental results show that our best E2E model outperforms a conventional
pipeline by a relative ~27% on the F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilevel Transformer For Multimodal Emotion Recognition. (arXiv:2211.07711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07711">
<div class="article-summary-box-inner">
<span><p>Multimodal emotion recognition has attracted much attention recently. Fusing
multiple modalities effectively with limited labeled data is a challenging
task. Considering the success of pre-trained model and fine-grained nature of
emotion expression, it is reasonable to take these two aspects into
consideration. Unlike previous methods that mainly focus on one aspect, we
introduce a novel multi-granularity framework, which combines fine-grained
representation with pre-trained utterance-level representation. Inspired by
Transformer TTS, we propose a multilevel transformer model to perform
fine-grained multimodal emotion recognition. Specifically, we explore different
methods to incorporate phoneme-level embedding with word-level embedding. To
perform multi-granularity learning, we simply combine multilevel transformer
model with Albert. Extensive experimental results show that both our multilevel
transformer model and multi-granularity model outperform previous
state-of-the-art approaches on IEMOCAP dataset with text transcripts and speech
signal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloning Ideology and Style using Deep Learning. (arXiv:2211.07712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07712">
<div class="article-summary-box-inner">
<span><p>Text generation tasks have gotten the attention of researchers in the last
few years because of their applications on a large scale.In the past, many
researchers focused on task-based text generations.Our research focuses on text
generation based on the ideology and style of a specific author, and text
generation on a topic that was not written by the same author in the past.Our
trained model requires an input prompt containing initial few words of text to
produce a few paragraphs of text based on the ideology and style of the author
on which the model is trained.Our methodology to accomplish this task is based
on Bi-LSTM.The Bi-LSTM model is used to make predictions at the character
level, during the training corpus of a specific author is used along with the
ground truth corpus.A pre-trained model is used to identify the sentences of
ground truth having contradiction with the author's corpus to make our language
model inclined.During training, we have achieved a perplexity score of 2.23 at
the character level. The experiments show a perplexity score of around 3 over
the test dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Long Is Enough? Exploring the Optimal Intervals of Long-Range Clinical Note Language Modeling. (arXiv:2211.07713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07713">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LMs) have been widely adopted in
biomedical and clinical domains, introducing many powerful LMs such as bio-lm
and BioELECTRA. However, the applicability of these methods to real clinical
use cases is hindered, due to the limitation of pre-trained LMs in processing
long textual data with thousands of words, which is a common length for a
clinical note. In this work, we explore long-range adaptation from such LMs
with Longformer, allowing the LMs to capture longer clinical notes context. We
conduct experiments on three n2c2 challenges datasets and a longitudinal
clinical dataset from Hong Kong Hospital Authority electronic health record
(EHR) system to show the effectiveness and generalizability of this concept,
achieving 10\% F1-score improvement. Based on our experiments, we conclude that
capturing a longer clinical note interval is beneficial to the model
performance, but there are different cut-off intervals to achieve the optimal
performance for different target variables. Our code is available at
https://github.com/HLTCHKUST/long-biomedical-model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Attention Weights as Explanations from an Information Theoretic Perspective. (arXiv:2211.07714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07714">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms have recently demonstrated impressive performance on a
range of NLP tasks, and attention scores are often used as a proxy for model
explainability. However, there is a debate on whether attention weights can, in
fact, be used to identify the most important inputs to a model. We approach
this question from an information theoretic perspective by measuring the mutual
information between the model output and the hidden states. From extensive
experiments, we draw the following conclusions: (i) Additive and Deep attention
mechanisms are likely to be better at preserving the information between the
hidden states and the model output (compared to Scaled Dot-product); (ii)
ablation studies indicate that Additive attention can actively learn to explain
the importance of its input hidden representations; (iii) when attention values
are nearly the same, the rank order of attention values is not consistent with
the rank order of the mutual information(iv) Using Gumbel-Softmax with a
temperature lower than one, tends to produce a more skewed attention score
distribution compared to softmax and hence is a better choice for explainable
design; (v) some building blocks are better at preserving the correlation
between the ordered list of mutual information and attention weights order (for
e.g., the combination of BiLSTM encoder and Additive attention). Our findings
indicate that attention mechanisms do have the potential to function as a
shortcut to model explanations when they are carefully combined with other
model elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast DistilBERT on CPUs. (arXiv:2211.07715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07715">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models have become the standard approach to
solving natural language processing tasks. However, industry adoption usually
requires the maximum throughput to comply with certain latency constraints that
prevents Transformer models from being used in production. To address this gap,
model compression techniques such as quantization and pruning may be used to
improve inference efficiency. However, these compression techniques require
specialized software to apply and deploy at scale. In this work, we propose a
new pipeline for creating and running Fast Transformer models on CPUs,
utilizing hardware-aware pruning, knowledge distillation, quantization, and our
own Transformer inference runtime engine with optimized kernels for sparse and
quantized operators. We demonstrate the efficiency of our pipeline by creating
a Fast DistilBERT model showing minimal accuracy loss on the question-answering
SQuADv1.1 benchmark, and throughput results under typical production
constraints and environments. Our results outperform existing state-of-the-art
Neural Magic's DeepSparse runtime performance by up to 50% and up to 4.1x
performance speedup over ONNX Runtime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Text Matching for Automated Auditing using Sentence Transformers. (arXiv:2211.07716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07716">
<div class="article-summary-box-inner">
<span><p>Natural language processing methods have several applications in automated
auditing, including document or passage classification, information retrieval,
and question answering. However, training such models requires a large amount
of annotated data which is scarce in industrial settings. At the same time,
techniques like zero-shot and unsupervised learning allow for application of
models pre-trained using general domain data to unseen domains.
</p>
<p>In this work, we study the efficiency of unsupervised text matching using
Sentence-Bert, a transformer-based model, by applying it to the semantic
similarity of financial passages. Experimental results show that this model is
robust to documents from in- and out-of-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07717">
<div class="article-summary-box-inner">
<span><p>We describe the development of a model to detect user-level clinical
depression based on a user's temporal social media posts. Our model uses a
Depression Symptoms Detection (DSD) model, which is trained on the largest
existing samples of clinician annotated tweets for clinical depression
symptoms. We subsequently use our DSD model to extract clinically relevant
features, e.g., depression scores and their consequent temporal patterns, as
well as user posting activity patterns, e.g., quantifying their ``no activity''
or ``silence.'' Furthermore, to evaluate the efficacy of these extracted
features, we create three kinds of datasets including a test dataset, from two
existing well-known benchmark datasets for user-level depression detection. We
then provide accuracy measures based on single features, baseline features and
feature ablation tests, at several different levels of temporal granularity,
data distributions, and clinical depression detection related settings to draw
a complete picture of the impact of these features across our created datasets.
Finally, we show that, in general, only semantic oriented representation models
perform well. However, clinical features may enhance overall performance
provided that the training and testing distribution is similar, and there is
more data in a user's timeline. Further, we show that the predictive capability
of depression scores increase significantly while used in a more sensitive
clinical depression detection settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07730">
<div class="article-summary-box-inner">
<span><p>Zero-shot transfer learning for document understanding is a crucial yet
under-investigated scenario to help reduce the high cost involved in annotating
document entities. We present a novel query-based framework, QueryForm, that
extracts entity values from form-like documents in a zero-shot fashion.
QueryForm contains a dual prompting mechanism that composes both the document
schema and a specific entity type into a query, which is used to prompt a
Transformer model to perform a single entity extraction task. Furthermore, we
propose to leverage large-scale query-entity pairs generated from form-like
webpages with weak HTML annotations to pre-train QueryForm. By unifying
pre-training and fine-tuning into the same query-based framework, QueryForm
enables models to learn from structured documents containing various entities
and layouts, leading to better generalization to target document types without
the need for target-specific training data. QueryForm sets new state-of-the-art
average F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%)
zero-shot benchmark, with a smaller model size and no additional image input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07733">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models (PMLMs) are commonly used when
dealing with data from multiple languages and cross-lingual transfer. However,
PMLMs are trained on varying amounts of data for each language. In practice
this means their performance is often much better on English than many other
languages. We explore to what extent this also applies to moral norms. Do the
models capture moral norms from English and impose them on other languages? Do
the models exhibit random and thus potentially harmful beliefs in certain
languages? Both these issues could negatively impact cross-lingual transfer and
potentially lead to harmful outcomes. In this paper, we (1) apply the
MoralDirection framework to multilingual models, comparing results in German,
Czech, Arabic, Mandarin Chinese, and English, (2) analyse model behaviour on
filtered parallel subtitles corpora, and (3) apply the models to a Moral
Foundations Questionnaire, comparing with human responses from different
countries. Our experiments demonstrate that, indeed, PMLMs encode differing
moral biases, but these do not necessarily correspond to cultural differences
or commonalities in human opinions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure. (arXiv:2211.07743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07743">
<div class="article-summary-box-inner">
<span><p>Generative models have demonstrated impressive results on Aspect-based
Sentiment Analysis (ABSA) tasks, particularly for the emerging task of
extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these
models struggle with implicit sentiment expressions, which are commonly
observed in opinionated content such as online reviews. In this work, we
introduce GEN-SCL-NAT, which consists of two techniques for improved structured
generation for ACOS quadruple extraction. First, we propose GEN-SCL, a
supervised contrastive learning objective that aids quadruple prediction by
encouraging the model to produce input representations that are discriminable
across key input attributes, such as sentiment polarity and the existence of
implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured
generation format that better adapts autoregressive encoder-decoder models to
extract quadruples in a generative fashion. Experimental results show that
GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging
1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.
Additionally, we see significant gains on implicit aspect and opinion splits
that have been shown as challenging for existing ACOS approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations. (arXiv:2211.07769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07769">
<div class="article-summary-box-inner">
<span><p>Children's speech recognition is a vital, yet largely overlooked domain when
building inclusive speech technologies. The major challenge impeding progress
in this domain is the lack of adequate child speech corpora; however, recent
advances in self-supervised learning have created a new opportunity for
overcoming this problem of data scarcity. In this paper, we leverage
self-supervised adult speech representations and use three well-known child
speech corpora to build models for children's speech recognition. We assess the
performance of fine-tuning on both native and non-native children's speech,
examine the effect of cross-domain child corpora, and investigate the minimum
amount of child speech required to fine-tune a model which outperforms a
state-of-the-art adult model. We also analyze speech recognition performance
across children's ages. Our results demonstrate that fine-tuning with
cross-domain child corpora leads to relative improvements of up to 46.08% and
45.53% for native and non-native child speech respectively, and absolute
improvements of 14.70% and 31.10%. We also show that with as little as 5 hours
of transcribed children's speech, it is possible to fine-tune a children's
speech recognition system that outperforms a state-of-the-art adult model
fine-tuned on 960 hours of adult speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07828">
<div class="article-summary-box-inner">
<span><p>Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced
impressive gains over purely parametric LMs, by leveraging large-scale
neighborhood retrieval over external memory datastores. However, there has been
little investigation into adapting such models for new domains. This work
attempts to fill that gap and suggests the following approaches for adapting
$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding
neighborhood retrieval over an additional adaptation datastore, and 3) adapting
the weights (scores) of retrieved neighbors using a learned Rescorer module. We
study each adaptation strategy separately, as well as the combined performance
improvement through ablation experiments and an extensive set of evaluations
run over seven adaptation domains. Our combined adaptation approach
consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)
baselines that construct datastores from the adaptation data. On average, we
see perplexity improvements of 17.1\% and 16\% for these respective baselines,
across domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07830">
<div class="article-summary-box-inner">
<span><p>Although pretrained language models (PLMs) can be prompted to perform a wide
range of language tasks, it remains an open question how much this ability
comes from generalizable linguistic representations versus more surface-level
lexical patterns. To test this, we present a structured prompting approach that
can be used to prompt for linguistic structure prediction tasks, allowing us to
perform zero- and few-shot sequence tagging with autoregressive PLMs. We
evaluate this approach on part-of-speech tagging, named entity recognition, and
sentence chunking and demonstrate strong few-shot performance in all cases. We
also find that, though the surface forms of the tags provide some signal,
structured prompting can retrieve linguistic structure even with arbitrary
labels, indicating that PLMs contain this knowledge in a general manner robust
to label choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating How Fine-tuning on Bimodal Data Effects Code Generation. (arXiv:2211.07842v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07842">
<div class="article-summary-box-inner">
<span><p>Despite the increase in popularity of language models for code generation, it
is still unknown how training on bimodal coding forums affects a model's code
generation performance and reliability. We, therefore, collect a dataset of
over 2.2M StackOverflow questions with answers for finetuning. These fine-tuned
models have average $pass@k$ improvements of 54.64% and 85.35% on the HumanEval
(Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021)
tasks, respectively. This regime further decreases the number of generated
programs with both syntax and runtime errors. However, we find that at higher
temperatures, there are significant decreases to the model's ability to
generate runnable programs despite higher $pass@k$ scores, underscoring the
need for better methods of incorporating such data that mitigate these side
effects. The code can be found
https://github.com/gabeorlanski/bimodalcode-generation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Spelling Check with Nearest Neighbors. (arXiv:2211.07843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07843">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Check (CSC) aims to detect and correct error tokens in
Chinese contexts, which has a wide range of applications. In this paper, we
introduce InfoKNN-CSC, extending the standard CSC model by linearly
interpolating it with a k-nearest neighbors (kNN) model. Moreover, the
phonetic, graphic, and contextual information (info) of tokens and contexts are
elaborately incorporated into the design of the query and key of kNN, according
to the characteristics of the task. After retrieval, in order to match the
candidates more accurately, we also perform reranking methods based on the
overlap of the n-gram values and inputs. Experiments on the SIGHAN benchmarks
demonstrate that the proposed model achieves state-of-the-art performance with
substantial improvements over existing work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relationship of the language distance to English ability of a country. (arXiv:2211.07855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07855">
<div class="article-summary-box-inner">
<span><p>Language difference is one of the factors that hinder the acquisition of
second language skills. In this article, we introduce a novel solution that
leverages the strength of deep neural networks to measure the semantic
dissimilarity between languages based on their word distributions in the
embedding space of the multilingual pre-trained language model (e.g.,BERT).
Then, we empirically examine the effectiveness of the proposed semantic
language distance (SLD) in explaining the consistent variation in English
ability of countries, which is proxied by their performance in the
Internet-Based Test of English as Foreign Language (TOEFL iBT). The
experimental results show that the language distance demonstrates negative
influence on a country's average English ability. Interestingly, the effect is
more significant on speaking and writing subskills, which pertain to the
productive aspects of language learning. Besides, we provide specific
recommendations for future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey for Efficient Open Domain Question Answering. (arXiv:2211.07886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07886">
<div class="article-summary-box-inner">
<span><p>Open domain question answering (ODQA) is a longstanding task aimed at
answering factual questions from a large knowledge corpus without any explicit
evidence in natural language processing (NLP). Recent works have predominantly
focused on improving the answering accuracy and achieved promising progress.
However, higher accuracy often comes with more memory consumption and inference
latency, which might not necessarily be efficient enough for direct deployment
in the real world. Thus, a trade-off between accuracy, memory consumption and
processing speed is pursued. In this paper, we provide a survey of recent
advances in the efficiency of ODQA models. We walk through the ODQA models and
conclude the core techniques on efficiency. Quantitative analysis on memory
cost, processing speed, accuracy and overall comparison are given. We hope that
this work would keep interested scholars informed of the advances and open
challenges in ODQA efficiency research, and thus contribute to the further
development of ODQA efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Phrase-based Sequence-to-Sequence Learning. (arXiv:2211.07906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07906">
<div class="article-summary-box-inner">
<span><p>We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Open-Ended Stressor Responses to Predict Depressive Symptoms across Demographics. (arXiv:2211.07932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07932">
<div class="article-summary-box-inner">
<span><p>Stressors are related to depression, but this relationship is complex. We
investigate the relationship between open-ended text responses about stressors
and depressive symptoms across gender and racial/ethnic groups. First, we use
topic models and other NLP tools to find thematic and vocabulary differences
when reporting stressors across demographic groups. We train language models
using self-reported stressors to predict depressive symptoms, finding a
relationship between stressors and depression. Finally, we find that
differences in stressors translate to downstream performance differences across
demographic groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs. (arXiv:2211.07950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07950">
<div class="article-summary-box-inner">
<span><p>Can we teach natural language understanding models to track their beliefs
through intermediate points in text? We propose a representation learning
framework called breakpoint modeling that allows for learning of this type.
Given any text encoder and data marked with intermediate states (breakpoints)
along with corresponding textual queries viewed as true/false propositions
(i.e., the candidate beliefs of a model, consisting of information changing
through time) our approach trains models in an efficient and end-to-end fashion
to build intermediate representations that facilitate teaching and direct
querying of beliefs at arbitrary points alongside solving other end tasks. To
show the benefit of our approach, we experiment with a diverse set of NLU tasks
including relational reasoning on CLUTRR and narrative understanding on bAbI.
Using novel belief prediction tasks for both tasks, we show the benefit of our
main breakpoint transformer, based on T5, over conventional representation
learning approaches in terms of processing efficiency, prediction accuracy and
prediction consistency, all with minimal to no effect on corresponding QA end
tasks. To show the feasibility of incorporating our belief tracker into more
complex reasoning pipelines, we also obtain SOTA performance on the
three-tiered reasoning challenge for the TRIP benchmark (around 23-32% absolute
improvement on Tasks 2-3).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview on Controllable Text Generation via Variational Auto-Encoders. (arXiv:2211.07954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07954">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural-based generative modeling have reignited the hopes
of having computer systems capable of conversing with humans and able to
understand natural language. The employment of deep neural architectures has
been largely explored in a multitude of context and tasks to fulfill various
user needs. On one hand, producing textual content that meets specific
requirements is of priority for a model to seamlessly conduct conversations
with different groups of people. On the other hand, latent variable models
(LVM) such as variational auto-encoders (VAEs) as one of the most popular
genres of generative models are designed to characterize the distributional
pattern of textual data. Thus they are inherently capable of learning the
integral textual features that are worth exploring for controllable pursuits.
</p>
<p>\noindent This overview gives an introduction to existing generation schemes,
problems associated with text variational auto-encoders, and a review of
several applications about the controllable generation that are instantiations
of these general formulations,\footnote{A detailed paper list is available at
\url{https://github.com/ImKeTT/CTG-latentAEs}} as well as related datasets,
metrics and discussions for future researches. Hopefully, this overview will
provide an overview of living questions, popular methodologies and raw thoughts
for controllable language generation under the scope of variational
auto-encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of discourse and conversation impairments in patients with dementia. (arXiv:2211.07971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07971">
<div class="article-summary-box-inner">
<span><p>Neurodegeneration characterizes patients with different dementia subtypes
(e.g., patients with Alzheimer's Disease, Primary Progressive Aphasia, and
Parkinson's Disease), leading to progressive decline in cognitive, linguistic,
and social functioning. Speech and language impairments are early symptoms in
patients with focal forms of neurodegenerative conditions, coupled with
deficits in cognitive, social, and behavioral domains. This paper reviews the
findings on language and communication deficits and identifies the effects of
dementia on the production and perception of discourse. It discusses findings
concerning (i) language function, cognitive representation, and impairment ,
(ii) communicative competence, emotions, empathy, and theory-of-mind, and (iii)
speech-in-interaction. It argues that clinical discourse analysis can provide a
comprehensive assessment of language and communication skills in patients,
which complements the existing neurolinguistic evaluation for (differential)
diagnosis, prognosis, and treatment efficacy evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark and Dataset for Post-OCR text correction in Sanskrit. (arXiv:2211.07980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07980">
<div class="article-summary-box-inner">
<span><p>Sanskrit is a classical language with about 30 million extant manuscripts fit
for digitisation, available in written, printed or scannedimage forms. However,
it is still considered to be a low-resource language when it comes to available
digital resources. In this work, we release a post-OCR text correction dataset
containing around 218,000 sentences, with 1.5 million words, from 30 different
books. Texts in Sanskrit are known to be diverse in terms of their linguistic
and stylistic usage since Sanskrit was the 'lingua franca' for discourse in the
Indian subcontinent for about 3 millennia. Keeping this in mind, we release a
multi-domain dataset, from areas as diverse as astronomy, medicine and
mathematics, with some of them as old as 18 centuries. Further, we release
multiple strong baselines as benchmarks for the task, based on pre-trained
Seq2Seq language models. We find that our best-performing model, consisting of
byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1),
yields a 23% point increase over the OCR output in terms of word and character
error rates. Moreover, we perform extensive experiments in evaluating these
models on their performance and analyse common causes of mispredictions both at
the graphemic and lexical levels. Our code and dataset is publicly available at
https://github.com/ayushbits/pe-ocr-sanskrit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers. (arXiv:2211.08025v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08025">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) is an emerging paradigm that enables distributed
users to collaboratively and iteratively train machine learning models without
sharing their private data. Motivated by the effectiveness and robustness of
self-attention-based architectures, researchers are turning to using
pre-trained Transformers (i.e., foundation models) instead of traditional
convolutional neural networks in FL to leverage their excellent transfer
learning capabilities. Despite recent progress, how pre-trained Transformer
models play a role in FL remains obscure, that is, how to efficiently fine-tune
these pre-trained models in FL and how FL users could benefit from this new
paradigm. In this paper, we explore this issue and demonstrate that the
fine-tuned Transformers achieve extraordinary performance on FL, and that the
lightweight fine-tuning method facilitates a fast convergence rate and low
communication costs. Concretely, we conduct a rigorous empirical study of three
tuning methods (i.e., modifying the input, adding extra modules, and adjusting
the backbone) using two types of pre-trained models (i.e., vision-language
models and vision models) for FL. Our experiments show that 1) Fine-tuning the
bias term of the backbone performs best when relying on a strong pre-trained
model; 2) The vision-language model (e.g., CLIP) outperforms the pure vision
model (e.g., ViT) and is more robust to the few-shot settings; 3) Compared to
pure local training, FL with pre-trained models has a higher accuracy because
it alleviates the problem of over-fitting. We will release our code and
encourage further exploration of pre-trained Transformers and FL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches. (arXiv:2211.08029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08029">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is one of the machine learning applications which can be
done using text, speech, or image data gathered from social media spaces.
Detecting emotion can help us in different fields, including opinion mining.
With the spread of social media, different platforms like Twitter have become
data sources, and the language used in these platforms is informal, making the
emotion detection task difficult. EmoPars and ArmanEmo are two new
human-labeled emotion datasets for the Persian language. These datasets,
especially EmoPars, are suffering from inequality between several samples
between two classes. In this paper, we evaluate EmoPars and compare them with
ArmanEmo. Throughout this analysis, we use data augmentation techniques, data
re-sampling, and class-weights with Transformer-based Pretrained Language
Models(PLMs) to handle the imbalance problem of these datasets. Moreover,
feature selection is used to enhance the models' performance by emphasizing the
text's specific features. In addition, we provide a new policy for selecting
data from EmoPars, which selects the high-confidence samples; as a result, the
model does not see samples that do not have specific emotion during training.
Our model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and
EmoPars, respectively, which are new state-of-the-art results in these
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual and Multimodal Topic Modelling with Pretrained Embeddings. (arXiv:2211.08057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08057">
<div class="article-summary-box-inner">
<span><p>This paper presents M3L-Contrast -- a novel multimodal multilingual (M3L)
neural topic model for comparable data that maps texts from multiple languages
and images into a shared topic space. Our model is trained jointly on texts and
images and takes advantage of pretrained document and image embeddings to
abstract the complexities between different languages and modalities. As a
multilingual topic model, it produces aligned language-specific topics and as
multimodal model, it infers textual representations of semantic concepts in
images. We demonstrate that our model is competitive with a zero-shot topic
model in predicting topic distributions for comparable multilingual data and
significantly outperforms a zero-shot model in predicting topic distributions
for comparable texts and images. We also show that our model performs almost as
well on unaligned embeddings as it does on aligned embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08073">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) improve the model generalization by
leveraging massive data as the training corpus in the pre-training phase.
However, currently, the out-of-distribution (OOD) generalization becomes a
generally ill-posed problem, even for the large-scale PLMs in natural language
understanding tasks, which prevents the deployment of NLP methods in the real
world. To facilitate the research in this direction, this paper makes the first
attempt to establish a unified benchmark named GLUE-X, highlighting the
importance of OOD robustness and providing insights on how to measure the
robustness of a model and how to improve it. To this end, we collect 13
publicly available datasets as OOD test data, and conduct evaluations on 8
classic NLP tasks over \emph{18} popularly used models. Our findings confirm
that the OOD accuracy in NLP tasks needs to be paid more attention to since the
significant performance decay compared to ID accuracy has been found in all
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded VQA by Lattice-based Retrieval. (arXiv:2211.08086v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08086">
<div class="article-summary-box-inner">
<span><p>Visual Grounding (VG) in Visual Question Answering (VQA) systems describes
how well a system manages to tie a question and its answer to relevant image
regions. Systems with strong VG are considered intuitively interpretable and
suggest an improved scene understanding. While VQA accuracy performances have
seen impressive gains over the past few years, explicit improvements to VG
performance and evaluation thereof have often taken a back seat on the road to
overall accuracy improvements. A cause of this originates in the predominant
choice of learning paradigm for VQA systems, which consists of training a
discriminative classifier over a predetermined set of answer options.
</p>
<p>In this work, we break with the dominant VQA modeling paradigm of
classification and investigate VQA from the standpoint of an information
retrieval task. As such, the developed system directly ties VG into its core
search procedure. Our system operates over a weighted, directed, acyclic graph,
a.k.a. "lattice", which is derived from the scene graph of a given image in
conjunction with region-referring expressions extracted from the question.
</p>
<p>We give a detailed analysis of our approach and discuss its distinctive
properties and limitations. Our approach achieves the strongest VG performance
among examined systems and exhibits exceptional generalization capabilities in
a number of scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08099">
<div class="article-summary-box-inner">
<span><p>Generative modeling has been the dominant approach for large-scale
pretraining and zero-shot generalization. In this work, we challenge this
convention by showing that discriminative approaches perform substantially
better than generative ones on a large number of NLP tasks. Technically, we
train a single discriminator to predict whether a text sample comes from the
true data distribution, similar to GANs. Since many NLP tasks can be formulated
as selecting from a few options, we use this discriminator to predict the
option with the highest probability. This simple formulation achieves
state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by
16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning
setting, our approach also achieves new state-of-the-art results on a wide
range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,
our approach requires minimal prompting efforts, which largely improves
robustness and is essential for real-world applications. Furthermore, we also
jointly train a generalized UD in combination with generative tasks, which
maintains its advantage on discriminative tasks and simultaneously works on
generative tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Pronunciation Assessment with Multi-Aspect Attention. (arXiv:2211.08102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08102">
<div class="article-summary-box-inner">
<span><p>Automatic pronunciation assessment is a major component of a
computer-assisted pronunciation training system. To provide in-depth feedback,
scoring pronunciation at various levels of granularity such as phoneme, word,
and utterance, with diverse aspects such as accuracy, fluency, and
completeness, is essential. However, existing multi-aspect multi-granularity
methods simultaneously predict all aspects at all granularity levels;
therefore, they have difficulty in capturing the linguistic hierarchy of
phoneme, word, and utterance. This limitation further leads to neglecting
intimate cross-aspect relations at the same linguistic unit. In this paper, we
propose a Hierarchical Pronunciation Assessment with Multi-aspect Attention
(HiPAMA) model, which hierarchically represents the granularity levels to
directly capture their linguistic structures and introduces multi-aspect
attention that reflects associations across aspects at the same level to create
more connotative representations. By obtaining relational information from both
the granularity- and aspect-side, HiPAMA can take full advantage of multi-task
learning. Remarkable improvements in the experimental results on the
speachocean762 datasets demonstrate the robustness of HiPAMA, particularly in
the difficult-to-assess aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition. (arXiv:2211.08104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08104">
<div class="article-summary-box-inner">
<span><p>We present DualNER, a simple and effective framework to make full use of both
annotated source language corpus and unlabeled target language text for
zero-shot cross-lingual named entity recognition (NER). In particular, we
combine two complementary learning paradigms of NER, i.e., sequence labeling
and span prediction, into a unified multi-task framework. After obtaining a
sufficient NER model trained on the source data, we further train it on the
target data in a {\it dual-teaching} manner, in which the pseudo-labels for one
task are constructed from the prediction of the other task. Moreover, based on
the span prediction, an entity-aware regularization is proposed to enhance the
intrinsic cross-lingual alignment between the same entities in different
languages. Experiments and analysis demonstrate the effectiveness of our
DualNER. Code is available at https://github.com/lemon0830/dualNER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Active Learning Pipeline for Legal Text Classification. (arXiv:2211.08112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08112">
<div class="article-summary-box-inner">
<span><p>Active Learning (AL) is a powerful tool for learning with less labeled data,
in particular, for specialized domains, like legal documents, where unlabeled
data is abundant, but the annotation requires domain expertise and is thus
expensive. Recent works have shown the effectiveness of AL strategies for
pre-trained language models. However, most AL strategies require a set of
labeled samples to start with, which is expensive to acquire. In addition,
pre-trained language models have been shown unstable during fine-tuning with
small datasets, and their embeddings are not semantically meaningful. In this
work, we propose a pipeline for effectively using active learning with
pre-trained language models in the legal domain. To this end, we leverage the
available unlabeled data in three phases. First, we continue pre-training the
model to adapt it to the downstream task. Second, we use knowledge distillation
to guide the model's embeddings to a semantically meaningful space. Finally, we
propose a simple, yet effective, strategy to find the initial set of labeled
samples with fewer actions compared to existing methods. Our experiments on
Contract-NLI, adapted to the classification task, and LEDGAR benchmarks show
that our approach outperforms standard AL strategies, and is more efficient.
Furthermore, our pipeline reaches comparable results to the fully-supervised
approach with a small performance gap, and dramatically reduced annotation
cost. Code and the adapted data will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08142">
<div class="article-summary-box-inner">
<span><p>Mathematical notation makes up a large portion of STEM literature, yet,
finding semantic representations for formulae remains a challenging problem.
Because mathematical notation is precise and its meaning changes significantly
with small character shifts, the methods that work for natural text do not
necessarily work well for mathematical expressions. In this work, we describe
an approach for representing mathematical expressions in a continuous vector
space. We use the encoder of a sequence-to-sequence architecture, trained on
visually different but mathematically equivalent expressions, to generate
vector representations (embeddings). We compare this approach with an
autoencoder and show that the former is better at capturing mathematical
semantics. Finally, to expedite future projects, we publish a corpus of
equivalent transcendental and algebraic expression pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSynGEC: Incorporating Constituent-based Syntax for Grammatical Error Correction with a Tailored GEC-Oriented Parser. (arXiv:2211.08158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08158">
<div class="article-summary-box-inner">
<span><p>Recently, Zhang et al. (2022) propose a syntax-aware grammatical error
correction (GEC) approach, named SynGEC, showing that incorporating tailored
dependency-based syntax of the input sentence is quite beneficial to GEC. This
work considers another mainstream syntax formalism, i.e., constituent-based
syntax. By drawing on the successful experience of SynGEC, we first propose an
extended constituent-based syntax scheme to accommodate errors in ungrammatical
sentences. Then, we automatically obtain constituency trees of ungrammatical
sentences to train a GEC-oriented constituency parser by using parallel GEC
data as a pivot. For syntax encoding, we employ the graph convolutional network
(GCN). Experimental results show that our method, named CSynGEC, yields
substantial improvements over strong baselines. Moreover, we investigate the
integration of constituent-based and dependency-based syntax for GEC in two
ways: 1) intra-model combination, which means using separate GCNs to encode
both kinds of syntax for decoding in a single model; 2)inter-model combination,
which means gathering and selecting edits predicted by different models to
achieve final corrections. We find that the former method improves recall over
using one standalone syntax formalism while the latter improves precision, and
both lead to better F0.5 values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Type Information Utilized Event Detection via Multi-Channel GNNs in Electrical Power Systems. (arXiv:2211.08168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08168">
<div class="article-summary-box-inner">
<span><p>Event detection in power systems aims to identify triggers and event types,
which helps relevant personnel respond to emergencies promptly and facilitates
the optimization of power supply strategies. However, the limited length of
short electrical record texts causes severe information sparsity, and numerous
domain-specific terminologies of power systems makes it difficult to transfer
knowledge from language models pre-trained on general-domain texts. Traditional
event detection approaches primarily focus on the general domain and ignore
these two problems in the power system domain. To address the above issues, we
propose a Multi-Channel graph neural network utilizing Type information for
Event Detection in power systems, named MC-TED, leveraging a semantic channel
and a topological channel to enrich information interaction from short texts.
Concretely, the semantic channel refines textual representations with semantic
similarity, building the semantic information interaction among potential
event-related words. The topological channel generates a relation-type-aware
graph modeling word dependencies, and a word-type-aware graph integrating
part-of-speech tags. To further reduce errors worsened by professional
terminologies in type analysis, a type learning mechanism is designed for
updating the representations of both the word type and relation type in the
topological channel. In this way, the information sparsity and professional
term occurrence problems can be alleviated by enabling interaction between
topological and semantic information. Furthermore, to address the lack of
labeled data in power systems, we built a Chinese event detection dataset based
on electrical Power Event texts, named PoE. In experiments, our model achieves
compelling results not only on the PoE dataset, but on general-domain event
detection datasets including ACE 2005 and MAVEN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Question Answering over Knowledge Bases. (arXiv:2211.08170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08170">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge bases (KBQA) has become a popular approach
to help users extract information from knowledge bases. Although several
systems exist, choosing one suitable for a particular application scenario is
difficult. In this article, we provide a comparative study of six
representative KBQA systems on eight benchmark datasets. In that, we study
various question types, properties, languages, and domains to provide insights
on where existing systems struggle. On top of that, we propose an advanced
mapping algorithm to aid existing models in achieving superior results.
Moreover, we also develop a multilingual corpus COVID-KGQA, which encourages
COVID-19 research and multilingualism for the diversity of future AI. Finally,
we discuss the key findings and their implications as well as performance
guidelines and some future improvements. Our source code is available at
\url{https://github.com/tamlhp/kbqa}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use. (arXiv:2211.08192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08192">
<div class="article-summary-box-inner">
<span><p>Large transformer-based language models, e.g. BERT and GPT-3, outperform
previous architectures on most natural language processing tasks. Such language
models are first pre-trained on gigantic corpora of text and later used as
base-model for finetuning on a particular task. Since the pre-training step is
usually not repeated, base models are not up-to-date with the latest
information. In this paper, we update RobBERT, a RoBERTa-based state-of-the-art
Dutch language model, which was trained in 2019. First, the tokenizer of
RobBERT is updated to include new high-frequent tokens present in the latest
Dutch OSCAR corpus, e.g. corona-related words. Then we further pre-train the
RobBERT model using this dataset. To evaluate if our new model is a plug-in
replacement for RobBERT, we introduce two additional criteria based on concept
drift of existing tokens and alignment for novel tokens.We found that for
certain language tasks this update results in a significant performance
increase. These results highlight the benefit of continually updating a
language model to account for evolving language use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dependence on Frequency of Word Embedding Similarity Measures. (arXiv:2211.08203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08203">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that static word embeddings can encode word
frequency information. However, little has been studied about this phenomenon
and its effects on downstream tasks. In the present work, we systematically
study the association between frequency and semantic similarity in several
static word embeddings. We find that Skip-gram, GloVe and FastText embeddings
tend to produce higher semantic similarity between high-frequency words than
between other frequency combinations. We show that the association between
frequency and similarity also appears when words are randomly shuffled. This
proves that the patterns found are not due to real semantic associations
present in the texts, but are an artifact produced by the word embeddings.
Finally, we provide an example of how word frequency can strongly impact the
measurement of gender bias with embedding-based metrics. In particular, we
carry out a controlled experiment that shows that biases can even change sign
or reverse their order by manipulating word frequencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications. (arXiv:2211.08228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08228">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OpenIE) has been used in the pipelines of
various NLP tasks. Unfortunately, there is no clear consensus on which models
to use in which tasks. Muddying things further is the lack of comparisons that
take differing training sets into account. In this paper, we present an
application-focused empirical survey of neural OpenIE models, training sets,
and benchmarks in an effort to help users choose the most suitable OpenIE
systems for their applications. We find that the different assumptions made by
different models and datasets have a statistically significant effect on
performance, making it important to choose the most appropriate model for one's
applications. We demonstrate the applicability of our recommendations on a
downstream Complex QA application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08233">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
Remarkably, TIM-Net outperforms the latest domain-adaptation method on the
cross-corpus SER tasks, demonstrating strong generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search. (arXiv:2211.08237v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08237">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition (SER) classifies audio into emotion categories
such as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion
Recognition (SER) is a common application for popular languages, it continues
to be a problem for low-resourced languages, i.e., languages with no pretrained
speech-to-text recognition models. This paper firstly proposes a
language-specific model that extract emotional information from multiple
pre-trained speech models, and then designs a multi-domain model that
simultaneously performs SER for various languages. Our multidomain model
employs a multi-gating mechanism to generate unique weighted feature
combination for each language, and also searches for specific neural network
structure for each language through a neural architecture search module. In
addition, we introduce a contrastive auxiliary loss to build more separable
representations for audio data. Our experiments show that our model raises the
state-of-the-art accuracy by 3% for German and 14.3% for French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08238">
<div class="article-summary-box-inner">
<span><p>Given the fact description text of a legal case, legal judgment prediction
(LJP) aims to predict the case's charge, law article and penalty term. A core
problem of LJP is how to distinguish confusing legal cases, where only subtle
text differences exist. Previous studies fail to distinguish different
classification errors with a standard cross-entropy classification loss, and
ignore the numbers in the fact description for predicting the term of penalty.
To tackle these issues, in this work, first, we propose a moco-based supervised
contrastive learning to learn distinguishable representations, and explore the
best strategy to construct positive example pairs to benefit all three subtasks
of LJP simultaneously. Second, in order to exploit the numbers in legal cases
for predicting the penalty terms of certain cases, we further enhance the
representation of the fact description with extracted crime amounts which are
encoded by a pre-trained numeracy model. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results,
especially on confusing legal cases. Ablation studies also demonstrate the
effectiveness of each component.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08264">
<div class="article-summary-box-inner">
<span><p>The availability of large, high-quality datasets has been one of the main
drivers of recent progress in question answering (QA). Such annotated datasets
however are difficult and costly to collect, and rarely exist in languages
other than English, rendering QA technology inaccessible to underrepresented
languages. An alternative to building large monolingual training datasets is to
leverage pre-trained language models (PLMs) under a few-shot learning setting.
Our approach, QAmeleon, uses a PLM to automatically generate multilingual data
upon which QA models are trained, thus avoiding costly annotation. Prompt
tuning the PLM for data synthesis with only five examples per language delivers
accuracy superior to translation-based baselines, bridges nearly 60% of the gap
between an English-only baseline and a fully supervised upper bound trained on
almost 50,000 hand labeled examples, and always leads to substantial
improvements compared to fine-tuning a QA model directly on labeled examples in
low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show
that few-shot prompt tuning for data synthesis scales across languages and is a
viable alternative to large-scale annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An FNet based Auto Encoder for Long Sequence News Story Generation. (arXiv:2211.08295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08295">
<div class="article-summary-box-inner">
<span><p>In this paper, we design an auto encoder based off of Google's FNet
Architecture in order to generate text from a subset of news stories contained
in Google's C4 dataset. We discuss previous attempts and methods to generate
text from autoencoders and non LLM Models. FNET poses multiple advantages to
BERT based encoders in the realm of efficiency which train 80% faster on GPUs
and 70% faster on TPUs. We then compare outputs of how this autencoder perfroms
on different epochs. Finally, we analyze what outputs the encoder produces with
different seed text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PARTNR: Pick and place Ambiguity Resolving by Trustworthy iNteractive leaRning. (arXiv:2211.08304v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08304">
<div class="article-summary-box-inner">
<span><p>Several recent works show impressive results in mapping language-based human
commands and image scene observations to direct robot executable policies
(e.g., pick and place poses). However, these approaches do not consider the
uncertainty of the trained policy and simply always execute actions suggested
by the current policy as the most probable ones. This makes them vulnerable to
domain shift and inefficient in the number of required demonstrations. We
extend previous works and present the PARTNR algorithm that can detect
ambiguities in the trained policy by analyzing multiple modalities in the pick
and place poses using topological analysis. PARTNR employs an adaptive,
sensitivity-based, gating function that decides if additional user
demonstrations are required. User demonstrations are aggregated to the dataset
and used for subsequent training. In this way, the policy can adapt promptly to
domain shift and it can minimize the number of required demonstrations for a
well-trained policy. The adaptive threshold enables to achieve the
user-acceptable level of ambiguity to execute the policy autonomously and in
turn, increase the trustworthiness of our system. We demonstrate the
performance of PARTNR in a table-top pick and place task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FolkScope: Intention Knowledge Graph Construction for Discovering E-commerce Commonsense. (arXiv:2211.08316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08316">
<div class="article-summary-box-inner">
<span><p>As stated by Oren Etzioni, ``commonsense is the dark matter of artificial
intelligence''. In e-commerce, understanding users' needs or intentions
requires substantial commonsense knowledge, e.g., ``A user bought an iPhone and
a compatible case because the user wanted the phone to be protected''. In this
paper, we present FolkScope, an intention knowledge graph construction
framework, to reveal the structure of humans' minds about purchasing items on
e-commerce platforms such as Amazon. As commonsense knowledge is usually
ineffable and not expressed explicitly, it is challenging to perform any kind
of information extraction. Thus, we propose a new approach that leverages the
generation power of large-scale language models and human-in-the-loop
annotations to semi-automatically construct the knowledge graph. We annotate a
large amount of assertions for both plausibility and typicality of an intention
that can explain a purchasing or co-purchasing behavior, where the intention
can be an open reason or a predicate falling into one of 18 categories aligning
with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we populate the
annotated information to all automatically generated ones, and further
structurize the assertions using pattern mining and conceptualization to form
more condensed and abstractive knowledge. We evaluate our knowledge graph using
both intrinsic quality measures and a downstream application, i.e.,
recommendation. The comprehensive study shows that our knowledge graph can well
model e-commerce commonsense knowledge and can have many potential
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08358">
<div class="article-summary-box-inner">
<span><p>Few-shot classification in NLP has recently made great strides due to the
availability of large foundation models that, through priming and prompting,
are highly effective few-shot learners. However, this approach has high
variance across different sets of few shots and across different finetuning
runs. For example, we find that validation accuracy on RTE can vary by as much
as 27 points. In this context, we make two contributions for more effective
few-shot learning. First, we propose novel ensembling methods and show that
they substantially reduce variance. Second, since performance depends a lot on
the set of few shots selected, active learning is promising for few-shot
classification. Based on our stable ensembling method, we build on existing
work on active learning and introduce a new criterion: inter-prompt uncertainty
sampling with diversity. We present the first active learning based approach to
select training examples for prompt-based learning and show that it outperforms
prior work on active learning. Finally, we show that our combined method, MEAL
(Multiprompt finetuning and prediction Ensembling with Active Learning),
improves overall performance of prompt-based finetuning by 2.3 absolute points
on five different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying text using machine learning models and determining conversation drift. (arXiv:2211.08365v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08365">
<div class="article-summary-box-inner">
<span><p>Text classification helps analyse texts for semantic meaning and relevance,
by mapping the words against this hierarchy. An analysis of various types of
texts is invaluable to understanding both their semantic meaning, as well as
their relevance. Text classification is a method of categorising documents. It
combines computer text classification and natural language processing to
analyse text in aggregate. This method provides a descriptive categorization of
the text, with features like content type, object field, lexical
characteristics, and style traits. In this research, the authors aim to use
natural language feature extraction methods in machine learning which are then
used to train some of the basic machine learning models like Naive Bayes,
Logistic Regression, and Support Vector Machine. These models are used to
detect when a teacher must get involved in a discussion when the lines go
off-topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08369">
<div class="article-summary-box-inner">
<span><p>A popular approach to unveiling the black box of neural NLP models is to
leverage saliency methods, which assign scalar importance scores to each input
component. A common practice for evaluating whether an interpretability method
is \textit{faithful} and \textit{plausible} has been to use
evaluation-by-agreement -- multiple methods agreeing on an explanation
increases its credibility. However, recent work has found that even saliency
methods have weak rank correlations and advocated for the use of alternative
diagnostic methods. In our work, we demonstrate that rank correlation is not a
good fit for evaluating agreement and argue that Pearson-$r$ is a better suited
alternative. We show that regularization techniques that increase faithfulness
of attention explanations also increase agreement between saliency methods.
Through connecting our findings to instance categories based on training
dynamics we show that, surprisingly, easy-to-learn instances exhibit low
agreement in saliency method explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatics in Grounded Language Learning: Phenomena, Tasks, and Modeling Approaches. (arXiv:2211.08371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08371">
<div class="article-summary-box-inner">
<span><p>People rely heavily on context to enrich meaning beyond what is literally
said, enabling concise but effective communication. To interact successfully
and naturally with people, user-facing artificial intelligence systems will
require similar skills in pragmatics: relying on various types of context --
from shared linguistic goals and conventions, to the visual and embodied world
-- to use language effectively.
</p>
<p>We survey existing grounded settings and pragmatic modeling approaches and
analyze how the task goals, environmental contexts, and communicative
affordances in each work enrich linguistic meaning. We present recommendations
for future grounded task design to naturally elicit pragmatic phenomena, and
suggest directions that focus on a broader range of communicative contexts and
affordances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Language Models with Knowledge Graph Reasoning for Question Answering. (arXiv:2211.08380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08380">
<div class="article-summary-box-inner">
<span><p>Answering open-domain questions requires world knowledge about in-context
entities. As pre-trained Language Models (LMs) lack the power to store all
required knowledge, external knowledge sources, such as knowledge graphs, are
often used to augment LMs. In this work, we propose knOwledge REasOning
empowered Language Model (OREO-LM), which consists of a novel Knowledge
Interaction Layer that can be flexibly plugged into existing Transformer-based
LMs to interact with a differentiable Knowledge Graph Reasoning module
collaboratively. In this way, LM guides KG to walk towards the desired answer,
while the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and
T5, we show significant performance gain, achieving state-of-art results in the
Closed-Book setting. The performance enhancement is mainly from the KG
reasoning's capacity to infer missing relational facts. In addition, OREO-LM
provides reasoning paths as rationales to interpret the model's decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness. (arXiv:2211.08386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08386">
<div class="article-summary-box-inner">
<span><p>In this thesis, we investigated the relevance, faithfulness, and succinctness
aspects of Long Form Question Answering (LFQA). LFQA aims to generate an
in-depth, paragraph-length answer for a given question, to help bridge the gap
between real scenarios and the existing open-domain QA models which can only
extract short-span answers. LFQA is quite challenging and under-explored. Few
works have been done to build an effective LFQA system. It is even more
challenging to generate a good-quality long-form answer relevant to the query
and faithful to facts, since a considerable amount of redundant, complementary,
or contradictory information will be contained in the retrieved documents.
Moreover, no prior work has been investigated to generate succinct answers. We
are among the first to research the LFQA task. We pioneered the research
direction to improve the answer quality in terms of 1) query-relevance, 2)
answer faithfulness, and 3) answer succinctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation. (arXiv:2211.08387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08387">
<div class="article-summary-box-inner">
<span><p>Lexically constrained text generation is one of the constrained text
generation tasks, which aims to generate text that covers all the given
constraint lexicons. While the existing approaches tackle this problem using a
lexically constrained beam search algorithm or dedicated model using
non-autoregressive decoding, there is a trade-off between the generated text
quality and the hard constraint satisfaction. We introduce AutoTemplate, a
simple yet effective lexically constrained text generation framework divided
into template generation and lexicalization tasks. The template generation is
to generate the text with the placeholders, and lexicalization replaces them
into the constraint lexicons to perform lexically constrained text generation.
We conducted the experiments on two tasks: keywords-to-sentence generations and
entity-guided summarization. Experimental results show that the AutoTemplate
outperforms the competitive baselines on both tasks while satisfying the hard
lexical constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing Semantics into Speech Encoders. (arXiv:2211.08402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08402">
<div class="article-summary-box-inner">
<span><p>Recent studies find existing self-supervised speech encoders contain
primarily acoustic rather than semantic information. As a result, pipelined
supervised automatic speech recognition (ASR) to large language model (LLM)
systems achieve state-of-the-art results on semantic spoken language tasks by
utilizing rich semantic representations from the LLM. These systems come at the
cost of labeled audio transcriptions, which is expensive and time-consuming to
obtain. We propose a task-agnostic unsupervised way of incorporating semantic
information from LLMs into self-supervised speech encoders without labeled
audio transcriptions. By introducing semantics, we improve existing speech
encoder spoken language understanding performance by over 10\% on intent
classification, with modest gains in named entity resolution and slot filling,
and spoken question answering FF1 score by over 2\%. Our unsupervised approach
achieves similar performance as supervised methods trained on over 100 hours of
labeled audio transcripts, demonstrating the feasibility of unsupervised
semantic augmentations to existing speech encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08411">
<div class="article-summary-box-inner">
<span><p>The internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, there is a huge variability in the number of times
a given piece of information appears on the web. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in their pre-training datasets. In particular, we show that a
language model's ability to answer a fact-based question relates to how many
documents associated with that question were seen during pre-training. We
identify these relevant documents by entity linking pre-training datasets and
counting documents that contain the same entities as a given question-answer
pair. Our results demonstrate strong correlational and causal relationships
between accuracy and relevant document count for numerous question answering
datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes
(e.g., 176B parameters). Moreover, we find that while larger models are better
at learning long-tail knowledge, we estimate that today's models must be scaled
by many orders of magnitude to reach competitive QA performance on questions
with little support in the pre-training data. Finally, we show that
retrieval-augmentation can reduce the dependence on relevant document count,
presenting a promising approach for capturing the long-tail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Factual Consistency of Large Language Models Through Summarization. (arXiv:2211.08412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08412">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have proven to be effective on a large
variety of tasks, they are also known to hallucinate information. To measure
whether an LLM prefers factually consistent continuations of its input, we
propose a new benchmark called FIB(Factual Inconsistency Benchmark) that
focuses on the task of summarization. Specifically, our benchmark involves
comparing the scores an LLM assigns to a factually consistent versus a
factually inconsistent summary for an input news article. For factually
consistent summaries, we use human-written reference summaries that we manually
verify as factually consistent. To generate summaries that are factually
inconsistent, we generate summaries from a suite of summarization models that
we have manually annotated as factually inconsistent. A model's factual
consistency is then measured according to its accuracy, i.e.\ the proportion of
documents where it assigns a higher score to the factually consistent summary.
To validate the usefulness of FIB, we evaluate 23 large language models ranging
from 1B to 176B parameters from six different model families including BLOOM
and OPT. We find that existing LLMs generally assign a higher score to
factually consistent summaries than to factually inconsistent summaries.
However, if the factually inconsistent summaries occur verbatim in the
document, then LLMs assign a higher score to these factually inconsistent
summaries than factually consistent summaries. We validate design choices in
our benchmark including the scoring method and source of distractor summaries.
Our code and benchmark data can be found at https://github.com/r-three/fib.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best-First Beam Search. (arXiv:2007.03909v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03909">
<div class="article-summary-box-inner">
<span><p>Decoding for many NLP tasks requires an effective heuristic algorithm for
approximating exact search since the problem of searching the full output space
is often intractable, or impractical in many settings. The default algorithm
for this job is beam search -- a pruned version of breadth-first search. Quite
surprisingly, beam search often returns better results than exact inference due
to beneficial search bias for NLP tasks. In this work, we show that the
standard implementation of beam search can be made up to 10x faster in
practice. Our method assumes that the scoring function is monotonic in the
sequence length, which allows us to safely prune hypotheses that cannot be in
the final set of hypotheses early on. We devise effective monotonic
approximations to popular nonmonontic scoring functions, including length
normalization and mutual information decoding. Lastly, we propose a
memory-reduced variant of Best-First Beam Search, which has a similar
beneficial search bias in terms of downstream performance, but runs in a
fraction of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Guarantees for De-identifying Text Transformations. (arXiv:2008.03101v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03101">
<div class="article-summary-box-inner">
<span><p>Machine Learning approaches to Natural Language Processing tasks benefit from
a comprehensive collection of real-life user data. At the same time, there is a
clear need for protecting the privacy of the users whose data is collected and
processed. For text collections, such as, e.g., transcripts of voice
interactions or patient records, replacing sensitive parts with benign
alternatives can provide de-identification. However, how much privacy is
actually guaranteed by such text transformations, and are the resulting texts
still useful for machine learning? In this paper, we derive formal privacy
guarantees for general text transformation-based de-identification methods on
the basis of Differential Privacy. We also measure the effect that different
ways of masking private information in dialog transcripts have on a subsequent
machine learning task. To this end, we formulate different masking strategies
and compare their privacy-utility trade-offs. In particular, we compare a
simple redact approach with more sophisticated word-by-word replacement using
deep learning models on multiple natural language understanding tasks like
named entity recognition, intent detection, and dialog act classification. We
find that only word-by-word replacement is robust against performance drops in
various tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing. (arXiv:2103.02227v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02227">
<div class="article-summary-box-inner">
<span><p>Data augmentation has attracted a lot of research attention in the deep
learning era for its ability in alleviating data sparseness. The lack of
labeled data for unseen evaluation databases is exactly the major challenge for
cross-domain text-to-SQL parsing. Previous works either require human
intervention to guarantee the quality of generated data, or fail to handle
complex SQL queries. This paper presents a simple yet effective data
augmentation framework. First, given a database, we automatically produce a
large number of SQL queries based on an abstract syntax tree grammar. For
better distribution matching, we require that at least 80% of SQL patterns in
the training data are covered by generated queries. Second, we propose a
hierarchical SQL-to-question generation model to obtain high-quality natural
language questions, which is the major contribution of this work. Finally, we
design a simple sampling strategy that can greatly improve training efficiency
given large amounts of generated data. Experiments on three cross-domain
datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that
our proposed data augmentation framework can consistently improve performance
over strong baselines, and the hierarchical generation component is the key for
the improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Learning Event Extraction: Approaches and Applications. (arXiv:2107.02126v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02126">
<div class="article-summary-box-inner">
<span><p>Event extraction (EE) is a crucial research task for promptly apprehending
event information from massive textual data. With the rapid development of deep
learning, EE based on deep learning technology has become a research hotspot.
Numerous methods, datasets, and evaluation metrics have been proposed in the
literature, raising the need for a comprehensive and updated survey. This
article fills the research gap by reviewing the state-of-the-art approaches,
especially focusing on the general domain EE based on deep learning models. We
introduce a new literature classification of current general domain EE research
according to the task definition. Afterward, we summarize the paradigm and
models of EE approaches, and then discuss each of them in detail. As an
important aspect, we summarize the benchmarks that support tests of predictions
and evaluation metrics. A comprehensive comparison among different approaches
is also provided in this survey. Finally, we conclude by summarizing future
research directions facing the research area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07410">
<div class="article-summary-box-inner">
<span><p>Given the recent proliferation of false claims online, there has been a lot
of manual fact-checking effort. As this is very time-consuming, human
fact-checkers can benefit from tools that can support them and make them more
efficient. Here, we focus on building a system that could provide such support.
Given an input document, it aims to detect all sentences that contain a claim
that can be verified by some previously fact-checked claims (from a given
database). The output is a re-ranked list of the document sentences, so that
those that can be verified are ranked as high as possible, together with
corresponding evidence. Unlike previous work, which has looked into claim
retrieval, here we take a document-level perspective. We create a new manually
annotated dataset for this task, and we propose suitable evaluation measures.
We further experiment with a learning-to-rank approach, achieving sizable
performance gains over several strong baselines. Our analysis demonstrates the
importance of modeling text similarity and stance, while also taking into
account the veracity of the retrieved previously fact-checked claims. We
believe that this research would be of interest to fact-checkers, journalists,
media, and regulatory authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07031">
<div class="article-summary-box-inner">
<span><p>An interactive instruction following task has been proposed as a benchmark
for learning to map natural language instructions and first-person vision into
sequences of actions to interact with objects in 3D environments. We found that
an existing end-to-end neural model for this task tends to fail to interact
with objects of unseen attributes and follow various instructions. We assume
that this problem is caused by the high sensitivity of neural feature
extraction to small changes in vision and language inputs. To mitigate this
problem, we propose a neuro-symbolic approach that utilizes high-level symbolic
features, which are robust to small changes in raw inputs, as intermediate
representations. We verify the effectiveness of our model with the subtask
evaluation on the ALFRED benchmark. Our experiments show that our approach
significantly outperforms the end-to-end neural model by 9, 46, and 74 points
in the success rate on the ToggleObject, PickupObject, and SliceObject subtasks
in unseen environments respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Language Modeling. (arXiv:2110.08413v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08413">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models are critical components of modern NLP
pipelines. Yet, they suffer from spurious correlations, poor out-of-domain
generalization, and biases. Inspired by recent progress in causal machine
learning, in particular the invariant risk minimization (IRM) paradigm, we
propose invariant language modeling, a framework for learning invariant
representations that generalize better across multiple environments. In
particular, we adapt a game-theoretic formulation of IRM (IRM-games) to
language models, where the invariance emerges from a specific training schedule
in which all the environments compete to optimize their own
environment-specific loss by updating subsets of the model in a round-robin
fashion. We focus on controlled experiments to precisely demonstrate the
ability of our method to (i) remove structured noise, (ii) ignore specific
spurious correlations without affecting global performance, and (iii) achieve
better out-of-domain generalization. These benefits come with a negligible
computational overhead compared to standard training, do not require changing
the local loss, and can be applied to any language model. We believe this
framework is promising to help mitigate spurious correlations and biases in
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05955">
<div class="article-summary-box-inner">
<span><p>A recurring challenge of crowdsourcing NLP datasets at scale is that human
writers often rely on repetitive patterns when crafting examples, leading to a
lack of linguistic diversity. We introduce a novel approach for dataset
creation based on worker and AI collaboration, which brings together the
generative strength of language models and the evaluative strength of humans.
Starting with an existing dataset, MultiNLI for natural language inference
(NLI), our approach uses dataset cartography to automatically identify examples
that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose
new examples with similar patterns. Machine generated examples are then
automatically filtered, and finally revised and labeled by human crowdworkers.
The resulting dataset, WANLI, consists of 107,885 NLI examples and presents
unique empirical strengths over existing NLI datasets. Remarkably, training a
model on WANLI improves performance on eight out-of-domain test sets we
consider, including by 11% on HANS and 9% on Adversarial NLI, compared to
training on the 4x larger MultiNLI. Moreover, it continues to be more effective
than MultiNLI augmented with other NLI datasets. Our results demonstrate the
promise of leveraging natural language generation techniques and re-imagining
the role of humans in the dataset creation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11176">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a growing interest in designing text generation
systems from a discourse coherence perspective, e.g., modeling the
interdependence between sentences. Still, recent BERT-based evaluation metrics
are weak in recognizing coherence, and thus are not reliable in a way to spot
the discourse-level improvements of those text generation systems. In this
work, we introduce DiscoScore, a parametrized discourse metric, which uses BERT
to model discourse coherence from different perspectives, driven by Centering
theory. Our experiments encompass 16 non-discourse and discourse metrics,
including DiscoScore and popular coherence models, evaluated on summarization
and document-level machine translation (MT). We find that (i) the majority of
BERT-based metrics correlate much worse with human rated coherence than early
discourse metrics, invented a decade ago; (ii) the recent state-of-the-art
BARTScore is weak when operated at system level -- which is particularly
problematic as systems are typically compared in this manner. DiscoScore, in
contrast, achieves strong system-level correlation with human ratings, not only
in coherence but also in factual consistency and other aspects, and surpasses
BARTScore by over 10 correlation points on average. Further, aiming to
understand DiscoScore, we provide justifications to the importance of discourse
coherence for evaluation metrics, and explain the superiority of one variant
over another. Our code is available at
\url{https://github.com/AIPHES/DiscoScore}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10232">
<div class="article-summary-box-inner">
<span><p>In this paper, we present DuReader_retrieval, a large-scale Chinese dataset
for passage retrieval. DuReader_retrieval contains more than 90K queries and
over 8M unique passages from a commercial search engine. To alleviate the
shortcomings of other datasets and ensure the quality of our benchmark, we (1)
reduce the false negatives in development and test sets by manually annotating
results pooled from multiple retrievers, and (2) remove the training queries
that are semantically similar to the development and testing queries.
Additionally, we provide two out-of-domain testing sets for cross-domain
evaluation, as well as a set of human translated queries for for cross-lingual
retrieval evaluation. The experiments demonstrate that DuReader_retrieval is
challenging and a number of problems remain unsolved, such as the salient
phrase mismatch and the syntactic mismatch between queries and paragraphs.
These experiments also show that dense retrievers do not generalize well across
domains, and cross-lingual retrieval is essentially challenging.
DuReader_retrieval is publicly available at
https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Dual Encoder Architectures for Question Answering. (arXiv:2204.07120v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07120">
<div class="article-summary-box-inner">
<span><p>Dual encoders have been used for question-answering (QA) and information
retrieval (IR) tasks with good results. Previous research focuses on two major
types of dual encoders, Siamese Dual Encoder (SDE), with parameters shared
across two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly
parameterized encoders. In this work, we explore different ways in which the
dual encoder can be structured, and evaluate how these differences can affect
their efficacy in terms of QA retrieval tasks. By evaluating on MS MARCO, open
domain NQ and the MultiReQA benchmarks, we show that SDE performs significantly
better than ADE. We further propose three different improved versions of ADEs
by sharing or freezing parts of the architectures between two encoder towers.
We find that sharing parameters in projection layers would enable ADEs to
perform competitively with or outperform SDEs. We further explore and explain
why parameter sharing in projection layer significantly improves the efficacy
of the dual encoders, by directly probing the embedding spaces of the two
encoder towers with t-SNE algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10989">
<div class="article-summary-box-inner">
<span><p>Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited in scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
pliable and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges to represent rich
compositional semantics. Moreover, we propose an inheritance hierarchy
mechanism focusing on domain extensibility. Additionally, we annotated
DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with
DMR. We propose two evaluation tasks to evaluate different dialogue models and
a novel coreference resolution model GNNCoref for the graph-based coreference
resolution task. Experiments show that DMR can be parsed well with pre-trained
Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankGen: Improving Text Generation with Large Ranking Models. (arXiv:2205.09726v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09726">
<div class="article-summary-box-inner">
<span><p>Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter encoder
model for English that scores model generations given a prefix. RankGen can be
flexibly incorporated as a scoring function in beam search and used to decode
from any pretrained language model. We train RankGen using large-scale
contrastive learning to map a prefix close to the ground-truth sequence that
follows it and far away from two types of negatives: (1) random sequences from
the same document as the prefix, and (2) sequences generated from a large
language model conditioned on the prefix. Experiments across four different
language models (345M-11B parameters) and two domains show that RankGen
significantly outperforms decoding algorithms like nucleus, top-k, and typical
sampling, as well as contrastive decoding and search, on both automatic metrics
(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English
writers (74.5% human preference over nucleus sampling). Analysis reveals that
RankGen outputs are more relevant to the prefix and improve continuity and
coherence compared to baselines. We release our model checkpoints, code, and
human preference data with explanations to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fine-grained Interpretability Evaluation Benchmark for Neural NLP. (arXiv:2205.11097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11097">
<div class="article-summary-box-inner">
<span><p>While there is increasing concern about the interpretability of neural
models, the evaluation of interpretability remains an open problem, due to the
lack of proper evaluation datasets and metrics. In this paper, we present a
novel benchmark to evaluate the interpretability of both neural models and
saliency methods. This benchmark covers three representative NLP tasks:
sentiment analysis, textual similarity and reading comprehension, each provided
with both English and Chinese annotated data. In order to precisely evaluate
the interpretability, we provide token-level rationales that are carefully
annotated to be sufficient, compact and comprehensive. We also design a new
metric, i.e., the consistency between the rationales before and after
perturbations, to uniformly evaluate the interpretability on different types of
tasks. Based on this benchmark, we conduct experiments on three typical models
with three saliency methods, and unveil their strengths and weakness in terms
of interpretability. We will release this benchmark
https://www.luge.ai/#/luge/task/taskDetail?taskId=15 and hope it can facilitate
the research in building trustworthy systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11697">
<div class="article-summary-box-inner">
<span><p>Recently Convolution-augmented Transformer (Conformer) has shown promising
results in Automatic Speech Recognition (ASR), outperforming the previous best
published Transformer Transducer. In this work, we believe that the output
information of each block in the encoder and decoder is not completely
inclusive, in other words, their output information may be complementary. We
study how to take advantage of the complementary information of each block in a
parameter-efficient way, and it is expected that this may lead to more robust
performance. Therefore we propose the Block-augmented Transformer for speech
recognition, named Blockformer. We have implemented two block ensemble methods:
the base Weighted Sum of the Blocks Output (Base-WSBO), and the
Squeeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).
Experiments have proved that the Blockformer significantly outperforms the
state-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER
of 4.29\% without using a language model and 4.05\% with an external language
model on the testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. (arXiv:2210.03797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03797">
<div class="article-summary-box-inner">
<span><p>Recent progress in language model pre-training has led to important
improvements in Named Entity Recognition (NER). Nonetheless, this progress has
been mainly tested in well-formatted documents such as news, Wikipedia, or
scientific articles. In social media the landscape is different, in which it
adds another layer of complexity due to its noisy and dynamic nature. In this
paper, we focus on NER in Twitter, one of the largest social media platforms,
and construct a new NER dataset, TweetNER7, which contains seven entity types
annotated over 11,382 tweets from September 2019 to August 2021. The dataset
was constructed by carefully distributing the tweets over time and taking
representative trends as a basis. Along with the dataset, we provide a set of
language model baselines and perform an analysis on the language model
performance on the task, especially analyzing the impact of different time
periods. In particular, we focus on three important temporal aspects in our
analysis: short-term degradation of NER models over time, strategies to
fine-tune a language model over different periods, and self-labeling as an
alternative to lack of recently-labeled data. TweetNER7 is released publicly
(https://huggingface.co/datasets/tner/tweetner7) along with the models
fine-tuned on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05098">
<div class="article-summary-box-inner">
<span><p>The ability to extract high-quality translation dictionaries from monolingual
word embedding spaces depends critically on the geometric similarity of the
spaces -- their degree of "isomorphism." We address the root-cause of faulty
cross-lingual mapping: that word embedding training resulted in the underlying
spaces being non-isomorphic. We incorporate global measures of isomorphism
directly into the Skip-gram loss function, successfully increasing the relative
isomorphism of trained word embedding spaces and improving their ability to be
mapped to a shared cross-lingual space. The result is improved bilingual
lexicon induction in general data conditions, under domain mismatch, and with
training algorithm dissimilarities. We release IsoVec at
https://github.com/kellymarchisio/isovec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13952">
<div class="article-summary-box-inner">
<span><p>We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering. (arXiv:2210.14353v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14353">
<div class="article-summary-box-inner">
<span><p>We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17406">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been reported to have strong performance on
natural language processing tasks. However, performance metrics such as
accuracy do not measure the quality of the model in terms of its ability to
robustly represent complex linguistic structure. In this work, we propose a
framework to evaluate the robustness of linguistic representations using
probing tasks. We leverage recent advances in extracting emergent linguistic
constructs from LLMs and apply syntax-preserving perturbations to test the
stability of these constructs in order to better understand the representations
learned by LLMs. Empirically, we study the performance of four LLMs across six
different corpora on the proposed robustness measures. We provide evidence that
context-free representation (e.g., GloVe) are in some cases competitive with
context-dependent representations from modern LLMs (e.g., BERT), yet equally
brittle to syntax-preserving manipulations. Emergent syntactic representations
in neural networks are brittle, thus our work poses the attention on the risk
of comparing such structures to those that are object of a long lasting debate
in linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token. (arXiv:2211.04898v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04898">
<div class="article-summary-box-inner">
<span><p>The pre-training of masked language models (MLMs) consumes massive
computation to achieve good results on downstream NLP tasks, resulting in a
large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as
placeholders and gather the contextualized information from unmasked tokens to
restore the corrupted information. It raises the question of whether we can
append [MASK]s at a later layer, to reduce the sequence length for earlier
layers and make the pre-training more efficient. We show: (1) [MASK]s can
indeed be appended at a later layer, being disentangled from the word
embedding; (2) The gathering of contextualized information from unmasked tokens
can be conducted with a few layers. By further increasing the masking rate from
15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with
only 78% and 68% of the original computational budget without any degradation
on the GLUE benchmark. When pre-training with the original budget, our method
outperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06774">
<div class="article-summary-box-inner">
<span><p>When trained on large-scale datasets, image captioning models can understand
the content of images from a general domain but often fail to generate
accurate, detailed captions. To improve performance, pretraining-and-finetuning
has been a key strategy for image captioning. However, we find that large-scale
bidirectional training between image and text enables zero-shot image
captioning. In this paper, we introduce Bidirectional Image Text Training in
largER Scale, BITTERS, an efficient training and inference framework for
zero-shot image captioning. We also propose a new evaluation benchmark which
comprises of high quality datasets and an extensive set of metrics to properly
evaluate zero-shot captioning accuracy and societal bias. We additionally
provide an efficient finetuning approach for keyword extraction. We show that
careful selection of large-scale training set and model architecture is the key
to achieving zero-shot image captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What would Harry say? Building Dialogue Agents for Characters in a Story. (arXiv:2211.06869v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06869">
<div class="article-summary-box-inner">
<span><p>We present HPD: Harry Potter Dialogue Dataset to facilitate the study of
building dialogue agents for characters in a story. It differs from existing
dialogue datasets in two aspects: 1) HPD provides rich background information
about the novel Harry Potter, including scene, character attributes, and
character relations; 2) All these background information will change as the
story goes on. In other words, each dialogue session in HPD correlates to a
different background, and the storyline determines how the background changes.
We evaluate some baselines (e.g., GPT-2, BOB) on both automatic and human
metrics to determine how well they can generate Harry Potter-like responses.
Experimental results indicate that although the generated responses are fluent
and relevant to the dialogue history, they are remained to sound out of
character for Harry, indicating there is a large headroom for future studies.
Our dataset is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Adversarial Training with Robust Early-Bird Tickets. (arXiv:2211.07263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07263">
<div class="article-summary-box-inner">
<span><p>Adversarial training is one of the most powerful methods to improve the
robustness of pre-trained language models (PLMs). However, this approach is
typically more expensive than traditional fine-tuning because of the necessity
to generate adversarial examples via gradient descent. Delving into the
optimization process of adversarial training, we find that robust connectivity
patterns emerge in the early training phase (typically $0.15\sim0.3$ epochs),
far before parameters converge. Inspired by this finding, we dig out robust
early-bird tickets (i.e., subnetworks) to develop an efficient adversarial
training method: (1) searching for robust tickets with structured sparsity in
the early stage; (2) fine-tuning robust tickets in the remaining time. To
extract the robust tickets as early as possible, we design a ticket convergence
metric to automatically terminate the searching process. Experiments show that
the proposed efficient adversarial training method can achieve up to $7\times
\sim 13 \times$ training speedups while maintaining comparable or even better
robustness compared to the most competitive state-of-the-art adversarial
training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing and Adversarial: Improve ASR with Speaker Labels. (arXiv:2211.06369v1 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06369">
<div class="article-summary-box-inner">
<span><p>ASR can be improved by multi-task learning (MTL) with domain enhancing or
domain adversarial training, which are two opposite objectives with the aim to
increase/decrease domain variance towards domain-aware/agnostic ASR,
respectively. In this work, we study how to best apply these two opposite
objectives with speaker labels to improve conformer-based ASR. We also propose
a novel adaptive gradient reversal layer for stable and effective adversarial
training without tuning effort. Detailed analysis and experimental verification
are conducted to show the optimal positions in the ASR neural network (NN) to
apply speaker enhancing and adversarial training. We also explore their
combination for further improvement, achieving the same performance as
i-vectors plus adversarial training. Our best speaker-based MTL achieves 7\%
relative improvement on the Switchboard Hub5'00 set. We also investigate the
effect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-16 23:14:26.470456644 UTC">2022-11-16 23:14:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>