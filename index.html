<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-15T01:30:00Z">11-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">FinTech for Social Good: A Research Agenda from NLP Perspective. (arXiv:2211.06431v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06431">
<div class="article-summary-box-inner">
<span><p>Making our research results positively impact on society and environment is
one of the goals our community has been pursuing recently. Although financial
technology (FinTech) is one of the popular application fields, we notice that
there is no discussion on how NLP can help in FinTech for the social good. When
mentioning FinTech for social good, people are talking about financial
inclusion and green finance. However, the role of NLP in these directions only
gets limited discussions. To fill this gap, this paper shares our idea of how
we can use NLP in FinTech for social good. We hope readers can rethink the
relationship between finance and NLP based on our sharing, and further join us
in improving the financial literacy of individual investors and improving the
supports for impact investment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning. (arXiv:2211.06452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06452">
<div class="article-summary-box-inner">
<span><p>The prevalence of abusive language on different online platforms has been a
major concern that raises the need for automated cross-platform abusive
language detection. However, prior works focus on concatenating data from
multiple platforms, inherently adopting Empirical Risk Minimization (ERM)
method. In this work, we address this challenge from the perspective of domain
generalization objective. We design SCL-Fish, a supervised contrastive learning
integrated meta-learning algorithm to detect abusive language on unseen
platforms. Our experimental analysis shows that SCL-Fish achieves better
performance over ERM and the existing state-of-the-art models. We also show
that SCL-Fish is data-efficient and achieves comparable performance with the
large-scale pre-trained models upon finetuning for the abusive language
detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-to-Speech Translation For A Real-world Unwritten Language. (arXiv:2211.06474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06474">
<div class="article-summary-box-inner">
<span><p>We study speech-to-speech translation (S2ST) that translates speech from one
language into another language and focuses on building systems to support
languages without standard text writing systems. We use English-Taiwanese
Hokkien as a case study, and present an end-to-end solution from training data
collection, modeling choices to benchmark dataset release. First, we present
efforts on creating human annotated data, automatically mining data from large
unlabeled speech datasets, and adopting pseudo-labeling to produce weakly
supervised data. On the modeling, we take advantage of recent advances in
applying self-supervised discrete representations as target for prediction in
S2ST and show the effectiveness of leveraging additional text supervision from
Mandarin, a language similar to Hokkien, in model training. Finally, we release
an S2ST benchmark set to facilitate future research in this field. The demo can
be found at https://huggingface.co/spaces/facebook/Hokkien_Translation .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A unified one-shot prosody and speaker conversion system with self-supervised discrete speech units. (arXiv:2211.06535v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06535">
<div class="article-summary-box-inner">
<span><p>We present a unified system to realize one-shot voice conversion (VC) on the
pitch, rhythm, and speaker attributes. Existing works generally ignore the
correlation between prosody and language content, leading to the degradation of
naturalness in converted speech. Additionally, the lack of proper language
features prevents these systems from accurately preserving language content
after conversion. To address these issues, we devise a cascaded modular system
leveraging self-supervised discrete speech units as language representation.
These discrete units provide duration information essential for rhythm
modeling. Our system first extracts utterance-level prosody and speaker
representations from the raw waveform. Given the prosody representation, a
prosody predictor estimates pitch, energy, and duration for each discrete unit
in the utterance. A synthesizer further reconstructs speech based on the
predicted prosody, speaker representation, and discrete units. Experiments show
that our system outperforms previous approaches in naturalness,
intelligibility, speaker transferability, and prosody transferability. Code and
samples are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06552">
<div class="article-summary-box-inner">
<span><p>Human intelligence can remarkably adapt quickly to new tasks and
environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research which
can enable similar capabilities in machines, we made the following
contributions (1) formalized the collaborative embodied agent using natural
language task; (2) developed a tool for extensive and scalable data collection;
and (3) collected the first dataset for interactive grounded language
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong and Continual Learning Dialogue Systems. (arXiv:2211.06553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06553">
<div class="article-summary-box-inner">
<span><p>Dialogue systems, commonly known as chatbots, have gained escalating
popularity in recent times due to their wide-spread applications in carrying
out chit-chat conversations with users and task-oriented dialogues to
accomplish various user tasks. Existing chatbots are usually trained from
pre-collected and manually-labeled data and/or written with handcrafted rules.
Many also use manually-compiled knowledge bases (KBs). Their ability to
understand natural language is still limited, and they tend to produce many
errors resulting in poor user satisfaction. Typically, they need to be
constantly improved by engineers with more labeled data and more manually
compiled knowledge. This book introduces the new paradigm of lifelong learning
dialogue systems to endow chatbots the ability to learn continually by
themselves through their own self-initiated interactions with their users and
working environments to improve themselves. As the systems chat more and more
with users or learn more and more from external sources, they become more and
more knowledgeable and better and better at conversing. The book presents the
latest developments and techniques for building such continual learning
dialogue systems that continuously learn new language expressions and lexical
and factual knowledge during conversation from users and off conversation from
external sources, acquire new training examples during conversation, and learn
conversational skills. Apart from these general topics, existing works on
continual learning of some specific aspects of dialogue systems are also
surveyed. The book concludes with a discussion of open challenges for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness of DistilHuBERT to Unseen Noisy Conditions via Data Augmentation, Curriculum Learning, and Multi-Task Enhancement. (arXiv:2211.06562v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06562">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation learning aims to extract meaningful
factors from the speech signal that can later be used across different
downstream tasks, such as speech and/or emotion recognition. Existing models,
such as HuBERT, however, can be fairly large thus may not be suitable for edge
speech applications. Moreover, realistic applications typically involve speech
corrupted by noise and room reverberation, hence models need to provide
representations that are robust to such environmental factors. In this study,
we build on the so-called DistilHuBERT model, which distils HuBERT to a
fraction of its original size, with three modifications, namely: (i) augment
the training data with noise and reverberation, while the student model needs
to distill the clean representations from the teacher model; (ii) introduce a
curriculum learning approach where increasing levels of noise are introduced as
the model trains, thus helping with convergence and with the creation of more
robust representations; and (iii) introduce a multi-task learning approach
where the model also reconstructs the clean waveform jointly with the
distillation task, thus also acting as an enhancement step to ensure additional
environment robustness to the representation. Experiments on three SUPERB tasks
show the advantages of the proposed method not only relative to the original
DistilHuBERT, but also to the original HuBERT, thus showing the advantages of
the proposed method for ``in the wild'' edge speech applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Textual Adversaries with Minimal Perturbation. (arXiv:2211.06571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06571">
<div class="article-summary-box-inner">
<span><p>Many word-level adversarial attack approaches for textual data have been
proposed in recent studies. However, due to the massive search space consisting
of combinations of candidate words, the existing approaches face the problem of
preserving the semantics of texts when crafting adversarial counterparts. In
this paper, we develop a novel attack strategy to find adversarial texts with
high similarity to the original texts while introducing minimal perturbation.
The rationale is that we expect the adversarial texts with small perturbation
can better preserve the semantic meaning of original texts. Experiments show
that, compared with state-of-the-art attack approaches, our approach achieves
higher success rates and lower perturbation rates in four benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06607">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis is a trending topic with the explosion of
multimodal content on the web. Present studies in multimodal sentiment analysis
rely on large-scale supervised data. Collating supervised data is
time-consuming and labor-intensive. As such, it is essential to investigate the
problem of few-shot multimodal sentiment analysis. Previous works in few-shot
models generally use language model prompts, which can improve performance in
low-resource settings. However, the textual prompt ignores the information from
other modalities. We propose Multimodal Probabilistic Fusion Prompts, which can
provide diverse cues for multimodal sentiment detection. We first design a
unified multimodal prompt to reduce the discrepancy in different modal prompts.
To improve the robustness of our model, we then leverage multiple diverse
prompts for each input and propose a probabilistic method to fuse the output
predictions. Extensive experiments conducted on three datasets confirm the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConceptX: A Framework for Latent Concept Analysis. (arXiv:2211.06642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06642">
<div class="article-summary-box-inner">
<span><p>The opacity of deep neural networks remains a challenge in deploying
solutions where explanation is as important as precision. We present ConceptX,
a human-in-the-loop framework for interpreting and annotating latent
representational space in pre-trained Language Models (pLMs). We use an
unsupervised method to discover concepts learned in these models and enable a
graphical interface for humans to generate explanations for the concepts. To
facilitate the process, we provide auto-annotations of the concepts (based on
traditional linguistic ontologies). Such annotations enable development of a
linguistic resource that directly represents latent concepts learned within
deep NLP models. These include not just traditional linguistic concepts, but
also task-specific or sensitive concepts (words grouped based on gender or
religious connotation) that helps the annotators to mark bias in the model. The
framework consists of two parts (i) concept discovery and (ii) annotation
platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLPeer: A Unified Resource for the Computational Study of Peer Review. (arXiv:2211.06651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06651">
<div class="article-summary-box-inner">
<span><p>Peer review is a core component of scholarly publishing, yet it is
time-consuming, requires considerable expertise, and is prone to error. The
applications of NLP for peer reviewing assistance aim to mitigate those issues,
but the lack of clearly licensed datasets and multi-domain corpora prevent the
systematic study of NLP for peer review. To remedy this, we introduce NLPeer --
the first ethically sourced multidomain corpus of more than 5k papers and 11k
review reports from five different venues. In addition to the new datasets of
paper drafts, camera-ready versions and peer reviews from the NLP community, we
establish a unified data representation, and augment previous peer review
datasets to include parsed, structured paper representations, rich metadata and
versioning information. Our work paves the path towards systematic,
multi-faceted, evidence-based study of peer review in NLP and beyond. We make
NLPeer publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing Segmentation Ambiguity in Neural Linguistic Steganography. (arXiv:2211.06662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06662">
<div class="article-summary-box-inner">
<span><p>Previous studies on neural linguistic steganography, except Ueoka et al.
(2021), overlook the fact that the sender must detokenize cover texts to avoid
arousing the eavesdropper's suspicion. In this paper, we demonstrate that
segmentation ambiguity indeed causes occasional decoding failures at the
receiver's side. With the near-ubiquity of subwords, this problem now affects
any language. We propose simple tricks to overcome this problem, which are even
applicable to languages without explicit word boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities. (arXiv:2211.06679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06679">
<div class="article-summary-box-inner">
<span><p>In this work, we present a conceptually simple and effective method to train
a strong bilingual multimodal representation model. Starting from the
pretrained multimodal representation model CLIP released by OpenAI, we switched
its text encoder with a pretrained multilingual text encoder XLM-R, and aligned
both languages and image representations by a two-stage training schema
consisting of teacher learning and contrastive learning. We validate our method
through evaluations of a wide range of tasks. We set new state-of-the-art
performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and
COCO-CN. Further, we obtain very close performances with CLIP on almost all
tasks, suggesting that one can simply alter the text encoder in CLIP for
extended capabilities such as multilingual understanding. Our models and code
are available at https://github.com/FlagAI-Open/FlagAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06774">
<div class="article-summary-box-inner">
<span><p>When trained on large-scale datasets, image captioning models can understand
the content of images from a general domain but often fail to generate
accurate, detailed captions. To improve performance, pretraining-and-finetuning
has been a key strategy for image captioning. However, we find that large-scale
bidirectional training between image and text enables zero-shot image
captioning. In this paper, we introduce Bidirectional Image Text Training in
largER Scale, BITTERS, an efficient training and inference framework for
zero-shot image captioning. We also propose a new evaluation benchmark which
comprises of high quality datasets and an extensive set of metrics to properly
evaluate zero-shot captioning accuracy and societal bias. We additionally
provide an efficient finetuning approach for keyword extraction. We show that
careful selection of large-scale training set and model architecture is the key
to achieving zero-shot image captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Data Augmentation for Patient Outcomes Prediction. (arXiv:2211.06778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06778">
<div class="article-summary-box-inner">
<span><p>Deep learning models have demonstrated superior performance in various
healthcare applications. However, the major limitation of these deep models is
usually the lack of high-quality training data due to the private and sensitive
nature of this field. In this study, we propose a novel textual data
augmentation method to generate artificial clinical notes in patients'
Electronic Health Records (EHRs) that can be used as additional training data
for patient outcomes prediction. Essentially, we fine-tune the generative
language model GPT-2 to synthesize labeled text with the original training
data. More specifically, We propose a teacher-student framework where we first
pre-train a teacher model on the original data, and then train a student model
on the GPT-augmented data under the guidance of the teacher. We evaluate our
method on the most common patient outcome, i.e., the 30-day readmission rate.
The experimental results show that deep models can improve their predictive
performance with the augmented data, indicating the effectiveness of the
proposed architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FPT: Improving Prompt Tuning Efficiency via Progressive Training. (arXiv:2211.06840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06840">
<div class="article-summary-box-inner">
<span><p>Recently, prompt tuning (PT) has gained increasing attention as a
parameter-efficient way of tuning pre-trained language models (PLMs). Despite
extensively reducing the number of tunable parameters and achieving satisfying
performance, PT is training-inefficient due to its slow convergence. To improve
PT's training efficiency, we first make some novel observations about the
prompt transferability of "partial PLMs", which are defined by compressing a
PLM in depth or width. We observe that the soft prompts learned by different
partial PLMs of various sizes are similar in the parameter space, implying that
these soft prompts could potentially be transferred among partial PLMs.
Inspired by these observations, we propose Fast Prompt Tuning (FPT), which
starts by conducting PT using a small-scale partial PLM, and then progressively
expands its depth and width until the full-model size. After each expansion, we
recycle the previously learned soft prompts as initialization for the enlarged
partial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5
tasks and show that FPT could save over 30% training computations while
achieving comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausaLM: Causal Model Explanation Through Counterfactual Language Models. (arXiv:2005.13407v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13407">
<div class="article-summary-box-inner">
<span><p>Understanding predictions made by deep neural networks is notoriously
difficult, but also crucial to their dissemination. As all machine learning
based methods, they are as good as their training data, and can also capture
unwanted biases. While there are tools that can help understand whether such
biases exist, they do not distinguish between correlation and causation, and
might be ill-suited for text-based models and for reasoning about high level
language concepts. A key problem of estimating the causal effect of a concept
of interest on a given model is that this estimation requires the generation of
counterfactual examples, which is challenging with existing generation
technology. To bridge that gap, we propose CausaLM, a framework for producing
causal model explanations using counterfactual language representation models.
Our approach is based on fine-tuning of deep contextualized embedding models
with auxiliary adversarial tasks derived from the causal graph of the problem.
Concretely, we show that by carefully choosing auxiliary adversarial
pre-training tasks, language representation models such as BERT can effectively
learn a counterfactual representation for a given concept of interest, and be
used to estimate its true causal effect on model performance. A byproduct of
our method is a language representation model that is unaffected by the tested
concept, which can be useful in mitigating unwanted bias ingrained in the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHARE: a System for Hierarchical Assistive Recipe Editing. (arXiv:2105.08185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08185">
<div class="article-summary-box-inner">
<span><p>The large population of home cooks with dietary restrictions is under-served
by existing cooking resources and recipe generation models. To help them, we
propose the task of controllable recipe editing: adapt a base recipe to satisfy
a user-specified dietary constraint. This task is challenging, and cannot be
adequately solved with human-written ingredient substitution rules or existing
end-to-end recipe generation models. We tackle this problem with SHARE: a
System for Hierarchical Assistive Recipe Editing, which performs simultaneous
ingredient substitution before generating natural-language steps using the
edited ingredients. By decoupling ingredient and step editing, our step
generator can explicitly integrate the available ingredients. Experiments on
the novel RecipePairs dataset -- 83K pairs of similar recipes where each recipe
satisfies one of seven dietary constraints -- demonstrate that SHARE produces
convincing, coherent recipes that are appropriate for a target dietary
constraint. We further show through human evaluations and real-world cooking
trials that recipes edited by SHARE can be easily followed by home cooks to
create appealing dishes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Math Word Problems using Pretrained Multilingual Language Models. (arXiv:2105.08928v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08928">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit math word problems~(MWPs) from the cross-lingual
and multilingual perspective. We construct our MWP solvers over pretrained
multilingual language models using sequence-to-sequence model with copy
mechanism. We compare how the MWP solvers perform in cross-lingual and
multilingual scenarios. To facilitate the comparison of cross-lingual
performance, we first adapt the large-scale English dataset MathQA as a
counterpart of the Chinese dataset Math23K. Then we extend several English
datasets to bilingual datasets through machine translation plus human
annotation. Our experiments show that the MWP solvers may not be transferred to
a different language even if the target expressions have the same operator set
and constants. But for both cross-lingual and multilingual cases, it can be
better generalized if problem types exist on both source language and target
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07410">
<div class="article-summary-box-inner">
<span><p>Given the recent proliferation of false claims online, there has been a lot
of manual fact-checking effort. As this is very time-consuming, human
fact-checkers can benefit from tools that can support them and make them more
efficient. Here, we focus on building a system that could provide such support.
Given an input document, it aims to detect all sentences that contain a claim
that can be verified by some previously fact-checked claims (from a given
database). The output is a re-ranked list of the document sentences, so that
those that can be verified are ranked as high as possible, together with
corresponding evidence. Unlike previous work, which has looked into claim
retrieval, here we take a document-level perspective. We create a new manually
annotated dataset for this task, and we propose suitable evaluation measures.
We further experiment with a learning-to-rank approach, achieving sizable
performance gains over several strong baselines. Our analysis demonstrates the
importance of modeling text similarity and stance, while also taking into
account the veracity of the retrieved previously fact-checked claims. We
believe that this research would be of interest to fact-checkers, journalists,
media, and regulatory authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method. (arXiv:2109.09617v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09617">
<div class="article-summary-box-inner">
<span><p>Lyric-to-melody generation is an important task in automatic songwriting.
Previous lyric-to-melody generation systems usually adopt end-to-end models
that directly generate melodies from lyrics, which suffer from several issues:
1) lack of paired lyric-melody training data; 2) lack of control on generated
melodies. In this paper, we develop TeleMelody, a two-stage lyric-to-melody
generation system with music template (e.g., tonality, chord progression,
rhythm pattern, and cadence) to bridge the gap between lyrics and melodies
(i.e., the system consists of a lyric-to-template module and a
template-to-melody module). TeleMelody has two advantages. First, it is data
efficient. The template-to-melody module is trained in a self-supervised way
(i.e., the source template is extracted from the target melody) that does not
need any lyric-melody paired data. The lyric-to-template module is made up of
some rules and a lyric-to-rhythm model, which is trained with paired
lyric-rhythm data that is easier to obtain than paired lyric-melody data.
Second, it is controllable. The design of template ensures that the generated
melodies can be controlled by adjusting the musical elements in template. Both
subjective and objective experimental evaluations demonstrate that TeleMelody
generates melodies with higher quality, better controllability, and less
requirement on paired lyric-melody data than previous generation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actionable Entities Recognition Benchmark for Interactive Fiction. (arXiv:2109.13855v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13855">
<div class="article-summary-box-inner">
<span><p>This paper presents a new natural language processing task - Actionable
Entities Recognition (AER) - recognition of entities that protagonists could
interact with for further plot development. Though similar to classical Named
Entity Recognition (NER), it has profound differences. In particular, it is
crucial for interactive fiction, where the agent needs to detect entities that
might be useful in the future. We also discuss if AER might be further helpful
for the systems dealing with narrative processing since actionable entities
profoundly impact the causal relationship in a story. We validate the proposed
task on two previously available datasets and present a new benchmark dataset
for the AER task that includes 5550 descriptions with one or more actionable
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06918">
<div class="article-summary-box-inner">
<span><p>Despite their recent popularity and well-known advantages, dense retrievers
still lag behind sparse methods such as BM25 in their ability to reliably match
salient phrases and rare entities in the query and to generalize to
out-of-domain data. It has been argued that this is an inherent limitation of
dense models. We rebut this claim by introducing the Salient Phrase Aware
Retriever (SPAR), a dense retriever with the lexical matching capacity of a
sparse model. We show that a dense Lexical Model {\Lambda} can be trained to
imitate a sparse one, and SPAR is built by augmenting a standard dense
retriever with {\Lambda}. Empirically, SPAR shows superior performance on a
range of tasks including five question answering datasets, MS MARCO passage
retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain
evaluation, exceeding the performance of state-of-the-art dense and sparse
retrievers. The code and models of SPAR are available at:
https://github.com/facebookresearch/dpr-scale/tree/main/spar
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Embedded Knowledge Graph Multi-hop Question Answering by introducing Relational Chain Reasoning. (arXiv:2110.12679v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12679">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Question Answering (KGQA) aims to answer user-questions from
a knowledge graph (KG) by identifying the reasoning relations between topic
entity and answer. As a complex branch task of KGQA, multi-hop KGQA requires
reasoning over the multi-hop relational chain preserved in KG to arrive at the
right answer. Despite recent successes, the existing works on answering
multi-hop complex questions still face the following challenges: i) The absence
of an explicit relational chain order reflected in user-question stems from a
misunderstanding of a user's intentions. ii) Incorrectly capturing relational
types on weak supervision of which dataset lacks intermediate reasoning chain
annotations due to expensive labeling cost. iii) Failing to consider implicit
relations between the topic entity and the answer implied in structured KG
because of limited neighborhoods size constraint in subgraph retrieval-based
algorithms.To address these issues in multi-hop KGQA, we propose a novel model
herein, namely Relational Chain based Embedded KGQA (Rce-KGQA), which
simultaneously utilizes the explicit relational chain revealed in natural
language question and the implicit relational chain stored in structured KG.
Our extensive empirical study on three open-domain benchmarks proves that our
method significantly outperforms the state-of-the-art counterparts like
GraftNet, PullNet and EmbedKGQA. Comprehensive ablation experiments also verify
the effectiveness of our method on the multi-hop KGQA task. We have made our
model's source code available at github:
https://github.com/albert-jin/Rce-KGQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01901">
<div class="article-summary-box-inner">
<span><p>Object grounding tasks aim to locate the target object in an image through
verbal communications. Understanding human command is an important process
needed for effective human-robot communication. However, this is challenging
because human commands can be ambiguous and erroneous. This paper aims to
disambiguate the human's referring expressions by allowing the agent to ask
relevant questions based on semantic data obtained from scene graphs. We test
if our agent can use relations between objects from a scene graph to ask
semantically relevant questions that can disambiguate the original user
command. In this paper, we present Incremental Grounding using Scene Graphs
(IGSG), a disambiguation model that uses semantic data from an image scene
graph and linguistic structures from a language scene graph to ground objects
based on human command. Compared to the baseline, IGSG shows promising results
in complex real-world scenes where there are multiple identical target objects.
IGSG can effectively disambiguate ambiguous or wrong referring expressions by
asking disambiguating questions back to the user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-box Prompt Learning for Pre-trained Language Models. (arXiv:2201.08531v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08531">
<div class="article-summary-box-inner">
<span><p>The increasing scale of general-purpose Pre-trained Language Models (PLMs)
necessitates the study of more efficient adaptation across different downstream
tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)
to resonate with pragmatic interactions between the cloud infrastructure and
edge devices. Particularly, instead of fine-tuning the model in the cloud, we
adapt PLMs by prompt learning, which efficiently optimizes only a few
parameters of the discrete prompts. Moreover, we consider the scenario that we
do not have access to the parameters and gradients of the pre-trained models,
except for its outputs given inputs. This black-box setting secures the cloud
infrastructure from potential attack and misuse to cause a single-point
failure, which is preferable to the white-box counterpart by current
infrastructures. Under this black-box constraint, we apply a variance-reduced
policy gradient algorithm to estimate the gradients of parameters in the
categorical distribution of each discrete prompt. In light of our method, the
user devices can efficiently tune their tasks by querying the PLMs bounded by a
range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the
proposed algorithm achieves significant improvement on eight benchmarks in a
cloud-device collaboration manner. Finally, we conduct in-depth case studies to
comprehensively analyze our method in terms of various data sizes, prompt
lengths, training budgets, optimization objectives, prompt transferability, and
explanations of the learned prompts. Our code will be available at
https://github.com/shizhediao/Black-Box-Prompt-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models. (arXiv:2202.04053v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04053">
<div class="article-summary-box-inner">
<span><p>Recently, DALL-E, a multimodal transformer language model, and its variants
(including diffusion models) have shown high-quality text-to-image generation
capabilities. However, despite the interesting image generation results, there
has not been a detailed analysis on how to evaluate such models. In this work,
we investigate the visual reasoning capabilities and social biases of different
text-to-image models, covering both multimodal transformer language models and
diffusion models. First, we measure three visual reasoning skills: object
recognition, object counting, and spatial relation understanding. For this, we
propose PaintSkills, a compositional diagnostic dataset and evaluation toolkit
that measures these skills. In our experiments, there exists a large gap
between the performance of recent text-to-image models and the upper bound
accuracy in object counting and spatial relation understanding skills. Second,
we assess gender and skin tone biases by measuring the variance of the
gender/skin tone distribution based on automated and human evaluation. We
demonstrate that recent text-to-image models learn specific gender/skin tone
biases from web image-text pairs. We hope that our work will help guide future
progress in improving text-to-image generation models on visual reasoning
skills and learning socially unbiased representations. Code and data:
https://github.com/j-min/DallEval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UDAAN: Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01644">
<div class="article-summary-box-inner">
<span><p>We introduce UDAAN, an open-source post-editing tool that can reduce manual
editing efforts to quickly produce publishable-standard documents in several
Indic languages. UDAAN has an end-to-end Machine Translation (MT) plus
post-editing pipeline wherein users can upload a document to obtain raw MT
output. Further, users can edit the raw translations using our tool. UDAAN
offers several advantages: a) Domain-aware, vocabulary-based lexical
constrained MT. b) source-target and target-target lexicon suggestions for
users. Replacements are based on the source and target texts lexicon alignment.
c) Translation suggestions are based on logs created during user interaction.
d) Source-target sentence alignment visualisation that reduces the cognitive
load of users during editing. e) Translated outputs from our tool are available
in multiple formats: docs, latex, and PDF. We also provide the facility to use
around 100 in-domain dictionaries for lexicon-aware machine translation.
Although we limit our experiments to English-to-Hindi translation, our tool is
independent of the source and target languages. Experimental results based on
the usage of the tools and users feedback show that our tool speeds up the
translation time by approximately a factor of three compared to the baseline
method of translating documents from scratch. Our tool is available for both
Windows and Linux platforms. The tool is open-source under MIT license, and the
source code can be accessed from our website, https://www.udaanproject.org.
Demonstration and tutorial videos for various features of our tool can be
accessed here. Our MT pipeline can be accessed at
https://udaaniitb.aicte-india.org/udaan/translate/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScienceWorld: Is your Agent Smarter than a 5th Grader?. (arXiv:2203.07540v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07540">
<div class="article-summary-box-inner">
<span><p>We present ScienceWorld, a benchmark to test agents' scientific reasoning
abilities in a new interactive text environment at the level of a standard
elementary school science curriculum. Despite the transformer-based progress
seen in question-answering and scientific text processing, we find that current
models cannot reason about or explain learned science concepts in novel
contexts. For instance, models can easily answer what the conductivity of a
known material is but struggle when asked how they would conduct an experiment
in a grounded environment to find the conductivity of an unknown material. This
begs the question of whether current models are simply retrieving answers by
way of seeing a large number of similar examples or if they have learned to
reason about concepts in a reusable manner. We hypothesize that agents need to
be grounded in interactive environments to achieve such reasoning capabilities.
Our experiments provide empirical evidence supporting this hypothesis --
showing that a 1.5 million parameter agent trained interactively for 100k steps
outperforms a 11 billion parameter model statically trained for scientific
question-answering and reasoning from millions of expert demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Science Named Entity Recognition in the Open Research Knowledge Graph. (arXiv:2203.14579v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14579">
<div class="article-summary-box-inner">
<span><p>Domain-specific named entity recognition (NER) on Computer Science (CS)
scholarly articles is an information extraction task that is arguably more
challenging for the various annotation aims that can beset the task and has
been less studied than NER in the general domain. Given that significant
progress has been made on NER, we believe that scholarly domain-specific NER
will receive increasing attention in the years to come. Currently, progress on
CS NER -- the focus of this work -- is hampered in part by its recency and the
lack of a standardized annotation aim for scientific entities/terms. This work
proposes a standardized task by defining a set of seven contribution-centric
scholarly entities for CS NER viz., research problem, solution, resource,
language, tool, method, and dataset. Following which, its main contributions
are: combines existing CS NER resources that maintain their annotation focus on
the set or subset of contribution-centric scholarly entities we consider;
further, noting the need for big data to train neural NER models, this work
additionally supplies thousands of contribution-centric entity annotations from
article titles and abstracts, thus releasing a cumulative large novel resource
for CS NER; and, finally, trains a sequence labeling CS NER model inspired
after state-of-the-art neural architectures from the general domain NER task.
Throughout the work, several practical considerations are made which can be
useful to information technology designers of the digital libraries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition. (arXiv:2205.05832v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05832">
<div class="article-summary-box-inner">
<span><p>Recently, Flat-LAttice Transformer (FLAT) has achieved great success in
Chinese Named Entity Recognition (NER). FLAT performs lexical enhancement by
constructing flat lattices, which mitigates the difficulties posed by blurred
word boundaries and the lack of word semantics. In FLAT, the positions of
starting and ending characters are used to connect a matching word. However,
this method is likely to match more words when dealing with long texts,
resulting in long input sequences. Therefore, it significantly increases the
memory and computational costs of the self-attention module. To deal with this
issue, we advocate a novel lexical enhancement method, InterFormer, that
effectively reduces the amount of computational and memory costs by
constructing non-flat lattices. Furthermore, with InterFormer as the backbone,
we implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context
feature encoding. Compared with FLAT, it reduces unnecessary attention
calculations in "word-character" and "word-word". This reduces the memory usage
by about 50% and can use more extensive lexicons or higher batches for network
training. The experimental results obtained on several well-known benchmarks
demonstrate the superiority of the proposed method over the state-of-the-art
hybrid (character-word) models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data. (arXiv:2205.06350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06350">
<div class="article-summary-box-inner">
<span><p>Borrowing ideas from {\em Production functions} in micro-economics, in this
paper we introduce a framework to systematically evaluate the performance and
cost trade-offs between machine-translated and manually-created labelled data
for task-specific fine-tuning of massively multilingual language models. We
illustrate the effectiveness of our framework through a case-study on the
TyDIQA-GoldP dataset. One of the interesting conclusions of the study is that
if the cost of machine translation is greater than zero, the optimal
performance at least cost is always achieved with at least some or only
manually-created data. To our knowledge, this is the first attempt towards
extending the concept of production functions to study data collection
strategies for training multilingual models, and can serve as a valuable tool
for other similar cost vs data trade-offs in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Static Models and Test Sets: Benchmarking the Potential of Pre-trained Models Across Tasks and Languages. (arXiv:2205.06356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06356">
<div class="article-summary-box-inner">
<span><p>Although recent Massively Multilingual Language Models (MMLMs) like mBERT and
XLMR support around 100 languages, most existing multilingual NLP benchmarks
provide evaluation data in only a handful of these languages with little
linguistic diversity. We argue that this makes the existing practices in
multilingual evaluation unreliable and does not provide a full picture of the
performance of MMLMs across the linguistic landscape. We propose that the
recent work done in Performance Prediction for NLP tasks can serve as a
potential solution in fixing benchmarking in Multilingual NLP by utilizing
features related to data and language typology to estimate the performance of
an MMLM on different languages. We compare performance prediction with
translating test data with a case study on four different multilingual
datasets, and observe that these methods can provide reliable estimates of the
performance that are often on-par with the translation based approaches,
without the need for any additional translation as well as evaluation costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naturalistic Causal Probing for Morpho-Syntax. (arXiv:2205.07043v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07043">
<div class="article-summary-box-inner">
<span><p>Probing has become a go-to methodology for interpreting and analyzing deep
neural models in natural language processing. However, there is still a lack of
understanding of the limitations and weaknesses of various types of probes. In
this work, we suggest a strategy for input-level intervention on naturalistic
sentences. Using our approach, we intervene on the morpho-syntactic features of
a sentence, while keeping the rest of the sentence unchanged. Such an
intervention allows us to causally probe pre-trained models. We apply our
naturalistic causal probing framework to analyze the effects of grammatical
gender and number on contextualized representations extracted from three
pre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and
GPT-2. Our experiments suggest that naturalistic interventions lead to stable
estimates of the causal effects of various linguistic properties. Moreover, our
experiments demonstrate the importance of naturalistic causal probing when
analyzing pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08288">
<div class="article-summary-box-inner">
<span><p>Prior to deep learning the semantic parsing community has been interested in
understanding and modeling the range of possible word alignments between
natural language sentences and their corresponding meaning representations.
Sequence-to-sequence models changed the research landscape suggesting that we
no longer need to worry about alignments since they can be learned
automatically by means of an attention mechanism. More recently, researchers
have started to question such premise. In this work we investigate whether
seq2seq models can handle both simple and complex alignments. To answer this
question we augment the popular Geo semantic parsing dataset with alignment
annotations and create Geo-Aligned. We then study the performance of standard
seq2seq models on the examples that can be aligned monotonically versus
examples that require more complex alignments. Our empirical study shows that
performance is significantly better over monotonic alignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12219">
<div class="article-summary-box-inner">
<span><p>The ability to converse with humans and follow commands in natural language
is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can
relieve people's burden of holding a controller all the time, allow
multitasking, and make drone control more accessible for people with
disabilities or with their hands occupied. To this end, we introduce Aerial
Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language
conversation. We build a drone simulator with a continuous photorealistic
environment and collect a new AVDN dataset of over 3k recorded navigation
trajectories with asynchronous human-human dialogs between commanders and
followers. The commander provides initial navigation instruction and further
guidance by request, while the follower navigates the drone in the simulator
and asks questions when needed. During data collection, followers' attention on
the drone's visual observation is also recorded. Based on the AVDN dataset, we
study the tasks of aerial navigation from (full) dialog history and propose an
effective Human Attention Aided (HAA) baseline model, which learns to predict
both navigation waypoints and human attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting. (arXiv:2206.00761v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00761">
<div class="article-summary-box-inner">
<span><p>The availability of large pre-trained models is changing the landscape of
Machine Learning research and practice, moving from a training-from-scratch to
a fine-tuning paradigm. While in some applications the goal is to "nudge" the
pre-trained distribution towards preferred outputs, in others it is to steer it
towards a different distribution over the sample space. Two main paradigms have
emerged to tackle this challenge: Reward Maximization (RM) and, more recently,
Distribution Matching (DM). RM applies standard Reinforcement Learning (RL)
techniques, such as Policy Gradients, to gradually increase the reward signal.
DM prescribes to first make explicit the target distribution that the model is
fine-tuned to approximate. Here we explore the theoretical connections between
the two paradigms, and show that methods such as KL-control developed for RM
can also be construed as belonging to DM. We further observe that while DM
differs from RM, it can suffer from similar training difficulties, such as high
gradient variance. We leverage connections between the two paradigms to import
the concept of baseline into DM methods. We empirically validate the benefits
of adding a baseline on an array of controllable language generation tasks such
as constraining topic, sentiment, and gender distributions in texts sampled
from a language model. We observe superior performance in terms of constraint
satisfaction, stability and sample efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swiss German Speech to Text system evaluation. (arXiv:2207.00412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00412">
<div class="article-summary-box-inner">
<span><p>We present an in-depth evaluation of four commercially available
Speech-to-Text (STT) systems for Swiss German. The systems are anonymized and
referred to as system a-d in this report. We compare the four systems to our
STT model, referred to as FHNW from hereon after, and provide details on how we
trained our model. To evaluate the models, we use two STT datasets from
different domains. The Swiss Parliament Corpus (SPC) test set and a private
dataset in the news domain with an even distribution across seven dialect
regions. We provide a detailed error analysis to detect the three systems'
strengths and weaknesses. This analysis is limited by the characteristics of
the two test sets. Our model scored the highest bilingual evaluation understudy
(BLEU) on both datasets. On the SPC test set, we obtain a BLEU score of 0.607,
whereas the best commercial system reaches a BLEU score of 0.509. On our
private test set, we obtain a BLEU score of 0.722 and the best commercial
system a BLEU score of 0.568.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Length Generalization in Large Language Models. (arXiv:2207.04901v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04901">
<div class="article-summary-box-inner">
<span><p>The ability to extrapolate from short problem instances to longer ones is an
important form of out-of-distribution generalization in reasoning tasks, and is
crucial when learning from datasets where longer problem instances are rare.
These include theorem proving, solving quantitative mathematics problems, and
reading/summarizing novels. In this paper, we run careful empirical studies
exploring the length generalization capabilities of transformer-based language
models. We first establish that naively finetuning transformers on length
generalization tasks shows significant generalization deficiencies independent
of model scale. We then show that combining pretrained large language models'
in-context learning abilities with scratchpad prompting (asking the model to
output solution steps before producing an answer) results in a dramatic
improvement in length generalization. We run careful failure analyses on each
of the learning modalities and identify common sources of mistakes that
highlight opportunities in equipping language models with the ability to
generalize to longer problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Protein Knowledge Graph Construction and Applications. (arXiv:2207.10080v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10080">
<div class="article-summary-box-inner">
<span><p>Existing data-centric methods for protein science generally cannot
sufficiently capture and leverage biology knowledge, which may be crucial for
many protein tasks. To facilitate research in this field, we create
ProteinKG65, a knowledge graph for protein science. Using gene ontology and
Uniprot knowledge base as a basis, we transform and integrate various kinds of
knowledge with aligned descriptions and protein sequences, respectively, to GO
terms and protein entities. ProteinKG65 is mainly dedicated to providing a
specialized protein knowledge graph, bringing the knowledge of Gene Ontology to
protein function and structure prediction. We also illustrate the potential
applications of ProteinKG65 with a prototype. Our dataset can be downloaded at
https://w3id.org/proteinkg65.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems. (arXiv:2208.08191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08191">
<div class="article-summary-box-inner">
<span><p>Vision-Transformers are widely used in various vision tasks. Meanwhile, there
is another line of works starting with the MLP-mixer trying to achieve similar
performance using mlp-based architectures. Interestingly, until now those
mlp-based architectures have not been adapted for NLP tasks. Additionally,
until now, mlp-based architectures have failed to achieve state-of-the-art
performance in vision tasks. In this paper, we analyze the expressive power of
mlp-based architectures in modeling dependencies between multiple different
inputs simultaneously, and show an exponential gap between the attention and
the mlp-based mechanisms. Our results suggest a theoretical explanation for the
mlp inability to compete with attention-based mechanisms in NLP problems, they
also suggest that the performance gap in vision tasks may be due to the mlp
relative weakness in modeling dependencies between multiple different
locations, and that combining smart input permutations with mlp architectures
may not be enough to close the performance gap alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-stage Information Retrieval for Vietnamese Legal Texts. (arXiv:2209.14494v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14494">
<div class="article-summary-box-inner">
<span><p>This study deals with the problem of information retrieval (IR) for
Vietnamese legal texts. Despite being well researched in many languages,
information retrieval has still not received much attention from the Vietnamese
research community. This is especially true for the case of legal documents,
which are hard to process. This study proposes a new approach for information
retrieval for Vietnamese legal documents using sentence-transformer. Besides,
various experiments are conducted to make comparisons between different
transformer models, ranking scores, syllable-level, and word-level training.
The experiment results show that the proposed model outperforms models used in
current research on information retrieval for Vietnamese documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts. (arXiv:2210.03690v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03690">
<div class="article-summary-box-inner">
<span><p>Anaphora resolution is an important task for information extraction across a
range of languages, text genres, and domains, motivating the need for methods
that do not require large annotated datasets. In-context learning has emerged
as a promising approach, yet there are a number of challenges in applying
in-context learning to resolve anaphora. For example, encoding a single
in-context demonstration that consists of: an anaphor, a paragraph-length
context, and a list of corresponding antecedents, requires conditioning a
language model on a long sequence of tokens, limiting the number of
demonstrations per prompt. In this paper, we present MICE (Mixtures of
In-Context Experts), which we demonstrate is effective for few-shot anaphora
resolution in scientific protocols (Tamari et al., 2021). Given only a handful
of training examples, MICE combines the predictions of hundreds of in-context
experts, yielding a 30% increase in F1 score over a competitive prompt
retrieval baseline. Furthermore, we show MICE can be used to train compact
student models without sacrificing performance. As far as we are aware, this is
the first work to present experimental results demonstrating the effectiveness
of in-context learning on the task of few-shot anaphora resolution in
scientific protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogic: Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04185">
<div class="article-summary-box-inner">
<span><p>Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When enough data is available, our method can still serve as an effective
data augmentation method. Human evaluation results also show that our simulated
dialogues have near-human fluency and annotation accuracy. The code and data
are available at \textbf{\url{https://github.com/Leezekun/dialogic}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Personalization of CTC Speech Recognition Models with Contextual Adapters and Adaptive Boosting. (arXiv:2210.09510v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09510">
<div class="article-summary-box-inner">
<span><p>End-to-end speech recognition models trained using joint Connectionist
Temporal Classification (CTC)-Attention loss have gained popularity recently.
In these models, a non-autoregressive CTC decoder is often used at inference
time due to its speed and simplicity. However, such models are hard to
personalize because of their conditional independence assumption that prevents
output tokens from previous time steps to influence future predictions. To
tackle this, we propose a novel two-way approach that first biases the encoder
with attention over a predefined list of rare long-tail and out-of-vocabulary
(OOV) words and then uses dynamic boosting and phone alignment network during
decoding to further bias the subword predictions. We evaluate our approach on
open-source VoxPopuli and in-house medical datasets to showcase a 60%
improvement in F1 score on domain-specific rare words over a strong CTC
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11715">
<div class="article-summary-box-inner">
<span><p>Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification. (arXiv:2210.14523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14523">
<div class="article-summary-box-inner">
<span><p>Extreme multi-label text classification (XMTC) is the task of finding the
most relevant subset labels from an extremely large-scale label collection.
Recently, some deep learning models have achieved state-of-the-art results in
XMTC tasks. These models commonly predict scores for all labels by a fully
connected layer as the last layer of the model. However, such models can't
predict a relatively complete and variable-length label subset for each
document, because they select positive labels relevant to the document by a
fixed threshold or take top k labels in descending order of scores. A less
popular type of deep learning models called sequence-to-sequence (Seq2Seq)
focus on predicting variable-length positive labels in sequence style. However,
the labels in XMTC tasks are essentially an unordered set rather than an
ordered sequence, the default order of labels restrains Seq2Seq models in
training. To address this limitation in Seq2Seq, we propose an autoregressive
sequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates
predictions in student-forcing scheme and is trained by a loss function based
on bipartite matching which enables permutation-invariance. Meanwhile, we use
the optimal transport distance as a measurement to force the model to focus on
the closest labels in semantic label space. Experiments show that OTSeq2Set
outperforms other competitive baselines on 4 benchmark datasets. Especially, on
the Wikipedia dataset with 31k labels, it outperforms the state-of-the-art
Seq2Seq method by 16.34% in micro-F1 score. The code is available at
https://github.com/caojie54/OTSeq2Set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00768">
<div class="article-summary-box-inner">
<span><p>Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., "there is a mug in some grass" vs.
"there is some grass in a mug"). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers on Multilingual Clause-Level Morphology. (arXiv:2211.01736v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01736">
<div class="article-summary-box-inner">
<span><p>This paper describes our winning systems in MRL: The 1st Shared Task on
Multilingual Clause-level Morphology (EMNLP 2022 Workshop) designed by KUIS AI
NLP team. We present our work for all three parts of the shared task:
inflection, reinflection, and analysis. We mainly explore transformers with two
approaches: (i) training models from scratch in combination with data
augmentation, and (ii) transfer learning with prefix-tuning at multilingual
morphological tasks. Data augmentation significantly improves performance for
most languages in the inflection and reinflection tasks. On the other hand,
Prefix-tuning on a pre-trained mGPT model helps us to adapt analysis tasks in
low-data and multilingual settings. While transformer architectures with data
augmentation achieved the most promising results for inflection and
reinflection tasks, prefix-tuning on mGPT received the highest results for the
analysis task. Our systems received 1st place in all three tasks in MRL 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse scaling can become U-shaped. (arXiv:2211.02011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02011">
<div class="article-summary-box-inner">
<span><p>Although scaling language models improves performance on a range of tasks,
there are apparently some scenarios where scaling hurts performance. For
instance, the Inverse Scaling Prize Round 1 (McKensie et al., 2022) identified
four "inverse scaling" tasks, for which performance gets worse for larger
models. These tasks were evaluated on models of up to 280B parameters, trained
up to 500 zettaFLOPs of compute.
</p>
<p>This paper takes a closer look at these four tasks. We evaluate models of up
to 540B parameters, trained on five times more compute than those evaluated in
the Inverse Scaling Prize. With this increased range of model sizes and
training compute, two out of the four tasks exhibit what we call "U-shaped
scaling" -- performance decreases up to a certain model size, and then
increases again up to the largest model evaluated. One hypothesis is that
U-shaped scaling occurs when a task comprises a "true task" and a "distractor
task". Medium-size models can do the distractor task, which hurts performance,
while only large-enough models can ignore the distractor task and do the true
task. The existence of U-shaped scaling implies that inverse scaling may not
hold for larger models.
</p>
<p>Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT)
prompting, in addition to basic prompting without CoT. With CoT prompting, all
four tasks show either U-shaped scaling or positive scaling, achieving perfect
solve rates on two tasks and several sub-tasks. This suggests that the term
"inverse scaling task" is under-specified -- a given task may be inverse
scaling for one prompt but positive or U-shaped scaling for a different prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Progress on Scalable Oversight for Large Language Models. (arXiv:2211.03540v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03540">
<div class="article-summary-box-inner">
<span><p>Developing safe and useful general-purpose AI systems will require us to make
progress on scalable oversight: the problem of supervising systems that
potentially outperform us on most skills relevant to the task at hand.
Empirical work on this problem is not straightforward, since we do not yet have
systems that broadly exceed our abilities. This paper discusses one of the
major ways we think about this problem, with a focus on ways it can be studied
empirically. We first present an experimental design centered on tasks for
which human specialists succeed but unaided humans and current general AI
systems fail. We then present a proof-of-concept experiment meant to
demonstrate a key feature of this experimental design and show its viability
with two question-answering tasks: MMLU and time-limited QuALITY. On these
tasks, we find that human participants who interact with an unreliable
large-language-model dialog assistant through chat -- a trivial baseline
strategy for scalable oversight -- substantially outperform both the model
alone and their own unaided performance. These results are an encouraging sign
that scalable oversight will be tractable to study with present models and
bolster recent findings that large language models can productively assist
humans with difficult tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Zero-shot Event Extraction with Context-Definition Alignment. (arXiv:2211.05156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05156">
<div class="article-summary-box-inner">
<span><p>Event extraction (EE) is the task of identifying interested event mentions
from text. Conventional efforts mainly focus on the supervised setting.
However, these supervised models cannot generalize to event types out of the
pre-defined ontology. To fill this gap, many efforts have been devoted to the
zero-shot EE problem. This paper follows the trend of modeling event-type
semantics but moves one step further. We argue that using the static embedding
of the event type name might not be enough because a single word could be
ambiguous, and we need a sentence to define the type semantics accurately. To
model the definition semantics, we use two separate transformer models to
project the contextualized event mentions and corresponding definitions into
the same embedding space and then minimize their embedding distance via
contrastive learning. On top of that, we also propose a warming phase to help
the model learn the minor difference between similar definitions. We name our
approach Zero-shot Event extraction with Definition (ZED). Experiments on the
MAVEN dataset show that our model significantly outperforms all previous
zero-shot EE methods with fast inference speed due to the disjoint design.
Further experiments also show that ZED can be easily applied to the few-shot
setting when the annotation is available and consistently outperforms baseline
supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VieCap4H-VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05405">
<div class="article-summary-box-inner">
<span><p>Image captioning is currently a challenging task that requires the ability to
both understand visual information and use human language to describe this
visual information in the image. In this paper, we propose an efficient way to
improve the image understanding ability of transformer-based method by
extending Object Relation Transformer architecture with Attention on Attention
mechanism. Experiments on the VieCap4H dataset show that our proposed method
significantly outperforms its original structure on both the public test and
private test of the Image Captioning shared task held by VLSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-game Toxic Language Detection: Shared Task and Attention Residuals. (arXiv:2211.05995v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05995">
<div class="article-summary-box-inner">
<span><p>In-game toxic language becomes the hot potato in the gaming industry and
community. There have been several online game toxicity analysis frameworks and
models proposed. However, it is still challenging to detect toxicity due to the
nature of in-game chat, which has extremely short length. In this paper, we
describe how the in-game toxic language shared task has been established using
the real-world in-game chat data. In addition, we propose and introduce the
model/framework for toxic language token tagging (slot filling) from the
in-game chat. The data and code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction. (arXiv:2211.06014v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06014">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) aims to extract structured information from
heterogeneous sources. IE from natural language texts include sub-tasks such as
Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction
(EE). Most IE systems require comprehensive understandings of sentence
structure, implied semantics, and domain knowledge to perform well; thus, IE
tasks always need adequate external resources and annotations. However, it
takes time and effort to obtain more human annotations. Low-Resource
Information Extraction (LRIE) strives to use unsupervised data, reducing the
required resources and human annotation. In practice, existing systems either
utilize self-training schemes to generate pseudo labels that will cause the
gradual drift problem, or leverage consistency regularization methods which
inevitably possess confirmation bias. To alleviate confirmation bias due to the
lack of feedback loops in existing LRIE learning paradigms, we develop a
Gradient Imitation Reinforcement Learning (GIRL) method to encourage
pseudo-labeled data to imitate the gradient descent direction on labeled data,
which can force pseudo-labeled data to achieve better optimization capabilities
similar to labeled data. Based on how well the pseudo-labeled data imitates the
instructive gradient descent direction obtained from labeled data, we design a
reward to quantify the imitation process and bootstrap the optimization
capability of pseudo-labeled data through trial and error. In addition to
learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage
GIRL to solve all IE sub-tasks (named entity recognition, relation extraction,
and event extraction) in low-resource settings (semi-supervised IE and few-shot
IE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Lottery Tickets for Pre-trained Language Models. (arXiv:2211.03013v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03013">
<div class="article-summary-box-inner">
<span><p>Recent works on Lottery Ticket Hypothesis have shown that pre-trained
language models (PLMs) contain smaller matching subnetworks(winning tickets)
which are capable of reaching accuracy comparable to the original models.
However, these tickets are proved to be notrobust to adversarial examples, and
even worse than their PLM counterparts. To address this problem, we propose a
novel method based on learning binary weight masks to identify robust tickets
hidden in the original PLMs. Since the loss is not differentiable for the
binary mask, we assign the hard concrete distribution to the masks and
encourage their sparsity using a smoothing approximation of L0
regularization.Furthermore, we design an adversarial loss objective to guide
the search for robust tickets and ensure that the tickets perform well bothin
accuracy and robustness. Experimental results show the significant improvement
of the proposed method over previous work on adversarial robustness evaluation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-15 23:14:39.527945526 UTC">2022-11-15 23:14:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>