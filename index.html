<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-22T01:30:00Z">11-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10443">
<div class="article-summary-box-inner">
<span><p>Substance use, substance use disorder, and overdoses related to substance use
are major public health problems globally and in the United States. A key
aspect of addressing these problems from a public health standpoint is improved
surveillance. Traditional surveillance systems are laggy, and social media are
potentially useful sources of timely data. However, mining knowledge from
social media is challenging, and requires the development of advanced
artificial intelligence, specifically natural language processing (NLP) and
machine learning methods. We developed a sophisticated end-to-end pipeline for
mining information about nonmedical prescription medication use from social
media, namely Twitter and Reddit. Our pipeline employs supervised machine
learning and NLP for filtering out noise and characterizing the chatter. In
this paper, we describe our end-to-end pipeline developed over four years. In
addition to describing our data mining infrastructure, we discuss existing
challenges in social media mining for toxicovigilance, and possible future
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Refinement based on Triplet BERT-Networks. (arXiv:2211.10460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10460">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding techniques are widely used for knowledge graph
refinement tasks such as graph completion and triple classification. These
techniques aim at embedding the entities and relations of a Knowledge Graph
(KG) in a low dimensional continuous feature space. This paper adopts a
transformer-based triplet network creating an embedding space that clusters the
information about an entity or relation in the KG. It creates textual sequences
from facts and fine-tunes a triplet network of pre-trained transformer-based
language models. It adheres to an evaluation paradigm that relies on an
efficient spatial semantic search technique. We show that this evaluation
protocol is more adapted to a few-shot setting for the relation prediction
task. Our proposed GilBERT method is evaluated on triplet classification and
relation prediction tasks on multiple well-known benchmark knowledge graphs
such as FB13, WN11, and FB15K. We show that GilBERT achieves better or
comparable results to the state-of-the-art performance on these two refinement
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Generation From Text. (arXiv:2211.10511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10511">
<div class="article-summary-box-inner">
<span><p>In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)
generation system from textual inputs, separating the overall process into two
stages. The graph nodes are generated first using pretrained language model,
followed by a simple edge construction head, enabling efficient KG extraction
from the text. For each stage we consider several architectural choices that
can be used depending on the available training resources. We evaluated the
model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art
performance on text-to-RDF generation task, as well as on New York Times (NYT)
and a large-scale TekGen datasets, showing strong overall performance,
outperforming the existing baselines. We believe that the proposed system can
serve as a viable KG construction alternative to the existing linearization or
sampling-based graph generation approaches. Our code can be found at
https://github.com/IBM/Grapher
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems. (arXiv:2211.10596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10596">
<div class="article-summary-box-inner">
<span><p>Automation of dialogue system evaluation is a driving force for the efficient
development of dialogue systems. This paper introduces the bipartite-play
method, a dialogue collection method for automating dialogue system evaluation.
It addresses the limitations of existing dialogue collection methods: (i)
inability to compare with systems that are not publicly available, and (ii)
vulnerability to cheating by intentionally selecting systems to be compared.
Experimental results show that the automatic evaluation using the
bipartite-play method mitigates these two drawbacks and correlates as strongly
with human subjectivity as existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Assisted Language Models for Identifying Check-worthy Sentences. (arXiv:2211.10678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10678">
<div class="article-summary-box-inner">
<span><p>We propose a new uniform framework for text classification and ranking that
can automate the process of identifying check-worthy sentences in political
debates and speech transcripts. Our framework combines the semantic analysis of
the sentences, with additional entity embeddings obtained through the
identified entities within the sentences. In particular, we analyse the
semantic meaning of each sentence using state-of-the-art neural language models
such as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained
from knowledge graph (KG) embedding models. Specifically, we instantiate our
framework using five different language models, entity embeddings obtained from
six different KG embedding models, as well as two combination methods leading
to several Entity-Assisted neural language models. We extensively evaluate the
effectiveness of our framework using two publicly available datasets from the
CLEF' 2019 &amp; 2020 CheckThat! Labs. Our results show that the neural language
models significantly outperform traditional TF.IDF and LSTM methods. In
addition, we show that the ALBERT model is consistently the most effective
model among all the tested neural language models. Our entity embeddings
significantly outperform other existing approaches from the literature that are
based on similarity and relatedness scores between the entities in a sentence,
when used alongside a KG embedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Instance Relation Augmentation for Long-tailed Multi-label Text Classification. (arXiv:2211.10685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10685">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification (MLTC) is one of the key tasks in natural
language processing. It aims to assign multiple target labels to one document.
Due to the uneven popularity of labels, the number of documents per label
follows a long-tailed distribution in most cases. It is much more challenging
to learn classifiers for data-scarce tail labels than for data-rich head
labels. The main reason is that head labels usually have sufficient
information, e.g., a large intra-class diversity, while tail labels do not. In
response, we propose a Pairwise Instance Relation Augmentation Network (PIRAN)
to augment tailed-label documents for balancing tail labels and head labels.
PIRAN consists of a relation collector and an instance generator. The former
aims to extract the document pairwise relations from head labels. Taking these
relations as perturbations, the latter tries to generate new document instances
in high-level feature space around the limited given tailed-label instances.
Meanwhile, two regularizers (diversity and consistency) are designed to
constrain the generation process. The consistency-regularizer encourages the
variance of tail labels to be close to head labels and further balances the
whole datasets. And diversity-regularizer makes sure the generated instances
have diversity and avoids generating redundant instances. Extensive
experimental results on three benchmark datasets demonstrate that PIRAN
consistently outperforms the SOTA methods, and dramatically improves the
performance of tail labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReInform: Selecting paths with reinforcement learning for contextualized link prediction. (arXiv:2211.10688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10688">
<div class="article-summary-box-inner">
<span><p>We propose to use reinforcement learning to inform transformer-based
contextualized link prediction models by providing paths that are most useful
for predicting the correct answer. This is in contrast to previous approaches,
that either used reinforcement learning (RL) to directly search for the answer,
or based their prediction on limited or randomly selected context. Our
experiments on WN18RR and FB15k-237 show that contextualized link prediction
models consistently outperform RL-based answer search, and that additional
improvements (of up to 13.5\% MRR) can be gained by combining RL with a link
prediction model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suffering from Vaccines or from Government? : Partisan Bias in COVID-19 Vaccine Adverse Events Coverage. (arXiv:2211.10707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10707">
<div class="article-summary-box-inner">
<span><p>Vaccine adverse events have been presumed to be a relatively objective
measure that is immune to political polarization. The real-world data, however,
shows the correlation between presidential disapproval ratings and the
subjective severity of adverse events. This paper investigates the partisan
bias in COVID vaccine adverse events coverage with language models that can
classify the topic of vaccine-related articles and the political disposition of
news comments. Based on 90K news articles from 52 major newspaper companies, we
found that conservative media are inclined to report adverse events more
frequently than their liberal counterparts, while the coverage itself was
statistically uncorrelated with the severity of real-world adverse events. The
users who support the conservative opposing party were more likely to write the
popular comments from 2.3K random sampled articles on news platforms. This
research implies that bipartisanship can still play a significant role in
forming public opinion on the COVID vaccine even after the majority of the
population's vaccination
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical Language Change Is Self-Organized Criticality. (arXiv:2211.10709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10709">
<div class="article-summary-box-inner">
<span><p>One way to resolve the actuation problem of metaphorical language change is
to provide a statistical profile of metaphorical constructions and generative
rules with antecedent conditions. Based on arguments from the view of language
as complex systems and the dynamic view of metaphor, this paper argues that
metaphorical language change qualifies as a self-organized criticality state
and the linguistic expressions of a metaphor can be profiled as a fractal with
spatio-temporal correlations. Synchronously, these metaphorical expressions
self-organize into a self-similar, scale-invariant fractal that follows a
power-law distribution; temporally, long range inter-dependence constrains the
self-organization process by the way of transformation rules that are intrinsic
of a language system. This argument is verified in the paper with statistical
analyses of twelve randomly selected Chinese verb metaphors in a large-scale
diachronic corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture. (arXiv:2211.10780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10780">
<div class="article-summary-box-inner">
<span><p>This paper introduces ArtELingo, a new benchmark and dataset, designed to
encourage work on diversity across languages and cultures. Following ArtEmis, a
collection of 80k artworks from WikiArt with 0.45M emotion labels and
English-only captions, ArtELingo adds another 0.79M annotations in Arabic and
Chinese, plus 4.8K in Spanish to evaluate "cultural-transfer" performance. More
than 51K artworks have 5 annotations or more in 3 languages. This diversity
makes it possible to study similarities and differences across languages and
cultures. Further, we investigate captioning tasks, and find diversity improves
the performance of baseline models. ArtELingo is publicly available at
https://www.artelingo.org/ with standard splits and baseline models. We hope
our work will help ease future research on multilinguality and culturally-aware
AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation. (arXiv:2211.10797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10797">
<div class="article-summary-box-inner">
<span><p>In the study, we empirically compare the two recently proposed decoding
methods, i.e. Contrastive Search (CS) and Contrastive Decoding (CD), for
open-ended text generation. The automatic evaluation results suggest that,
while CS performs worse than CD on the MAUVE metric, it substantially surpasses
CD on the diversity and coherence metrics. More notably, extensive human
evaluations across three different domains demonstrate that human annotators
are universally more in favor of CS over CD with substantial margins.
</p>
<p>The contradicted results between MAUVE and human evaluations reveal that
MAUVE does not accurately reflect human preferences. Therefore, we call upon
the research community to develop better evaluation metrics for open-ended text
generation. To ensure the reproducibility of our work, we have open-sourced all
our code, evaluation results, as well as human annotations at
https://github.com/yxuansu/Contrastive_Search_versus_Contrastive_Decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining State-of-the-Art Models with Maximal Marginal Relevance for Few-Shot and Zero-Shot Multi-Document Summarization. (arXiv:2211.10808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10808">
<div class="article-summary-box-inner">
<span><p>In Natural Language Processing, multi-document summarization (MDS) poses many
challenges to researchers above those posed by single-document summarization
(SDS). These challenges include the increased search space and greater
potential for the inclusion of redundant information. While advancements in
deep learning approaches have led to the development of several advanced
language models capable of summarization, the variety of training data specific
to the problem of MDS remains relatively limited. Therefore, MDS approaches
which require little to no pretraining, known as few-shot or zero-shot
applications, respectively, could be beneficial additions to the current set of
tools available in summarization. To explore one possible approach, we devise a
strategy for combining state-of-the-art models' outputs using maximal marginal
relevance (MMR) with a focus on query relevance rather than document diversity.
Our MMR-based approach shows improvement over some aspects of the current
state-of-the-art results in both few-shot and zero-shot MDS applications while
maintaining a state-of-the-art standard of output by all available metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeDR: Segment Representation Learning for Long Documents Dense Retrieval. (arXiv:2211.10841v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10841">
<div class="article-summary-box-inner">
<span><p>Recently, Dense Retrieval (DR) has become a promising solution to document
retrieval, where document representations are used to perform effective and
efficient semantic search. However, DR remains challenging on long documents,
due to the quadratic complexity of its Transformer-based encoder and the finite
capacity of a low-dimension embedding. Current DR models use suboptimal
strategies such as truncating or splitting-and-pooling to long documents
leading to poor utilization of whole document information. In this work, to
tackle this problem, we propose Segment representation learning for long
documents Dense Retrieval (SeDR). In SeDR, Segment-Interaction Transformer is
proposed to encode long documents into document-aware and segment-sensitive
representations, while it holds the complexity of splitting-and-pooling and
outperforms other segment-interaction patterns on DR. Since GPU memory
requirements for long document encoding causes insufficient negatives for DR
training, Late-Cache Negative is further proposed to provide additional cache
negatives for optimizing representation learning. Experiments on MS MARCO and
TREC-DL datasets show that SeDR achieves superior performance among DR models,
and confirm the effectiveness of SeDR on long document retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mulco: Recognizing Chinese Nested Named Entities Through Multiple Scopes. (arXiv:2211.10854v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10854">
<div class="article-summary-box-inner">
<span><p>Nested Named Entity Recognition (NNER) has been a long-term challenge to
researchers as an important sub-area of Named Entity Recognition. NNER is where
one entity may be part of a longer entity, and this may happen on multiple
levels, as the term nested suggests. These nested structures make traditional
sequence labeling methods unable to properly recognize all entities. While
recent researches focus on designing better recognition methods for NNER in a
variety of languages, the Chinese NNER (CNNER) still lacks attention, where a
free-for-access, CNNER-specialized benchmark is absent. In this paper, we aim
to solve CNNER problems by providing a Chinese dataset and a learning-based
model to tackle the issue. To facilitate the research on this task, we release
ChiNesE, a CNNER dataset with 20,000 sentences sampled from online passages of
multiple domains, containing 117,284 entities failing in 10 categories, where
43.8 percent of those entities are nested. Based on ChiNesE, we propose Mulco,
a novel method that can recognize named entities in nested structures through
multiple scopes. Each scope use a designed scope-based sequence labeling
method, which predicts an anchor and the length of a named entity to recognize
it. Experiment results show that Mulco has outperformed several baseline
methods with the different recognizing schemes on ChiNesE. We also conduct
extensive experiments on ACE2005 Chinese corpus, where Mulco has achieved the
best performance compared with the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Interrogation for Attributing Language Models. (arXiv:2211.10877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10877">
<div class="article-summary-box-inner">
<span><p>This paper presents solutions to the Machine Learning Model Attribution
challenge (MLMAC) collectively organized by MITRE, Microsoft, Schmidt-Futures,
Robust-Intelligence, Lincoln-Network, and Huggingface community. The challenge
provides twelve open-sourced base versions of popular language models developed
by well-known organizations and twelve fine-tuned language models for text
generation. The names and architecture details of fine-tuned models were kept
hidden, and participants can access these models only through the rest APIs
developed by the organizers. Given these constraints, the goal of the contest
is to identify which fine-tuned models originated from which base model. To
solve this challenge, we have assumed that fine-tuned models and their
corresponding base versions must share a similar vocabulary set with a matching
syntactical writing style that resonates in their generated outputs. Our
strategy is to develop a set of queries to interrogate base and fine-tuned
models. And then perform one-to-many pairing between them based on similarities
in their generated responses, where more than one fine-tuned model can pair
with a base model but not vice-versa. We have employed four distinct approaches
for measuring the resemblance between the responses generated from the models
of both sets. The first approach uses evaluation metrics of the machine
translation, and the second uses a vector space model. The third approach uses
state-of-the-art multi-class text classification, Transformer models. Lastly,
the fourth approach uses a set of Transformer based binary text classifiers,
one for each provided base model, to perform multi-class text classification in
a one-vs-all fashion. This paper reports implementation details, comparison,
and experimental studies, of these approaches along with the final obtained
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error correction and extraction in request dialogs. (arXiv:2004.04243v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.04243">
<div class="article-summary-box-inner">
<span><p>We propose a dialog system utility component that gets the two last
utterances of a user and can detect whether the last utterance is an error
correction of the second last utterance. If yes, it corrects the second last
utterance according to the error correction in the last utterance. In addition,
the proposed component outputs the extracted pairs of reparandum and repair
entity. This component offers two advantages, learning the concept of
corrections to avoid collecting corrections for every new domain and extracting
reparandum and repair pairs, which offers the possibility to learn out of it.
</p>
<p>For the error correction one sequence labeling and two sequence to sequence
approaches are presented. For the error correction detection these three error
correction approaches can also be used and in addition, we present a sequence
classification approach. One error correction detection and one error
correction approach can be combined to a pipeline or the error correction
approaches can be trained and used end-to-end to avoid two components. We
modified the EPIC-KITCHENS-100 dataset to evaluate the approaches for
correcting entity phrases in request dialogs. For error correction detection
and correction, we got an accuracy of 97.54 % on synthetic validation data and
an accuracy of 69.27 % on human-created real-world test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Attention with Performers. (arXiv:2009.14794v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14794">
<div class="article-summary-box-inner">
<span><p>We introduce Performers, Transformer architectures which can estimate regular
(softmax) full-rank-attention Transformers with provable accuracy, but using
only linear (as opposed to quadratic) space and time complexity, without
relying on any priors such as sparsity or low-rankness. To approximate softmax
attention-kernels, Performers use a novel Fast Attention Via positive
Orthogonal Random features approach (FAVOR+), which may be of independent
interest for scalable kernel methods. FAVOR+ can be also used to efficiently
model kernelizable attention mechanisms beyond softmax. This representational
power is crucial to accurately compare softmax with other kernels for the first
time on large-scale tasks, beyond the reach of regular Transformers, and
investigate optimal attention-kernels. Performers are linear architectures
fully compatible with regular Transformers and with strong theoretical
guarantees: unbiased or nearly-unbiased estimation of the attention matrix,
uniform convergence and low estimation variance. We tested Performers on a rich
set of tasks stretching from pixel-prediction through text models to protein
sequence modeling. We demonstrate competitive results with other examined
efficient sparse and dense attention methods, showcasing effectiveness of the
novel attention-learning paradigm leveraged by Performers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v9 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Entity Abstraction Help Generative Transformers Reason?. (arXiv:2201.01787v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01787">
<div class="article-summary-box-inner">
<span><p>We study the utility of incorporating entity type abstractions into
pre-trained Transformers and test these methods on four NLP tasks requiring
different forms of logical reasoning: (1) compositional language understanding
with text-based relational reasoning (CLUTRR), (2) abductive reasoning
(ProofWriter), (3) multi-hop question answering (HotpotQA), and (4)
conversational question answering (CoQA). We propose and empirically explore
three ways to add such abstraction: (i) as additional input embeddings, (ii) as
a separate sequence to encode, and (iii) as an auxiliary prediction task for
the model. Overall, our analysis demonstrates that models with abstract entity
knowledge performs better than without it. The best abstraction aware models
achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model
achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for
HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our
results suggest that the benefit of explicit abstraction is significant in
formally defined logical reasoning settings requiring many reasoning hops, but
point to the notion that it is less beneficial for NLP tasks having less formal
logical structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GUDN: A novel guide network with label reinforcement strategy for extreme multi-label text classification. (arXiv:2201.11582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11582">
<div class="article-summary-box-inner">
<span><p>In natural language processing, extreme multi-label text classification is an
emerging but essential task. The problem of extreme multi-label text
classification (XMTC) is to recall some of the most relevant labels for a text
from an extremely large label set. Large-scale pre-trained models have brought
a new trend to this problem. Though the large-scale pre-trained models have
made significant achievements on this problem, the valuable fine-tuned methods
have yet to be studied. Though label semantics have been introduced in XMTC,
the vast semantic gap between texts and labels has yet to gain enough
attention. This paper builds a new guide network (GUDN) to help fine-tune the
pre-trained model to instruct classification later. Furthermore, GUDN uses raw
label semantics combined with a helpful label reinforcement strategy to
effectively explore the latent space between texts and labels, narrowing the
semantic gap, which can further improve predicted accuracy. Experimental
results demonstrate that GUDN outperforms state-of-the-art methods on Eurlex-4k
and has competitive results on other popular datasets. In an additional
experiment, we investigated the input lengths' influence on the
Transformer-based model's accuracy. Our source code is released at
https://t.hk.uy/aFSH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UDAAN: Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01644">
<div class="article-summary-box-inner">
<span><p>We introduce UDAAN, an open-source post-editing tool that can reduce manual
editing efforts to quickly produce publishable-standard documents in several
Indic languages. UDAAN has an end-to-end Machine Translation (MT) plus
post-editing pipeline wherein users can upload a document to obtain raw MT
output. Further, users can edit the raw translations using our tool. UDAAN
offers several advantages: a) Domain-aware, vocabulary-based lexical
constrained MT. b) source-target and target-target lexicon suggestions for
users. Replacements are based on the source and target texts lexicon alignment.
c) Translation suggestions are based on logs created during user interaction.
d) Source-target sentence alignment visualisation that reduces the cognitive
load of users during editing. e) Translated outputs from our tool are available
in multiple formats: docs, latex, and PDF. We also provide the facility to use
around 100 in-domain dictionaries for lexicon-aware machine translation.
Although we limit our experiments to English-to-Hindi translation, our tool is
independent of the source and target languages. Experimental results based on
the usage of the tools and users feedback show that our tool speeds up the
translation time by approximately a factor of three compared to the baseline
method of translating documents from scratch. Our tool is available for both
Windows and Linux platforms. The tool is open-source under MIT license, and the
source code can be accessed from our website at https://www.udaanproject.org.
Demonstration and tutorial videos for various features of our tool can be
accessed at https://www.youtube.com/channel/UClfK7iC8J7b22bj3GwAUaCw. Our MT
pipeline can be accessed at https://udaaniitb.aicte-india.org/udaan/translate/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05435">
<div class="article-summary-box-inner">
<span><p>Performance of text classification models tends to drop over time due to
changes in data, which limits the lifetime of a pretrained model. Therefore an
ability to predict a model's ability to persist over time can help design
models that can be effectively used over a longer period of time. In this
paper, we provide a thorough discussion into the problem, establish an
evaluation setup for the task. We look at this problem from a practical
perspective by assessing the ability of a wide range of language models and
classification algorithms to persist over time, as well as how dataset
characteristics can help predict the temporal stability of different models. We
perform longitudinal classification experiments on three datasets spanning
between 6 and 19 years, and involving diverse tasks and types of data. By
splitting the longitudinal datasets into years, we perform a comprehensive set
of experiments by training and testing across data that are different numbers
of years apart from each other, both in the past and in the future. This
enables a gradual investigation into the impact of the temporal gap between
training and test sets on the classification performance, as well as measuring
the extent of the persistence over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05862">
<div class="article-summary-box-inner">
<span><p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in
achieving representation learning and generation for natural language at the
same time. Nevertheless, existing VAE-based language models either employ
elementary RNNs, which is not powerful to handle complex works in the
multi-task situation, or fine-tunes two pre-trained language models (PLMs) for
any downstream task, which is a huge drain on resources. In this paper, we
propose the first VAE framework empowered with adaptive GPT-2s (AdaVAE).
Different from existing systems, we unify both the encoder\&amp;decoder of the VAE
model using GPT-2s with adaptive parameter-efficient components, and further
introduce Latent Attention operation to better construct latent space from
transformer models. Experiments from multiple dimensions validate that AdaVAE
is competent to effectively organize language in three related tasks (language
modeling, representation modeling and guided text generation) even with less
than $15\%$ activated parameters in training. Our code is available at
\url{https://github.com/ImKeTT/AdaVAE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LeRaC: Learning Rate Curriculum. (arXiv:2205.09180v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09180">
<div class="article-summary-box-inner">
<span><p>Most curriculum learning methods require an approach to sort the data samples
by difficulty, which is often cumbersome to perform. In this work, we propose a
novel curriculum learning approach termed Learning Rate Curriculum (LeRaC),
which leverages the use of a different learning rate for each layer of a neural
network to create a data-free curriculum during the initial training epochs.
More specifically, LeRaC assigns higher learning rates to neural layers closer
to the input, gradually decreasing the learning rates as the layers are placed
farther away from the input. The learning rates increase at various paces
during the first training iterations, until they all reach the same value. From
this point on, the neural model is trained as usual. This creates a model-level
curriculum learning strategy that does not require sorting the examples by
difficulty and is compatible with any neural network, generating higher
performance levels regardless of the architecture. We conduct comprehensive
experiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100,
Tiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D)
domains, considering various convolutional (ResNet-18, Wide-ResNet-50,
DenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr)
architectures, comparing our approach with the conventional training regime.
Moreover, we also compare with Curriculum by Smoothing (CBS), a
state-of-the-art data-free curriculum learning approach. Unlike CBS, our
performance improvements over the standard training regime are consistent
across all datasets and models. Furthermore, we significantly surpass CBS in
terms of training time (there is no additional cost over the standard training
regime for LeRaC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Linking with Entity Representation by Multiple Embeddings. (arXiv:2205.10498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10498">
<div class="article-summary-box-inner">
<span><p>We propose a simple and practical method for named entity linking (NEL),
based on entity representation by multiple embeddings. To explore this method,
and to review its dependency on parameters, we measure its performance on
Namesakes, a highly challenging dataset of ambiguously named entities. Our
observations suggest that the minimal number of mentions required to create a
knowledge base (KB) entity is very important for NEL performance. The number of
embeddings is less important and can be kept small, within as few as 10 or
less. We show that our representations of KB entities can be adjusted using
only KB data, and the adjustment can improve NEL performance. We also compare
NEL performance of embeddings obtained from tuning language model on diverse
news texts as opposed to tuning on more uniform texts from public datasets
XSum, CNN / Daily Mail. We found that tuning on diverse news provides better
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain Sign Language Translation Learned from Online Video. (arXiv:2205.12870v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12870">
<div class="article-summary-box-inner">
<span><p>Existing work on sign language translation - that is, translation from sign
language videos into sentences in a written language - has focused mainly on
(1) data collected in a controlled environment or (2) data in a specific
domain, which limits the applicability to real-world settings. In this paper,
we introduce OpenASL, a large-scale American Sign Language (ASL) - English
dataset collected from online video sites (e.g., YouTube). OpenASL contains 288
hours of ASL videos in multiple domains from over 200 signers and is the
largest publicly available ASL translation dataset to date. To tackle the
challenges of sign language translation in realistic settings and without
glosses, we propose a set of techniques including sign search as a pretext task
for pre-training and fusion of mouthing and handshape features. The proposed
techniques produce consistent and large improvements in translation quality,
over baseline models based on prior work. Our data and code are publicly
available at https://github.com/chevalierNoir/OpenASL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16001">
<div class="article-summary-box-inner">
<span><p>A good automatic evaluation metric for language generation ideally correlates
highly with human judgements of text quality. Yet, there is a dearth of such
metrics, which inhibits the rapid and efficient progress of language
generators. One exception is the recently proposed Mauve. In theory, Mauve
measures an information-theoretic divergence between two probability
distributions over strings: one representing the language generator under
evaluation; the other representing the true natural language distribution.
Mauve's authors argue that its success comes from the qualitative properties of
their proposed divergence. Yet in practice, as this divergence is uncomputable,
Mauve approximates it by measuring the divergence between multinomial
distributions over clusters instead, where cluster assignments are attained by
grouping strings based on a pre-trained language model's embeddings. As we
show, however, this is not a tight approximation -- in either theory or
practice. This begs the question: why does Mauve work so well? In this work, we
show that Mauve was right for the wrong reasons, and that its newly proposed
divergence is not necessary for its high performance. In fact, classical
divergences paired with its proposed cluster-based approximation may actually
serve as better evaluation metrics. We finish the paper with a probing
analysis; this analysis leads us to conclude that -- by encoding syntactic- and
coherence-level features of text, while ignoring surface-level features -- such
cluster-based substitutes to string distributions may simply be better for
evaluating state-of-the-art language generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models (Mostly) Know What They Know. (arXiv:2207.05221v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05221">
<div class="article-summary-box-inner">
<span><p>We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability "P(True)" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict "P(IK)", the probability
that "I know" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval and Span Extraction. (arXiv:2207.05817v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05817">
<div class="article-summary-box-inner">
<span><p>Medical entity span extraction and linking are critical steps for many
healthcare NLP tasks. Most existing entity extraction methods either have a
fixed vocabulary of medical entities or require span annotations. In this
paper, we propose a method for linking an open set of entities that does not
require any span annotations. Our method, Open Set Label Attention Transformer
(OSLAT), uses the label-attention mechanism to learn candidate-entity
contextualized text representations. We find that OSLAT can not only link
entities but is also able to implicitly learn spans associated with entities.
We evaluate OSLAT on two tasks: (1) span extraction trained without explicit
span annotations, and (2) entity linking trained without span-level annotation.
We test the generalizability of our method by training two separate models on
two datasets with low entity overlap and comparing cross-dataset performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing for Systems Engineering: Automatic Generation of Systems Modelling Language Diagrams. (arXiv:2208.05008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.05008">
<div class="article-summary-box-inner">
<span><p>The design of complex engineering systems is an often long and articulated
process that highly relies on engineers' expertise and professional judgment.
As such, the typical pitfalls of activities involving the human factor often
manifest themselves in terms of lack of completeness or exhaustiveness of the
analysis, inconsistencies across design choices or documentation, as well as an
implicit degree of subjectivity. An approach is proposed to assist systems
engineers in the automatic generation of systems diagrams from unstructured
natural language text. Natural Language Processing (NLP) techniques are used to
extract entities and their relationships from textual resources (e.g.,
specifications, manuals, technical reports, maintenance reports) available
within an organisation, and convert them into Systems Modelling Language
(SysML) diagrams, with particular focus on structure and requirement diagrams.
The intention is to provide the users with a more standardised, comprehensive
and automated starting point onto which subsequently refine and adapt the
diagrams according to their needs. The proposed approach is flexible and
open-domain. It consists of six steps which leverage open-access tools, and it
leads to an automatic generation of SysML diagrams without intermediate
modelling requirement, but through the specification of a set of parameters by
the user. The applicability and benefits of the proposed approach are shown
through six case studies having different textual sources as inputs, and
benchmarked against manually defined diagram elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Embeddings for Text. (arXiv:2208.08386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08386">
<div class="article-summary-box-inner">
<span><p>We propose a new kind of embedding for natural language text that deeply
represents semantic meaning. Standard text embeddings use the outputs from
hidden layers of a pretrained language model. In our method, we let a language
model learn from the text and then literally pick its brain, taking the actual
weights of the model's neurons to generate a vector. We call this
representation of the text a neural embedding. We confirm the ability of this
representation to reflect semantics of the text by an analysis of its behavior
on several datasets, and by a comparison of neural embedding with state of the
art sentence embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Grounding of Inter-lingual Word-Embeddings. (arXiv:2209.03714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03714">
<div class="article-summary-box-inner">
<span><p>Visual grounding of Language aims at enriching textual representations of
language with multiple sources of visual knowledge such as images and videos.
Although visual grounding is an area of intense research, inter-lingual aspects
of visual grounding have not received much attention. The present study
investigates the inter-lingual visual grounding of word embeddings. We propose
an implicit alignment technique between the two spaces of vision and language
in which inter-lingual textual information interacts in order to enrich
pre-trained textual word embeddings. We focus on three languages in our
experiments, namely, English, Arabic, and German. We obtained visually grounded
vector representations for these languages and studied whether visual grounding
on one or multiple languages improved the performance of embeddings on word
similarity and categorization benchmarks. Our experiments suggest that
inter-lingual knowledge improves the performance of grounded embeddings in
similar languages such as German and English. However, inter-lingual grounding
of German or English with Arabic led to a slight degradation in performance on
word similarity benchmarks. On the other hand, we observed an opposite trend on
categorization benchmarks where Arabic had the most improvement on English. In
the discussion section, several reasons for those findings are laid out. We
hope that our experiments provide a baseline for further research on
inter-lingual visual grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Language Model Prompting in Support of Semi-autonomous Task Learning. (arXiv:2209.07636v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07636">
<div class="article-summary-box-inner">
<span><p>Language models (LLMs) offer potential as a source of knowledge for agents
that need to acquire new task competencies within a performance environment. We
describe efforts toward a novel agent capability that can construct cues (or
"prompts") that result in useful LLM responses for an agent learning a new
task. Importantly, responses must not only be "reasonable" (a measure used
commonly in research on knowledge extraction from LLMs) but also specific to
the agent's task context and in a form that the agent can interpret given its
native language capacities. We summarize a series of empirical investigations
of prompting strategies and evaluate responses against the goals of targeted
and actionable responses for task learning. Our results demonstrate that
actionable task knowledge can be obtained from LLMs in support of online agent
task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12226">
<div class="article-summary-box-inner">
<span><p>Recent research has revealed undesirable biases in NLP data and models.
However, these efforts focus on social disparities in West, and are not
directly portable to other geo-cultural contexts. In this paper, we focus on
NLP fair-ness in the context of India. We start with a brief account of the
prominent axes of social disparities in India. We build resources for fairness
evaluation in the Indian context and use them to demonstrate prediction biases
along some of the axes. We then delve deeper into social stereotypes for Region
andReligion, demonstrating its prevalence in corpora and models. Finally, we
outline a holistic research agenda to re-contextualize NLP fairness research
for the Indian context, ac-counting for Indian societal context, bridging
technological gaps in NLP capabilities and re-sources, and adapting to Indian
cultural values. While we focus on India, this framework can be generalized to
other geo-cultural contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Surprising Computational Power of Nondeterministic Stack RNNs. (arXiv:2210.01343v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01343">
<div class="article-summary-box-inner">
<span><p>Traditional recurrent neural networks (RNNs) have a fixed, finite number of
memory cells. In theory (assuming bounded range and precision), this limits
their formal language recognition power to regular languages, and in practice,
RNNs have been shown to be unable to learn many context-free languages (CFLs).
In order to expand the class of languages RNNs recognize, prior work has
augmented RNNs with a nondeterministic stack data structure, putting them on
par with pushdown automata and increasing their language recognition power to
CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic
CFLs), but in this paper, we show that nondeterminism and the neural controller
interact to produce two more unexpected abilities. First, the nondeterministic
stack RNN can recognize not only CFLs, but also many non-context-free
languages. Second, it can recognize languages with much larger alphabet sizes
than one might expect given the size of its stack alphabet. Finally, to
increase the information capacity in the stack and allow it to solve more
complicated tasks with large alphabet sizes, we propose a new version of the
nondeterministic stack that simulates stacks of vectors rather than discrete
symbols. We demonstrate perplexity improvements with this new model on the Penn
Treebank language modeling benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts. (arXiv:2210.02063v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02063">
<div class="article-summary-box-inner">
<span><p>The sentiment analysis task has various applications in practice. In the
sentiment analysis task, words and phrases that represent positive and negative
emotions are important. Finding out the words that represent the emotion from
the text can improve the performance of the classification models for the
sentiment analysis task. In this paper, we propose a methodology that combines
the emotion lexicon with the classification model for enhancing the accuracy of
the models. Our experimental results show that the emotion lexicon combined
with the classification model improves the performance of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask Me Anything: A simple strategy for prompting language models. (arXiv:2210.02441v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02441">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt that demonstrates how to perform the task and
no additional training. Prompting is a brittle process wherein small
modifications to the prompt can cause large variations in the model
predictions, and therefore significant effort is dedicated towards designing a
painstakingly "perfect prompt" for a task. To mitigate the high degree of
effort involved in prompt-design, we instead ask whether producing multiple
effective, yet imperfect, prompts and aggregating them can lead to a high
quality prompting strategy. Our observations motivate our proposed prompting
method, ASK ME ANYTHING (AMA). We first develop an understanding of the
effective prompt formats, finding that question-answering (QA) prompts, which
encourage open-ended generation ("Who went to the park?") tend to outperform
those that restrict the model outputs ("John went to the park. Output True or
False."). Our approach recursively uses the LLM itself to transform task inputs
to the effective QA format. We apply the collected prompts to obtain several
noisy votes for the input's true label. We find that the prompts can have very
different accuracies and complex dependencies and thus propose to use weak
supervision, a procedure for combining the noisy predictions, to produce the
final predictions for the inputs. We evaluate AMA across open-source model
families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B
parameters), demonstrating an average performance lift of 10.2% over the
few-shot baseline. This simple strategy enables the open-source GPT-J-6B model
to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular
benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms
few-shot GPT3-175B. We release our code here:
https://github.com/HazyResearch/ama_prompting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07321">
<div class="article-summary-box-inner">
<span><p>Advances in natural language generation (NLG) have resulted in machine
generated text that is increasingly difficult to distinguish from human
authored text. Powerful open-source models are freely available, and
user-friendly tools democratizing access to generative models are
proliferating. The great potential of state-of-the-art NLG systems is tempered
by the multitude of avenues for abuse. Detection of machine generated text is a
key countermeasure for reducing abuse of NLG models, with significant technical
challenges and numerous open problems. We provide a survey that includes both
1) an extensive analysis of threat models posed by contemporary NLG systems,
and 2) the most complete review of machine generated text detection methods to
date. This survey places machine generated text within its cybersecurity and
social context, and provides strong guidance for future work addressing the
most critical threat models, and ensuring detection systems themselves
demonstrate trustworthiness through fairness, robustness, and accountability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition. (arXiv:2210.09049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09049">
<div class="article-summary-box-inner">
<span><p>Few-shot Named Entity Recognition (NER) aims to identify named entities with
very little annotated data. Previous methods solve this problem based on
token-wise classification, which ignores the information of entity boundaries,
and inevitably the performance is affected by the massive non-entity tokens. To
this end, we propose a seminal span-based prototypical network (SpanProto) that
tackles few-shot NER via a two-stage approach, including span extraction and
mention classification. In the span extraction stage, we transform the
sequential tags into a global boundary matrix, enabling the model to focus on
the explicit boundary information. For mention classification, we leverage
prototypical learning to capture the semantic representations for each labeled
span and make the model better adapt to novel-class entities. To further
improve the model performance, we split out the false positives generated by
the span extractor but not labeled in the current episode set, and then present
a margin-based loss to separate them from each prototype region. Experiments
over multiple benchmarks demonstrate that our model outperforms strong
baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots. (arXiv:2210.11060v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11060">
<div class="article-summary-box-inner">
<span><p>This paper introduces Doc2Bot, a novel dataset for building machines that
help users seek information via conversations. This is of particular interest
for companies and organizations that own a large number of manuals or
instruction books. Despite its potential, the nature of our task poses several
challenges: (1) documents contain various structures that hinder the ability of
machines to comprehend, and (2) user information needs are often
underspecified. Compared to prior datasets that either focus on a single
structural type or overlook the role of questioning to uncover user needs, the
Doc2Bot dataset is developed to target such challenges systematically. Our
dataset contains over 100,000 turns based on Chinese documents from five
domains, larger than any prior document-grounded dialog dataset for information
seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track
user intentions, (2) dialog policy learning to plan system actions and
contents, and (3) response generation which generates responses based on the
outputs of the dialog policy. Baseline methods based on the latest deep
learning models are presented, indicating that our proposed tasks are
challenging and worthy of further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning. (arXiv:2210.12587v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12587">
<div class="article-summary-box-inner">
<span><p>Prompt tuning approaches, which learn task-specific soft prompts for a
downstream task conditioning on frozen pre-trained models, have attracted
growing interest due to its parameter efficiency. With large language models
and sufficient training data, prompt tuning performs comparably to full-model
tuning. However, with limited training samples in few-shot settings, prompt
tuning fails to match the performance of full-model fine-tuning. In this work,
we focus on improving the few-shot performance of prompt tuning by transferring
knowledge from soft prompts of source tasks. Recognizing the good
generalization capabilities of ensemble methods in low-data regime, we first
experiment and show that a simple ensemble of model predictions based on
different source prompts, outperforms existing multi-prompt knowledge transfer
approaches such as source prompt fusion in the few-shot setting. Motivated by
this observation, we further investigate model ensembles and propose
Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the
contribution of each source model for each target sample separately when
ensembling source model outputs. Through this way, SESoM inherits the superior
generalization of model ensemble approaches and simultaneously captures the
sample-specific competence of each source prompt. We conduct experiments across
a diverse set of eight NLP tasks using models of different scales (T5-{base,
large, XL}) and find that SESoM consistently outperforms the existing models of
the same as well as larger parametric scale by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14502">
<div class="article-summary-box-inner">
<span><p>A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing Model Bugs with Natural Language Patches. (arXiv:2211.03318v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03318">
<div class="article-summary-box-inner">
<span><p>Current approaches for fixing systematic problems in NLP models (e.g. regex
patches, finetuning on more data) are either brittle, or labor-intensive and
liable to shortcuts. In contrast, humans often provide corrections to each
other through natural language. Taking inspiration from this, we explore
natural language patches -- declarative statements that allow developers to
provide corrective feedback at the right level of abstraction, either
overriding the model (``if a review gives 2 stars, the sentiment is negative'')
or providing additional information the model may lack (``if something is
described as the bomb, then it is good''). We model the task of determining if
a patch applies separately from the task of integrating patch information, and
show that with a small amount of synthetic data, we can teach models to
effectively use real patches on real data -- 1 to 7 patches improve accuracy by
~1-4 accuracy points on different slices of a sentiment analysis dataset, and
F1 by 7 points on a relation extraction dataset. Finally, we show that
finetuning on as many as 100 labeled examples may be needed to match the
performance of a small set of language patches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05705">
<div class="article-summary-box-inner">
<span><p>The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. In this work, we introduce a novel task
of conversational aspect-based sentiment quadruple analysis, namely DiaASQ,
aiming to detect the sentiment quadruple of
\emph{target-aspect-opinion-sentiment} in a dialogue. DiaASQ bridges the gap
between fine-grained sentiment analysis and conversational opinion mining. We
manually construct a large-scale high-quality DiaASQ dataset in both Chinese
and English languages. We deliberately develop a neural model to benchmark the
task, which advances in effectively performing end-to-end quadruple prediction,
and manages to incorporate rich dialogue-specific and discourse feature
representations for better cross-utterance quadruple extraction. We finally
point out several potential future works to facilitate the follow-up research
of this new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-game Toxic Language Detection: Shared Task and Attention Residuals. (arXiv:2211.05995v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05995">
<div class="article-summary-box-inner">
<span><p>In-game toxic language becomes the hot potato in the gaming industry and
community. There have been several online game toxicity analysis frameworks and
models proposed. However, it is still challenging to detect toxicity due to the
nature of in-game chat, which has extremely short length. In this paper, we
describe how the in-game toxic language shared task has been established using
the real-world in-game chat data. In addition, we propose and introduce the
model/framework for toxic language token tagging (slot filling) from the
in-game chat. The data and code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06552">
<div class="article-summary-box-inner">
<span><p>Human intelligence can remarkably adapt quickly to new tasks and
environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research which
can enable similar capabilities in machines, we made the following
contributions (1) formalized the collaborative embodied agent using natural
language task; (2) developed a tool for extensive and scalable data collection;
and (3) collected the first dataset for interactive grounded language
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities. (arXiv:2211.06679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06679">
<div class="article-summary-box-inner">
<span><p>In this work, we present a conceptually simple and effective method to train
a strong bilingual/multilingual multimodal representation model. Starting from
the pre-trained multimodal representation model CLIP released by OpenAI, we
altered its text encoder with a pre-trained multilingual text encoder XLM-R,
and aligned both languages and image representations by a two-stage training
schema consisting of teacher learning and contrastive learning. We validate our
method through evaluations of a wide range of tasks. We set new
state-of-the-art performances on a bunch of tasks including ImageNet-CN,
Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with
CLIP on almost all tasks, suggesting that one can simply alter the text encoder
in CLIP for extended capabilities such as multilingual understanding. Our
models and code are available at https://github.com/FlagAI-Open/FlagAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Parsing as Tagging. (arXiv:2211.07344v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07344">
<div class="article-summary-box-inner">
<span><p>There have been many proposals to reduce constituency parsing to tagging in
the literature. To better understand what these approaches have in common, we
cast several existing proposals into a unifying pipeline consisting of three
steps: linearization, learning, and decoding. In particular, we show how to
reduce tetratagging, a state-of-the-art constituency tagger, to shift--reduce
parsing by performing a right-corner transformation on the grammar and making a
specific independence assumption. Furthermore, we empirically evaluate our
taxonomy of tagging pipelines with different choices of linearizers, learners,
and decoders. Based on the results in English and a set of 8 typologically
diverse languages, we conclude that the linearization of the derivation tree
and its alignment with the input sequence is the most critical factor in
achieving accurate taggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08584">
<div class="article-summary-box-inner">
<span><p>Radiology report summarization is a growing area of research. Given the
Findings and/or Background sections of a radiology report, the goal is to
generate a summary (called an Impression section) that highlights the key
observations and conclusions of the radiology study. Recent efforts have
released systems that achieve promising performance as measured by widely used
summarization metrics such as BLEU and ROUGE. However, the research area of
radiology report summarization currently faces important limitations. First,
most of the results are reported on private datasets. This limitation prevents
the ability to reproduce results and fairly compare different systems and
solutions. Secondly, to the best of our knowledge, most research is carried out
on chest X-rays. Sometimes, studies even omit to mention the concerned modality
and anatomy in the radiology reports used for their experiments. To palliate
these limitations, we propose a new dataset of six different modalities and
anatomies based on the MIMIC-III database. We further release our results and
the data splits used to carry out our experiments. Finally, we propose a simple
report summarization system that outperforms the previous replicable research
on the existing dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09783">
<div class="article-summary-box-inner">
<span><p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-22 23:13:42.276866356 UTC">2022-11-22 23:13:42 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>