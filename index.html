<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-05T01:30:00Z">04-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01209">
<div class="article-summary-box-inner">
<span><p>Unsupervised Relation Extraction (RE) aims to identify relations between
entities in text, without having access to labeled data during training. This
setting is particularly relevant for domain specific RE where no annotated
dataset is available and for open-domain RE where the types of relations are a
priori unknown. Although recent approaches achieve promising results, they
heavily depend on hyperparameters whose tuning would most often require labeled
data. To mitigate the reliance on hyperparameters, we propose PromptORE, a
''Prompt-based Open Relation Extraction'' model. We adapt the novel
prompt-tuning paradigm to work in an unsupervised setting, and use it to embed
sentences expressing a relation. We then cluster these embeddings to discover
candidate relations, and we experiment different strategies to automatically
estimate an adequate number of clusters. To the best of our knowledge,
PromptORE is the first unsupervised RE model that does not need hyperparameter
tuning. Results on three general and specific domain datasets show that
PromptORE consistently outperforms state-of-the-art models with a relative gain
of more than 40% in B 3 , V-measure and ARI. Qualitative analysis also
indicates PromptORE's ability to identify semantically coherent clusters that
are very close to true relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01228">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models for code (PLMCs) have gained attention in recent
research. These models are pre-trained on large-scale datasets using
multi-modal objectives. However, fine-tuning them requires extensive
supervision and is limited by the size of the dataset provided. We aim to
improve this issue by proposing a simple data augmentation framework. Our
framework utilizes knowledge gained during the pre-training and fine-tuning
stage to generate pseudo data, which is then used as training data for the next
step. We incorporate this framework into the state-of-the-art language models,
such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework
significantly improves PLMCs' performance in code-related sequence generation
tasks, such as code summarization and code generation in the CodeXGLUE
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01233">
<div class="article-summary-box-inner">
<span><p>Language modeling have shown impressive progress in generating compelling
text with good accuracy and high semantic coherence. An interesting research
direction is to augment these powerful models for specific applications using
contextual information. In this work, we explore multi-modal language modeling
for healthcare applications. We are interested in outcome prediction and
patient triage in hospital emergency department based on text information in
chief complaints and vital signs recorded at triage. We adapt Perceiver - a
modality-agnostic transformer-based model that has shown promising results in
several applications. Since vital-sign modality is represented in tabular
format, we modified Perceiver position encoding to ensure permutation
invariance. We evaluated the multi-modal language model for the task of
diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the
experimental analysis, we show that mutli-modality improves the prediction
performance compared with models trained solely on text or vital signs. We
identified disease categories for which multi-modality leads to performance
improvement and show that for these categories, vital signs have added
predictive power. By analyzing the cross-attention layer, we show how
multi-modality contributes to model predictions. This work gives interesting
insights on the development of multi-modal language models for healthcare
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01238">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of large language models (LLMs) in
email spam detection by comparing prominent models from three distinct
families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we
examine well-established machine learning techniques for spam detection, such
as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance
of these models across four public datasets, utilizing different numbers of
training samples (full training set and few-shot settings). Our findings reveal
that, in the majority of cases, LLMs surpass the performance of the popular
baseline techniques, particularly in few-shot scenarios. This adaptability
renders LLMs uniquely suited to spam detection tasks, where labeled samples are
limited in number and models require frequent updates. Additionally, we
introduce Spam-T5, a Flan-T5 model that has been specifically adapted and
fine-tuned for the purpose of detecting email spam. Our results demonstrate
that Spam-T5 surpasses baseline models and other LLMs in the majority of
scenarios, particularly when there are a limited number of training samples
available. Our code is publicly available at
https://github.com/jpmorganchase/emailspamdetection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01240">
<div class="article-summary-box-inner">
<span><p>Pain is a common reason for accessing healthcare resources and is a growing
area of research, especially in its overlap with mental health. Mental health
electronic health records are a good data source to study this overlap.
However, much information on pain is held in the free text of these records,
where mentions of pain present a unique natural language processing problem due
to its ambiguous nature. This project uses data from an anonymised mental
health electronic health records database. The data are used to train a machine
learning based classification algorithm to classify sentences as discussing
patient pain or not. This will facilitate the extraction of relevant pain
information from large databases, and the use of such outputs for further
studies on pain and mental health. 1,985 documents were manually
triple-annotated for creation of gold standard training data, which was used to
train three commonly used classification algorithms. The best performing model
achieved an F1-score of 0.98 (95% CI 0.98-0.99).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01241">
<div class="article-summary-box-inner">
<span><p>The increase in abusive content on online social media platforms is impacting
the social life of online users. Use of offensive and hate speech has been
making so-cial media toxic. Homophobia and transphobia constitute offensive
comments against LGBT+ community. It becomes imperative to detect and handle
these comments, to timely flag or issue a warning to users indulging in such
behaviour. However, automated detection of such content is a challenging task,
more so in Dravidian languages which are identified as low resource languages.
Motivated by this, the paper attempts to explore applicability of different
deep learning mod-els for classification of the social media comments in
Malayalam and Tamil lan-guages as homophobic, transphobic and
non-anti-LGBT+content. The popularly used deep learning models- Convolutional
Neural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and
transformer-based learning models (Multilingual BERT and IndicBERT) are applied
to the classification problem. Results obtained show that IndicBERT outperforms
the other imple-mented models, with obtained weighted average F1-score of 0.86
and 0.77 for Malayalam and Tamil, respectively. Therefore, the present work
confirms higher performance of IndicBERT on the given task in selected
Dravidian languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs. (arXiv:2304.01242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01242">
<div class="article-summary-box-inner">
<span><p>Clinical evidence encompasses the associations and impacts between patients,
interventions (such as drugs or physiotherapy), problems, and outcomes. The
goal of recommending clinical evidence is to provide medical practitioners with
relevant information to support their decision-making processes and to generate
new evidence. Our specific task focuses on recommending evidence based on
clinical problems. However, the direct connections between certain clinical
problems and related evidence are often sparse, creating a challenge of link
sparsity. Additionally, to recommend appropriate evidence, it is essential to
jointly exploit both topological relationships among evidence and textual
information describing them. To address these challenges, we define two
knowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to
represent the topological and linguistic relations among evidential elements,
respectively. We also introduce a multi-channel heterogeneous learning model
and a fusional attention mechanism to handle the co-reference-text
heterogeneity in evidence recommendation. Our experiments demonstrate that our
model outperforms state-of-the-art methods on open data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01246">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI
heatwave due to its human-like conversations with detailed and articulate
answers across many domains of knowledge. While LLMs are being quickly applied
to many AI application domains, we are interested in the following question:
Can safety analysis for safety-critical systems make use of LLMs? To answer, we
conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic
Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent
techniques for hazard analysis, is known to have limitations such as high
complexity and subjectivity, which this paper aims to explore the use of
ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA
are investigated by considering its interaction with human experts: one-off
simplex interaction, recurring simplex interaction, and recurring duplex
interaction. Comparative results reveal that: (i) using ChatGPT without human
experts' intervention can be inadequate due to reliability and accuracy issues
of LLMs; (ii) more interactions between ChatGPT and human experts may yield
better results; and (iii) using ChatGPT in STPA with extra care can outperform
human safety experts alone, as demonstrated by reusing an existing comparison
method with baselines. In addition to making the first attempt to apply LLMs in
safety analysis, this paper also identifies key challenges (e.g.,
trustworthiness concern of LLMs, the need of standardisation) for future
research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for Translation with Semi-Supervised Pseudo-Parallel Document Generation. (arXiv:2304.01282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01282">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-training significantly improves many multilingual NLP tasks,
including machine translation. Most existing methods are based on some variants
of masked language modeling and text-denoising objectives on monolingual data.
Multilingual pre-training on monolingual data ignores the availability of
parallel data in many language pairs. Also, some other works integrate the
available human-generated parallel translation data in their pre-training. This
kind of parallel data is definitely helpful, but it is limited even in
high-resource language pairs. This paper introduces a novel semi-supervised
method, SPDG, that generates high-quality pseudo-parallel data for multilingual
pre-training. First, a denoising model is pre-trained on monolingual data to
reorder, add, remove, and substitute words, enhancing the pre-training
documents' quality. Then, we generate different pseudo-translations for each
pre-training document using dictionaries for word-by-word translation and
applying the pre-trained denoising model. The resulting pseudo-parallel data is
then used to pre-train our multilingual sequence-to-sequence model, PEACH. Our
experiments show that PEACH outperforms existing approaches used in training
mT5 and mBART on various translation tasks, including supervised, zero- and
few-shot scenarios. Moreover, PEACH's ability to transfer knowledge between
similar languages makes it particularly useful for low-resource languages. Our
results demonstrate that with high-quality dictionaries for generating accurate
pseudo-parallel, PEACH can be valuable for low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01295">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD, a parallel and large-scale
multilingual conversation dataset that we created by translating the
English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into
105 other languages. XSGD contains approximately 330k utterances per language.
To facilitate aligned cross-lingual representations, we develop an efficient
prompt-tuning-based method for learning alignment prompts. We also investigate
two different classifiers: NLI-based and vanilla classifiers, and test
cross-lingual capability enabled by the aligned prompts. We evaluate our
model's cross-lingual generalization capabilities on two conversation tasks:
slot-filling and intent classification. Our results demonstrate the strong and
efficient modeling ability of NLI-based classifiers and the large cross-lingual
transfer improvements achieved by our aligned prompts, particularly in few-shot
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki. (arXiv:2304.01319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01319">
<div class="article-summary-box-inner">
<span><p>One of the major challenges that under-represented and endangered language
communities face in language technology is the lack or paucity of language
data. This is also the case of the Southern varieties of the Kurdish and Laki
languages for which very limited resources are available with insubstantial
progress in tools. To tackle this, we provide a few approaches that rely on the
content of local news websites, a local radio station that broadcasts content
in Southern Kurdish and fieldwork for Laki. In this paper, we describe some of
the challenges of such under-represented languages, particularly in writing and
standardization, and also, in retrieving sources of data and retro-digitizing
handwritten content to create a corpus for Southern Kurdish and Laki. In
addition, we study the task of language identification in light of the other
variants of Kurdish and Zaza-Gorani languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALI: A Language Identification Benchmark for Perso-Arabic Scripts. (arXiv:2304.01322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01322">
<div class="article-summary-box-inner">
<span><p>The Perso-Arabic scripts are a family of scripts that are widely adopted and
used by various linguistic communities around the globe. Identifying various
languages using such scripts is crucial to language technologies and
challenging in low-resource setups. As such, this paper sheds light on the
challenges of detecting languages using Perso-Arabic scripts, especially in
bilingual communities where ``unconventional'' writing is practiced. To address
this, we use a set of supervised techniques to classify sentences into their
languages. Building on these, we also propose a hierarchical model that targets
clusters of languages that are more often confused by the classifiers. Our
experiment results indicate the effectiveness of our solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grand Challenge On Detecting Cheapfakes. (arXiv:2304.01328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01328">
<div class="article-summary-box-inner">
<span><p>Cheapfake is a recently coined term that encompasses non-AI ("cheap")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of Document Similarity Algorithms. (arXiv:2304.01330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01330">
<div class="article-summary-box-inner">
<span><p>Document similarity is an important part of Natural Language Processing and
is most commonly used for plagiarism-detection and text summarization. Thus,
finding the overall most effective document similarity algorithm could have a
major positive impact on the field of Natural Language Processing. This report
sets out to examine the numerous document similarity algorithms, and determine
which ones are the most useful. It addresses the most effective document
similarity algorithm by categorizing them into 3 types of document similarity
algorithms: statistical algorithms, neural networks, and corpus/knowledge-based
algorithms. The most effective algorithms in each category are also compared in
our work using a series of benchmark datasets and evaluations that test every
possible area that each algorithm could be used in.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks. (arXiv:2304.01331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01331">
<div class="article-summary-box-inner">
<span><p>Event data, or structured records of ``who did what to whom'' that are
automatically extracted from text, is an important source of data for scholars
of international politics. The high cost of developing new event datasets,
especially using automated systems that rely on hand-built dictionaries, means
that most researchers draw on large, pre-existing datasets such as ICEWS rather
than developing tailor-made event datasets optimized for their specific
research question. This paper describes a ``bag of tricks'' for efficient,
custom event data production, drawing on recent advances in natural language
processing (NLP) that allow researchers to rapidly produce customized event
datasets. The paper introduces techniques for training an event category
classifier with active learning, identifying actors and the recipients of
actions in text using large language models and standard machine learning
classifiers and pretrained ``question-answering'' models from NLP, and
resolving mentions of actors to their Wikipedia article to categorize them. We
describe how these techniques produced the new POLECAT global event dataset
that is intended to replace ICEWS, along with examples of how scholars can
quickly produce smaller, custom event datasets. We publish example code and
models to implement our new techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies. (arXiv:2304.01344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01344">
<div class="article-summary-box-inner">
<span><p>End-to-end relation extraction (E2ERE) is an important task in information
extraction, more so for biomedicine as scientific literature continues to grow
exponentially. E2ERE typically involves identifying entities (or named entity
recognition (NER)) and associated relations, while most RE tasks simply assume
that the entities are provided upfront and end up performing relation
classification. E2ERE is inherently more difficult than RE alone given the
potential snowball effect of errors from NER leading to more errors in RE. A
complex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI,
2017) that identifies relations between chemical compounds and genes/proteins
in scientific literature. ChemProt is included in all recent biomedical natural
language processing benchmarks including BLUE, BLURB, and BigBio. However, its
treatment in these benchmarks and in other separate efforts is typically not
end-to-end, with few exceptions. In this effort, we employ a span-based
pipeline approach to produce a new state-of-the-art E2ERE performance on the
ChemProt dataset, resulting in $&gt; 4\%$ improvement in F1-score over the prior
best effort. Our results indicate that a straightforward fine-grained
tokenization scheme helps span-based approaches excel in E2ERE, especially with
regards to handling complex named entities. Our error analysis also identifies
a few key failure modes in E2ERE for ChemProt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01352">
<div class="article-summary-box-inner">
<span><p>We present a simple cross-lingual plagiarism detection method applicable to a
large number of languages. The presented approach leverages open multilingual
thesauri for candidate retrieval task and pre-trained multilingual BERT-based
language models for detailed analysis. The method does not rely on machine
translation and word sense disambiguation when in use, and therefore is
suitable for a large number of languages, including under-resourced languages.
The effectiveness of the proposed approach is demonstrated for several existing
and new benchmarks, achieving state-of-the-art results for French, Russian, and
Armenian languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01373">
<div class="article-summary-box-inner">
<span><p>How do large language models (LLMs) develop and evolve over the course of
training? How do these patterns change as models scale? To answer these
questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on
public data seen in the exact same order and ranging in size from 70M to 12B
parameters. We provide public access to 154 checkpoints for each one of the 16
models, alongside tools to download and reconstruct their exact training
dataloaders for further study. We intend \textit{Pythia} to facilitate research
in many areas, and we present several case studies including novel results in
memorization, term frequency effects on few-shot performance, and reducing
gender bias. We demonstrate that this highly controlled setup can be used to
yield novel insights toward LLMs and their training dynamics. Trained models,
analysis code, training code, and training data can be found at
https://github.com/EleutherAI/pythia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01412">
<div class="article-summary-box-inner">
<span><p>We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation
turns between agents working at Statistics Canada and online users looking for
published data tables. The conversations stem from genuine intents, are held in
English or French, and lead to agents retrieving one of over 5000 complex data
tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of
relevant tables based on a on-going conversation, and (2) automatic generation
of appropriate agent responses at each turn. We investigate the difficulty of
each task by establishing strong baselines. Our experiments on a temporal data
split reveal that all models struggle to generalize to future conversations, as
we observe a significant drop in performance across both tasks when we move
from the validation to the test set. In addition, we find that response
generation models struggle to decide when to return a table. Considering that
the tasks pose significant challenges to existing models, we encourage the
community to develop models for our task, which can be directly used to help
knowledge workers find relevant tables for live chat users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thematic context vector association based on event uncertainty for Twitter. (arXiv:2304.01423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01423">
<div class="article-summary-box-inner">
<span><p>Keyword extraction is a crucial process in text mining. The extraction of
keywords with respective contextual events in Twitter data is a big challenge.
The challenging issues are mainly because of the informality in the language
used. The use of misspelled words, acronyms, and ambiguous terms causes
informality. The extraction of keywords with informal language in current
systems is pattern based or event based. In this paper, contextual keywords are
extracted using thematic events with the help of data association. The thematic
context for events is identified using the uncertainty principle in the
proposed system. The thematic contexts are weighed with the help of vectors
called thematic context vectors which signifies the event as certain or
uncertain. The system is tested on the Twitter COVID-19 dataset and proves to
be effective. The system extracts event-specific thematic context vectors from
the test dataset and ranks them. The extracted thematic context vectors are
used for the clustering of contextual thematic vectors which improves the
silhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF.
The thematic context vector can be used in other applications like
Cyberbullying, sarcasm detection, figurative language detection, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarity based Sarcasm Detection using Semigraph. (arXiv:2304.01424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01424">
<div class="article-summary-box-inner">
<span><p>Sarcasm is an advanced linguistic expression often found on various online
platforms. Sarcasm detection is challenging in natural language processing
tasks that affect sentiment analysis. This article presents the inventive
method of the semigraph, including semigraph construction and sarcasm detection
processes. A variation of the semigraph is suggested in the pattern-relatedness
of the text document. The proposed method is to obtain the sarcastic and
non-sarcastic polarity scores of a document using a semigraph. The sarcastic
polarity score represents the possibility that a document will become
sarcastic. Sarcasm is detected based on the polarity scoring model. The
performance of the proposed model enhances the existing prior art approach to
sarcasm detection. In the Amazon product review, the model achieved the
accuracy, recall, and f-measure of 0.87, 0.79, and 0.83, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Vector Grounding Problem. (arXiv:2304.01481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01481">
<div class="article-summary-box-inner">
<span><p>The remarkable performance of large language models (LLMs) on complex
linguistic tasks has sparked a lively debate on the nature of their
capabilities. Unlike humans, these models learn language exclusively from
textual data, without direct interaction with the real world. Nevertheless,
they can generate seemingly meaningful text about a wide range of topics. This
impressive accomplishment has rekindled interest in the classical 'Symbol
Grounding Problem,' which questioned whether the internal representations and
outputs of classical symbolic AI systems could possess intrinsic meaning.
Unlike these systems, modern LLMs are artificial neural networks that compute
over vectors rather than symbols. However, an analogous problem arises for such
systems, which we dub the Vector Grounding Problem. This paper has two primary
objectives. First, we differentiate various ways in which internal
representations can be grounded in biological or artificial systems,
identifying five distinct notions discussed in the literature: referential,
sensorimotor, relational, communicative, and epistemic grounding.
Unfortunately, these notions of grounding are often conflated. We clarify the
differences between them, and argue that referential grounding is the one that
lies at the heart of the Vector Grounding Problem. Second, drawing on theories
of representational content in philosophy and cognitive science, we propose
that certain LLMs, particularly those fine-tuned with Reinforcement Learning
from Human Feedback (RLHF), possess the necessary features to overcome the
Vector Grounding Problem, as they stand in the requisite causal-historical
relations to the world that underpin intrinsic meaning. We also argue that,
perhaps unexpectedly, multimodality and embodiment are neither necessary nor
sufficient conditions for referential grounding in artificial systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01483">
<div class="article-summary-box-inner">
<span><p>Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have
recently attracted increasing interest, research enthusiasm, and business
demand. However, their massive computation resources and huge memory footprint
are inevitable challenges. To tackle this issue, we propose BCT, a framework of
blockwise compression for transformers without retraining, to lower deployment
thresholds. BCT achieves more fine-grained compression of the whole
transformer, including embedding, matrix multiplication, GELU, Softmax, layer
normalization, and all the intermediate results. As a case, we compress an
efficient model with BCT and evaluate it on several General Language
Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve
a less than 0.90% accuracy drop in most tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01487">
<div class="article-summary-box-inner">
<span><p>ChatGPT has become a global sensation. As ChatGPT and other Large Language
Models (LLMs) emerge, concerns of misusing them in various ways increase, such
as disseminating fake news, plagiarism, manipulating public opinion, cheating,
and fraud. Hence, distinguishing AI-generated from human-generated becomes
increasingly essential. Researchers have proposed various detection
methodologies, ranging from basic binary classifiers to more complex
deep-learning models. Some detection techniques rely on statistical
characteristics or syntactic patterns, while others incorporate semantic or
contextual information to improve accuracy. The primary objective of this study
is to provide a comprehensive and contemporary assessment of the most recent
techniques in ChatGPT detection. Additionally, we evaluated other AI-generated
text detection tools that do not specifically claim to detect ChatGPT-generated
content to assess their performance in detecting ChatGPT-generated content. For
our evaluation, we have curated a benchmark dataset consisting of prompts from
ChatGPT and humans, including diverse questions from medical, open Q&amp;A, and
finance domains and user-generated responses from popular social networking
platforms. The dataset serves as a reference to assess the performance of
various techniques in detecting ChatGPT-generated content. Our evaluation
results demonstrate that none of the existing methods can effectively detect
ChatGPT-generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01492">
<div class="article-summary-box-inner">
<span><p>The truth is significantly hampered by massive rumors that spread along with
breaking news or popular topics. Since there is sufficient corpus gathered from
the same domain for model training, existing rumor detection algorithms show
promising performance on yesterday's news. However, due to a lack of training
data and prior expert knowledge, they are poor at spotting rumors concerning
unforeseen events, especially those propagated in different languages (i.e.,
low-resource regimes). In this paper, we propose a unified contrastive transfer
framework to detect rumors by adapting the features learned from well-resourced
rumor data to that of the low-resourced. More specifically, we first represent
rumor circulated on social media as an undirected topology, and then train a
Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our
model explicitly breaks the barriers of the domain and/or language issues, via
language alignment and a novel domain-adaptive contrastive learning mechanism.
To enhance the representation learning from a small set of target events, we
reveal that rumor-indicative signal is closely correlated with the uniformity
of the distribution of these events. We design a target-wise contrastive
training mechanism with three data augmentation strategies, capable of unifying
the representations by distinguishing target events. Extensive experiments
conducted on four low-resource datasets collected from real-world microblog
platforms demonstrate that our framework achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for detecting rumors
at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models. (arXiv:2304.01515v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01515">
<div class="article-summary-box-inner">
<span><p>Token-based masked generative models are gaining popularity for their fast
inference time with parallel decoding. While recent token-based approaches
achieve competitive performance to diffusion-based models, their generation
performance is still suboptimal as they sample multiple tokens simultaneously
without considering the dependence among them. We empirically investigate this
problem and propose a learnable sampling model, Text-Conditioned Token
Selection (TCTS), to select optimal tokens via localized supervision with text
information. TCTS improves not only the image quality but also the semantic
alignment of the generated images with the given texts. To further improve the
image quality, we introduce a cohesive sampling strategy, Frequency Adaptive
Sampling (FAS), to each group of tokens divided according to the self-attention
maps. We validate the efficacy of TCTS combined with FAS with various
generative tasks, demonstrating that it significantly outperforms the baselines
in image-text alignment and image quality. Our text-conditioned sampling
framework further reduces the original inference time by more than 50% without
modifying the original generative model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01563">
<div class="article-summary-box-inner">
<span><p>The multi-modal entity alignment (MMEA) aims to find all equivalent entity
pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and
neighboring entities are valuable for the alignment task, but existing works
ignore contextual gap problems that the aligned entities have different numbers
of attributes on specific modality when learning entity representations. In
this paper, we propose a novel attribute-consistent knowledge graph
representation learning framework for MMEA (ACK-MMEA) to compensate the
contextual gaps through incorporating consistent alignment knowledge.
Attribute-consistent KGs (ACKGs) are first constructed via multi-modal
attribute uniformization with merge and generate operators so that each entity
has one and only one uniform feature in each modality. The ACKGs are then fed
into a relation-aware graph neural network with random dropouts, to obtain
aggregated relation representations and robust entity representations. In order
to evaluate the ACK-MMEA facilitated for entity alignment, we specially design
a joint alignment loss for both entity and attribute evaluation. Extensive
experiments conducted on two benchmark datasets show that our approach achieves
excellent performance compared to its competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01597">
<div class="article-summary-box-inner">
<span><p>Masked language modeling (MLM) plays a key role in pretraining large language
models. But the MLM objective is often dominated by high-frequency words that
are sub-optimal for learning factual knowledge. In this work, we propose an
approach for influencing MLM pretraining in a way that can improve language
model performance on a variety of knowledge-intensive tasks. We force the
language model to prioritize informative words in a fully unsupervised way.
Experiments demonstrate that the proposed approach can significantly improve
the performance of pretrained language models on tasks such as factual recall,
question answering, sentiment analysis, and natural language inference in a
closed-book setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDeR: A Dataset for Exploring Dependency Relations Between Events. (arXiv:2304.01612v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01612">
<div class="article-summary-box-inner">
<span><p>Relation extraction is a central task in natural language processing (NLP)
and information retrieval (IR) research. We argue that an important type of
relation not explored in NLP or IR research to date is that of an event being
an argument - required or optional - of another event. We introduce the
human-annotated Event Dependency Relation dataset (EDeR) which provides this
dependency relation. The annotation is done on a sample of documents from the
OntoNotes dataset, which has the added benefit that it integrates with
existing, orthogonal, annotations of this dataset. We investigate baseline
approaches for predicting the event dependency relation, the best of which
achieves an accuracy of 82.61 for binary argument/non-argument classification.
We show that recognizing this relation leads to more accurate event extraction
(semantic role labelling) and can improve downstream tasks that depend on this,
such as co-reference resolution. Furthermore, we demonstrate that predicting
the three-way classification into the required argument, optional argument or
non-argument is a more challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01621">
<div class="article-summary-box-inner">
<span><p>Cross-lingual science journalism generates popular science stories of
scientific articles different from the source language for a non-expert
audience. Hence, a cross-lingual popular summary must contain the salient
content of the input document, and the content should be coherent,
comprehensible, and in a local language for the targeted audience. We improve
these aspects of cross-lingual summary generation by joint training of two
high-level NLP tasks, simplification and cross-lingual summarization. The
former task reduces linguistic complexity, and the latter focuses on
cross-lingual abstractive summarization. We propose a novel multi-task
architecture - SimCSum consisting of one shared encoder and two parallel
decoders jointly learning simplification and cross-lingual summarization. We
empirically investigate the performance of SimCSum by comparing it with several
strong baselines over several evaluation metrics and by human evaluation.
Overall, SimCSum demonstrates statistically significant improvements over the
state-of-the-art on two non-synthetic cross-lingual scientific datasets.
Furthermore, we conduct an in-depth investigation into the linguistic
properties of generated summaries and an error analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01622">
<div class="article-summary-box-inner">
<span><p>Similar Case Matching (SCM) is designed to determine whether two cases are
similar. The task has an essential role in the legal system, helping legal
professionals to find relevant cases quickly and thus deal with them more
efficiently. Existing research has focused on improving the model's performance
but not on its interpretability. Therefore, this paper proposes a pipeline
framework for interpretable SCM, which consists of four modules: a judicial
feature sentence identification module, a case matching module, a feature
sentence alignment module, and a conflict disambiguation module. Unlike
existing SCM methods, our framework will identify feature sentences in a case
that contain essential information, perform similar case matching based on the
extracted feature sentence results, and align the feature sentences in the two
cases to provide evidence for the similarity of the cases. SCM results may
conflict with feature sentence alignment results, and our framework further
disambiguates against this inconsistency. The experimental results show the
effectiveness of our framework, and our work provides a new benchmark for
interpretable SCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01638">
<div class="article-summary-box-inner">
<span><p>Because of the inevitable cost and complexity of transformer and pre-trained
models, efficiency concerns are raised for long text classification. Meanwhile,
in the highly sensitive domains, e.g., healthcare and legal long-text mining,
potential model distrust, yet underrated and underexplored, may hatch vital
apprehension. Existing methods generally segment the long text, encode each
piece with the pre-trained model, and use attention or RNNs to obtain long text
representation for classification. In this work, we propose a simple but
effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace
attention/RNNs in the above framework. Unlike prior efforts, SWIPE can
effectively learn the label of the entire text with supervised training, while
perceive the labels of the segments and estimate their contributions to the
long-text labeling in an unsupervised manner. As a general classifier, SWIPE
can endorse different encoders, and it outperforms SOTA models in terms of
classification accuracy and model efficiency. It is noteworthy that SWIPE
achieves superior interpretability to transparentize long text classification
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Image Captioning with Discriminative Finetuning. (arXiv:2304.01662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01662">
<div class="article-summary-box-inner">
<span><p>Neural captioners are typically trained to mimic human-generated references
without optimizing for any specific communication goal, leading to problems
such as the generation of vague captions. In this paper, we show that
fine-tuning an out-of-the-box neural captioner with a self-supervised
discriminative communication objective helps to recover a plain, visually
descriptive language that is more informative about image contents. Given a
target image, the system must learn to produce a description that enables an
out-of-the-box text-conditioned image retriever to identify such image among a
set of candidates. We experiment with the popular ClipCap captioner, also
replicating the main results with BLIP. In terms of similarity to ground-truth
human descriptions, the captions emerging from discriminative finetuning lag
slightly behind those generated by the non-finetuned model, when the latter is
trained and tested on the same caption dataset. However, when the model is used
without further tuning to generate captions for out-of-domain datasets, our
discriminatively-finetuned captioner generates descriptions that resemble human
references more than those produced by the same captioner without finetuning.
We further show that, on the Conceptual Captions dataset, discriminatively
finetuned captions are more helpful than either vanilla ClipCap captions or
ground-truth captions for human annotators tasked with an image discrimination
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01665">
<div class="article-summary-box-inner">
<span><p>Language models have achieved impressive results in natural language
processing tasks, but their ability to perform symbolic operations and
arithmetic operations, remains limited, which attribute to their learn the
rules implicitly from data. We explore how to incorporate compiled neural
networks (CoNNs) which weight is specially designed, into the architecture of
language models to enable the language model trained by gradient to obtain
fully rule comprehension ability. The incorporation of compiled neural networks
offers a promising direction for improving the performance of language models
on compound tasks, particularly in areas that require a deeper comprehension of
abstract rules beyond recognizing patterns in training data. Our method, which
call "Neural Comprehension", helps language models achieve absolute accuracy in
symbolic operations, thereby enhancing their ability for rule reasoning,
symbolic reasoning, and arithmetic reasoning. Our code is publicly available
at: \url{https://github.com/WENGSYX/Neural-Comprehension}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01666">
<div class="article-summary-box-inner">
<span><p>Semantic Shift Detection (SSD) is the task of identifying, interpreting, and
assessing the possible change over time in the meanings of a target word.
Traditionally, SSD has been addressed by linguists and social scientists
through manual and time-consuming activities. In the recent years,
computational approaches based on Natural Language Processing and word
embeddings gained increasing attention to automate SSD as much as possible. In
particular, over the past three years, significant advancements have been made
almost exclusively based on word contextualised embedding models, which can
handle the multiple usages/meanings of the words and better capture the related
semantic shifts. In this paper, we survey the approaches based on
contextualised embeddings for SSD (i.e., CSSDetection) and we propose a
classification framework characterised by meaning representation,
time-awareness, and learning modality dimensions. The framework is exploited i)
to review the measures for shift assessment, ii) to compare the approaches on
performance, and iii) to discuss the current issues in terms of scalability,
interpretability, and robustness. Open challenges and future research
directions about CSSDetection are finally outlined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01680">
<div class="article-summary-box-inner">
<span><p>This paper investigates how Transformer language models (LMs) fine-tuned for
acceptability classification capture linguistic features. Our approach uses the
best practices of topological data analysis (TDA) in NLP: we construct directed
attention graphs from attention matrices, derive topological features from
them, and feed them to linear classifiers. We introduce two novel features,
chordality, and the matching number, and show that TDA-based classifiers
outperform fine-tuning baselines. We experiment with two datasets, CoLA and
RuCoLA in English and Russian, typologically different languages. On top of
that, we propose several black-box introspection techniques aimed at detecting
changes in the attention mode of the LMs during fine-tuning, defining the LM's
prediction confidences, and associating individual heads with fine-grained
grammar phenomena. Our results contribute to understanding the behavior of
monolingual LMs in the acceptability classification task, provide insights into
the functional roles of attention heads, and highlight the advantages of
TDA-based approaches for analyzing LMs. We release the code and the
experimental results for further uptake.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rumour Detection and Analysis on Twitter. (arXiv:2304.01712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01712">
<div class="article-summary-box-inner">
<span><p>In recent years people have become increasingly reliant on social media to
read news and get information, and some social media users post unsubstantiated
information to gain attention. Such information is known as rumours. Nowadays,
rumour detection is receiving a growing amount of attention because of the
pandemic of the New Coronavirus, which has led to a large number of rumours
being spread. In this paper, a Natural Language Processing (NLP) system is
built to predict rumours. The best model is applied to the COVID-19 tweets to
conduct exploratory data analysis. The contribution of this study is twofold:
(1) to compare rumours and facts using state-of-the-art natural language
processing models in two dimensions: language structure and propagation route.
(2) An analysis of how rumours differ from facts in terms of their lexical use
and the emotions they imply. This study shows that linguistic structure is a
better feature to distinguish rumours from facts compared to the propagation
path. In addition, rumour tweets contain more vocabulary related to politics
and negative emotions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation. (arXiv:2304.01746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01746">
<div class="article-summary-box-inner">
<span><p>ChatGPT, a large-scale language model based on the advanced GPT-3.5
architecture, has shown remarkable potential in various Natural Language
Processing (NLP) tasks. However, there is currently a dearth of comprehensive
study exploring its potential in the area of Grammatical Error Correction
(GEC). To showcase its capabilities in GEC, we design zero-shot
chain-of-thought (CoT) and few-shot CoT settings using in-context learning for
ChatGPT. Our evaluation involves assessing ChatGPT's performance on five
official test sets in three different languages, along with three
document-level GEC test sets in English. Our experimental results and human
evaluations demonstrate that ChatGPT has excellent error detection capabilities
and can freely correct errors to make the corrected sentences very fluent,
possibly due to its over-correction tendencies and not adhering to the
principle of minimal edits. Additionally, its performance in non-English and
low-resource settings highlights its potential in multilingual GEC tasks.
However, further analysis of various types of errors at the document-level has
shown that ChatGPT cannot effectively correct agreement, coreference, tense
errors across sentences, and cross-sentence boundary errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01752">
<div class="article-summary-box-inner">
<span><p>Vision-Language (V-L) models trained with contrastive learning to align the
visual and language modalities have been shown to be strong few-shot learners.
Soft prompt learning is the method of choice for few-shot downstream adaption
aiming to bridge the modality gap caused by the distribution shift induced by
the new domain. While parameter-efficient, prompt learning still requires
access to the model weights and can be computationally infeasible for large
models with billions of parameters. To address these shortcomings, in this
work, we describe a black-box method for V-L few-shot adaptation that (a)
operates on pre-computed image and text features and hence works without access
to the model's weights, (b) it is orders of magnitude faster at training time,
(c) it is amenable to both supervised and unsupervised training, and (d) it can
be even used to align image and text features computed from uni-modal models.
To achieve this, we propose Linear Feature Alignment (LFA), a simple linear
approach for V-L re-alignment in the target domain. LFA is initialized from a
closed-form solution to a least-squares problem and then it is iteratively
updated by minimizing a re-ranking loss. Despite its simplicity, our approach
can even surpass soft-prompt learning methods as shown by extensive experiments
on 11 image and 2 video datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01774">
<div class="article-summary-box-inner">
<span><p>Human-in-the-loop topic modelling incorporates users' knowledge into the
modelling process, enabling them to refine the model iteratively. Recent
research has demonstrated the value of user feedback, but there are still
issues to consider, such as the difficulty in tracking changes, comparing
different models and the lack of evaluation based on real-world examples of
use. We developed a novel, interactive human-in-the-loop topic modeling system
with a user-friendly interface that enables users compare and record every step
they take, and a novel topic words suggestion feature to help users provide
feedback that is faithful to the ground truth. Our system also supports not
only what traditional topic models can do, i.e., learning the topics from the
whole corpus, but also targeted topic modelling, i.e., learning topics for
specific aspects of the corpus. In this article, we provide an overview of the
system and present the results of a series of user studies designed to assess
the value of the system in progressively more realistic applications of topic
modelling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01852">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of ChatGPT and GPT-4,
state-of-the-art large language models (LLM) from the GPT series, and their
prospective applications across diverse domains. Indeed, key innovations such
as large-scale pre-training that captures knowledge across the entire world
wide web, instruction fine-tuning and Reinforcement Learning from Human
Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability
and performance. We performed an in-depth analysis of 194 relevant papers on
arXiv, encompassing trend analysis, word cloud representation, and distribution
analysis across various application domains. The findings reveal a significant
and increasing interest in ChatGPT/GPT-4 research, predominantly centered on
direct natural language processing applications, while also demonstrating
considerable potential in areas ranging from education and history to
mathematics, medicine, and physics. This study endeavors to furnish insights
into ChatGPT's capabilities, potential implications, ethical concerns, and
offer direction for future advancements in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01890">
<div class="article-summary-box-inner">
<span><p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for
the countries of Brazil, Germany, India and Kenya, to aid training and
interpretability of models. We demonstrate how our lexicon can be used to
interpret model predictions, showing that models developed to classify extreme
speech rely heavily on target words when making predictions. Further, we
propose a method to aid shot selection for training in low-resource settings
via HATELEXICON. In few-shot learning, the selection of shots is of paramount
importance to model performance. In our work, we simulate a few-shot setting
for German and Hindi, using HASOC data for training and the Multilingual
HateCheck (MHC) as a benchmark. We show that selecting shots based on our
lexicon leads to models performing better on MHC than models trained on shots
sampled randomly. Thus, when given only a few training examples, using our
lexicon to select shots containing more sociocultural information leads to
better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants. (arXiv:2304.01894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01894">
<div class="article-summary-box-inner">
<span><p>In this work, we develop language models for the Sanskrit language, namely
Bidirectional Encoder Representations from Transformers (BERT) and its
variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using
Devanagari Sanskrit text corpus. Then we extracted the features for the given
text from these models. We applied the dimensional reduction and clustering
techniques on the features to generate an extractive summary for a given
Sanskrit document. Along with the extractive text summarization techniques, we
have also created and released a Sanskrit Devanagari text corpus publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01904">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have recently shown remarkable performance on reasoning
tasks by explicitly generating intermediate inferences, e.g., chain-of-thought
prompting. However, these intermediate inference steps may be inappropriate
deductions from the initial context and lead to incorrect final predictions.
Here we introduce REFINER, a framework for finetuning LMs to explicitly
generate intermediate reasoning steps while interacting with a critic model
that provides automated feedback on the reasoning. Specifically, the critic
provides structured feedback that the reasoning LM uses to iteratively improve
its intermediate arguments. Empirical evaluations of REFINER on three diverse
reasoning tasks show significant improvements over baseline LMs of comparable
scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic
significantly improves reasoning without finetuning the reasoner. Finally, our
critic model is trained without expensive human-in-the-loop data but can be
substituted with humans at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01905">
<div class="article-summary-box-inner">
<span><p>We present dual-attention neural biasing, an architecture designed to boost
Wake Words (WW) recognition and improve inference time latency on speech
recognition tasks. This architecture enables a dynamic switch for its runtime
compute paths by exploiting WW spotting to select which branch of its attention
networks to execute for an input audio frame. With this approach, we
effectively improve WW spotting accuracy while saving runtime compute cost as
defined by floating point operations (FLOPs). Using an in-house de-identified
dataset, we demonstrate that the proposed dual-attention network can reduce the
compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the
number of parameters. This architecture improves WW F1 score by $16\%$ relative
and improves generic rare word error rate by $3\%$ relative compared to the
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resources and Few-shot Learners for In-context Learning in Slavic Languages. (arXiv:2304.01922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01922">
<div class="article-summary-box-inner">
<span><p>Despite the rapid recent progress in creating accurate and compact in-context
learners, most recent work focuses on in-context learning (ICL) for tasks in
English. However, the ability to interact with users of languages outside
English presents a great potential for broadening the applicability of language
technologies to non-English speakers.
</p>
<p>In this work, we collect the infrastructure necessary for training and
evaluation of ICL in a selection of Slavic languages: Czech, Polish, and
Russian. We link a diverse set of datasets and cast these into a unified
instructional format through a set of transformations and newly-crafted
templates written purely in target languages. Using the newly-curated dataset,
we evaluate a set of the most recent in-context learners and compare their
results to the supervised baselines. Finally, we train, evaluate and publish a
set of in-context learning models that we train on the collected resources and
compare their performance to previous work.
</p>
<p>We find that ICL models tuned in English are also able to learn some tasks
from non-English contexts, but multilingual instruction fine-tuning
consistently improves the ICL ability. We also find that the massive multitask
training can be outperformed by single-task training in the target language,
uncovering the potential for specializing in-context learners to the
language(s) of their application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01933">
<div class="article-summary-box-inner">
<span><p>The success of large language models (LLMs), like GPT-3 and ChatGPT, has led
to the development of numerous cost-effective and accessible alternatives that
are created by fine-tuning open-access LLMs with task-specific data (e.g.,
ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning
methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly
one of the most attractive topics, as it only requires fine-tuning a few
external parameters instead of the entire LLMs while achieving comparable or
even better performance. To enable further research on PEFT methods of LLMs,
this paper presents LLM-Adapters, an easy-to-use framework that integrates
various adapters into LLMs and can execute these adapter-based PEFT methods of
LLMs for different tasks. The framework includes state-of-the-art open-access
LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such
as Series adapter, Parallel adapter, and LoRA. The framework is designed to be
research-friendly, efficient, modular, and extendable, allowing the integration
of new adapters and the evaluation of them with new and larger-scale LLMs.
Furthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we
conduct experiments on six math reasoning datasets. The results demonstrate
that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra
trainable parameters yields comparable, and in some cases superior, performance
to that of powerful LLMs (175B) in zero-shot inference on simple math reasoning
datasets. Overall, we provide a promising framework for fine-tuning large LLMs
on downstream tasks. We believe the proposed LLMs-Adapters will advance
adapter-based PEFT research, facilitate the deployment of research pipelines,
and enable practical applications to real-world systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01938">
<div class="article-summary-box-inner">
<span><p>We present the first study to investigate Large Language Models (LLMs) in
answering radiation oncology physics questions. Because popular exams like AP
Physics, LSAT, and GRE have large test-taker populations and ample test
preparation resources in circulation, they may not allow for accurately
assessing the true potential of LLMs. This paper proposes evaluating LLMs on a
highly-specialized topic, radiation oncology physics, which may be more
pertinent to scientific and medical communities in addition to being a valuable
benchmark of LLMs. We developed an exam consisting of 100 radiation oncology
physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT
(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against
medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs
as well as medical physicists, on average. The performance of ChatGPT (GPT-4)
was further improved when prompted to explain first, then answer. ChatGPT
(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices
across a number of trials, whether correct or incorrect, a characteristic that
was not observed in the human test groups. In evaluating ChatGPTs (GPT-4)
deductive reasoning ability using a novel approach (substituting the correct
answer with "None of the above choices is the correct answer."), ChatGPT
(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of
an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,
its intrinsic properties did not allow for further improvement when scoring
based on a majority vote across trials. In contrast, a team of medical
physicists were able to greatly outperform ChatGPT (GPT-4) using a majority
vote. This study suggests a great potential for LLMs to work alongside
radiation oncology experts as highly knowledgeable assistants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01961">
<div class="article-summary-box-inner">
<span><p>This paper presents the AToMiC (Authoring Tools for Multimedia Content)
dataset, designed to advance research in image/text cross-modal retrieval.
While vision-language pretrained transformers have led to significant
improvements in retrieval effectiveness, existing research has relied on
image-caption datasets that feature only simplistic image-text relationships
and underspecified user models of retrieval tasks. To address the gap between
these oversimplified settings and real-world applications for multimedia
content creation, we introduce a new approach for building retrieval test
collections. We leverage hierarchical structures and diverse domains of texts,
styles, and types of images, as well as large-scale image-document associations
embedded in Wikipedia. We formulate two tasks based on a realistic user model
and validate our dataset through retrieval experiments using baseline models.
AToMiC offers a testbed for scalable, diverse, and reproducible multimedia
retrieval research. Finally, the dataset provides the basis for a dedicated
track at the 2023 Text Retrieval Conference (TREC), and is publicly available
at https://github.com/TREC-AToMiC/AToMiC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01969">
<div class="article-summary-box-inner">
<span><p>Text classification typically requires a substantial amount of
human-annotated data to serve as supervision, which is costly to obtain in
dynamic emerging domains. Certain methods seek to address this problem by
solely relying on the surface text of class names to serve as extremely weak
supervision. However, existing methods fail to account for single-class
documents discussing multiple topics. Both topic diversity and vague sentences
may introduce noise into the document's underlying representation and
consequently the precision of the predicted class. Furthermore, current work
focuses on text granularities (documents, sentences, or words) independently,
which limits the degree of coarse- or fine-grained context that we can jointly
extract from all three to identify significant subtext for classification. In
order to address this problem, we propose MEGClass, an extremely
weakly-supervised text classification method to exploit Mutually-Enhancing Text
Granularities. Specifically, MEGClass constructs class-oriented sentence and
class representations based on keywords for performing a sentence-level
confidence-weighted label ensemble in order to estimate a document's initial
class distribution. This serves as the target distribution for a multi-head
attention network with a class-weighted contrastive loss. This network learns
contextualized sentence representations and weights to form document
representations that reflect its original document and sentence-level topic
diversity. Retaining this heterogeneity allows MEGClass to select the most
class-indicative documents to serve as iterative feedback for enhancing the
class representations. Finally, these top documents are used to fine-tune a
pre-trained text classifier. As demonstrated through extensive experiments on
six benchmark datasets, MEGClass outperforms other weakly and extremely weakly
supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue-Contextualized Re-ranking for Medical History-Taking. (arXiv:2304.01974v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01974">
<div class="article-summary-box-inner">
<span><p>AI-driven medical history-taking is an important component in symptom
checking, automated patient intake, triage, and other AI virtual care
applications. As history-taking is extremely varied, machine learning models
require a significant amount of data to train. To overcome this challenge,
existing systems are developed using indirect data or expert knowledge. This
leads to a training-inference gap as models are trained on different kinds of
data than what they observe at inference time. In this work, we present a
two-stage re-ranking approach that helps close the training-inference gap by
re-ranking the first-stage question candidates using a dialogue-contextualized
model. For this, we propose a new model, global re-ranker, which cross-encodes
the dialogue with all questions simultaneously, and compare it with several
existing neural baselines. We test both transformer and S4-based language model
backbones. We find that relative to the expert system, the best performance is
achieved by our proposed global re-ranker with a transformer backbone,
resulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%
higher mean average precision (mAP).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. (arXiv:2304.01982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01982">
<div class="article-summary-box-inner">
<span><p>Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]
allow token-level interactions between queries and documents, and hence achieve
state of the art on many information retrieval benchmarks. However, their
non-linear scoring function cannot be scaled to millions of documents,
necessitating a three-stage process for inference: retrieving initial
candidates via token retrieval, accessing all token vectors, and scoring the
initial candidate documents. The non-linear scoring function is applied over
all token vectors of each candidate document, making the inference process
complicated and slow. In this paper, we aim to simplify the multi-vector
retrieval by rethinking the role of token retrieval. We present XTR,
ConteXtualized Token Retriever, which introduces a simple, yet novel, objective
function that encourages the model to retrieve the most important document
tokens first. The improvement to token retrieval allows XTR to rank candidates
only using the retrieved tokens rather than all tokens in the document, and
enables a newly designed scoring stage that is two-to-three orders of magnitude
cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the
state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis
confirms our decision to revisit the token retrieval stage, as XTR demonstrates
much better recall of the token retrieval stage compared to ColBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15949">
<div class="article-summary-box-inner">
<span><p>Transformer networks have revolutionized NLP representation learning since
they were introduced. Though a great effort has been made to explain the
representation in transformers, it is widely recognized that our understanding
is not sufficient. One important reason is that there lack enough visualization
tools for detailed analysis. In this paper, we propose to use dictionary
learning to open up these "black boxes" as linear superpositions of transformer
factors. Through visualization, we demonstrate the hierarchical semantic
structures captured by the transformer factors, e.g., word-level polysemy
disambiguation, sentence-level pattern formation, and long-range dependency.
While some of these patterns confirm the conventional prior linguistic
knowledge, the rest are relatively unexpected, which may provide new insights.
We hope this visualization tool can bring further knowledge and a better
understanding of how transformer networks work. The code is available at
https://github.com/zeyuyun1/TransformerVis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00396">
<div class="article-summary-box-inner">
<span><p>Social media has become extremely influential when it comes to policy making
in modern societies, especially in the western world, where platforms such as
Twitter allow users to follow politicians, thus making citizens more involved
in political discussion. In the same vein, politicians use Twitter to express
their opinions, debate among others on current topics and promote their
political agendas aiming to influence voter behaviour. In this paper, we
attempt to analyse tweets of politicians from three European countries and
explore the virality of their tweets. Previous studies have shown that tweets
conveying negative sentiment are likely to be retweeted more frequently. By
utilising state-of-the-art pre-trained language models, we performed sentiment
analysis on hundreds of thousands of tweets collected from members of
parliament in Greece, Spain and the United Kingdom, including devolved
administrations. We achieved this by systematically exploring and analysing the
differences between influential and less popular tweets. Our analysis indicates
that politicians' negatively charged tweets spread more widely, especially in
more recent times, and highlights interesting differences between political
parties as well as between politicians and the general population.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05711">
<div class="article-summary-box-inner">
<span><p>Despite recent advances of AI, story understanding remains an open and
under-investigated problem. We collect, preprocess, and publicly release a
video-language story dataset, Synopses of Movie Narratives (SYMON), containing
5,193 video summaries of popular movies and TV series. SYMON captures
naturalistic story-telling videos for human audience made by human creators. As
a prototypical and naturalistic story dataset, SYMON features high coverage of
multimodal story events, abundant mental-state descriptions, and large semantic
gaps between the visual and the textual modalities. We establish benchmarks on
video-text retrieval and zero-shot alignment on movie summary videos, which
showcase the importance of in-domain data in story understanding. With SYMON,
we hope to lay the groundwork for progress in multimodal story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation. (arXiv:2205.12649v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12649">
<div class="article-summary-box-inner">
<span><p>Data sparsity is a main problem hindering the development of code-switching
(CS) NLP systems. In this paper, we investigate data augmentation techniques
for synthesizing dialectal Arabic-English CS text. We perform lexical
replacements using word-aligned parallel corpora where CS points are either
randomly chosen or learnt using a sequence-to-sequence model. We compare these
approaches against dictionary-based replacements. We assess the quality of the
generated sentences through human evaluation and evaluate the effectiveness of
data augmentation on machine translation (MT), automatic speech recognition
(ASR), and speech translation (ST) tasks. Results show that using a predictive
model results in more natural CS sentences compared to the random approach, as
reported in human judgements. In the downstream tasks, despite the random
approach generating more data, both approaches perform equally (outperforming
dictionary-based replacements). Overall, data augmentation achieves 34%
improvement in perplexity, 5.2% relative improvement on WER for ASR task,
+4.0-5.1 BLEU points on MT task, and +2.1-2.2 BLEU points on ST over a baseline
trained on available data without augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07316">
<div class="article-summary-box-inner">
<span><p>Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.02821">
<div class="article-summary-box-inner">
<span><p>We propose a two-stage approach for training a single NMT model to translate
unseen languages both to and from English. For the first stage, we initialize
an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform
multilingual fine-tuning on parallel data in 40 languages to English. We find
this model can generalize to zero-shot translations on unseen languages. For
the second stage, we leverage this generalization ability to generate synthetic
parallel data from monolingual datasets, then bidirectionally train with
successive rounds of back-translation.
</p>
<p>Our approach, which we EcXTra (English-centric Crosslingual (X) Transfer), is
conceptually simple, only using a standard cross-entropy objective throughout.
It is also data-driven, sequentially leveraging auxiliary parallel data and
monolingual data. We evaluate unsupervised NMT results for 7 low-resource
languages, and find that each round of back-translation training further
refines bidirectional performance. Our final single EcXTra-trained model
achieves competitive translation performance in all translation directions,
notably establishing a new state-of-the-art for English-to-Kazakh (22.9 &gt; 10.4
BLEU). Our code is available at https://github.com/manestay/EcXTra .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05135">
<div class="article-summary-box-inner">
<span><p>Learning fine-grained movements is a challenging topic in robotics,
particularly in the context of robotic hands. One specific instance of this
challenge is the acquisition of fingerspelling sign language in robots. In this
paper, we propose an approach for learning dexterous motor imitation from video
examples without additional information. To achieve this, we first build a URDF
model of a robotic hand with a single actuator for each joint. We then leverage
pre-trained deep vision models to extract the 3D pose of the hand from RGB
videos. Next, using state-of-the-art reinforcement learning algorithms for
motion imitation (namely, proximal policy optimization and soft actor-critic),
we train a policy to reproduce the movement extracted from the demonstrations.
We identify the optimal set of hyperparameters for imitation based on a
reference motion. Finally, we demonstrate the generalizability of our approach
by testing it on six different tasks, corresponding to fingerspelled letters.
Our results show that our approach is able to successfully imitate these
fine-grained movements without additional information, highlighting its
potential for real-world applications in robotics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15088">
<div class="article-summary-box-inner">
<span><p>Persona-based dialogue systems aim to generate consistent responses based on
historical context and predefined persona. Unlike conventional dialogue
generation, the persona-based dialogue needs to consider both dialogue context
and persona, posing a challenge for coherent training. Specifically, this
requires a delicate weight balance between context and persona. To achieve
that, in this paper, we propose an effective framework with Persona-Adaptive
Attention (PAA), which adaptively integrates the weights from the persona and
context information via our designed attention. In addition, a dynamic masking
mechanism is applied to the PAA to not only drop redundant information in
context and persona but also serve as a regularization mechanism to avoid
overfitting. Experimental results demonstrate the superiority of the proposed
PAA framework compared to the strong baselines in both automatic and human
evaluation. Moreover, the proposed PAA approach can perform equivalently well
in a low-resource regime compared to models trained in a full-data setting,
which achieve a similar result with only 20% to 30% of data compared to the
larger models trained in the full-data setting. To fully exploit the
effectiveness of our design, we designed several variants for handling the
weighted information in different ways, showing the necessity and sufficiency
of our weighting and masking designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01515">
<div class="article-summary-box-inner">
<span><p>Predicting personality traits based on online posts has emerged as an
important task in many fields such as social network analysis. One of the
challenges of this task is assembling information from various posts into an
overall profile for each user. While many previous solutions simply concatenate
the posts into a long document and then encode the document by sequential or
hierarchical models, they introduce unwarranted orders for the posts, which may
mislead the models. In this paper, we propose a dynamic deep graph
convolutional network (D-DGCN) to overcome the above limitation. Specifically,
we design a learn-to-connect approach that adopts a dynamic multi-hop structure
instead of a deterministic structure, and combine it with a DGCN module to
automatically learn the connections between posts. The modules of post encoder,
learn-to-connect, and DGCN are jointly trained in an end-to-end manner.
Experimental results on the Kaggle and Pandora datasets show the superior
performance of D-DGCN to state-of-the-art baselines. Our code is available at
https://github.com/djz233/D-DGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04054">
<div class="article-summary-box-inner">
<span><p>Given a piece of text, a video clip and a reference audio, the movie dubbing
(also known as visual voice clone V2C) task aims to generate speeches that
match the speaker's emotion presented in the video using the desired speaker
voice as reference. V2C is more challenging than conventional text-to-speech
tasks as it additionally requires the generated speech to exactly match the
varying emotions and speaking speed presented in the video. Unlike previous
works, we propose a novel movie dubbing architecture to tackle these problems
via hierarchical prosody modelling, which bridges the visual information to
corresponding speech prosody from three aspects: lip, face, and scene.
Specifically, we align lip movement to the speech duration, and convey facial
expression to speech energy and pitch via attention mechanism based on valence
and arousal representations inspired by recent psychology findings. Moreover,
we design an emotion booster to capture the atmosphere from global video
scenes. All these embeddings together are used to generate mel-spectrogram and
then convert to speech waves via existing vocoder. Extensive experimental
results on the Chem and V2C benchmark datasets demonstrate the favorable
performance of the proposed method. The source code and trained models will be
released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical multimodal transformers for Multi-Page DocVQA. (arXiv:2212.05935v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05935">
<div class="article-summary-box-inner">
<span><p>Document Visual Question Answering (DocVQA) refers to the task of answering
questions from document images. Existing work on DocVQA only considers
single-page documents. However, in real scenarios documents are mostly composed
of multiple pages that should be processed altogether. In this work we extend
DocVQA to the multi-page scenario. For that, we first create a new dataset,
MP-DocVQA, where questions are posed over multi-page documents instead of
single pages. Second, we propose a new hierarchical method, Hi-VT5, based on
the T5 architecture, that overcomes the limitations of current methods to
process long multi-page documents. The proposed method is based on a
hierarchical transformer architecture where the encoder summarizes the most
relevant information of every page and then, the decoder takes this summarized
information to generate the final answer. Through extensive experimentation, we
demonstrate that our method is able, in a single stage, to answer the questions
and provide the page that contains the relevant information to find the answer,
which can be used as a kind of explainability measure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03494">
<div class="article-summary-box-inner">
<span><p>Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02427">
<div class="article-summary-box-inner">
<span><p>We explore the possibility of meta-learning for the language-independent
unsupervised tokenization problem for English, Russian, and Chinese. We
implement the meta-learning approach for automatic determination of
hyper-parameters of the unsupervised tokenization model proposed in earlier
works, relying on various human-independent fitness functions such as
normalised anti-entropy, compression factor and cross-split F1 score, as well
as additive and multiplicative composite combinations of the three metrics,
testing them against the conventional F1 tokenization score. We find a fairly
good correlation between the latter and the additive combination of the former
three metrics for English and Russian. In case of Chinese, we find a
significant correlation between the F 1 score and the compression factor. Our
results suggest the possibility of robust unsupervised tokenization of
low-resource and dead languages and allow us to think about human languages in
terms of the evolution of efficient symbolic communication codes with different
structural optimisation schemes that have evolved in different human cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12582">
<div class="article-summary-box-inner">
<span><p>The advancement of speech technologies has been remarkable, yet its
integration with African languages remains limited due to the scarcity of
African speech corpora. To address this issue, we present AfroDigits, a
minimalist, community-driven dataset of spoken digits for African languages,
currently covering 38 African languages. As a demonstration of the practical
applications of AfroDigits, we conduct audio digit classification experiments
on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo
(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R
models. Our experiments reveal a useful insight on the effect of mixing African
speech corpora during finetuning. AfroDigits is the first published audio digit
dataset for African languages and we believe it will, among other things, pave
the way for Afro-centric speech applications such as the recognition of
telephone numbers, and street numbers. We release the dataset and platform
publicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and
https://huggingface.co/spaces/chrisjay/afro-speech respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14524">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated their significant potential to
be applied for addressing various application tasks. However, traditional
recommender systems continue to face great challenges such as poor
interactivity and explainability, which actually also hinder their broad
deployment in real-world systems. To address these limitations, this paper
proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender
System) that innovatively augments LLMs for building conversational recommender
systems by converting user profiles and historical interactions into prompts.
Chat-Rec is demonstrated to be effective in learning user preferences and
establishing connections between users and products through in-context
learning, which also makes the recommendation process more interactive and
explainable. What's more, within the Chat-Rec framework, user's preferences can
transfer to different products for cross-domain recommendations, and
prompt-based injection of information into LLMs can also handle the cold-start
scenarios with new items. In our experiments, Chat-Rec effectively improve the
results of top-k recommendations and performs better in zero-shot rating
prediction task. Chat-Rec offers a novel approach to improving recommender
systems and presents new practical scenarios for the implementation of AIGC (AI
generated content) in recommender system studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17612">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the range of oBERTa language models, an
easy-to-use set of language models which allows Natural Language Processing
(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without
expertise in model compression. Specifically, oBERTa extends existing work on
pruning, knowledge distillation, and quantization and leverages frozen
embeddings improves distillation and model initialization to deliver higher
accuracy on a broad range of transfer tasks. In generating oBERTa, we explore
how the highly optimized RoBERTa differs from the BERT for pruning during
pre-training and finetuning. We find it less amenable to compression during
fine-tuning. We explore the use of oBERTa on seven representative NLP tasks and
find that the improved compression techniques allow a pruned oBERTa model to
match the performance of BERTbase and exceed the performance of Prune OFA Large
on the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,
respectively faster in inference. We release our code, training regimes, and
associated model for broad usage to encourage usage and experimentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17710">
<div class="article-summary-box-inner">
<span><p>The proliferation of automated conversational systems such as chatbots,
spoken-dialogue systems, and smart speakers, has significantly impacted modern
digital life. However, these systems are primarily designed to provide answers
to well-defined questions rather than to support users in exploring complex,
ill-defined questions. In this paper, we aim to push the boundaries of
conversational systems by examining the types of nebulous, open-ended questions
that can best be answered through conversation. We first sampled 500 questions
from one million open-ended requests posted on AskReddit, and then recruited
online crowd workers to answer eight inquiries about these questions. We also
performed open coding to categorize the questions into 27 different domains. We
found that the issues people believe require conversation to resolve
satisfactorily are highly social and personal. Our work provides insights into
how future research could be geared to align with users' needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v2 [physics.pop-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17853">
<div class="article-summary-box-inner">
<span><p>In what will likely be a litany of generative-model-themed arXiv submissions
celebrating April the 1st, we evaluate the capacity of state-of-the-art
transformer models to create a paper detailing the detection of a Pulsar Wind
Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)
Array. We do this to evaluate the ability of such models to interpret
astronomical observations and sources based on language information alone, and
to assess potential means by which fraudulently generated scientific papers
could be identified during peer review (given that reliable generative model
watermarking has yet to be deployed for these tools). We conclude that our jobs
as astronomers are safe for the time being. From this point on, prompts given
to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT
is shown in black, whereas analysis by the (human) authors is in blue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01196">
<div class="article-summary-box-inner">
<span><p>Chat models, such as ChatGPT, have shown impressive capabilities and have
been rapidly adopted across numerous domains. However, these models are only
accessible through a restricted API, creating barriers for new research and
progress in the field. We propose a pipeline that can automatically generate a
high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a
conversation with itself. Subsequently, we employ parameter-efficient tuning to
enhance LLaMA, an open-source large language model. The resulting model, named
Baize, demonstrates good performance in multi-turn dialogues with guardrails
that minimize potential risks. The Baize models and data are released for
research purposes only at https://github.com/project-baize/baize. An online
demo is also available at
https://huggingface.co/spaces/project-baize/baize-lora-7B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01046">
<div class="article-summary-box-inner">
<span><p>Throughout schooling, students are tested on reading comprehension and
logical reasoning. Students have developed various strategies for completing
such exams, some of which are generally thought to outperform others. One such
strategy involves emphasizing relative accuracy over absolute accuracy and can
theoretically produce the correct answer without full knowledge of the
information required to solve the question. This paper examines the
effectiveness of applying such a strategy to train transfer learning models to
solve reading comprehension and logical reasoning questions. The models were
evaluated on the ReClor dataset, a challenging reading comprehension and
logical reasoning benchmark. While previous studies targeted logical reasoning
skills, we focus on a general training method and model architecture. We
propose the polytuplet loss function, an extension of the triplet loss
function, to ensure prioritization of learning the relative correctness of
answer choices over learning the true accuracy of each choice. Our results
indicate that models employing polytuplet loss outperform existing baseline
models. Although polytuplet loss is a promising alternative to other
contrastive loss functions, further research is required to quantify the
benefits it may present.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-05 23:09:28.309549218 UTC">2023-04-05 23:09:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>