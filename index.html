<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-03T01:30:00Z">10-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00034">
<div class="article-summary-box-inner">
<span><p>This paper explores network binarization, a radical form of quantization,
compressing model weights to a single bit, specifically for Large Language
Models (LLMs) compression. Due to previous binarization methods collapsing
LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can
achieve extreme low-bit quantization while maintaining the linguistic reasoning
capacity of quantized LLMs. Specifically, our exploration first uncovers the
ineffectiveness of naive applications of existing binarization algorithms and
highlights the imperative role of salient weights in achieving low-bit
quantization. Thus, PB-LLM filters a small ratio of salient weights during
binarization, allocating them to higher-bit storage, i.e.,
partially-binarization. PB-LLM is extended to recover the capacities of
quantized LMMs, by analyzing from the perspective of post-training quantization
(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts
from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian
matrix and successfully recover the reasoning capacity of PB-LLM in low-bit.
Under QAT, we freeze the salient weights during training, explore the
derivation of optimal scaling factors crucial for minimizing the quantization
error, and propose a scaling mechanism based on this derived scaling strategy
for residual binarized weights. Those explorations and the developed
methodologies significantly contribute to rejuvenating the performance of
low-bit quantized LLMs and present substantial advancements in the field of
network binarization for LLMs.The code is available at
https://github.com/hahnyuan/BinaryLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00074">
<div class="article-summary-box-inner">
<span><p>To comprehensively assess the capacity of current models for complex
reasoning, it is crucial to assess their step-by-step reasoning in a scalable
manner. Established reference-based evaluation metrics rely on human-annotated
reasoning chains to assess the model-derived chains. However, such
``gold-standard'' human-written reasoning chains may not be unique and their
acquisition is often labor-intensive. Existing reference-free reasoning metrics
eliminate the need for human-crafted reasoning chains as references, but they
typically require fine-tuning on datasets with human-derived reasoning chains,
which complicates the process and raises concerns regarding generalizability
across diverse datasets. To address these challenges, we harness GPT-4 to
automatically evaluate reasoning chain quality, obviating the need for
human-crafted references. Leveraging the Socratic method, we devise tailored
prompts to enhance reference-free reasoning evaluation, which we term SocREval
(Socratic method for Reasoning Evaluation). Empirical results from four human
annotated datasets reveal that SocREval significantly improves GPT-4's
performance, surpassing existing reference-free and reference-based reasoning
evaluation metrics. Beyond its demonstrated efficacy, our proposed framework,
large language models (LLMs) with the Socratic method, proves to be both
cost-efficient and robust to prompt writing and example selection, as
substantiated by our in-depth analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality. (arXiv:2310.00092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00092">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00100">
<div class="article-summary-box-inner">
<span><p>The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning. (arXiv:2310.00141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00141">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) models are typically trained on large
datasets of transcribed speech. As language evolves and new terms come into
use, these models can become outdated and stale. In the context of models
trained on the server but deployed on edge devices, errors may result from the
mismatch between server training data and actual on-device usage. In this work,
we seek to continually learn from on-device user corrections through Federated
Learning (FL) to address this issue. We explore techniques to target fresh
terms that the model has not previously encountered, learn long-tail words, and
mitigate catastrophic forgetting. In experimental evaluations, we find that the
proposed techniques improve model recognition of fresh terms, while preserving
quality on the overall language distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Prompt Rewriting for Personalized Text Generation. (arXiv:2310.00152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00152">
<div class="article-summary-box-inner">
<span><p>Facilitated by large language models (LLMs), personalized text generation has
become a rapidly growing research direction. Most existing studies focus on
designing specialized models for a particular domain, or they require
fine-tuning the LLMs to generate personalized text. We consider a typical
scenario in which the large language model, which generates personalized
output, is frozen and can only be accessed through APIs. Under this constraint,
all one can do is to improve the input text (i.e., text prompts) sent to the
LLM, a procedure that is usually done manually. In this paper, we propose a
novel method to automatically revise prompts for personalized text generation.
The proposed method takes the initial prompts generated by a state-of-the-art,
multistage framework for personalized generation and rewrites a few critical
components that summarize and synthesize the personal context. The prompt
rewriter employs a training paradigm that chains together supervised learning
(SL) and reinforcement learning (RL), where SL reduces the search space of RL
and RL facilitates end-to-end training of the rewriter. Using datasets from
three representative domains, we demonstrate that the rewritten prompts
outperform both the original prompts and the prompts optimized via supervised
learning or reinforcement learning alone. In-depth analysis of the rewritten
prompts shows that they are not only human readable, but also able to guide
manual revision of prompts when there is limited resource to employ
reinforcement learning to train the prompt rewriter, or when it is costly to
deploy an automatic prompt rewriter for inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00160">
<div class="article-summary-box-inner">
<span><p>Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions
through the automatic generation of instructional data using a handful of
human-written seeds. Instead of general alignment, in this work, we focus on
self-alignment for expert domain specialization (e.g., biomedicine),
discovering it to be very effective for improving zero-shot and few-shot
performance in target domains of interest. As a preliminary, we first present
the benchmark results of existing aligned models within a specialized domain,
which reveals the marginal effect that "generic" instruction-following training
has on downstream expert domains' performance. To remedy this, we explore
self-specialization that leverages domain-specific unlabelled data and a few
labeled seeds for the self-alignment process. When augmented with retrieval to
reduce hallucination and enhance concurrency of the alignment,
self-specialization offers an effective (and efficient) way of "carving out" an
expert model out of a "generalist", pre-trained LLM where different domains of
expertise are originally combined in a form of "superposition". Our
experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in
terms of data and parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm. (arXiv:2310.00178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00178">
<div class="article-summary-box-inner">
<span><p>Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Strategies for Modeling Sign Language Phonology. (arXiv:2310.00195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00195">
<div class="article-summary-box-inner">
<span><p>Like speech, signs are composed of discrete, recombinable features called
phonemes. Prior work shows that models which can recognize phonemes are better
at sign recognition, motivating deeper exploration into strategies for modeling
sign language phonemes. In this work, we learn graph convolution networks to
recognize the sixteen phoneme "types" found in ASL-LEX 2.0. Specifically, we
explore how learning strategies like multi-task and curriculum learning can
leverage mutually useful information between phoneme types to facilitate better
modeling of sign language phonemes. Results on the Sem-Lex Benchmark show that
curriculum learning yields an average accuracy of 87% across all phoneme types,
outperforming fine-tuning and multi-task strategies for most phoneme types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes. (arXiv:2310.00196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00196">
<div class="article-summary-box-inner">
<span><p>Sign language recognition and translation technologies have the potential to
increase access and inclusion of deaf signing communities, but research
progress is bottlenecked by a lack of representative data. We introduce a new
resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The
Benchmark is the current largest of its kind, consisting of over 84k videos of
isolated sign productions from deaf ASL signers who gave informed consent and
received compensation. Human experts aligned these videos with other sign
language resources including ASL-LEX, SignBank, and ASL Citizen, enabling
useful expansions for sign and phonological feature recognition. We present a
suite of experiments which make use of the linguistic information in ASL-LEX,
evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated
sign recognition (ISR). We use an SL-GCN model to show that the phonological
features are recognizable with 85% accuracy, and that they are effective as an
auxiliary target to ISR. Learning to recognize phonological features alongside
gloss results in a 6% improvement for few-shot ISR accuracy and a 2%
improvement for ISR accuracy overall. Instructions for downloading the data can
be found at https://github.com/leekezar/SemLex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Pragmatic Differences Between Disciplines. (arXiv:2310.00204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00204">
<div class="article-summary-box-inner">
<span><p>Scholarly documents have a great degree of variation, both in terms of
content (semantics) and structure (pragmatics). Prior work in scholarly
document understanding emphasizes semantics through document summarization and
corpus topic modeling but tends to omit pragmatics such as document
organization and flow. Using a corpus of scholarly documents across 19
disciplines and state-of-the-art language modeling techniques, we learn a fixed
set of domain-agnostic descriptors for document sections and "retrofit" the
corpus to these descriptors (also referred to as "normalization"). Then, we
analyze the position and ordering of these descriptors across documents to
understand the relationship between discipline and structure. We report
within-discipline structural archetypes, variability, and between-discipline
comparisons, supporting the hypothesis that scholarly communities, despite
their size, diversity, and breadth, share similar avenues for expressing their
work. Our findings lay the foundation for future work in assessing research
quality, domain style transfer, and further pragmatic analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Unseen Multiword Expressions in American Sign Language. (arXiv:2310.00207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00207">
<div class="article-summary-box-inner">
<span><p>Multiword expressions present unique challenges in many translation tasks. In
an attempt to ultimately apply a multiword expression detection system to the
translation of American Sign Language, we built and tested two systems that
apply word embeddings from GloVe to determine whether or not the word
embeddings of lexemes can be used to predict whether or not those lexemes
compose a multiword expression. It became apparent that word embeddings carry
data that can detect non-compositionality with decent accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00212">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Difficulty of Training Transformers. (arXiv:2004.08249v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08249">
<div class="article-summary-box-inner">
<span><p>Transformers have proved effective in many NLP tasks. However, their training
requires non-trivial efforts regarding designing cutting-edge optimizers and
learning rate schedulers carefully (e.g., conventional SGD fails to train
Transformers effectively). Our objective here is to understand $\textit{what
complicates Transformer training}$ from both empirical and theoretical
perspectives. Our analysis reveals that unbalanced gradients are not the root
cause of the instability of training. Instead, we identify an amplification
effect that influences training substantially -- for each layer in a
multi-layer Transformer model, heavy dependency on its residual branch makes
training unstable, since it amplifies small parameter perturbations (e.g.,
parameter updates) and results in significant disturbances in the model output.
Yet we observe that a light dependency limits the model potential and leads to
inferior trained models. Inspired by our analysis, we propose Admin
($\textbf{Ad}$aptive $\textbf{m}$odel $\textbf{in}$itialization) to stabilize
stabilize the early stage's training and unleash its full potential in the late
stage. Extensive experiments show that Admin is more stable, converges faster,
and leads to better performance. Implementations are released at:
https://github.com/LiyuanLucasLiu/Transforemr-Clinic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06774">
<div class="article-summary-box-inner">
<span><p>When trained on large-scale datasets, image captioning models can understand
the content of images from a general domain but often fail to generate
accurate, detailed captions. To improve performance, pretraining-and-finetuning
has been a key strategy for image captioning. However, we find that large-scale
bidirectional training between image and text enables zero-shot image
captioning. In this paper, we introduce Bidirectional Image Text Training in
largER Scale, BITTERS, an efficient training and inference framework for
zero-shot image captioning. We also propose a new evaluation benchmark which
comprises of high quality datasets and an extensive set of metrics to properly
evaluate zero-shot captioning accuracy and societal bias. We additionally
provide an efficient finetuning approach for keyword extraction. We show that
careful selection of large-scale training set and model architecture is the key
to achieving zero-shot image captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09110">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are becoming the foundation for almost all major
language technologies, but their capabilities, limitations, and risks are not
well understood. We present Holistic Evaluation of Language Models (HELM) to
improve the transparency of language models. First, we taxonomize the vast
space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)
that are of interest for LMs. Then we select a broad subset based on coverage
and feasibility, noting what's missing or underrepresented (e.g. question
answering for neglected English dialects, metrics for trustworthiness). Second,
we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,
robustness, fairness, bias, toxicity, and efficiency) for each of 16 core
scenarios when possible (87.5% of the time). This ensures metrics beyond
accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We
also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze
specific aspects (e.g. reasoning, disinformation). Third, we conduct a
large-scale evaluation of 30 prominent language models (spanning open,
limited-access, and closed models) on all 42 scenarios, 21 of which were not
previously used in mainstream LM evaluation. Prior to HELM, models on average
were evaluated on just 17.9% of the core HELM scenarios, with some prominent
models not sharing a single scenario in common. We improve this to 96.0%: now
all 30 models have been densely benchmarked on the same core scenarios and
metrics under standardized conditions. Our evaluation surfaces 25 top-level
findings. For full transparency, we release all raw model prompts and
completions publicly for further analysis, as well as a general modular
toolkit. We intend for HELM to be a living benchmark for the community,
continuously updated with new scenarios, metrics, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11259">
<div class="article-summary-box-inner">
<span><p>The generation of molecules with desired properties has gained tremendous
popularity, revolutionizing the way scientists design molecular structures and
providing valuable support for chemical and drug design. However, despite the
potential of language models in molecule generation, they face numerous
challenges such as the generation of syntactically or chemically flawed
molecules, narrow domain focus, and limitations in creating diverse and
directionally feasible molecules due to a dearth of annotated data or external
molecular databases. To tackle these challenges, we introduce MolGen, a
pre-trained molecular language model tailored specifically for molecule
generation. Through the reconstruction of over 100 million molecular SELFIES,
MolGen internalizes profound structural and grammatical insights. This is
further enhanced by domain-agnostic molecular prefix tuning, fostering robust
knowledge transfer across diverse domains. Importantly, our self-feedback
paradigm steers the model away from ``molecular hallucinations'', ensuring
alignment between the model's estimated probabilities and real-world chemical
preferences. Extensive experiments on well-known benchmarks underscore MolGen's
optimization capabilities in properties such as penalized logP, QED, and
molecular docking. Additional analyses affirm its proficiency in accurately
capturing molecule distributions, discerning intricate structural patterns, and
efficiently exploring the chemical space. Code is available at
https://github.com/zjunlp/MolGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map. (arXiv:2302.00456v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00456">
<div class="article-summary-box-inner">
<span><p>Given that Transformers are ubiquitous in wide tasks, interpreting their
internals is a pivotal issue. Still, their particular components, feed-forward
(FF) blocks, have typically been less analyzed despite their substantial
parameter amounts. We analyze the input contextualization effects of FF blocks
by rendering them in the attention maps as a human-friendly visualization
scheme. Our experiments with both masked- and causal-language models reveal
that FF networks modify the input contextualization to emphasize specific types
of linguistic compositions. In addition, FF and its surrounding components tend
to cancel out each other's effects, suggesting potential redundancy in the
processing of the Transformer layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Instability of Fine-Tuning. (arXiv:2302.07778v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07778">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models on downstream tasks with varying
random seeds has been shown to be unstable, especially on small datasets. Many
previous studies have investigated this instability and proposed methods to
mitigate it. However, most studies only used the standard deviation of
performance scores (SD) as their measure, which is a narrow characterization of
instability. In this paper, we analyze SD and six other measures quantifying
instability at different levels of granularity. Moreover, we propose a
systematic framework to evaluate the validity of these measures. Finally, we
analyze the consistency and difference between different measures by
reassessing existing instability mitigation methods. We hope our results will
inform the development of better measurements of fine-tuning instability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14838">
<div class="article-summary-box-inner">
<span><p>Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models. (arXiv:2303.03124v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.03124">
<div class="article-summary-box-inner">
<span><p>Interpretability and human oversight are fundamental pillars of deploying
complex NLP models into real-world applications. However, applying
explainability and human-in-the-loop methods requires technical proficiency.
Despite existing toolkits for model understanding and analysis, options to
integrate human feedback are still limited. We propose IFAN, a framework for
real-time explanation-based interaction with NLP models. Through IFAN's
interface, users can provide feedback to selected model explanations, which is
then integrated through adapter layers to align the model with human rationale.
We show the system to be effective in debiasing a hate speech classifier with
minimal impact on performance. IFAN also offers a visual admin system and API
to manage models (and datasets) as well as control access rights. A demo is
live at https://ifan.ml.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes. (arXiv:2303.09892v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09892">
<div class="article-summary-box-inner">
<span><p>Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03897">
<div class="article-summary-box-inner">
<span><p>The internet gives the world an open platform to express their views and
share their stories. While this is very valuable, it makes fake news one of our
society's most pressing problems. Manual fact checking process is time
consuming, which makes it challenging to disprove misleading assertions before
they cause significant harm. This is he driving interest in automatic fact or
claim verification. Some of the existing datasets aim to support development of
automating fact-checking techniques, however, most of them are text based.
Multi-modal fact verification has received relatively scant attention. In this
paper, we provide a multi-modal fact-checking dataset called FACTIFY 2,
improving Factify 1 by using new data sources and adding satire articles.
Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three
broad categories - support, no-evidence, and refute, with sub-categories based
on the entailment of visual and textual data. We also provide a BERT and Vison
Transformer based baseline, which achieves 65% F1 score in the test set. The
baseline codes and the dataset will be made available at
https://github.com/surya1701/Factify-2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Memory Capacity of ChatGPT: An Empirical Study. (arXiv:2305.03731v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03731">
<div class="article-summary-box-inner">
<span><p>Working memory is a critical aspect of both human intelligence and artificial
intelligence, serving as a workspace for the temporary storage and manipulation
of information. In this paper, we systematically assess the working memory
capacity of ChatGPT, a large language model developed by OpenAI, by examining
its performance in verbal and spatial n-back tasks under various conditions.
Our experiments reveal that ChatGPT has a working memory capacity limit
strikingly similar to that of humans. Furthermore, we investigate the impact of
different instruction strategies on ChatGPT's performance and observe that the
fundamental patterns of a capacity limit persist. From our empirical findings,
we propose that n-back tasks may serve as tools for benchmarking the working
memory capacity of large language models and hold potential for informing
future efforts aimed at enhancing AI working memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v5 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09770">
<div class="article-summary-box-inner">
<span><p>Despite a surge collection of XAI methods, users still struggle to obtain
required AI explanations. Previous research suggests chatbots as dynamic
solutions, but the effective design of conversational XAI agents for practical
human needs remains under-explored. This paper focuses on Conversational XAI
for AI-assisted scientific writing tasks. Drawing from human linguistic
theories and formative studies, we identify four design rationales:
"multifaceted", "controllability", "mix-initiative", "context-aware
drill-down". We incorporate them into an interactive prototype, ConvXAI, which
facilitates heterogeneous AI explanations for scientific writing through
dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based
baseline on improving human-perceived understanding and writing improvement.
The paper further discusses the practical human usage patterns in interacting
with ConvXAI for scientific co-writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10865">
<div class="article-summary-box-inner">
<span><p>The difficulty of appropriately assigning credit is particularly heightened
in cooperative MARL with sparse reward, due to the concurrent time and
structural scales involved. Automatic subgoal generation (ASG) has recently
emerged as a viable MARL approach inspired by utilizing subgoals in
intrinsically motivated reinforcement learning. However, end-to-end learning of
complex task planning from sparse rewards without prior knowledge, undoubtedly
requires massive training samples. Moreover, the diversity-promoting nature of
existing ASG methods can lead to the "over-representation" of subgoals,
generating numerous spurious subgoals of limited relevance to the actual task
reward and thus decreasing the sample efficiency of the algorithm. To address
this problem and inspired by the disentangled representation learning, we
propose a novel "disentangled" decision-making method, Semantically Aligned
task decomposition in MARL (SAMA), that prompts pretrained language models with
chain-of-thought that can suggest potential goals, provide suitable goal
decomposition and subgoal allocation as well as self-reflection-based
replanning. Additionally, SAMA incorporates language-grounded RL to train each
agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages
in sample efficiency compared to state-of-the-art ASG methods, as evidenced by
its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11499">
<div class="article-summary-box-inner">
<span><p>Large language Models (LLMs) have achieved promising performance on
arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting. However, LLMs face challenges in maintaining factual consistency
during reasoning, exhibiting tendencies to condition overlooking, question
misinterpretation, and condition hallucination over given problems. Existing
methods use coarse-grained feedback (e.g., whether the answer is correct) to
improve factual consistency. In this work, we propose RCoT (Reversing
Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by
automatically detecting and rectifying factual inconsistency in LLMs, generated
solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct
the problem based on generated solutions. Then fine-grained comparisons between
the original problem and the reconstructed problem expose the factual
inconsistency in the original solutions. To rectify the solution, RCoT
formulates detected factual inconsistency into fine-grained feedback to guide
LLMs in revising solutions. Experimental results demonstrate improvements of
RCoT over standard CoT, Self-Consistency and Self-Refine across seven
arithmetic datasets. Moreover, we find that manually written fine-grained
feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT
reaches 94.6% accuracy on GSM8K), encouraging the community to further explore
the fine-grained feedback generation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11738">
<div class="article-summary-box-inner">
<span><p>Recent developments in large language models (LLMs) have been impressive.
However, these models sometimes show inconsistencies and problematic behavior,
such as hallucinating facts, generating flawed code, or creating offensive and
toxic content. Unlike these models, humans typically utilize external tools to
cross-check and refine their initial content, like using a search engine for
fact-checking, or a code interpreter for debugging. Inspired by this
observation, we introduce a framework called CRITIC that allows LLMs, which are
essentially "black boxes" to validate and progressively amend their own outputs
in a manner similar to human interaction with tools. More specifically,
starting with an initial output, CRITIC interacts with appropriate tools to
evaluate certain aspects of the text, and then revises the output based on the
feedback obtained during this validation process. Comprehensive evaluations
involving free-form question answering, mathematical program synthesis, and
toxicity reduction demonstrate that CRITIC consistently enhances the
performance of LLMs. Meanwhile, our research highlights the crucial importance
of external feedback in promoting the ongoing self-improvement of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13330">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that it is possible to train an $\textit{unsupervised}$
automatic speech recognition (ASR) system using only unpaired audio and text.
Existing unsupervised ASR methods assume that no labeled data can be used for
training. We argue that even if one does not have any labeled audio for a given
language, there is $\textit{always}$ labeled data available for other
languages. We show that it is possible to use character-level acoustic models
(AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new
language. Here, "unsupervised" means no labeled audio is available for the
$\textit{target}$ language. Our approach is based on two key ingredients: (i)
generating pseudo-labels (PLs) of the $\textit{target}$ language using some
$\textit{other}$ language AM and (ii) constraining these PLs with a
$\textit{target language model}$. Our approach is effective on Common Voice:
e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms
character-based wav2vec-U 2.0 by 15% absolute WER on LJSpeech with 800h of
labeled German data instead of 60k hours of unlabeled English data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14279">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved widespread success on a variety of
in-context few-shot tasks, but this success is typically evaluated via
correctness rather than consistency. We argue that self-consistency is an
important criteria for valid multi-step reasoning in tasks where the solution
is composed of the answers to multiple sub-steps. We propose two types of
self-consistency that are particularly important for multi-step reasoning --
hypothetical consistency (a model's ability to predict what its output would be
in a hypothetical other context) and compositional consistency (consistency of
a model's final outputs when intermediate sub-steps are replaced with the
model's outputs for those steps). We demonstrate that multiple variants of the
GPT-3/-4 models exhibit poor consistency rates across both types of consistency
on a variety of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. (arXiv:2305.15090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15090">
<div class="article-summary-box-inner">
<span><p>Information extraction tasks such as event extraction require an in-depth
understanding of the output structure and sub-task dependencies. They heavily
rely on task-specific training data in the form of (passage, target structure)
pairs to obtain reasonable performance. However, obtaining such data through
human annotation is costly, leading to a pressing need for low-resource
information extraction approaches that require minimal human labeling for
real-world applications. Fine-tuning supervised models with synthesized
training data would be a generalizable method, but the existing data generation
methods either still rely on large-scale ground-truth data or cannot be applied
to complicated IE tasks due to their poor performance. To address these
challenges, we propose STAR, a data generation method that leverages Large
Language Models (LLMs) to synthesize data instances given limited seed
demonstrations, thereby boosting low-resource information extraction
performance. Our approach involves generating target structures (Y) followed by
generating passages (X), all accomplished with the aid of LLMs. We design
fine-grained step-by-step instructions to obtain the initial data instances. We
further reduce errors and improve data quality through self-reflection error
identification and self-refinement with iterative revision. Our experiments
show that the data generated by STAR significantly improves the performance of
low-resource event extraction and relation extraction tasks, even surpassing
the effectiveness of human-curated data. Human assessment of the data quality
shows STAR-generated data exhibits higher passage quality and better align with
the task definitions compared with the human-curated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15852">
<div class="article-summary-box-inner">
<span><p>Large language models (large LMs) are susceptible to producing text that
contains hallucinated content. An important instance of this problem is
self-contradiction, where the LM generates two contradictory sentences within
the same context. In this work, we present a comprehensive investigation into
self-contradiction for various instruction-tuned LMs, covering evaluation,
detection, and mitigation. Our analysis reveals the prevalence of
self-contradictions when LMs generate text for open-domain topics, e.g., in
17.7% of all sentences produced by ChatGPT. Self-contradiction also complements
retrieval-based methods, as a large portion of them (e.g., 35.8% for ChatGPT)
cannot be verified using Wikipedia. We then propose a novel prompting-based
framework designed to effectively detect and mitigate self-contradictions. Our
detector achieves high accuracy, e.g., around 80% F1 score when prompting
ChatGPT. The mitigation algorithm iteratively refines the generated text to
remove contradictory information while preserving text fluency and
informativeness. Importantly, our entire framework is applicable to black-box
LMs and does not require external grounded knowledge. Our approach is
practically effective and has been released as a push-button tool to benefit
the public, available at https://chatprotect.ai/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Passive learning of active causal strategies in agents and language models. (arXiv:2305.16183v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16183">
<div class="article-summary-box-inner">
<span><p>What can be learned about causality and experimentation from passive data?
This question is salient given recent successes of passively-trained language
models in interactive domains such as tool use. Passive learning is inherently
limited. However, we show that purely passive learning can in fact allow an
agent to learn generalizable strategies for determining and using causal
structures, as long as the agent can intervene at test time. We formally
illustrate that learning a strategy of first experimenting, then seeking goals,
can allow generalization from passive learning in principle. We then show
empirically that agents trained via imitation on expert data can indeed
generalize at test time to infer and use causal links which are never present
in the training data; these agents can also generalize experimentation
strategies to novel variable sets never observed in training. We then show that
strategies for causal intervention and exploitation can be generalized from
passive data even in a more complex environment with high-dimensional
observations, with the support of natural language explanations. Explanations
can even allow passive learners to generalize out-of-distribution from
perfectly-confounded training data. Finally, we show that language models,
trained only on passive next-word prediction, can generalize causal
intervention strategies from a few-shot prompt containing examples of
experimentation, together with explanations and reasoning. These results
highlight the surprising power of passive learning of active causal strategies,
and may help to understand the behaviors and capabilities of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stable Anisotropic Regularization. (arXiv:2305.19358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19358">
<div class="article-summary-box-inner">
<span><p>Given the success of Large Language Models (LLMs), there has been
considerable interest in studying the properties of model activations. The
literature overwhelmingly agrees that LLM representations are dominated by a
few ``outlier dimensions'' with exceedingly high variance and magnitude.
Several studies in Natural Language Processing (NLP) have sought to mitigate
the impact of such outlier dimensions and force LLMs to be isotropic (i.e.,
have uniform variance across all dimensions in embedding space). Isotropy is
thought to be a desirable property for LLMs that improves model performance and
more closely aligns textual representations with human intuition. However, many
of the claims regarding isotropy in NLP have been based on the average cosine
similarity of embeddings, which has recently been shown to be a flawed measure
of isotropy. In this paper, we propose I-STAR: IsoScore*-based STable
Anisotropic Regularization, a novel regularization method that can be used to
increase or decrease levels of isotropy in embedding space during training.
I-STAR uses IsoScore*, the first accurate measure of isotropy that is both
differentiable and stable on mini-batch computations. In contrast to several
previous works, we find that decreasing isotropy in contextualized embeddings
improves performance on the majority of tasks and models considered in this
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05079">
<div class="article-summary-box-inner">
<span><p>In this work, we present a method to add perturbations to the code
descriptions to create new inputs in natural language (NL) from
well-intentioned developers that diverge from the original ones due to the use
of new words or because they miss part of them. The goal is to analyze how and
to what extent perturbations affect the performance of AI code generators in
the context of security-oriented code. First, we show that perturbed
descriptions preserve the semantics of the original, non-perturbed ones. Then,
we use the method to assess the robustness of three state-of-the-art code
generators against the newly perturbed inputs, showing that the performance of
these AI-based solutions is highly affected by perturbations in the NL
descriptions. To enhance their robustness, we use the method to perform data
augmentation, i.e., to increase the variability and diversity of the NL
descriptions in the training data, proving its effectiveness against both
perturbed and non-perturbed code descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08018">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), with their remarkable task-handling
capabilities and innovative outputs, have catalyzed significant advancements
across a spectrum of fields. However, their proficiency within specialized
domains such as biomolecular studies remains limited. To address this
challenge, we introduce Mol-Instructions, a comprehensive instruction dataset
designed for the biomolecular domain. Mol-Instructions encompasses three key
components: molecule-oriented instructions, protein-oriented instructions, and
biomolecular text instructions. Each component aims to improve the
understanding and prediction capabilities of LLMs concerning biomolecular
features and behaviors. Through extensive instruction tuning experiments on
LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large
models' performance in the intricate realm of biomolecular studies, thus
fostering progress in the biomolecular research community. Mol-Instructions is
publicly available for ongoing research and will undergo regular updates to
enhance its applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textbooks Are All You Need. (arXiv:2306.11644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11644">
<div class="article-summary-box-inner">
<span><p>We introduce phi-1, a new large language model for code, with significantly
smaller size than competing models: phi-1 is a Transformer-based model with
1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook
quality" data from the web (6B tokens) and synthetically generated textbooks
and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains
pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays
surprising emergent properties compared to phi-1-base, our model before our
finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller
model with 350M parameters trained with the same pipeline as phi-1 that still
achieves 45% on HumanEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03354">
<div class="article-summary-box-inner">
<span><p>In real-world applications, users often require both translations and
transcriptions of speech to enhance their comprehension, particularly in
streaming scenarios where incremental generation is necessary. This paper
introduces a streaming Transformer-Transducer that jointly generates automatic
speech recognition (ASR) and speech translation (ST) outputs using a single
decoder. To produce ASR and ST content effectively with minimal latency, we
propose a joint token-level serialized output training method that interleaves
source and target words by leveraging an off-the-shelf textual aligner.
Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings
demonstrate that our approach achieves the best quality-latency balance. With
an average ASR latency of 1s and ST latency of 1.3s, our model shows no
degradation or even improves output quality compared to separate ASR and ST
models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the
multilingual case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03917">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved remarkable success in the field of
natural language processing, enabling better human-computer interaction using
natural language. However, the seamless integration of speech signals into LLMs
has not been explored well. The "decoder-only" architecture has also not been
well studied for speech processing tasks. In this research, we introduce
Speech-LLaMA, a novel approach that effectively incorporates acoustic
information into text-based large language models. Our method leverages
Connectionist Temporal Classification and a simple audio encoder to map the
compressed acoustic features to the continuous semantic space of the LLM. In
addition, we further probe the decoder-only architecture for speech-to-text
tasks by training a smaller scale randomly initialized speech-LLaMA model from
speech-text paired data alone. We conduct experiments on multilingual
speech-to-text translation tasks and demonstrate a significant improvement over
strong baselines, highlighting the potential advantages of decoder-only models
for speech-to-text conversion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06930">
<div class="article-summary-box-inner">
<span><p>Modular vision-language models (Vision-LLMs) align pretrained image encoders
with frozen large language models (LLMs), representing a computationally much
more efficient alternative to end-to-end training of large vision-language
models from scratch, which is prohibitively expensive for most researchers and
practitioners. Vision-LLMs instead post-hoc condition LLMs to `understand' the
output of an image encoder. With the abundance of readily available
high-quality English image-text data as well as monolingual English LLMs, the
research focus has been on English-only Vision-LLMs. Multilingual
vision-language models are still predominantly obtained via expensive
end-to-end pretraining, resulting in comparatively smaller models, trained on
limited multilingual image data supplemented with text-only multilingual
corpora. In this work, we present mBLIP, the first multilingual Vision-LLM,
which we obtain in a computationally efficient manner -- on consumer hardware
and using only a few million training examples -- by leveraging a pretrained
multilingual LLM. To this end, we \textit{re-align} an image encoder previously
tuned to an English LLM to a new, multilingual LLM -- for this, we leverage
multilingual data from a mix of vision-and-language tasks, which we obtain by
machine-translating high-quality English data to 95 languages. On the IGLUE
benchmark, mBLIP yields results competitive with state-of-the-art models.
Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms
PaLI-X (a model with 55B parameters). Compared to these very large multilingual
vision-language models trained from scratch, we obtain mBLIP by training orders
of magnitude fewer parameters on magnitudes less data. We release our model and
code at \url{https://github.com/gregor-ge/mBLIP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. (arXiv:2307.07697v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07697">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have achieved significant success in
various tasks, they often struggle with hallucination problems, especially in
scenarios requiring deep and responsible reasoning. These issues could be
partially addressed by introducing external knowledge graphs (KG) in LLM
reasoning. In this paper, we propose a new LLM-KG integrating paradigm
``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to
interactively explore related entities and relations on KGs and perform
reasoning based on the retrieved knowledge. We further implement this paradigm
by introducing a new approach called Think-on-Graph (ToG), in which the LLM
agent iteratively executes beam search on KG, discovers the most promising
reasoning paths, and returns the most likely reasoning results. We use a number
of well-designed experiments to examine and illustrate the following advantages
of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has
the ability of knowledge traceability and knowledge correctability by
leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible
plug-and-play framework for different LLMs, KGs and prompting strategies
without any additional training cost; 4) the performance of ToG with small LLM
models could exceed large LLM such as GPT-4 in certain scenarios and this
reduces the cost of LLM deployment and application. As a training-free method
with lower computational cost and better generality, ToG achieves overall SOTA
in 6 out of 9 datasets where most previous SOTAs rely on additional training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08701">
<div class="article-summary-box-inner">
<span><p>Large language models~(LLMs) strengthen instruction-following capability
through instruction-finetuning (IFT) on supervised instruction/response data.
However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly
contain many low-quality instances with incorrect or irrelevant responses,
which are misleading and detrimental to IFT. In this paper, we propose a simple
and effective data selection strategy that automatically identifies and filters
out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we
introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered
from the 52k Alpaca data. AlpaGasus significantly outperforms the original
Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human
evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM
(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also
provides 5.7x faster training, reducing the training time for a 7B variant from
80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the
efficacy of our method across diverse datasets, base models, and LLM filters.
Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be
generally applied to instruction-tuning data, leading to faster training and
better instruction-following models. Our project page is available at:
\url{https://lichang-chen.github.io/AlpaGasus/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11394">
<div class="article-summary-box-inner">
<span><p>MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12856">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models (LLMs) have recently achieved better
generalization and sample efficiency in autonomous web automation. However, the
performance on real-world websites has still suffered from (1) open domainness,
(2) limited context length, and (3) lack of inductive bias on HTML. We
introduce WebAgent, an LLM-driven agent that learns from self-experience to
complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical
sub-instructions, summarizes long HTML documents into task-relevant snippets,
and acts on websites via Python programs generated from those. We design
WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new
pre-trained LLMs for long HTML documents using local and global attention
mechanisms and a mixture of long-span denoising objectives, for planning and
summarization. We empirically demonstrate that our modular recipe improves the
success on real websites by over 50%, and that HTML-T5 is the best model to
solve various HTML understanding tasks; achieving 18.7% higher success rate
than the prior method on MiniWoB web automation benchmark, and SoTA performance
on Mind2Web, an offline task planning evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01544">
<div class="article-summary-box-inner">
<span><p>Language models demonstrate remarkable capacity to generalize representations
learned in one modality to downstream tasks in other modalities. Can we trace
this ability to individual neurons? We study the case where a frozen text
transformer is augmented with vision using a self-supervised visual encoder and
a single linear projection learned on an image-to-text task. Outputs of the
projection layer are not immediately decodable into language describing image
content; instead, we find that translation between modalities occurs deeper
within the transformer. We introduce a procedure for identifying "multimodal
neurons" that convert visual representations into corresponding text, and
decoding the concepts they inject into the model's residual stream. In a series
of experiments, we show that multimodal neurons operate on specific visual
concepts across inputs, and have a systematic causal effect on image
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08493">
<div class="article-summary-box-inner">
<span><p>Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in measuring LLMs' real effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
at the instance level; using this information, our approach then assesses wider
contamination at the partition level. To estimate contamination of individual
instances, we employ "guided instruction:" a prompt consisting of the dataset
name, partition type, and the random-length initial segment of a reference
instance, asking the LLM to complete it. An instance is flagged as contaminated
if the LLM's output either exactly or nearly matches the latter segment of the
reference. To understand if an entire partition is contaminated, we propose two
ideas. The first idea marks a dataset partition as contaminated if the average
overlap score with the reference instances (as measured by ROUGE-L or BLEURT)
is statistically significantly better with the completions from guided
instruction compared to a "general instruction" that does not include the
dataset and partition name. The second idea marks a dataset partition as
contaminated if a classifier based on GPT-4 with few-shot in-context learning
prompt marks multiple generated completions as exact/near-exact matches of the
corresponding reference instances. Our best method achieves an accuracy between
92% and 100% in detecting if an LLM is contaminated with seven datasets,
containing train and test/validation partitions, when contrasted with manual
evaluation by human experts. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10819">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown remarkable proficiency in following
instructions, making them valuable in customer-facing applications. However,
their impressive capabilities also raise concerns about the amplification of
risks posed by adversarial instructions, which can be injected into the model
input by third-party attackers to manipulate LLMs' original instructions and
prompt unintended actions and content. Therefore, it is crucial to understand
LLMs' ability to accurately discern which instructions to follow to ensure
their safe deployment in real-world scenarios. In this paper, we propose a
pioneering benchmark for automatically evaluating the robustness of
instruction-following LLMs against adversarial instructions injected in the
prompt. The objective of this benchmark is to quantify the extent to which LLMs
are influenced by injected adversarial instructions and assess their ability to
differentiate between these injected adversarial instructions and original user
instructions. Through experiments conducted with state-of-the-art
instruction-following LLMs, we uncover significant limitations in their
robustness against adversarial instruction injection attacks. Furthermore, our
findings indicate that prevalent instruction-tuned models are prone to being
``overfitted'' to follow any instruction phrase in the prompt without truly
understanding which instructions should be followed. This highlights the need
to address the challenge of training models to comprehend prompts instead of
merely following instruction phrases and completing the text. The data and code
can be found at \url{https://github.com/Leezekun/Adv-Instruct-Eval}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12030">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) like ChatGPT and GPT-4 have attracted great
attention given their surprising performance on a wide range of NLP tasks.
Length controlled generation of LLMs emerges as an important topic, which
enables users to fully leverage the capability of LLMs in more real-world
scenarios like generating a proper answer or essay of a desired length. In
addition, the autoregressive generation in LLMs is extremely time-consuming,
while the ability of controlling this generated length can reduce the inference
cost by limiting the length. Therefore, we propose a prompt-based length
control method to achieve high-accuracy length controlled generation. In
particular, we adopt reinforcement learning with the reward signal given by
either trainable or rule-based reward models, which further enhances the
length-control ability of LLMs by rewarding outputs that follows pre-defined
control instruction. To enable rule-based inference, we also introduce standard
prompt extractor to collect the standard control information from users' input.
Experiments show that our method significantly improves the accuracy of
prompt-based length control for summarization task on popular datasets like
CNNDM and NYT. Both the standard prompt extractor and the RL-tuned model have
show strong generalization ability to unseen control prompt templates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16458">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks and to be appropriately specialized to
particular domains. Bioinformatics provides an important domain. In this field
generating functional programs poses additional notable challenges due to the
amount of specialized domain knowledge, the need for complicated data
operations, and intricate functional dependencies between the operations. Here,
we present BioCoder, a benchmark developed to evaluate existing pre-trained
models in generating bioinformatics code. In relation to function-code
generation, BioCoder covers potential package dependencies, class declarations,
and global variables. It incorporates 1026 functions and 1243 methods in Python
and Java from GitHub and 253 examples from the Rosalind Project. BioCoder
incorporates a fuzz-testing framework for evaluation, and we have applied it to
evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder,
StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. The results
highlight two key aspects of successful models: 1) that they contain specific
domain knowledge of bioinformatics (beyond just coding knowledge); 2) that they
accommodate a long prompt with full context (i.e. functional dependencies). Our
dataset, benchmark, Docker images, and scripts required for testing are all
available at https://github.com/gersteinlab/biocoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16463">
<div class="article-summary-box-inner">
<span><p>Large language models exhibit enhanced zero-shot performance on various tasks
when fine-tuned with instruction-following data. Multimodal
instruction-following models extend these capabilities by integrating both text
and images. However, existing models such as MiniGPT-4 face challenges in
maintaining dialogue coherence in scenarios involving multiple images. A
primary reason is the lack of a specialized dataset for this critical
application. To bridge these gaps, we present SparklesChat, a multimodal
instruction-following model for open-ended dialogues across multiple images. To
support the training, we introduce SparklesDialogue, the first
machine-generated dialogue dataset tailored for word-level interleaved
multi-image and text interactions. Furthermore, we construct SparklesEval, a
GPT-assisted benchmark for quantitatively assessing a model's conversational
competence across multiple images and dialogue turns. Our experiments validate
the effectiveness of SparklesChat in understanding and reasoning across
multiple images and dialogue turns. Specifically, SparklesChat outperformed
MiniGPT-4 on established vision-and-language benchmarks, including the BISON
binary image selection task and the NLVR2 visual reasoning task. Moreover,
SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding
MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26. Qualitative
evaluations further demonstrate SparklesChat's generality in handling
real-world applications. All resources are available at
https://github.com/HYPJUDY/Sparkles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging. (arXiv:2309.01026v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01026">
<div class="article-summary-box-inner">
<span><p>We present a method for zero-shot recommendation of multimodal non-stationary
content that leverages recent advancements in the field of generative AI. We
propose rendering inputs of different modalities as textual descriptions and to
utilize pre-trained LLMs to obtain their numerical representations by computing
semantic embeddings. Once unified representations of all content items are
obtained, the recommendation can be performed by computing an appropriate
similarity metric between them without any additional learning. We demonstrate
our approach on a synthetic multimodal nudging environment, where the inputs
consist of tabular, textual, and visual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05494">
<div class="article-summary-box-inner">
<span><p>Social media platforms play an essential role in crisis communication, but
analyzing crisis-related social media texts is challenging due to their
informal nature. Transformer-based pre-trained models like BERT and RoBERTa
have shown success in various NLP tasks, but they are not tailored for
crisis-related texts. Furthermore, general-purpose sentence encoders are used
to generate sentence embeddings, regardless of the textual complexities in
crisis-related texts. Advances in applications like text classification,
semantic search, and clustering contribute to effective processing of
crisis-related texts, which is essential for emergency responders to gain a
comprehensive view of a crisis event, whether historical or real-time. To
address these gaps in crisis informatics literature, this study introduces
CrisisTransformers, an ensemble of pre-trained language models and sentence
encoders trained on an extensive corpus of over 15 billion word tokens from
tweets associated with more than 30 crisis events, including disease outbreaks,
natural disasters, conflicts, and other critical incidents. We evaluate
existing models and CrisisTransformers on 18 crisis-specific public datasets.
Our pre-trained models outperform strong baselines across all datasets in
classification tasks, and our best-performing sentence encoder improves the
state-of-the-art by 17.43% in sentence encoding tasks. Additionally, we
investigate the impact of model initialization on convergence and evaluate the
significance of domain-specific models in generating semantically meaningful
sentence embeddings. All models are publicly released
(https://huggingface.co/crisistransformers), with the anticipation that they
will serve as a robust baseline for tasks involving the analysis of
crisis-related social media texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05653">
<div class="article-summary-box-inner">
<span><p>We introduce MAmmoTH, a series of open-source large language models (LLMs)
specifically tailored for general math problem-solving. The MAmmoTH models are
trained on MathInstruct, our meticulously curated instruction tuning dataset.
MathInstruct is compiled from 13 math datasets with intermediate rationales,
six of which have rationales newly curated by us. It presents a unique hybrid
of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also
ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT
not only unleashes the potential of tool use but also allows different thought
processes for different math problems. As a result, the MAmmoTH series
substantially outperform existing open-source models on nine mathematical
reasoning datasets across all scales with an average accuracy gain between 16%
and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a
competition-level dataset), which exceeds the best open-source 7B model
(WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH,
even surpassing GPT-4's CoT result. Our work underscores the importance of
diverse problem coverage and the use of hybrid rationales in developing
superior math generalist models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07915">
<div class="article-summary-box-inner">
<span><p>Since the resurgence of deep learning, vision-language models (VLMs) enhanced
by large language models (LLMs) have grown exponentially in popularity.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images, making VLMs
less effective in downstream vision-language tasks. In this paper, we address
the limitation above by 1) introducing MMICL, a new approach to allow the VLM
to deal with multi-modal inputs efficiently; 2) proposing a novel context
scheme to augment the in-context learning ability of the VLM; 3) constructing
the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the
VLM's ability to understand complex multi-modal prompts. Our experiments
confirm that MMICL achieves new state-of-the-art zero-shot performance on a
wide range of general vision-language tasks, especially for complex benchmarks,
including MME and MMBench. Our analysis demonstrates that MMICL effectively
tackles the challenge of complex multi-modal prompt understanding and emerges
the impressive ICL ability. Furthermore, we observe that MMICL successfully
alleviates language bias in VLMs, a common issue for VLMs that often leads to
hallucination when faced with extensive textual context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08448">
<div class="article-summary-box-inner">
<span><p>The evaluation of large language models is an essential task in the field of
language understanding and generation. As language models continue to advance,
the need for effective benchmarks to assess their performance has become
imperative. In the context of Traditional Chinese, there is a scarcity of
comprehensive and diverse benchmarks to evaluate the capabilities of language
models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,
and FGC dataset. To address this gap, we propose a novel set of benchmarks that
leverage existing English datasets and are tailored to evaluate language models
in Traditional Chinese. These benchmarks encompass a wide range of tasks,
including contextual question-answering, summarization, classification, and
table understanding. The proposed benchmarks offer a comprehensive evaluation
framework, enabling the assessment of language models' capabilities across
different tasks. In this paper, we evaluate the performance of GPT-3.5,
Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.
The evaluation results highlight that our model, Model 7-C, achieves
performance comparable to GPT-3.5 with respect to a part of the evaluated
capabilities. In an effort to advance the evaluation of language models in
Traditional Chinese and stimulate further research in this field, we have
open-sourced our benchmark and opened the model for trial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch. (arXiv:2309.10706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10706">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with billions of parameters have demonstrated
outstanding performance on various natural language processing tasks. This
report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model,
to contribute an LLM variant to the Chinese-oriented open-source model
community. We enhance OpenBA with effective and efficient techniques as well as
adopt a three-stage training strategy to train the model from scratch. Our
solution can also achieve very competitive performance with only 380B tokens,
which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the
MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides
the main details to pre-train an analogous model, including pre-training data
processing, Bilingual Flan data collection, the empirical observations that
inspire our model architecture design, training objectives of different stages,
and other enhancement techniques. Additionally, we also provide the fine-tuning
details of OpenBA on four downstream tasks. We have refactored our code to
follow the design principles of the Huggingface Transformers Library, making it
more convenient for developers to use, and released checkpoints of different
training stages at https://huggingface.co/openBA. More details of our project
are available at https://github.com/OpenNLG/openBA.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11054">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem
solving. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs,
including the self-describing program, the comment-describing program, and the
non-describing program. Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python and Wolfram Language. Through
extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs
often have superior effectiveness in math problem solving. Notably, the best
performing combination with 30B parameters beats GPT-3.5-turbo by a significant
margin. The results show that self-describing program offers greater diversity
and thus can generally achieve higher performance. We also find that Python is
a better choice of language than Wolfram for program CoTs. The experimental
results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11998">
<div class="article-summary-box-inner">
<span><p>Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
https://huggingface.co/datasets/lmsys/lmsys-chat-1m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12491">
<div class="article-summary-box-inner">
<span><p>We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish "doctora" for "female doctor") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature. (arXiv:2309.13061v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13061">
<div class="article-summary-box-inner">
<span><p>Published biomedical information has and continues to rapidly increase. The
recent advancements in Natural Language Processing (NLP), have generated
considerable interest in automating the extraction, normalization, and
representation of biomedical knowledge about entities such as genes and
diseases. Our study analyzes germline abstracts in the construction of
knowledge graphs of the of the immense work that has been done in this area for
genes and diseases. This paper presents SimpleGermKG, an automatic knowledge
graph construction approach that connects germline genes and diseases. For the
extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model
on biomedical corpora. We propose an ontology-based and rule-based algorithm to
standardize and disambiguate medical terms. For semantic relationships between
articles, genes, and diseases, we implemented a part-whole relation approach to
connect each entity with its data source and visualize them in a graph-based
knowledge representation. Lastly, we discuss the knowledge graph applications,
limitations, and challenges to inspire the future research of germline corpora.
Our knowledge graph contains 297 genes, 130 diseases, and 46,747 triples.
Graph-based visualizations are used to show the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games. (arXiv:2309.13702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13702">
<div class="article-summary-box-inner">
<span><p>In role-playing games a Game Master (GM) is the player in charge of the game,
who must design the challenges the players face and narrate the outcomes of
their actions. In this work we discuss some challenges to model GMs from an
Interactive Storytelling and Natural Language Processing perspective. Following
those challenges we propose three test categories to evaluate such dialogue
systems, and we use them to test ChatGPT, Bard and OpenAssistant as
out-of-the-box GMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. (arXiv:2309.13876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13876">
<div class="article-summary-box-inner">
<span><p>Pre-training speech models on large volumes of data has achieved remarkable
success. OpenAI Whisper is a multilingual multitask model trained on 680k hours
of supervised speech data. It generalizes well to various speech recognition
and translation benchmarks even in a zero-shot setup. However, the full
pipeline for developing such models (from data collection to training) is not
publicly accessible, which makes it difficult for researchers to further
improve its performance and address training-related issues such as efficiency,
robustness, fairness, and bias. This work presents an Open Whisper-style Speech
Model (OWSM), which reproduces Whisper-style training using an open-source
toolkit and publicly available data. OWSM even supports more translation
directions and can be more efficient to train. We will publicly release all
scripts used for data preparation, training, inference, and scoring as well as
pre-trained models and training logs to promote open science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14327">
<div class="article-summary-box-inner">
<span><p>Most of the existing multi-modal models, hindered by their incapacity to
adeptly manage interleaved image-and-text inputs in multi-image, multi-round
dialogues, face substantial constraints in resource allocation for training and
data accessibility, impacting their adaptability and scalability across varied
interaction realms. To address this, we present the DeepSpeed-VisualChat
framework, designed to optimize Large Language Models (LLMs) by incorporating
multi-modal capabilities, with a focus on enhancing the proficiency of Large
Vision and Language Models in handling interleaved inputs. Our framework is
notable for (1) its open-source support for multi-round and multi-image
dialogues, (2) introducing an innovative multi-modal causal attention
mechanism, and (3) utilizing data blending techniques on existing datasets to
assure seamless interactions in multi-round, multi-image conversations.
Compared to existing frameworks, DeepSpeed-VisualChat shows superior
scalability up to 70B parameter language model size, representing a significant
advancement in multi-modal language models and setting a solid foundation for
future explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15806">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) present an intriguing avenue for exploration in
the field of formal theorem proving. Nevertheless, their full potential,
particularly concerning the mitigation of hallucinations and refinement through
prover error messages, remains an area that has yet to be thoroughly
investigated. To enhance the effectiveness of LLMs in the field, we introduce
the Lyra, a new framework that employs two distinct correction mechanisms: Tool
Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in
the post-processing of formal proofs, we leverage prior knowledge to utilize
predefined prover tools (e.g., Sledgehammer) for guiding the replacement of
incorrect tools. Tool Correction significantly contributes to mitigating
hallucinations, thereby improving the overall accuracy of the proof. In
addition, we introduce Conjecture Correction, an error feedback mechanism
designed to interact with prover to refine formal proof conjectures with prover
error messages. Compared to the previous refinement framework, the proposed
Conjecture Correction refines generation with instruction but does not collect
paired (generation, error &amp; refinement) prompts. Our method has achieved
state-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)
and test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. We
believe Tool Correction (post-process for hallucination mitigation) and
Conjecture Correction (subgoal adjustment from interaction with environment)
could provide a promising avenue for future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">At Which Training Stage Does Code Data Help LLMs Reasoning?. (arXiv:2309.16298v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16298">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited remarkable reasoning capabilities
and become the foundation of language technologies. Inspired by the great
success of code data in training LLMs, we naturally wonder at which training
stage introducing code data can really help LLMs reasoning. To this end, this
paper systematically explores the impact of code data on LLMs at different
stages. Concretely, we introduce the code data at the pre-training stage,
instruction-tuning stage, and both of them, respectively. Then, the reasoning
capability of LLMs is comprehensively and fairly evaluated via six reasoning
tasks in five domains. We critically analyze the experimental results and
provide conclusions with insights. First, pre-training LLMs with the mixture of
code and text can significantly enhance LLMs' general reasoning capability
almost without negative transfer on other tasks. Besides, at the
instruction-tuning stage, code data endows LLMs the task-specific reasoning
capability. Moreover, the dynamic mixing strategy of code and text data assists
LLMs to learn reasoning capability step-by-step during training. These insights
deepen the understanding of LLMs regarding reasoning ability for their
application, such as scientific question answering, legal support, etc. The
source code and model parameters are released at the
link:~\url{https://github.com/yingweima2022/CodeLLM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying CLIP Data. (arXiv:2309.16671v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16671">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) is an approach that has
advanced research and applications in computer vision, fueling modern
recognition systems and generative models. We believe that the main ingredient
to the success of CLIP is its data and not the model architecture or
pre-training objective. However, CLIP only provides very limited information
about its data and how it has been collected, leading to works that aim to
reproduce CLIP's data by filtering with its model parameters. In this work, we
intend to reveal CLIP's data curation approach and in our pursuit of making it
open to the community introduce Metadata-Curated Language-Image Pre-training
(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's
concepts) and yields a balanced subset over the metadata distribution. Our
experimental study rigorously isolates the model and training settings,
concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M
image-text data pairs outperforms CLIP's data on multiple standard benchmarks.
In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,
surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining
the same training budget, attains 72.4%. Our observations hold across various
model sizes, exemplified by ViT-H achieving 80.5%, without any
bells-and-whistles. Curation code and training data distribution on metadata is
made available at https://github.com/facebookresearch/MetaCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17157">
<div class="article-summary-box-inner">
<span><p>In the current user-server interaction paradigm of prompted generation with
large language models (LLM) on cloud, the server fully controls the generation
process, which leaves zero options for users who want to keep the generated
text to themselves. We propose LatticeGen, a cooperative framework in which the
server still handles most of the computation while the user controls the
sampling operation. The key idea is that the true generated sequence is mixed
with noise tokens by the user and hidden in a noised lattice. Considering
potential attacks from a hypothetically malicious server and how the user can
defend against it, we propose the repeated beam-search attack and the mixing
noise scheme. In our experiments we apply LatticeGen to protect both prompt and
generation. It is shown that while the noised lattice degrades generation
quality, LatticeGen successfully protects the true generation to a remarkable
degree under strong attacks (more than 50% of the semantic remains hidden as
measured by BERTScore).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17444">
<div class="article-summary-box-inner">
<span><p>Text-conditioned diffusion models have emerged as a promising tool for neural
video generation. However, current models still struggle with intricate
spatiotemporal prompts and often generate restricted or incorrect motion (e.g.,
even lacking the ability to be prompted for objects moving from left to right).
To address these limitations, we introduce LLM-grounded Video Diffusion (LVD).
Instead of directly generating videos from the text inputs, LVD first leverages
a large language model (LLM) to generate dynamic scene layouts based on the
text inputs and subsequently uses the generated layouts to guide a diffusion
model for video generation. We show that LLMs are able to understand complex
spatiotemporal dynamics from text alone and generate layouts that align closely
with both the prompts and the object motion patterns typically observed in the
real world. We then propose to guide video diffusion models with these layouts
by adjusting the attention maps. Our approach is training-free and can be
integrated into any video diffusion model that admits classifier guidance. Our
results demonstrate that LVD significantly outperforms its base video diffusion
model and several strong baseline methods in faithfully generating videos with
the desired attributes and motion patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17446">
<div class="article-summary-box-inner">
<span><p>Recently, large language models (LLMs), especially those that are pretrained
on code, have demonstrated strong capabilities in generating programs from
natural language inputs in a few-shot or even zero-shot manner. Despite
promising results, there is a notable lack of a comprehensive evaluation of
these models language-to-code generation capabilities. Existing studies often
focus on specific tasks, model architectures, or learning paradigms, leading to
a fragmented understanding of the overall landscape. In this work, we present
L2CEval, a systematic evaluation of the language-to-code generation
capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,
math reasoning and Python programming, analyzing the factors that potentially
affect their performance, such as model size, pretraining data, instruction
tuning, and different prompting methods. In addition to assessing model
performance, we measure confidence calibration for the models and conduct human
evaluations of the output programs. This enables us to identify and analyze the
typical failure modes across various tasks and models. L2CEval offers a
comprehensive understanding of the capabilities and limitations of LLMs in
language-to-code generation. We also release the evaluation framework and all
model outputs, hoping to lay the groundwork for further future research in this
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14402">
<div class="article-summary-box-inner">
<span><p>Language models can store vast amounts of factual knowledge, but their
ability to use this knowledge for logical reasoning remains questionable. This
paper explores a language model's ability to manipulate its stored knowledge
during inference. We focus on four manipulation types: retrieval (e.g., "What
is person A's attribute X"), classification (e.g., "Is A's attribute X even or
odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse
search (e.g., "Which person's attribute X equals T?")
</p>
<p>We observe that pre-trained language models like GPT2/3/4 excel in knowledge
retrieval but struggle with simple classification or comparison tasks unless
Chain of Thoughts (CoTs) are employed during both training and inference. They
also perform poorly in inverse knowledge search, irrespective of the prompts.
Our primary contribution is a synthetic dataset for a controlled experiment
that confirms these inherent weaknesses: a language model cannot efficiently
manipulate knowledge from pre-training data, even when such knowledge is
perfectly stored and fully extractable in the models, and despite adequate
instruct fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-03 23:10:52.181208606 UTC">2023-10-03 23:10:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>