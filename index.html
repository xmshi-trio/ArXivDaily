<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-16T01:30:00Z">05-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?. (arXiv:2305.07666v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07666">
<div class="article-summary-box-inner">
<span><p>Much discussion about large language models and language-and-vision models
has focused on whether these models are intelligent agents. We present an
alternative perspective. We argue that these artificial intelligence models are
cultural technologies that enhance cultural transmission in the modern world,
and are efficient imitation engines. We explore what AI models can tell us
about imitation and innovation by evaluating their capacity to design new tools
and discover novel causal structures, and contrast their responses with those
of human children. Our work serves as a first step in determining which
particular representations and competences, as well as which kinds of knowledge
or skill, can be derived from particular learning techniques and data.
Critically, our findings suggest that machines may need more than large scale
language and images to achieve what a child can do.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07677">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) have proven to be effective for second-pass
rescoring in Automatic Speech Recognition (ASR) systems. In this work, we
propose Masked Audio Text Encoder (MATE), a multi-modal masked language model
rescorer which incorporates acoustic representations into the input space of
MLM. We adopt contrastive learning for effectively aligning the modalities by
learning shared representations. We show that using a multi-modal rescorer is
beneficial for domain generalization of the ASR system when target domain data
is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and
3%-7% on out-of-domain datasets, over the text-only baseline. Additionally,
with very limited amount of training data (0.8 hours), MATE achieves a WER
reduction of 8%-23% over the first-pass baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Language Models to Detect Alarming Student Responses. (arXiv:2305.07709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07709">
<div class="article-summary-box-inner">
<span><p>This article details the advances made to a system that uses artificial
intelligence to identify alarming student responses. This system is built into
our assessment platform to assess whether a student's response indicates they
are a threat to themselves or others. Such responses may include details
concerning threats of violence, severe depression, suicide risks, and
descriptions of abuse. Driven by advances in natural language processing, the
latest model is a fine-tuned language model trained on a large corpus
consisting of student responses and supplementary texts. We demonstrate that
the use of a language model delivers a substantial improvement in accuracy over
the previous iterations of this system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Tree Kernel Computation. (arXiv:2305.07717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07717">
<div class="article-summary-box-inner">
<span><p>Tree kernels are fundamental tools that have been leveraged in many
applications, particularly those based on machine learning for Natural Language
Processing tasks. In this paper, we devise a parallel implementation of the
sequential algorithm for the computation of some tree kernels of two finite
sets of trees (Ouali-Sebti, 2015). Our comparison is narrowed on a sequential
implementation of SubTree kernel computation. This latter is mainly reduced to
an intersection of weighted tree automata. Our approach relies on the nature of
the data parallelism source inherent in this computation by deploying the
MapReduce paradigm. One of the key benefits of our approach is its versatility
in being adaptable to a wide range of substructure tree kernel-based learning
methods. To evaluate the efficacy of our parallel approach, we conducted a
series of experiments that compared it against the sequential version using a
diverse set of synthetic tree language datasets that were manually crafted for
our analysis. The reached results clearly demonstrate that the proposed
parallel algorithm outperforms the sequential one in terms of latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07759">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are powerful tools for natural language processing, but
they often struggle to produce coherent and fluent text when they are small.
Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can
rarely generate coherent and consistent English text beyond a few words even
after extensive training. This raises the question of whether the emergence of
the ability to produce coherent English text only occurs at larger scales (with
hundreds of millions of parameters or more) and complex architectures (with
many layers of global attention).
</p>
<p>In this work, we introduce TinyStories, a synthetic dataset of short stories
that only contain words that a typical 3 to 4-year-olds usually understand,
generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train
and evaluate LMs that are much smaller than the state-of-the-art models (below
10 million total parameters), or have much simpler architectures (with only one
transformer block), yet still produce fluent and consistent stories with
several paragraphs that are diverse and have almost perfect grammar, and
demonstrate reasoning capabilities.
</p>
<p>We also introduce a new paradigm for the evaluation of language models: We
suggest a framework which uses GPT-4 to grade the content generated by these
models as if those were stories written by students and graded by a (human)
teacher. This new paradigm overcomes the flaws of standard benchmarks which
often requires the model's output to be very structures, and moreover provides
a multidimensional score for the model, providing scores for different
capabilities such as grammar, creativity and consistency.
</p>
<p>We hope that TinyStories can facilitate the development, analysis and
research of LMs, especially for low-resource or specialized domains, and shed
light on the emergence of language capabilities in LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Melody-Guided Lyrics Generation. (arXiv:2305.07760v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07760">
<div class="article-summary-box-inner">
<span><p>Automatic song writing is a topic of significant practical interest. However,
its research is largely hindered by the lack of training data due to copyright
concerns and challenged by its creative nature. Most noticeably, prior works
often fall short of modeling the cross-modal correlation between melody and
lyrics due to limited parallel data, hence generating lyrics that are less
singable. Existing works also lack effective mechanisms for content control, a
much desired feature for democratizing song creation for people with limited
music background. In this work, we propose to generate pleasantly listenable
lyrics without training on melody-lyric aligned data. Instead, we design a
hierarchical lyric generation framework that disentangles training (based
purely on text) from inference (melody-guided text generation). At inference
time, we leverage the crucial alignments between melody and lyrics and compile
the given melody into constraints to guide the generation process. Evaluation
results show that our model can generate high-quality lyrics that are more
singable, intelligible, coherent, and in rhyme than strong baselines including
those supervised on parallel data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Authoring for Rules and Actions. (arXiv:2305.07763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07763">
<div class="article-summary-box-inner">
<span><p>Knowledge representation and reasoning (KRR) systems describe and reason with
complex concepts and relations in the form of facts and rules. Unfortunately,
wide deployment of KRR systems runs into the problem that domain experts have
great difficulty constructing correct logical representations of their domain
knowledge. Knowledge engineers can help with this construction process, but
there is a deficit of such specialists. The earlier Knowledge Authoring Logic
Machine (KALM) based on Controlled Natural Language (CNL) was shown to have
very high accuracy for authoring facts and questions. More recently, KALMFL, a
successor of KALM, replaced CNL with factual English, which is much less
restrictive and requires very little training from users. However, KALMFL has
limitations in representing certain types of knowledge, such as authoring rules
for multi-step reasoning or understanding actions with timestamps. To address
these limitations, we propose KALMRA to enable authoring of rules and actions.
Our evaluation using the UTI guidelines benchmark shows that KALMRA achieves a
high level of correctness (100%) on rule authoring. When used for authoring and
reasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI
benchmark, demonstrating its effectiveness in more sophisticated KRR jobs.
Finally, we illustrate the logical reasoning capabilities of KALMRA by drawing
attention to the problems faced by the recently made famous AI, ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models. (arXiv:2305.07766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07766">
<div class="article-summary-box-inner">
<span><p>Temporal Logic (TL) can be used to rigorously specify complex high-level
specification for systems in many engineering applications. The translation
between natural language (NL) and TL has been under-explored due to the lack of
dataset and generalizable model across different application domains. In this
paper, we propose an accurate and generalizable transformation framework of
English instructions from NL to TL, exploring the use of Large Language Models
(LLMs) at multiple stages. Our contributions are twofold. First, we develop a
framework to create a dataset of NL-TL pairs combining LLMs and human
annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5
models on the lifted versions (i.e., the specific Atomic Propositions (AP) are
hidden) of the NL and TL. The enhanced generalizability originates from two
aspects: 1) Usage of lifted NL-TL characterizes common logical structures,
without constraints of specific domains. 2) Application of LLMs in dataset
creation largely enhances corpus richness. We test the generalization of
trained models on five varied domains. To achieve full NL-TL transformation, we
either combine the lifted model with AP recognition task or do the further
finetuning on each specific domain. During the further finetuning, our model
achieves higher accuracy (&gt;95%) using only &lt;10% training data, compared with
the baseline sequence to sequence (Seq2Seq) model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07789">
<div class="article-summary-box-inner">
<span><p>The dominant paradigm of textual question answering systems is based on
end-to-end neural networks, which excels at answering natural language
questions but falls short on complex ones. This stands in contrast to the broad
adaptation of semantic parsing approaches over structured data sources (e.g.,
relational database, knowledge graphs), that convert natural language questions
to logical forms and execute them with query engines. Towards combining the
strengths of neural and symbolic methods, we propose a framework of question
parsing and execution on textual QA. It comprises two central pillars: (1) We
parse the question of varying complexity into an intermediate representation,
named H-expression, which is composed of simple questions as the primitives and
symbolic operations representing the relationships among them; (2) To execute
the resulting H-expressions, we design a hybrid executor, which integrates the
deterministic rules to translate the symbolic operations with a drop-in neural
reader network to answer each decomposed simple question. Hence, the proposed
framework can be viewed as a top-down question parsing followed by a bottom-up
answer backtracking. The resulting H-expressions closely guide the execution
process, offering higher precision besides better interpretability while still
preserving the advantages of the neural readers for resolving its primitive
elements. Our extensive experiments on MuSiQue, 2WikiQA, HotpotQA, and NQ show
that the proposed parsing and hybrid execution framework outperforms existing
approaches in supervised, few-shot, and zero-shot settings, while also
effectively exposing its underlying reasoning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07795">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) have been successful in many natural language
processing tasks. However, real-world stereotype biases are likely to be
reflected in MLMs due to their learning from large text corpora. Most of the
evaluation metrics proposed in the past adopt different masking strategies,
designed with the log-likelihood of MLMs. They lack holistic considerations
such as variance for stereotype bias and anti-stereotype bias samples. In this
paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples
output by MLMs are considered Gaussian distributions. Two evaluation metrics,
Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score
(JSDivS) are proposed to evaluate social biases in MLMs The experimental
results on the public datasets StereoSet and CrowS-Pairs demonstrate that
KLDivS and JSDivS are more stable and interpretable compared to the metrics
proposed in the past.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07797">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning is omnipresent in human communications and thus is an
important feature for open-domain dialogue systems. However, evaluating
commonsense in dialogue systems is still an open challenge. We take the first
step by focusing on event commonsense that considers events and their
relations, and is crucial in both dialogues and general commonsense reasoning.
We propose ACCENT, an event commonsense evaluation metric empowered by
commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation
tuples from a dialogue, and then evaluates the response by scoring the tuples
in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct
the first public event commonsense evaluation dataset for open-domain
dialogues. Our experiments show that ACCENT is an efficient metric for event
commonsense evaluation, which achieves higher correlations with human judgments
than existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07804">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made significant strides in natural
language processing but face challenges in terms of computational expense and
inefficiency as they grow in size, especially in domain-specific tasks. Small
Language Models (SLMs), on the other hand, often struggle in these tasks due to
limited capacity and training data. In this paper, we introduce Dr. LLaMA, a
method for improving SLMs through generative data augmentation using LLMs,
focusing on medical question-answering tasks and the PubMedQA dataset. Our
findings indicate that LLMs effectively refine and diversify existing
question-answer pairs, resulting in improved performance of a much smaller
model on domain-specific QA datasets after fine-tuning. This study highlights
the challenges of using LLMs for domain-specific question answering and
suggests potential research directions to address these limitations, ultimately
aiming to create more efficient and capable models for specialized
applications. We have also made our code available for interested researchers
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement. (arXiv:2305.07824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07824">
<div class="article-summary-box-inner">
<span><p>Generating proper embedding of sentences through an unsupervised way is
beneficial to semantic matching and retrieval problems in real-world scenarios.
This paper presents Representation ALchemy (RepAL), an extremely simple
post-processing method that enhances sentence representations. The basic idea
in RepAL is to de-emphasize redundant information of sentence embedding
generated by pre-trained models. Through comprehensive experiments, we show
that RepAL is free of training and is a plug-and-play method that can be
combined with most existing unsupervised sentence learning models. We also
conducted in-depth analysis to understand RepAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-aware Dimension Selection for Static Word Embedding by Mixed Product Distance. (arXiv:2305.07826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07826">
<div class="article-summary-box-inner">
<span><p>Static word embedding is still useful, particularly for context-unavailable
tasks, because in the case of no context available, pre-trained language models
often perform worse than static word embeddings. Although dimension is a key
factor determining the quality of static word embeddings, automatic dimension
selection is rarely discussed. In this paper, we investigate the impact of word
frequency on the dimension selection, and empirically find that word frequency
is so vital that it needs to be taken into account during dimension selection.
Based on such an empirical finding, this paper proposes a dimension selection
method that uses a metric (Mixed Product Distance, MPD) to select a proper
dimension for word embedding algorithms without training any word embedding.
Through applying a post-processing function to oracle matrices, the MPD-based
method can de-emphasize the impact of word frequency. Experiments on both
context-unavailable and context-available tasks demonstrate the better
efficiency-performance trade-off of our MPD-based dimension selection method
over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Geometry of Multilingual Language Models: An Equality Lens. (arXiv:2305.07839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07839">
<div class="article-summary-box-inner">
<span><p>Understanding the representations of different languages in multilingual
language models is essential for comprehending their cross-lingual properties,
predicting their performance on downstream tasks, and identifying any biases
across languages. In our study, we analyze the geometry of three multilingual
language models in Euclidean space and find that all languages are represented
by unique geometries. Using a geometric separability index we find that
although languages tend to be closer according to their linguistic family, they
are almost separable with languages from other families. We also introduce a
Cross-Lingual Similarity Index to measure the distance of languages with each
other in the semantic space. Our findings indicate that the low-resource
languages are not represented as good as high resource languages in any of the
models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking. (arXiv:2305.07868v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07868">
<div class="article-summary-box-inner">
<span><p>The rapid proliferation of information in the digital era underscores the
importance of accurate historical representation and interpretation. While
artificial intelligence has shown promise in various fields, its potential for
historical fact-checking and gap-filling remains largely untapped. This study
evaluates the performance of three large language models LLMs GPT 3.5, GPT 4,
and GoogleBARD in the context of predicting and verifying historical events
based on given data. A novel metric, Distance to Reality (DTR), is introduced
to assess the models' outputs against established historical facts. The results
reveal a substantial potential for AI in historical studies, with GPT 4
demonstrating superior performance. This paper underscores the need for further
research into AI's role in enriching our understanding of the past and bridging
historical knowledge gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Use Concerns of Generative AI and Large Language Models. (arXiv:2305.07882v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07882">
<div class="article-summary-box-inner">
<span><p>We suggest the implementation of the Dual Use Research of Concern (DURC)
framework, originally designed for life sciences, to the domain of generative
AI, with a specific focus on Large Language Models (LLMs). With its
demonstrated advantages and drawbacks in biological research, we believe the
DURC criteria can be effectively redefined for LLMs, potentially contributing
to improved AI governance. Acknowledging the balance that must be struck when
employing the DURC framework, we highlight its crucial political role in
enhancing societal awareness of the impact of generative AI. As a final point,
we offer a series of specific recommendations for applying the DURC approach to
LLM research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07893">
<div class="article-summary-box-inner">
<span><p>One of the components of natural language processing that has received a lot
of investigation recently is semantic textual similarity. In computational
linguistics and natural language processing, assessing the semantic similarity
of words, phrases, paragraphs, and texts is crucial. Calculating the degree of
semantic resemblance between two textual pieces, paragraphs, or phrases
provided in both monolingual and cross-lingual versions is known as semantic
similarity. Cross lingual semantic similarity requires corpora in which there
are sentence pairs in both the source and target languages with a degree of
semantic similarity between them. Many existing cross lingual semantic
similarity models use a machine translation due to the unavailability of cross
lingual semantic similarity dataset, which the propagation of the machine
translation error reduces the accuracy of the model. On the other hand, when we
want to use semantic similarity features for machine translation the same
machine translations should not be used for semantic similarity. For Persian,
which is one of the low resource languages, no effort has been made in this
regard and the need for a model that can understand the context of two
languages is felt more than ever. In this article, the corpus of semantic
textual similarity between sentences in Persian and English languages has been
produced for the first time by using linguistic experts. We named this dataset
PESTS (Persian English Semantic Textual Similarity). This corpus contains 5375
sentence pairs. Also, different models based on transformers have been
fine-tuned using this dataset. The results show that using the PESTS dataset,
the Pearson correlation of the XLM ROBERTa model increases from 85.87% to
95.62%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07895">
<div class="article-summary-box-inner">
<span><p>Large models have recently played a dominant role in natural language
processing and multimodal vision-language learning. It remains less explored
about their efficacy in text-related visual tasks. We conducted a comprehensive
study of existing publicly available multimodal models, evaluating their
performance in text recognition, text-based visual question answering, and key
information extraction. Our findings reveal strengths and weaknesses in these
models, which primarily rely on semantic understanding for word recognition and
exhibit inferior perception of individual character shapes. They also display
indifference towards text length and have limited capabilities in detecting
fine-grained features in images. Consequently, these results demonstrate that
even the current most powerful large multimodal models cannot match
domain-specific methods in traditional text tasks and face greater challenges
in more complex tasks. Most importantly, the baseline results showcased in this
study could provide a foundational framework for the conception and assessment
of innovative strategies targeted at enhancing zero-shot multimodal techniques.
Evaluation pipeline will be available at
https://github.com/Yuliang-Liu/MultimodalOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07912">
<div class="article-summary-box-inner">
<span><p>Temporal Knowledge graph completion (TKGC) is a crucial task that involves
reasoning at known timestamps to complete the missing part of facts and has
attracted more and more attention in recent years. Most existing methods focus
on learning representations based on graph neural networks while inaccurately
extracting information from timestamps and insufficiently utilizing the implied
information in relations. To address these problems, we propose a novel TKGC
model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We
convert a series of sampled quadruples into pre-trained language model inputs
and convert intervals between timestamps into different prompts to make
coherent sentences with implicit semantic information. We train our model with
a masking strategy to convert TKGC task into a masked token prediction task,
which can leverage the semantic information in pre-trained language models.
Experiments on three benchmark datasets and extensive analysis demonstrate that
our model has great competitiveness compared to other models with four metrics.
Our model can effectively incorporate information from temporal knowledge
graphs into the language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07922">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) pretrained on vast source code have achieved
prominent progress in code intelligence. However, existing code LLMs have two
main limitations in terms of architecture and pretraining tasks. First, they
often adopt a specific architecture (encoder-only or decoder-only) or rely on a
unified encoder-decoder network for different downstream tasks. The former
paradigm is limited by inflexibility in applications while in the latter, the
model is treated as a single system for all tasks, leading to suboptimal
performance on a subset of tasks. Secondly, they often employ a limited set of
pretraining objectives which might not be relevant to some downstream tasks and
hence result in substantial performance degrade. To address these limitations,
we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which
component modules can be flexibly combined to suit a wide range of downstream
code tasks. Such flexibility is enabled by our proposed mixture of pretraining
objectives to mitigate the pretrain-finetune discrepancy. These objectives
cover span denoising, contrastive learning, text-code matching, and causal LM
pretraining tasks, on both unimodal and bimodal multilingual code corpora.
Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs
without training from scratch to efficiently scale up our models, and explore
instruction-tuning to align with natural language instructions. We extensively
evaluate CodeT5+ on over 20 code-related benchmarks in different settings,
including zero-shot, finetuning, and instruction-tuning. We observe
state-of-the-art (SoTA) model performance on various code-related tasks, such
as code generation and completion, math programming, and text-to-code retrieval
tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA
results on HumanEval code generation task against other open code LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training. (arXiv:2305.07927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07927">
<div class="article-summary-box-inner">
<span><p>Multilingual vision-language (V&amp;L) pre-training has achieved remarkable
progress in learning universal representations across different modalities and
languages. In spite of recent success, there still remain challenges limiting
further improvements of V&amp;L pre-trained models in multilingual settings.
Particularly, current V&amp;L pre-training methods rely heavily on strictly-aligned
multilingual image-text pairs generated from English-centric datasets through
machine translation. However, the cost of collecting and translating such
strictly-aligned datasets is usually unbearable. In this paper, we propose
Regularized Contrastive Cross-lingual Cross-modal (RC^3) pre-training, which
further exploits more abundant weakly-aligned multilingual image-text pairs.
Specifically, we design a regularized cross-lingual visio-textual contrastive
learning objective that constrains the representation proximity of
weakly-aligned visio-textual inputs according to textual relevance. Besides,
existing V&amp;L pre-training approaches mainly deal with visual inputs by either
region-of-interest (ROI) features or patch embeddings. We flexibly integrate
the two forms of visual features into our model for pre-training and downstream
multi-modal tasks. Extensive experiments on 5 downstream multi-modal tasks
across 6 languages demonstrate the effectiveness of our proposed method over
competitive contrast models with stronger zero-shot capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07928">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is of key importance to launching multilingual
pre-trained language models for real applications. To support cost-effective
language inference in multilingual settings, we propose AMTSS, an adaptive
multi-teacher single-student distillation framework, which allows distilling
knowledge from multiple teachers to a single student. We first introduce an
adaptive learning strategy and teacher importance weight, which enables a
student to effectively learn from max-margin teachers and easily adapt to new
languages. Moreover, we present a shared student encoder with different
projection layers in support of multiple languages, which contributes to
largely reducing development and machine cost. Experimental results show that
AMTSS gains competitive results on the public XNLI dataset and the realistic
industrial dataset AliExpress (AE) in the E-commerce scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07961">
<div class="article-summary-box-inner">
<span><p>A Conversational Recommender System (CRS) offers increased transparency and
control to users by enabling them to engage with the system through a real-time
multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an
unprecedented ability to converse naturally and incorporate world knowledge and
common-sense reasoning into language understanding, unlocking the potential of
this paradigm. However, effectively leveraging LLMs within a CRS introduces new
technical challenges, including properly understanding and controlling a
complex conversation and retrieving from external sources of information. These
issues are exacerbated by a large, evolving item corpus and a lack of
conversational data for training. In this paper, we provide a roadmap for
building an end-to-end large-scale CRS using LLMs. In particular, we propose
new implementations for user preference understanding, flexible dialogue
management and explainable recommendations as part of an integrated
architecture powered by LLMs. For improved personalization, we describe how an
LLM can consume interpretable natural language user profiles and use them to
modulate session-level context. To overcome conversational data limitations in
the absence of an existing production CRS, we propose techniques for building a
controllable LLM-based user simulator to generate synthetic conversations. As a
proof of concept we introduce RecLLM, a large-scale CRS for YouTube videos
built on LaMDA, and demonstrate its fluency and diverse functionality through
some illustrative example conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07969">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach for detecting ChatGPT-generated vs.
human-written text using language models. To this end, we first collected and
released a pre-processed dataset named OpenGPTText, which consists of rephrased
content generated using ChatGPT. We then designed, implemented, and trained two
different models for text classification, using Robustly Optimized BERT
Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),
respectively. Our models achieved remarkable results, with an accuracy of over
97% on the test dataset, as evaluated through various metrics. Furthermore, we
conducted an interpretability study to showcase our model's ability to extract
and differentiate key features between human-written and ChatGPT-generated
text. Our findings provide important insights into the effective use of
language models to detect generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis. (arXiv:2305.07972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07972">
<div class="article-summary-box-inner">
<span><p>Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a
major driver of financial market returns. We construct the largest tokenized
and annotated dataset of FOMC speeches, meeting minutes, and press conference
transcripts in order to understand how monetary policy influences financial
markets. In this study, we develop a novel task of hawkish-dovish
classification and benchmark various pre-trained language models on the
proposed dataset. Using the best-performing model (RoBERTa-large), we construct
a measure of monetary policy stance for the FOMC document release days. To
evaluate the constructed measure, we study its impact on the treasury market,
stock market, and macroeconomic indicators. Our dataset, models, and code are
publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Faithful Factual Error Correction. (arXiv:2305.07982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07982">
<div class="article-summary-box-inner">
<span><p>Faithfully correcting factual errors is critical for maintaining the
integrity of textual knowledge bases and preventing hallucinations in
sequence-to-sequence models. Drawing on humans' ability to identify and correct
factual errors, we present a zero-shot framework that formulates questions
about input claims, looks for correct answers in the given evidence, and
assesses the faithfulness of each correction based on its consistency with the
evidence. Our zero-shot framework outperforms fully-supervised approaches, as
demonstrated by experiments on the FEVER and SciFact datasets, where our
outputs are shown to be more faithful. More importantly, the decomposability
nature of our framework inherently provides interpretability. Additionally, to
reveal the most suitable metrics for evaluating factual error corrections, we
analyze the correlation between commonly used metrics with human judgments in
terms of three different dimensions regarding intelligibility and faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07984">
<div class="article-summary-box-inner">
<span><p>Detecting negatives (such as non-entailment relationships, unanswerable
questions, and false claims) is an important and challenging aspect of many
natural language understanding tasks. Though manually collecting challenging
negative examples can help models detect them, it is both costly and
domain-specific. In this work, we propose Self-labeled Counterfactuals for
Extrapolating to Negative Examples (SCENE), an automatic method for
synthesizing training data that greatly improves models' ability to detect
challenging negative examples. In contrast with standard data augmentation,
which synthesizes new examples for existing labels, SCENE can synthesize
negative examples zero-shot from only positive ones. Given a positive example,
SCENE perturbs it with a mask infilling model, then determines whether the
resulting example is negative based on a self-training heuristic. With access
to only answerable training examples, SCENE can close 69.6% of the performance
gap on SQuAD 2.0, a dataset where half of the evaluation examples are
unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends
to boolean question answering and recognizing textual entailment, and improves
generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07988">
<div class="article-summary-box-inner">
<span><p>The conventional summarization model often fails to capture critical
information in meeting transcripts, as meeting corpus usually involves multiple
parties with lengthy conversations and is stuffed with redundant and trivial
content. To tackle this problem, we present SVB, an effective and efficient
framework for meeting summarization that `compress' the redundancy while
preserving important content via three processes: sliding-window dialogue
restoration and \textbf{S}coring, channel-wise importance score
\textbf{V}oting, and relative positional \textbf{B}ucketing. Specifically,
under the self-supervised paradigm, the sliding-window scoring aims to rate the
importance of each token from multiple views. Then these ratings are aggregated
by channel-wise voting. Tokens with high ratings will be regarded as salient
information and labeled as \textit{anchors}. Finally, to tailor the lengthy
input to an acceptable length for the language model, the relative positional
bucketing algorithm is performed to retain the anchors while compressing other
irrelevant contents in different granularities. Without large-scale
pre-training or expert-grade annotating tools, our proposed method outperforms
previous state-of-the-art approaches. A vast amount of evaluations and analyses
are conducted to prove the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Previously Fact-Checked Claim Retrieval. (arXiv:2305.07991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07991">
<div class="article-summary-box-inner">
<span><p>Fact-checkers are often hampered by the sheer amount of online content that
needs to be fact-checked. NLP can help them by retrieving already existing
fact-checks relevant to the content being investigated. This paper introduces a
new multilingual dataset -- MultiClaim -- for previously fact-checked claim
retrieval. We collected 28k posts in 27 languages from social media, 206k
fact-checks in 39 languages written by professional fact-checkers, as well as
31k connections between these two groups. This is the most extensive and the
most linguistically diverse dataset of this kind to date. We evaluated how
different unsupervised methods fare on this dataset and its various dimensions.
We show that evaluating such a diverse dataset has its complexities and proper
care needs to be taken before interpreting the results. We also evaluated a
supervised fine-tuning approach, improving upon the unsupervised method
significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Safeguards: Exploring the Security Risks of ChatGPT. (arXiv:2305.08005v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08005">
<div class="article-summary-box-inner">
<span><p>The increasing popularity of large language models (LLMs) such as ChatGPT has
led to growing concerns about their safety, security risks, and ethical
implications. This paper aims to provide an overview of the different types of
security risks associated with ChatGPT, including malicious text and code
generation, private data disclosure, fraudulent services, information
gathering, and producing unethical content. We present an empirical study
examining the effectiveness of ChatGPT's content filters and explore potential
ways to bypass these safeguards, demonstrating the ethical implications and
security risks that persist in LLMs even when protections are in place. Based
on a qualitative analysis of the security implications, we discuss potential
strategies to mitigate these risks and inform researchers, policymakers, and
industry professionals about the complex security challenges posed by LLMs like
ChatGPT. This study contributes to the ongoing discussion on the ethical and
security implications of LLMs, underscoring the need for continued research in
this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance. (arXiv:2305.08010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08010">
<div class="article-summary-box-inner">
<span><p>Current Virtual Mental Health Assistants (VMHAs) provide counseling and
suggestive care. They refrain from patient diagnostic assistance because they
lack training in safety-constrained and specialized clinical process knowledge.
In this work, we define Proknow as an ordered set of information that maps to
evidence-based guidelines or categories of conceptual understanding to experts
in a domain. We also introduce a new dataset of diagnostic conversations guided
by safety constraints and Proknow that healthcare professionals use. We develop
a method for natural language question generation (NLG) that collects
diagnostic information from the patient interactively. We demonstrate the
limitations of using state-of-the-art large-scale language models (LMs) on this
dataset. Our algorithm models the process knowledge through explicitly modeling
safety, knowledge capture, and explainability. LMs augmented with ProKnow
guided method generated 89% safer questions in the depression and anxiety
domain. The Explainability of the generated question is assessed by computing
similarity with concepts in depression and anxiety knowledge bases. Overall,
irrespective of the type of LMs augmented with our ProKnow, we achieved an
average 82% improvement over simple pre-trained LMs on safety, explainability,
and process-guided question generation. We qualitatively and quantitatively
evaluate the efficacy of the proposed ProKnow-guided methods by introducing
three new evaluation metrics for safety, explainability, and process knowledge
adherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering. (arXiv:2305.08059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08059">
<div class="article-summary-box-inner">
<span><p>Event-Level Video Question Answering (EVQA) requires complex reasoning across
video events to obtain the visual information needed to provide optimal
answers. However, despite significant progress in model performance, few
studies have focused on using the explicit semantic connections between the
question and visual information especially at the event level. There is need
for using such semantic connections to facilitate complex reasoning across
video frames. Therefore, we propose a semantic-aware dynamic
retrospective-prospective reasoning approach for video-based question
answering. Specifically, we explicitly use the Semantic Role Labeling (SRL)
structure of the question in the dynamic reasoning process where we decide to
move to the next frame based on which part of the SRL structure (agent, verb,
patient, etc.) of the question is being focused on. We conduct experiments on a
benchmark EVQA dataset - TrafficQA. Results show that our proposed approach
achieves superior performance compared to previous state-of-the-art models. Our
code will be made publicly available for research use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08067">
<div class="article-summary-box-inner">
<span><p>Most End-to-End SLU methods depend on the pretrained ASR or language model
features for intent prediction. However, other essential information in speech,
such as prosody, is often ignored. Recent research has shown improved results
in classifying dialogue acts by incorporating prosodic information. The margins
of improvement in these methods are minimal as the neural models ignore
prosodic features. In this work, we propose prosody-attention, which uses the
prosodic features differently to generate attention maps across time frames of
the utterance. Then we propose prosody-distillation to explicitly learn the
prosodic information in the acoustic encoder rather than concatenating the
implicit prosodic features. Both the proposed methods improve the baseline
results, and the prosody-distillation method gives an intent classification
accuracy improvement of 8\% and 2\% on SLURP and STOP datasets over the prosody
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08088">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown increasing power on various natural
language processing (NLP) tasks. However, tuning these models for downstream
tasks usually needs exorbitant costs or is unavailable due to commercial
considerations. Recently, black-box tuning has been proposed to address this
problem by optimizing task-specific prompts without accessing the gradients and
hidden representations. However, most existing works have yet fully exploited
the potential of gradient-free optimization under the scenario of few-shot
learning. In this paper, we describe BBT-RGB, a suite of straightforward and
complementary techniques for enhancing the efficiency and performance of
black-box optimization. Specifically, our method includes three plug-and-play
components: (1) Two-stage derivative-free optimization strategy that
facilitates fast convergence and mitigates overfitting; (2) Automatic
verbalizer construction with its novel usage under few-shot settings; (3)
Better prompt initialization policy based on instruction search and
auto-selected demonstration. Extensive experiments across various tasks on
natural language understanding and inference demonstrate the effectiveness of
our method. Our codes are publicly available at
https://github.com/QiushiSun/BBT-RGB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08096">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is a promising technique for model compression in
neural machine translation. However, where the knowledge hides in KD is still
not clear, which may hinder the development of KD. In this work, we first
unravel this mystery from an empirical perspective and show that the knowledge
comes from the top-1 predictions of teachers, which also helps us build a
potential connection between word- and sequence-level KD. Further, we point out
two inherent issues in vanilla word-level KD based on this finding. Firstly,
the current objective of KD spreads its focus to whole distributions to learn
the knowledge, yet lacks special treatment on the most crucial top-1
information. Secondly, the knowledge is largely covered by the golden
information due to the fact that most top-1 predictions of teachers overlap
with ground-truth tokens, which further restricts the potential of KD. To
address these issues, we propose a novel method named \textbf{T}op-1
\textbf{I}nformation \textbf{E}nhanced \textbf{K}nowledge \textbf{D}istillation
(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the
learning of the top-1 information from the teacher. Additionally, we develop an
iterative KD procedure to infuse more additional knowledge by distilling on the
data without ground-truth targets. Experiments on WMT'14 English-German, WMT'14
English-French and WMT'16 English-Romanian demonstrate that our method can
respectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU
scores and significantly outperform the vanilla word-level KD baseline.
Besides, our method shows higher generalizability on different teacher-student
capacity gaps than existing KD techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08099">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have
demonstrated state-of-the-art performance on automatic speech recognition (ASR)
and proved to be extremely useful in low label-resource settings. However, the
success of SSL models has yet to transfer to utterance-level tasks such as
speaker, emotion, and language recognition, which still require supervised
fine-tuning of the SSL models to obtain good performance. We argue that the
problem is caused by the lack of disentangled representations and an
utterance-level learning objective for these tasks. Inspired by how HuBERT uses
clustering to discover hidden acoustic units, we formulate a factor analysis
(FA) model that uses the discovered hidden acoustic units to align the SSL
features. The underlying utterance-level representations are disentangled from
the content of speech using probabilistic inference on the aligned features.
Furthermore, the variational lower bound derived from the FA model provides an
utterance-level objective, allowing error gradients to be backpropagated to the
Transformer layers to learn highly discriminative acoustic units. When used in
conjunction with HuBERT's masked prediction training, our models outperform the
current best model, WavLM, on all utterance-level non-semantic tasks on the
SUPERB benchmark with only 20% of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08135">
<div class="article-summary-box-inner">
<span><p>Existing knowledge-enhanced methods have achieved remarkable results in
certain QA tasks via obtaining diverse knowledge from different knowledge
bases. However, limited by the properties of retrieved knowledge, they still
have trouble benefiting from both the knowledge relevance and distinguishment
simultaneously. To address the challenge, we propose CPACE, a Concept-centric
Prompt-bAsed Contrastive Explanation Generation model, which aims to convert
obtained symbolic knowledge into a contrastive explanation for better
distinguishing the differences among given candidates. Firstly, following
previous works, we retrieve different types of symbolic knowledge with a
concept-centric knowledge extraction module. After that, we generate
corresponding contrastive explanations using acquired symbolic knowledge and
explanation prompts as guidance for better modeling the knowledge
distinguishment and interpretability. Finally, we regard the generated
contrastive explanation as external knowledge for downstream task enhancement.
We conduct a series of experiments on three widely-used question-answering
datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the
help of generated contrastive explanation, our CPACE model achieves new SOTA on
CSQA (89.8% on the testing set, 0.9% higher than human performance), and gains
impressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaLS: Lexical Substitution via Pretrained Paraphraser. (arXiv:2305.08146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08146">
<div class="article-summary-box-inner">
<span><p>Lexical substitution (LS) aims at finding appropriate substitutes for a
target word in a sentence. Recently, LS methods based on pretrained language
models have made remarkable progress, generating potential substitutes for a
target word through analysis of its contextual surroundings. However, these
methods tend to overlook the preservation of the sentence's meaning when
generating the substitutes. This study explores how to generate the substitute
candidates from a paraphraser, as the generated paraphrases from a paraphraser
contain variations in word choice and preserve the sentence's meaning. Since we
cannot directly generate the substitutes via commonly used decoding strategies,
we propose two simple decoding strategies that focus on the variations of the
target word during decoding. Experimental results show that our methods
outperform state-of-the-art LS methods based on pre-trained language models on
three benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation. (arXiv:2305.08152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08152">
<div class="article-summary-box-inner">
<span><p>Collaborative stories, which are texts created through the collaborative
efforts of multiple authors with different writing styles and intentions, pose
unique challenges for NLP models. Understanding and generating such stories
remains an underexplored area due to the lack of open-domain corpora. To
address this, we introduce STORYWARS, a new dataset of over 40,000
collaborative stories written by 9,400 different authors from an online
platform. We design 12 task types, comprising 7 understanding and 5 generation
task types, on STORYWARS, deriving 101 diverse story-related tasks in total as
a multi-task benchmark covering all fully-supervised, few-shot, and zero-shot
scenarios. Furthermore, we present our instruction-tuned model, INSTRUCTSTORY,
for the story tasks showing that instruction tuning, in addition to achieving
superior results in zero-shot and few-shot scenarios, can also obtain the best
performance on the fully-supervised tasks in STORYWARS, establishing strong
multi-task benchmark performances on STORYWARS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset of Film Reviews. (arXiv:2305.08173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08173">
<div class="article-summary-box-inner">
<span><p>This paper introduces Cro-FiReDa, a sentiment-annotated dataset for Croatian
in the domain of movie reviews. The dataset, which contains over 10,000
sentences, has been annotated at the sentence level. In addition to presenting
the overall annotation process, we also present benchmark results based on the
transformer-based fine-tuning approach
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus. (arXiv:2305.08187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08187">
<div class="article-summary-box-inner">
<span><p>This article presents a sentence-level sentiment dataset for the Croatian
news domain. In addition to the 3K annotated texts already present, our dataset
contains 14.5K annotated sentence occurrences that have been tagged with 5
classes. We provide baseline scores in addition to the annotation process and
inter-annotator agreement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08195">
<div class="article-summary-box-inner">
<span><p>Interactive semantic parsing based on natural language (NL) feedback, where
users provide feedback to correct the parser mistakes, has emerged as a more
practical scenario than the traditional one-shot semantic parsing. However,
prior work has heavily relied on human-annotated feedback data to train the
interactive semantic parser, which is prohibitively expensive and not scalable.
In this work, we propose a new task of simulating NL feedback for interactive
semantic parsing. We accompany the task with a novel feedback evaluator. The
evaluator is specifically designed to assess the quality of the simulated
feedback, based on which we decide the best feedback simulator from our
proposed variants. On a text-to-SQL dataset, we show that our feedback
simulator can generate high-quality NL feedback to boost the error correction
ability of a specific parser. In low-data settings, our feedback simulator can
help achieve comparable error correction performance as trained using the
costly, full set of human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment. (arXiv:2305.08200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08200">
<div class="article-summary-box-inner">
<span><p>When communicating with elders with cognitive impairment, cognitive
stimulation (CS) help to maintain the cognitive health of elders. Data sparsity
is the main challenge in building CS-based dialogue systems, particularly in
the Chinese language. To fill this gap, we construct a Chinese CS conversation
(CSConv) dataset, which contains about 2.6K groups of dialogues with CS
principles and emotional support strategy labels. Making chit chat while
providing emotional support is overlooked by the majority of existing cognitive
dialogue systems. In this paper, we propose a multi-source knowledge fusion
method for CS dialogue (CSD), to generate open-ended responses guided by the CS
principle and emotional support strategy. We first use a progressive mask
method based on external knowledge to learn encoders as effective classifiers,
which is the prerequisite to predict the CS principle and emotional support
strategy of the target response. Then a decoder interacts with the perceived CS
principle and emotional support strategy to generate responses. Extensive
experiments conducted on the CSConv dataset demonstrate the effectiveness of
the proposed method, while there is still a large space for improvement
compared to human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08208">
<div class="article-summary-box-inner">
<span><p>There have been growing concerns regarding the out-of-domain generalization
ability of natural language processing (NLP) models, particularly in
question-answering (QA) tasks. Current synthesized data augmentation methods
for QA are hampered by increased training costs. To address this issue, we
propose a novel approach that combines prompting methods and linear probing
then fine-tuning strategy, which does not entail additional cost. Our method
has been theoretically and empirically shown to be effective in enhancing the
generalization ability of both generative and discriminative models. Our
approach outperforms state-of-the-art baselines, with an average increase in F1
score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any
pre-trained models and offers a promising solution to the under-explored
cross-domain QA task. We release our source code at GitHub*.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement. (arXiv:2305.08227v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08227">
<div class="article-summary-box-inner">
<span><p>Multi-frame algorithms for single-channel speech enhancement are able to take
advantage from short-time correlations within the speech signal. Deep Filtering
(DF) was proposed to directly estimate a complex filter in frequency domain to
take advantage of these correlations. In this work, we present a real-time
speech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is
enabled by exploiting domain knowledge of speech production and psychoacoustic
perception. Our model is able to match state-of-the-art speech enhancement
benchmarks while achieving a real-time-factor of 0.19 on a single threaded
notebook CPU. The framework as well as pretrained weights have been published
under an open source license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12227">
<div class="article-summary-box-inner">
<span><p>We explore the use of residual networks and neural attention for multiple
argument mining tasks. We propose a residual architecture that exploits
attention, multi-task learning, and makes use of ensemble, without any
assumption on document or argument structure. We present an extensive
experimental evaluation on five different corpora of user-generated comments,
scientific publications, and persuasive essays. Our results show that our
approach is a strong competitor against state-of-the-art architectures with a
higher computational footprint or corpus-specific design, representing an
interesting compromise between generality, performance accuracy and reduced
model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quinductor: a multilingual data-driven method for generating reading-comprehension questions using Universal Dependencies. (arXiv:2103.10121v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10121">
<div class="article-summary-box-inner">
<span><p>We propose a multilingual data-driven method for generating reading
comprehension questions using dependency trees. Our method provides a strong,
mostly deterministic, and inexpensive-to-train baseline for less-resourced
languages. While a language-specific corpus is still required, its size is
nowhere near those required by modern neural question generation (QG)
architectures. Our method surpasses QG baselines previously reported in the
literature and shows a good performance in terms of human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01263">
<div class="article-summary-box-inner">
<span><p>Sample-and-rank is a key decoding strategy for modern generation-based
dialogue systems. It helps achieve diverse and high-quality responses by
selecting an answer from a small pool of generated candidates. The current
state-of-the-art ranking methods mainly use an encoding paradigm called
Cross-Encoder, which separately encodes each context-candidate pair and ranks
the candidates according to their fitness scores. However, Cross-Encoder
repeatedly encodes the same lengthy context for each candidate, resulting in
high computational costs. Poly-Encoder addresses the above problems by reducing
the interaction between context and candidates, but with a price of performance
drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps
the full attention over each pair as in Cross-Encoder while only encoding the
context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with
the context in one forward pass. We use the same positional embedding for all
candidates to ensure they are treated equally and design a new attention
mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking
paradigms using different attention and response concatenation methods.
Extensive experiments show that our proposed paradigm achieves new
state-of-the-art results on four benchmark datasets with high computational
efficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X
faster inference speed on the Ubuntu V2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13005">
<div class="article-summary-box-inner">
<span><p>In the last few years, the memory requirements to train state-of-the-art
neural networks have far exceeded the DRAM capacities of modern hardware
accelerators. This has necessitated the development of efficient algorithms to
train these neural networks in parallel on large-scale GPU-based clusters.
Since computation is relatively inexpensive on modern GPUs, designing and
implementing extremely efficient communication in these parallel training
algorithms is critical for extracting the maximum performance. This paper
presents AxoNN, a parallel deep learning framework that exploits asynchrony and
message-driven execution to schedule neural network operations on each GPU,
thereby reducing GPU idle time and maximizing hardware efficiency. By using the
CPU memory as a scratch space for offloading data periodically during training,
AxoNN is able to reduce GPU memory consumption by four times. This allows us to
increase the number of parameters per GPU by four times, thus reducing the
amount of communication and increasing performance by over 13%. When tested
against large transformer models with 12-100 billion parameters on 48-384
NVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of
theoretical peak and reduces the training time by 22-37 days (15-25% speedup)
as compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16965">
<div class="article-summary-box-inner">
<span><p>While self-supervised speech representation learning (SSL) models serve a
variety of downstream tasks, these models have been observed to overfit to the
domain from which the unlabelled data originates. To alleviate this issue, we
propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant
weights from models pre-trained on large amounts of out-of-domain (OOD) data.
Intuitively, this helps to make space for the target-domain ASR finetuning. The
redundant weights can be identified through various pruning strategies which
have been discussed in detail as a part of this work. Specifically, we
investigate the effect of the recently discovered Task-Agnostic and Task-Aware
pruning on PADA and propose a new pruning paradigm based on the latter, which
we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial
pruning mask from a well fine-tuned OOD model, which makes it starkly different
from the rest of the pruning strategies discussed in the paper. Our proposed
CD-TAW methodology achieves up to 20.6% relative WER improvement over our
baseline when fine-tuned on a 2-hour subset of Switchboard data without
language model (LM) decoding. Furthermore, we conduct a detailed analysis to
highlight the key design choices of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09593">
<div class="article-summary-box-inner">
<span><p>Vision outlooker improves the performance of vision transformers, which
implements a self-attention mechanism by adding an outlook attention, a form of
local attention.
</p>
<p>In natural language processing, as has been the case in computer vision and
other domains, transformer-based models constitute the state-of-the-art for
most processing tasks. In this domain, too, many authors have argued and
demonstrated the importance of local context.
</p>
<p>We present an outlook attention mechanism, COOL, for natural language
processing. COOL, added on top of the self-attention layers of a
transformer-based model, encodes local syntactic context considering word
proximity and more pair-wise constraints than dynamic convolution used by
existing approaches.
</p>
<p>A comparative empirical performance evaluation of an implementation of COOL
with different transformer-based models confirms the opportunity for
improvement over a baseline using the original models alone for various natural
language processing tasks, including question answering. The proposed approach
achieves competitive performance with existing state-of-the-art methods on some
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v4 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08406">
<div class="article-summary-box-inner">
<span><p>Tweets are the most concise form of communication in online social media,
wherein a single tweet has the potential to make or break the discourse of the
conversation. Online hate speech is more accessible than ever, and stifling its
propagation is of utmost importance for social media companies and users for
congenial communication. Most of the research barring a recent few has focused
on classifying an individual tweet regardless of the tweet thread/context
leading up to that point. One of the classical approaches to curb hate speech
is to adopt a reactive strategy after the hate speech postage. The ex-post
facto strategy results in neglecting subtle posts that do not show the
potential to instigate hate speech on their own but may portend in the
subsequent discussion ensuing in the post's replies. In this paper, we propose
DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring
in through its reply chain in the future. It uses the semantic and propagating
structure of the tweet threads to maximize the contextual information leading
up to and the fall of hate intensity at each subsequent tweet. We explore three
publicly available Twitter datasets -- Anti-Racism contains the reply tweets of
a collection of social media discourse on racist remarks during US political
and Covid-19 background; Anti-Social presents a dataset of 40 million tweets
amidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents
Twitter datasets collated based on anti-Asian behaviours during COVID-19
pandemic. All the curated datasets consist of structural graph information of
the Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art
baselines significantly. It beats the best baseline by an 11% margin on the
Person correlation coefficient and a decrease of 25% on RMSE for the
Anti-Racism dataset with a similar performance on the other two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Audio Captioning and Language-Based Audio Retrieval. (arXiv:2207.04156v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04156">
<div class="article-summary-box-inner">
<span><p>This project involved participation in the DCASE 2022 Competition (Task 6)
which had two subtasks: (1) Automated Audio Captioning and (2) Language-Based
Audio Retrieval. The first subtask involved the generation of a textual
description for audio samples, while the goal of the second was to find audio
samples within a fixed dataset that match a given description. For both
subtasks, the Clotho dataset was used. The models were evaluated on BLEU1,
BLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio
captioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have
conducted a handful of experiments that modify the baseline models for these
tasks. Our final architecture for Automated Audio Captioning is close to the
baseline performance, while our model for Language-Based Audio Retrieval has
surpassed its counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation. (arXiv:2208.08845v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08845">
<div class="article-summary-box-inner">
<span><p>Empathetic conversation is psychologically supposed to be the result of
conscious alignment and interaction between the cognition and affection of
empathy. However, existing empathetic dialogue models usually consider only the
affective aspect or treat cognition and affection in isolation, which limits
the capability of empathetic response generation. In this work, we propose the
CASE model for empathetic dialogue generation. It first builds upon a
commonsense cognition graph and an emotional concept graph and then aligns the
user's cognition and affection at both the coarse-grained and fine-grained
levels. Through automatic and manual evaluation, we demonstrate that CASE
outperforms state-of-the-art baselines of empathetic dialogues and can generate
more empathetic and informative responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.10734">
<div class="article-summary-box-inner">
<span><p>Dynamic contextualised word embeddings (DCWEs) represent the temporal
semantic variations of words. We propose a method for learning DCWEs by
time-adapting a pretrained Masked Language Model (MLM) using time-sensitive
templates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively
at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised
method to select (a) \emph{pivot} terms related to both $C_1$ and $C_2$, and
(b) \emph{anchor} terms that are associated with a specific pivot term in each
individual snapshot. We then generate prompts by filling manually compiled
templates using the extracted pivot and anchor terms. Moreover, we propose an
automatic method to learn time-sensitive templates from $C_1$ and $C_2$,
without requiring any human supervision. Next, we use the generated prompts to
adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple
experiments show that our proposed method reduces the perplexity of test
sentences in $C_2$, outperforming the current state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing. (arXiv:2208.13423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.13423">
<div class="article-summary-box-inner">
<span><p>Non-parallel text style transfer is an important task in natural language
generation. However, previous studies concentrate on the token or sentence
level, such as sentence sentiment and formality transfer, but neglect long
style transfer at the discourse level. Long texts usually involve more
complicated author linguistic preferences such as discourse structures than
sentences. In this paper, we formulate the task of non-parallel story
author-style transfer, which requires transferring an input story into a
specified author style while maintaining source semantics. To tackle this
problem, we propose a generation model, named StoryTrans, which leverages
discourse representations to capture source content information and transfer
them to target styles with learnable style embeddings. We use an additional
training objective to disentangle stylistic features from the learned discourse
representation to prevent the model from degenerating to an auto-encoder.
Moreover, to enhance content preservation, we design a mask-and-fill framework
to explicitly fuse style-specific keywords of source texts into generation.
Furthermore, we constructed new datasets for this task in Chinese and English,
respectively. Extensive experiments show that our model outperforms strong
baselines in overall performance of style transfer and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06049">
<div class="article-summary-box-inner">
<span><p>NLP in the legal domain has seen increasing success with the emergence of
Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.
PLMs trained over European and US legal text are available publicly; however,
legal text from other domains (countries), such as India, have a lot of
distinguishing characteristics. With the rapidly increasing volume of Legal NLP
applications in various countries, it has become necessary to pre-train such
LMs over legal text of other countries as well. In this work, we attempt to
investigate pre-training in the Indian legal domain. We re-train (continue
pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian
legal data, as well as train a model from scratch with a vocabulary based on
Indian legal text. We apply these PLMs over three benchmark legal NLP tasks --
Legal Statute Identification from facts, Semantic Segmentation of Court
Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian
and non-Indian (EU, UK) datasets. We observe that our approach not only
enhances performance on the new domain (Indian texts) but also over the
original domain (European and UK texts). We also conduct explainability
experiments for a qualitative comparison of all these different PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Round-Trip Translation for Machine Translation Evaluation. (arXiv:2209.07351v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07351">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation on low-resource language translation suffers from a
deficiency of parallel corpora. Round-trip translation could be served as a
clever and straightforward technique to alleviate the requirement of the
parallel evaluation corpus. However, there was an observation of obscure
correlations between the evaluation scores by forward and round-trip
translations in the era of statistical machine translation (SMT). In this
paper, we report the surprising finding that round-trip translation can be used
for automatic evaluation without the references. Firstly, our revisit on the
round-trip translation in SMT evaluation unveils that its long-standing
misunderstanding is essentially caused by copying mechanism. After removing
copying mechanism in SMT, round-trip translation scores can appropriately
reflect the forward translation performance. Then, we demonstrate the
rectification is overdue as round-trip translation could benefit multiple
machine translation evaluation tasks. To be more specific, round-trip
translation could be used i) to predict corresponding forward translation
scores; ii) to improve the performance of the recently advanced quality
estimation model; and iii) to identify adversarial competitors in shared tasks
via cross-system verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revamping Multilingual Agreement Bidirectionally via Switched Back-translation for Multilingual Neural Machine Translation. (arXiv:2209.13940v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13940">
<div class="article-summary-box-inner">
<span><p>Despite the fact that multilingual agreement (MA) has shown its importance
for multilingual neural machine translation (MNMT), current methodologies in
the field have two shortages: (i) require parallel data between multiple
language pairs, which is not always realistic and (ii) optimize the agreement
in an ambiguous direction, which hampers the translation performance. We
present \textbf{B}idirectional \textbf{M}ultilingual \textbf{A}greement via
\textbf{S}witched \textbf{B}ack-\textbf{t}ranslation (\textbf{BMA-SBT}), a
novel and universal multilingual agreement framework for fine-tuning
pre-trained MNMT models, which (i) exempts the need for aforementioned parallel
data by using a novel method called switched BT that creates synthetic text
written in another source language using the translation target and (ii)
optimizes the agreement bidirectionally with the Kullback-Leibler Divergence
loss. Experiments indicate that BMA-SBT clearly improves the strong baselines
on the task of MNMT with three benchmarks: TED Talks, News, and Europarl.
In-depth analyzes indicate that BMA-SBT brings additive improvements to the
conventional BT method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15206">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a theoretical framework to explain the efficacy of
prompt learning in zero/few-shot scenarios. First, we prove that conventional
pre-training and fine-tuning paradigm fails in few-shot scenarios due to
overfitting the unrepresentative labelled data. We then detail the assumption
that prompt learning is more effective because it empowers pre-trained language
model that is built upon massive text corpora, as well as domain-related human
knowledge to participate more in prediction and thereby reduces the impact of
limited label information provided by the small training set. We further
hypothesize that language discrepancy can measure the quality of prompting.
Comprehensive experiments are performed to verify our assumptions. More
remarkably, inspired by the theoretical framework, we propose an
annotation-agnostic template selection method based on perplexity, which
enables us to ``forecast'' the prompting performance in advance. This approach
is especially encouraging because existing work still relies on development set
to post-hoc evaluate templates. Experiments show that this method leads to
significant prediction benefits compared to state-of-the-art zero-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02592">
<div class="article-summary-box-inner">
<span><p>While Self-Supervised Learning has helped reap the benefit of the scale from
the available unlabeled data, the learning paradigms are continuously being
bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which
uses clustering and an augmentation-based cross-contrastive loss as its
self-supervised objective. Through the clustering module, we scale down the
influence of those negative examples that are highly similar to the positive.
The Cross-Contrastive loss is computed between the encoder output of the
original sample and the quantizer output of its augmentation and vice-versa,
bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up
to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on
the test-clean and test-other sets, respectively, of LibriSpeech, without the
use of any language model. The proposed method also achieves up to 14.9%
relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on
Switchboard data. We make all our codes publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04105">
<div class="article-summary-box-inner">
<span><p>With the advent of pretrained language models (LMs), increasing research
efforts have been focusing on infusing commonsense and domain-specific
knowledge to prepare LMs for downstream tasks. These works attempt to leverage
knowledge graphs, the de facto standard of symbolic knowledge representation,
along with pretrained LMs. While existing approaches have leveraged external
knowledge, it remains an open question how to jointly incorporate knowledge
graphs representing varying contexts, from local (e.g., sentence), to
document-level, to global knowledge, to enable knowledge-rich exchange across
these contexts. Such rich contextualization can be especially beneficial for
long document understanding tasks since standard pretrained LMs are typically
bounded by the input sequence length. In light of these challenges, we propose
KALM, a Knowledge-Aware Language Model that jointly leverages knowledge in
local, document-level, and global contexts for long document understanding.
KALM first encodes long documents and knowledge graphs into the three
knowledge-aware context representations. It then processes each context with
context-specific layers, followed by a context fusion layer that facilitates
knowledge exchange to derive an overarching document representation. Extensive
experiments demonstrate that KALM achieves state-of-the-art performance on six
long document understanding tasks and datasets. Further analyses reveal that
the three knowledge-aware contexts are complementary and they all contribute to
model performance, while the importance and information exchange patterns of
different contexts vary with respect to different tasks and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection. (arXiv:2210.09167v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09167">
<div class="article-summary-box-inner">
<span><p>There is an ongoing debate on whether neural networks can grasp the
quasi-regularities in languages like humans. In a typical quasi-regularity
task, English past tense inflections, the neural network model has long been
criticized that it learns only to generalize the most frequent pattern, but not
the regular pattern, thus can not learn the abstract categories of regular and
irregular and is dissimilar to human performance. In this work, we train a set
of transformer models with different settings to examine their behavior on this
task. The models achieved high accuracy on unseen regular verbs and some
accuracy on unseen irregular verbs. The models' performance on the regulars is
heavily affected by type frequency and ratio but not token frequency and ratio,
and vice versa for the irregulars. The different behaviors on the regulars and
irregulars suggest that the models have some degree of symbolic learning on the
regularity of the verbs. In addition, the models are weakly correlated with
human behavior on nonce verbs. Although the transformer model exhibits some
level of learning on the abstract category of verb regularity, its performance
does not fit human data well, suggesting that it might not be a good cognitive
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01246">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new Self-Supervised Learning (SSL) algorithm
called data2vec-aqc, for speech representation learning from unlabeled speech
data. Our goal is to improve SSL for speech in domains where both unlabeled and
labeled data are limited. Building on the recently introduced data2vec, we
introduce additional modules to the data2vec framework that leverage the
benefit of data augmentations, quantized representations, and clustering. The
interaction between these modules helps solve the cross-contrastive loss as an
additional self-supervised objective. data2vec-aqc achieves up to 14.1% and
20.9% relative WER improvement over the existing state-of-the-art data2vec
system over the test-clean and test-other sets, respectively of LibriSpeech,
without the use of any language model (LM). Our proposed model also achieves up
to 17.8\% relative WER gains over the baseline data2vec when fine-tuned on a
subset of the Switchboard dataset. Code:
https://github.com/Speech-Lab-IITM/data2vec-aqc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A minor extension of the logistic equation for growth of word counts on online media: Parametric description of diversity of growth phenomena in society. (arXiv:2211.16733v2 [physics.soc-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16733">
<div class="article-summary-box-inner">
<span><p>To understand the growing phenomena of new vocabulary on nationwide online
social media, we analyzed monthly word count time series extracted from
approximately 1 billion Japanese blog articles from 2007 to 2019. In
particular, we first introduced the extended logistic equation by adding one
parameter to the original equation and showed that the model can consistently
reproduce various patterns of actual growth curves, such as the logistic
function, linear growth, and finite-time divergence. Second, by analyzing the
model parameters, we found that the typical growth pattern is not only a
logistic function, which often appears in various complex systems, but also a
nontrivial growth curve that starts with an exponential function and
asymptotically approaches a power function without a steady state. Furthermore,
we observed a connection between the functional form of growth and the
peak-out. Finally, we showed that the proposed model and statistical properties
are also valid for Google Trends data (English, French, Spanish, and Japanese),
which is a time series of the nationwide popularity of search queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16865">
<div class="article-summary-box-inner">
<span><p>A temporal knowledge graph (TKG) stores the events derived from the data
involving time. Predicting events is extremely challenging due to the
time-sensitive property of events. Besides, the previous TKG completion (TKGC)
approaches cannot represent both the timeliness and the causality properties of
events, simultaneously. To address these challenges, we propose a Logic and
Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive
representation involving timeliness and causality of events, together with the
time-independent representation of events from the perspective of commonsense.
Specifically, we design a temporal rule learning algorithm to construct a
rule-guided predicate embedding regularization strategy for learning the
causality among events. Furthermore, we could accurately evaluate the
plausibility of events via auxiliary commonsense knowledge. The experimental
results of TKGC task illustrate the significant performance improvements of our
model compared with the existing approaches. More interestingly, our model is
able to provide the explainability of the predicted results in the view of
causal inference. The source code and datasets of this paper are available at
https://github.com/ngl567/LCGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v5 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03760">
<div class="article-summary-box-inner">
<span><p>Recent studies have proposed unified user modeling frameworks that leverage
user behavior data from various applications. Many of them benefit from
utilizing users' behavior sequences as plain texts, representing rich
information in any domain or system without losing generality. Hence, a
question arises: Can language modeling for user history corpus help improve
recommender systems? While its versatile usability has been widely investigated
in many domains, its applications to recommender systems still remain
underexplored. We show that language modeling applied directly to task-specific
user histories achieves excellent results on diverse recommendation tasks.
Also, leveraging additional task-agnostic user histories delivers significant
performance benefits. We further demonstrate that our approach can provide
promising transfer learning capabilities for a broad spectrum of real-world
recommender systems, even on unseen domains and services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07530">
<div class="article-summary-box-inner">
<span><p>Multilingual machine translation models can benefit from synergy between
different language pairs, but also suffer from interference. While there is a
growing number of sophisticated methods that aim to eliminate interference, our
understanding of interference as a phenomenon is still limited. This work
identifies the main factors that contribute to interference in multilingual
machine translation. Through systematic experimentation, we find that
interference (or synergy) are primarily determined by model size, data size,
and the proportion of each language pair within the total dataset. We observe
that substantial interference occurs mainly when the model is very small with
respect to the available training data, and that using standard transformer
configurations with less than one billion parameters largely alleviates
interference and promotes synergy. Moreover, we show that tuning the sampling
temperature to control the proportion of each language pair in the data is key
to balancing the amount of interference between low and high resource language
pairs effectively, and can lead to superior performance overall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models. (arXiv:2212.07752v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07752">
<div class="article-summary-box-inner">
<span><p>Despite the success of multilingual sequence-to-sequence pre-training, most
existing approaches rely on document-level monolingual corpora in many
different languages, sentence-level bilingual corpora,\footnote{In this paper,
we use `bilingual corpora' to denote parallel corpora with `bilingual
translation pairs' in many different language pairs, each consisting of two
sentences/documents with the same meaning written in different languages. We
use `trilingual corpora' to denote parallel corpora with `trilingual
translation pairs' in many different language combinations, each consisting of
three sentences/documents.} and sometimes synthetic document-level bilingual
corpora. This hampers the performance with cross-lingual document-level tasks
such as document-level translation. Therefore, we propose to mine and leverage
document-level trilingual parallel corpora to improve sequence-to-sequence
multilingual pre-training. We present \textbf{Tri}angular Document-level
\textbf{P}re-training (\textbf{TRIP}), which is the first in the field to
accelerate the conventional monolingual and bilingual objectives into a
trilingual objective with a novel method called Grafting. Experiments show that
TRIP achieves several strong state-of-the-art (SOTA) scores on three
multilingual document-level machine translation benchmarks and one
cross-lingual abstractive summarization benchmark, including consistent
improvements by up to 3.11 d-BLEU points and 8.9 ROUGE-L points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09086">
<div class="article-summary-box-inner">
<span><p>We investigate response generation for multi-turn dialogue in
generative-based chatbots. Existing generative models based on RNNs (Recurrent
Neural Networks) usually employ the last hidden state to summarize the
sequences, which makes models unable to capture the subtle variability observed
in different dialogues and cannot distinguish the differences between dialogues
that are similar in composition. In this paper, we propose a Pseudo-Variational
Gated Recurrent Unit (PVGRU) component without posterior knowledge through
introducing a recurrent summarizing variable into the GRU, which can aggregate
the accumulated distribution variations of subsequences. PVGRU can perceive the
subtle semantic variability through summarizing variables that are optimized by
the devised distribution consistency and reconstruction objectives. In
addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model
based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve
the diversity and relevance of responses on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Summarization Re-ranking. (arXiv:2212.09593v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09593">
<div class="article-summary-box-inner">
<span><p>With the rise of task-specific pre-training objectives, abstractive
summarization models like PEGASUS offer appealing zero-shot performance on
downstream summarization tasks. However, the performance of such unsupervised
models still lags significantly behind their supervised counterparts. Similarly
to the supervised setup, we notice a very high variance in quality among
summary candidates from these models while only one candidate is kept as the
summary output. In this paper, we propose to re-rank summary candidates in an
unsupervised manner, aiming to close the performance gap between unsupervised
and supervised models. Our approach improves the unsupervised PEGASUS by up to
7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted
summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%
from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on
a dataset, evaluating on another).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10002">
<div class="article-summary-box-inner">
<span><p>Recent work in open-domain question answering (ODQA) has shown that
adversarial poisoning of the search collection can cause large drops in
accuracy for production systems. However, little to no work has proposed
methods to defend against these attacks. To do so, we rely on the intuition
that redundant information often exists in large corpora. To find it, we
introduce a method that uses query augmentation to search for a diverse set of
passages that could answer the original question but are less likely to have
been poisoned. We integrate these new passages into the model through the
design of a novel confidence method, comparing the predicted answer to its
appearance in the retrieved contexts (what we call \textit{Confidence from
Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but
effective way to defend against poisoning attacks that provides gains of nearly
20\% exact match across varying levels of data poisoning/knowledge conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10559">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models have shown surprising in-context learning
(ICL) ability. With a few demonstration input-label pairs, they can predict the
label for an unseen input without parameter updates. Despite the great success
in performance, its working mechanism still remains an open question. In this
paper, we explain language models as meta-optimizers and understand in-context
learning as implicit finetuning. Theoretically, we figure out that Transformer
attention has a dual form of gradient descent. On top of it, we understand ICL
as follows: GPT first produces meta-gradients according to the demonstration
examples, and then these meta-gradients are applied to the original GPT to
build an ICL model. We comprehensively compare the behaviors of in-context
learning and explicit finetuning on real tasks to provide empirical evidence
that supports our understanding. Experimental results show that in-context
learning behaves similarly to explicit finetuning from multiple perspectives.
Inspired by the dual form between Transformer attention and gradient descent,
we design a momentum-based attention by analogy with gradient descent with
momentum. The improved performance over vanilla attention further supports our
understanding from another perspective, and more importantly, shows the
potential to utilize our understanding for future model design. The code is
available at \url{https://aka.ms/icl}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontologically Faithful Generation of Non-Player Character Dialogues. (arXiv:2212.10618v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10618">
<div class="article-summary-box-inner">
<span><p>We introduce a language generation task grounded in a popular video game
environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)
requires models to produce trees of dialogue between video game characters that
accurately reflect quest and entity specifications stated in natural language.
KNUDGE is constructed from side quest dialogues drawn directly from game data
of Obsidian Entertainment's The Outer Worlds, leading to real-world
complexities in generation: (1) dialogues are branching trees as opposed to
linear chains of utterances; (2) utterances must remain faithful to the game
lore -- character personas, backstories, and entity relationships; and (3) a
dialogue must accurately reveal new quest details to the human player. We
report results for a set of neural generation models using supervised and
in-context learning techniques; we find competent performance but room for
future work addressing the challenges of creating realistic, game-quality
dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10947">
<div class="article-summary-box-inner">
<span><p>When applied for processing long text, Large Language Models (LLMs) are
limited by their context window. Existing efforts to address this limitation
involve training specialized architectures, and cannot be easily applied to
off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that
alleviates the context window restriction for any off-the-shelf LLM without
further training. The key to the approach is to carve a long context into
chunks (``windows''), restrict the attention mechanism to apply only within
each window, and re-use the positional embeddings across the windows. Our main
results test the PCW approach on in-context learning with models that range in
size between 750 million and 178 billion parameters, and show substantial
improvements for tasks with diverse input and output spaces. We show additional
benefits in other settings where long context windows may be beneficial:
multi-hop questions and retrieval-augmented question answering with multiple
retrieved documents. Our results highlight Parallel Context Windows as a
promising method for applying off-the-shelf LLMs in a range of settings that
require long text sequences. We make our code publicly available at
https://github.com/ai21labs/parallel-context-windows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04312">
<div class="article-summary-box-inner">
<span><p>Word embedding has become ubiquitous and is widely used in various text
mining and natural language processing (NLP) tasks, such as information
retrieval, semantic analysis, and machine translation, among many others.
Unfortunately, it is prohibitively expensive to train the word embedding in a
relatively large corpus. We propose a graph-based word embedding algorithm,
called Word-Graph2vec, which converts the large corpus into a word
co-occurrence graph, then takes the word sequence samples from this graph by
randomly traveling and trains the word embedding on this sampling corpus in the
end. We posit that because of the stable vocabulary, relative idioms, and fixed
expressions in English, the size and density of the word co-occurrence graph
change slightly with the increase in the training corpus. So that
Word-Graph2vec has stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world datasets show
that the proposed algorithm outperforms traditional Skip-Gram by four-five
times in terms of efficiency, while the error generated by the random walk
sampling is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01313">
<div class="article-summary-box-inner">
<span><p>This work provides a formalization of Knowledge Graphs (KGs) as a new class
of graphs that we denote doubly exchangeable attributed graphs, where node and
pairwise (joint 2-node) representations must be equivariant to permutations of
both node ids and edge (&amp; node) attributes (relations &amp; node features).
Double-permutation equivariant KG representations open a new research direction
in KGs. We show that this equivariance imposes a structural representation of
relations that allows neural networks to perform complex logical reasoning
tasks in KGs. Finally, we introduce a general blueprint for such equivariant
representations and test a simple GNN-based double-permutation equivariant
neural architecture that achieve state-of-the-art Hits@10 test accuracy in the
WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately
perform logical reasoning tasks that no existing methods can perform, to the
best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models for Non-autoregressive Text Generation: A Survey. (arXiv:2303.06574v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.06574">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing an improved text generation quality. In this
survey, we review the recent progress in diffusion models for NAR text
generation. As the background, we first present the general definition of
diffusion models and the text diffusion models, and then discuss their merits
for NAR generation. As the core content, we further introduce two mainstream
diffusion models in existing work of text diffusion, and review the key designs
of the diffusion process. Moreover, we discuss the utilization of pre-trained
language models (PLMs) for text diffusion models and introduce optimization
techniques for text data. Finally, we discuss several promising directions and
conclude this paper. Our survey aims to provide researchers with a systematic
reference of related research on text diffusion models for NAR generation. We
present our collection of text diffusion models at
https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Artificial Empathy for Human-Centered Design: A Framework. (arXiv:2303.10583v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10583">
<div class="article-summary-box-inner">
<span><p>In the early stages of the design process, designers explore opportunities by
discovering unmet needs and developing innovative concepts as potential
solutions. From a human-centered design perspective, designers must develop
empathy with people to truly understand their needs. However, developing
empathy is a complex and subjective process that relies heavily on the
designer's empathic capability. Therefore, the development of empathic
understanding is intuitive, and the discovery of underlying needs is often
serendipitous. This paper aims to provide insights from artificial intelligence
research to indicate the future direction of AI-driven human-centered design,
taking into account the essential role of empathy. Specifically, we conduct an
interdisciplinary investigation of research areas such as data-driven user
studies, empathic understanding development, and artificial empathy. Based on
this foundation, we discuss the role that artificial empathy can play in
human-centered design and propose an artificial empathy framework for
human-centered design. Building on the mechanisms behind empathy and insights
from empathic design research, the framework aims to break down the rather
complex and subjective concept of empathy into components and modules that can
potentially be modeled computationally. Furthermore, we discuss the expected
benefits of developing such systems and identify current research gaps to
encourage future research efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12294">
<div class="article-summary-box-inner">
<span><p>Neural network models have been proposed to explain the grapheme-phoneme
mapping process in humans for many alphabet languages. These models not only
successfully learned the correspondence of the letter strings and their
pronunciation, but also captured human behavior in nonce word naming tasks. How
would the neural models perform for a non-alphabet language (e.g., Chinese)
unknown character task? How well would the model capture human behavior? In
this study, we first collect human speakers' answers on unknown character
naming tasks and then evaluate a set of transformer models by comparing their
performances with human behaviors on an unknown Chinese character naming task.
We found that the models and humans behaved very similarly, that they had
similar accuracy distribution for each character, and had a substantial overlap
in answers. In addition, the models' answers are highly correlated with humans'
answers. These results suggested that the transformer models can well capture
human's character naming behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13465">
<div class="article-summary-box-inner">
<span><p>Traditionally, approximate dynamic programming is employed in dialogue
generation with greedy policy improvement through action sampling, as the
natural language action space is vast. However, this practice is inefficient
for reinforcement learning (RL) due to the sparsity of eligible responses with
high action values, which leads to weak improvement sustained by random
sampling. This paper presents theoretical analysis and experiments that reveal
the performance of the dialogue policy is positively correlated with the
sampling size. To overcome this limitation, we introduce a novel
dual-granularity Q-function that explores the most promising response category
to intervene in the sampling process. Our approach extracts actions based on a
grained hierarchy, thereby achieving the optimum with fewer policy iterations.
Additionally, we use offline RL and learn from multiple reward functions
designed to capture emotional nuances in human interactions. Empirical studies
demonstrate that our algorithm outperforms baselines across automatic metrics
and human evaluations. Further testing reveals that our algorithm exhibits both
explainability and controllability and generates responses with higher expected
rewards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Reasoning, A Survey. (arXiv:2303.14725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14725">
<div class="article-summary-box-inner">
<span><p>This survey paper proposes a clearer view of natural language reasoning in
the field of Natural Language Processing (NLP), both conceptually and
practically. Conceptually, we provide a distinct definition for natural
language reasoning in NLP, based on both philosophy and NLP scenarios, discuss
what types of tasks require reasoning, and introduce a taxonomy of reasoning.
Practically, we conduct a comprehensive literature review on natural language
reasoning in NLP, mainly covering classical logical reasoning, natural language
inference, multi-hop question answering, and commonsense reasoning. The paper
also identifies and views backward reasoning, a powerful paradigm for
multi-step reasoning, and introduces defeasible reasoning as one of the most
important future directions in natural language reasoning research. We focus on
single-modality unstructured natural language text, excluding neuro-symbolic
techniques and mathematical reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Prompt Cell: A Portable Control Module for Effective Prompt Tuning. (arXiv:2304.05642v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05642">
<div class="article-summary-box-inner">
<span><p>As a novel approach to tuning pre-trained models, prompt tuning involves
freezing the parameters in downstream tasks while inserting trainable
embeddings into inputs in the first layer. However, previous methods have
mainly focused on the initialization of prompt embeddings. The strategy of
training and utilizing prompt embeddings in a reasonable way has become a
limiting factor in the effectiveness of prompt tuning. To address this issue,
we introduce the Global Prompt Cell (GPC), a portable control module for prompt
tuning that selectively preserves prompt information across all encoder layers.
Our experimental results demonstrate a 5.8% improvement on SuperGLUE datasets
compared to vanilla prompt tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07849">
<div class="article-summary-box-inner">
<span><p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for
digital human applications that instruction finetunes on a wide range of
dialogue tasks in a unified internet-augmented format. Different from other
open-domain dialogue models that focus on large-scale pre-training and scaling
up model size or dialogue corpus, we aim to build a powerful and practical
dialogue system for digital human with diverse skills and good multi-task
generalization by internet-augmented instruction tuning. To this end, we first
conduct large-scale pre-training on both common document corpus and dialogue
data with curriculum learning, so as to inject various world knowledge and
dialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue
tasks spanning diverse features of knowledge, personality, multi-turn memory,
and empathy, on which we further instruction tune \modelname via unified
natural language instruction templates. External knowledge from an internet
search is also used during instruction finetuning for alleviating the problem
of knowledge hallucinations. We show that \modelname outperforms
state-of-the-art Chinese dialogue systems on both automatic and human
evaluation, and demonstrates strong multi-task generalization on a variety of
text understanding and generation tasks. In addition, we deploy \modelname to
real-world applications such as Smart Speaker and Instant Message applications
with fast inference. Our models and code will be made publicly available on
ModelScope: https://modelscope.cn/models/damo/ChatPLUG-3.7B and Github:
https://github.com/X-PLUG/ChatPLUG .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12036">
<div class="article-summary-box-inner">
<span><p>Node representation learning in a network is an important machine learning
technique for encoding relational information in a continuous vector space
while preserving the inherent properties and structures of the network.
Recently, unsupervised node embedding methods such as DeepWalk, LINE,
struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model
and perform better performance in several downstream tasks such as node
classification and link prediction than the existing relational models.
However, providing post-hoc explanations of Skip-gram-based embeddings remains
a challenging problem because of the lack of explanation methods and
theoretical studies applicable for embeddings. In this paper, we first show
that global explanations to the Skip-gram-based embeddings can be found by
computing bridgeness under a spectral cluster-aware local perturbation.
Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD,
is proposed that allows the top-q global explanations about learned graph
embedding vectors more efficiently. Experiments show that the ranking of nodes
by scores using GRAPH-wGD is highly correlated with true bridgeness scores. We
also observe that the top-q node-level explanations selected by GRAPH-wGD have
higher importance scores and produce more changes in class label prediction
when perturbed, compared with the nodes selected by recent alternatives, using
five real-world graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12986">
<div class="article-summary-box-inner">
<span><p>The development of large-scale Chinese language models is flourishing, yet
there is a lack of corresponding capability assessments. Therefore, we propose
a test to measure the multitask accuracy of large Chinese language models. This
test encompasses four major domains, including medicine, law, psychology, and
education, with 15 subtasks in medicine and 8 subtasks in education. We found
that the best-performing models in the zero-shot setting outperformed the
worst-performing models by nearly 18.6 percentage points on average. Across the
four major domains, the highest average zero-shot accuracy of all models is
0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot
accuracy of 0.693 in clinical medicine, which was the highest accuracy among
all models across all subtasks. All models performed poorly in the legal
domain, with the highest zero-shot accuracy reaching only 0.239. By
comprehensively evaluating the breadth and depth of knowledge across multiple
disciplines, this test can more accurately identify the shortcomings of the
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13994">
<div class="article-summary-box-inner">
<span><p>We present SweCTRL-Mini, a large Swedish language model that can be used for
inference and fine-tuning on a single consumer-grade GPU. The model is based on
the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),
which means that users of the SweCTRL-Mini model can control the genre of the
generated text by inserting special tokens in the generation prompts.
SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a
set of Swedish novels. In this article, we provide (1) a detailed account of
the utilized training data and text pre-processing steps, to the extent that it
is possible to check whether a specific phrase/source was a part of the
training data, and (2) an evaluation of the model on both discriminative tasks,
using automatic evaluation methods, and generative tasks, using human referees.
We also compare the generative capabilities of the model with those of GPT-3.
SweCTRL-Mini is fully open and available for download.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00382">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs have shown promise for several cybersecurity tasks, such as
vulnerability assessment and threat analysis. In this work, we present a new
method for constructing a vulnerability knowledge graph from information in the
National Vulnerability Database (NVD). Our approach combines named entity
recognition (NER), relation extraction (RE), and entity prediction using a
combination of neural models, heuristic rules, and knowledge graph embeddings.
We demonstrate how our method helps to fix missing entities in knowledge graphs
used for cybersecurity and evaluate the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00969">
<div class="article-summary-box-inner">
<span><p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries, and the accompanying CryCeleb 2023 task - a public speaker
verification challenge based on infant cry sounds. We release for academic
usage more than 6 hours of manually segmented cry sounds from 786 newborns to
encourage research in infant cry analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03319">
<div class="article-summary-box-inner">
<span><p>Encoding long sequences in Natural Language Processing (NLP) is a challenging
problem. Though recent pretraining language models achieve satisfying
performances in many NLP tasks, they are still restricted by a pre-defined
maximum length, making them challenging to be extended to longer sequences. So
some recent works utilize hierarchies to model long sequences. However, most of
them apply sequential models for upper hierarchies, suffering from long
dependency issues. In this paper, we alleviate these issues through a
graph-based method. We first chunk the sequence with a fixed length to model
the sentence-level information. We then leverage graphs to model intra- and
cross-sentence correlations with a new attention mechanism. Additionally, due
to limited standard benchmarks for long document classification (LDC), we
propose a new challenging benchmark, totaling six datasets with up to 53k
samples and 4034 average tokens' length. Evaluation shows our model surpasses
competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence
dataset. Our method is shown to outperform hierarchical sequential models with
better performance and scalability, especially for longer sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03445">
<div class="article-summary-box-inner">
<span><p>Figurative language is a challenge for language models since its
interpretation is based on the use of words in a way that deviates from their
conventional order and meaning. Yet, humans can easily understand and interpret
metaphors, similes or idioms as they can be derived from embodied metaphors.
Language is a proxy for embodiment and if a metaphor is conventional and
lexicalised, it becomes easier for a system without a body to make sense of
embodied concepts. Yet, the intricate relation between embodiment and features
such as concreteness or age of acquisition has not been studied in the context
of figurative language interpretation concerning language models. Hence, the
presented study shows how larger language models perform better at interpreting
metaphoric sentences when the action of the metaphorical sentence is more
embodied. The analysis rules out multicollinearity with other features (e.g.
word length or concreteness) and provides initial evidence that larger language
models conceptualise embodied concepts to a degree that facilitates figurative
language understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04476">
<div class="article-summary-box-inner">
<span><p>The speech-to-singing (STS) voice conversion task aims to generate singing
samples corresponding to speech recordings while facing a major challenge: the
alignment between the target (singing) pitch contour and the source (speech)
content is difficult to learn in a text-free situation. This paper proposes
AlignSTS, an STS model based on explicit cross-modal alignment, which views
speech variance such as pitch and content as different modalities. Inspired by
the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)
adopts a novel rhythm adaptor to predict the target rhythm representation to
bridge the modality gap between content and pitch, where the rhythm
representation is computed in a simple yet effective way and is quantized into
a discrete space; and 2) uses the predicted rhythm representation to re-align
the content based on cross-attention and conducts a cross-modal fusion for
re-synthesize. Extensive experiments show that AlignSTS achieves superior
performance in terms of both objective and subjective metrics. Audio samples
are available at https://alignsts.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset. (arXiv:2305.04582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04582">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) is a fundamental task in information extraction,
whose extension to multilingual settings has been hindered by the lack of
supervised resources comparable in size to large English datasets such as
TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED
dataset, covering 12 typologically diverse languages from 9 language families,
which is created by machine-translating TACRED instances and automatically
projecting their entity annotations. We analyze translation and annotation
projection quality, identify error categories, and experimentally evaluate
fine-tuned pretrained mono- and multilingual language models in common transfer
learning scenarios. Our analyses show that machine translation is a viable
strategy to transfer RE instances, with native speakers judging more than 83%
of the translated instances to be linguistically and semantically acceptable.
We find monolingual RE model performance to be comparable to the English
original for many of the target languages, and that multilingual models trained
on a combination of English and target language data can outperform their
monolingual counterparts. However, we also observe a variety of translation and
annotation projection errors, both due to the MT systems and linguistic
features of the target languages, such as pronoun-dropping, compounding and
inflection, that degrade dataset quality and RE model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04928">
<div class="article-summary-box-inner">
<span><p>Supervised named entity recognition (NER) in the biomedical domain is
dependent on large sets of annotated texts with the given named entities, whose
creation can be time-consuming and expensive. Furthermore, the extraction of
new entities often requires conducting additional annotation tasks and
retraining the model. To address these challenges, this paper proposes a
transformer-based method for zero- and few-shot NER in the biomedical domain.
The method is based on transforming the task of multi-class token
classification into binary token classification (token contains the searched
entity or does not contain the searched entity) and pre-training on a larger
amount of datasets and biomedical entities, from where the method can learn
semantic relations between the given and potential classes. We have achieved
average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94%
for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical
entities with PubMedBERT fine-tuned model. The results demonstrate the
effectiveness of the proposed method for recognizing new entities with limited
examples, with comparable or better results from the state-of-the-art zero- and
few-shot NER methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANALOGICAL -- A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05050">
<div class="article-summary-box-inner">
<span><p>Over the past decade, analogies, in the form of word-level analogies, have
played a significant role as an intrinsic measure of evaluating the quality of
word embedding methods such as word2vec. Modern large language models (LLMs),
however, are primarily evaluated on extrinsic measures based on benchmarks such
as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs
can draw analogies between long texts. In this paper, we present ANALOGICAL, a
new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of
long text with six levels of complexity -- (i) word, (ii) word vs. sentence,
(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using
thirteen datasets and three different distance measures, we evaluate the
abilities of eight LLMs in identifying analogical pairs in the semantic vector
space. Our evaluation finds that it is increasingly challenging for LLMs to
identify analogies when going up the analogy taxonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05280">
<div class="article-summary-box-inner">
<span><p>Compared to news and chat summarization, the development of meeting
summarization is hugely decelerated by the limited data. To this end, we
introduce a versatile Chinese meeting summarization dataset, dubbed VCSum,
consisting of 239 real-life meetings, with a total duration of over 230 hours.
We claim our dataset is versatile because we provide the annotations of topic
segmentation, headlines, segmentation summaries, overall meeting summaries, and
salient sentences for each meeting transcript. As such, the dataset can adapt
to various summarization tasks or methods, including segmentation-based
summarization, multi-granularity summarization and retrieval-then-generate
summarization. Our analysis confirms the effectiveness and robustness of VCSum.
We also provide a set of benchmark models regarding different downstream
summarization tasks on VCSum to facilitate further research. The dataset and
code will be released at https://github.com/hahahawu/VCSum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05471">
<div class="article-summary-box-inner">
<span><p>With the recent advances in natural language processing (NLP), a vast number
of applications have emerged across various use cases. Among the plethora of
NLP applications, many academic researchers are motivated to do work that has a
positive social impact, in line with the recent initiatives of NLP for Social
Good (NLP4SG). However, it is not always obvious to researchers how their
research efforts are tackling today's big social problems. Thus, in this paper,
we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks
that can help identify NLP4SG papers and characterize the NLP4SG landscape by:
(1) identifying the papers that address a social problem, (2) mapping them to
the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying
the task they are solving and the methods they are using. Using
state-of-the-art NLP models, we address each of these tasks and use them on the
entire ACL Anthology, resulting in a visualization workspace that gives
researchers a comprehensive overview of the field of NLP4SG. Our website is
available at https://nlp4sg.vercel.app . We released our data at
https://huggingface.co/datasets/feradauto/NLP4SGPapers and code at
https://github.com/feradauto/nlp4sg .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05976">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been widely studied for their ability to
store and utilize positive knowledge. However, negative knowledge, such as
"lions don't live in the ocean", is also ubiquitous in the world but rarely
mentioned explicitly in the text. What do LLMs know about negative knowledge?
This work examines the ability of LLMs to negative commonsense knowledge. We
design a constrained keywords-to-sentence generation task (CG) and a Boolean
question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs
frequently fail to generate valid sentences grounded in negative commonsense
knowledge, yet they can correctly answer polar yes-or-no questions. We term
this phenomenon the belief conflict of LLMs. Our further analysis shows that
statistical shortcuts and negation reporting bias from language modeling
pre-training cause this conflict.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06569">
<div class="article-summary-box-inner">
<span><p>Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06754">
<div class="article-summary-box-inner">
<span><p>Transformer architectures are complex and their use in NLP, while it has
engendered many successes, makes their interpretability or explainability
challenging. Recent debates have shown that attention maps and attribution
methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this
paper, we present some of their limitations and introduce COCKATIEL, which
successfully addresses some of them. COCKATIEL is a novel, post-hoc,
concept-based, model-agnostic XAI technique that generates meaningful
explanations from the last layer of a neural net model trained on an NLP
classification task by using Non-Negative Matrix Factorization (NMF) to
discover the concepts the model leverages to make predictions and by exploiting
a Sensitivity Analysis to estimate accurately the importance of each of these
concepts for the model. It does so without compromising the accuracy of the
underlying model or requiring a new one to be trained. We conduct experiments
in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's
superior ability to discover concepts that align with humans' on Transformer
models without any supervision, we objectively verify the faithfulness of its
explanations through fidelity metrics, and we showcase its ability to provide
meaningful explanations in two different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06984">
<div class="article-summary-box-inner">
<span><p>Lexical matching remains the de facto evaluation method for open-domain
question answering (QA). Unfortunately, lexical matching fails completely when
a plausible candidate answer does not appear in the list of gold answers, which
is increasingly the case as we shift from extractive to generative models. The
recent success of large language models (LLMs) for QA aggravates lexical
matching failures since candidate answers become longer, thereby making
matching with the gold answers even more challenging. Without accurate
evaluation, the true progress in open-domain QA remains unknown. In this paper,
we conduct a thorough analysis of various open-domain QA models, including
LLMs, by manually evaluating their answers on a subset of NQ-open, a popular
benchmark. Our assessments reveal that while the true performance of all models
is significantly underestimated, the performance of the InstructGPT (zero-shot)
LLM increases by nearly +60%, making it on par with existing top models, and
the InstructGPT (few-shot) model actually achieves a new state-of-the-art on
NQ-open. We also find that more than 50% of lexical matching failures are
attributed to semantically equivalent answers. We further demonstrate that
regex matching ranks QA models consistent with human judgments, although still
suffering from unnecessary strictness. Finally, we demonstrate that automated
evaluation models are a reasonable surrogate for lexical matching in some
circumstances, but not for long-form answers generated by LLMs. The automated
models struggle in detecting hallucinations in LLM answers and are thus unable
to evaluate LLMs. At this time, there appears to be no substitute for human
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07375">
<div class="article-summary-box-inner">
<span><p>Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT's upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-16 23:10:56.220654194 UTC">2023-05-16 23:10:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>