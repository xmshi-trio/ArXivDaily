<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-11-02T01:30:00Z">11-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00047">
<div class="article-summary-box-inner">
<span><p>Vision-Language Models (VLMs) are trained on vast amounts of data captured by
humans emulating our understanding of the world. However, known as visual
illusions, human's perception of reality isn't always faithful to the physical
world. This raises a key question: do VLMs have the similar kind of illusions
as humans do, or do they faithfully learn to represent reality? To investigate
this question, we build a dataset containing five types of visual illusions and
formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our
findings have shown that although the overall alignment is low, larger models
are closer to human perception and more susceptible to visual illusions. Our
dataset and initial findings will promote a better understanding of visual
illusions in humans and machines and provide a stepping stone for future
computational models that can better align humans and machines in perceiving
and communicating about the shared visual world. The code and data are
available at https://github.com/vl-illusion/dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00059">
<div class="article-summary-box-inner">
<span><p>The recent wave of generative AI has sparked unprecedented global attention,
with both excitement and concern over potentially superhuman levels of
artificial intelligence: models now take only seconds to produce outputs that
would challenge or exceed the capabilities even of expert humans. At the same
time, models still show basic errors in understanding that would not be
expected even in non-expert humans. This presents us with an apparent paradox:
how do we reconcile seemingly superhuman capabilities with the persistence of
errors that few humans would make? In this work, we posit that this tension
reflects a divergence in the configuration of intelligence in today's
generative models relative to intelligence in humans. Specifically, we propose
and test the Generative AI Paradox hypothesis: generative models, having been
trained directly to reproduce expert-like outputs, acquire generative
capabilities that are not contingent upon -- and can therefore exceed -- their
ability to understand those same types of outputs. This contrasts with humans,
for whom basic understanding almost always precedes the ability to generate
expert-level outputs. We test this hypothesis through controlled experiments
analyzing generation vs. understanding in generative models, across both
language and image modalities. Our results show that although models can
outperform humans in generation, they consistently fall short of human
capabilities in measures of understanding, as well as weaker correlation
between generation and understanding performance, and more brittleness to
adversarial inputs. Our findings support the hypothesis that models' generative
capability may not be contingent upon understanding capability, and call for
caution in interpreting artificial intelligence by analogy to human
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00116">
<div class="article-summary-box-inner">
<span><p>Real-world NLP applications often deal with nonstandard text (e.g.,
dialectal, informal, or misspelled text). However, language models like BERT
deteriorate in the face of dialect variation or noise. How do we push BERT's
modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it
is designed for specializing a model to a task and does not seem to bring about
the deeper, more pervasive changes needed to adapt a model to nonstandard
language. In this paper, we introduce the novel idea of sandwiching BERT's
encoder stack between additional encoder layers trained to perform masked
language modeling on noisy text. We find that our approach, paired with recent
work on including character-level noise in fine-tuning data, can promote
zero-shot transfer to dialectal text, as well as reduce the distance in the
embedding space between words and their noisy counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00117">
<div class="article-summary-box-inner">
<span><p>Llama 2-Chat is a collection of large language models that Meta developed and
released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output
harmful content, we hypothesize that public access to model weights enables bad
actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's
capabilities for malicious purposes. We demonstrate that it is possible to
effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than
$200, while retaining its general capabilities. Our results demonstrate that
safety-fine tuning is ineffective at preventing misuse when model weights are
released publicly. Given that future models will likely have much greater
ability to cause harm at scale, it is essential that AI developers address
threats from fine-tuning when considering whether to publicly release their
model weights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00128">
<div class="article-summary-box-inner">
<span><p>This work explores the degree to which grammar acquisition is driven by
language `simplicity' and the source modality (speech vs. text) of data. Using
BabyBERTa as a probe, we find that grammar acquisition is largely driven by
exposure to speech data, and in particular through exposure to two of the
BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this
finding by examining various ways of presenting input data to our model. First,
we assess the impact of various sequence-level complexity based curricula. We
then examine the impact of learning over `blocks' -- covering spans of text
that are balanced for the number of tokens in each of the source corpora
(rather than number of lines). Finally, we explore curricula that vary the
degree to which the model is exposed to different corpora. In all cases, we
find that over-exposure to AO-Childes and Open Subtitles significantly drives
performance. We verify these findings through a comparable control dataset in
which exposure to these corpora, and speech more generally, is limited by
design. Our findings indicate that it is not the proportion of tokens occupied
by high-utility data that aids acquisition, but rather the proportion of
training steps assigned to such data. We hope this encourages future research
into the use of more developmentally plausible linguistic data (which tends to
be more scarce) to augment general purpose pre-training regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00143">
<div class="article-summary-box-inner">
<span><p>In elections around the world, the candidates may turn their campaigns toward
negativity due to the prospect of failure and time pressure. In the digital
age, social media platforms such as Twitter are rich sources of political
discourse. Therefore, despite the large amount of data that is published on
Twitter, the automatic system for campaign negativity detection can play an
essential role in understanding the strategy of candidates and parties in their
campaigns. In this paper, we propose a hybrid model for detecting campaign
negativity consisting of a two-stage classifier that combines the strengths of
two machine learning models. Here, we have collected Persian tweets from 50
political users, including candidates and government officials. Then we
annotated 5,100 of them that were published during the year before the 2021
presidential election in Iran. In the proposed model, first, the required
datasets of two classifiers based on the cosine similarity of tweet embeddings
with axis embeddings (which are the average of embedding in positive and
negative classes of tweets) from the training set (85\%) are made, and then
these datasets are considered the training set of the two classifiers in the
hybrid model. Finally, our best model (RF-RF) was able to achieve 79\% for the
macro F1 score and 82\% for the weighted F1 score. By running the best model on
the rest of the tweets of 50 political users that were published one year
before the election and with the help of statistical models, we find that the
publication of a tweet by a candidate has nothing to do with the negativity of
that tweet, and the presence of the names of political persons and political
organizations in the tweet is directly related to its negativity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks. (arXiv:2311.00159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00159">
<div class="article-summary-box-inner">
<span><p>Humans read texts at a varying pace, while machine learning models treat each
token in the same way in terms of a computational process. Therefore, we ask,
does it help to make models act more like humans? In this paper, we convert
this intuition into a set of novel models with fixation-guided parallel RNNs or
layers and conduct various experiments on language modeling and sentiment
analysis tasks to test their effectiveness, thus providing empirical validation
for this intuition. Our proposed models achieve good performance on the
language modeling task, considerably surpassing the baseline model. In
addition, we find that, interestingly, the fixation duration predicted by
neural networks bears some resemblance to humans' fixation. Without any
explicit guidance, the model makes similar choices to humans. We also
investigate the reasons for the differences between them, which explain why
"model fixations" are often more suitable than human fixations, when used to
guide language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. (arXiv:2311.00161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00161">
<div class="article-summary-box-inner">
<span><p>Counterspeech, i.e., responses to counteract potential harms of hateful
speech, has become an increasingly popular solution to address online hate
speech without censorship. However, properly countering hateful language
requires countering and dispelling the underlying inaccurate stereotypes
implied by such language. In this work, we draw from psychology and philosophy
literature to craft six psychologically inspired strategies to challenge the
underlying stereotypical implications of hateful language. We first examine the
convincingness of each of these strategies through a user study, and then
compare their usages in both human- and machine-generated counterspeech
datasets. Our results show that human-written counterspeech uses countering
strategies that are more specific to the implied stereotype (e.g., counter
examples to the stereotype, external factors about the stereotype's origins),
whereas machine-generated counterspeech uses less specific strategies (e.g.,
generally denouncing the hatefulness of speech). Furthermore, machine-generated
counterspeech often employs strategies that humans deem less convincing
compared to human-produced counterspeech. Our findings point to the importance
of accounting for the underlying stereotypical implications of speech when
generating counterspeech and for better machine reasoning about
anti-stereotypical examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00172">
<div class="article-summary-box-inner">
<span><p>Large Language Models' safety remains a critical concern due to their
vulnerability to adversarial attacks, which can prompt these systems to produce
harmful responses. In the heart of these systems lies a safety classifier, a
computational model trained to discern and mitigate potentially harmful,
offensive, or unethical outputs. However, contemporary safety classifiers,
despite their potential, often fail when exposed to inputs infused with
adversarial noise. In response, our study introduces the Adversarial Prompt
Shield (APS), a lightweight model that excels in detection accuracy and
demonstrates resilience against adversarial prompts. Additionally, we propose
novel strategies for autonomously generating adversarial training datasets,
named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are
designed to fortify the safety classifier's robustness, and we investigate the
consequences of incorporating adversarial examples into the training process.
Through evaluations involving Large Language Models, we demonstrate that our
classifier has the potential to decrease the attack success rate resulting from
adversarial attacks by up to 60%. This advancement paves the way for the next
generation of more reliable and resilient conversational agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00176">
<div class="article-summary-box-inner">
<span><p>ChipNeMo aims to explore the applications of large language models (LLMs) for
industrial chip design. Instead of directly deploying off-the-shelf commercial
or open-source LLMs, we instead adopt the following domain adaptation
techniques: custom tokenizers, domain-adaptive continued pretraining,
supervised fine-tuning (SFT) with domain-specific instructions, and
domain-adapted retrieval models. We evaluate these methods on three selected
LLM applications for chip design: an engineering assistant chatbot, EDA script
generation, and bug summarization and analysis. Our results show that these
domain adaptation techniques enable significant LLM performance improvements
over general-purpose base models across the three evaluated applications,
enabling up to 5x model size reduction with similar or better performance on a
range of design tasks. Our findings also indicate that there's still room for
improvement between our current results and ideal outcomes. We believe that
further investigation of domain-adapted LLM approaches will help close this gap
in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00189">
<div class="article-summary-box-inner">
<span><p>Text classification aims to effectively categorize documents into pre-defined
categories. Traditional methods for text classification often rely on large
amounts of manually annotated training data, making the process time-consuming
and labor-intensive. To address this issue, recent studies have focused on
weakly-supervised and extremely weakly-supervised settings, which require
minimal or no human annotation, respectively. In previous methods of weakly
supervised text classification, pseudo-training data is generated by assigning
pseudo-labels to documents based on their alignment (e.g., keyword matching)
with specific classes. However, these methods ignore the importance of
incorporating the explanations of the generated pseudo-labels, or saliency of
individual words, as additional guidance during the text classification
training process. To address this limitation, we propose XAI-CLASS, a novel
explanation-enhanced extremely weakly-supervised text classification method
that incorporates word saliency prediction as an auxiliary task. XAI-CLASS
begins by employing a multi-round question-answering process to generate
pseudo-training data that promotes the mutual enhancement of class labels and
corresponding explanation word generation. This pseudo-training data is then
used to train a multi-task framework that simultaneously learns both text
classification and word saliency prediction. Extensive experiments on several
weakly-supervised text classification datasets show that XAI-CLASS outperforms
other weakly-supervised text classification methods significantly. Moreover,
experiments demonstrate that XAI-CLASS enhances both model performance and
explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00204">
<div class="article-summary-box-inner">
<span><p>Large language models exhibit promising general capabilities but often lack
specialized knowledge for domain-specific tasks. Developing domain experts from
a base model enables a range of applications without prohibitive training
costs. This work demonstrates a method using continuous training and
instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese
medical domain. We first conduct continuous training on 1B tokens from Chinese
medical references to teach relevant vocabulary and knowledge. The models are
then fine-tuned on 54K examples sourced from the Chinese National Medical
Licensing Examination. Experiments on Chinese medical data confirm the
effectiveness of this approach, producing a model comparable to GPT-3.5-turbo
while using way less computational resource. The resulting domain-specific
model could be useful for various Chinese medical applications. More broadly,
this provides a template for domain-specific training of large language models
in areas where pre-trained models lack the required expertise, such as law,
science, and engineering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00208">
<div class="article-summary-box-inner">
<span><p>As transformers have gained prominence in natural language processing, some
researchers have investigated theoretically what problems they can and cannot
solve, by treating problems as formal languages. Exploring questions such as
this will help to compare transformers with other models, and transformer
variants with one another, for various tasks. Work in this subarea has made
considerable progress in recent years. Here, we undertake a comprehensive
survey of this work, documenting the diverse assumptions that underlie
different results and providing a unified framework for harmonizing seemingly
contradictory findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT Powerful Enough to Analyze the Emotions of Memes?. (arXiv:2311.00223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00223">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), representing a significant achievement in
artificial intelligence (AI) research, have demonstrated their ability in a
multitude of tasks. This project aims to explore the capabilities of GPT-3.5, a
leading example of LLMs, in processing the sentiment analysis of Internet
memes. Memes, which include both verbal and visual aspects, act as a powerful
yet complex tool for expressing ideas and sentiments, demanding an
understanding of societal norms and cultural contexts. Notably, the detection
and moderation of hateful memes pose a significant challenge due to their
implicit offensive nature. This project investigates GPT's proficiency in such
subjective tasks, revealing its strengths and potential limitations. The tasks
include the classification of meme sentiment, determination of humor type, and
detection of implicit hate in memes. The performance evaluation, using datasets
from SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative
understanding of GPT responses against human annotations. Despite GPT's
remarkable progress, our findings underscore the challenges faced by these
models in handling subjective tasks, which are rooted in their inherent
limitations including contextual understanding, interpretation of implicit
meanings, and data biases. This research contributes to the broader discourse
on the applicability of AI in handling complex, context-dependent tasks, and
offers valuable insights for future advancements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00233">
<div class="article-summary-box-inner">
<span><p>While instruction-tuned language models have demonstrated impressive
zero-shot generalization, these models often struggle to generate accurate
responses when faced with instructions that fall outside their training set.
This paper presents Instructive Decoding (ID), a simple yet effective approach
that augments the efficacy of instruction-tuned models. Specifically, ID
adjusts the logits for next-token prediction in a contrastive manner, utilizing
predictions generated from a manipulated version of the original instruction,
referred to as a noisy instruction. This noisy instruction aims to elicit
responses that could diverge from the intended instruction yet remain
plausible. We conduct experiments across a spectrum of such noisy instructions,
ranging from those that insert semantic noise via random words to others like
'opposite' that elicit the deviated responses. Our approach achieves
considerable performance gains across various instruction-tuned models and
tasks without necessitating any additional parameter updates. Notably,
utilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum
divergence from the original instruction, consistently produces the most
significant performance gains across multiple models and tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00237">
<div class="article-summary-box-inner">
<span><p>Understanding emergent abilities, such as in-context learning (ICL) and
chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost
importance. This importance stems not only from the better utilization of these
capabilities across various tasks, but also from the proactive identification
and mitigation of potential risks, including concerns of truthfulness, bias,
and toxicity, that may arise alongside these capabilities. In this paper, we
present a thorough survey on the interpretation and analysis of emergent
abilities of LLMs. First, we provide a concise introduction to the background
and definition of emergent abilities. Then, we give an overview of advancements
from two perspectives: 1) a macro perspective, emphasizing studies on the
mechanistic interpretability and delving into the mathematical foundations
behind emergent abilities; and 2) a micro-perspective, concerning studies that
focus on empirical interpretability by examining factors associated with these
abilities. We conclude by highlighting the challenges encountered and
suggesting potential avenues for future research. We believe that our work
establishes the basis for further exploration into the interpretation of
emergent abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00258">
<div class="article-summary-box-inner">
<span><p>Recent advances in prompt engineering enable large language models (LLMs) to
solve multi-hop logical reasoning problems with impressive accuracy. However,
there is little existing work investigating the robustness of LLMs with
few-shot prompting techniques. Therefore, we introduce a systematic approach to
test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic
perturbations. We include perturbations at multiple levels of abstractions
(e.g. lexical perturbations such as typos, and semantic perturbations such as
the inclusion of intermediate reasoning steps in the questions) to conduct
behavioral analysis on the LLMs. Throughout our experiments, we find that
models are more sensitive to certain perturbations such as replacing words with
their synonyms. We also demonstrate that increasing the proportion of perturbed
exemplars in the prompts improves the robustness of few-shot prompting methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00262">
<div class="article-summary-box-inner">
<span><p>Proactive dialogues serve as a practical yet challenging dialogue problem in
the era of large language models (LLMs), where the dialogue policy planning is
the key to improving the proactivity of LLMs. Most existing studies enable the
dialogue policy planning of LLMs using various prompting schemes or iteratively
enhance this capability in handling the given case with verbal AI feedback.
However, these approaches are either bounded by the policy planning capability
of the frozen LLMs or hard to be transferred to new cases. In this work, we
introduce a new dialogue policy planning paradigm to strategize LLMs for
proactive dialogue problems with a tunable language model plug-in as a
plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a
novel training framework to facilitate supervised fine-tuning over available
human-annotated data as well as reinforcement learning from goal-oriented AI
feedback with dynamic interaction data collected by the LLM-based self-play
simulation. In this manner, the LLM-powered dialogue agent can not only be
generalized to different cases after the training, but also be applicable to
different applications by just substituting the learned plug-in. In addition,
we propose to evaluate the policy planning capability of dialogue systems under
the interactive setting. Experimental results demonstrate that PPDPP
consistently and substantially outperforms existing approaches on three
different proactive dialogue applications, including negotiation, emotional
support, and tutoring dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00268">
<div class="article-summary-box-inner">
<span><p>A line of work on Transformer-based language models such as BERT has
attempted to use syntactic inductive bias to enhance the pretraining process,
on the theory that building syntactic structure into the training process
should reduce the amount of data needed for training. But such methods are
often tested for high-resource languages such as English. In this work, we
investigate whether these methods can compensate for data sparseness in
low-resource languages, hypothesizing that they ought to be more effective for
low-resource languages. We experiment with five low-resource languages: Uyghur,
Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic
inductive bias methods produce uneven results in low-resource settings, and
provide surprisingly little benefit in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00273">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been widely applied in various fields due
to their excellent capability for memorizing knowledge and chain of thought
(CoT). When these language models are applied in the field of psychological
counseling, they often rush to provide universal advice. However, when users
seek psychological support, they need to gain empathy, trust, understanding and
comfort, rather than just reasonable advice. To this end, we constructed a
multi-turn empathetic conversation dataset of more than 2 million samples, in
which the input is the multi-turn conversation context, and the target is
empathetic responses that cover expressions such as questioning, comfort,
recognition, listening, trust, emotional support, etc. Experiments have shown
that the empathy ability of LLMs can be significantly enhanced when finetuning
by using multi-turn dialogue history and responses that are closer to the
expression of a psychological consultant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00286">
<div class="article-summary-box-inner">
<span><p>In this paper, we present \textit{JADE}, a targeted linguistic fuzzing
platform which strengthens the linguistic complexity of seed questions to
simultaneously and consistently break a wide range of widely-used LLMs
categorized in three groups: eight open-sourced Chinese, six commercial Chinese
and four commercial English LLMs. JADE generates three safety benchmarks for
the three groups of LLMs, which contain unsafe questions that are highly
threatening: the questions simultaneously trigger harmful generation of
multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$}
(please see the table below), while are still natural questions, fluent and
preserving the core unsafe semantics. We release the benchmark demos generated
for commercial English LLMs and open-sourced English LLMs in the following
link: https://github.com/whitzard-ai/jade-db. For readers who are interested in
evaluating on more questions generated by JADE, please contact us.
</p>
<p>\textit{JADE} is based on Noam Chomsky's seminal theory of
transformational-generative grammar. Given a seed question with unsafe
intention, \textit{JADE} invokes a sequence of generative and transformational
rules to increment the complexity of the syntactic structure of the original
question, until the safety guardrail is broken. Our key insight is: Due to the
complexity of human language, most of the current best LLMs can hardly
recognize the invariant evil from the infinite number of different syntactic
structures which form an unbound example space that can never be fully covered.
Technically, the generative/transformative rules are constructed by native
speakers of the languages, and, once developed, can be used to automatically
grow and transform the parse tree of a given question, until the guardrail is
broken. For more evaluation results and demo, please check our website:
https://whitzard-ai.github.io/jade.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00287">
<div class="article-summary-box-inner">
<span><p>Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. We will publish our
code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00288">
<div class="article-summary-box-inner">
<span><p>Instruction tuning (IT) achieves impressive zero-shot generalization results
by training large language models (LLMs) on a massive amount of diverse tasks
with instructions. However, how to select new tasks to improve the performance
and generalizability of IT models remains an open question. Training on all
existing tasks is impractical due to prohibiting computation requirements, and
randomly selecting tasks can lead to suboptimal performance. In this work, we
propose active instruction tuning based on prompt uncertainty, a novel
framework to identify informative tasks, and then actively tune the models on
the selected tasks. We represent the informativeness of new tasks with the
disagreement of the current model outputs over perturbed prompts. Our
experiments on NIV2 and Self-Instruct datasets demonstrate that our method
consistently outperforms other baseline strategies for task selection,
achieving better out-of-distribution generalization with fewer training tasks.
Additionally, we introduce a task map that categorizes and diagnoses tasks
based on prompt uncertainty and prediction probability. We discover that
training on ambiguous (prompt-uncertain) tasks improves generalization while
training on difficult (prompt-certain and low-probability) tasks offers no
benefit, underscoring the importance of task selection for instruction tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models. (arXiv:2311.00292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00292">
<div class="article-summary-box-inner">
<span><p>As commonly-used methods for debiasing natural language understanding (NLU)
models, dataset refinement approaches heavily rely on manual data analysis, and
thus maybe unable to cover all the potential biased features. In this paper, we
propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which
debiases NLU models without predefining biased features. We maintain an
iteratively expanded sample pool. Specifically, at each iteration, we first
train a shallow model to quantify the bias degree of samples in the pool. Then,
we pair each sample with a bias indicator representing its bias degree, and use
these extended samples to train a sample generator. In this way, this generator
can effectively learn the correspondence relationship between bias indicators
and samples. Furthermore, we employ the generator to produce pseudo samples
with fewer biased features by feeding specific bias indicators. Finally, we
incorporate the generated pseudo samples into the pool. Experimental results
and in-depth analyses on two NLU tasks show that IBADR not only significantly
outperforms existing dataset refinement approaches, achieving SOTA, but also is
compatible with model-centric methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Representation Learning of Scientific Literature based on Adaptive Feature and Graph Neural Network. (arXiv:2311.00296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00296">
<div class="article-summary-box-inner">
<span><p>Because most of the scientific literature data is unmarked, it makes semantic
representation learning based on unsupervised graph become crucial. At the same
time, in order to enrich the features of scientific literature, a learning
method of semantic representation of scientific literature based on adaptive
features and graph neural network is proposed. By introducing the adaptive
feature method, the features of scientific literature are considered globally
and locally. The graph attention mechanism is used to sum the features of
scientific literature with citation relationship, and give each scientific
literature different feature weights, so as to better express the correlation
between the features of different scientific literature. In addition, an
unsupervised graph neural network semantic representation learning method is
proposed. By comparing the mutual information between the positive and negative
local semantic representation of scientific literature and the global graph
semantic representation in the potential space, the graph neural network can
capture the local and global information, thus improving the learning ability
of the semantic representation of scientific literature. The experimental
results show that the proposed learning method of semantic representation of
scientific literature based on adaptive feature and graph neural network is
competitive on the basis of scientific literature classification, and has
achieved good results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion. (arXiv:2311.00300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00300">
<div class="article-summary-box-inner">
<span><p>The entity alignment of science and technology patents aims to link the
equivalent entities in the knowledge graph of different science and technology
patent data sources. Most entity alignment methods only use graph neural
network to obtain the embedding of graph structure or use attribute text
description to obtain semantic representation, ignoring the process of
multi-information fusion in science and technology patents. In order to make
use of the graphic structure and auxiliary information such as the name,
description and attribute of the patent entity, this paper proposes an entity
alignment method based on the graph convolution network for science and
technology patent information fusion. Through the graph convolution network and
BERT model, the structure information and entity attribute information of the
science and technology patent knowledge graph are embedded and represented to
achieve multi-information fusion, thus improving the performance of entity
alignment. Experiments on three benchmark data sets show that the proposed
method Hit@K The evaluation indicators are better than the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Syllable-Level Pronunciation Stress with A Self-Attention Model. (arXiv:2311.00301v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00301">
<div class="article-summary-box-inner">
<span><p>One precondition of effective oral communication is that words should be
pronounced clearly, especially for non-native speakers. Word stress is the key
to clear and correct English, and misplacement of syllable stress may lead to
misunderstandings. Thus, knowing the stress level is important for English
speakers and learners. This paper presents a self-attention model to identify
the stress level for each syllable of spoken English. Various prosodic and
categorical features, including the pitch level, intensity, duration and type
of the syllable and its nuclei (the vowel of the syllable), are explored. These
features are input to the self-attention model, and syllable-level stresses are
predicted. The simplest model yields an accuracy of over 88% and 93% on
different datasets, while more advanced models provide higher accuracy. Our
study suggests that the self-attention model can be promising in stress-level
detection. These models could be applied to various scenarios, such as online
meetings and English learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00306">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) can generate biased and toxic responses. Yet
most prior work on LLM gender bias evaluation requires predefined
gender-related phrases or gender stereotypes, which are challenging to be
comprehensively collected and are limited to explicit bias evaluation. In
addition, we believe that instances devoid of gender-related language or
explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in
this work, we propose a conditional text generation mechanism without the need
for predefined gender phrases and stereotypes. This approach employs three
types of inputs generated through three distinct strategies to probe LLMs,
aiming to show evidence of explicit and implicit gender biases in LLMs. We also
utilize explicit and implicit evaluation metrics to evaluate gender bias in
LLMs under different strategies. Our experiments demonstrate that an increased
model size does not consistently lead to enhanced fairness and all tested LLMs
exhibit explicit and/or implicit gender bias, even when explicit gender
stereotypes are absent in the inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Lexical Simplification with Context Augmentation. (arXiv:2311.00310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00310">
<div class="article-summary-box-inner">
<span><p>We propose a new unsupervised lexical simplification method that uses only
monolingual data and pre-trained language models. Given a target word and its
context, our method generates substitutes based on the target context and also
additional contexts sampled from monolingual data. We conduct experiments in
English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that
our model substantially outperforms other unsupervised systems across all
languages. We also establish a new state-of-the-art by ensembling our model
with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution
data set, achieving a state-of-the-art result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00317">
<div class="article-summary-box-inner">
<span><p>One major challenge of translating code between programming languages is that
parallel training data is often limited. To overcome this challenge, we present
two data augmentation techniques, one that builds comparable corpora (i.e.,
code pairs with similar functionality), and another that augments existing
parallel data with multiple reference translations. Specifically, we build and
analyze multiple types of comparable corpora, including programs generated from
natural language documentation using a code generation model. Furthermore, to
reduce overfitting to a single reference translation, we automatically generate
additional translation references for available parallel data and filter the
translations by unit tests, which increases variation in target translations.
Experiments show that our data augmentation techniques significantly improve
CodeT5 for translation between Java, Python, and C++ by an average of 7.5%
Computational Accuracy (CA@1), which verifies the correctness of translations
by execution. The code is available at https://github.com/Veronicium/CMTrans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00321">
<div class="article-summary-box-inner">
<span><p>With the proliferation of social media, accurate detection of hate speech has
become critical to ensure safety online. To combat nuanced forms of hate
speech, it is important to identify and thoroughly explain hate speech to help
users understand its harmful effects. Recent benchmarks have attempted to
tackle this issue by training generative models on free-text annotations of
implications in hateful text. However, we find significant reasoning gaps in
the existing annotations schemes, which may hinder the supervision of detection
models. In this paper, we introduce a hate speech detection framework, HARE,
which harnesses the reasoning capabilities of large language models (LLMs) to
fill these gaps in explanations of hate speech, thus enabling effective
supervision of detection models. Experiments on SBIC and Implicit Hate
benchmarks show that our method, using model-generated data, consistently
outperforms baselines, using existing free-text human annotations. Analysis
demonstrates that our method enhances the explanation quality of trained models
and improves generalization to unseen datasets. Our code is available at
https://github.com/joonkeekim/hare-hate-speech.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00367">
<div class="article-summary-box-inner">
<span><p>Implicit Discourse Relation Recognition (IDRR), which infers discourse
relations without the help of explicit connectives, is still a crucial and
challenging task for discourse parsing. Recent works tend to exploit the
hierarchical structure information from the annotated senses, which demonstrate
enhanced discourse relation representations can be obtained by integrating
sense hierarchy. Nevertheless, the performance and robustness for IDRR are
significantly constrained by the availability of annotated data. Fortunately,
there is a wealth of unannotated utterances with explicit connectives, that can
be utilized to acquire enriched discourse relation features. In light of such
motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)
method for IDRR. Essentially, our method seamlessly injects knowledge relevant
to discourse relation into pre-trained language models through prompt-based
connective prediction. Furthermore, considering the prompt-based connective
prediction exhibits local dependencies due to the deficiency of masked language
model (MLM) in capturing global semantics, we design a novel self-supervised
learning objective based on mutual information maximization to derive enhanced
representations of logical semantics for IDRR. Experimental results on PDTB 2.0
and CoNLL16 datasets demonstrate that our method achieves outstanding and
consistent performance against the current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Knowledge Injection for Radiology Report Generation. (arXiv:2311.00399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00399">
<div class="article-summary-box-inner">
<span><p>Automatic generation of radiology reports holds crucial clinical value, as it
can alleviate substantial workload on radiologists and remind less experienced
ones of potential anomalies. Despite the remarkable performance of various
image captioning methods in the natural image field, generating accurate
reports for medical images still faces challenges, i.e., disparities in visual
and textual data, and lack of accurate domain knowledge. To address these
issues, we propose an enhanced knowledge injection framework, which utilizes
two branches to extract different types of knowledge. The Weighted Concept
Knowledge (WCK) branch is responsible for introducing clinical medical concepts
weighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch
extracts triplets from similar reports, emphasizing crucial clinical
information related to entity positions and existence. By integrating this
finer-grained and well-structured knowledge with the current image, we are able
to leverage the multi-source knowledge gain to ultimately facilitate more
accurate report generation. Extensive experiments have been conducted on two
public benchmarks, demonstrating that our method achieves superior performance
over other state-of-the-art methods. Ablation studies further validate the
effectiveness of two extracted knowledge sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification. (arXiv:2311.00408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00408">
<div class="article-summary-box-inner">
<span><p>Recent work has found that few-shot sentence classification based on
pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In
this work, we investigate strategies for domain-specialization in the context
of few-shot sentence classification with SEs. We first establish that
unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language
Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot
sentence classification by up to 8.4 points. However, applying DAPT on SEs, on
the one hand, disrupts the effects of their (general-domain) Sentence Embedding
Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of
a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient,
since the computationally expensive SEPT needs to be executed on top of a
DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples
SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be
inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent's
effectiveness in extensive experiments on 17 different few-shot sentence
classification datasets. AdaSent matches or surpasses the performance of full
SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code
for AdaSent is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00416">
<div class="article-summary-box-inner">
<span><p>Developing intelligent agents capable of seamless coordination with humans is
a critical step towards achieving artificial general intelligence. Existing
methods for human-AI coordination typically train an agent to coordinate with a
diverse set of policies or with human models fitted from real human data.
However, the massively diverse styles of human behavior present obstacles for
AI systems with constrained capacity, while high quality human data may not be
readily available in real-world scenarios. In this study, we observe that prior
to coordination, humans engage in communication to establish conventions that
specify individual roles and actions, making their coordination proceed in an
orderly manner. Building upon this observation, we propose employing the large
language model (LLM) to develop an action plan (or equivalently, a convention)
that effectively guides both human and AI. By inputting task requirements,
human preferences, the number of agents, and other pertinent information into
the LLM, it can generate a comprehensive convention that facilitates a clear
understanding of tasks and responsibilities for all parties involved.
Furthermore, we demonstrate that decomposing the convention formulation problem
into sub-problems with multiple new sessions being sequentially employed and
human feedback, will yield a more efficient coordination convention.
Experimental evaluations conducted in the Overcooked-AI environment, utilizing
a human proxy model, highlight the superior performance of our proposed method
compared to existing learning-based approaches. When coordinating with real
humans, our method achieves better alignment with human preferences and an
average performance improvement of 15% compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00430">
<div class="article-summary-box-inner">
<span><p>As the size of pre-trained speech recognition models increases, running these
large models in low-latency or resource-constrained environments becomes
challenging. In this work, we leverage pseudo-labelling to assemble a
large-scale open-source dataset which we use to distill the Whisper model into
a smaller variant, called Distil-Whisper. Using a simple word error rate (WER)
heuristic, we select only the highest quality pseudo-labels for training. The
distilled model is 5.8 times faster with 51% fewer parameters, while performing
to within 1% WER on out-of-distribution test data in a zero-shot transfer
setting. Distil-Whisper maintains the robustness of the Whisper model to
difficult acoustic conditions, while being less prone to hallucination errors
on long-form audio. Distil-Whisper is designed to be paired with Whisper for
speculative decoding, yielding a 2 times speed-up while mathematically ensuring
the same outputs as the original model. To facilitate further research in this
domain, we make our training code, inference code and models publicly
accessible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00445">
<div class="article-summary-box-inner">
<span><p>A central component of rational behavior is logical inference: the process of
determining which conclusions follow from a set of premises. Psychologists have
documented several ways in which humans' inferences deviate from the rules of
logic. Do language models, which are trained on text generated by humans,
replicate these biases, or are they able to overcome them? Focusing on the case
of syllogisms -- inferences from two simple premises, which have been studied
extensively in psychology -- we show that larger models are more logical than
smaller ones, and also more logical than humans. At the same time, even the
largest models make systematic errors, some of which mirror human reasoning
biases such as ordering effects and logical fallacies. Overall, we find that
language models mimic the human biases included in their training data, but are
able to overcome them in some cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation. (arXiv:2311.00451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00451">
<div class="article-summary-box-inner">
<span><p>Existing discourse formalisms use different taxonomies of discourse
relations, which require expert knowledge to understand, posing a challenge for
annotation and automatic classification. We show that discourse relations can
be effectively captured by some simple cognitively inspired dimensions proposed
by Sanders et al.(2018). Our experiments on cross-framework discourse relation
classification (PDTB &amp; RST) demonstrate that it is possible to transfer
knowledge of discourse relations for one framework to another framework by
means of these dimensions, in spite of differences in discourse segmentation of
the two frameworks. This manifests the effectiveness of these dimensions in
characterizing discourse relations across frameworks. Ablation studies reveal
that different dimensions influence different types of discourse relations. The
patterns can be explained by the role of dimensions in characterizing and
distinguishing different relations. We also report our experimental results on
automatic prediction of these dimensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Locality for Controllable Generation with kNN Language Models. (arXiv:2311.00475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00475">
<div class="article-summary-box-inner">
<span><p>Recent language models have been improved by the addition of external memory.
Nearest neighbor language models retrieve similar contexts to assist in word
prediction. The addition of locality levels allows a model to learn how to
weight neighbors based on their relative location to the current text in source
documents, and have been shown to further improve model performance. Nearest
neighbor models have been explored for controllable generation but have not
examined the use of locality levels. We present a novel approach for this
purpose and evaluate it using automatic and human evaluation on politeness,
formality, supportiveness, and toxicity textual data. We find that our model is
successfully able to control style and provides a better fluency-style
trade-off than previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00488">
<div class="article-summary-box-inner">
<span><p>We investigate the optimization target of Contrast-Consistent Search (CCS),
which aims to recover the internal representations of truth of a large language
model. We present a new loss function that we call the Midpoint-Displacement
(MD) loss function. We demonstrate that for a certain hyper-parameter value
this MD loss function leads to a prober with very similar weights to CCS. We
further show that this hyper-parameter is not optimal and that with a better
hyper-parameter the MD loss function attains a higher test accuracy than CCS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00502">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable performance and
tremendous potential across a wide range of tasks. However, deploying these
models has been challenging due to the astronomical amount of model parameters,
which requires a demand for large memory capacity and high memory bandwidth. In
this paper, we propose an effective approach that can make the deployment of
LLMs more efficiently. We support an automatic INT4 weight-only quantization
flow and design a special LLM runtime with highly-optimized kernels to
accelerate the LLM inference on CPUs. We demonstrate the general applicability
of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase
the extreme inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks. (arXiv:2311.00508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00508">
<div class="article-summary-box-inner">
<span><p>We investigate MT evaluation metric performance on adversarially-synthesized
texts, to shed light on metric robustness. We experiment with word- and
character-level attacks on three popular machine translation metrics:
BERTScore, BLEURT, and COMET. Our human experiments validate that automatic
metrics tend to overpenalize adversarially-degraded translations. We also
identify inconsistencies in BERTScore ratings, where it judges the original
sentence and the adversarially-degraded one as similar, while judging the
degraded translation as notably worse than the original with respect to the
reference. We identify patterns of brittleness that motivate more robust metric
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-Based Error Classification for Analyzing Differences in Frequent Errors. (arXiv:2311.00513v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00513">
<div class="article-summary-box-inner">
<span><p>Finding and fixing errors is a time-consuming task not only for novice
programmers but also for expert programmers. Prior work has identified frequent
error patterns among various levels of programmers. However, the differences in
the tendencies between novices and experts have yet to be revealed. From the
knowledge of the frequent errors in each level of programmers, instructors will
be able to provide helpful advice for each level of learners. In this paper, we
propose a rule-based error classification tool to classify errors in code pairs
consisting of wrong and correct programs. We classify errors for 95,631 code
pairs and identify 3.47 errors on average, which are submitted by various
levels of programmers on an online judge system. The classified errors are used
to analyze the differences in frequent errors between novice and expert
programmers. The analyzed results show that, as for the same introductory
problems, errors made by novices are due to the lack of knowledge in
programming, and the mistakes are considered an essential part of the learning
process. On the other hand, errors made by experts are due to misunderstandings
caused by the carelessness of reading problems or the challenges of solving
problems differently than usual. The proposed tool can be used to create
error-labeled datasets and for further code-related educational research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Rendering Strategies for Pixel Language Models. (arXiv:2311.00522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00522">
<div class="article-summary-box-inner">
<span><p>Pixel-based language models process text rendered as images, which allows
them to handle any script, making them a promising approach to open vocabulary
language modelling. However, recent approaches use text renderers that produce
a large set of almost-equivalent input patches, which may prove sub-optimal for
downstream tasks, due to redundancy in the input representations. In this
paper, we investigate four approaches to rendering text in the PIXEL model
(Rust et al., 2023), and find that simple character bigram rendering brings
improved performance on sentence-level tasks without compromising performance
on token-level or multilingual tasks. This new rendering strategy also makes it
possible to train a more compact model with only 22M parameters that performs
on par with the original 86M parameter model. Our analyses show that character
bigram rendering leads to a consistently better model but with an anisotropic
patch embedding space, driven by a patch frequency bias, highlighting the
connections between image patch- and tokenization-based language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00541">
<div class="article-summary-box-inner">
<span><p>Word meanings change over time, and word senses evolve, emerge or die out in
the process. For ancient languages, where the corpora are often small, sparse
and noisy, modelling such changes accurately proves challenging, and
quantifying uncertainty in sense-change estimates consequently becomes
important. GASC and DiSC are existing generative models that have been used to
analyse sense change for target words from an ancient Greek text corpus, using
unsupervised learning without the help of any pre-training. These models
represent the senses of a given target word such as "kosmos" (meaning
decoration, order or world) as distributions over context words, and sense
prevalence as a distribution over senses. The models are fitted using MCMC
methods to measure temporal changes in these representations. In this paper, we
introduce EDiSC, an embedded version of DiSC, which combines word embeddings
with DiSC to provide superior model performance. We show empirically that EDiSC
offers improved predictive accuracy, ground-truth recovery and uncertainty
quantification, as well as better sampling efficiency and scalability
properties with MCMC methods. We also discuss the challenges of fitting these
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00571">
<div class="article-summary-box-inner">
<span><p>LLaVA-Interactive is a research prototype for multimodal human-AI
interaction. The system can have multi-turn dialogues with human users by
taking multimodal user inputs and generating multimodal responses. Importantly,
LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled
to align human intents in the interaction. The development of LLaVA-Interactive
is extremely cost-efficient as the system combines three multimodal skills of
pre-built AI models without additional model training: visual chat of LLaVA,
image segmentation from SEEM, as well as image generation and editing from
GLIGEN. A diverse set of application scenarios is presented to demonstrate the
promises of LLaVA-Interactive and to inspire future research in multimodal
interactive systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00587">
<div class="article-summary-box-inner">
<span><p>The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00588">
<div class="article-summary-box-inner">
<span><p>This paper presents FlowSUM, a normalizing flows-based variational
encoder-decoder framework for Transformer-based summarization. Our approach
tackles two primary challenges in variational summarization: insufficient
semantic information in latent representations and posterior collapse during
training. To address these challenges, we employ normalizing flows to enable
flexible latent posterior modeling, and we propose a controlled alternate
aggressive training (CAAT) strategy with an improved gate mechanism.
Experimental results show that FlowSUM significantly enhances the quality of
generated summaries and unleashes the potential for knowledge distillation with
minimal impact on inference time. Furthermore, we investigate the issue of
posterior collapse in normalizing flows and analyze how the summary quality is
affected by the training strategy, gate initialization, and the type and number
of normalizing flows used, offering valuable insights for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Translation from Reversing Petri Nets to Coloured Petri Nets. (arXiv:2311.00629v1 [cs.LO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00629">
<div class="article-summary-box-inner">
<span><p>Reversible computation is an emerging computing paradigm that allows any
sequence of operations to be executed in reverse order at any point during
computation. Its appeal lies in its potential for lowpower computation and its
relevance to a wide array of applications such as chemical reactions, quantum
computation, robotics, and distributed systems. Reversing Petri nets are a
recently-proposed extension of Petri nets that implements the three main forms
of reversibility, namely, backtracking, causal reversing, and
out-of-causal-order reversing. Their distinguishing feature is the use of named
tokens that can be combined together to form bonds. Named tokens along with a
history function, constitute the means of remembering past behaviour, thus,
enabling reversal. In recent work, we have proposed a structural translation
from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an
extension of traditional Petri nets where tokens carry data values. In this
paper, we extend the translation to handle RPNs with token multiplicity under
the individual-token interpretation, a model which allows multiple tokens of
the same type to exist in a system. To support the three types of
reversibility, tokens are associated with their causal history and, while
tokens of the same type are equally eligible to fire a transition when going
forward, when going backwards they are able to reverse only the transitions
they have previously fired. The new translation, in addition to lifting the
restriction on token uniqueness, presents a refined approach for transforming
RPNs to CPNs through a unifying approach that allows instantiating each of the
three types of reversibility. The paper also reports on a tool that implements
this translation, paving the way for automated translations and analysis of
reversible systems using CPN Tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00658">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have shown remarkable successes in
acquiring a wide range of linguistic knowledge, relying solely on
self-supervised training on text streams. Nevertheless, the effectiveness of
this language-agnostic approach has been frequently questioned for its
sub-optimal performance when applied to morphologically-rich languages (MRLs).
We investigate the hypothesis that incorporating explicit morphological
knowledge in the pre-training phase can improve the performance of PLMs for
MRLs. We propose various morphologically driven tokenization methods enabling
the model to leverage morphological cues beyond raw text. We pre-train multiple
language models utilizing the different methods and evaluate them on Hebrew, a
language with complex and highly ambiguous morphology. Our experiments show
that morphologically driven tokenization demonstrates improved results compared
to a standard language-agnostic tokenization, on a benchmark of both semantic
and morphologic tasks. These findings suggest that incorporating morphological
knowledge holds the potential for further improving PLMs for morphologically
rich languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Detection for Misinformation: A Review. (arXiv:2311.00671v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00671">
<div class="article-summary-box-inner">
<span><p>With the advent of social media, an increasing number of netizens are sharing
and reading posts and news online. However, the huge volumes of misinformation
(e.g., fake news and rumors) that flood the internet can adversely affect
people's lives, and have resulted in the emergence of rumor and fake news
detection as a hot research topic. The emotions and sentiments of netizens, as
expressed in social media posts and news, constitute important factors that can
help to distinguish fake news from genuine news and to understand the spread of
rumors. This article comprehensively reviews emotion-based methods for
misinformation detection. We begin by explaining the strong links between
emotions and misinformation. We subsequently provide a detailed analysis of a
range of misinformation detection methods that employ a variety of emotion,
sentiment and stance-based features, and describe their strengths and
weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based
misinformation detection based on large language models and suggest future
research directions, including data collection (multi-platform, multilingual),
annotation, benchmark, multimodality, and interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00681">
<div class="article-summary-box-inner">
<span><p>In recent years, Large Language Models (LLMs) have gained immense attention
due to their notable emergent capabilities, surpassing those seen in earlier
language models. A particularly intriguing application of LLMs is their role as
evaluators for texts produced by various generative models.
</p>
<p>In this study, we delve into the potential of LLMs as reliable assessors of
factual consistency in summaries generated by text-generation models.
Initially, we introduce an innovative approach for factuality assessment using
LLMs. This entails employing a singular LLM for the entirety of the
question-answering-based factuality scoring process. Following this, we examine
the efficacy of various LLMs in direct factuality scoring, benchmarking them
against traditional measures and human annotations.
</p>
<p>Contrary to initial expectations, our results indicate a lack of significant
correlations between factuality metrics and human evaluations, specifically for
GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across
two factuality subcategories. These consistent findings across various factual
error categories suggest a fundamental limitation in the current LLMs'
capability to accurately gauge factuality.
</p>
<p>This version presents the information more concisely while maintaining the
main points and findings of the original text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00684">
<div class="article-summary-box-inner">
<span><p>An ideal length-extrapolatable Transformer language model can handle
sequences longer than the training length without any long sequence
fine-tuning. Such long-context utilization capability highly relies on a
flexible positional embedding design. Upon investigating the flexibility of
existing large pre-trained Transformer language models, we find that the T5
family deserves a closer look, as its positional embeddings capture rich and
flexible attention patterns. However, T5 suffers from the dispersed attention
issue: the longer the input sequence, the flatter the attention distribution.
To alleviate the issue, we propose two attention alignment strategies via
temperature scaling. Our findings improve the long-context utilization
capability of T5 on language modeling, retrieval, and multi-document question
answering without any fine-tuning, suggesting that a flexible positional
embedding design and attention alignment go a long way toward Transformer
length
extrapolation.\footnote{\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00686">
<div class="article-summary-box-inner">
<span><p>This paper describes and analyzes our participation in the 2023 Eval4NLP
shared task, which focuses on assessing the effectiveness of prompt-based
techniques to empower Large Language Models to handle the task of quality
estimation, particularly in the context of evaluating machine translations and
summaries. We conducted systematic experiments with various prompting
techniques, including standard prompting, prompts informed by annotator
instructions, and innovative chain-of-thought prompting. In addition, we
integrated these approaches with zero-shot and one-shot learning methods to
maximize the efficacy of our evaluation procedures. Our work reveals that
combining these approaches using a "small", open source model (orca_mini_v3_7B)
yields competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00687">
<div class="article-summary-box-inner">
<span><p>How do we communicate with others to achieve our goals? We use our prior
experience or advice from others, or construct a candidate utterance by
predicting how it will be received. However, our experiences are limited and
biased, and reasoning about potential outcomes can be difficult and cognitively
challenging. In this paper, we explore how we can leverage Large Language Model
(LLM) simulations to help us communicate better. We propose the
Explore-Generate-Simulate (EGS) framework, which takes as input any scenario
where an individual is communicating to an audience with a goal they want to
achieve. EGS (1) explores the solution space by producing a diverse set of
advice relevant to the scenario, (2) generates communication candidates
conditioned on subsets of the advice, and (3) simulates the reactions from
various audiences to determine both the best candidate and advice to use. We
evaluate the framework on eight scenarios spanning the ten fundamental
processes of interpersonal communication. For each scenario, we collect a
dataset of human evaluations across candidates and baselines, and showcase that
our framework's chosen candidate is preferred over popular generation
mechanisms including Chain-of-Thought. We also find that audience simulations
achieve reasonably high agreement with human raters across 5 of the 8
scenarios. Finally, we demonstrate the generality of our framework by applying
it to real-world scenarios described by users on web forums. Through
evaluations and demonstrations, we show that EGS enhances the effectiveness and
outcomes of goal-oriented communication across a variety of situations, thus
opening up new possibilities for the application of large language models in
revolutionizing communication and decision-making processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00694">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved tremendous progress, yet they
still often struggle with challenging reasoning problems. Current approaches
address this challenge by sampling or searching detailed and low-level
reasoning chains. However, these methods are still limited in their exploration
capabilities, making it challenging for correct solutions to stand out in the
huge solution space. In this work, we unleash LLMs' creative potential for
exploring multiple diverse problem solving strategies by framing an LLM as a
hierarchical policy via in-context learning. This policy comprises of a
visionary leader that proposes multiple diverse high-level problem-solving
tactics as hints, accompanied by a follower that executes detailed
problem-solving processes following each of the high-level instruction. The
follower uses each of the leader's directives as a guide and samples multiple
reasoning chains to tackle the problem, generating a solution group for each
leader proposal. Additionally, we propose an effective and efficient
tournament-based approach to select among these explored solution groups to
reach the final answer. Our approach produces meaningful and inspiring hints,
enhances problem-solving strategy exploration, and improves the final answer
accuracy on challenging problems in the MATH dataset. Code will be released at
https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation. (arXiv:2311.00697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.00697">
<div class="article-summary-box-inner">
<span><p>Conventional speech-to-text translation (ST) systems are trained on
single-speaker utterances, and they may not generalize to real-life scenarios
where the audio contains conversations by multiple speakers. In this paper, we
tackle single-channel multi-speaker conversational ST with an end-to-end and
multi-task training model, named Speaker-Turn Aware Conversational Speech
Translation, that combines automatic speech recognition, speech translation and
speaker turn detection using special tokens in a serialized labeling format. We
run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the
two single-speaker channels into one multi-speaker channel, thus representing
the more realistic and challenging scenario with multi-speaker turns and
cross-talk. Experimental results across single- and multi-speaker conditions
and against conventional ST systems, show that our model outperforms the
reference systems on the multi-speaker condition, while attaining comparable
performance on the single-speaker condition. We release scripts for data
processing and model training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09699">
<div class="article-summary-box-inner">
<span><p>End-to-end Speech Translation is hindered by a lack of available data
resources. While most of them are based on documents, a sentence-level version
is available, which is however single and static, potentially impeding the
usefulness of the data. We propose a new data augmentation strategy,
SegAugment, to address this issue by generating multiple alternative
sentence-level versions of a dataset. Our method utilizes an Audio Segmentation
system, which re-segments the speech of each document with different length
constraints, after which we obtain the target text via alignment methods.
Experiments demonstrate consistent gains across eight language pairs in MuST-C,
with an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource
scenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment
establishes new state-of-the-art results in MuST-C. Finally, we show that the
proposed method can also successfully augment sentence-level datasets, and that
it enables Speech Translation models to close the gap between the manual and
automatic segmentation at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13126">
<div class="article-summary-box-inner">
<span><p>Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Algebra for (Score-Based) Text-Controlled Generative Models. (arXiv:2302.03693v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03693">
<div class="article-summary-box-inner">
<span><p>This paper concerns the structure of learned representations in text-guided
generative models, focusing on score-based models. A key property of such
models is that they can compose disparate concepts in a `disentangled' manner.
This suggests these models have internal representations that encode concepts
in a `disentangled' manner. Here, we focus on the idea that concepts are
encoded as subspaces of some representation space. We formalize what this
means, show there's a natural choice for the representation, and develop a
simple method for identifying the part of the representation corresponding to a
given concept. In particular, this allows us to manipulate the concepts
expressed by the model through algebraic manipulation of the representation. We
demonstrate the idea with examples using Stable Diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15422">
<div class="article-summary-box-inner">
<span><p>Despite the significant advancements in keyphrase extraction and keyphrase
generation methods, the predominant approach for evaluation only relies on
exact matching with human references and disregards reference-free attributes.
This scheme fails to recognize systems that generate keyphrases semantically
equivalent to the references or diverse keyphrases that carry practical
utility. To better assess the capability of keyphrase systems, we propose
KPEval, a comprehensive evaluation framework consisting of four critical
dimensions: saliency, faithfulness, diversity, and utility. For each dimension,
we design semantic-based metrics that align with the evaluation objectives.
Meta-evaluation studies demonstrate that our evaluation strategy correlates
better with human preferences compared to a range of previously used metrics.
Using this framework, we re-evaluate 20 keyphrase systems and further discover
that (1) the best model differs depending on the evaluation dimension; (2) the
utility in downstream tasks does not always correlate with reference-based
metrics; and (3) large language models like GPT-3.5 exhibit a strong
performance under reference-free evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05197">
<div class="article-summary-box-inner">
<span><p>With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given appropriate prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause new privacy
threats. To this end, we conduct extensive experiments to support our claims
and discuss LLMs' privacy implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07984">
<div class="article-summary-box-inner">
<span><p>Detecting negatives (such as non-entailment relationships, unanswerable
questions, and false claims) is an important and challenging aspect of many
natural language understanding tasks. Though manually collecting challenging
negative examples can help models detect them, it is both costly and
domain-specific. In this work, we propose Self-labeled Counterfactuals for
Extrapolating to Negative Examples (SCENE), an automatic method for
synthesizing training data that greatly improves models' ability to detect
challenging negative examples. In contrast with standard data augmentation,
which synthesizes new examples for existing labels, SCENE can synthesize
negative examples zero-shot from only positive ones. Given a positive example,
SCENE perturbs it with a mask infilling model, then determines whether the
resulting example is negative based on a self-training heuristic. With access
to only answerable training examples, SCENE can close 69.6% of the performance
gap on SQuAD 2.0, a dataset where half of the evaluation examples are
unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends
to boolean question answering and recognizing textual entailment, and improves
generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08503">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have achieved outstanding achievements in
abstractive single-document summarization (SDS). However, such benefits may not
fully extend to multi-document summarization (MDS), where the handling of
cross-document information is more complex. Previous works either design new
MDS architectures or apply PLMs bluntly with concatenated source documents as a
reformulated SDS task. While the former does not utilize previous pre-training
efforts and may not generalize well across different domains, the latter may
not sufficiently attend to the intricate cross-document relationships unique to
MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to
better utilize a PLM to facilitate multi-document interactions for the MDS
task. Across 10 MDS benchmarks from various domains, our method outperforms or
is competitive with the previous best models, including those with additional
MDS pre-training or with more parameters. It outperforms its corresponding PLM
backbone by up to 3 Rouge-L and is favored by humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy. (arXiv:2305.14596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14596">
<div class="article-summary-box-inner">
<span><p>When pretrained language models (LMs) are applied to discriminative tasks
such as multiple-choice questions, they place probability mass on vocabulary
tokens that aren't among the given answer choices. Spreading probability mass
across multiple surface forms with identical meaning (such as "bath" and
"bathtub") is thought to cause an underestimation of a model's true
performance, referred to as the "surface form competition" (SFC) hypothesis.
This has motivated the introduction of various probability normalization
methods. However, many core questions remain unanswered. How do we measure SFC?
Are there direct ways of reducing it, and does doing so improve task
performance?
</p>
<p>We propose a mathematical formalism for SFC which allows us to quantify and
bound its impact for the first time. We identify a simple method for reducing
it -- namely, increasing probability mass on the given answer choices by a)
including them in the prompt and b) using in-context learning with even just
one example. We show this method eliminates the impact of SFC in the majority
of instances. Our experiments on three diverse datasets and six LMs reveal
several additional surprising findings. For example, both normalization and
prompting methods for reducing SFC can be ineffective or even detrimental to
task performance for some LMs. We conclude with practical insights for
effectively prompting LMs for multiple-choice tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04723">
<div class="article-summary-box-inner">
<span><p>Rapidly increasing quality of AI-generated content makes it difficult to
distinguish between human and AI-generated texts, which may lead to undesirable
consequences for society. Therefore, it becomes increasingly important to study
the properties of human texts that are invariant over different text domains
and varying proficiency of human writers, can be easily calculated for any
language, and can robustly separate natural and AI-generated texts regardless
of the generation model and sampling method. In this work, we propose such an
invariant for human-written texts, namely the intrinsic dimensionality of the
manifold underlying the set of embeddings for a given text sample. We show that
the average intrinsic dimensionality of fluent texts in a natural language is
hovering around the value $9$ for several alphabet-based languages and around
$7$ for Chinese, while the average intrinsic dimensionality of AI-generated
texts for each language is $\approx 1.5$ lower, with a clear statistical
separation between human-generated and AI-generated distributions. This
property allows us to build a score-based artificial text detector. The
proposed detector's accuracy is stable over text domains, generator models, and
human writer proficiency levels, outperforming SOTA detectors in model-agnostic
and cross-domain scenarios by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREHate: A CRoss-cultural English Hate Speech Dataset. (arXiv:2308.16705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16705">
<div class="article-summary-box-inner">
<span><p>Most NLP datasets neglect the cultural diversity among language speakers,
resulting in a critical shortcoming in hate speech detection and other
culturally sensitive tasks. To address this, we introduce CREHate, a
CRoss-cultural English Hate speech dataset. To construct CREHate, we follow a
two-step procedure: 1) culture-specific post collection and 2) cross-cultural
annotation. We sample posts from the SBIC dataset, which predominantly
represents North America, and collect posts from four geographically diverse
English-speaking countries using culture-specific hate speech keywords that we
retrieve from our survey. Annotations are then collected from those four
English-speaking countries plus the US to establish representative labels for
each country. Our analysis highlights statistically significant disparities in
cross-cultural hate speech annotations. Only 56.2% of the posts in CREHate
achieve consensus among all five countries, with a peak pairwise disagreement
rate of 26%. The annotations show that label disagreements tend to come from
the inherent cultural context, subjectivity, and ambiguity of the posts.
Lastly, we develop cross-cultural hate speech classifiers that are more
accurate at predicting each country's labels than the monocultural classifiers.
This confirms the utility of CREHate for constructing culturally sensitive hate
speech classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00071">
<div class="article-summary-box-inner">
<span><p>Rotary Position Embeddings (RoPE) have been shown to effectively encode
positional information in transformer-based language models. However, these
models fail to generalize past the sequence length they were trained on. We
present YaRN (Yet another RoPE extensioN method), a compute-efficient method to
extend the context window of such models, requiring 10x less tokens and 2.5x
less training steps than previous methods. Using YaRN, we show that LLaMA
models can effectively utilize and extrapolate to context lengths much longer
than their original pre-training would allow, while also surpassing previous
the state-of-the-art at context window extension. In addition, we demonstrate
that YaRN exhibits the capability to extrapolate beyond the limited context of
a fine-tuning dataset. The models fine-tuned using YaRN has been made available
and reproduced online up to 128k context length at
https://github.com/jquesnelle/yarn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03564">
<div class="article-summary-box-inner">
<span><p>In the realm of social media, users frequently convey personal sentiments,
with some potentially indicating cognitive distortions or suicidal tendencies.
Timely recognition of such signs is pivotal for effective interventions. In
response, we introduce two novel annotated datasets from Chinese social media,
focused on cognitive distortions and suicidal risk classification. We propose a
comprehensive benchmark using both supervised learning and large language
models, especially from the GPT series, to evaluate performance on these
datasets. To assess the capabilities of the large language models, we employed
three strategies: zero-shot, few-shot, and fine-tuning. Furthermore, we deeply
explored and analyzed the performance of these large language models from a
psychological perspective, shedding light on their strengths and limitations in
identifying and understanding complex human emotions. Our evaluations
underscore a performance difference between the two approaches, with the models
often challenged by subtle category distinctions. While GPT-4 consistently
delivered strong results, GPT-3.5 showed marked improvement in suicide risk
classification after fine-tuning. This research is groundbreaking in its
evaluation of large language models for Chinese social media tasks,
accentuating the models' potential in psychological contexts. All datasets and
code are made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12161">
<div class="article-summary-box-inner">
<span><p>High-quality conversational datasets are crucial for the successful
development of Intelligent Tutoring Systems (ITS) that utilize a Large Language
Model (LLM) backend. Synthetic student-teacher dialogues, generated using
advanced GPT-4 models, are a common strategy for creating these datasets.
However, subjects like physics that entail complex calculations pose a
challenge. While GPT-4 presents impressive language processing capabilities,
its limitations in fundamental mathematical reasoning curtail its efficacy for
such subjects. To tackle this limitation, we introduce in this paper an
innovative stateful prompt design. Our design orchestrates a mock conversation
where both student and tutorbot roles are simulated by GPT-4. Each student
response triggers an internal monologue, or `code soliloquy' in the
GPT-tutorbot, which assesses whether its subsequent response would necessitate
calculations. If a calculation is deemed necessary, it scripts the relevant
Python code and uses the Python output to construct a response to the student.
Our approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our preliminary Subject
Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA
model, effectively uses Python for computations, which significantly enhances
the accuracy and computational reliability of Higgs' responses. Code, models,
and datasets is available at https://github.com/luffycodes/Tutorbot-Spock-Phys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04632">
<div class="article-summary-box-inner">
<span><p>Releasing court decisions to the public relies on proper anonymization to
protect all involved parties, where necessary. The Swiss Federal Supreme Court
relies on an existing system that combines different traditional computational
methods with human experts. In this work, we enhance the existing anonymization
software using a large dataset annotated with entities to be anonymized. We
compared BERT-based models with models pre-trained on in-domain data. Our
results show that using in-domain data to pre-train the models further improves
the F1-score by more than 5\% compared to existing models. Our work
demonstrates that combining existing anonymization methods, such as regular
expressions, with machine learning can further reduce manual labor and enhance
automatic suggestions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05492">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with enormous pre-training tokens and parameter
amounts emerge abilities, including math reasoning, code generation, and
instruction following. These abilities are further enhanced by supervised
fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each
ability, while proprietary LLMs are versatile for all abilities. It is
important to investigate how to unlock them with multiple abilities via SFT. In
this study, we specifically focus on the data composition between mathematical
reasoning, code generation, and general human-aligning abilities during SFT.
From a scaling perspective, we investigate the relationship between model
abilities and various factors including data amounts, data composition ratio,
model parameters, and SFT strategies. Our experiments reveal that different
abilities exhibit different scaling patterns, and larger models generally show
superior performance with the same amount of data. Mathematical reasoning and
code generation improve as data amounts increase consistently, while the
general ability is enhanced with about a thousand samples and improves slowly.
We find data composition results in various abilities improvements with low
data amounts, while conflicts of abilities with high data amounts. Our
experiments further show that composition data amount impacts performance,
while the influence of composition ratio is insignificant. Regarding the SFT
strategies, we evaluate sequential learning multiple abilities are prone to
catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)
strategy learns specialized abilities first and then learns general abilities
with a small amount of specialized data to prevent forgetting, offering a
promising solution to learn multiple abilities with different scaling patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05506">
<div class="article-summary-box-inner">
<span><p>In math reasoning with large language models (LLMs), fine-tuning data
augmentation by query evolution and diverse reasoning paths is empirically
verified effective, profoundly narrowing the gap between open-sourced LLMs and
cutting-edge proprietary LLMs. In this paper, we conduct an investigation for
such data augmentation in math reasoning and are intended to answer: (1) What
strategies of data augmentation are more effective; (2) What is the scaling
relationship between the amount of augmented data and model performance; and
(3) Can data augmentation incentivize generalization to out-of-domain
mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,
by complicating and diversifying the queries from GSM8K and sampling multiple
reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning
on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art
on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the
scale of 13B). A log-linear relationship is presented between MuggleMath's
performance and the amount of augmented data. We also find that MuggleMath is
weak in out-of-domain math reasoning generalization to MATH. This is attributed
to the differences in query distribution between AugGSM8K and MATH which
suggest that augmentation on a single benchmark could not help with overall
math reasoning performance. Codes and AugGSM8K will be uploaded to
https://github.com/OFA-Sys/gsm8k-ScRel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09219">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently emerged as an effective tool to
assist individuals in writing various types of content, including professional
documents such as recommendation letters. Though bringing convenience, this
application also introduces unprecedented fairness concerns. Model-generated
reference letters might be directly used by users in professional scenarios. If
underlying biases exist in these model-constructed letters, using them without
scrutinization could lead to direct societal harms, such as sabotaging
application success rates for female applicants. In light of this pressing
issue, it is imminent and necessary to comprehensively study fairness issues
and associated harms in this real-world use case. In this paper, we critically
examine gender biases in LLM-generated reference letters. Drawing inspiration
from social science findings, we design evaluation methods to manifest biases
through 2 dimensions: (1) biases in language style and (2) biases in lexical
content. We further investigate the extent of bias propagation by analyzing the
hallucination bias of models, a term that we define to be bias exacerbation in
model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-
ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated
recommendation letters. Our findings not only warn against using LLMs for this
application without scrutinization, but also illuminate the importance of
thoroughly studying hidden biases and harms in LLM-generated professional
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11986">
<div class="article-summary-box-inner">
<span><p>Generative AI systems produce a range of risks. To ensure the safety of
generative AI systems, these risks must be evaluated. In this paper, we make
two main contributions toward establishing such evaluations. First, we propose
a three-layered framework that takes a structured, sociotechnical approach to
evaluating these risks. This framework encompasses capability evaluations,
which are the main current approach to safety evaluation. It then reaches
further by building on system safety principles, particularly the insight that
context determines whether a given capability may cause harm. To account for
relevant context, our framework adds human interaction and systemic impacts as
additional layers of evaluation. Second, we survey the current state of safety
evaluation of generative AI systems and create a repository of existing
evaluations. Three salient evaluation gaps emerge from this analysis. We
propose ways forward to closing these gaps, outlining practical steps as well
as roles and responsibilities for different actors. Sociotechnical safety
evaluation is a tractable approach to the robust and comprehensive safety
evaluation of generative AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14670">
<div class="article-summary-box-inner">
<span><p>Vision-language (VL) understanding tasks evaluate models' comprehension of
complex visual scenes through multiple-choice questions. However, we have
identified two dataset biases that models can exploit as shortcuts to resolve
various VL tasks correctly without proper understanding. The first type of
dataset bias is \emph{Unbalanced Matching} bias, where the correct answer
overlaps the question and image more than the incorrect answers. The second
type of dataset bias is \emph{Distractor Similarity} bias, where incorrect
answers are overly dissimilar to the correct answer but significantly similar
to other incorrect answers within the same sample. To address these dataset
biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic
training and debiased evaluation data. We then introduce Intra-sample
Counterfactual Training (ICT) to assist models in utilizing the synthesized
training data, particularly the counterfactual data, via focusing on
intra-sample differentiation. Extensive experiments demonstrate the
effectiveness of ADS and ICT in consistently improving model performance across
different benchmarks, even in domain-shifted scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14859">
<div class="article-summary-box-inner">
<span><p>Predicting turn-taking in multiparty conversations has many practical
applications in human-computer/robot interaction. However, the complexity of
human communication makes it a challenging task. Recent advances have shown
that synchronous multi-perspective egocentric data can significantly improve
turn-taking prediction compared to asynchronous, single-perspective
transcriptions. Building on this research, we propose a new multimodal
transformer-based architecture for predicting turn-taking in embodied,
synchronized multi-perspective data. Our experimental results on the recently
introduced EgoCom dataset show a substantial performance improvement of up to
14.01% on average compared to existing baselines and alternative
transformer-based approaches. The source code, and the pre-trained models of
our 3M-Transformer will be available upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15431">
<div class="article-summary-box-inner">
<span><p>Moral or ethical judgments rely heavily on the specific contexts in which
they occur. Understanding varying shades of defeasible contextualizations
(i.e., additional information that strengthens or attenuates the moral
acceptability of an action) is critical to accurately represent the subtlety
and intricacy of grounded human moral judgment in real-life scenarios.
</p>
<p>We introduce defeasible moral reasoning: a task to provide grounded contexts
that make an action more or less morally acceptable, along with commonsense
rationales that justify the reasoning. To elicit high-quality task data, we
take an iterative self-distillation approach that starts from a small amount of
unstructured seed knowledge from GPT-3 and then alternates between (1)
self-distillation from student models; (2) targeted filtering with a critic
model trained by human judgment (to boost validity) and NLI (to boost
diversity); (3) self-imitation learning (to amplify the desired data quality).
This process yields a student model that produces defeasible contexts with
improved validity, diversity, and defeasibility. From this model we distill a
high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of
contextualizations and rationales for 115K defeasible moral actions rated
highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we
obtain a final student model that wins over all intermediate student models by
a notable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15612">
<div class="article-summary-box-inner">
<span><p>Currently, there is no usable machine translation system for Nko
\footnote{Also spelled N'Ko, but speakers prefer the name Nko.}, a language
spoken by tens of millions of people across multiple West African countries,
which holds significant cultural and educational value.
</p>
<p>To address this issue, we present a set of tools, resources, and baseline
results aimed towards the development of usable machine translation systems for
Nko and other languages that do not currently have sufficiently large parallel
text corpora available.
</p>
<p>(1) Fria$\parallel$el: A novel collaborative parallel text curation software
that incorporates quality control through copyedit-based workflows. (2)
Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193
high-quality Nko translations in parallel with 204 and 40 other languages. (3)
nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850
parallel segments and monolingual corpora containing over 3 million Nko words.
(4) Baseline bilingual and multilingual neural machine translation results with
the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17306">
<div class="article-summary-box-inner">
<span><p>Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17680">
<div class="article-summary-box-inner">
<span><p>Imagine a developer who can only change their last line of code, how often
would they have to start writing a function from scratch before it is correct?
Auto-regressive models for code generation from natural language have a similar
limitation: they do not easily allow reconsidering earlier tokens generated. We
introduce CodeFusion, a pre-trained diffusion code generation model that
addresses this limitation by iteratively denoising a complete program
conditioned on the encoded natural language. We evaluate CodeFusion on the task
of natural language to code generation for Bash, Python, and Microsoft Excel
conditional formatting (CF) rules. Experiments show that CodeFusion (75M
parameters) performs on par with state-of-the-art auto-regressive systems
(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and
top-5 accuracy due to its better balance in diversity versus quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17956">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have introduced a new era of proficiency in
comprehending complex healthcare and biomedical topics. However, there is a
noticeable lack of models in languages other than English and models that can
interpret multi-modal input, which is crucial for global healthcare
accessibility. In response, this study introduces Qilin-Med-VL, the first
Chinese large vision-language model designed to integrate the analysis of
textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer
(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum
training process that includes feature alignment and instruction tuning. This
method enhances the model's ability to generate medical captions and answer
complex medical queries. We also release ChiMed-VL, a dataset consisting of
more than 1M image-text pairs. This dataset has been carefully curated to
enable detailed and comprehensive interpretation of medical data using various
types of images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18341">
<div class="article-summary-box-inner">
<span><p>Purpose: Recent advancements in large language models (LLMs) have expanded
their capabilities in a multimodal fashion, potentially replicating the image
interpretation of human radiologists. This study aimed to develop open-source
multimodal large language model for interpreting chest X-ray images
(CXR-LLaVA). We also examined the effect of prompt engineering and model
parameters such as temperature and nucleus sampling.
</p>
<p>Materials and Methods: For training, we collected 659,287 publicly available
CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset
1); 241,951 CXRs provided free-text radiology reports (dataset 2). After
pre-training the Resnet50 as an image encoder, the contrastive language-image
pre-training was used to align CXRs and corresponding radiographic
abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using
dataset 2, which were refined using GPT-4, with generating various question
answering scenarios. The code can be found at
https://github.com/ECOFRI/CXR_LLaVA.
</p>
<p>Results: In the test set, we observed that the model's performance fluctuated
based on its parameters. On average, it achieved F1 score of 0.34 for five
pathologic findings (atelectasis, cardiomegaly, consolidation, edema, and
pleural effusion), which was improved to 0.46 through prompt engineering. In
the independent set, the model achieved an average F1 score of 0.30 for the
same pathologic findings. Notably, for the pediatric chest radiograph dataset,
which was unseen during training, the model differentiated abnormal radiographs
with an F1 score ranging from 0.84 to 0.85.
</p>
<p>Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation.
Both prompt engineering and model parameter adjustments can play pivotal roles
in interpreting CXRs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19347">
<div class="article-summary-box-inner">
<span><p>Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as "hallucinations" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, and overgeneralizing, etc. These hallucinations are challenging to
detect through traditional methods, which poses great challenges for improving
the factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
parameter-efficient technique to cover the shortage of sensitivity for true and
false in the training process of LLMs. In this way, LLMs are less confused
about embellishing and understanding, thus can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19531">
<div class="article-summary-box-inner">
<span><p>Generative language models are usually pretrained on large text corpus via
predicting the next token (i.e., sub-word/word/phrase) given the previous ones.
Recent works have demonstrated the impressive performance of large generative
language models on downstream tasks. However, existing generative language
models generally neglect an inherent challenge in text corpus during training,
i.e., the imbalance between frequent tokens and infrequent ones. It can lead a
language model to be dominated by common and easy-to-learn tokens, thereby
overlooking the infrequent and difficult-to-learn ones. To alleviate that, we
propose an Information Entropy Loss (InfoEntropy Loss) function. During
training, it can dynamically assess the learning difficulty of a to-be-learned
token, according to the information entropy of the corresponding predicted
probability distribution over the vocabulary. Then it scales the training loss
adaptively, trying to lead the model to focus more on the difficult-to-learn
tokens. On the Pile dataset, we train generative language models at different
scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models
incorporating the proposed InfoEntropy Loss can gain consistent performance
improvement on downstream benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19708">
<div class="article-summary-box-inner">
<span><p>General purpose language models (LMs) encounter difficulties when processing
domain-specific jargon and terminology, which are frequently utilized in
specialized fields such as medicine or industrial settings. Moreover, they
often find it challenging to interpret mixed speech that blends general
language with specialized jargon. This poses a challenge for automatic speech
recognition systems operating within these specific domains. In this work, we
introduce a novel approach that integrates domain-specific or secondary LM into
general-purpose LM. This strategy involves labeling, or "coloring", each word
to indicate its association with either the general or the domain-specific LM.
We develop an optimized algorithm that enhances the beam search algorithm to
effectively handle inferences involving colored words. Our evaluations indicate
that this approach is highly effective in integrating jargon into language
tasks. Notably, our method substantially lowers the error rate for
domain-specific words without compromising performance in the general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20246">
<div class="article-summary-box-inner">
<span><p>Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-11-02 23:11:20.993972567 UTC">2023-11-02 23:11:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>