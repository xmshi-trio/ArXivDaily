<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-20T01:30:00Z">12-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">EffMulti: Efficiently Modeling Complex Multimodal Interactions for Emotion Analysis. (arXiv:2212.08661v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08661">
<div class="article-summary-box-inner">
<span><p>Humans are skilled in reading the interlocutor's emotion from multimodal
signals, including spoken words, simultaneous speech, and facial expressions.
It is still a challenge to effectively decode emotions from the complex
interactions of multimodal signals. In this paper, we design three kinds of
multimodal latent representations to refine the emotion analysis process and
capture complex multimodal interactions from different views, including a
intact three-modal integrating representation, a modality-shared
representation, and three modality-individual representations. Then, a
modality-semantic hierarchical fusion is proposed to reasonably incorporate
these representations into a comprehensive interaction representation. The
experimental results demonstrate that our EffMulti outperforms the
state-of-the-art methods. The compelling performance benefits from its
well-designed framework with ease of implementation, lower computing
complexity, and less trainable parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning. (arXiv:2212.08686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08686">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) have shown remarkable reasoning performance
using explanations (or ``chain-of-thought'' (CoT)) for in-context learning. On
the other hand, these reasoning tasks are usually presumed to be more
approachable for symbolic programming. To make progress towards understanding
in-context learning, we curate synthetic datasets containing equivalent
(natural, symbolic) data pairs, where symbolic examples contain first-order
logic rules and predicates from knowledge bases (KBs). Then we revisit
neuro-symbolic approaches and use Language Models as Logic Programmer (LMLP)
that learns from demonstrations containing logic rules and corresponding
examples to iteratively reason over KBs, recovering Prolog's backward chaining
algorithm. Comprehensive experiments are included to systematically compare
LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more
than 25% higher accuracy than CoT on length generalization benchmarks even with
fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers. (arXiv:2212.08700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08700">
<div class="article-summary-box-inner">
<span><p>Language Models appear to perform poorly on quantification. We ask how badly.
'Few'-type quantifiers, as in 'few children like vegetables' might pose a
particular challenge for Language Models, since the sentence components without
the quantifier are likely to co-occur, and because 'few'-type quantifiers are
rare. We present 960 sentences stimuli from two human neurolinguistic
experiments to 22 autoregressive transformer models of differing sizes. Not
only do the models perform poorly on 'few'-type quantifiers, but overall the
larger the model, the worse its performance. We interpret this inverse scaling
as suggesting that larger models increasingly reflect online rather than
offline human processing, and argue that decreasing performance of larger
models may challenge uses of Language Models as the basis for Natural Language
Systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recognition. (arXiv:2212.08703v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08703">
<div class="article-summary-box-inner">
<span><p>This paper presents a class of new fast non-trainable entropy-based
confidence estimation methods for automatic speech recognition. We show how
per-frame entropy values can be normalized and aggregated to obtain a
confidence measure per unit and per word for Connectionist Temporal
Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models.
Proposed methods have similar computational complexity to the traditional
method based on the maximum per-frame probability, but they are more
adjustable, have a wider effective threshold range, and better push apart the
confidence distributions of correct and incorrect words. We evaluate the
proposed confidence measures on LibriSpeech test sets, and show that they are
up to 2 and 4 times better than confidence estimation based on the maximum
per-frame probability at detecting incorrect words for Conformer-CTC and
Conformer-RNN-T models, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Story Planning. (arXiv:2212.08718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08718">
<div class="article-summary-box-inner">
<span><p>Automated plot generation is the challenge of generating a sequence of events
that will be perceived by readers as the plot of a coherent story. Traditional
symbolic planners plan a story from a goal state and guarantee logical causal
plot coherence but rely on a library of hand-crafted actions with their
preconditions and effects. This closed world setting limits the length and
diversity of what symbolic planners can generate. On the other hand,
pre-trained neural language models can generate stories with great diversity,
while being generally incapable of ending a story in a specified manner and can
have trouble maintaining coherence. In this paper, we present an approach to
story plot generation that unifies causal planning with neural language models.
We propose to use commonsense knowledge extracted from large language models to
recursively expand a story plot in a backward chaining fashion. Specifically,
our system infers the preconditions for events in the story and then events
that will cause those conditions to become true. We performed automatic
evaluation to measure narrative coherence as indicated by the ability to answer
questions about whether different events in the story are causally related to
other events. Results indicate that our proposed method produces more coherent
plotlines than several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08724">
<div class="article-summary-box-inner">
<span><p>Self-training (ST) has prospered again in language understanding by
augmenting the fine-tuning of pre-trained language models when labeled data is
insufficient. However, it remains challenging to incorporate ST into
attribute-controllable language generation. Augmented by only self-generated
pseudo text, generation models over-emphasize exploitation of the previously
learned space, suffering from a constrained generalization boundary. We revisit
ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly
models text generation and classification with a shared Variational AutoEncoder
and corrupts the generated pseudo text by two kinds of flexible noise to
disturb the space. In this way, our model could construct and utilize both
pseudo text from given labels and pseudo labels from available unlabeled text,
which are gradually refined during the ST process. We theoretically demonstrate
that DuNST can be regarded as enhancing exploration towards the potential real
text space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks show that DuNST could significantly boost control
accuracy while maintaining comparable generation fluency and diversity against
several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08756">
<div class="article-summary-box-inner">
<span><p>Machine learning models can reach high performance on benchmark natural
language processing (NLP) datasets but fail in more challenging settings. We
study this issue when a pre-trained model learns dataset artifacts in natural
language inference (NLI), the topic of studying the logical relationship
between a pair of text sequences. We provide a variety of techniques for
analyzing and locating dataset artifacts inside the crowdsourced Stanford
Natural Language Inference (SNLI) corpus. We study the stylistic pattern of
dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a
unique multi-scale data augmentation technique with two distinct frameworks: a
behavioral testing checklist at the sentence level and lexical synonym criteria
at the word level. Specifically, our combination method enhances our model's
resistance to perturbation testing, enabling it to continuously outperform the
pre-trained baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RISE: Leveraging Retrieval Techniques for Summarization Evaluation. (arXiv:2212.08775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08775">
<div class="article-summary-box-inner">
<span><p>Evaluating automatically-generated text summaries is a challenging task.
While there have been many interesting approaches, they still fall short of
human evaluations. We present RISE, a new approach for evaluating summaries by
leveraging techniques from information retrieval. RISE is first trained as a
retrieval task using a dual-encoder retrieval setup, and can then be
subsequently utilized for evaluating a generated summary given an input
document, without gold reference summaries. RISE is especially well suited when
working on new datasets where one may not have reference summaries available
for evaluation. We conduct comprehensive experiments on the SummEval benchmark
(Fabbri et al., 2021) and the results show that RISE has higher correlation
with human evaluations compared to many past approaches to summarization
evaluation. Furthermore, RISE also demonstrates data-efficiency and
generalizability across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations. (arXiv:2212.08780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08780">
<div class="article-summary-box-inner">
<span><p>There has been great progress in unifying various table-to-text tasks using a
single encoder-decoder model trained via multi-task learning (Xie et al.,
2022). However, existing methods typically encode task information with a
simple dataset name as a prefix to the encoder. This not only limits the
effectiveness of multi-task learning, but also hinders the model's ability to
generalize to new domains or tasks that were not seen during training, which is
crucial for real-world applications. In this paper, we propose compositional
task configurations, a set of prompts prepended to the encoder to improve
cross-task generalization of unified models. We design the task configurations
to explicitly specify the task type, as well as its input and output types. We
show that this not only allows the model to better learn shared knowledge
across different tasks at training, but also allows us to control the model by
composing new configurations that apply novel input-output combinations in a
zero-shot manner. We demonstrate via experiments over ten table-to-text tasks
that our method outperforms the UnifiedSKG baseline by noticeable margins in
both in-domain and zero-shot settings, with average improvements of +0.5 and
+12.6 from using a T5-large backbone, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance of Synthesizing High-quality Data for Text-to-SQL Parsing. (arXiv:2212.08785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08785">
<div class="article-summary-box-inner">
<span><p>Recently, there has been increasing interest in synthesizing data to improve
downstream text-to-SQL tasks. In this paper, we first examined the existing
synthesized datasets and discovered that state-of-the-art text-to-SQL
algorithms did not further improve on popular benchmarks when trained with
augmented synthetic data. We observed two shortcomings: illogical synthetic SQL
queries from independent column sampling and arbitrary table joins. To address
these issues, we propose a novel synthesis framework that incorporates key
relationships from schema, imposes strong typing, and conducts
schema-distance-weighted column sampling. We also adopt an intermediate
representation (IR) for the SQL-to-text task to further improve the quality of
the generated natural language questions. When existing powerful semantic
parsers are pre-finetuned on our high-quality synthesized data, our experiments
show that these models have significant accuracy boosts on popular benchmarks,
including new state-of-the-art performance on Spider.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08802">
<div class="article-summary-box-inner">
<span><p>We present Relational Sentence Embedding (RSE), a new paradigm to further
discover the potential of sentence embeddings. Prior work mainly models the
similarity between sentences based on their embedding distance. Because of the
complex semantic meanings conveyed, sentence pairs can have various relation
types, including but not limited to entailment, paraphrasing, and
question-answer. It poses challenges to existing embedding methods to capture
such relational information. We handle the problem by learning associated
relational embeddings. Specifically, a relation-wise translation operation is
applied to the source sentence to infer the corresponding target sentence with
a pre-trained Siamese-based encoder. The fine-grained relational similarity
scores can be computed from learned embeddings. We benchmark our method on 19
datasets covering a wide range of tasks, including semantic textual similarity,
transfer, and domain-specific tasks. Experimental results show that our method
is effective and flexible in modeling sentence relations and outperforms a
series of state-of-the-art sentence embedding methods.
https://github.com/BinWang28/RSE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Datastore, Better Translation: Generating Datastores from Pre-Trained Models for Nearest Neural Machine Translation. (arXiv:2212.08822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08822">
<div class="article-summary-box-inner">
<span><p>Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method
of augmenting neural machine translation (NMT) with a token-level nearest
neighbor retrieval mechanism. The effectiveness of kNNMT directly depends on
the quality of retrieved neighbors. However, original kNNMT builds datastores
based on representations from NMT models, which would result in poor retrieval
accuracy when NMT models are not good enough, leading to sub-optimal
translation performance. In this paper, we propose PRED, a framework that
leverages Pre-trained models for Datastores in kNN-MT. Better representations
from pre-trained models allow us to build datastores of better quality. We also
design a novel contrastive alignment objective to mitigate the representation
gap between the NMT model and pre-trained models, enabling the NMT model to
retrieve from better datastores. We conduct extensive experiments on both
bilingual and multilingual translation benchmarks, including WMT17 English
$\leftrightarrow$ Chinese, WMT14 English $\leftrightarrow$ German, IWSLT14
German $\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical
results demonstrate the effectiveness of PRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation. (arXiv:2212.08841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08841">
<div class="article-summary-box-inner">
<span><p>Dense retrievers have made significant strides in obtaining state-of-the-art
results on text retrieval and open-domain question answering (ODQA). Yet most
of these achievements were made possible with the help of large annotated
datasets, unsupervised learning for dense retrieval models remains an open
problem. In this work, we explore two categories of methods for creating pseudo
query-document pairs, named query extraction (QExt) and transferred query
generation (TQGen), to augment the retriever training in an annotation-free and
scalable manner. Specifically, QExt extracts pseudo queries by document
structures or selecting salient random spans, and TQGen utilizes generation
models trained for other NLP tasks (e.g., summarization) to produce pseudo
queries. Extensive experiments show that dense retrievers trained with
individual augmentation methods can perform comparably well with multiple
strong baselines, and combining them leads to further improvements, achieving
state-of-the-art performance of unsupervised dense retrieval on both BEIR and
ODQA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08853">
<div class="article-summary-box-inner">
<span><p>Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">'If you build they will come': Automatic Identification of News-Stakeholders to detect Party Preference in News Coverage. (arXiv:2212.08864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08864">
<div class="article-summary-box-inner">
<span><p>The coverage of different stakeholders mentioned in the news articles
significantly impacts the slant or polarity detection of the concerned news
publishers. For instance, the pro-government media outlets would give more
coverage to the government stakeholders to increase their accessibility to the
news audiences. In contrast, the anti-government news agencies would focus more
on the views of the opponent stakeholders to inform the readers about the
shortcomings of government policies. In this paper, we address the problem of
stakeholder extraction from news articles and thereby determine the inherent
bias present in news reporting. Identifying potential stakeholders in
multi-topic news scenarios is challenging because each news topic has different
stakeholders. The research presented in this paper utilizes both contextual
information and external knowledge to identify the topic-specific stakeholders
from news articles. We also apply a sequential incremental clustering algorithm
to group the entities with similar stakeholder types. We carried out all our
experiments on news articles on four Indian government policies published by
numerous national and international news agencies. We also further generalize
our system, and the experimental results show that the proposed model can be
extended to other news topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Rich Textual User-Product Context for Improving Sentiment Analysis. (arXiv:2212.08888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08888">
<div class="article-summary-box-inner">
<span><p>User and product information associated with a review is useful for sentiment
polarity prediction. Typical approaches incorporating such information focus on
modeling users and products as implicitly learned representation vectors. Most
do not exploit the potential of historical reviews, or those that currently do
require unnecessary modifications to model architecture or do not make full use
of user/product associations. The contribution of this work is twofold: i) a
method to explicitly employ historical reviews belonging to the same
user/product to initialize representations, and ii) efficient incorporation of
textual associations between users and products via a user-product
cross-context module. Experiments on IMDb, Yelp-2013 and Yelp-2014 benchmarks
show that our approach substantially outperforms previous state-of-the-art.
Since we employ BERT-base as the encoder, we additionally provide experiments
in which our approach performs well with Span-BERT and Longformer. Furthermore,
experiments where the reviews of each user/product in the training data are
downsampled demonstrate the effectiveness of our approach under a low-resource
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering Performance through Manual Annotation: Costs, Benefits and Strategies. (arXiv:2212.08897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08897">
<div class="article-summary-box-inner">
<span><p>Recently proposed systems for open-domain question answering (OpenQA) require
large amounts of training data to achieve state-of-the-art performance.
However, data annotation is known to be time-consuming and therefore expensive
to acquire. As a result, the appropriate datasets are available only for a
handful of languages (mainly English and Chinese). In this work, we introduce
and publicly release PolQA, the first Polish dataset for OpenQA. It consists of
7,000 questions, 87,525 manually labeled evidence passages, and a corpus of
over 7,097,322 candidate passages. Each question is classified according to its
formulation, type, as well as entity type of the answer. This resource allows
us to evaluate the impact of different annotation choices on the performance of
the QA system and propose an efficient annotation strategy that increases the
passage retrieval performance by 10.55 p.p. while reducing the annotation cost
by 82%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL. (arXiv:2212.08902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08902">
<div class="article-summary-box-inner">
<span><p>The task of text-to-SQL is to convert a natural language question to its
corresponding SQL query in the context of relational tables. Existing
text-to-SQL parsers generate a "plausible" SQL query for an arbitrary user
question, thereby failing to correctly handle problematic user questions. To
formalize this problem, we conduct a preliminary study on the observed
ambiguous and unanswerable cases in text-to-SQL and summarize them into 6
feature categories. Correspondingly, we identify the causes behind each
category and propose requirements for handling ambiguous and unanswerable
questions. Following this study, we propose a simple yet effective
counterfactual example generation approach for the automatic generation of
ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a
weakly supervised model DTE (Detecting-Then-Explaining) for error detection,
localization, and explanation. Experimental results show that our model
achieves the best result on both real-world examples and generated examples
compared with various baselines. We will release data and code for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Styles in Neural Machine Translation with Activation Prompt. (arXiv:2212.08909v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08909">
<div class="article-summary-box-inner">
<span><p>Neural machine translation(NMT) has aroused wide attention due to its
impressive quality. Beyond quality, controlling translation styles is also an
important demand for many languages. Previous related studies mainly focus on
controlling formality and gain some improvements. However, they still face two
challenges. The first is the evaluation limitation. Style contains abundant
information including lexis, syntax, etc. But only formality is well studied.
The second is the heavy reliance on iterative fine-tuning when new styles are
required. Correspondingly, this paper contributes in terms of the benchmark and
approach. First, we re-visit this task and propose a multiway stylized machine
translation (MSMT) benchmark, which includes multiple categories of styles in
four language directions to push the boundary of this task. Second, we propose
a method named style activation prompt (StyleAP) by retrieving prompts from
stylized monolingual corpus, which needs no extra fine-tuning. Experiments show
that StyleAP could effectively control the style of translation and achieve
remarkable performance. All of our data and code are released at
https://github.com/IvanWang0730/StyleAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation. (arXiv:2212.08911v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08911">
<div class="article-summary-box-inner">
<span><p>To alleviate the data scarcity problem in End-to-end speech translation (ST),
pre-training on data for speech recognition and machine translation is
considered as an important technique. However, the modality gap between speech
and text prevents the ST model from efficiently inheriting knowledge from the
pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It
adapts the speech features with a new shrinking mechanism to mitigate the
length mismatch between speech and text features by predicting word boundaries.
Experiments on the MUST-C dataset demonstrate that AdaTranS achieves better
performance than the other shrinking-based methods, with higher inference speed
and lower memory usage. Further experiments also show that AdaTranS can be
equipped with additional alignment losses to further improve performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim Optimization in Computational Argumentation. (arXiv:2212.08913v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08913">
<div class="article-summary-box-inner">
<span><p>An optimal delivery of arguments is key to persuasion in any debate, both for
humans and for AI systems. This requires the use of clear and fluent claims
relevant to the given debate. Prior work has studied the automatic assessment
of argument quality extensively. Yet, no approach actually improves the quality
so far. Our work is the first step towards filling this gap. We propose the
task of claim optimization: to rewrite argumentative claims to optimize their
delivery. As an initial approach, we first generate a candidate set of
optimized claims using a sequence-to-sequence model, such as BART, while taking
into account contextual information. Our key idea is then to rerank generated
candidates with respect to different quality metrics to find the best
optimization. In automatic and human evaluation, we outperform different
reranking baselines on an English corpus, improving 60% of all claims
(worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our
approach often specifies claims with details, whereas it adds less evidence
than humans do. Moreover, its capabilities generalize well to other domains,
such as instructional texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Beam Search Reranking. (arXiv:2212.08926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08926">
<div class="article-summary-box-inner">
<span><p>Reranking methods in machine translation aim to close the gap between common
evaluation metrics (e.g. BLEU) and maximum likelihood learning and decoding
algorithms. Prior works address this challenge by training models to rerank
beam search candidates according to their predicted BLEU scores, building upon
large models pretrained on massive monolingual corpora -- a privilege that was
never made available to the baseline translation model. In this work, we
examine a simple approach for training rerankers to predict translation
candidates' BLEU scores without introducing additional data or parameters. Our
approach can be used as a clean baseline, decoupled from external factors, for
future research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Information Extraction with Cross-Task and Cross-Instance High-Order Modeling. (arXiv:2212.08929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08929">
<div class="article-summary-box-inner">
<span><p>Prior works on Information Extraction (IE) typically predict different tasks
and instances (e.g., event triggers, entities, roles, relations) independently,
while neglecting their interactions and leading to model inefficiency. In this
work, we introduce a joint IE framework, HighIE, that learns and predicts
multiple IE tasks by integrating high-order cross-task and cross-instance
dependencies. Specifically, we design two categories of high-order factors:
homogeneous factors and heterogeneous factors. Then, these factors are utilized
to jointly predict labels of all instances. To address the intractability
problem of exact high-order inference, we incorporate a high-order neural
decoder that is unfolded from a mean-field variational inference method. The
experimental results show that our approach achieves consistent improvements on
three IE tasks compared with our baseline and prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards leveraging latent knowledge and Dialogue context for real-world conversational question answering. (arXiv:2212.08946v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08946">
<div class="article-summary-box-inner">
<span><p>In many real-world scenarios, the absence of external knowledge source like
Wikipedia restricts question answering systems to rely on latent internal
knowledge in limited dialogue data. In addition, humans often seek answers by
asking several questions for more comprehensive information. As the dialog
becomes more extensive, machines are challenged to refer to previous
conversation rounds to answer questions. In this work, we propose to leverage
latent knowledge in existing conversation logs via a neural Retrieval-Reading
system, enhanced with a TFIDF-based text summarizer refining lengthy
conversational history to alleviate the long context issue. Our experiments
show that our Retrieval-Reading system can exploit retrieved background
knowledge to generate significantly better answers. The results also indicate
that our context summarizer significantly helps both the retriever and the
reader by introducing more concise and less noisy contextual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the C: Retargetable Decompilation using Neural Machine Translation. (arXiv:2212.08950v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08950">
<div class="article-summary-box-inner">
<span><p>The problem of reversing the compilation process, decompilation, is an
important tool in reverse engineering of computer software. Recently,
researchers have proposed using techniques from neural machine translation to
automate the process in decompilation. Although such techniques hold the
promise of targeting a wider range of source and assembly languages, to date
they have primarily targeted C code. In this paper we argue that existing
neural decompilers have achieved higher accuracy at the cost of requiring
language-specific domain knowledge such as tokenizers and parsers to build an
abstract syntax tree (AST) for the source language, which increases the
overhead of supporting new languages. We explore a different tradeoff that, to
the extent possible, treats the assembly and source languages as plain text,
and show that this allows us to build a decompiler that is easily retargetable
to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on
Go, Fortran, OCaml, and C, and examine the impact of parameters such as
tokenization and training data selection on the quality of decompilation,
finding that it achieves comparable decompilation results to prior work in
neural decompilation with significantly less domain knowledge. We will release
our training data, trained decompilation models, and code to help encourage
future research into language-agnostic decompilation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language model acceptability judgements are not always robust to context. (arXiv:2212.08979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08979">
<div class="article-summary-box-inner">
<span><p>Targeted syntactic evaluations of language models ask whether models show
stable preferences for syntactically acceptable content over minimal-pair
unacceptable inputs. Most targeted syntactic evaluation datasets ask models to
make these judgements with just a single context-free sentence as input. This
does not match language models' training regime, in which input sentences are
always highly contextualized by the surrounding corpus. This mismatch raises an
important question: how robust are models' syntactic judgements in different
contexts? In this paper, we investigate the stability of language models'
performance on targeted syntactic evaluations as we vary properties of the
input context: the length of the context, the types of syntactic phenomena it
contains, and whether or not there are violations of grammaticality. We find
that model judgements are generally robust when placed in randomly sampled
linguistic contexts. However, they are substantially unstable for contexts
containing syntactic structures matching those in the critical test content.
Among all tested models (GPT-2 and five variants of OPT), we significantly
improve models' judgements by providing contexts with matching syntactic
structures, and conversely significantly worsen them using unacceptable
contexts with matching but violated syntactic structures. This effect is
amplified by the length of the context, except for unrelated inputs. We show
that these changes in model performance are not explainable by simple features
matching the context and the test inputs, such as lexical overlap and
dependency overlap. This sensitivity to highly specific syntactic features of
the context can only be explained by the models' implicit in-context learning
abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Authorship Style Transfer with In-Context Learning. (arXiv:2212.08986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08986">
<div class="article-summary-box-inner">
<span><p>Authorship style transfer involves altering the style of text to match the
style of some target author whilst preserving the semantic meaning of the
original text. Existing approaches to unsupervised authorship style transfer
like STRAP have largely focused on style transfer for target authors with many
examples of their writing style through books, speeches, or other published
works (Krishna et al., 2020). Due to this high-resource training data
requirement (often greater than 100,000 words), these approaches are often only
useful for style transfer to the style of published authors, politicians, or
other well-known figures and authorship styles. In this paper, we attempt to
perform low-resource authorship style transfer, a more challenging class of
authorship style transfer where only a limited amount of text in the target
author's style may exist. In our experiments, we specifically choose source and
target authors from Reddit to perform style transfer over their Reddit posts,
limiting ourselves to just 16 posts (on average $\approx$ 500 words) of the
target author's style. We then propose a method for automatic evaluation on the
low-resource authorship style transfer task utilizing authorship and style
representation embeddings (Rivera-Soto et al., 2021; Wegmann et al., 2022). We
evaluate our style transferred outputs with the proposed automatic evaluation
method and find that our method, STYLL, is able to outperform STRAP and a
comprehensive set of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter Dataset. (arXiv:2212.08987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08987">
<div class="article-summary-box-inner">
<span><p>Most recent semantic frame parsing systems for spoken language understanding
(SLU) are designed based on recurrent neural networks. These systems display
decent performance on benchmark SLU datasets such as ATIS or SNIPS, which
contain short utterances with relatively simple patterns. However, the current
semantic frame parsing models lack a mechanism to handle out-of-distribution
(\emph{OOD}) patterns and out-of-vocabulary (\emph{OOV}) tokens. In this paper,
we introduce a robust semantic frame parsing pipeline that can handle both
\emph{OOD} patterns and \emph{OOV} tokens in conjunction with a new complex
Twitter dataset that contains long tweets with more \emph{OOD} patterns and
\emph{OOV} tokens. The new pipeline demonstrates much better results in
comparison to state-of-the-art baseline SLU models on both the SNIPS dataset
and the new Twitter dataset (Our new Twitter dataset can be downloaded from
https://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also
build an E2E application to demo the feasibility of our algorithm and show why
it is useful in real application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoE: a Panel of Experts for Generalized Automatic Dialogue Assessment. (arXiv:2212.08992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08992">
<div class="article-summary-box-inner">
<span><p>Chatbots are expected to be knowledgeable across multiple domains, e.g. for
daily chit-chat, exchange of information, and grounding in emotional
situations. To effectively measure the quality of such conversational agents, a
model-based automatic dialogue evaluation metric (ADEM) is expected to perform
well across multiple domains. Despite significant progress, an ADEM that works
well in one domain does not necessarily generalize to another. This calls for a
dedicated network architecture for domain generalization. To tackle the
multi-domain dialogue evaluation task, we propose a Panel of Experts (PoE), a
multitask network that consists of a shared transformer encoder and a
collection of lightweight adapters. The shared encoder captures the general
knowledge of dialogues across domains, while each adapter specializes in one
specific domain and serves as a domain expert. To validate the idea, we
construct a high-quality multi-domain dialogue dataset leveraging data
augmentation and pseudo-labeling. The PoE network is comprehensively assessed
on 16 dialogue evaluation datasets spanning a wide range of dialogue domains.
It achieves state-of-the-art performance in terms of mean Spearman correlation
over all the evaluation datasets. It exhibits better zero-shot generalization
than existing state-of-the-art ADEMs and the ability to easily adapt to new
domains with few-shot transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Sentiment Analysis in Fake Review Detection. (arXiv:2212.08995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08995">
<div class="article-summary-box-inner">
<span><p>Fake review identification is an important topic and has gained the interest
of experts all around the world. Identifying fake reviews is challenging for
researchers, and there are several primary challenges to fake review detection.
We propose developing an initial research paper for investigating fake reviews
by using sentiment analysis. Ten research papers are identified that show fake
reviews, and they discuss currently available solutions for predicting or
detecting fake reviews. They also show the distribution of fake and truthful
reviews through the analysis of sentiment. We summarize and compare previous
studies related to fake reviews. We highlight the most significant challenges
in the sentiment evaluation process and demonstrate that there is a significant
impact on sentiment scores used to identify fake feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-level Feedback Generation for English Language Learners: Does Data Augmentation Help?. (arXiv:2212.08999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08999">
<div class="article-summary-box-inner">
<span><p>In this paper, we present strong baselines for the task of Feedback Comment
Generation for Writing Learning. Given a sentence and an error span, the task
is to generate a feedback comment explaining the error. Sentences and feedback
comments are both in English. We experiment with LLMs and also create multiple
pseudo datasets for the task, investigating how it affects the performance of
our system. We present our results for the task along with extensive analysis
of the generated comments with the aim of aiding future studies in feedback
comment generation for English language learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Coreference Resolution based on Reinforcement Learning. (arXiv:2212.09028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09028">
<div class="article-summary-box-inner">
<span><p>The target of a coreference resolution system is to cluster all mentions that
refer to the same entity in a given context. All coreference resolution systems
need to solve two subtasks; one task is to detect all of the potential
mentions, and the other is to learn the linking of an antecedent for each
possible mention. In this paper, we propose a reinforcement learning
actor-critic-based neural coreference resolution system, which can achieve both
mention detection and mention clustering by leveraging an actor-critic deep
reinforcement learning technique and a joint training algorithm. We experiment
on the BERT model to generate different input span representations. Our model
with the BERT span representation achieves the state-of-the-art performance
among the models on the CoNLL-2012 Shared Task English Test Set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Preferences across Languages on Community Question Answering Platforms. (arXiv:2212.09045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09045">
<div class="article-summary-box-inner">
<span><p>With the steady emergence of community question answering (CQA) platforms
like Quora, StackExchange, and WikiHow, users now have an unprecedented access
to information on various kind of queries and tasks. Moreover, the rapid
proliferation and localization of these platforms spanning geographic and
linguistic boundaries offer a unique opportunity to study the task requirements
and preferences of users in different socio-linguistic groups. In this study,
we implement an entity-embedding model trained on a large longitudinal dataset
of multi-lingual and task-oriented question-answer pairs to uncover and
quantify the (i) prevalence and distribution of various online tasks across
linguistic communities, and (ii) emerging and receding trends in task
popularity over time in these communities. Our results show that there exists
substantial variance in task preference as well as popularity trends across
linguistic communities on the platform. Findings from this study will help Q&amp;A
platforms better curate and personalize content for non-English users, while
also offering valuable insights to businesses looking to target non-English
speaking communities online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction. (arXiv:2212.09052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09052">
<div class="article-summary-box-inner">
<span><p>Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,
sentiment and opinion term triplets from sentences. Since the initial datasets
used to evaluate models on ASTE had flaws, several studies later corrected the
initial datasets and released new versions of the datasets independently. As a
result, different studies select different versions of datasets to evaluate
their methods, which makes ASTE-related works hard to follow. In this paper, we
analyze the relation between different versions of datasets and suggest that
the entire-space version should be used for ASTE. Besides the sentences
containing triplets and the triplets in the sentences, the entire-space version
additionally includes the sentences without triplets and the aspect terms which
do not belong to any triplets. Hence, the entire-space version is consistent
with real-world scenarios and evaluating models on the entire-space version can
better reflect the models' performance in real-world scenarios. In addition,
experimental results show that evaluating models on non-entire-space datasets
inflates the performance of existing models and models trained on the
entire-space version can obtain better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Digital "Echo Chambers": The Role of Viewpoint Diversity in Political Discussion. (arXiv:2212.09056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09056">
<div class="article-summary-box-inner">
<span><p>Increasingly taking place in online spaces, modern political conversations
are typically perceived to be unproductively affirming -- siloed in so called
``echo chambers'' of exclusively like-minded discussants. Yet, to date we lack
sufficient means to measure viewpoint diversity in conversations. To this end,
in this paper, we operationalize two viewpoint metrics proposed for recommender
systems and adapt them to the context of social media conversations. This is
the first study to apply these two metrics (Representation and Fragmentation)
to real world data and to consider the implications for online conversations
specifically. We apply these measures to two topics -- daylight savings time
(DST), which serves as a control, and the more politically polarized topic of
immigration. We find that the diversity scores for both Fragmentation and
Representation are lower for immigration than for DST. Further, we find that
while pro-immigrant views receive consistent pushback on the platform,
anti-immigrant views largely operate within echo chambers. We observe less
severe yet similar patterns for DST. Taken together, Representation and
Fragmentation paint a meaningful and important new picture of viewpoint
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEATs: Audio Pre-Training with Acoustic Tokenizers. (arXiv:2212.09058v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09058">
<div class="article-summary-box-inner">
<span><p>The massive growth of self-supervised learning (SSL) has been witnessed in
language, vision, speech, and audio domains over the past few years. While
discrete label prediction is widely adopted for other modalities, the
state-of-the-art audio SSL models still employ reconstruction loss for
pre-training. Compared with reconstruction loss, semantic-rich discrete label
prediction encourages the SSL model to abstract the high-level audio semantics
and discard the redundant details as in human perception. However, a
semantic-rich acoustic tokenizer for general audio pre-training is usually not
straightforward to obtain, due to the continuous property of audio and
unavailable phoneme sequences like speech. To tackle this challenge, we propose
BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder
representation from Audio Transformers, where an acoustic tokenizer and an
audio SSL model are optimized by iterations. In the first iteration, we use
random projection as the acoustic tokenizer to train an audio SSL model in a
mask and label prediction manner. Then, we train an acoustic tokenizer for the
next iteration by distilling the semantic knowledge from the pre-trained or
fine-tuned audio SSL model. The iteration is repeated with the hope of mutual
promotion of the acoustic tokenizer and audio SSL model. The experimental
results demonstrate our acoustic tokenizers can generate discrete labels with
rich audio semantics and our audio SSL models achieve state-of-the-art results
across various audio classification benchmarks, even outperforming previous
models that use more training data and model parameters significantly.
Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for
audio-only models without using any external data, and 98.1% accuracy on
ESC-50. The code and pre-trained models are available at https://aka.ms/beats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let's Negotiate! A Survey of Negotiation Dialogue Systems. (arXiv:2212.09072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09072">
<div class="article-summary-box-inner">
<span><p>Negotiation is one of the crucial abilities in human communication, and there
has been a resurgent research interest in negotiation dialogue systems
recently, which goal is to empower intelligent agents with such ability that
can efficiently help humans resolve conflicts or reach beneficial agreements.
Although there have been many explorations in negotiation dialogue systems, a
systematic review of this task has to date remained notably absent. To this
end, we aim to fill this gap by reviewing contemporary studies in the emerging
field of negotiation dialogue systems, covering benchmarks, evaluations, and
methodologies. Furthermore, we also discuss potential future directions,
including multi-modal, multi-party, and cross-cultural negotiation scenarios.
Our goal is to provide the community with a systematic overview of negotiation
dialogue systems and to inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesis and Evaluation of a Domain-specific Large Data Set for Dungeons & Dragons. (arXiv:2212.09080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09080">
<div class="article-summary-box-inner">
<span><p>This paper introduces the Forgotten Realms Wiki (FRW) data set and domain
specific natural language generation using FRW along with related analyses.
Forgotten Realms is the de-facto default setting of the popular open ended
tabletop fantasy role playing game, Dungeons &amp; Dragons. The data set was
extracted from the Forgotten Realms Fandom wiki consisting of more than over
45,200 articles. The FRW data set is constituted of 11 sub-data sets in a
number of formats: raw plain text, plain text annotated by article title,
directed link graphs, wiki info-boxes annotated by the wiki article title,
Poincar\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models
of the corpus. This is the first data set of this size for the Dungeons &amp;
Dragons domain. We then present a pairwise similarity comparison benchmark
which utilizes similarity measures. In addition, we perform D&amp;D domain specific
natural language generation using the corpus and evaluate the named entity
classification with respect to the lore of Forgotten Realms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09086">
<div class="article-summary-box-inner">
<span><p>We investigate response generation for multi-turn dialogue in
generative-based chatbots. Existing generative models based on RNNs (Recurrent
Neural Networks) usually employ the last hidden state to summarize the
sequences, which makes models unable to capture the subtle variability observed
in different dialogues and cannot distinguish the differences between dialogues
that are similar in composition. In this paper, we propose a Pseudo-Variational
Gated Recurrent Unit (PVGRU) component without posterior knowledge through
introducing a recurrent summarizing variable into the GRU, which can aggregate
the accumulated distribution variations of subsequences. PVGRU can perceive the
subtle semantic variability through summarizing variables that are optimized by
the devised distribution consistency and reconstruction objectives. In
addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model
based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve
the diversity and relevance of responses on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09095">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, suggesting that
induction heads are among the heads capable of more sophisticated behaviors
associated with in-context learning. Overall, our study provides several
insights that indicate large language models may be under-trained to perform
in-context learning and opens up questions on how to pre-train language models
to more effectively perform in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continually Learning from Existing Models: Knowledge Accumulation for Neural Machine Translation. (arXiv:2212.09097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09097">
<div class="article-summary-box-inner">
<span><p>Although continually extending an existing NMT model to new domains or
languages has attracted intensive interest in recent years, the equally
valuable problem of continually improving a given NMT model in its domain by
leveraging knowledge from an unlimited number of existing NMT models is not
explored yet. To facilitate the study, we propose a formal definition for the
problem named knowledge accumulation for NMT (KA-NMT) with corresponding
datasets and evaluation metrics and develop a novel method for KA-NMT. We
investigate a novel knowledge detection algorithm to identify beneficial
knowledge from existing models at token level, and propose to learn from
beneficial knowledge and learn against other knowledge simultaneously to
improve learning efficiency. To alleviate catastrophic forgetting, we further
propose to transfer knowledge from previous to current version of the given
model. Extensive experiments show that our proposed method significantly and
consistently outperforms representative baselines under homogeneous,
heterogeneous, and malicious model settings for different language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning. (arXiv:2212.09104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09104">
<div class="article-summary-box-inner">
<span><p>A hallmark of human intelligence is the ability to learn new concepts purely
from language. Several recent approaches have explored training machine
learning models via natural language supervision. However, these approaches
fall short in leveraging linguistic quantifiers (such as 'always' or 'rarely')
and mimicking humans in compositionally learning complex tasks. Here, we
present LaSQuE, a method that can learn zero-shot classifiers from language
explanations by using three new strategies - (1) modeling the semantics of
linguistic quantifiers in explanations (including exploiting ordinal strength
relationships, such as 'always' &gt; 'likely'), (2) aggregating information from
multiple explanations using an attention-based mechanism, and (3) model
training via curriculum learning. With these strategies, LaSQuE outperforms
prior work, showing an absolute gain of up to 7% in generalizing to unseen
real-world classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Sampling for Dense Retrieval with Document Expansion. (arXiv:2212.09114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09114">
<div class="article-summary-box-inner">
<span><p>The dual-encoder has become the de facto architecture for dense retrieval.
Typically, it computes the latent representations of the query and document
independently, thus failing to fully capture the interactions between the query
and document. To alleviate this, recent work expects to get query-informed
representations of documents. During training, it expands the document with a
real query, while replacing the real query with a generated pseudo query at
inference. This discrepancy between training and inference makes the dense
retrieval model pay more attention to the query information but ignore the
document when computing the document representation. As a result, it even
performs worse than the vanilla dense retrieval model, since its performance
depends heavily on the relevance between the generated queries and the real
query. In this paper, we propose a curriculum sampling strategy, which also
resorts to the pseudo query at training and gradually increases the relevance
of the generated query to the real query. In this way, the retrieval model can
learn to extend its attention from the document only to both the document and
query, hence getting high-quality query-informed document representations.
Experimental results on several passage retrieval datasets show that our
approach outperforms the previous dense retrieval methods1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing. (arXiv:2212.09125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09125">
<div class="article-summary-box-inner">
<span><p>Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,
president, politician) of a given entity mention (e.g., Joe Biden) in context.
State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.
CE concatenates the mention (and its context) with each type and feeds the
pairs into a pretrained language model (PLM) to score their relevance. It
brings deeper interaction between mention and types to reach better performance
but has to perform N (type set size) forward passes to infer types of a single
mention. CE is therefore very slow in inference when the type set is large
(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a
recall-expand-filter manner. The recall and expand stages prune the large type
set and generate K (K is typically less than 256) most relevant type candidates
for each mention. At the filter stage, we use a novel model called MCCE to
concurrently encode and score these K candidates in only one forward pass to
obtain the final type prediction. We investigate different variants of MCCE and
extensive experiments show that MCCE under our paradigm reaches SOTA
performance on ultra-fine entity typing and is thousands of times faster than
the cross-encoder. We also found MCCE is very effective in fine-grained (130
types) and coarse-grained (9 types) entity typing. Our code is available at
\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars. (arXiv:2212.09140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09140">
<div class="article-summary-box-inner">
<span><p>We study grammar induction with mildly context-sensitive grammars for
unsupervised discontinuous parsing. Using the probabilistic linear context-free
rewriting system (LCFRS) formalism, our approach fixes the rule structure in
advance and focuses on parameter learning with maximum likelihood. To reduce
the computational complexity of both parsing and parameter estimation, we
restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)
and further discard rules that require O(n^6) time to parse, reducing inference
to O(n^5). We find that using a large number of nonterminals is beneficial and
thus make use of tensor decomposition-based rank-space dynamic programming with
an embedding-based parameterization of rule probabilities to scale up the
number of nonterminals. Experiments on German and Dutch show that our approach
is able to induce linguistically meaningful trees with continuous and
discontinuous structures
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09146">
<div class="article-summary-box-inner">
<span><p>The emergence of large pretrained models has enabled language models to
achieve superior performance in common NLP tasks, including language modeling
and question answering, compared to previous static word representation
methods. Augmenting these models with a retriever to retrieve the related text
and documents as supporting information has shown promise in effectively
solving NLP problems in a more interpretable way given that the additional
knowledge is injected explicitly rather than being captured in the models'
parameters. In spite of the recent progress, our analysis on
retriever-augmented language models shows that this class of language models
still lack reasoning over the retrieved documents. In this paper, we study the
strengths and weaknesses of different retriever-augmented language models such
as REALM, kNN-LM, FiD, ATLAS, and Flan-T5 in reasoning over the selected
documents in different tasks. In particular, we analyze the reasoning failures
of each of these models and study how the models' failures in reasoning are
rooted in the retriever module as well as the language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Isotropy and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09170">
<div class="article-summary-box-inner">
<span><p>Incorporating contrastive learning objectives in sentence representation
learning (SRL) has yielded significant improvements on many sentence-level NLP
tasks. However, It is not well understood why contrastive learning works for
learning sentence-level semantics. In this paper, we take a closer look at
contrastive sentence representation learning through the lens of isotropy and
learning dynamics. We interpret its success stories through the geometry of the
representation shifts. We show that contrastive learning brings isotropy, and
surprisingly learns to converge tokens to similar positions in the semantic
space if given the signal that they are in the same sentence. Also, what we
formalize as "spurious contextualization" is mitigated for semantically
meaningful tokens, while augmented for functional ones. The embedding space is
pushed toward the origin during training, with more areas now better defined.
We ablate these findings by observing the learning dynamic with different
training temperatures, batch sizes and pooling methods. With these findings, we
aim to shed light on future designs of sentence representation learning
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09171">
<div class="article-summary-box-inner">
<span><p>As more and more conversational and translation systems are deployed in
production, it is essential to implement and to develop effective control
mechanisms guaranteeing their proper functioning and security. An essential
component to ensure safe system behavior is out-of-distribution (OOD)
detection, which aims at detecting whether an input sample is statistically far
from the training distribution. Although OOD detection is a widely covered
topic in classification tasks, it has received much less attention in text
generation. This paper addresses the problem of OOD detection for machine
translation and dialog generation from an operational perspective. Our
contributions include: (i) RAINPROOF a Relative informAItioN Projection ODD
detection framework; and (ii) a more operational evaluation setting for OOD
detection. Surprisingly, we find that OOD detection is not necessarily aligned
with task-specific measures. The OOD detector may filter out samples that are
well processed by the model and keep samples that are not, leading to weaker
performance. Our results show that RAINPROOF breaks this curse and achieve good
results in OOD detection while increasing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09180">
<div class="article-summary-box-inner">
<span><p>There has been great recent advancement in human-computer chat. However,
proper evaluation currently requires human judgements that produce notoriously
high-variance metrics due to their inherent subjectivity. Furthermore, there is
little standardization in the methods and labels used for evaluation, with an
overall lack of work to compare and assess the validity of various evaluation
approaches. As a consequence, existing evaluation results likely leave an
incomplete picture of the strengths and weaknesses of open-domain chatbots. We
aim towards a dimensional evaluation of human-computer chat that can reliably
measure several distinct aspects of chat quality. To this end, we present our
novel human evaluation method that quantifies the rate of several
quality-related chatbot behaviors. Our results demonstrate our method to be
more suitable for dimensional chat evaluation than alternative likert-style or
comparative methods. We then use our validated method and existing methods to
evaluate four open-domain chat models from the recent literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09196">
<div class="article-summary-box-inner">
<span><p>The recent advent of large language models - large neural networks trained on
a simple predictive objective over a massive corpus of natural language - has
reinvigorated debate over whether human cognitive capacities might emerge in
such generic models given sufficient training data. Of particular interest is
the ability of these models to reason about novel problems zero-shot, without
any direct training on those problems. In human cognition, this capacity is
closely tied to an ability to reason by analogy. Here, we performed a direct
comparison between human reasoners and a large language model (GPT-3) on a
range of analogical tasks, including a novel text-based matrix reasoning task
closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed
a surprisingly strong capacity for abstract pattern induction, matching or even
surpassing human capabilities in most settings. Our results indicate that large
language models such as GPT-3 have acquired an emergent ability to find
zero-shot solutions to a broad range of analogy problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OASum: Large-Scale Open Domain Aspect-based Summarization. (arXiv:2212.09233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09233">
<div class="article-summary-box-inner">
<span><p>Aspect or query-based summarization has recently caught more attention, as it
can generate differentiated summaries based on users' interests. However, the
current dataset for aspect or query-based summarization either focuses on
specific domains, contains relatively small-scale instances, or includes only a
few aspect types. Such limitations hinder further explorations in this
direction. In this work, we take advantage of crowd-sourcing knowledge on
Wikipedia.org and automatically create a high-quality, large-scale open-domain
aspect-based summarization dataset named OASum, which contains more than 3.7
million instances with around 1 million different aspects on 2 million
Wikipedia pages. We provide benchmark results on OAsum and demonstrate its
ability for diverse aspect-based summarization generation. To overcome the data
scarcity problem on specific domains, we also perform zero-shot, few-shot, and
fine-tuning on seven downstream datasets. Specifically, zero/few-shot and
fine-tuning results show that the model pre-trained on our corpus demonstrates
a strong aspect or query-focused generation ability compared with the backbone
model. Our dataset and pre-trained checkpoints are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAL: Persona-Augmented Emotional Support Conversation Generation. (arXiv:2212.09235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09235">
<div class="article-summary-box-inner">
<span><p>Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that our proposed model,
PAL, achieves state-of-the-art results, outperforming the baselines on the
studied benchmark. Our code and data are publicly available at
https://github.com/chengjl19/PAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09246">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models, despite their rapid advancements powered by
scale, still fall short of robust commonsense capabilities. And yet, scale
appears to be the winning recipe; after all, the largest models seem to have
acquired the largest amount of commonsense capabilities. Or is it?
</p>
<p>In this paper, we investigate the possibility of a seemingly impossible
match: can smaller language models with dismal commonsense capabilities (i.e.,
GPT-2), ever win over models that are orders of magnitude larger and better
(i.e., GPT-3), if the smaller models are powered with novel commonsense
distillation algorithms? The key intellectual question we ask here is whether
it is possible, if at all, to design a learning algorithm that does not benefit
from scale, yet leads to a competitive level of commonsense acquisition. In
this work, we study the generative models of commonsense knowledge, focusing on
the task of generating generics, statements of commonsense facts about everyday
concepts, e.g., birds can fly.
</p>
<p>We introduce a novel commonsense distillation framework, I2D2, that loosely
follows the Symbolic Knowledge Distillation of West et al. but breaks the
dependence on the extreme-scale models as the teacher model by two innovations:
(1) the novel adaptation of NeuroLogic Decoding to enhance the generation
quality of the weak, off-the-shelf language models, and (2) self-imitation
learning to iteratively learn from the model's own enhanced commonsense
acquisition capabilities. Empirical results suggest that scale is not the only
way, as novel algorithms can be a promising alternative. Moreover, our study
leads to a new corpus of generics, Gen-A-Tomic, that is of the largest and
highest quality available to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language to Code Generation in Interactive Data Science Notebooks. (arXiv:2212.09248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09248">
<div class="article-summary-box-inner">
<span><p>Computational notebooks, such as Jupyter notebooks, are interactive computing
environments that are ubiquitous among data scientists to perform data
wrangling and analytic tasks. To measure the performance of AI pair programmers
that automatically synthesize programs for those tasks given natural language
(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation
problems using the pandas data analysis framework in data science notebooks.
ARCADE features multiple rounds of NL-to-code problems from the same notebook.
It requires a model to understand rich multi-modal contexts, such as existing
notebook cells and their execution states as well as previous turns of
interaction. To establish a strong baseline on this challenging task, we
develop PaChiNCo, a 62B code language model (LM) for Python computational
notebooks, which significantly outperforms public code LMs. Finally, we explore
few-shot prompting strategies to elicit better code with step-by-step
decomposition and NL explanation, showing the potential to improve the
diversity and explainability of model predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Language Model Behaviors with Model-Written Evaluations. (arXiv:2212.09251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09251">
<div class="article-summary-box-inner">
<span><p>As language models (LMs) scale, they develop many novel behaviors, good and
bad, exacerbating the need to evaluate how they behave. Prior work creates
evaluations with crowdwork (which is time-consuming and expensive) or existing
data sources (which are not always available). Here, we automatically generate
evaluations with LMs. We explore approaches with varying amounts of human
effort, from instructing LMs to write yes/no questions to making complex
Winogender schemas with multiple stages of LM-based generation and filtering.
Crowdworkers rate the examples as highly relevant and agree with 90-100% of
labels, sometimes more so than corresponding human-written datasets. We
generate 154 datasets and discover new cases of inverse scaling where LMs get
worse with size. Larger LMs repeat back a dialog user's preferred answer
("sycophancy") and express greater desire to pursue concerning goals like
resource acquisition and goal preservation. We also find some of the first
examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF
makes LMs worse. For example, RLHF makes LMs express stronger political views
(on gun rights and immigration) and a greater desire to avoid shut down.
Overall, LM-written evaluations are high-quality and let us quickly discover
many novel LM behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems. (arXiv:2212.09252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09252">
<div class="article-summary-box-inner">
<span><p>Many dialogue systems (DSs) lack characteristics humans have, such as emotion
perception, factuality, and informativeness. Enhancing DSs with knowledge
alleviates this problem, but, as many ways of doing so exist, keeping track of
all proposed methods is difficult. Here, we present the first survey of
knowledge-enhanced DSs. We define three categories of systems - internal,
external, and hybrid - based on the knowledge they use. We survey the
motivation for enhancing DSs with knowledge, used datasets, and methods for
knowledge search, knowledge encoding, and knowledge incorporation. Finally, we
propose how to improve existing systems based on theories from linguistics and
cognitive science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization. (arXiv:2212.09254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09254">
<div class="article-summary-box-inner">
<span><p>Robustness evaluation against adversarial examples has become increasingly
important to unveil the trustworthiness of the prevailing deep models in
natural language processing (NLP). However, in contrast to the computer vision
domain where the first-order projected gradient descent (PGD) is used as the
benchmark approach to generate adversarial examples for robustness evaluation,
there lacks a principled first-order gradient-based robustness evaluation
framework in NLP. The emerging optimization challenges lie in 1) the discrete
nature of textual inputs together with the strong coupling between the
perturbation location and the actual content, and 2) the additional constraint
that the perturbed text should be fluent and achieve a low perplexity under a
language model. These challenges make the development of PGD-like NLP attacks
difficult. To bridge the gap, we propose TextGrad, a new attack generator using
gradient-driven optimization, supporting high-accuracy and high-quality
assessment of adversarial robustness in NLP. Specifically, we address the
aforementioned challenges in a unified optimization framework. And we develop
an effective convex relaxation method to co-optimize the continuously-relaxed
site selection and perturbation variables and leverage an effective sampling
method to establish an accurate mapping from the continuous optimization
variables to the discrete textual perturbations. Moreover, as a first-order
attack generation method, TextGrad can be baked into adversarial training to
further improve the robustness of NLP models. Extensive experiments are
provided to demonstrate the effectiveness of TextGrad not only in attack
generation for robustness evaluation but also in adversarial defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi hash embeddings in spaCy. (arXiv:2212.09255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09255">
<div class="article-summary-box-inner">
<span><p>The distributed representation of symbols is one of the key technologies in
machine learning systems today, playing a pivotal role in modern natural
language processing. Traditional word embeddings associate a separate vector
with each word. While this approach is simple and leads to good performance, it
requires a lot of memory for representing a large vocabulary. To reduce the
memory footprint, the default embedding layer in spaCy is a hash embeddings
layer. It is a stochastic approximation of traditional embeddings that provides
unique vectors for a large number of words without explicitly storing a
separate vector for each of them. To be able to compute meaningful
representations for both known and unknown words, hash embeddings represent
each word as a summary of the normalized word form, subword information and
word shape. Together, these features produce a multi-embedding of a word. In
this technical report we lay out a bit of history and introduce the embedding
methods in spaCy in detail. Second, we critically evaluate the hash embedding
architecture with multi-embeddings on Named Entity Recognition datasets from a
variety of domains and languages. The experiments validate most key design
choices behind spaCy's embedders, but we also uncover a few surprising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBoosting: Black-Box Text Classification with Ten Forward Passes. (arXiv:2212.09257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09257">
<div class="article-summary-box-inner">
<span><p>We describe PromptBoosting, a query-efficient procedure for building a text
classifier from a neural language model (LM) without access to the LM's
parameters, gradients, or hidden representations. This form of "black-box"
classifier training has become increasingly important as the cost of training
and inference in large-scale LMs grows. But existing black-box LM classifier
learning approaches are themselves computationally inefficient, typically
specializing LMs to the target task by searching in a large space of (discrete
or continuous) prompts using zeroth-order optimization methods. Instead of
directly optimizing in prompt space, PromptBoosting obtains a small pool of
prompts via a gradient-free approach and then constructs a large pool of weak
learners by pairing these prompts with different elements of the LM's output
distribution. These weak learners are then ensembled using the AdaBoost
algorithm. The entire learning process requires only a small number of forward
passes and no backward pass. Experiments show that PromptBoosting achieves
state-of-the-art performance in multiple black-box few-shot classification
tasks, and matches or outperforms full fine-tuning in both few-shot and
standard learning paradigms, while training 10x faster than existing black-box
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Dataset Evaluation: Reliability, Difficulty, and Validity. (arXiv:2212.09272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09272">
<div class="article-summary-box-inner">
<span><p>Datasets serve as crucial training resources and model performance trackers.
However, existing datasets have exposed a plethora of problems, inducing biased
models and unreliable evaluation results. In this paper, we propose a
model-agnostic dataset evaluation framework for automatic dataset quality
evaluation. We seek the statistical properties of the datasets and address
three fundamental dimensions: reliability, difficulty, and validity, following
a classical testing theory. Taking the Named Entity Recognition (NER) datasets
as a case study, we introduce $9$ statistical metrics for a statistical dataset
evaluation framework. Experimental results and human evaluation validate that
our evaluation framework effectively assesses various aspects of the dataset
quality. Furthermore, we study how the dataset scores on our statistical
metrics affect the model performance, and appeal for dataset quality evaluation
or targeted dataset improvement before training or testing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL. (arXiv:2212.09278v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09278">
<div class="article-summary-box-inner">
<span><p>Conversational text-to-SQL is designed to translate multi-turn natural
language questions into their corresponding SQL queries. Most state-of-the-art
conversational text- to-SQL methods are incompatible with generative
pre-trained language models (PLMs), such as T5. In this paper, we present a
two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs'
ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA
first decomposes the main task into several related sub-tasks and then unifies
them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific
natural language prompts to boost the main task from multi-task training. Later
in the fine-tuning stage, we propose four SQL perturbations to alleviate the
error propagation problem. MIGA tends to achieve state-of-the-art performance
on two benchmarks (SparC and CoSQL). We also provide extensive analyses and
discussions to shed light on some new perspectives for conversational
text-to-SQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09282">
<div class="article-summary-box-inner">
<span><p>Logical reasoning of text is an important ability that requires understanding
the information present in the text, their interconnections, and then reasoning
through them to infer new conclusions. Prior works on improving the logical
reasoning ability of language models require complex processing of training
data (e.g., aligning symbolic knowledge to text), yielding task-specific data
augmentation solutions that restrict the learning of general logical reasoning
skills. In this work, we propose APOLLO, an adaptively pretrained language
model that has improved logical reasoning abilities. We select a subset of
Wikipedia, based on a set of logical inference keywords, for continued
pretraining of a language model. We use two self-supervised loss functions: a
modified masked language modeling loss where only specific parts-of-speech
words, that would likely require more reasoning than basic language
understanding, are masked, and a sentence-level classification loss that
teaches the model to distinguish between entailment and contradiction types of
sentences. The proposed training paradigm is both simple and independent of
task formats. We demonstrate the effectiveness of APOLLO by comparing it with
prior baselines on two logical reasoning datasets. APOLLO performs comparably
on ReClor and outperforms baselines on LogiQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations. (arXiv:2212.09284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09284">
<div class="article-summary-box-inner">
<span><p>Speech systems are sensitive to accent variations. This is especially
challenging in the Indian context, with an abundance of languages but a dearth
of linguistic studies characterising pronunciation variations. The growing
number of L2 English speakers in India reinforces the need to study accents and
L1-L2 interactions. We investigate the accents of Indian English (IE) speakers
and report in detail our observations, both specific and common to all regions.
In particular, we observe the phonemic variations and phonotactics occurring in
the speakers' native languages and apply this to their English pronunciations.
We demonstrate the influence of 18 Indian languages on IE by comparing the
native language pronunciations with IE pronunciations obtained jointly from
existing literature studies and phonetically annotated speech of 80 speakers.
Consequently, we are able to validate the intuitions of Indian language
influences on IE pronunciations by justifying pronunciation rules from the
perspective of Indian language phonology. We obtain a comprehensive description
in terms of universal and region-specific characteristics of IE, which
facilitates accent conversion and adaptation of existing ASR and TTS systems to
different Indian accents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT: The End of Online Exam Integrity?. (arXiv:2212.09292v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09292">
<div class="article-summary-box-inner">
<span><p>This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation. (arXiv:2212.09305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09305">
<div class="article-summary-box-inner">
<span><p>Is it possible to leverage large scale raw and raw parallel corpora to build
a general learned metric? Existing learned metrics have gaps to human
judgements, are model-dependent or are limited to the domains or tasks where
human ratings are available. In this paper, we propose SEScore2, a model-based
metric pretrained over million-scale synthetic dataset constructed by our novel
retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation
to human judgements without any human rating supervisions. Importantly, our
unsupervised SEScore2 can outperform supervised metrics, which are trained on
the News human ratings, at the TED domain. We evaluate SEScore2 over four text
generation tasks across three languages. SEScore2 outperforms all prior
unsupervised evaluation metrics in machine translation, speech translation,
data-to-text and dialogue generation, with average Kendall improvements 0.158.
SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue
generation and overall correlation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text. (arXiv:2212.09306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09306">
<div class="article-summary-box-inner">
<span><p>Identifying named entities such as a person, location or organization, in
documents can highlight key information to readers. Training Named Entity
Recognition (NER) models requires an annotated data set, which can be a
time-consuming labour-intensive task. Nevertheless, there are publicly
available NER data sets for general English. Recently there has been interest
in developing NER for legal text. However, prior work and experimental results
reported here indicate that there is a significant degradation in performance
when NER methods trained on a general English data set are applied to legal
text. We describe a publicly available legal NER data set, called E-NER, based
on legal company filings available from the US Securities and Exchange
Commission's EDGAR data set. Training a number of different NER algorithms on
the general English CoNLL-2003 corpus but testing on our test collection
confirmed significant degradations in accuracy, as measured by the F1-score, of
between 29.4\% and 60.4\%, compared to training and testing on the E-NER
collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension. (arXiv:2212.09353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09353">
<div class="article-summary-box-inner">
<span><p>Open-retrieval conversational machine reading comprehension (OCMRC) simulates
real-life conversational interaction scenes. Machines are required to make a
decision of "Yes/No/Inquire" or generate a follow-up question when the decision
is "Inquire" based on retrieved rule texts, user scenario, user question, and
dialogue history. Recent studies explored the methods to reduce the information
gap between decision-making and question generation and thus improve the
performance of generation. However, the information gap still exists because
these pipeline structures are still limited in decision-making, span
extraction, and question rephrasing three stages. Decision-making and
generation are reasoning separately, and the entailment reasoning utilized in
decision-making is hard to share through all stages. To tackle the above
problem, we proposed a novel one-stage end-to-end framework, called Entailment
Fused-T5 (EFT), to bridge the information gap between decision-making and
generation in a global understanding manner. The extensive experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance on the OR-ShARC benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09359">
<div class="article-summary-box-inner">
<span><p>End-to-end Speech Translation (E2E ST) aims to translate source speech into
target translation without generating the intermediate transcript. However,
existing approaches for E2E ST degrade considerably when only limited ST data
are available. We observe that an ST model's performance strongly correlates
with its embedding similarity from speech and transcript. In this paper, we
propose Word-Aligned COntrastive learning (WACO), a novel method for few-shot
speech-to-text translation. Our key idea is bridging word-level representations
for both modalities via contrastive learning. We evaluate WACO and other
methods on the MuST-C dataset, a widely used ST benchmark. Our experiments
demonstrate that WACO outperforms the best baseline methods by 0.7-8.5 BLEU
points with only 1-hour parallel data. Code is available at
https://anonymous.4open.science/r/WACO .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching Relation Extraction with OpenIE. (arXiv:2212.09376v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09376">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) is a sub-discipline of information extraction (IE)
which focuses on the prediction of a relational predicate from a
natural-language input unit (such as a sentence, a clause, or even a short
paragraph consisting of multiple sentences and/or clauses). Together with
named-entity recognition (NER) and disambiguation (NED), RE forms the basis for
many advanced IE tasks such as knowledge-base (KB) population and verification.
In this work, we explore how recent approaches for open information extraction
(OpenIE) may help to improve the task of RE by encoding structured information
about the sentences' principal units, such as subjects, objects, verbal
phrases, and adverbials, into various forms of vectorized (and hence
unstructured) representations of the sentences. Our main conjecture is that the
decomposition of long and possibly convoluted sentences into multiple smaller
clauses via OpenIE even helps to fine-tune context-sensitive language models
such as BERT (and its plethora of variants) for RE. Our experiments over two
annotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy
of our enriched models compared to existing RE approaches. Our best results
reach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,
proving the effectiveness of our approach on competitive benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Gating: A Parameter Efficient Tuning Method for Zero-Shot Multi-Source Translation. (arXiv:2212.09387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09387">
<div class="article-summary-box-inner">
<span><p>Multi-source translation (MST), which typically receives multiple source
sentences of the same meaning in different languages, has been shown superior
to single-source translation. As the quantity of multi-source parallel data is
limited, taking full advantage of single-source data and limited multi-source
data to make models perform well when receiving as many as possible sources
remains a challenge. Unlike previous work mostly devoted to supervised
scenarios, we focus on zero-shot MST: expecting models to be able to process
unseen combinations of multiple sources, e.g., unseen language combinations,
during inference. We propose a simple yet effective parameter efficient method,
named Prompt Gating, which appends prompts to the model inputs and attaches
gates on the extended hidden states for each encoder layer. It shows strong
zero-shot transferability (+9.0 BLEU points maximally) and remarkable
compositionality (+15.6 BLEU points maximally) on MST, and also shows its
superiorities over baselines on lexically constrained translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-sense embeddings through a word sense disambiguation process. (arXiv:2101.08700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08700">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding has seen an increasing number of publications
in the last few years, especially after robust word embeddings models became
prominent, when they proved themselves able to capture and represent semantic
relationships from massive amounts of data. Nevertheless, traditional models
often fall short in intrinsic issues of linguistics, such as polysemy and
homonymy. Any expert system that makes use of natural language in its core, can
be affected by a weak semantic representation of text, resulting in inaccurate
outcomes based on poor decisions. To mitigate such issues, we propose a novel
approach called Most Suitable Sense Annotation (MSSA), that disambiguates and
annotates each word by its specific sense, considering the semantic effects of
its context. Our approach brings three main contributions to the semantic
representation scenario: (i) an unsupervised technique that disambiguates and
annotates words by their senses, (ii) a multi-sense embeddings model that can
be extended to any traditional word embeddings algorithm, and (iii) a recurrent
methodology that allows our models to be re-used and their representations
refined. We test our approach on six different benchmarks for the word
similarity task, showing that our approach can produce state-of-the-art results
and outperforms several more complex state-of-the-art systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced word embeddings using multi-semantic representation through lexical chains. (arXiv:2101.09023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09023">
<div class="article-summary-box-inner">
<span><p>The relationship between words in a sentence often tells us more about the
underlying semantic content of a document than its actual words, individually.
In this work, we propose two novel algorithms, called Flexible Lexical Chain II
and Fixed Lexical Chain II. These algorithms combine the semantic relations
derived from lexical chains, prior knowledge from lexical databases, and the
robustness of the distributional hypothesis in word embeddings as building
blocks forming a single system. In short, our approach has three main
contributions: (i) a set of techniques that fully integrate word embeddings and
lexical chains; (ii) a more robust semantic representation that considers the
latent relation between words in a document; and (iii) lightweight word
embeddings models that can be extended to any natural language task. We intend
to assess the knowledge of pre-trained models to evaluate their robustness in
the document classification task. The proposed techniques are tested against
seven word embeddings algorithms using five different machine learning
classifiers over six scenarios in the document classification task. Our results
show the integration between lexical chains and word embeddings representations
sustain state-of-the-art results, even against more complex systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings. (arXiv:2105.08585v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08585">
<div class="article-summary-box-inner">
<span><p>It is well-known that typical word embedding methods such as Word2Vec and
GloVe have the property that the meaning can be composed by adding up the
embeddings (additive compositionality). Several theories have been proposed to
explain additive compositionality, but the following questions remain
unanswered: (Q1) The assumptions of those theories do not hold for the
practical word embedding. (Q2) Ordinary additive compositionality can be seen
as an AND operation of word meanings, but it is not well understood how other
operations, such as OR and NOT, can be computed by the embeddings. We address
these issues by the idea of frequency-weighted centering at its core. This
paper proposes a post-processing method for bridging the gap between practical
word embedding and the assumption of theory about additive compositionality as
an answer to (Q1). It also gives a method for taking OR or NOT of the meaning
by linear operation of word embedding as an answer to (Q2). Moreover, we
confirm experimentally that the accuracy of AND operation, i.e., the ordinary
additive compositionality, can be improved by our post-processing method (3.5x
improvement in top-100 accuracy) and that OR and NOT operations can be
performed correctly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00841">
<div class="article-summary-box-inner">
<span><p>Multi-hop machine reading comprehension is a challenging task in natural
language processing, which requires more reasoning ability across multiple
documents. Spectral models based on graph convolutional networks grant
inferring abilities and lead to competitive results. However, part of them
still faces the challenge of analyzing the reasoning in a human-understandable
way. Inspired by the concept of the Grandmother Cells in cognitive
neuroscience, a spatial graph attention framework named ClueReader was proposed
in this paper, imitating the procedure. This model is designed to assemble the
semantic features in multi-level representations and automatically concentrate
or alleviate information for reasoning via the attention mechanism. The name
ClueReader is a metaphor for the pattern of the model: regard the subjects of
queries as the start points of clues, take the reasoning entities as bridge
points, consider the latent candidate entities as the grandmother cells, and
the clues end up in candidate entities. The proposed model allows us to
visualize the reasoning graph, then analyze the importance of edges connecting
two entities and the selectivity in the mention and candidate nodes, which can
be easier to be comprehended empirically. The official evaluations in the
open-domain multi-hop reading dataset WikiHop and the Drug-drug Interactions
dataset MedHop prove the validity of our approach and show the probability of
the application of the model in the molecular biology domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation. (arXiv:2110.07150v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07150">
<div class="article-summary-box-inner">
<span><p>Open-Domain Generative Question Answering has achieved impressive performance
in English by combining document-level retrieval with answer generation. These
approaches, which we refer to as GenQA, can generate complete sentences,
effectively answering both factoid and non-factoid questions. In this paper, we
extend GenQA to the multilingual and cross-lingual settings. For this purpose,
we first introduce GenTyDiQA, an extension of the TyDiQA dataset with
well-formed and complete answers for Arabic, Bengali, English, Japanese, and
Russian. Based on GenTyDiQA, we design a cross-lingual generative model that
produces full-sentence answers by exploiting passages written in multiple
languages, including languages different from the question. Our cross-lingual
generative system outperforms answer sentence selection baselines for all 5
languages and monolingual generative pipelines for three out of five languages
studied.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClimateBert: A Pretrained Language Model for Climate-Related Text. (arXiv:2110.12010v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12010">
<div class="article-summary-box-inner">
<span><p>Over the recent years, large pretrained language models (LM) have
revolutionized the field of natural language processing (NLP). However, while
pretraining on general language has been shown to work very well for common
language, it has been observed that niche language poses problems. In
particular, climate-related texts include specific language that common LMs can
not represent accurately. We argue that this shortcoming of today's LMs limits
the applicability of modern NLP to the broad field of text processing of
climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based
language model that is further pretrained on over 2 million paragraphs of
climate-related texts, crawled from various sources such as common news,
research articles, and climate reporting of companies. We find that CLIMATEBERT
leads to a 48% improvement on a masked language model objective which, in turn,
leads to lowering error rates by 3.57% to 35.71% for various climate-related
downstream tasks like text classification, sentiment analysis, and
fact-checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05337">
<div class="article-summary-box-inner">
<span><p>Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that are more natural and better meet the
specific constraints in practical applications. In recent years, methods using
large-scale pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the lower level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks which may require different types of
controlled constraints. In this paper, we present a systematic critical review
on the common tasks, main approaches and evaluation methods in this area.
Finally, we discuss the challenges that the field is facing, and put forward
various promising future directions. To the best of our knowledge, this is the
first survey paper to summarize CTG techniques from the perspective of PLMs. We
hope it can help researchers in related fields to quickly track the academic
frontier, providing them with a landscape of the area and a roadmap for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05786">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the resurgence of knowledge engineering which is
featured by the fast growth of knowledge graphs. However, most of existing
knowledge graphs are represented with pure symbols, which hurts the machine's
capability to understand the real world. The multi-modalization of knowledge
graphs is an inevitable key step towards the realization of human-level machine
intelligence. The results of this endeavor are Multi-modal Knowledge Graphs
(MMKGs). In this survey on MMKGs constructed by texts and images, we first give
definitions of MMKGs, followed with the preliminaries on multi-modal tasks and
techniques. We then systematically review the challenges, progresses and
opportunities on the construction and application of MMKGs respectively, with
detailed analyses of the strength and weakness of different solutions. We
finalize this survey with open research problems relevant to MMKGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records. (arXiv:2203.03540v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03540">
<div class="article-summary-box-inner">
<span><p>There is an increasing interest in developing artificial intelligence (AI)
systems to process and interpret electronic health records (EHRs). Natural
language processing (NLP) powered by pretrained language models is the key
technology for medical AI systems utilizing clinical narratives. However, there
are few clinical language models, the largest of which trained in the clinical
domain is comparatively small at 110 million parameters (compared with billions
of parameters in the general domain). It is not clear how large clinical
language models with billions of parameters can help medical AI systems utilize
unstructured EHRs. In this study, we develop from scratch a large clinical
language model - GatorTron - using &gt;90 billion words of text (including &gt;82
billion words of de-identified clinical text) and systematically evaluate it on
5 clinical NLP tasks including clinical concept extraction, medical relation
extraction, semantic textual similarity, natural language inference (NLI), and
medical question answering (MQA). We examine how (1) scaling up the number of
parameters and (2) scaling up the size of the training data could benefit these
NLP tasks. GatorTron models scale up the clinical language model from 110
million to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%
and 9.5% improvement in accuracy for NLI and MQA), which can be applied to
medical AI systems to improve healthcare delivery. The GatorTron models are
publicly available at:
https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12131">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have achieved remarkable success in
natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are
pre-trained in an unsupervised manner using the large-scale general corpus. In
the meanwhile, an increasing number of models pre-trained with labeled data
(i.e., ``supervised pre-training'') showcase superior performance compared to
unsupervised pre-trained models. Motivated by the success of supervised
pre-training, we propose Multi-task superVised Pre-training~(MVP) for natural
language generation. We collect a large-scale natural language generation
corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we
unify these examples into a general text-to-text format to pre-train the text
generation model MVP in a supervised manner. For each task, we further
pre-train specific soft prompts to stimulate the model's capacity to perform a
specific task. Extensive experiments have demonstrated the effectiveness and
generality of our MVP model in a number of NLG tasks, which achieves
state-of-the-art performance on $13$ out of $17$ datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction-driven history-aware policies for robotic manipulations. (arXiv:2209.04899v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04899">
<div class="article-summary-box-inner">
<span><p>In human environments, robots are expected to accomplish a variety of
manipulation tasks given simple natural language instructions. Yet, robotic
manipulation is extremely challenging as it requires fine-grained motor
control, long-term memory as well as generalization to previously unseen tasks
and environments. To address these challenges, we propose a unified
transformer-based approach that takes into account multiple inputs. In
particular, our transformer architecture integrates (i) natural language
instructions and (ii) multi-view scene observations while (iii) keeping track
of the full history of observations and actions. Such an approach enables
learning dependencies between history and instructions and improves
manipulation precision using multiple views. We evaluate our method on the
challenging RLBench benchmark and on a real-world robot. Notably, our approach
scales to 74 diverse RLBench tasks and outperforms the state of the art. We
also address instruction-conditioned tasks and demonstrate excellent
generalization to previously unseen variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Finetuning for Robust Continual Multilingual Learning. (arXiv:2209.06767v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06767">
<div class="article-summary-box-inner">
<span><p>We study the underexplored problem of Continual Multilingual Learning, where
a multilingual model, already trained on task-specific data from all supported
languages, is continually updated using batches of new multilingual training
data for the same task. We show that naively updating the multilingual model
can lead to losses in performance over a subset of languages although the
aggregated performance metric shows an improvement. We establish this
phenomenon over four tasks belonging to three task families (token-level,
sentence-level and seq2seq). We then build upon recent advances in
parameter-efficient finetuning to develop novel finetuning strategies that
allow us to jointly minimize language-specific forgetting while encouraging
positive cross-lingual transfer observed in this setup. Our proposed pipeline,
LAFT-URIEL, improves the spread of gains over the supported languages while
reducing the magnitude of language-specific losses incurred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Unlearning for Mitigating Privacy Risks in Language Models. (arXiv:2210.01504v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01504">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (LMs) memorize a vast amount of knowledge during
initial pretraining, including information that may violate the privacy of
personal lives and identities. Previous work addressing privacy issues for
language models has mostly focused on data preprocessing and differential
privacy methods, both requiring re-training the underlying LM. We propose
knowledge unlearning as an alternative method to reduce privacy risks for LMs
post hoc. We show that simply performing gradient ascent on target token
sequences is effective at forgetting them with little to no degradation of
general language modeling performances for larger LMs; it sometimes even
substantially improves the underlying LM with just a few iterations. We also
find that sequential unlearning is better than trying to unlearn all the data
at once and that unlearning is highly dependent on which kind of data (domain)
is forgotten. By showing comparisons with a previous data preprocessing method
and a decoding method known to mitigate privacy risks for LMs, we show that
unlearning can give a stronger empirical privacy guarantee in scenarios where
the data vulnerable to extraction attacks are known a priori while being much
more efficient and robust. We release the code and dataset needed to replicate
our results at https://github.com/joeljang/knowledge-unlearning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Prompt Tuning for Text Summarization. (arXiv:2211.01837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01837">
<div class="article-summary-box-inner">
<span><p>Prompts with different control signals (e.g., length, keywords, etc.) can be
used to control text summarization. When control signals are available, they
can control the properties of generated summaries and potentially improve
summarization quality (since more information are given). Unfortunately,
control signals are not already available during inference time. In this paper,
we propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which
is a single model that can be applied in both controlled and uncontrolled
(without control signals) modes. During training, Lotus learns latent prompt
representations from prompts with gold control signals using a contrastive
learning objective. Experiments show Lotus in uncontrolled mode consistently
improves upon strong (uncontrollable) summarization models across four
different summarization datasets. We also demonstrate generated summaries can
be controlled using prompts with user specified control tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMentry: A Language Model Benchmark of Elementary Language Tasks. (arXiv:2211.02069v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02069">
<div class="article-summary-box-inner">
<span><p>As the performance of large language models rapidly improves, benchmarks are
getting larger and more complex as well. We present LMentry, a benchmark that
avoids this "arms race" by focusing on a compact set of tasks that are trivial
to humans, e.g. writing a sentence containing a specific word, identifying
which words in a list belong to a specific category, or choosing which of two
words is longer. LMentry is specifically designed to provide quick and
interpretable insights into the capabilities and robustness of large language
models. Our experiments reveal a wide variety of failure cases that, while
immediately obvious to humans, pose a considerable challenge for large language
models, including OpenAI's latest 175B-parameter instruction-tuned model,
TextDavinci002. LMentry complements contemporary evaluation approaches of large
language models, providing a quick, automatic, and easy-to-run "unit test",
without resorting to large benchmark suites of complex tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems. (arXiv:2211.03648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03648">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) task-oriented dialogue (ToD) systems are prone to fall into
the so-called "likelihood trap", resulting in generated responses which are
dull, repetitive, and often inconsistent with dialogue history. Comparing
ranked lists of multiple generated responses against the "gold response" (from
evaluation data) reveals a wide diversity in response quality, with many good
responses placed lower in the ranked list. The main challenge, addressed in
this work, is then how to reach beyond greedily generated system responses,
that is, how to obtain and select such high-quality responses from the list of
overgenerated responses at inference without availability of the gold response.
To this end, we propose a simple yet effective reranking method which aims to
select high-quality items from the lists of responses initially overgenerated
by the system. The idea is to use any sequence-level (similarity) scoring
function to divide the semantic space of responses into high-scoring versus
low-scoring partitions. At training, the high-scoring partition comprises all
generated responses whose similarity to the gold response is higher than the
similarity of the greedy response to the gold response. At inference, the aim
is to estimate the probability that each overgenerated response belongs to the
high-scoring partition, given only previous dialogue history. We validate the
robustness and versatility of our proposed method on the standard MultiWOZ
dataset: our methods improve a state-of-the-art E2E ToD system by 2.0 BLEU, 1.6
ROUGE, and 1.3 METEOR scores, achieving new peak results. Additional
experiments on the BiTOD dataset and human evaluation further ascertain the
generalisability and effectiveness of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What would Harry say? Building Dialogue Agents for Characters in a Story. (arXiv:2211.06869v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06869">
<div class="article-summary-box-inner">
<span><p>We have a Christmas gift for Harry Potter fans all over the world. In this
paper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry
Potter-like dialogue agents. Such a task is typically viewed as a variant of
personalized dialogue agents, but they differ significantly in three respects:
1) Harry lived in a virtual world of wizards, thus, real-world commonsense may
not apply to Harry's conversations; 2) Harry's behavior is strongly linked to
background information in conversations: the scene, its attributes and its
relationship to other speakers; and 3) Such backgrounds are dynamically altered
as the storyline goes on. The HPD dataset, as the first dataset to facilitate
the study of dialogue agent construction for characters within a story,
provides rich contextual information about each dialogue session such as
scenes, character attributes, and relations. More importantly, all the
background information will change over the course of the story. In addition,
HPD could support both dialogue generation and retrieval tasks. We evaluate
baselines such as Dialog-GPT and BOB to determine the extent to which they can
generate Harry Potter-like responses. The experimental results disappoint us in
that although the generated responses are fluent, they still seem out of
character for Harry. Besides, we validate the current most robust dialogue
agent, ChatGPT, which also can't generate plausible Harry-Potter-like responses
in some cases, either. Our results suggest that there is much scope for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09102">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) that have been trained on multilingual but not
parallel text exhibit a remarkable ability to translate between languages. We
probe this ability in an in-depth study of the pathways language model (PaLM),
which has demonstrated the strongest machine translation (MT) performance among
similarly-trained LLMs to date. We investigate various strategies for choosing
translation examples for few-shot prompting, concluding that example quality is
the most important factor. Using optimized prompts, we revisit previous
assessments of PaLM's MT capabilities with more recent test sets, modern MT
metrics, and human evaluation, and find that its performance, while impressive,
still lags that of state-of-the-art supervised systems. We conclude by
providing an analysis of PaLM's MT output which reveals some interesting
properties and prospects for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09783">
<div class="article-summary-box-inner">
<span><p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing. (arXiv:2211.10265v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10265">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have motivated research on what kinds of
knowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is
a natural approach for gauging such knowledge. BioLAMA generates prompts for
biomedical factual knowledge triples and uses the Top-k accuracy metric to
evaluate different PLMs' knowledge. However, existing research has shown that
such prompt-based knowledge probing methods can only probe a lower bound of
knowledge. Many factors like prompt-based probing biases make the LAMA
benchmark unreliable and unstable. This problem is more prominent in BioLAMA.
The severe long-tailed distribution in vocabulary and large-N-M relation make
the performance gap between LAMA and BioLAMA remain notable. To address these,
we introduce context variance into the prompt generation and propose a new
rank-change-based evaluation metric. Different from the previous known-unknown
evaluation criteria, we propose the concept of "Misunderstand" in LAMA for the
first time. Through experiments on 12 PLMs, our context variance prompts and
Understand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to
large-N-M relations and rare relations. We also conducted a set of control
experiments to disentangle "understand" from just "read and copy".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning. (arXiv:2211.15076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15076">
<div class="article-summary-box-inner">
<span><p>Video captioning aims to generate natural language sentences that describe
the given video accurately. Existing methods obtain favorable generation by
exploring richer visual representations in encode phase or improving the
decoding ability. However, the long-tailed problem hinders these attempts at
low-frequency tokens, which rarely occur but carry critical semantics, playing
a vital role in the detailed generation. In this paper, we introduce a novel
Refined Semantic enhancement method towards Frequency Diffusion (RSFD), a
captioning model that constantly perceives the linguistic representation of the
infrequent tokens. Concretely, a Frequency-Aware Diffusion (FAD) module is
proposed to comprehend the semantics of low-frequency tokens to break through
generation limitations. In this way, the caption is refined by promoting the
absorption of tokens with insufficient occurrence. Based on FAD, we design a
Divergent Semantic Supervisor (DSS) module to compensate for the information
loss of high-frequency tokens brought by the diffusion process, where the
semantics of low-frequency tokens is further emphasized to alleviate the
long-tailed problem. Extensive experiments indicate that RSFD outperforms the
state-of-the-art methods on two benchmark datasets, i.e., MSR-VTT and MSVD,
demonstrate that the enhancement of low-frequency tokens semantics can obtain a
competitive generation effect. Code is available at
https://github.com/lzp870/RSFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16944">
<div class="article-summary-box-inner">
<span><p>Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to improve generalization. We
further present AIONER, a general-purpose BioNER tool based on cutting-edge
deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark
tasks and show that AIONER is effective, robust, and compares favorably to
other state-of-the-art approaches such as multi-task learning. We further
demonstrate the practical utility of AIONER in three independent tasks to
recognize entity types not previously seen in training data, as well as the
advantages of AIONER over existing methods for processing biomedical text at a
large scale (e.g., the entire PubMed data).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04068">
<div class="article-summary-box-inner">
<span><p>While pre-trained Chinese language models have demonstrated impressive
performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task
remains a challenge. Previous research has explored using information such as
glyphs and phonetics to improve the ability to distinguish misspelled
characters, with good results. However, the generalization ability of these
models is not well understood: it is unclear whether they incorporate
glyph-phonetic information and, if so, whether this information is fully
utilized. In this paper, we aim to better understand the role of glyph-phonetic
information in the CSC task and suggest directions for improvement.
Additionally, we propose a new, more challenging, and practical setting for
testing the generalizability of CSC models. All code is made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Level Debiased Natural Language Understanding. (arXiv:2212.05421v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05421">
<div class="article-summary-box-inner">
<span><p>Natural language understanding (NLU) models often rely on dataset biases
rather than intended task-relevant features to achieve high performance on
specific datasets. As a result, these models perform poorly on datasets outside
the training distribution. Some recent studies address this issue by reducing
the weights of biased samples during the training process. However, these
methods still encode biased latent features in representations and neglect the
dynamic nature of bias, which hinders model prediction. We propose an NLU
debiasing method, named debiasing contrastive learning (DCT), to simultaneously
alleviate the above problems based on contrastive learning. We devise a
debiasing, positive sampling strategy to mitigate biased latent features by
selecting the least similar biased positive samples. We also propose a dynamic
negative sampling strategy to capture the dynamic influence of biases by
employing a bias-only model to dynamically select the most similar biased
negative samples. We conduct experiments on three NLU benchmark datasets.
Experimental results show that DCT outperforms state-of-the-art baselines on
out-of-distribution datasets while maintaining in-distribution performance. We
also verify that DCT can reduce biased latent features from the model's
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging. (arXiv:2212.05956v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05956">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a commonly used technique for improving the
generalization of compact Pre-trained Language Models (PLMs) on downstream
tasks. However, such methods impose the additional burden of training a
separate teacher model for every new dataset. Alternatively, one may directly
work on the improvement of the optimization procedure of the compact model
toward better generalization. Recent works observe that the flatness of the
local minimum correlates well with better generalization. In this work, we
adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a
flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on
various NLP tasks (text classification, question answering, and generation) and
different model architectures and demonstrate that our adaptation improves the
generalization without extra computation cost. Moreover, we observe that this
simple optimization technique is able to outperform the state-of-the-art KD
methods for compact models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lisan: Yemeni, Iraqi, Libyan, and Sudanese Arabic Dialect Copora with Morphological Annotations. (arXiv:2212.06468v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06468">
<div class="article-summary-box-inner">
<span><p>This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and
Libyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.
We collected the content of the corpora from several social media platforms.
The Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.
The corpora of the other three dialects (~ 50K tokens each) came manually from
Facebook and YouTube posts and comments.
</p>
<p>Thirty five (35) annotators who are native speakers of the target dialects
carried out the annotations. The annotators segemented all words in the four
corpora into prefixes, stems and suffixes and labeled each with different
morphological features such as part of speech, lemma, and a gloss in English.
An Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the
annation. The annotators were trained on a set of guidelines and on how to use
ADAT. We developed ADAT to assist the annotators and to ensure compatibility
with SAMA and Curras tagsets. The tool is open source, and the four corpora are
also available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Azimuth: Systematic Error Analysis for Text Classification. (arXiv:2212.08216v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08216">
<div class="article-summary-box-inner">
<span><p>We present Azimuth, an open-source and easy-to-use tool to perform error
analysis for text classification. Compared to other stages of the ML
development cycle, such as model training and hyper-parameter tuning, the
process and tooling for the error analysis stage are less mature. However, this
stage is critical for the development of reliable and trustworthy AI systems.
To make error analysis more systematic, we propose an approach comprising
dataset analysis and model quality assessment, which Azimuth facilitates. We
aim to help AI practitioners discover and address areas where the model does
not generalize by leveraging and integrating a range of ML techniques, such as
saliency maps, similarity, uncertainty, and behavioral analyses, all in one
tool. Our code and documentation are available at
github.com/servicenow/azimuth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Small Language Models to Reason. (arXiv:2212.08410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08410">
<div class="article-summary-box-inner">
<span><p>Chain of thought prompting successfully improves the reasoning capabilities
of large language models, achieving state of the art results on a range of
datasets. However, these reasoning capabilities only appear to emerge in models
with a size of over 100 billion parameters. In this paper, we explore the
transfer of such reasoning capabilities to models with less than 100 billion
parameters via knowledge distillation. Specifically, we finetune a student
model on the chain of thought outputs generated by a larger teacher model. Our
experiments show that the proposed method improves task performance across
arithmetic, commonsense and symbolic reasoning datasets. For example, the
accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on
PaLM-540B generated chains of thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v5 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08793">
<div class="article-summary-box-inner">
<span><p>Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, to
motivate saliency-based supervision, we analyze oracle KG-augmented models
which directly use saliency explanations as extra inputs for guiding their
attention. Third, we propose SalKG, a framework for KG-augmented models to
learn from coarse and/or fine saliency explanations. Given saliency
explanations created from a task's training set, SalKG jointly trains the model
to predict the explanations, then solve the task by attending to KG features
highlighted by the predicted explanations. On three commonsense QA benchmarks
(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can
yield considerable performance gains -- up to 2.76% absolute improvement on
CSQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIREX: A Unified Learning Framework for Language Model Rationale Extraction. (arXiv:2112.08802v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08802">
<div class="article-summary-box-inner">
<span><p>An extractive rationale explains a language model's (LM's) prediction on a
given task instance by highlighting the text inputs that most influenced the
prediction. Ideally, rationale extraction should be faithful (reflective of
LM's actual behavior) and plausible (convincing to humans), without
compromising the LM's (i.e., task model's) task performance. Although
attribution algorithms and select-predict pipelines are commonly used in
rationale extraction, they both rely on certain heuristics that hinder them
from satisfying all three desiderata. In light of this, we propose UNIREX, a
flexible learning framework which generalizes rationale extractor optimization
as follows: (1) specify architecture for a learned rationale extractor; (2)
select explainability objectives (i.e., faithfulness and plausibility
criteria); and (3) jointly the train task model and rationale extractor on the
task using selected objectives. UNIREX enables replacing prior works' heuristic
design choices with a generic learned rationale extractor in (1) and optimizing
it for all three desiderata in (2)-(3). To facilitate comparison between
methods w.r.t. multiple desiderata, we introduce the Normalized Relative Gain
(NRG) metric. Across five text classification datasets, our best UNIREX
configuration outperforms baselines by an average of 32.9% NRG. Plus, we find
that UNIREX-trained rationale extractors can even generalize to unseen datasets
and tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swing Distillation: A Privacy-Preserving Knowledge Distillation Framework. (arXiv:2212.08349v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08349">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has been widely used for model compression and
knowledge transfer. Typically, a big teacher model trained on sufficient data
transfers knowledge to a small student model. However, despite the success of
KD, little effort has been made to study whether KD leaks the training data of
the teacher model. In this paper, we experimentally reveal that KD suffers from
the risk of privacy leakage. To alleviate this issue, we propose a novel
knowledge distillation method, swing distillation, which can effectively
protect the private information of the teacher model from flowing to the
student model. In our framework, the temperature coefficient is dynamically and
adaptively adjusted according to the degree of private information contained in
the data, rather than a predefined constant hyperparameter. It assigns
different temperatures to tokens according to the likelihood that a token in a
position contains private information. In addition, we inject noise into soft
targets provided to the student model, in order to avoid unshielded knowledge
transfer. Experiments on multiple datasets and tasks demonstrate that the
proposed swing distillation can significantly reduce (by over 80% in terms of
canary exposure) the risk of privacy leakage in comparison to KD with
competitive or better performance. Furthermore, swing distillation is robust
against the increasing privacy budget.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-20 23:12:37.074915436 UTC">2022-12-20 23:12:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>