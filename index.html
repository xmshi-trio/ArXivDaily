<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-15T01:30:00Z">12-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Image Style Transfer from Freeform Text. (arXiv:2212.06868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06868">
<div class="article-summary-box-inner">
<span><p>This paper creates a novel method of deep neural style transfer by generating
style images from freeform user text input. The language model and style
transfer model form a seamless pipeline that can create output images with
similar losses and improved quality when compared to baseline style transfer
methods. The language model returns a closely matching image given a style text
and description input, which is then passed to the style transfer model with an
input content image to create a final output. A proof-of-concept tool is also
developed to integrate the models and demonstrate the effectiveness of deep
image style transfer from freeform text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Envisioning a Human-AI collaborative system to transform policies into decision models. (arXiv:2212.06882v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06882">
<div class="article-summary-box-inner">
<span><p>Regulations govern many aspects of citizens' daily lives. Governments and
businesses routinely automate these in the form of coded rules (e.g., to check
a citizen's eligibility for specific benefits). However, the path to automation
is long and challenging. To address this, recent global initiatives for digital
government, proposing to simultaneously express policy in natural language for
human consumption as well as computationally amenable rules or code, are
gathering broad public-sector interest. We introduce the problem of
semi-automatically building decision models from eligibility policies for
social services, and present an initial emerging approach to shorten the route
from policy documents to executable, interpretable and standardised decision
models using AI, NLP and Knowledge Graphs. Despite the many open domain
challenges, in this position paper we explore the enormous potential of AI to
assist government agencies and policy experts in scaling the production of both
human-readable and machine executable policy rules, while improving
transparency, interpretability, traceability and accountability of the decision
making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrase Identification with Deep Learning: A Review of Datasets and Methods. (arXiv:2212.06933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06933">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of AI technology has made text generation tools like
GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can
pose serious threat to the credibility of various forms of media if these
technologies are used for plagiarism, including scientific literature and news
sources. Despite the development of automated methods for paraphrase
identification, detecting this type of plagiarism remains a challenge due to
the disparate nature of the datasets on which these methods are trained. In
this study, we review traditional and current approaches to paraphrase
identification and propose a refined typology of paraphrases. We also
investigate how this typology is represented in popular datasets and how
under-representation of certain types of paraphrases impacts detection
capabilities. Finally, we outline new directions for future research and
datasets in the pursuit of more effective paraphrase detection using AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models can be Fully Zero-Shot Learners. (arXiv:2212.06950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06950">
<div class="article-summary-box-inner">
<span><p>How can we extend a pre-trained model to many language understanding tasks,
without labeled or additional unlabeled data? Pre-trained language models
(PLMs) have been effective for a wide range of NLP tasks. However, existing
approaches either require fine-tuning on downstream labeled datasets or
manually constructing proper prompts. In this paper, we propose nonparametric
prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike
previous methods, NPPrompt uses only pre-trained language models and does not
require any labeled data or additional raw corpus for further fine-tuning, nor
does it rely on humans to construct a comprehensive set of prompt label words.
We evaluate NPPrompt against previous major few-shot and zero-shot learning
methods on diverse NLP tasks: including text classification, text entailment,
similar text retrieval, and paraphrasing. Experimental results demonstrate that
our NPPrompt outperforms the previous best fully zero-shot method by big
margins, with absolute gains of 12.8% in accuracy on text classification and
18.9% on the GLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding. (arXiv:2212.06971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06971">
<div class="article-summary-box-inner">
<span><p>From a visual scene containing multiple people, human is able to distinguish
each individual given the context descriptions about what happened before,
their mental/physical states or intentions, etc. Above ability heavily relies
on human-centric commonsense knowledge and reasoning. For example, if asked to
identify the "person who needs healing" in an image, we need to first know that
they usually have injuries or suffering expressions, then find the
corresponding visual clues before finally grounding the person. We present a
new commonsense task, Human-centric Commonsense Grounding, that tests the
models' ability to ground individuals given the context descriptions about what
happened before, and their mental/physical states or intentions. We further
create a benchmark, HumanCog, a dataset with 130k grounded commonsensical
descriptions annotated on 67k images, covering diverse types of commonsense and
visual scenes. We set up a context-object-aware method as a strong baseline
that outperforms previous pre-trained and non-pretrained models. Further
analysis demonstrates that rich visual commonsense and powerful integration of
multi-modal commonsense are essential, which sheds light on future works. Data
and code will be available https://github.com/Hxyou/HumanCog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Prosody Representations with Unsupervised Speech Reconstruction. (arXiv:2212.06972v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06972">
<div class="article-summary-box-inner">
<span><p>Human speech can be characterized by different components, including semantic
content, speaker identity and prosodic information. Significant progress has
been made in disentangling representations for semantic content and speaker
identity in Automatic Speech Recognition (ASR) and speaker verification tasks
respectively. However, it is still an open challenging research question to
extract prosodic information because of the intrinsic association of different
attributes, such as timbre and rhythm, and because of the need for unsupervised
training schemes to achieve robust large-scale and speaker-independent ASR. The
aim of this paper is to address the disentanglement of emotional prosody from
speech based on unsupervised reconstruction. Specifically, we identify, design,
implement and integrate three crucial components in our proposed speech
reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech
signals into discrete units for semantic content, (2) a pretrained speaker
verification model to generate speaker identity embeddings, and (3) a trainable
prosody encoder to learn prosody representations. We first pretrain the
Prosody2Vec representations on unlabelled emotional speech corpora, then
fine-tune the model on specific datasets to perform Speech Emotion Recognition
(SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective
evaluations on the EVC task suggest that Prosody2Vec effectively captures
general prosodic features that can be smoothly transferred to other emotional
speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the
prosody features learned by Prosody2Vec are complementary and beneficial for
the performance of widely used speech pretraining models and surpass the
state-of-the-art methods when combining Prosody2Vec with HuBERT
representations. Some audio samples can be found on our demo website.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach. (arXiv:2212.07043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07043">
<div class="article-summary-box-inner">
<span><p>Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation. (arXiv:2212.07072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07072">
<div class="article-summary-box-inner">
<span><p>Word Sense Disambiguation (WSD) is an NLP task aimed at determining the
correct sense of a word in a sentence from discrete sense choices. Although
current systems have attained unprecedented performances for such tasks, the
nonuniform distribution of word senses during training generally results in
systems performing poorly on rare senses. To this end, we consider data
augmentation to increase the frequency of these least frequent senses (LFS) to
reduce the distributional bias of senses during training. We propose
Sense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that
maintains the sense of a target word. SMSMix smoothly blends two sentences
using mask prediction while preserving the relevant span determined by saliency
scores to maintain a specific word's sense. To the best of our knowledge, this
is the first attempt to apply mixup in NLP while preserving the meaning of a
specific word. With extensive experiments, we validate that our augmentation
method can effectively give more information about rare senses during training
with maintained target sense label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Similarity-Based Curriculum Learning for Image Captioning. (arXiv:2212.07075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07075">
<div class="article-summary-box-inner">
<span><p>Image captioning models require the high-level generalization ability to
describe the contents of various images in words. Most existing approaches
treat the image-caption pairs equally in their training without considering the
differences in their learning difficulties. Several image captioning approaches
introduce curriculum learning methods that present training data with
increasing levels of difficulty. However, their difficulty measurements are
either based on domain-specific features or prior model training. In this
paper, we propose a simple yet efficient difficulty measurement for image
captioning using cross-modal similarity calculated by a pretrained
vision-language model. Experiments on the COCO and Flickr30k datasets show that
our proposed approach achieves superior performance and competitive convergence
speed to baselines without requiring heuristics or incurring additional
training costs. Moreover, the higher model performance on difficult examples
and unseen data also demonstrates the generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog. (arXiv:2212.07112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07112">
<div class="article-summary-box-inner">
<span><p>Harvesting question-answer (QA) pairs from customer service chatlog in the
wild is an efficient way to enrich the knowledge base for customer service
chatbots in the cold start or continuous integration scenarios. Prior work
attempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which
fails to integrate the incomplete utterances from the dialog context for
composite QA retrieval. In this paper, we propose N-to-N QA extraction task in
which the derived questions and corresponding answers might be separated across
different utterances. We introduce a suite of generative/discriminative tagging
based methods with end-to-end and two-stage variants that perform well on 5
customer service datasets and for the first time setup a benchmark for N-to-N
DialogQAE with utterance and session level evaluation metrics. With a deep dive
into extracted QA pairs, we find that the relations between and inside the QA
pairs can be indicators to analyze the dialogue structure, e.g. information
seeking, clarification, barge-in and elaboration. We also show that the
proposed models can adapt to different domains and languages, and reduce the
labor cost of knowledge accumulation in the real-world product dialogue
platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability of Text Processing and Retrieval Methods: A Critical Survey. (arXiv:2212.07126v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07126">
<div class="article-summary-box-inner">
<span><p>Deep Learning and Machine Learning based models have become extremely popular
in text processing and information retrieval. However, the non-linear
structures present inside the networks make these models largely inscrutable. A
significant body of research has focused on increasing the transparency of
these models. This article provides a broad overview of research on the
explainability and interpretability of natural language processing and
information retrieval methods. More specifically, we survey approaches that
have been applied to explain word embeddings, sequence modeling, attention
modules, transformers, BERT, and document ranking. The concluding section
suggests some possible directions for future research on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07127">
<div class="article-summary-box-inner">
<span><p>With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text. (arXiv:2212.07156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07156">
<div class="article-summary-box-inner">
<span><p>Modal verbs (e.g., "can", "should", or "must") occur highly frequently in
scientific articles. Decoding their function is not straightforward: they are
often used for hedging, but they may also denote abilities and restrictions.
Understanding their meaning is important for various NLP tasks such as writing
assistance or accurate information extraction from scientific text.
</p>
<p>To foster research on the usage of modals in this genre, we introduce the
MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances
in five scientific domains annotated for their semantic, pragmatic, or
rhetorical function. We systematically evaluate a set of competitive neural
architectures on MIST. Transfer experiments reveal that leveraging
non-scientific data is of limited benefit for modeling the distinctions in
MIST. Our corpus analysis provides evidence that scientific communities differ
in their usage of modal verbs, yet, classifiers trained on scientific data
generalize to some extent to unseen scientific domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Learning for Cross-Lingual Sentiment Analysis. (arXiv:2212.07160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07160">
<div class="article-summary-box-inner">
<span><p>This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building and Evaluating Universal Named-Entity Recognition English corpus. (arXiv:2212.07162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07162">
<div class="article-summary-box-inner">
<span><p>This article presents the application of the Universal Named Entity framework
to generate automatically annotated corpora. By using a workflow that extracts
Wikipedia data and meta-data and DBpedia information, we generated an English
dataset which is described and evaluated. Furthermore, we conducted a set of
experiments to improve the annotations in terms of precision, recall, and
F1-measure. The final dataset is available and the established workflow can be
applied to any language with existing Wikipedia and DBpedia. As part of future
research, we intend to continue improving the annotation process and extend it
to other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech and Natural Language Processing Technologies for Pseudo-Pilot Simulator. (arXiv:2212.07164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07164">
<div class="article-summary-box-inner">
<span><p>This paper describes a simple yet efficient repetition-based modular system
for speeding up air-traffic controllers (ATCos) training. E.g., a human pilot
is still required in EUROCONTROL's ESCAPE lite simulator (see
https://www.eurocontrol.int/simulator/escape) during ATCo training. However,
this need can be substituted by an automatic system that could act as a pilot.
In this paper, we aim to develop and integrate a pseudo-pilot agent into the
ATCo training pipeline by merging diverse artificial intelligence (AI) powered
modules. The system understands the voice communications issued by the ATCo,
and, in turn, it generates a spoken prompt that follows the pilot's phraseology
to the initial communication. Our system mainly relies on open-source AI tools
and air traffic control (ATC) databases, thus, proving its simplicity and ease
of replicability. The overall pipeline is composed of the following: (1) a
submodule that receives and pre-processes the input stream of raw audio, (2) an
automatic speech recognition (ASR) system that transforms audio into a sequence
of words; (3) a high-level ATC-related entity parser, which extracts relevant
information from the communication, i.e., callsigns and commands, and finally,
(4) a speech synthesizer submodule that generates responses based on the
high-level ATC entities previously extracted. Overall, we show that this system
could pave the way toward developing a real proof-of-concept pseudo-pilot
system. Hence, speeding up the training of ATCos while drastically reducing its
overall cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quotations, Coreference Resolution, and Sentiment Annotations in Croatian News Articles: An Exploratory Study. (arXiv:2212.07172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07172">
<div class="article-summary-box-inner">
<span><p>This paper presents a corpus annotated for the task of direct-speech
extraction in Croatian. The paper focuses on the annotation of the quotation,
co-reference resolution, and sentiment annotation in SETimes news corpus in
Croatian and on the analysis of its language-specific differences compared to
English. From this, a list of the phenomena that require special attention when
performing these annotations is derived. The generated corpus with quotation
features annotations can be used for multiple tasks in the field of Natural
Language Processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Negative Style Transfer in Hybrid Dialogue System. (arXiv:2212.07183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07183">
<div class="article-summary-box-inner">
<span><p>As the functionality of dialogue systems evolves, hybrid dialogue systems
that accomplish user-specific goals and participate in open-topic chitchat with
users are attracting growing attention. Existing research learns both tasks
concurrently utilizing a multi-task fusion technique but ignores the negative
transfer phenomenon induced by the unique textual style differences. Therefore,
contrastive learning based on the latent variable model is used to decouple the
various textual genres in the latent space. We devise supervised and
self-supervised positive and negative sample constructions for diverse
datasets. In addition, to capitalize on the style information contained in the
decoupled latent variables, we employ a style prefix that incorporates latent
variables further to control the generation of responses with varying styles.
We performed extensive experiments on three dialogue datasets, including a
hybrid dialogue dataset and two task-oriented dialogue datasets. The
experimental results demonstrate that our method can mitigate the negative
style transfer issue and achieves state-of-the-art performance on multiple
dialogue datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VTCC-NLP at NL4Opt competition subtask 1: An Ensemble Pre-trained language models for Named Entity Recognition. (arXiv:2212.07219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07219">
<div class="article-summary-box-inner">
<span><p>We propose a combined three pre-trained language models (XLM-R, BART, and
DeBERTa-V3) as an empower of contextualized embedding for named entity
recognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th
on the leaderboard at NL4Opt competition subtask 1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07220">
<div class="article-summary-box-inner">
<span><p>Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
samples, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. Though many efforts have been devoted
to CLS, none of them notice the phenomenon of translationese. In this paper, we
first confirm that the different approaches to constructing CLS datasets will
lead to different degrees of translationese. Then we design systematic
experiments to investigate how translationese affects CLS model evaluation and
performance when it appears in source documents or target summaries. In detail,
we find that (1) the translationese in documents or summaries of test sets
might lead to the discrepancy between human judgment and automatic evaluation;
(2) the translationese in training sets would harm model performance in the
real scene; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Furthermore, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Byte and Wordpiece Level Models for Massively Multilingual Semantic Parsing. (arXiv:2212.07223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07223">
<div class="article-summary-box-inner">
<span><p>Token free approaches have been successfully applied to a series of word and
span level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece
based (mT5) sequence to sequence model on the 51 languages of the MASSIVE
multilingual semantic parsing dataset. We examine multiple experimental
settings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic
data. By leveraging a state-of-the-art label projection method for machine
translated examples, we are able to reduce the gap in exact match accuracy to
only 5 points with respect to a model trained on gold data from all the
languages. We additionally provide insights on the cross-lingual transfer of
ByT5 and show how the model compares with respect to mT5 across all parameter
sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning. (arXiv:2212.07249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07249">
<div class="article-summary-box-inner">
<span><p>Long-form numerical reasoning in financial analysis aims to generate a
reasoning program to calculate the correct answer for a given question.
Previous work followed a retriever-generator framework, where the retriever
selects key facts from a long-form document, and the generator generates a
reasoning program based on retrieved facts. However, they treated all facts
equally without considering the different contributions of facts with and
without numbers. Meanwhile, the program consistency were ignored under
supervised training, resulting in lower training accuracy and diversity. To
solve these problems, we proposed APOLLO to improve the long-form numerical
reasoning framework. For the retriever, we adopt a number-aware negative
sampling strategy to enable the retriever to be more discriminative on key
numerical facts. For the generator, we design consistency-based reinforcement
learning and target program augmentation strategy based on the consistency of
program execution results. Experimental results on the FinQA and ConvFinQA
leaderboard verify the effectiveness of our proposed method, achieving the new
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling. (arXiv:2212.07284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07284">
<div class="article-summary-box-inner">
<span><p>Static subword tokenization algorithms have been an essential component of
recent works on language modeling. However, their static nature results in
important flaws that degrade the models' downstream performance and robustness.
In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.
MANTa is a differentiable tokenizer trained end-to-end with the language model.
The resulting system offers a trade-off between the expressiveness of
byte-level models and the speed of models trained using subword tokenization.
In addition, our tokenizer is highly explainable since it produces an explicit
segmentation of sequences into blocks. We evaluate our pre-trained model on
several English datasets from different domains as well as on synthetic noise.
We find that MANTa improves robustness to character perturbations and
out-of-domain data. We then show that MANTa performs comparably to other models
on the general-domain GLUE benchmark. Finally, we show that it is considerably
faster than strictly byte-level models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Emotion-Cause Pair Extraction via Learning to Link. (arXiv:2002.10710v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10710">
<div class="article-summary-box-inner">
<span><p>Emotion-cause pair extraction (ECPE), as an emergent natural language
processing task, aims at jointly investigating emotions and their underlying
causes in documents. It extends the previous emotion cause extraction (ECE)
task, yet without requiring a set of pre-given emotion clauses as in ECE.
Existing approaches to ECPE generally adopt a two-stage method, i.e., (1)
emotion and cause detection, and then (2) pairing the detected emotions and
causes. Such pipeline method, while intuitive, suffers from two critical
issues, including error propagation across stages that may hinder the
effectiveness, and high computational cost that would limit the practical
application of the method. To tackle these issues, we propose a multi-task
learning model that can extract emotions, causes and emotion-cause pairs
simultaneously in an end-to-end manner. Specifically, our model regards pair
extraction as a link prediction task, and learns to link from emotion clauses
to cause clauses, i.e., the links are directional. Emotion extraction and cause
extraction are incorporated into the model as auxiliary tasks, which further
boost the pair extraction. Experiments are conducted on an ECPE benchmarking
dataset. The results show that our proposed model outperforms a range of
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Chemical Language Representations Capture Molecular Structure and Properties. (arXiv:2106.09553v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09553">
<div class="article-summary-box-inner">
<span><p>Models based on machine learning can enable accurate and fast molecular
property predictions, which is of interest in drug discovery and material
design. Various supervised machine learning models have demonstrated promising
performance, but the vast chemical space and the limited availability of
property labels make supervised learning challenging. Recently, unsupervised
transformer-based language models pretrained on a large unlabelled corpus have
produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, we present molecular embeddings
obtained by training an efficient transformer encoder model, MoLFormer, which
uses rotary positional embeddings. This model employs a linear attention
mechanism, coupled with highly distributed training, on SMILES sequences of 1.1
billion unlabelled molecules from the PubChem and ZINC datasets. We show that
the learned molecular representation outperforms existing baselines, including
supervised and self-supervised graph neural networks and language models, on
several downstream tasks from ten benchmark datasets. They perform
competitively on two others. Further analyses, specifically through the lens of
attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns
the spatial relationships between atoms within a molecule. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient chemical and structural information to predict various
distinct molecular properties, including quantum-chemical properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Based Speaker Role Identification for Air Traffic Control Speech Recognition. (arXiv:2108.12175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12175">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) for air traffic control is generally
trained by pooling Air Traffic Controller (ATCO) and pilot data into one set.
This is motivated by the fact that pilot's voice communications are more scarce
than ATCOs. Due to this data imbalance and other reasons (e.g., varying
acoustic conditions), the speech from ATCOs is usually recognized more
accurately than from pilots. Automatically identifying the speaker roles is a
challenging task, especially in the case of the noisy voice recordings
collected using Very High Frequency (VHF) receivers or due to the
unavailability of the push-to-talk (PTT) signal, i.e., both audio channels are
mixed. In this work, we propose to (1) automatically segment the ATCO and pilot
data based on an intuitive approach exploiting ASR transcripts and (2)
subsequently consider an automatic recognition of ATCOs' and pilots' voice as
two separate tasks. Our work is performed on VHF audio data with high noise
levels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is
recognized to be helpful for various speech-based machine-learning tasks.
Specifically, for the speaker role identification task, the module is
represented by a simple yet efficient knowledge-based system exploiting a
grammar defined by the International Civil Aviation Organization (ICAO). The
system accepts text as the input, either manually verified annotations or
automatically generated transcripts. The developed approach provides an average
accuracy in speaker role identification of about 83%. Finally, we show that
training an acoustic model for ASR tasks separately (i.e., separate models for
ATCOs and pilots) or using a multitask approach is well suited for the noisy
data and outperforms the traditional ASR system where all data is pooled
together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEREO: Scientific Text Reuse in Open Access Publications. (arXiv:2112.11800v3 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11800">
<div class="article-summary-box-inner">
<span><p>We present the Webis-STEREO-21 dataset, a massive collection of Scientific
Text Reuse in Open-access publications. It contains more than 91 million cases
of reused text passages found in 4.2 million unique open-access publications.
Featuring a high coverage of scientific disciplines and varieties of reuse, as
well as comprehensive metadata to contextualize each case, our dataset
addresses the most salient shortcomings of previous ones on scientific writing.
Webis-STEREO-21 allows for tackling a wide range of research questions from
different scientific backgrounds, facilitating both qualitative and
quantitative analysis of the phenomenon as well as a first-time grounding on
the base rate of text reuse in scientific publications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multi-Granularity Summarization. (arXiv:2201.12502v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12502">
<div class="article-summary-box-inner">
<span><p>Text summarization is a user-preference based task, i.e., for one document,
users often have different priorities for summary. As a key aspect of
customization in summarization, granularity is used to measure the semantic
coverage between the summary and source document. However, developing systems
that can generate summaries with customizable semantic coverage is still an
under-explored topic. In this paper, we propose the first unsupervised
multi-granularity summarization framework, GranuSum. We take events as the
basic semantic units of the source documents and propose to rank these events
by their salience. We also develop a model to summarize input documents with
given events as anchors and hints. By inputting different numbers of events,
GranuSum is capable of producing multi-granular summaries in an unsupervised
manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple
summaries at different granularities for each document cluster. Experimental
results confirm the substantial superiority of GranuSum on multi-granularity
summarization over strong baselines. Further, by exploiting the event
information, GranuSum also exhibits state-of-the-art performance under the
conventional unsupervised abstractive setting. Dataset for this paper can be
found at: https://github.com/maszhongming/GranuDUC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FREDA: Flexible Relation Extraction Data Annotation. (arXiv:2204.07150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07150">
<div class="article-summary-box-inner">
<span><p>To effectively train accurate Relation Extraction models, sufficient and
properly labeled data is required. Adequately labeled data is difficult to
obtain and annotating such data is a tricky undertaking. Previous works have
shown that either accuracy has to be sacrificed or the task is extremely
time-consuming, if done accurately. We are proposing an approach in order to
produce high-quality datasets for the task of Relation Extraction quickly.
Neural models, trained to do Relation Extraction on the created datasets,
achieve very good results and generalize well to other datasets. In our study,
we were able to annotate 10,022 sentences for 19 relations in a reasonable
amount of time, and trained a commonly used baseline model for each relation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing. (arXiv:2209.14901v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14901">
<div class="article-summary-box-inner">
<span><p>The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling. (arXiv:2210.08753v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08753">
<div class="article-summary-box-inner">
<span><p>Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation. (arXiv:2210.12902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12902">
<div class="article-summary-box-inner">
<span><p>Human reading comprehension often requires reasoning of event semantic
relations in narratives, represented by Event-centric Question-Answering (QA).
To address event-centric QA, we propose a novel QA model with contrastive
learning and invertible event transformation, call TranCLR. Our proposed model
utilizes an invertible transformation matrix to project semantic vectors of
events into a common event embedding space, trained with contrastive learning,
and thus naturally inject event semantic knowledge into mainstream QA
pipelines. The transformation matrix is fine-tuned with the annotated event
relation types between events that occurred in questions and those in answers,
using event-aware question vectors. Experimental results on the Event Semantic
Relation Reasoning (ESTER) dataset show significant improvements in both
generative and extractive settings compared to the existing strong baselines,
achieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact
Match (EM) score under the multi-answer setting. Qualitative analysis reveals
the high quality of the generated answers by TranCLR, demonstrating the
feasibility of injecting event knowledge into QA model learning. Our code and
models can be found at https://github.com/LuJunru/TranCLR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition, Attention, or Both?. (arXiv:2210.12958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12958">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel architecture called Composition Attention
Grammars (CAGs) that recursively compose subtrees into a single vector
representation with a composition function, and selectively attend to previous
structural information with a self-attention mechanism. We investigate whether
these components -- the composition function and the self-attention mechanism
-- can both induce human-like syntactic generalization. Specifically, we train
language models (LMs) with and without these two components with the model
sizes carefully controlled, and evaluate their syntactic generalization
performance against six test circuits on the SyntaxGym benchmark. The results
demonstrated that the composition function and the self-attention mechanism
both play an important role to make LMs more human-like, and closer inspection
of linguistic phenomenon implied that the composition function allowed
syntactic features, but not semantic features, to percolate into subtree
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans. (arXiv:2212.05546v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05546">
<div class="article-summary-box-inner">
<span><p>Importance: Social determinants of health (SDOH) are known to be associated
with increased risk of suicidal behaviors, but few studies utilized SDOH from
unstructured electronic health record (EHR) notes.
</p>
<p>Objective: To investigate associations between suicide and recent SDOH,
identified using structured and unstructured data.
</p>
<p>Design: Nested case-control study.
</p>
<p>Setting: EHR data from the US Veterans Health Administration (VHA).
</p>
<p>Participants: 6,122,785 Veterans who received care in the US VHA between
October 1, 2010, and September 30, 2015.
</p>
<p>Exposures: Occurrence of SDOH over a maximum span of two years compared with
no occurrence of SDOH.
</p>
<p>Main Outcomes and Measures: Cases of suicide deaths were matched with 4
controls on birth year, cohort entry date, sex, and duration of follow-up. We
developed an NLP system to extract SDOH from unstructured notes. Structured
data, NLP on unstructured data, and combining them yielded seven, eight and
nine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence
intervals (CIs) were estimated using conditional logistic regression.
</p>
<p>Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382
person-years of follow-up (incidence rate 37.18 /100,000 person-years). Our
cohort was mostly male (92.23%) and white (76.99%). Across the six common SDOH
as covariates, NLP-extracted SDOH, on average, covered 84.38% of all SDOH
occurrences. All SDOH, measured by structured data and NLP, were significantly
associated with increased risk of suicide. The SDOH with the largest effects
was legal problems (aOR=2.67, 95% CI=2.46-2.89), followed by violence
(aOR=2.26, 95% CI=2.11-2.43). NLP-extracted and structured SDOH were also
associated with suicide.
</p>
<p>Conclusions and Relevance: NLP-extracted SDOH were always significantly
associated with increased risk of suicide among Veterans, suggesting the
potential of NLP in public health studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model. (arXiv:2212.06369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06369">
<div class="article-summary-box-inner">
<span><p>Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06711">
<div class="article-summary-box-inner">
<span><p>Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions towards more valid and
reliable TPC research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-15 23:13:09.705288072 UTC">2022-12-15 23:13:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>