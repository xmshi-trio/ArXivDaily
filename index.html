<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-23T01:30:00Z">06-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12443">
<div class="article-summary-box-inner">
<span><p>Mental distress like depression and anxiety contribute to the largest
proportion of the global burden of diseases. Automated diagnosis systems of
such disorders, empowered by recent innovations in Artificial Intelligence, can
pave the way to reduce the sufferings of the affected individuals. Development
of such systems requires information-rich and balanced corpora. In this work,
we introduce a novel mental distress analysis audio dataset DEPAC, labeled
based on established thresholds on depression and anxiety standard screening
tools. This large dataset comprises multiple speech tasks per individual, as
well as relevant demographic information. Alongside, we present a feature set
consisting of hand-curated acoustic and linguistic features, which were found
effective in identifying signs of mental illnesses in human speech. Finally, we
justify the quality and effectiveness of our proposed audio corpus and feature
set in predicting depression severity by comparing the performance of baseline
machine learning models built on this dataset with baseline models trained on
other well-known depression corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinformation as Information Pollution. (arXiv:2306.12466v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12466">
<div class="article-summary-box-inner">
<span><p>Social media feed algorithms are designed to optimize online social
engagements for the purpose of maximizing advertising profits, and therefore
have an incentive to promote controversial posts including misinformation. By
thinking about misinformation as information pollution, we can draw parallels
with environmental policy for countering pollution such as carbon taxes.
Similar to pollution, a Pigouvian tax on misinformation provides economic
incentives for social media companies to control the spread of misinformation
more effectively to avoid or reduce their misinformation tax, while preserving
some degree of freedom in platforms' response. In this paper, we highlight a
bird's eye view of a Pigouvian misinformation tax and discuss the key questions
and next steps for implementing such a taxing scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12509">
<div class="article-summary-box-inner">
<span><p>We view large language models (LLMs) as stochastic \emph{language layers} in
a network, where the learnable parameters are the natural language
\emph{prompts} at each layer. We stack two such layers, feeding the output of
one layer to the next. We call the stacked architecture a \emph{Deep Language
Network} (DLN). We first show how to effectively perform prompt optimization
for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs
(DLN-2), where two prompts must be learnt. We consider the output of the first
layer as a latent variable to marginalize, and devise a variational inference
algorithm for joint prompt training. A DLN-2 reaches higher performance than a
single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the
network is smaller and less powerful. The DLN code is open source:
https://github.com/microsoft/deep-language-networks .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Evaluation of Document Classification using RVL-CDIP. (arXiv:2306.12550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12550">
<div class="article-summary-box-inner">
<span><p>The RVL-CDIP benchmark is widely used for measuring performance on the task
of document classification. Despite its widespread use, we reveal several
undesirable characteristics of the RVL-CDIP benchmark. These include (1)
substantial amounts of label noise, which we estimate to be 8.1% (ranging
between 1.6% to 16.9% per document category); (2) presence of many ambiguous or
multi-label documents; (3) a large overlap between test and train splits, which
can inflate model performance metrics; and (4) presence of sensitive
personally-identifiable information like US Social Security numbers (SSNs). We
argue that there is a risk in using RVL-CDIP for benchmarking document
classifiers, as its limited scope, presence of errors (state-of-the-art models
now achieve accuracy error rates that are within our estimated label error
rate), and lack of diversity make it less than ideal for benchmarking. We
further advocate for the creation of a new document classification benchmark,
and provide recommendations for what characteristics such a resource should
include.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12552">
<div class="article-summary-box-inner">
<span><p>Recently, commonsense reasoning in text generation has attracted much
attention. Generative commonsense reasoning is the task that requires machines,
given a group of keywords, to compose a single coherent sentence with
commonsense plausibility. While existing datasets targeting generative
commonsense reasoning focus on everyday scenarios, it is unclear how well
machines reason under specific geographical and temporal contexts. We formalize
this challenging task as SituatedGen, where machines with commonsense should
generate a pair of contrastive sentences given a group of keywords including
geographical or temporal entities. We introduce a corresponding English dataset
consisting of 8,268 contrastive sentence pairs, which are built upon several
existing commonsense reasoning benchmarks with minimal manual labor.
Experiments show that state-of-the-art generative language models struggle to
generate sentences with commonsense plausibility and still lag far behind human
performance. Our dataset is publicly available at
https://github.com/yunx-z/situated_gen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12567">
<div class="article-summary-box-inner">
<span><p>This paper investigates whether current large language models exhibit biases
in logical reasoning, similar to humans. Specifically, we focus on syllogistic
reasoning, a well-studied form of inference in the cognitive science of human
deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,
originally designed for psychological experiments that assess human logical
abilities in syllogistic reasoning. The dataset consists of syllogistic
inferences in both English and Japanese. We examine three types of biases
observed in human syllogistic reasoning: belief biases, conversion errors, and
atmosphere effects. Our findings demonstrate that current large language models
struggle more with problems involving these three types of biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning. (arXiv:2306.12577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12577">
<div class="article-summary-box-inner">
<span><p>This paper introduces NoRefER, a novel referenceless quality metric for
automatic speech recognition (ASR) systems. Traditional reference-based metrics
for evaluating ASR systems require costly ground-truth transcripts. NoRefER
overcomes this limitation by fine-tuning a multilingual language model for
pair-wise ranking ASR hypotheses using contrastive learning with Siamese
network architecture. The self-supervised NoRefER exploits the known quality
relationships between hypotheses from multiple compression levels of an ASR for
learning to rank intra-sample hypotheses by quality, which is essential for
model comparisons. The semi-supervised version also uses a referenced dataset
to improve its inter-sample quality ranking, which is crucial for selecting
potentially erroneous samples. The results indicate that NoRefER correlates
highly with reference-based metrics and their intra-sample ranks, indicating a
high potential for referenceless ASR evaluation or a/b testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphological Inflection with Phonological Features. (arXiv:2306.12581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12581">
<div class="article-summary-box-inner">
<span><p>Recent years have brought great advances into solving morphological tasks,
mostly due to powerful neural models applied to various tasks as (re)inflection
and analysis. Yet, such morphological tasks cannot be considered solved,
especially when little training data is available or when generalizing to
previously unseen lemmas. This work explores effects on performance obtained
through various ways in which morphological models get access to subcharacter
phonological features that are the targets of morphological processes. We
design two methods to achieve this goal: one that leaves models as is but
manipulates the data to include features instead of characters, and another
that manipulates models to take phonological features into account when
building representations for phonemes. We elicit phonemic data from standard
graphemic data using language-specific grammars for languages with shallow
grapheme-to-phoneme mapping, and we experiment with two reinflection models
over eight languages. Our results show that our methods yield comparable
results to the grapheme-based baseline overall, with minor improvements in some
of the languages. All in all, we conclude that patterns in character
distributions are likely to allow models to infer the underlying phonological
characteristics, even when phonemes are not explicitly represented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. (arXiv:2306.12587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12587">
<div class="article-summary-box-inner">
<span><p>Revising scientific papers based on peer feedback is a challenging task that
requires not only deep scientific knowledge and reasoning, but also the ability
to recognize the implicit requests in high-level feedback and to choose the
best of many possible ways to update the manuscript in response. We introduce
this task for large language models and release ARIES, a dataset of review
comments and their corresponding paper edits, to enable training and evaluating
models. We study two versions of the task: comment-edit alignment and edit
generation, and evaluate several baselines, including GPT-4. We find that
models struggle even to identify the edits that correspond to a comment,
especially in cases where the comment is phrased in an indirect way or where
the edit addresses the spirit of a comment but not the precise request. When
tasked with generating edits, GPT-4 often succeeds in addressing comments on a
surface level, but it rigidly follows the wording of the feedback rather than
the underlying intent, and includes fewer technical details than human-written
edits. We hope that our formalization, dataset, and analysis will form a
foundation for future work in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Approach to exploiting Multiple Datasets from TalkBank. (arXiv:2306.12596v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12596">
<div class="article-summary-box-inner">
<span><p>TalkBank is an online database that facilitates the sharing of linguistics
research data. However, the existing TalkBank's API has limited data filtering
and batch processing capabilities. To overcome these limitations, this paper
introduces a pipeline framework that employs a hierarchical search approach,
enabling efficient complex data selection. This approach involves a quick
preliminary screening of relevant corpora that a researcher may need, and then
perform an in-depth search for target data based on specific criteria. The
identified files are then indexed, providing easier access for future analysis.
Furthermore, the paper demonstrates how data from different studies curated
with the framework can be integrated by standardizing and cleaning metadata,
allowing researchers to extract insights from a large, integrated dataset.
While being designed for TalkBank, the framework can also be adapted to process
data from other open-science platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12619">
<div class="article-summary-box-inner">
<span><p>Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12656">
<div class="article-summary-box-inner">
<span><p>Rare diseases (RDs) are collectively common and affect 300 million people
worldwide. Accurate phenotyping is critical for informing diagnosis and
treatment, but RD phenotypes are often embedded in unstructured text and
time-consuming to extract manually. While natural language processing (NLP)
models can perform named entity recognition (NER) to automate extraction, a
major bottleneck is the development of a large, annotated corpus for model
training. Recently, prompt learning emerged as an NLP paradigm that can lead to
more generalizable results without any (zero-shot) or few labeled samples
(few-shot). Despite growing interest in ChatGPT, a revolutionary large language
model capable of following complex human prompts and generating high-quality
responses, none have studied its NER performance for RDs in the zero- and
few-shot settings. To this end, we engineered novel prompts aimed at extracting
RD phenotypes and, to the best of our knowledge, are the first the establish a
benchmark for evaluating ChatGPT's performance in these settings. We compared
its performance to the traditional fine-tuning approach and conducted an
in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in
higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the
zero- and few-shot settings, respectively). Despite this, ChatGPT achieved
similar or higher accuracy for certain entities (i.e., rare diseases and signs)
in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with
appropriate prompt engineering, ChatGPT has the potential to match or
outperform fine-tuned language models for certain entity types with just one
labeled sample. While the proliferation of large language models may provide
opportunities for supporting RD diagnosis and treatment, researchers and
clinicians should critically evaluate model outputs and be well-informed of
their limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12659">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a vital tool for uncovering insights from financial
articles, news, and social media, shaping our understanding of market
movements. Despite the impressive capabilities of large language models (LLMs)
in financial natural language processing (NLP), they still struggle with
accurately interpreting numerical values and grasping financial context,
limiting their effectiveness in predicting financial sentiment. In this paper,
we introduce a simple yet effective instruction tuning approach to address
these issues. By transforming a small portion of supervised financial sentiment
analysis data into instruction data and fine-tuning a general-purpose LLM with
this method, we achieve remarkable advancements in financial sentiment
analysis. In the experiment, our approach outperforms state-of-the-art
supervised sentiment analysis models, as well as widely used LLMs like ChatGPT
and LLaMAs, particularly in scenarios where numerical understanding and
contextual comprehension are vital.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. (arXiv:2306.12672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12672">
<div class="article-summary-box-inner">
<span><p>How does language inform our downstream thinking? In particular, how do
humans make meaning from language -- and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose \textit{rational meaning construction}, a computational
framework for language-informed thinking that combines neural models of
language with probabilistic models for rational inference. We frame linguistic
meaning as a context-sensitive mapping from natural language into a
\textit{probabilistic language of thought} (PLoT) -- a general-purpose symbolic
substrate for probabilistic, generative world modeling. Our architecture
integrates two powerful computational tools that have not previously come
together: we model thinking with \textit{probabilistic programs}, an expressive
representation for flexible commonsense reasoning; and we model meaning
construction with \textit{large language models} (LLMs), which support
broad-coverage translation from natural language utterances to code expressions
in a probabilistic programming language. We illustrate our framework in action
through examples covering four core domains from cognitive science:
probabilistic reasoning, logical and relational reasoning, visual and physical
reasoning, and social reasoning about agents and their plans. In each, we show
that LLMs can generate context-sensitive translations that capture
pragmatically-appropriate linguistic meanings, while Bayesian inference with
the generated programs supports coherent and robust commonsense reasoning. We
extend our framework to integrate cognitively-motivated symbolic modules to
provide a unified commonsense thinking interface from language. Finally, we
explore how language can drive the construction of world models themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12679">
<div class="article-summary-box-inner">
<span><p>Introduction: Microblogging websites have massed rich data sources for
sentiment analysis and opinion mining. In this regard, sentiment classification
has frequently proven inefficient because microblog posts typically lack
syntactically consistent terms and representatives since users on these social
networks do not like to write lengthy statements. Also, there are some
limitations to low-resource languages. The Persian language has exceptional
characteristics and demands unique annotated data and models for the sentiment
analysis task, which are distinctive from text features within the English
dialect. Method: This paper first constructs a user opinion dataset called
ITRC-Opinion by collaborative environment and insource way. Our dataset
contains 60,000 informal and colloquial Persian texts from social microblogs
such as Twitter and Instagram. Second, this study proposes a new deep
convolutional neural network (CNN) model for more effective sentiment analysis
of colloquial text in social microblog posts. The constructed datasets are used
to evaluate the presented model. Furthermore, some models, such as LSTM,
CNN-RNN, BiLSTM, and BiGRU with different word embeddings, including Fasttext,
Glove, and Word2vec, investigated our dataset and evaluated the results.
Results: The results demonstrate the benefit of our dataset and the proposed
model (72% accuracy), displaying meaningful improvement in sentiment
classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity. (arXiv:2306.12689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12689">
<div class="article-summary-box-inner">
<span><p>Vector embeddings have become ubiquitous tools for many language-related
tasks. A leading embedding model is OpenAI's text-ada-002 which can embed
approximately 6,000 words into a 1,536-dimensional vector. While powerful,
text-ada-002 is not open source and is only available via API. We trained a
simple neural network to convert open-source 768-dimensional MPNet embeddings
into text-ada-002 embeddings. We compiled a subset of 50,000 online food
reviews. We calculated MPNet and text-ada-002 embeddings for each review and
trained a simple neural network to for 75 epochs. The neural network was
designed to predict the corresponding text-ada-002 embedding for a given MPNET
embedding. Our model achieved an average cosine similarity of 0.932 on 10,000
unseen reviews in our held-out test dataset. We manually assessed the quality
of our predicted embeddings for vector search over text-ada-002-embedded
reviews. While not as good as real text-ada-002 embeddings, predicted
embeddings were able to retrieve highly relevant reviews. Our final model,
Vec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training a
neural network with a more sophisticated architecture and a larger dataset of
paired embeddings to achieve greater performance. The ability to convert
between and align embedding spaces may be helpful for interoperability,
limiting dependence on proprietary models, protecting data privacy, reducing
costs, and offline operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Neural Machine Translation System for Indic to Indic Languages. (arXiv:2306.12693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12693">
<div class="article-summary-box-inner">
<span><p>This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs
implemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All
the models are evaluated using the BLEU score. In addition, the languages are
classified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and
West Indo-Aryan (WI). The effect of language relatedness on MNMT model
efficiency is studied. Owing to the presence of large corpora from English (EN)
to ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To
achieve this, English- Indic (EN-IL) models are also developed, with and
without the usage of related languages. Results reveal that using related
languages is beneficial for the WI group only, while it is detrimental for the
EI group and shows an inconclusive effect on the DR group, but it is useful for
EN-IL models. Thus, related language groups are used to develop pivot MNMT
models. Furthermore, the IL corpora are transliterated from the corresponding
scripts to a modified ITRANS script, and the best MNMT models from the previous
approaches are built on the transliterated corpus. It is observed that the
usage of pivot models greatly improves MNMT baselines with AS-TA achieving the
minimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,
ML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the
best. Transliteration also helps the models with few exceptions. The best
increment of scores is observed in ML, TA, and BN and the worst average
increment is observed in KN, HI, and PA, across all languages. The best model
obtained is the PA-HI language pair trained on PAWI transliterated corpus which
gives 24.29 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Generation for Advertising: A Survey. (arXiv:2306.12719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12719">
<div class="article-summary-box-inner">
<span><p>Natural language generation methods have emerged as effective tools to help
advertisers increase the number of online advertisements they produce. This
survey entails a review of the research trends on this topic over the past
decade, from template-based to extractive and abstractive approaches using
neural networks. Additionally, key challenges and directions revealed through
the survey, including metric optimization, faithfulness, diversity,
multimodality, and the development of benchmark datasets, are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12725">
<div class="article-summary-box-inner">
<span><p>Multimodal Entity Linking (MEL) is the task of mapping mentions with
multimodal contexts to the referent entities from a knowledge base (e.g.,
Wikipedia). Prior MEL methods mainly focus on designing complex multimodal
interaction mechanisms and require fine-tuning all model parameters, which can
be prohibitively costly and difficult to scale in the era of Large Language
Models (LLMs). In this work, we propose GEMEL, a simple yet effective
Generative Multimodal Entity Linking method, which leverages the capabilities
of LLMs from large-scale pre-training to directly generate target entity names.
We keep the vision and language model frozen and only train a linear layer to
enable cross-modality interactions. To adapt LLMs to the MEL task, we take
advantage of the emerging in-context learning (ICL) capability of LLMs by
retrieving multimodal instances as demonstrations. Extensive experiments show
that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves
state-of-the-art results on two well-established MEL datasets (4.1% accuracy
gains on WikiDiverse and 15.4% accuracy gains on WikiMEL). Our approach is
compatible with any off-the-shelf language model, paving the way towards an
efficient and general solution for utilizing LLMs in the MEL task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12756">
<div class="article-summary-box-inner">
<span><p>Recently, we have witnessed generative retrieval increasingly gaining
attention in the information retrieval (IR) field, which retrieves documents by
directly generating their identifiers. So far, much effort has been devoted to
developing effective generative retrieval models. There has been less attention
paid to the robustness perspective. When a new retrieval paradigm enters into
the real-world application, it is also critical to measure the
out-of-distribution (OOD) generalization, i.e., how would generative retrieval
models generalize to new distributions. To answer this question, firstly, we
define OOD robustness from three perspectives in retrieval problems: 1) The
query variations; 2) The unforeseen query types; and 3) The unforeseen tasks.
Based on this taxonomy, we conduct empirical studies to analyze the OOD
robustness of several representative generative retrieval models against dense
retrieval models. The empirical results indicate that the OOD robustness of
generative retrieval models requires enhancement. We hope studying the OOD
robustness of generative retrieval models would be advantageous to the IR
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation. (arXiv:2306.12766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12766">
<div class="article-summary-box-inner">
<span><p>Structured knowledge bases (KBs) are the backbone of many
know\-ledge-intensive applications, and their automated construction has
received considerable attention. In particular, open information extraction
(OpenIE) is often used to induce structure from a text. However, although it
allows high recall, the extracted knowledge tends to inherit noise from the
sources and the OpenIE algorithm. Besides, OpenIE tuples contain an open-ended,
non-canonicalized set of relations, making the extracted knowledge's downstream
exploitation harder. In this paper, we study the problem of mapping an open KB
into the fixed schema of an existing KB, specifically for the case of
commonsense knowledge. We propose approaching the problem by generative
translation, i.e., by training a language model to generate fixed-schema
assertions from open ones. Experiments show that this approach occupies a sweet
spot between traditional manual, rule-based, or classification-based
canonicalization and purely generative KB construction like COMET. Moreover, it
produces higher mapping accuracy than the former while avoiding the
association-based noise of the latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12794">
<div class="article-summary-box-inner">
<span><p>The advent and fast development of neural networks have revolutionized the
research on dialogue systems and subsequently have triggered various challenges
regarding their automatic evaluation. Automatic evaluation of open-domain
dialogue systems as an open challenge has been the center of the attention of
many researchers. Despite the consistent efforts to improve automatic metrics'
correlations with human evaluation, there have been very few attempts to assess
their robustness over multiple domains and dimensions. Also, their focus is
mainly on the English language. All of these challenges prompt the development
of automatic evaluation metrics that are reliable in various domains,
dimensions, and languages. This track in the 11th Dialogue System Technology
Challenge (DSTC11) is part of the ongoing effort to promote robust and
multilingual automatic evaluation metrics. This article describes the datasets
and baselines provided to participants and discusses the submission and result
details of the two proposed subtasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing in Electronic Health Records in Relation to Healthcare Decision-making: A Systematic Review. (arXiv:2306.12834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12834">
<div class="article-summary-box-inner">
<span><p>Background: Natural Language Processing (NLP) is widely used to extract
clinical insights from Electronic Health Records (EHRs). However, the lack of
annotated data, automated tools, and other challenges hinder the full
utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL)
and NLP techniques are studied and compared to understand the limitations and
opportunities in this space comprehensively.
</p>
<p>Methodology: After screening 261 articles from 11 databases, we included 127
papers for full-text review covering seven categories of articles: 1) medical
note classification, 2) clinical entity recognition, 3) text summarisation, 4)
deep learning (DL) and transfer learning architecture, 5) information
extraction, 6) Medical language translation and 7) other NLP applications. This
study follows the Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) guidelines.
</p>
<p>Result and Discussion: EHR was the most commonly used data type among the
selected articles, and the datasets were primarily unstructured. Various ML and
DL methods were used, with prediction or classification being the most common
application of ML or DL. The most common use cases were: the International
Classification of Diseases, Ninth Revision (ICD-9) classification, clinical
note analysis, and named entity recognition (NER) for clinical descriptions and
research on psychiatric disorders.
</p>
<p>Conclusion: We find that the adopted ML models were not adequately assessed.
In addition, the data imbalance problem is quite important, yet we must find
techniques to address this underlining problem. Future studies should address
key limitations in studies, primarily identifying Lupus Nephritis, Suicide
Attempts, perinatal self-harmed and ICD-9 classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12886">
<div class="article-summary-box-inner">
<span><p>The ongoing Russo-Ukrainian conflict has been a subject of intense media
coverage worldwide. Understanding the global narrative surrounding this topic
is crucial for researchers that aim to gain insights into its multifaceted
dimensions. In this paper, we present a novel dataset that focuses on this
topic by collecting and processing tweets posted by news or media companies on
social media across the globe. We collected tweets from February 2022 to May
2023 to acquire approximately 1.5 million tweets in 60 different languages.
Each tweet in the dataset is accompanied by processed tags, allowing for the
identification of entities, stances, concepts, and sentiments expressed. The
availability of the dataset serves as a valuable resource for researchers
aiming to investigate the global narrative surrounding the ongoing conflict
from various aspects such as who are the prominent entities involved, what
stances are taken, where do these stances originate, and how are the different
concepts related to the event portrayed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages. (arXiv:2306.12907v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12907">
<div class="article-summary-box-inner">
<span><p>We introduce a new proxy score for evaluating bitext mining based on
similarity in a multilingual embedding space: xSIM++. In comparison to xSIM,
this improved proxy leverages rule-based approaches to extend English sentences
in any evaluation set with synthetic, hard-to-distinguish examples which more
closely mirror the scenarios we encounter during large-scale mining. We
validate this proxy by running a significant number of bitext mining
experiments for a set of low-resource languages, and subsequently train NMT
systems on the mined data. In comparison to xSIM, we show that xSIM++ is better
correlated with the downstream BLEU scores of translation systems trained on
mined bitexts, providing a reliable proxy of bitext mining performance without
needing to run expensive bitext mining pipelines. xSIM++ also reports
performance for different error types, offering more fine-grained feedback for
model development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit spoken language diarization. (arXiv:2306.12913v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12913">
<div class="article-summary-box-inner">
<span><p>Spoken language diarization (LD) and related tasks are mostly explored using
the phonotactic approach. Phonotactic approaches mostly use explicit way of
language modeling, hence requiring intermediate phoneme modeling and
transcribed data. Alternatively, the ability of deep learning approaches to
model temporal dynamics may help for the implicit modeling of language
information through deep embedding vectors. Hence this work initially explores
the available speaker diarization frameworks that capture speaker information
implicitly to perform LD tasks. The performance of the LD system on synthetic
code-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and
for practical data is 22.50% and 60.38%, in terms of diarization error rate and
Jaccard error rate (JER), respectively. The performance degradation is due to
the data imbalance and resolved to some extent by using pre-trained wave2vec
embeddings that provide a relative improvement of 30.74% in terms of JER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12916">
<div class="article-summary-box-inner">
<span><p>While summarization has been extensively researched in natural language
processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a
largely unexplored area that has the potential to improve cross-cultural
accessibility, information sharing, and understanding. This paper
comprehensively addresses the CLCTS task, including dataset creation, modeling,
and evaluation. We build the first CLCTS corpus, leveraging historical fictive
texts and Wikipedia summaries in English and German, and examine the
effectiveness of popular transformer end-to-end models with different
intermediate task finetuning tasks. Additionally, we explore the potential of
ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report
evaluations from humans, ChatGPT, and several recent automatic evaluation
metrics where we find our intermediate task finetuned end-to-end models
generate bad to moderate quality summaries; ChatGPT as a summarizer (without
any finetuning) provides moderate to good quality outputs and as an evaluator
correlates moderately with human evaluations though it is prone to giving lower
scores. ChatGPT also seems to be very adept at normalizing historical text. We
finally test ChatGPT in a scenario with adversarially attacked and unseen
source documents and find that ChatGPT is better at omission and entity swap
than negating against its prior knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12925">
<div class="article-summary-box-inner">
<span><p>We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
https://google-research.github.io/seanet/audiopalm/examples
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12929">
<div class="article-summary-box-inner">
<span><p>Transformer models have been widely adopted in various domains over the last
years, and especially large language models have advanced the field of AI
significantly. Due to their size, the capability of these networks has
increased tremendously, but this has come at the cost of a significant increase
in necessary compute. Quantization is one of the most effective ways to reduce
the computational time and memory consumption of neural networks. Many studies
have shown, however, that modern transformer models tend to learn strong
outliers in their activations, making them difficult to quantize. To retain
acceptable performance, the existence of these outliers requires activations to
be in higher bitwidth or the use of different numeric formats, extra
fine-tuning, or other workarounds. We show that strong outliers are related to
very specific behavior of attention heads that try to learn a "no-op" or just a
partial update of the residual. To achieve the exact zeros needed in the
attention matrix for a no-update, the input to the softmax is pushed to be
larger and larger during training, causing outliers in other parts of the
network. Based on these observations, we propose two simple (independent)
modifications to the attention mechanism - clipped softmax and gated attention.
We empirically show that models pre-trained using our methods learn
significantly smaller outliers while maintaining and sometimes even improving
the floating-point task performance. This enables us to quantize transformers
to full INT8 quantization of the activations without any additional effort. We
demonstrate the effectiveness of our methods on both language models (BERT,
OPT) and vision transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling. (arXiv:2306.12951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12951">
<div class="article-summary-box-inner">
<span><p>ChatGPT sets a new record with the fastest-growing user base, as a chatbot
powered by a large language model (LLM). While it demonstrates state-of-the-art
capabilities in a variety of language-generating tasks, it also raises
widespread public concerns regarding its societal impact. In this paper, we
utilize natural language processing approaches to investigate the public
attitudes towards ChatGPT by applying sentiment analysis and topic modeling
techniques to Twitter data. Our result shows that the overall sentiment is
largely neutral to positive, which also holds true across different occupation
groups. Among a wide range of topics mentioned in tweets, the most popular
topics are Artificial Intelligence, Search Engines, Education, Writing, and
Question Answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversation Derailment Forecasting with Graph Convolutional Networks. (arXiv:2306.12982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12982">
<div class="article-summary-box-inner">
<span><p>Online conversations are particularly susceptible to derailment, which can
manifest itself in the form of toxic communication patterns like disrespectful
comments or verbal abuse. Forecasting conversation derailment predicts signs of
derailment in advance enabling proactive moderation of conversations. Current
state-of-the-art approaches to address this problem rely on sequence models
that treat dialogues as text streams. We propose a novel model based on a graph
convolutional neural network that considers dialogue user dynamics and the
influence of public perception on conversation utterances. Through empirical
evaluation, we show that our model effectively captures conversation dynamics
and outperforms the state-of-the-art models on the CGA and CMV benchmark
datasets by 1.5\% and 1.7\%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12991">
<div class="article-summary-box-inner">
<span><p>Speech Emotion Recognition (SER) typically relies on utterance-level
solutions. However, emotions conveyed through speech should be considered as
discrete speech events with definite temporal boundaries, rather than
attributes of the entire utterance. To reflect the fine-grained nature of
speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just
as Speaker Diarization answers the question of "Who speaks when?", Speech
Emotion Diarization answers the question of "Which emotion appears when?". To
facilitate the evaluation of the performance and establish a common benchmark
for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly
accessible speech emotion dataset that includes non-acted emotions recorded in
real-life conditions, along with manually-annotated boundaries of emotion
segments within the utterance. We provide competitive baselines and open-source
the code and the pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13000">
<div class="article-summary-box-inner">
<span><p>As generative language models are deployed in ever-wider contexts, concerns
about their political values have come to the forefront with critique from all
parts of the political spectrum that the models are biased and lack neutrality.
However, the question of what neutrality is and whether it is desirable remains
underexplored. In this paper, I examine neutrality through an audit of Delphi
[<a href="/abs/2110.07574">arXiv:2110.07574</a>], a large language model designed for crowdsourced ethics. I
analyse how Delphi responds to politically controversial questions compared to
different US political subgroups. I find that Delphi is poorly calibrated with
respect to confidence and exhibits a significant political skew. Based on these
results, I examine the question of neutrality from a data-feminist lens, in
terms of how notions of neutrality shift power and further marginalise unheard
voices. These findings can hopefully contribute to a more reflexive debate
about the normative questions of alignment and what role we want generative
models to play in society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13041">
<div class="article-summary-box-inner">
<span><p>Unlike classical lexical overlap metrics such as BLEU, most current
evaluation metrics for machine translation (for example, COMET or BERTScore)
are based on black-box large language models. They often achieve strong
correlations with human judgments, but recent research indicates that the
lower-quality classical metrics remain dominant, one of the potential reasons
being that their decision processes are more transparent. To foster more
widespread acceptance of novel high-quality metrics, explainability thus
becomes crucial. In this concept paper, we identify key properties as well as
key goals of explainable machine translation metrics and provide a
comprehensive synthesis of recent techniques, relating them to our established
goals and properties. In this context, we also discuss the latest
state-of-the-art approaches to explainable metrics based on generative models
such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation
approaches, including natural language explanations. We hope that our work can
help catalyze and guide future research on explainable evaluation metrics and,
mediately, also contribute to better and more transparent machine translation
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13047">
<div class="article-summary-box-inner">
<span><p>Multiple Choice examinations are a ubiquitous form of assessment that is used
to measure the ability of candidates across various domains and tasks.
Maintaining the quality of proposed questions is of great importance to test
designers, and therefore newly proposed questions go through several pre-test
evaluation stages before they can be deployed into real-world exams. This
process is currently quite manual, which can lead to time lags in the question
development cycle. Automating this process would lead to a large improvement in
efficiency, however, current datasets do not contain sufficient pre-test
analysis information. In this paper, we introduce CamChoice; a multiple-choice
comprehension dataset with questions at different target levels, where
questions have the true candidate selected options distributions. We introduce
the task of candidate distribution matching, propose several evaluation metrics
for the task, and demonstrate that automatic systems trained on RACE++ can be
leveraged as baselines for our task. We further demonstrate that these
automatic systems can be used for practical pre-test evaluation tasks such as
detecting underperforming distractors, where our detection systems can
automatically identify poor distractors that few candidates select. We release
the data publicly for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named entity recognition in resumes. (arXiv:2306.13062v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13062">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is used to extract information from various
documents and texts such as names and dates. It is important to extract
education and work experience information from resumes in order to filter them.
Considering the fact that all information in a resume has to be entered to the
companys system manually, automatizing this process will save time of the
companies. In this study, a deep learning-based semi-automatic named entity
recognition system has been implemented with a focus on resumes in the field of
IT. Firstly, resumes of employees from five different IT related fields has
been annotated. Six transformer based pre-trained models have been adapted to
named entity recognition problem using the annotated data. These models have
been selected among popular models in the natural language processing field.
The obtained system can recognize eight different entity types which are city,
date, degree, diploma major, job title, language, country and skill. Models
used in the experiments are compared using micro, macro and weighted F1 scores
and the performance of the methods was evaluated. Taking these scores into
account for test set the best micro and weighted F1 score is obtained by
RoBERTa and the best macro F1 score is obtained by Electra model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13063">
<div class="article-summary-box-inner">
<span><p>The task of empowering large language models (LLMs) to accurately express
their confidence, referred to as confidence elicitation, is essential in
ensuring reliable and trustworthy decision-making processes. Previous methods,
which primarily rely on model logits, have become less suitable for LLMs and
even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM
APIs). This leads to a growing need to explore the untapped area of
\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence,
in this study, we investigate approaches for confidence elicitation that do not
require model fine-tuning or access to proprietary information. We introduce
three categories of methods: verbalize-based, consistency-based, and their
hybrid methods for benchmarking, and evaluate their performance across five
types of datasets and four widely-used LLMs. Our analysis of these methods
uncovers several key insights: 1) LLMs often exhibit a high degree of
overconfidence when verbalizing their confidence; 2) Prompting strategies such
as CoT, Top-K and Multi-step confidences improve calibration of verbalized
confidence; 3) Consistency-based methods outperform the verbalized confidences
in most cases, with particularly notable improvements on the arithmetic
reasoning task; 4) Hybrid methods consistently deliver the best performance
over their baselines, thereby emerging as a promising state-of-the-art
approach; 5) Despite these advancements, all investigated methods continue to
struggle with challenging tasks, such as those requiring professional
knowledge, leaving significant scope for improvement of confidence elicitation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020. (arXiv:2306.13075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13075">
<div class="article-summary-box-inner">
<span><p>Investigators, funders, and the public desire knowledge on topics and trends
in publicly funded research but current efforts in manual categorization are
limited in scale and understanding. We developed a semi-automated approach to
extract and name research topics, and applied this to \$1.9B of NCI funding
over 21 years in the radiological sciences to determine micro- and macro-scale
research topics and funding trends. Our method relies on sequential clustering
of existing biomedical-based word embeddings, naming using subject matter
experts, and visualization to discover trends at a macroscopic scale above
individual topics. We present results using 15 and 60 cluster topics, where we
found that 2D projection of grant embeddings reveals two dominant axes:
physics-biology and therapeutic-diagnostic. For our dataset, we found that
funding for therapeutics- and physics-based research have outpaced diagnostics-
and biology-based research, respectively. We hope these results may (1) give
insight to funders on the appropriateness of their funding allocation, (2)
assist investigators in contextualizing their work and explore neighboring
research domains, and (3) allow the public to review where their tax dollars
are being allocated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13089">
<div class="article-summary-box-inner">
<span><p>Molecule property prediction has gained significant attention in recent
years. The main bottleneck is the label insufficiency caused by expensive lab
experiments. In order to alleviate this issue and to better leverage textual
knowledge for tasks, this study investigates the feasibility of employing
natural language instructions to accomplish molecule-related tasks in a
zero-shot setting. We discover that existing molecule-text models perform
poorly in this setting due to inadequate treatment of instructions and limited
capacity for graphs. To overcome these issues, we propose GIMLET, which unifies
language models for both graph and text data. By adopting generalized position
embedding, our model is extended to encode both graph structures and
instruction text without additional graph encoding modules. GIMLET also
decouples encoding of the graph from tasks instructions in the attention
mechanism, enhancing the generalization of graph features across novel tasks.
We construct a dataset consisting of more than two thousand molecule tasks with
corresponding instructions derived from task descriptions. We pretrain GIMLET
on the molecule tasks along with instructions, enabling the model to transfer
effectively to a broad range of tasks. Experimental results demonstrate that
GIMLET significantly outperforms molecule-text baselines in instruction-based
zero-shot learning, even achieving closed results to supervised GNN models on
tasks such as toxcast and muv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmTract: Extracting Emotions from Social Media. (arXiv:2112.03868v3 [q-fin.PR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03868">
<div class="article-summary-box-inner">
<span><p>We develop an open-source tool (EmTract) that extracts emotions from social
media text tailed for financial context. To do so, we annotate ten thousand
short messages from a financial social media platform (StockTwits) and combine
it with open-source emotion data. We then use a pre-tuned NLP model,
DistilBERT, augment its embedding space by including 4,861 tokens (emojis and
emoticons), and then fit it first on the open-source emotion data, then
transfer it to our annotated financial social media data. Our model outperforms
competing open-source state-of-the-art emotion classifiers, such as Emotion
English DistilRoBERTa-base on both human and chatGPT annotated data. Compared
to dictionary based methods, our methodology has three main advantages for
research in finance. First, our model is tailored to financial social media
text; second, it incorporates key aspects of social media data, such as
non-standard phrases, emojis, and emoticons; and third, it operates by
sequentially learning a latent representation that includes features such as
word order, word usage, and local context. Using EmTract, we explore the
relationship between investor emotions expressed on social media and asset
prices. We show that firm-specific investor emotions are predictive of daily
price movements. Our findings show that emotions and market dynamics are
closely related, and we provide a tool to help study the role emotions play in
financial markets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12593">
<div class="article-summary-box-inner">
<span><p>Recent research has revealed that deep neural networks often take dataset
biases as a shortcut to make decisions rather than understand tasks, leading to
failures in real-world applications. In this study, we focus on the spurious
correlation between word features and labels that models learn from the biased
data distribution of training data. In particular, we define the word highly
co-occurring with a specific label as biased word, and the example containing
biased word as biased example. Our analysis shows that biased examples are
easier for models to learn, while at the time of prediction, biased words make
a significantly higher contribution to the models' predictions, and models tend
to assign predicted labels over-relying on the spurious correlation between
words and labels. To mitigate models' over-reliance on the shortcut (i.e.
spurious correlation), we propose a training strategy Less-Learn-Shortcut
(LLS): our strategy quantifies the biased degree of the biased examples and
down-weights them accordingly. Experimental results on Question Matching,
Natural Language Inference and Sentiment Analysis tasks show that LLS is a
task-agnostic strategy and can improve the model performance on adversarial
data while maintaining good performance on in-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00221">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pretraining (VLP) models have recently successfully
facilitated many cross-modal downstream tasks. Most existing works evaluated
their systems by comparing the fine-tuned downstream task performance. However,
only average downstream task accuracy provides little information about the
pros and cons of each VLP method, let alone provides insights on how the
community can improve the systems in the future. Inspired by the CheckList for
testing natural language processing, we exploit VL-CheckList, a novel framework
to understand the capabilities of VLP models. The proposed method divides the
image-texting ability of a VLP model into three categories: objects,
attributes, and relations, and uses a novel taxonomy to further break down
these three aspects. We conduct comprehensive studies to analyze seven recently
popular VLP models via the proposed framework. Results confirm the
effectiveness of the proposed method by revealing fine-grained differences
among the compared models that were not visible from downstream task-only
evaluation. Further results show promising research direction in building
better VLP models. Our data and code are available at:
https://github.com/om-ai-lab/VL-CheckList.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11419">
<div class="article-summary-box-inner">
<span><p>This paper presents an in-depth study on a Sequentially Sampled Chunk
Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer
first demonstrates the significant performance gains from using the
sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the
Conformer encoder by allowing efficient cross-chunk interactions while keeping
linear complexities. Furthermore, it explores taking advantage of chunked
convolution to make use of the chunk-wise future context and integrates with
casual convolution in the convolution layers to further reduce CER. We verify
the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results
show that a state-of-the-art performance for streaming E2E ASR is achieved with
CER 5.33% without LM rescoring. And, owing to its linear complexity, the
SSC-Conformer can train with large batch sizes and infer more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15359">
<div class="article-summary-box-inner">
<span><p>The next step for intelligent dialog agents is to escape their role as silent
bystanders and become proactive. Well-defined proactive behavior may improve
human-machine cooperation, as the agent takes a more active role during
interaction and takes off responsibility from the user. However, proactivity is
a double-edged sword because poorly executed pre-emptive actions may have a
devastating effect not only on the task outcome but also on the relationship
with the user. For designing adequate proactive dialog strategies, we propose a
novel approach including both social as well as task-relevant features in the
dialog. Here, the primary goal is to optimize proactive behavior so that it is
task-oriented - this implies high task success and efficiency - while also
being socially effective by fostering user trust. Including both aspects in the
reward function for training a proactive dialog agent using reinforcement
learning showed the benefit of our approach for more successful human-machine
cooperation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10535">
<div class="article-summary-box-inner">
<span><p>Mathematical reasoning is a fundamental aspect of human intelligence and is
applicable in various fields, including science, engineering, finance, and
everyday life. The development of artificial intelligence (AI) systems capable
of solving math problems and proving theorems has garnered significant interest
in the fields of machine learning and natural language processing. For example,
mathematics serves as a testbed for aspects of reasoning that are challenging
for powerful deep learning models, driving new algorithmic and modeling
advances. On the other hand, recent advances in large-scale neural language
models have opened up new benchmarks and opportunities to use deep learning for
mathematical reasoning. In this survey paper, we review the key tasks,
datasets, and methods at the intersection of mathematical reasoning and deep
learning over the past decade. We also evaluate existing benchmarks and
methods, and discuss future research directions in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11309">
<div class="article-summary-box-inner">
<span><p>Extreme classification (XC) involves predicting over large numbers of classes
(thousands to millions), with real-world applications like news article
classification and e-commerce product tagging. The zero-shot version of this
task requires generalization to novel classes without additional supervision.
In this paper, we develop SemSup-XC, a model that achieves state-of-the-art
zero-shot and few-shot performance on three XC datasets derived from legal,
e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically
collected semantic class descriptions to represent classes and facilitate
generalization through a novel hybrid matching module that matches input
instances to class descriptions using a combination of semantic and lexical
similarity. Trained with contrastive learning, SemSup-XC significantly
outperforms baselines and establishes state-of-the-art performance on all three
datasets considered, gaining up to 12 precision points on zero-shot and more
than 10 precision points on one-shot tests, with similar gains for recall@10.
Our ablation studies highlight the relative importance of our hybrid matching
module and automatically collected class descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract Visual Reasoning Enabled by Language. (arXiv:2303.04091v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04091">
<div class="article-summary-box-inner">
<span><p>While artificial intelligence (AI) models have achieved human or even
superhuman performance in many well-defined applications, they still struggle
to show signs of broad and flexible intelligence. The Abstraction and Reasoning
Corpus (ARC), a visual intelligence benchmark introduced by Fran\c{c}ois
Chollet, aims to assess how close AI systems are to human-like cognitive
abilities. Most current approaches rely on carefully handcrafted
domain-specific program searches to brute-force solutions for the tasks present
in ARC. In this work, we propose a general learning-based framework for solving
ARC. It is centered on transforming tasks from the vision to the language
domain. This composition of language and vision allows for pre-trained models
to be leveraged at each stage, enabling a shift from handcrafted priors towards
the learned priors of the models. While not yet beating state-of-the-art models
on ARC, we demonstrate the potential of our approach, for instance, by solving
some ARC tasks that have not been solved previously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13994">
<div class="article-summary-box-inner">
<span><p>We present SweCTRL-Mini, a large Swedish language model that can be used for
inference and fine-tuning on a single consumer-grade GPU. The model is based on
the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),
which means that users of the SweCTRL-Mini model can control the genre of the
generated text by inserting special tokens in the generation prompts.
SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a
set of Swedish novels. In this article, we provide (1) a detailed account of
the utilized training data and text pre-processing steps, to the extent that it
is possible to check whether a specific phrase/source was a part of the
training data, and (2) an evaluation of the model on both discriminative tasks,
using automatic evaluation methods, and generative tasks, using human referees.
We also compare the generative capabilities of the model with those of GPT-3.
SweCTRL-Mini is fully open and available for download.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using ChatGPT for Entity Matching. (arXiv:2305.03423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03423">
<div class="article-summary-box-inner">
<span><p>Entity Matching is the task of deciding if two entity descriptions refer to
the same real-world entity. State-of-the-art entity matching methods often rely
on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks
of using these models for entity matching are that (i) the models require
significant amounts of fine-tuning data for reaching a good performance and
(ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using ChatGPT for entity matching as a
more robust, training data-efficient alternative to traditional Transformer
models. We perform experiments along three dimensions: (i) general prompt
design, (ii) in-context learning, and (iii) provision of higher-level matching
knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,
reaching a zero-shot performance of 82.35% F1 on a challenging matching task on
which RoBERTa requires 2000 training examples for reaching a similar
performance. Adding in-context demonstrations to the prompts further improves
the F1 by up to 7.85% when using similarity-based example selection. Always
using the same set of 10 handpicked demonstrations leads to an improvement of
4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be
guided by adding higher-level matching knowledge in the form of rules to the
prompts. Providing matching rules leads to similar performance gains as
providing in-context demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04118">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive capabilities in
general scenarios, exhibiting a level of aptitude that approaches, in some
aspects even surpasses, human-level intelligence. Among their numerous skills,
the translation abilities of LLMs have received considerable attention. In
contrast to traditional machine translation that focuses solely on
source-target mapping, LLM-based translation can potentially mimic the human
translation process that takes many preparatory steps to ensure high-quality
translation. This work aims to explore this possibility by proposing the MAPS
framework, which stands for Multi-Aspect Prompting and Selection. Specifically,
we enable LLMs to first analyze the given source text and extract three aspects
of translation-related knowledge: keywords, topics and relevant demonstrations
to guide the translation process. To filter out the noisy and unhelpful
knowledge, we employ a selection mechanism based on quality estimation.
Experiments suggest that MAPS brings significant and consistent improvements
over text-davinci-003 and Alpaca on eight translation directions from the
latest WMT22 test sets. Our further analysis shows that the extracted knowledge
is critical in resolving up to 59% of hallucination mistakes in translation.
Code is available at https://github.com/zwhe99/MAPS-mt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04493">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence (seq2seq) models have been widely used for natural
language processing, computer vision, and other deep learning tasks. We find
that seq2seq models trained with early-stopping suffer from issues at the token
level. In particular, while some tokens in the vocabulary demonstrate
overfitting, others underfit when training is stopped. Experiments show that
the phenomena are pervasive in different models, even in fine-tuned large
pretrained-models. We identify three major factors that influence token-level
fitting, which include token frequency, parts-of-speech, and prediction
discrepancy. Further, we find that external factors such as language, model
size, domain, data scale, and pretraining can also influence the fitting of
tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05480">
<div class="article-summary-box-inner">
<span><p>We would like to explore how morphemes can affect the performance of a
language model. We trained GPT-2 and Bert model with StateMorph for both
Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison,
we also trained a model with BPE and Morfessor. Our preliminary result shows
that StateMorph can help the model to converge more efficiently and achieve a
better validation score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v4 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09770">
<div class="article-summary-box-inner">
<span><p>Despite a surge collection of XAI methods, users still struggle to obtain
required AI explanations. Previous research suggests chatbots as dynamic
solutions, but the effective design of conversational XAI agents for practical
human needs remains under-explored. This paper focuses on Conversational XAI
for AI-assisted scientific writing tasks. Drawing from human linguistic
theories and formative studies, we identify four design rationales:
"multifaceted", "controllability", "mix-initiative", "context-aware
drill-down". We incorporate them into an interactive prototype, ConvXAI, which
facilitates heterogeneous AI explanations for scientific writing through
dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based
baseline on improving human-perceived understanding and writing improvement.
The paper further discusses the practical human usage patterns in interacting
with ConvXAI for scientific co-writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11554">
<div class="article-summary-box-inner">
<span><p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09442">
<div class="article-summary-box-inner">
<span><p>Deploying Large language models (LLMs) can pose hazards from harmful outputs
such as toxic or dishonest speech. Prior work has introduced tools that elicit
harmful outputs in order to identify and mitigate these risks. While this is a
valuable step toward securing language models, these approaches typically rely
on a pre-existing classifier for undesired outputs. This limits their
application to situations where the type of harmful behavior is known with
precision beforehand. However, this skips a central challenge of red teaming:
developing a contextual understanding of the behaviors that a model can
exhibit. Furthermore, when such a classifier already exists, red teaming has
limited marginal value because the classifier could simply be used to filter
training data or model outputs. In this work, we consider red teaming under the
assumption that the adversary is working from a high-level, abstract
specification of undesired behavior. The red team is expected to refine/extend
this specification and identify methods to elicit this behavior from the model.
Our red teaming framework consists of three steps: 1) Exploring the model's
behavior in the desired context; 2) Establishing a measurement of undesired
behavior (e.g., a classifier trained to reflect human evaluations); and 3)
Exploiting the model's flaws using this measure and an established red teaming
methodology. We apply this approach to red team GPT-2 and GPT-3 models to
systematically discover classes of prompts that elicit toxic and dishonest
statements. In doing so, we also construct and release the CommonClaim dataset
of 20,000 statements that have been labeled by human subjects as
common-knowledge-true, common-knowledge-false, or neither. Code is available at
https://github.com/thestephencasper/explore_establish_exploit_llms. CommonClaim
is available at https://github.com/Algorithmic-Alignment-Lab/CommonClaim.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09525">
<div class="article-summary-box-inner">
<span><p>Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09896">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown remarkable aptitude in code
generation but still struggle on challenging programming tasks. Self-repair --
in which the model debugs and fixes mistakes in its own code -- has recently
become a popular way to boost performance in these settings. However, only very
limited studies on how and when self-repair works effectively exist in the
literature, and one might wonder to what extent a model is really capable of
providing accurate feedback on why the code is wrong when that code was
generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's
ability to perform self-repair on APPS, a challenging dataset consisting of
diverse coding challenges. To do so, we first establish a new evaluation
strategy dubbed pass@t that measures the pass rate of the tasks against the
total number of tokens sampled from the model, enabling a fair comparison to
purely sampling-based approaches. With this evaluation strategy, we find that
the effectiveness of self-repair is only seen in GPT-4. We also observe that
self-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback
on the programs generated by GPT-3.5 and using expert human programmers to give
feedback on the programs generated by GPT-4, we unlock significant performance
gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11207">
<div class="article-summary-box-inner">
<span><p>Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has halted
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate Quilt: a large-scale vision-language
dataset consisting of $768,826$ image and text pairs. Quilt was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine Quilt with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: Quilt-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-23 23:12:26.794799984 UTC">2023-06-23 23:12:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>