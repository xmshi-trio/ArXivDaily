<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-27T01:30:00Z">10-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature. (arXiv:2210.14250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14250">
<div class="article-summary-box-inner">
<span><p>Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios. (arXiv:2210.14254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14254">
<div class="article-summary-box-inner">
<span><p>In psychotherapy interactions, the quality of a session is assessed by
codifying the communicative behaviors of participants during the conversation
through manual observation and annotation. Developing computational approaches
for automated behavioral coding can reduce the burden on human coders and
facilitate the objective evaluation of the intervention. In the real world,
however, implementing such algorithms is associated with data sparsity
challenges since privacy concerns lead to limited available in-domain data. In
this paper, we leverage a publicly available conversation-based dataset and
transfer knowledge to the low-resource behavioral coding task by performing an
intermediate language model training via meta-learning. We introduce a task
augmentation method to produce a large number of "analogy tasks" - tasks
similar to the target one - and demonstrate that the proposed framework
predicts target behaviors more accurately than all the other baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revision for Concision: A Constrained Paraphrase Generation Task. (arXiv:2210.14257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14257">
<div class="article-summary-box-inner">
<span><p>Academic writing should be concise as concise sentences better keep the
readers' attention and convey meaning clearly. Writing concisely is
challenging, for writers often struggle to revise their drafts. We introduce
and formulate revising for concision as a natural language processing task at
the sentence level. Revising for concision requires algorithms to use only
necessary words to rewrite a sentence while preserving its meaning. The revised
sentence should be evaluated according to its word choice, sentence structure,
and organization. The revised sentence also needs to fulfil semantic retention
and syntactic soundness. To aide these efforts, we curate and make available a
benchmark parallel dataset that can depict revising for concision. The dataset
contains 536 pairs of sentences before and after revising, and all pairs are
collected from college writing centres. We also present and evaluate the
approaches to this problem, which may assist researchers in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Evasion Attacks on Summarization Scoring. (arXiv:2210.14260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14260">
<div class="article-summary-box-inner">
<span><p>The automatic scoring of summaries is important as it guides the development
of summarizers. Scoring is also complex, as it involves multiple aspects such
as fluency, grammar, and even textual entailment with the source text. However,
summary scoring has not been considered a machine learning task to study its
accuracy and robustness. In this study, we place automatic scoring in the
context of regression machine learning tasks and perform evasion attacks to
explore its robustness. Attack systems predict a non-summary string from each
input, and these non-summary strings achieve competitive scores with good
summarizers on the most popular metrics: ROUGE, METEOR, and BERTScore. Attack
systems also "outperform" state-of-the-art summarization methods on ROUGE-1 and
ROUGE-L, and score the second-highest on METEOR. Furthermore, a BERTScore
backdoor is observed: a simple trigger can score higher than any automatic
summarization method. The evasion attacks in this work indicate the low
robustness of current scoring systems at the system level. We hope that our
highlighting of these proposed attacks will facilitate the development of
summary scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation. (arXiv:2210.14275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14275">
<div class="article-summary-box-inner">
<span><p>Capturing the similarities between human language units is crucial for
explaining how humans associate different objects, and therefore its
computation has received extensive attention, research, and applications. With
the ever-increasing amount of information around us, calculating similarity
becomes increasingly complex, especially in many cases, such as legal or
medical affairs, measuring similarity requires extra care and precision, as
small acts within a language unit can have significant real-world effects. My
research goal in this thesis is to develop regression models that account for
similarities between language units in a more refined way.
</p>
<p>Computation of similarity has come a long way, but approaches to debugging
the measures are often based on continually fitting human judgment values. To
this end, my goal is to develop an algorithm that precisely catches loopholes
in a similarity calculation. Furthermore, most methods have vague definitions
of the similarities they compute and are often difficult to interpret. The
proposed framework addresses both shortcomings. It constantly improves the
model through catching different loopholes. In addition, every refinement of
the model provides a reasonable explanation. The regression model introduced in
this thesis is called progressively refined similarity computation, which
combines attack testing with adversarial training. The similarity regression
model of this thesis achieves state-of-the-art performance in handling edge
cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenStance: Real-world Zero-shot Stance Detection. (arXiv:2210.14299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14299">
<div class="article-summary-box-inner">
<span><p>Prior studies of zero-shot stance detection identify the attitude of texts
towards unseen topics occurring in the same document corpus. Such task
formulation has three limitations: (i) Single domain/dataset. A system is
optimized on a particular dataset from a single domain; therefore, the
resulting system cannot work well on other datasets; (ii) the model is
evaluated on a limited number of unseen topics; (iii) it is assumed that part
of the topics has rich annotations, which might be impossible in real-world
applications. These drawbacks will lead to an impractical stance detection
system that fails to generalize to open domains and open-form topics. This work
defines OpenStance: open-domain zero-shot stance detection, aiming to handle
stance detection in an open world with neither domain constraints nor
topic-specific annotations. The key challenge of OpenStance lies in the
open-domain generalization: learning a system with fully unspecific supervision
but capable of generalizing to any dataset. To solve OpenStance, we propose to
combine indirect supervision, from textual entailment datasets, and weak
supervision, from data generated automatically by pre-trained Language Models.
Our single system, without any topic-specific supervision, outperforms the
supervised method on three popular datasets. To our knowledge, this is the
first work that studies stance detection under the open-domain zero-shot
setting. All data and code are publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Better Intent Representations for Financial Open Intent Classification. (arXiv:2210.14304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14304">
<div class="article-summary-box-inner">
<span><p>With the recent surge of NLP technologies in the financial domain, banks and
other financial entities have adopted virtual agents (VA) to assist customers.
A challenging problem for VAs in this domain is determining a user's reason or
intent for contacting the VA, especially when the intent was unseen or open
during the VA's training. One method for handling open intents is adaptive
decision boundary (ADB) post-processing, which learns tight decision boundaries
from intent representations to separate known and open intents. We propose
incorporating two methods for supervised pre-training of intent
representations: prefix-tuning and fine-tuning just the last layer of a large
language model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher
than the prior state-of-the-art ADB method for open intent classification on
the banking77 benchmark amongst others. Notably, we only supplement the
original ADB model with 0.1% additional trainable parameters. Ablation studies
also determine that our method yields better results than full fine-tuning the
entire model. We hypothesize that our findings could stimulate a new optimal
method of downstream tuning that combines parameter efficient tuning modules
with fine-tuning a subset of the base model's layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robust Incremental Learning over Many Multilingual Steps. (arXiv:2210.14307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14307">
<div class="article-summary-box-inner">
<span><p>Recent work in incremental learning has introduced diverse approaches to
tackle catastrophic forgetting from data augmentation to optimized training
regimes. However, most of them focus on very few training steps. We propose a
method for robust incremental learning over dozens of fine-tuning steps using
data from a variety of languages. We show that a combination of
data-augmentation and an optimized training regime allows us to continue
improving the model even for as many as fifty training steps. Crucially, our
augmentation strategy does not require retaining access to previous training
data and is suitable in scenarios with privacy constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models. (arXiv:2210.14328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14328">
<div class="article-summary-box-inner">
<span><p>Structural probing work has found evidence for latent syntactic information
in pre-trained language models. However, much of this analysis has focused on
monolingual models, and analyses of multilingual models have employed
correlational methods that are confounded by the choice of probing tasks. In
this study, we causally probe multilingual language models (XGLM and
multilingual BERT) as well as monolingual BERT-based models across various
languages; we do this by performing counterfactual perturbations on neuron
activations and observing the effect on models' subject-verb agreement
probabilities. We observe where in the model and to what extent syntactic
agreement is encoded in each language. We find significant neuron overlap
across languages in autoregressive multilingual language models, but not masked
language models. We also find two distinct layer-wise effect patterns and two
distinct sets of neurons used for syntactic agreement, depending on whether the
subject and verb are separated by other tokens. Finally, we find that
behavioral analyses of language models are likely underestimating how sensitive
masked language models are to syntactic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14348">
<div class="article-summary-box-inner">
<span><p>Privacy concerns have attracted increasing attention in data-driven products
and services. Existing legislation forbids arbitrary processing of personal
data collected from individuals. Generating synthetic versions of such data
with a formal privacy guarantee such as differential privacy (DP) is considered
to be a solution to address privacy concerns. In this direction, we show a
simple, practical, and effective recipe in the text domain: simply fine-tuning
a generative language model with DP allows us to generate useful synthetic text
while mitigating privacy concerns. Through extensive empirical analyses, we
demonstrate that our method produces synthetic data that is competitive in
terms of utility with its non-private counterpart and meanwhile provides strong
protection against potential privacy leakages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering. (arXiv:2210.14353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14353">
<div class="article-summary-box-inner">
<span><p>We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Product Safety in E-Commerce with NLP. (arXiv:2210.14363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14363">
<div class="article-summary-box-inner">
<span><p>Ensuring safety of the products offered to the customers is of paramount
importance to any e- commerce platform. Despite stringent quality and safety
checking of products listed on these platforms, occasionally customers might
receive a product that can pose a safety issue arising out of its use. In this
paper, we present an innovative mechanism of how a large scale multinational
e-commerce platform, Zalando, uses Natural Language Processing techniques to
assist timely investigation of the potentially unsafe products mined directly
from customer written claims in unstructured plain text. We systematically
describe the types of safety issues that concern Zalando customers. We
demonstrate how we map this core business problem into a supervised text
classification problem with highly imbalanced, noisy, multilingual data in a
AI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven
evaluation. Finally, we present detailed ablation studies to show a
comprehensive comparison between different classification techniques. We
conclude the work with how this NLP model was deployed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Monitor Model and its Misconceptions: A Clarification. (arXiv:2210.14367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14367">
<div class="article-summary-box-inner">
<span><p>Horizontal (automatic) and vertical (control) processes have long been
reported in human translation production (e.g., Konig 1987, Lorscher 1991,
Jaaskelainen 1996, de Groot 1997, Tirkkonen-Condit 2005, Macizo and Bajo 2006).
The Monitor Model (Schaeffer and Carl 2013, 2015) integrates horizontal and
vertical processes, assuming priming mechanisms underlie horizontal/automatic
processes, while vertical/monitoring processes implement consciously accessible
control mechanisms. Carl (2021a) argues that priming processes in translation
are part of perception-action loops, interpretable in an embodied/enactivist
framework. Carl (2022) develops a post-humanist view on translator-technology
interaction facilitated by priming mechanisms which enable representationally
unmediated translator-environment coupling. I substantiate these claims,
arguing that translation priming results in basic, non-representational
content. I update the Monitor Model with additional evidence and address an
accumulation of misconceptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport. (arXiv:2210.14378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14378">
<div class="article-summary-box-inner">
<span><p>Bilingual lexicons form a critical component of various natural language
processing applications, including unsupervised and semisupervised machine
translation and crosslingual information retrieval. We improve bilingual
lexicon induction performance across 40 language pairs with a graph-matching
method based on optimal transport. The method is especially strong with low
amounts of supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deploying a Retrieval based Response Model for Task Oriented Dialogues. (arXiv:2210.14379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14379">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems in industry settings need to have high
conversational capability, be easily adaptable to changing situations and
conform to business constraints. This paper describes a 3-step procedure to
develop a conversational model that satisfies these criteria and can
efficiently scale to rank a large set of response candidates. First, we provide
a simple algorithm to semi-automatically create a high-coverage template set
from historic conversations without any annotation. Second, we propose a neural
architecture that encodes the dialogue context and applicable business
constraints as profile features for ranking the next turn. Third, we describe a
two-stage learning strategy with self-supervised training, followed by
supervised fine-tuning on limited data collected through a human-in-the-loop
platform. Finally, we describe offline experiments and present results of
deploying our model with human-in-the-loop to converse with live customers
online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Sentiment Analysis for Code-Switched Text Data. (arXiv:2210.14380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14380">
<div class="article-summary-box-inner">
<span><p>Multilingual transformer language models have recently attracted much
attention from researchers and are used in cross-lingual transfer learning for
many NLP tasks such as text classification and named entity recognition.
However, similar methods for transfer learning from monolingual text to
code-switched text have not been extensively explored mainly due to the
following challenges: (1) Code-switched corpus, unlike monolingual corpus,
consists of more than one language and existing methods can't be applied
efficiently, (2) Code-switched corpus is usually made of resource-rich and
low-resource languages and upon using multilingual pre-trained language models,
the final model might bias towards resource-rich language. In this paper, we
focus on code-switched sentiment analysis where we have a labelled
resource-rich language dataset and unlabelled code-switched data. We propose a
framework that takes the distinction between resource-rich and low-resource
language into account. Instead of training on the entire code-switched corpus
at once, we create buckets based on the fraction of words in the resource-rich
language and progressively train from resource-rich language dominated samples
to low-resource language dominated samples. Extensive experiments across
multiple language pairs demonstrate that progressive training helps
low-resource language dominated samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14389">
<div class="article-summary-box-inner">
<span><p>Research on Korean grammatical error correction (GEC) is limited compared to
other major languages such as English and Chinese. We attribute this
problematic circumstance to the lack of a carefully designed evaluation
benchmark for Korean. Thus, in this work, we first collect three datasets from
different sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide
range of error types and annotate them using our newly proposed tool called
Korean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a
carefully designed edit alignment &amp; classification tool that considers the
nature of Korean on generating an alignment between a source sentence and a
target sentence, and identifies error types on each aligned edit. We also
present baseline models fine-tuned over our datasets. We show that the model
trained with our datasets significantly outperforms the public statistical GEC
system (Hanspell) on a wider range of error types, demonstrating the diversity
and usefulness of the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text. (arXiv:2210.14395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14395">
<div class="article-summary-box-inner">
<span><p>We present IMU2CLIP, a novel pre-training approach to align Inertial
Measurement Unit (IMU) motion sensor recordings with video and text, by
projecting them into the joint representation space of Contrastive
Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to
translate human motions (as measured by IMU sensors) into their corresponding
textual descriptions and videos -- while preserving the transitivity across
these modalities.
</p>
<p>We explore several new IMU-based applications that IMU2CLIP enables, such as
motion-based media retrieval and natural language reasoning tasks with motion
data. In addition, we show that IMU2CLIP can significantly improve the
downstream performance when fine-tuned for each application (e.g. activity
recognition), demonstrating the universal usage of IMU2CLIP as a new
pre-trained resource. Our code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RedPen: Region- and Reason-Annotated Dataset of Unnatural Speech. (arXiv:2210.14406v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14406">
<div class="article-summary-box-inner">
<span><p>Even with recent advances in speech synthesis models, the evaluation of such
models is based purely on human judgement as a single naturalness score, such
as the Mean Opinion Score (MOS). The score-based metric does not give any
further information about which parts of speech are unnatural or why human
judges believe they are unnatural. We present a novel speech dataset, RedPen,
with human annotations on unnatural speech regions and their corresponding
reasons. RedPen consists of 180 synthesized speeches with unnatural regions
annotated by crowd workers; These regions are then reasoned and categorized by
error types, such as voice trembling and background noise. We find that our
dataset shows a better explanation for unnatural speech regions than the
model-driven unnaturalness prediction. Our analysis also shows that each model
includes different types of error types. Summing up, our dataset successfully
shows the possibility that various error regions and types lie under the single
naturalness score. We believe that our dataset will shed light on the
evaluation and development of more interpretable speech models in the future.
Our dataset will be publicly available upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling the Graphotactics of Low-Resource Languages Using Sequential GANs. (arXiv:2210.14409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14409">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have been shown to aid in the creation
of artificial data in situations where large amounts of real data are difficult
to come by. This issue is especially salient in the computational linguistics
space, where researchers are often tasked with modeling the complex morphologic
and grammatical processes of low-resource languages. This paper will discuss
the implementation and testing of a GAN that attempts to model and reproduce
the graphotactics of a language using only 100 example strings. These
artificial, yet graphotactically compliant, strings are meant to aid in
modeling the morphological inflection of low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse-Aware Emotion Cause Extraction in Conversations. (arXiv:2210.14419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14419">
<div class="article-summary-box-inner">
<span><p>Emotion Cause Extraction in Conversations (ECEC) aims to extract the
utterances which contain the emotional cause in conversations. Most prior
research focuses on modelling conversational contexts with sequential encoding,
ignoring the informative interactions between utterances and
conversational-specific features for ECEC. In this paper, we investigate the
importance of discourse structures in handling utterance interactions and
conversationspecific features for ECEC. To this end, we propose a
discourse-aware model (DAM) for this task. Concretely, we jointly model ECEC
with discourse parsing using a multi-task learning (MTL) framework and
explicitly encode discourse structures via gated graph neural network (gated
GNN), integrating rich utterance interaction information to our model. In
addition, we use gated GNN to further enhance our ECEC model with
conversation-specific features. Results on the benchmark corpus show that DAM
outperform the state-of-theart (SOTA) systems in the literature. This suggests
that the discourse structure may contain a potential link between emotional
utterances and their corresponding cause expressions. It also verifies the
effectiveness of conversationalspecific features. The codes of this paper will
be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geographic Citation Gaps in NLP Research. (arXiv:2210.14424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14424">
<div class="article-summary-box-inner">
<span><p>In a fair world, people have equitable opportunities to education, to conduct
scientific research, to publish, and to get credit for their work, regardless
of where they live. However, it is common knowledge among researchers that a
vast number of papers accepted at top NLP venues come from a handful of western
countries and (lately) China; whereas, very few papers from Africa and South
America get published. Similar disparities are also believed to exist for paper
citation counts. In the spirit of "what we do not measure, we cannot improve",
this work asks a series of questions on the relationship between geographical
location and publication success (acceptance in top NLP venues and citation
impact). We first created a dataset of 70,000 papers from the ACL Anthology,
extracted their meta-information, and generated their citation network. We then
show that not only are there substantial geographical disparities in paper
acceptance and citation but also that these disparities persist even when
controlling for a number of variables such as venue of publication and
sub-field of NLP. Further, despite some steps taken by the NLP community to
improve geographical diversity, we show that the disparity in publication
metrics across locations is still on an increasing trend since the early 2000s.
We release our code and dataset here:
https://github.com/iamjanvijay/acl-cite-net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select. (arXiv:2210.14427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14427">
<div class="article-summary-box-inner">
<span><p>We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14431">
<div class="article-summary-box-inner">
<span><p>$N$-gram language models (LM) have been largely superseded by neural LMs as
the latter exhibits better performance. However, we find that $n$-gram models
can achieve satisfactory performance on a large proportion of testing cases,
indicating they have already captured abundant knowledge of the language with
relatively low computational cost. With this observation, we propose to learn a
neural LM that fits the residual between an $n$-gram LM and the real-data
distribution. The combination of $n$-gram and neural LMs not only allows the
neural part to focus on the deeper understanding of language but also provides
a flexible way to customize an LM by switching the underlying $n$-gram model
without changing the neural model. Experimental results on three typical
language tasks (i.e., language modeling, machine translation, and
summarization) demonstrate that our approach attains additional performance
gains over popular standalone neural models consistently. We also show that our
approach allows for effective domain adaptation by simply switching to a
domain-specific $n$-gram model, without any extra training. Our code is
released at https://github.com/ghrua/NgramRes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead. (arXiv:2210.14446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14446">
<div class="article-summary-box-inner">
<span><p>Segmentation for continuous Automatic Speech Recognition (ASR) has
traditionally used silence timeouts or voice activity detectors (VADs), which
are both limited to acoustic features. This segmentation is often overly
aggressive, given that people naturally pause to think as they speak.
Consequently, segmentation happens mid-sentence, hindering both punctuation and
downstream tasks like machine translation for which high-quality segmentation
is critical. Model-based segmentation methods that leverage acoustic features
are powerful, but without an understanding of the language itself, these
approaches are limited. We present a hybrid approach that leverages both
acoustic and language information to improve segmentation. Furthermore, we show
that including one word as a look-ahead boosts segmentation quality. On
average, our models improve segmentation-F0.5 score by 9.8% over baseline. We
show that this approach works for multiple languages. For the downstream task
of machine translation, it improves the translation BLEU score by an average of
1.05 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension. (arXiv:2210.14456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14456">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on dialogue reading comprehension (DRC), a task
extracting answer spans for questions from dialogues. Dialogue context modeling
in DRC is tricky due to complex speaker information and noisy dialogue context.
To solve the two problems, previous research proposes two self-supervised tasks
respectively: guessing who a randomly masked speaker is according to the
dialogue and predicting which utterance in the dialogue contains the answer.
Although these tasks are effective, there are still urging problems: (1)
randomly masking speakers regardless of the question cannot map the speaker
mentioned in the question to the corresponding speaker in the dialogue, and
ignores the speaker-centric nature of utterances. This leads to wrong answer
extraction from utterances in unrelated interlocutors' scopes; (2) the single
utterance prediction, preferring utterances similar to the question, is limited
in finding answer-contained utterances not similar to the question. To
alleviate these problems, we first propose a new key utterances extracting
method. It performs prediction on the unit formed by several contiguous
utterances, which can realize more answer-contained utterances. Based on
utterances in the extracted units, we then propose Question-Interlocutor Scope
Realized Graph (QuISG) modeling. As a graph constructed on the text of
utterances, QuISG additionally involves the question and question-mentioning
speaker names as nodes. To realize interlocutor scopes, speakers in the
dialogue are connected with the words in their corresponding utterances.
Experiments on the benchmarks show that our method can achieve better and
competitive results against previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts. (arXiv:2210.14463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14463">
<div class="article-summary-box-inner">
<span><p>Inductive knowledge graph completion requires models to comprehend the
underlying semantics and logic patterns of relations. With the advance of
pretrained language models, recent research have designed transformers for link
prediction tasks. However, empirical studies show that linearizing triples
affects the learning of relational patterns, such as inversion and symmetry. In
this paper, we propose Bi-Link, a contrastive learning framework with
probabilistic syntax prompts for link predictions. Using grammatical knowledge
of BERT, we efficiently search for relational prompts according to learnt
syntactical patterns that generalize to large knowledge graphs. To better
express symmetric relations, we design a symmetric link prediction model,
establishing bidirectional linking between forward prediction and backward
prediction. This bidirectional linking accommodates flexible self-ensemble
strategies at test time. In our experiments, Bi-Link outperforms recent
baselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).
Furthermore, we construct Zeshel-Ind as an in-domain inductive entity linking
the environment to evaluate Bi-Link. The experimental results demonstrate that
our method yields robust representations which can generalize under domain
shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eeny, meeny, miny, moe. How to choose data for morphological inflection. (arXiv:2210.14465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14465">
<div class="article-summary-box-inner">
<span><p>Data scarcity is a widespread problem in numerous natural language processing
(NLP) tasks for low-resource languages. Within morphology, the labour-intensive
work of tagging/glossing data is a serious bottleneck for both NLP and language
documentation. Active learning (AL) aims to reduce the cost of data annotation
by selecting data that is most informative for improving the model. In this
paper, we explore four sampling strategies for the task of morphological
inflection using a Transformer model: a pair of oracle experiments where data
is chosen based on whether the model already can or cannot inflect the test
forms correctly, as well as strategies based on high/low model confidence,
entropy, as well as random selection. We investigate the robustness of each
strategy across 30 typologically diverse languages. We also perform a more
in-depth case study of Nat\"ugu. Our results show a clear benefit to selecting
data based on model confidence and entropy. Unsurprisingly, the oracle
experiment, where only incorrectly handled forms are chosen for further
training, which is presented as a proxy for linguist/language consultant
feedback, shows the most improvement. This is followed closely by choosing
low-confidence and high-entropy predictions. We also show that despite the
conventional wisdom of larger data sets yielding better accuracy, introducing
more instances of high-confidence or low-entropy forms, or forms that the model
can already inflect correctly, can reduce model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning. (arXiv:2210.14469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14469">
<div class="article-summary-box-inner">
<span><p>Prefix-tuning, or more generally continuous prompt tuning, has become an
essential paradigm of parameter-efficient transfer learning. Using a large
pre-trained language model (PLM), prefix-tuning can obtain strong performance
by training only a small portion of parameters. In this paper, we propose to
understand and further develop prefix-tuning through the kernel lens.
Specifically, we make an analogy between \textit{prefixes} and \textit{inducing
variables} in kernel methods and hypothesize that \textit{prefixes} serving as
\textit{inducing variables} would improve their overall mechanism. From the
kernel estimator perspective, we suggest a new variant of prefix-tuning --
\textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning
while leveraging the residual form found in adapter-tuning. This mitigates the
initialization issue in prefix-tuning. Through comprehensive empirical
experiments on natural language understanding and generation tasks, we
demonstrate that inducer-tuning can close the performance gap between
prefix-tuning and fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource Languages. (arXiv:2210.14472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14472">
<div class="article-summary-box-inner">
<span><p>In the process of numerically modeling natural languages, developing language
embeddings is a vital step. However, it is challenging to develop functional
embeddings for resource-poor languages such as Sinhala, for which sufficiently
large corpora, effective language parsers, and any other required resources are
difficult to find. In such conditions, the exploitation of existing models to
come up with an efficacious embedding methodology to numerically represent text
could be quite fruitful. This paper explores the effectivity of several
one-tiered and two-tiered embedding architectures in representing Sinhala text
in the sentiment analysis domain. With our findings, the two-tiered embedding
architecture where the lower-tier consists of a word embedding and the
upper-tier consists of a sentence embedding has been proven to perform better
than one-tier word embeddings, by achieving a maximum F1 score of 88.04% in
contrast to the 83.76% achieved by word embedding models. Furthermore,
embeddings in the hyperbolic space are also developed and compared with
Euclidean embeddings in terms of performance. A sentiment data set consisting
of Facebook posts and associated reactions have been used for this research. To
effectively compare the performance of different embedding systems, the same
deep neural network structure has been trained on sentiment data with each of
the embedding systems used to encode the text associated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Language Models for Code Syntax Understanding. (arXiv:2210.14473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14473">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have demonstrated impressive performance in both
natural language processing and program understanding, which represent the
input as a token sequence without explicitly modeling its structure. Some prior
works show that pre-trained language models can capture the syntactic rules of
natural languages without finetuning on syntax understanding tasks. However,
there is limited understanding of how well pre-trained models understand the
code structure so far. In this work, we perform the first thorough benchmarking
of the state-of-the-art pre-trained models for identifying the syntactic
structures of programs. Specifically, we introduce CodeSyntax, a large-scale
dataset of programs annotated with the syntactic relationships in their
corresponding abstract syntax trees. Our key observation is that existing
language models pretrained on code still lack the understanding of code syntax.
In fact, these pre-trained programming language models fail to match the
performance of simple baselines based on positional offsets and keywords. We
also present a natural language benchmark to highlight the differences between
natural languages and programming languages in terms of syntactic structure
understanding. Our findings point out key limitations of existing pre-training
methods for programming languages, and suggest the importance of modeling code
syntactic structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding. (arXiv:2210.14486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14486">
<div class="article-summary-box-inner">
<span><p>Negation poses a challenge in many natural language understanding tasks.
Inspired by the fact that understanding a negated statement often requires
humans to infer affirmative interpretations, in this paper we show that doing
so benefits models for three natural language understanding tasks. We present
an automated procedure to collect pairs of sentences with negation and their
affirmative interpretations, resulting in over 150,000 pairs. Experimental
results show that leveraging these pairs helps (a) T5 generate affirmative
interpretations from negations in a previous benchmark, and (b) a RoBERTa-based
classifier solve the task of natural language inference. We also leverage our
pairs to build a plug-and-play neural generator that given a negated statement
generates an affirmative interpretation. Then, we incorporate the pretrained
generator into a RoBERTa-based classifier for sentiment analysis and show that
doing so improves the results. Crucially, our proposal does not require any
manual effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course. (arXiv:2210.14494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14494">
<div class="article-summary-box-inner">
<span><p>We introduce CS1QA, a dataset for code-based question answering in the
programming education domain. CS1QA consists of 9,237 question-answer pairs
gathered from chat logs in an introductory programming class using Python, and
17,698 unannotated chat data with code. Each question is accompanied with the
student's code, and the portion of the code relevant to answering the question.
We carefully design the annotation process to construct CS1QA, and analyze the
collected dataset in detail. The tasks for CS1QA are to predict the question
type, the relevant code snippet given the question and the code and retrieving
an answer from the annotated corpus. Results for the experiments on several
baseline models are reported and thoroughly analyzed. The tasks for CS1QA
challenge models to understand both the code and natural language. This unique
dataset can be used as a benchmark for source code comprehension and question
answering in the educational setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14502">
<div class="article-summary-box-inner">
<span><p>A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech-to-Speech Translation Through Unlabeled Text. (arXiv:2210.14514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14514">
<div class="article-summary-box-inner">
<span><p>Direct speech-to-speech translation (S2ST) is among the most challenging
problems in the translation paradigm due to the significant scarcity of S2ST
data. While effort has been made to increase the data size from unlabeled
speech by cascading pretrained speech recognition (ASR), machine translation
(MT) and text-to-speech (TTS) models; unlabeled text has remained relatively
under-utilized to improve S2ST. We propose an effective way to utilize the
massive existing unlabeled text from different languages to create a large
amount of S2ST data to improve S2ST performance by applying various acoustic
effects to the generated synthetic data. Empirically our method outperforms the
state of the art in Spanish-English translation by up to 2 BLEU. Significant
gains by the proposed method are demonstrated in extremely low-resource
settings for both Spanish-English and Russian-English translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification. (arXiv:2210.14523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14523">
<div class="article-summary-box-inner">
<span><p>Extreme multi-label text classification (XMTC) is the task of finding the
most relevant subset labels from an extremely large-scale label collection.
Recently, some deep learning models have achieved state-of-the-art results in
XMTC tasks. These models commonly predict scores for all labels by a fully
connected layer as the last layer of the model. However, such models can't
predict a relatively complete and variable-length label subset for each
document, because they select positive labels relevant to the document by a
fixed threshold or take top k labels in descending order of scores. A less
popular type of deep learning models called sequence-to-sequence (Seq2Seq)
focus on predicting variable-length positive labels in sequence style. However,
the labels in XMTC tasks are essentially an unordered set rather than an
ordered sequence, the default order of labels restrains Seq2Seq models in
training. To address this limitation in Seq2Seq, we propose an autoregressive
sequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates
predictions in student-forcing scheme and is trained by a loss function based
on bipartite matching which enables permutation-invariance. Meanwhile, we use
the optimal transport distance as a measurement to force the model to focus on
the closest labels in semantic label space. Experiments show that OTSeq2Set
outperforms other competitive baselines on 4 benchmark datasets. Especially, on
the Wikipedia dataset with 31k labels, it outperforms the state-of-the-art
Seq2Seq method by 16.34% in micro-F1 score. The code is available at
https://github.com/caojie54/OTSeq2Set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator. (arXiv:2210.14529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14529">
<div class="article-summary-box-inner">
<span><p>Task-Oriented Dialogue (TOD) systems are drawing more and more attention in
recent studies. Current methods focus on constructing pre-trained models or
fine-tuning strategies while the evaluation of TOD is limited by a policy
mismatch problem. That is, during evaluation, the user utterances are from the
annotated dataset while these utterances should interact with previous
responses which can have many alternatives besides annotated texts. Therefore,
in this work, we propose an interactive evaluation framework for TOD. We first
build a goal-oriented user simulator based on pre-trained models and then use
the user simulator to interact with the dialogue system to generate dialogues.
Besides, we introduce a sentence-level and a session-level score to measure the
sentence fluency and session coherence in the interactive evaluation.
Experimental results show that RL-based TOD systems trained by our proposed
user simulator can achieve nearly 98% inform and success rates in the
interactive evaluation of MultiWOZ dataset and the proposed scores measure the
response quality besides the inform and success rates. We are hoping that our
work will encourage simulator-based interactive evaluations in the TOD task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14531">
<div class="article-summary-box-inner">
<span><p>Instead of using a single ground truth for language processing tasks, several
recent studies have examined how to represent and predict the labels of the set
of annotators. However, often little or no information about annotators is
known, or the set of annotators is small. In this work, we examine a corpus of
social media posts about conflict from a set of 13k annotators and 210k
judgements of social norms. We provide a novel experimental setup that applies
personalization methods to the modeling of annotators and compare their
effectiveness for predicting the perception of social norms. We further provide
an analysis of performance across subsets of social situations that vary by the
closeness of the relationship between parties in conflict, and assess where
personalization helps the most.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look to the Right: Mitigating Relative Position Bias in Extractive Question Answering. (arXiv:2210.14541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14541">
<div class="article-summary-box-inner">
<span><p>Extractive question answering (QA) models tend to exploit spurious
correlations to make predictions when a training set has unintended biases.
This tendency results in models not being generalizable to examples where the
correlations do not hold. Determining the spurious correlations QA models can
exploit is crucial in building generalizable QA models in real-world
applications; moreover, a method needs to be developed that prevents these
models from learning the spurious correlations even when a training set is
biased. In this study, we discovered that the relative position of an answer,
which is defined as the relative distance from an answer span to the closest
question-context overlap word, can be exploited by QA models as superficial
cues for making predictions. Specifically, we find that when the relative
positions in a training set are biased, the performance on examples with
relative positions unseen during training is significantly degraded. To
mitigate the performance degradation for unseen relative positions, we propose
an ensemble-based debiasing method that does not require prior knowledge about
the distribution of relative positions. We demonstrate that the proposed method
mitigates the models' reliance on relative positions using the biased and full
SQuAD dataset. We hope that this study can help enhance the generalization
ability of QA models in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Bias Mitigation Procedure Based on the Stereotype Content Model. (arXiv:2210.14552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14552">
<div class="article-summary-box-inner">
<span><p>The Stereotype Content model (SCM) states that we tend to perceive minority
groups as cold, incompetent or both. In this paper we adapt existing work to
demonstrate that the Stereotype Content model holds for contextualised word
embeddings, then use these results to evaluate a fine-tuning process designed
to drive a language model away from stereotyped portrayals of minority groups.
We find the SCM terms are better able to capture bias than demographic agnostic
terms related to pleasantness. Further, we were able to reduce the presence of
stereotypes in the model through a simple fine-tuning procedure that required
minimal human and computer resources, without harming downstream performance.
We present this work as a prototype of a debiasing procedure that aims to
remove the need for a priori knowledge of the specifics of bias in the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis. (arXiv:2210.14556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14556">
<div class="article-summary-box-inner">
<span><p>Multimodal representation learning is a challenging task in which previous
work mostly focus on either uni-modality pre-training or cross-modality fusion.
In fact, we regard modeling multimodal representation as building a skyscraper,
where laying stable foundation and designing the main structure are equally
essential. The former is like encoding robust uni-modal representation while
the later is like integrating interactive information among different
modalities, both of which are critical to learning an effective multimodal
representation. Recently, contrastive learning has been successfully applied in
representation learning, which can be utilized as the pillar of the skyscraper
and benefit the model to extract the most important features contained in the
multimodal data. In this paper, we propose a novel framework named MultiModal
Contrastive Learning (MMCL) for multimodal representation to capture intra- and
inter-modality dynamics simultaneously. Specifically, we devise uni-modal
contrastive coding with an efficient uni-modal feature augmentation strategy to
filter inherent noise contained in acoustic and visual modality and acquire
more robust uni-modality representations. Besides, a pseudo siamese network is
presented to predict representation across different modalities, which
successfully captures cross-modal dynamics. Moreover, we design two contrastive
learning tasks, instance- and sentiment-based contrastive learning, to promote
the process of prediction and learn more interactive information related to
sentiment. Extensive experiments conducted on two public datasets demonstrate
that our method surpasses the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Sentence Sampling by Virtual Adversarial Perturbation. (arXiv:2210.14576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14576">
<div class="article-summary-box-inner">
<span><p>Active learning for sentence understanding attempts to reduce the annotation
cost by identifying the most informative examples. Common methods for active
learning use either uncertainty or diversity sampling in the pool-based
scenario. In this work, to incorporate both predictive uncertainty and sample
diversity, we propose Virtual Adversarial Perturbation for Active Learning
(VAPAL) , an uncertainty-diversity combination framework, using virtual
adversarial perturbation (Miyato et al., 2019) as model uncertainty
representation. VAPAL consistently performs equally well or even better than
the strong baselines on four sentence understanding datasets: AGNEWS, IMDB,
PUBMED, and SST-2, offering a potential option for active learning on sentence
understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws Beyond Backpropagation. (arXiv:2210.14593v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14593">
<div class="article-summary-box-inner">
<span><p>Alternatives to backpropagation have long been studied to better understand
how biological brains may learn. Recently, they have also garnered interest as
a way to train neural networks more efficiently. By relaxing constraints
inherent to backpropagation (e.g., symmetric feedforward and feedback weights,
sequential updates), these methods enable promising prospects, such as local
learning. However, the tradeoffs between different methods in terms of final
task performance, convergence speed, and ultimately compute and data
requirements are rarely outlined. In this work, we use scaling laws to study
the ability of Direct Feedback Alignment~(DFA) to train causal decoder-only
Transformers efficiently. Scaling laws provide an overview of the tradeoffs
implied by a modeling decision, up to extrapolating how it might transfer to
increasingly large models. We find that DFA fails to offer more efficient
scaling than backpropagation: there is never a regime for which the degradation
in loss incurred by using DFA is worth the potential reduction in compute
budget. Our finding comes at variance with previous beliefs in the alternative
training methods community, and highlights the need for holistic empirical
approaches to better understand modeling decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Knowledge Graphs for Automating AI of Digital Twins. (arXiv:2210.14596v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14596">
<div class="article-summary-box-inner">
<span><p>Digital Twins are digital representations of systems in the Internet of
Things (IoT) that are often based on AI models that are trained on data from
those systems. Semantic models are used increasingly to link these datasets
from different stages of the IoT systems life-cycle together and to
automatically configure the AI modelling pipelines. This combination of
semantic models with AI pipelines running on external datasets raises unique
challenges particular if rolled out at scale. Within this paper we will discuss
the unique requirements of applying semantic graphs to automate Digital Twins
in different practical use cases. We will introduce the benchmark dataset DTBM
that reflects these characteristics and look into the scaling challenges of
different knowledge graph technologies. Based on these insights we will propose
a reference architecture that is in-use in multiple products in IBM and derive
lessons learned for scaling knowledge graphs for configuring AI models for
Digital Twins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Multi-Task Learning for Abstractive Text Summarization. (arXiv:2210.14606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14606">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of multi-task learning and pre-finetuning for
natural language understanding, few works have studied the effects of task
families on abstractive text summarization. Task families are a form of task
grouping during the pre-finetuning stage to learn common skills, such as
reading comprehension. To close this gap, we analyze the influence of
multi-task learning strategies using task families for the English abstractive
text summarization task. We group tasks into one of three strategies, i.e.,
sequential, simultaneous, and continual multi-task learning, and evaluate
trained models through two downstream tasks. We find that certain combinations
of task families (e.g., advanced reading comprehension and natural language
inference) positively impact downstream performance. Further, we find that
choice and combinations of task families influence downstream performance more
than the training scheme, supporting the use of task families for abstractive
text summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A practical method for occupational skills detection in Vietnamese job listings. (arXiv:2210.14607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14607">
<div class="article-summary-box-inner">
<span><p>Vietnamese labor market has been under an imbalanced development. The number
of university graduates is growing, but so is the unemployment rate. This
situation is often caused by the lack of accurate and timely labor market
information, which leads to skill miss-matches between worker supply and the
actual market demands. To build a data monitoring and analytic platform for the
labor market, one of the main challenges is to be able to automatically detect
occupational skills from labor-related data, such as resumes and job listings.
Traditional approaches rely on existing taxonomy and/or large annotated data to
build Named Entity Recognition (NER) models. They are expensive and require
huge manual efforts. In this paper, we propose a practical methodology for
skill detection in Vietnamese job listings. Rather than viewing the task as a
NER task, we consider the task as a ranking problem. We propose a pipeline in
which phrases are first extracted and ranked in semantic similarity with the
phrases' contexts. Then we employ a final classification to detect skill
phrases. We collected three datasets and conducted extensive experiments. The
results demonstrated that our methodology achieved better performance than a
NER model in scarce datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective. (arXiv:2210.14650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14650">
<div class="article-summary-box-inner">
<span><p>Teaching neural models to generate narrative coherent texts is a critical
problem. Recent pre-trained language models have achieved promising results,
but there is still a gap between human written texts and machine-generated
outputs. In this work, we propose a novel multi-task training strategy for
coherent text generation grounded on the cognitive theory of writing, which
empowers the model to learn essential subskills needed for writing including
planning and reviewing besides end-to-end generation. We extensively evaluate
our model on three open-ended generation tasks including story generation, news
article writing and argument generation. Experiments show that our model
achieves better results on both few-shot and fully-supervised settings than
strong baselines, and human evaluations confirm that our model can generate
more coherent outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bilingual Parallel Corpus with Discourse Annotations. (arXiv:2210.14667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14667">
<div class="article-summary-box-inner">
<span><p>Machine translation (MT) has almost achieved human parity at sentence-level
translation. In response, the MT community has, in part, shifted its focus to
document-level translation. However, the development of document-level MT
systems is hampered by the lack of parallel document corpora. This paper
describes BWB, a large parallel corpus first introduced in Jiang et al. (2022),
along with an annotated test set. The BWB corpus consists of Chinese novels
translated by experts into English, and the annotated test set is designed to
probe the ability of machine translation systems to model various discourse
phenomena. Our resource is freely available, and we hope it will serve as a
guide and inspiration for more work in document-level machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Role of Centering Theory in the Context of Neural Coreference Resolution Systems. (arXiv:2210.14678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14678">
<div class="article-summary-box-inner">
<span><p>Centering theory (CT; Grosz et al., 1995) provides a linguistic analysis of
the structure of discourse. According to the theory, local coherence of
discourse arises from the manner and extent to which successive utterances make
reference to the same entities. In this paper, we investigate the connection
between centering theory and modern coreference resolution systems. We provide
an operationalization of centering and systematically investigate if neural
coreference resolvers adhere to the rules of centering theory by defining
various discourse metrics and developing a search-based methodology. Our
information-theoretic analysis reveals a positive dependence between
coreference and centering; but also shows that high-quality neural coreference
resolvers may not benefit much from explicitly modeling centering ideas. Our
analysis further shows that contextualized embeddings contain much of the
coherence information, which helps explain why CT can only provide little gains
to modern neural coreference resolvers which make use of pretrained
representations. Finally, we discuss factors that contribute to coreference
which are not modeled by CT such as world knowledge and recency bias. We
formulate a version of CT that also models recency and show that it captures
coreference information better compared to vanilla CT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pronunciation Generation for Foreign Language Words in Intra-Sentential Code-Switching Speech Recognition. (arXiv:2210.14691v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14691">
<div class="article-summary-box-inner">
<span><p>Code-Switching refers to the phenomenon of switching languages within a
sentence or discourse. However, limited code-switching , different language
phoneme-sets and high rebuilding costs throw a challenge to make the
specialized acoustic model for code-switching speech recognition. In this
paper, we make use of limited code-switching data as driving materials and
explore a shortcut to quickly develop intra-sentential code-switching
recognition skill on the commissioned native language acoustic model, where we
propose a data-driven method to make the seed lexicon which is used to train
grapheme-to-phoneme model to predict mapping pronunciations for foreign
language word in code-switching sentences. The core work of the data-driven
technology in this paper consists of a phonetic decoding method and different
selection methods. And for imbalanced word-level driving materials problem, we
have an internal assistance inspiration that learning the good pronunciation
rules in the words that possess sufficient materials using the
grapheme-to-phoneme model to help the scarce. Our experiments show that the
Mixed Error Rate in intra-sentential Chinese-English code-switching recognition
reduced from 29.15\%, acquired on the pure Chinese recognizer, to 12.13\% by
adding foreign language words' pronunciation through our data-driven approach,
and finally get the best result 11.14\% with the combination of different
selection methods and internal assistance tactic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Structured Prediction with Language Models. (arXiv:2210.14698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14698">
<div class="article-summary-box-inner">
<span><p>Recent years have seen a paradigm shift in NLP towards using pretrained
language models ({PLM}) for a wide range of tasks.
</p>
<p>However, there are many difficult design decisions to represent structures
(e.g. tagged text, coreference chains) in a way such that they can be captured
by PLMs.
</p>
<p>Prior work on structured prediction with PLMs typically flattens the
structured output into a sequence, which limits the quality of structural
information being learned and leads to inferior performance compared to classic
discriminative models.
</p>
<p>In this work, we describe an approach to model structures as sequences of
actions in an autoregressive manner with PLMs, allowing in-structure
dependencies to be learned without any loss.
</p>
<p>Our approach achieves the new state-of-the-art on all the structured
prediction tasks we looked at, namely, named entity recognition, end-to-end
relation extraction, and coreference resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?. (arXiv:2210.14699v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14699">
<div class="article-summary-box-inner">
<span><p>Language models are promising solutions for tackling increasing complex
problems. In software engineering, they recently attracted attention in code
assistants, with programs automatically written in a given programming language
from a programming task description in natural language. They have the
potential to save time and effort when writing code. However, these systems are
currently poorly understood, preventing them from being used optimally. In this
paper, we investigate the various input parameters of two language models, and
conduct a study to understand if variations of these input parameters (e.g.
programming task description and the surrounding context, creativity of the
language model, number of generated solutions) can have a significant impact on
the quality of the generated programs. We design specific operators for varying
input parameters and apply them over two code assistants (Copilot and Codex)
and two benchmarks representing algorithmic problems (HumanEval and LeetCode).
Our results showed that varying the input parameters can significantly improve
the performance of language models. However, there is a tight dependency when
varying the temperature, the prompt and the number of generated solutions,
making potentially hard for developers to properly control the parameters to
obtain an optimal result. This work opens opportunities to propose (automated)
strategies for improving performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks. (arXiv:2210.14712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14712">
<div class="article-summary-box-inner">
<span><p>We present Bloom Library, a linguistically diverse set of multimodal and
multilingual datasets for language modeling, image captioning, visual
storytelling, and speech synthesis/recognition. These datasets represent either
the most, or among the most, multilingual datasets for each of the included
downstream tasks. In total, the initial release of the Bloom Library datasets
covers 363 languages across 32 language families. We train downstream task
models for various languages represented in the data, showing the viability of
the data for future work in low-resource, multimodal NLP and establishing the
first known baselines for these downstream tasks in certain languages (e.g.,
Bisu [bzi], with an estimated population of 700 users). Some of these
first-of-their-kind baselines are comparable to state-of-the-art performance
for higher-resourced languages. The Bloom Library datasets are released under
Creative Commons licenses on the Hugging Face datasets hub to catalyze more
linguistically diverse research in the included downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Imbalanced Text Classification with Dynamic Curriculum Learning. (arXiv:2210.14724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14724">
<div class="article-summary-box-inner">
<span><p>Recent advances in pre-trained language models have improved the performance
for text classification tasks. However, little attention is paid to the
priority scheduling strategy on the samples during training. Humans acquire
knowledge gradually from easy to complex concepts, and the difficulty of the
same material can also vary significantly in different learning stages.
Inspired by this insights, we proposed a novel self-paced dynamic curriculum
learning (SPDCL) method for imbalanced text classification, which evaluates the
sample difficulty by both linguistic character and model capacity. Meanwhile,
rather than using static curriculum learning as in the existing research, our
SPDCL can reorder and resample training data by difficulty criterion with an
adaptive from easy to hard pace. The extensive experiments on several
classification tasks show the effectiveness of SPDCL strategy, especially for
the imbalanced dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition. (arXiv:2210.14725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14725">
<div class="article-summary-box-inner">
<span><p>The recent emergence of joint CTC-Attention model shows significant
improvement in automatic speech recognition (ASR). The improvement largely lies
in the modeling of linguistic information by decoder. The decoder
joint-optimized with an acoustic encoder renders the language model from
ground-truth sequences in an auto-regressive manner during training. However,
the training corpus of the decoder is limited to the speech transcriptions,
which is far less than the corpus needed to train an acceptable language model.
This leads to poor robustness of decoder. To alleviate this problem, we propose
linguistic-enhanced transformer, which introduces refined CTC information to
decoder during training process, so that the decoder can be more robust. Our
experiments on AISHELL-1 speech corpus show that the character error rate (CER)
is relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR
model, decoder is more sensitive to linguistic information than acoustic
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monotonic segmental attention for automatic speech recognition. (arXiv:2210.14742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14742">
<div class="article-summary-box-inner">
<span><p>We introduce a novel segmental-attention model for automatic speech
recognition. We restrict the decoder attention to segments to avoid quadratic
runtime of global attention, better generalize to long sequences, and
eventually enable streaming. We directly compare global-attention and different
segmental-attention modeling variants. We develop and compare two separate
time-synchronous decoders, one specifically taking the segmental nature into
account, yielding further improvements. Using time-synchronous decoding for
segmental models is novel and a step towards streaming applications. Our
experiments show the importance of a length model to predict the segment
boundaries. The final best segmental-attention model using segmental decoding
performs better than global-attention, in contrast to other monotonic attention
approaches in the literature. Further, we observe that the segmental model
generalizes much better to long sequences of up to several minutes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Task: Deriving Semantic Class Targets for the Physical Sciences. (arXiv:2210.14760v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14760">
<div class="article-summary-box-inner">
<span><p>We define deriving semantic class targets as a novel multi-modal task. By
doing so, we aim to improve classification schemes in the physical sciences
which can be severely abstracted and obfuscating. We address this task for
upcoming radio astronomy surveys and present the derived semantic radio galaxy
morphology class targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProSiT! Latent Variable Discovery with PROgressive SImilarity Thresholds. (arXiv:2210.14763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14763">
<div class="article-summary-box-inner">
<span><p>The most common ways to explore latent document dimensions are topic models
and clustering methods. However, topic models have several drawbacks: e.g.,
they require us to choose the number of latent dimensions a priori, and the
results are stochastic. Most clustering methods have the same issues and lack
flexibility in various ways, such as not accounting for the influence of
different topics on single documents, forcing word-descriptors to belong to a
single topic (hard-clustering) or necessarily relying on word representations.
We propose PROgressive SImilarity Thresholds - ProSiT, a deterministic and
interpretable method, agnostic to the input format, that finds the optimal
number of latent dimensions and only has two hyper-parameters, which can be set
efficiently via grid search. We compare this method with a wide range of topic
models and clustering methods on four benchmark data sets. In most setting,
ProSiT matches or outperforms the other methods in terms six metrics of topic
coherence and distinctiveness, producing replicable, deterministic results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models. (arXiv:2210.14803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14803">
<div class="article-summary-box-inner">
<span><p>Masked language models like BERT can perform text classification in a
zero-shot fashion by reformulating downstream tasks as text infilling. However,
this approach is highly sensitive to the template used to prompt the model, yet
practitioners are blind when designing them in strict zero-shot settings. In
this paper, we propose an alternative mining-based approach for zero-shot
learning. Instead of prompting language models, we use regular expressions to
mine labeled examples from unlabeled corpora, which can optionally be filtered
through prompting, and used to finetune a pretrained model. Our method is more
flexible and interpretable than prompting, and outperforms it on a wide range
of tasks when using comparable templates. Our results suggest that the success
of prompting can partly be explained by the model being exposed to similar
examples during pretraining, which can be directly retrieved through regular
expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples. (arXiv:2210.14814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14814">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) is critical for complex decision-making in
biomedical domain. One key question, for example, is whether a given biomedical
mechanism is supported by experimental evidence. This can be seen as an NLI
problem but there are no directly usable datasets to address this. The main
challenge is that manually creating informative negative examples for this task
is difficult and expensive. We introduce a novel semi-supervised procedure that
bootstraps an NLI dataset from existing biomedical dataset that pairs
mechanisms with experimental evidence in abstracts. We generate a range of
negative examples using nine strategies that manipulate the structure of the
underlying mechanisms both with rules, e.g., flip the roles of the entities in
the interaction, and, more importantly, as perturbations via logical
constraints in a neuro-logical decoding system. We use this procedure to create
a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark
two state-of-the-art biomedical classifiers. The best result we obtain is
around mid 70s in F1, suggesting the difficulty of the task. Critically, the
performance on the different classes of negative examples varies widely, from
97% F1 on the simple role change negative examples, to barely better than
chance on the negative examples generated using neuro-logic decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Curious Case of $\ell_2$ norm of Sense Embeddings. (arXiv:2210.14815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14815">
<div class="article-summary-box-inner">
<span><p>We show that the $\ell_2$ norm of a static sense embedding encodes
information related to the frequency of that sense in the training corpus used
to learn the sense embeddings. This finding can be seen as an extension of a
previously known relationship for word embeddings to sense embeddings. Our
experimental results show that, in spite of its simplicity, the $\ell_2$ norm
of sense embeddings is a surprisingly effective feature for several word sense
related tasks such as (a) most frequent sense prediction, (b) Word-in-Context
(WiC), and (c) Word Sense Disambiguation (WSD). In particular, by simply
including the $\ell_2$ norm of a sense embedding as a feature in a classifier,
we show that we can improve WiC and WSD methods that use static sense
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProVe: A Pipeline for Automated Provenance Verification of Knowledge Graphs against Textual Sources. (arXiv:2210.14846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14846">
<div class="article-summary-box-inner">
<span><p>Knowledge Graphs are repositories of information that gather data from a
multitude of domains and sources in the form of semantic triples, serving as a
source of structured data for various crucial applications in the modern web
landscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve
as secondary sources of information and depend on well-documented and
verifiable provenance to ensure their trustworthiness and usability. However,
their ability to systematically assess and assure the quality of this
provenance, most crucially whether it properly supports the graph's
information, relies mainly on manual processes that do not scale with size.
ProVe aims at remedying this, consisting of a pipelined approach that
automatically verifies whether a Knowledge Graph triple is supported by text
extracted from its documented provenance. ProVe is intended to assist
information curators and consists of four main steps involving rule-based
methods and machine learning models: text extraction, triple verbalisation,
sentence selection, and claim verification. ProVe is evaluated on a Wikidata
dataset, achieving promising results overall and excellent performance on the
binary classification task of detecting support from provenance, with 87.5%
accuracy and 82.9% F1-macro on text-rich sources. The evaluation data and
scripts used in this paper are available on GitHub and Figshare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality Detection using Multiple Annotation Decision. (arXiv:2210.14852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14852">
<div class="article-summary-box-inner">
<span><p>The paper describes the work that has been submitted to the 5th workshop on
Challenges and Applications of Automated Extraction of socio-political events
from text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3
that aims to detect causality in protest news corpus. The authors used
different large language models with customized cross-entropy loss functions
that exploit annotation information. The experiments showed that
bert-based-uncased with refined cross-entropy outperformed the others,
achieving a F1 score of 0.8501 on the Causal News Corpus dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Semantic Parsing: From Images to Abstract Meaning Representation. (arXiv:2210.14862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14862">
<div class="article-summary-box-inner">
<span><p>The success of scene graphs for visual scene understanding has brought
attention to the benefits of abstracting a visual input (e.g., image) into a
structured representation, where entities (people and objects) are nodes
connected by edges specifying their relations. Building these representations,
however, requires expensive manual annotation in the form of images paired with
their scene graphs or frames. These formalisms remain limited in the nature of
entities and relations they can capture. In this paper, we propose to leverage
a widely-used meaning representation in the field of natural language
processing, the Abstract Meaning Representation (AMR), to address these
shortcomings. Compared to scene graphs, which largely emphasize spatial
relationships, our visual AMR graphs are more linguistically informed, with a
focus on higher-level semantic concepts extrapolated from visual input.
Moreover, they allow us to generate meta-AMR graphs to unify information
contained in multiple image descriptions under one representation. Through
extensive experimentation and analysis, we demonstrate that we can re-purpose
an existing text-to-AMR parser to parse images into AMRs. Our findings point to
important future research directions for improved scene understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning. (arXiv:2210.14867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14867">
<div class="article-summary-box-inner">
<span><p>In this paper, we elaborate upon recipes for building multilingual
representation models that are not only competitive with existing
state-of-the-art models but are also more parameter efficient, thereby
promoting better adoption in resource-constrained scenarios and practical
applications. We show that going beyond English-centric bitexts, coupled with a
novel sampling strategy aimed at reducing under-utilization of training data,
substantially boosts performance across model sizes for both Electra and MLM
pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language
ENcodings using Transformers which not only achieves state-of-the-art
performance over 5 cross-lingual tasks within all model size bands, is also
competitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and
exhibits competitive performance with mT5 XXL while being 5x and 6x smaller
respectively. We then show that our proposed method helps ameliorate the curse
of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and
98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same
size band. We then analyze our models performance on extremely low resource
languages and posit that scaling alone may not be sufficient for improving the
performance in this scenario
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14868">
<div class="article-summary-box-inner">
<span><p>We present MBXP, an execution-based code completion benchmark in 10+
programming languages. This collection of datasets is generated by our
conversion framework that translates prompts and test cases from the original
MBPP dataset to the corresponding data in a target language. Based on this
benchmark, we are able to evaluate code generation models in a multi-lingual
fashion, and in particular discover generalization ability of language models
on out-of-domain languages, advantages of large multi-lingual models over
mono-lingual, benefits of few-shot prompting, and zero-shot translation
abilities. In addition, we use our code generation model to perform large-scale
bootstrapping to obtain synthetic canonical solutions in several languages.
These solutions can be used for other code-related evaluations such as
insertion-based, summarization, or code translation tasks where we demonstrate
results and release as part of our benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation. (arXiv:2009.09435v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09435">
<div class="article-summary-box-inner">
<span><p>Bolukbasi et al. (2016) presents one of the first gender bias mitigation
techniques for word embeddings. Their method takes pre-trained word embeddings
as input and attempts to isolate a linear subspace that captures most of the
gender bias in the embeddings. As judged by an analogical evaluation task,
their method virtually eliminates gender bias in the embeddings. However, an
implicit and untested assumption of their method is that the bias sub-space is
actually linear. In this work, we generalize their method to a kernelized,
non-linear version. We take inspiration from kernel principal component
analysis and derive a non-linear bias isolation technique. We discuss and
overcome some of the practical drawbacks of our method for non-linear gender
bias mitigation in word embeddings and analyze empirically whether the bias
subspace is actually linear. Our analysis shows that gender bias is in fact
well captured by a linear subspace, justifying the assumption of Bolukbasi et
al. (2016).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation. (arXiv:2101.06561v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06561">
<div class="article-summary-box-inner">
<span><p>Leaderboards have eased model development for many NLP datasets by
standardizing their evaluation and delegating it to an independent external
repository. Their adoption, however, is so far limited to tasks that can be
reliably evaluated in an automatic manner. This work introduces GENIE, an
extensible human evaluation leaderboard, which brings the ease of leaderboards
to text generation tasks. GENIE automatically posts leaderboard submissions to
crowdsourcing platforms asking human annotators to evaluate them on various
axes (e.g., correctness, conciseness, fluency) and compares their answers to
various automatic metrics. We introduce several datasets in English to GENIE,
representing four core challenges in text generation: machine translation,
summarization, commonsense reasoning, and machine comprehension. We provide
formal granular evaluation metrics and identify areas for future research. We
make GENIE publicly available and hope that it will spur progress in language
generation models as well as their automatic and manual evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09611">
<div class="article-summary-box-inner">
<span><p>Dependency parsing is a crucial step towards deep language understanding and,
therefore, widely demanded by numerous Natural Language Processing
applications. In particular, left-to-right and top-down transition-based
algorithms that rely on Pointer Networks are among the most accurate approaches
for performing dependency parsing. Additionally, it has been observed for the
top-down algorithm that Pointer Networks' sequential decoding can be improved
by implementing a hierarchical variant, more adequate to model dependency
structures. Considering all this, we develop a bottom-up-oriented Hierarchical
Pointer Network for the left-to-right parser and propose two novel
transition-based alternatives: an approach that parses a sentence in
right-to-left order and a variant that does it from the outside in. We
empirically test the proposed neural architecture with the different algorithms
on a wide variety of languages, outperforming the original approach in
practically all of them and setting new state-of-the-art results on the English
and Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11483">
<div class="article-summary-box-inner">
<span><p>Recently, the development of pre-trained language models has brought natural
language processing (NLP) tasks to the new state-of-the-art. In this paper we
explore the efficiency of various pre-trained language models. We pre-train a
list of transformer-based models with the same amount of text and the same
training steps. The experimental results shows that the most improvement upon
the origin BERT is adding the RNN-layer to capture more contextual information
for short text understanding. But there are no remarkable improvement for short
text understanding for similar BERT structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics. (arXiv:2112.08321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08321">
<div class="article-summary-box-inner">
<span><p>Recent works that revealed the vulnerability of dialogue state tracking (DST)
models to distributional shifts have made holistic comparisons on robustness
and qualitative analyses increasingly important for understanding their
relative performance. We present our findings from standardized and
comprehensive DST diagnoses, which have previously been sparse and
uncoordinated, using our toolkit, CheckDST, a collection of robustness tests
and failure mode analytics. We discover that different classes of DST models
have clear strengths and weaknesses, where generation models are more promising
for handling language variety while span-based classification models are more
robust to unseen entities. Prompted by this discovery, we also compare
checkpoints from the same model and find that the standard practice of
selecting checkpoints using validation loss/accuracy is prone to overfitting
and each model class has distinct patterns of failure. Lastly, we demonstrate
how our diagnoses motivate a pre-finetuning procedure with non-dialogue data
that offers comprehensive improvements to generation models by alleviating the
impact of distributional shifts through transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Large Scale Language Modeling with Mixtures of Experts. (arXiv:2112.10684v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10684">
<div class="article-summary-box-inner">
<span><p>Mixture of Experts layers (MoEs) enable efficient scaling of language models
through conditional computation. This paper presents a detailed empirical study
of how autoregressive MoE language models scale in comparison with dense models
in a wide range of settings: in- and out-of-domain language modeling, zero- and
few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning,
we find MoEs to be substantially more compute efficient. At more modest
training budgets, MoEs can match the performance of dense models using $\sim$4
times less compute. This gap narrows at scale, but our largest MoE model (1.1T
parameters) consistently outperforms a compute-equivalent dense model (6.7B
parameters). Overall, this performance gap varies greatly across tasks and
domains, suggesting that MoE and dense models generalize differently in ways
that are worthy of future study. We make our code and models publicly available
for research use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search. (arXiv:2201.10866v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10866">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the CodeRetriever model, which learns the
function-level code semantic representations through large-scale code-text
contrastive pre-training. We adopt two contrastive learning schemes in
CodeRetriever: unimodal contrastive learning and bimodal contrastive learning.
For unimodal contrastive learning, we design an unsupervised learning approach
to build semantic-related code pairs based on the documentation and function
name. For bimodal contrastive learning, we leverage the documentation and
in-line comments of code to build code-text pairs. Both contrastive objectives
can fully leverage large-scale code corpus for pre-training. Extensive
experimental results show that CodeRetriever achieves new state-of-the-art with
significant improvement over existing code pre-trained models, on eleven
domain/language-specific code search tasks with six programming languages in
different code granularity (function-level, snippet-level and statement-level).
These results demonstrate the effectiveness and robustness of CodeRetriever.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation. (arXiv:2202.07654v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07654">
<div class="article-summary-box-inner">
<span><p>The predictions of question answering (QA)systems are typically evaluated
against manually annotated finite sets of one or more answers. This leads to a
coverage limitation that results in underestimating the true performance of
systems, and is typically addressed by extending over exact match (EM) with
pre-defined rules or with the token-level F1 measure. In this paper, we present
the first systematic conceptual and data-driven analysis to examine the
shortcomings of token-level equivalence measures.
</p>
<p>To this end, we define the asymmetric notion of answer equivalence (AE),
accepting answers that are equivalent to or improve over the reference, and
publish over 23k human judgments for candidates produced by multiple QA systems
on SQuAD. Through a careful analysis of this data, we reveal and quantify
several concrete limitations of the F1 measure, such as a false impression of
graduality, or missing dependence on the question.
</p>
<p>Since collecting AE annotations for each evaluated model is expensive, we
learn a BERT matching (BEM) measure to approximate this task. Being a simpler
task than QA, we find BEM to provide significantly better AE approximations
than F1, and to more accurately reflect the performance of systems.
</p>
<p>Finally, we demonstrate the practical utility of AE and BEM on the concrete
application of minimal accurate prediction sets, reducing the number of
required answers by up to x2.6.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets. (arXiv:2202.12459v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12459">
<div class="article-summary-box-inner">
<span><p>In hate speech detection, developing training and evaluation datasets across
various domains is the critical issue. Whereas, major approaches crawl social
media texts and hire crowd-workers to annotate the data. Following this
convention often restricts the scope of pejorative expressions to a single
domain lacking generalization. Sometimes domain overlap between training corpus
and evaluation set overestimate the prediction performance when pretraining
language models on low-data language. To alleviate these problems in Korean, we
propose APEACH that asks unspecified users to generate hate speech examples
followed by minimal post-labeling. We find that APEACH can collect useful
datasets that are less sensitive to the lexical overlaps between the
pretraining corpus and the evaluation set, thereby properly measuring the model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RED-ACE: Robust Error Detection for ASR using Confidence Embeddings. (arXiv:2203.07172v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07172">
<div class="article-summary-box-inner">
<span><p>ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Corpus Quality Really Matter for Low-Resource Languages?. (arXiv:2203.08111v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08111">
<div class="article-summary-box-inner">
<span><p>The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking representation learning in Basque
as a case study, we explore tailored crawling (manually identifying and
scraping websites with high-quality content) as an alternative to filtering
CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
portion of popular multilingual corpora like CC100 and mC4, yet it has a much
higher quality according to native annotators. For instance, 66% of documents
are rated as high-quality for EusCrawl, in contrast with &lt;33% for both mC4 and
CC100. Nevertheless, we obtain similar results on downstream NLU tasks
regardless of the corpus used for pre-training. Our work suggests that NLU
performance in low-resource languages is not primarily constrained by the
quality of the data, and other factors like corpus size and domain coverage can
play a more important role.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning for Few-Shot Dialogue State Tracking. (arXiv:2203.08568v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08568">
<div class="article-summary-box-inner">
<span><p>Collecting and annotating task-oriented dialogues is time-consuming and
costly; thus, zero and few shot learning could greatly benefit dialogue state
tracking (DST). In this work, we propose an in-context learning (ICL) framework
for zero-shot and few-shot learning DST, where a large pre-trained language
model (LM) takes a test instance and a few exemplars as input, and directly
decodes the dialogue state without any parameter updates. To better leverage a
tabular domain description in the LM prompt, we reformulate DST into a
text-to-SQL problem. We also propose a novel approach to retrieve annotated
dialogues as exemplars. Empirical results on MultiWOZ show that our method
IC-DST substantially outperforms previous fine-tuned state-of-the-art models in
few-shot settings. In addition, we test IC-DST in zero-shot settings, in which
the model only takes a fixed task instruction as input, finding that it
outperforms previous zero-shot methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these multimodal harms due to
lacking measurement robustness and feature degradation. To address these
challenges, we investigate bias measures and apply ranking metrics for
image-text representations. We then investigate debiasing methods and show that
prepending learned embeddings to text queries that are jointly trained with
adversarial debiasing and a contrastive loss reduces various bias measures with
minimal degradation to the image-text representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11424">
<div class="article-summary-box-inner">
<span><p>We propose an explainable approach for relation extraction that mitigates the
tension between generalization and explainability by jointly training for the
two goals. Our approach uses a multi-task learning architecture, which jointly
trains a classifier for relation extraction, and a sequence model that labels
words in the context of the relation that explain the decisions of the relation
classifier. We also convert the model outputs to rules to bring global
explanations to this approach. This sequence model is trained using a hybrid
strategy: supervised, when supervision from pre-existing patterns is available,
and semi-supervised otherwise. In the latter situation, we treat the sequence
model's labels as latent variables, and learn the best assignment that
maximizes the performance of the relation classifier. We evaluate the proposed
approach on the two datasets and show that the sequence model provides labels
that serve as accurate explanations for the relation classifier's decisions,
and, importantly, that the joint training generally improves the performance of
the relation classifier. We also evaluate the performance of the generated
rules and show that the new rules are great add-on to the manual rules and
bring the rule-based system much closer to the neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling and Presenting Harmful Text in NLP Research. (arXiv:2204.14256v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14256">
<div class="article-summary-box-inner">
<span><p>Text data can pose a risk of harm. However, the risks are not fully
understood, and how to handle, present, and discuss harmful text in a safe way
remains an unresolved issue in the NLP community. We provide an analytical
framework categorising harms on three axes: (1) the harm type (e.g.,
misinformation, hate speech or racial stereotypes); (2) whether a harm is
\textit{sought} as a feature of the research design if explicitly studying
harmful content (e.g., training a hate speech classifier), versus
\textit{unsought} if harmful content is encountered when working on unrelated
problems (e.g., language generation or part-of-speech tagging); and (3) who it
affects, from people (mis)represented in the data to those handling the data
and those publishing on the data. We provide advice for practitioners, with
concrete steps for mitigating harm in research and in publication. To assist
implementation we introduce \textsc{HarmCheck} -- a documentation standard for
handling and presenting harmful text in research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. (arXiv:2205.03720v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03720">
<div class="article-summary-box-inner">
<span><p>The massive amount of trainable parameters in the pre-trained language models
(PLMs) makes them hard to be deployed to multiple downstream tasks. To address
this issue, parameter-efficient transfer learning methods have been proposed to
tune only a few parameters during fine-tuning while freezing the rest. This
paper looks at existing methods along this line through the \textit{kernel
lens}. Motivated by the connection between self-attention in transformer-based
PLMs and kernel learning, we propose \textit{kernel-wise adapters}, namely
\textit{Kernel-mix}, that utilize the kernel structure in self-attention to
guide the assignment of the tunable parameters. These adapters use guidelines
found in classical kernel learning and enable separate parameter tuning for
each attention head. Our empirical results, over a diverse set of natural
language generation and understanding tasks, show that our proposed adapters
can attain or improve the strong performance of existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankGen: Improving Text Generation with Large Ranking Models. (arXiv:2205.09726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09726">
<div class="article-summary-box-inner">
<span><p>Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter encoder
model for English that scores model generations given a prefix. RankGen can be
flexibly incorporated as a scoring function in beam search and used to decode
from any pretrained language model. We train RankGen using large-scale
contrastive learning to map a prefix close to the ground-truth sequence that
follows it and far away from two types of negatives: (1) random sequences from
the same document as the prefix, and (2) sequences generated from a large
language model conditioned on the prefix. Experiments across four different
language models (345M-11B parameters) and two domains show that RankGen
significantly outperforms decoding algorithms like nucleus, top-k, and typical
sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human
evaluations with English writers (74.5% human preference over nucleus
sampling). Analysis reveals that RankGen outputs are more relevant to the
prefix and improve continuity and coherence compared to baselines. We release
our model checkpoints, code, and human preference data with explanations to
facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11211">
<div class="article-summary-box-inner">
<span><p>End-to-End Speech Translation (E2E-ST) has received increasing attention due
to the potential of its less error propagation, lower latency, and fewer
parameters. However, the effectiveness of neural-based approaches to this task
is severely limited by the available training corpus, especially for domain
adaptation where in-domain triplet training data is scarce or nonexistent. In
this paper, we propose a novel non-parametric method that leverages
domain-specific text translation corpus to achieve domain adaptation for the
E2E-ST system. To this end, we first incorporate an additional encoder into the
pre-trained E2E-ST model to realize text translation modelling, and then unify
the decoder's output representation for text and speech translation tasks by
reducing the correspondent representation mismatch in available triplet
training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier
is introduced to produce the final translation distribution using the external
datastore built by the domain-specific text translation corpus, while the
universal output representation is adopted to perform a similarity search.
Experiments on the Europarl-ST benchmark demonstrate that when in-domain text
translation data is involved only, our proposed approach significantly improves
baseline by 12.82 BLEU on average in all translation directions, even
outperforming the strong in-domain fine-tuning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Bidirectionality in Language Model Pre-Training. (arXiv:2205.11726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11726">
<div class="article-summary-box-inner">
<span><p>Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start. (arXiv:2205.12209v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12209">
<div class="article-summary-box-inner">
<span><p>We present EdiT5 - a novel semi-autoregressive text-editing model designed to
combine the strengths of non-autoregressive text-editing and autoregressive
decoding. EdiT5 is faster during inference than conventional
sequence-to-sequence (seq2seq) models, while being capable of modelling
flexible input-output transformations.
</p>
<p>This is achieved by decomposing the generation process into three sub-tasks:
(1) tagging to decide on the subset of input tokens to be preserved in the
output, (2) re-ordering to define their order in the output text, and (3)
insertion to infill the missing tokens that are not present in the input. The
tagging and re-ordering steps, which are responsible for generating the largest
portion of the output, are non-autoregressive, while the insertion step uses an
autoregressive decoder.
</p>
<p>Depending on the task, EdiT5 on average requires significantly fewer
autoregressive steps, demonstrating speedups of up to 25x when compared to
seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5
checkpoint yielding comparable performance to T5 in high-resource settings when
evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,
and Decontextualization while clearly outperforming T5 in low-resource
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents. (arXiv:2205.12486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12486">
<div class="article-summary-box-inner">
<span><p>We argue that disentangling content selection from the budget used to cover
salient content improves the performance and applicability of abstractive
summarizers. Our method, FactorSum, does this disentanglement by factorizing
summarization into two steps through an energy function: (1) generation of
abstractive summary views; (2) combination of these views into a final summary,
following a budget and content guidance. This guidance may come from different
sources, including from an advisor model such as BART or BigBird, or in oracle
mode -- from the reference. This factorization achieves significantly higher
ROUGE scores on multiple benchmarks for long document summarization, namely
PubMed, arXiv, and GovReport. Most notably, our model is effective for domain
adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1
score on arXiv, which indicates a strong performance due to more flexible
budget adaptation and content selection less dependent on domain-specific
textual structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing. (arXiv:2205.12640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12640">
<div class="article-summary-box-inner">
<span><p>Entity typing aims at predicting one or more words that describe the type(s)
of a specific mention in a sentence. Due to shortcuts from surface patterns to
annotated entity labels and biased training, existing entity typing models are
subject to the problem of spurious correlations. To comprehensively investigate
the faithfulness and reliability of entity typing methods, we first
systematically define distinct kinds of model biases that are reflected mainly
from spurious correlations. Particularly, we identify six types of existing
model biases, including mention-context bias, lexical overlapping bias, named
entity bias, pronoun bias, dependency bias, and overgeneralization bias. To
mitigate model biases, we then introduce a counterfactual data augmentation
method. By augmenting the original training set with their debiased
counterparts, models are forced to fully comprehend sentences and discover the
fundamental cues for entity typing, rather than relying on spurious
correlations for shortcuts. Experimental results on the UFET dataset show our
counterfactual data augmentation approach helps improve generalization of
different entity typing models with consistently better performance on both the
original and debiased test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning. (arXiv:2205.12673v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12673">
<div class="article-summary-box-inner">
<span><p>Instruction tuning is an emergent paradigm in NLP wherein natural language
instructions are leveraged with language models to induce zero-shot performance
on unseen tasks. Instructions have been shown to enable good performance on
unseen tasks and datasets in both large and small language models. Dialogue is
an especially interesting area to explore instruction tuning because dialogue
systems perform multiple kinds of tasks related to language (e.g., natural
language understanding and generation, domain-specific interaction), yet
instruction tuning has not been systematically explored for dialogue-related
tasks. We introduce InstructDial, an instruction tuning framework for dialogue,
which consists of a repository of 48 diverse dialogue tasks in a unified
text-to-text format created from 59 openly available dialogue datasets. Next,
we explore cross-task generalization ability on models tuned on InstructDial
across diverse dialogue tasks. Our analysis reveals that InstructDial enables
good zero-shot performance on unseen datasets and tasks such as dialogue
evaluation and intent detection, and even better performance in a few-shot
setting. To ensure that models adhere to instructions, we introduce novel
meta-tasks. We establish benchmark zero-shot and few-shot performance of models
trained using the proposed framework on multiple dialogue tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation. (arXiv:2205.12697v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12697">
<div class="article-summary-box-inner">
<span><p>Logical table-to-text generation is a task that involves generating logically
faithful sentences from tables, which requires models to derive logical level
facts from table records via logical inference. It raises a new challenge on
the logical-level content planning of table-to-text models. However, directly
learning the logical inference knowledge from table-text pairs is very
difficult for neural models because of the ambiguity of natural language and
the scarcity of parallel data. Hence even large-scale pre-trained language
models present low logical fidelity on logical table-to-text. In this work, we
propose a PLOG (Pretrained Logical Form Generator) framework to improve the
generation fidelity. Specifically, PLOG is first pretrained on a
table-to-logic-form generation (table-to-logic) task, then finetuned on
downstream table-to-text tasks. The formal definition of logical forms enables
us to collect large amount of accurate logical forms from tables without human
annotation. In addition, PLOG can learn logical inference from table-logic
pairs much more definitely than from table-text pairs. To evaluate our model,
we further collect a controlled logical table-to-text dataset CONTLOG based on
an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms
strong baselines by a large margin on the logical fidelity, demonstrating the
effectiveness of table-to-logic pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Abilities of Large Language Models. (arXiv:2206.07682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07682">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has been shown to predictably improve performance
and sample efficiency on a wide range of downstream tasks. This paper instead
discusses an unpredictable phenomenon that we refer to as emergent abilities of
large language models. We consider an ability to be emergent if it is not
present in smaller models but is present in larger models. Thus, emergent
abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence implies that additional scaling
could further expand the range of capabilities of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Benefits of Free-Form Rationales. (arXiv:2206.11083v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11083">
<div class="article-summary-box-inner">
<span><p>Free-form rationales aim to aid model interpretability by supplying the
background knowledge that can help understand model decisions. Crowdsourced
rationales are provided for commonsense QA instances in popular datasets such
as CoS-E and ECQA, but their utility remains under-investigated. We present
human studies which show that ECQA rationales indeed provide additional
background information to understand a decision, while over 88% of CoS-E
rationales do not. Inspired by this finding, we ask: can the additional context
provided by free-form rationales benefit models, similar to human users? We
investigate the utility of rationales as an additional source of supervision,
by varying the quantity and quality of rationales during training. After
controlling for instances where rationales leak the correct answer while not
providing additional background knowledge, we find that incorporating only 5%
of rationales during training can boost model performance by 47.22% for CoS-E
and 57.14% for ECQA during inference. Moreover, we also show that rationale
quality matters: compared to crowdsourced rationales, T5-generated rationales
provide not only weaker supervision to models, but are also not helpful for
humans in aiding model interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11212">
<div class="article-summary-box-inner">
<span><p>Many past works aim to improve visual reasoning in models by supervising
feature importance (estimated by model explanation techniques) with human
annotations such as highlights of important image regions. However, recent work
has shown that performance gains from feature importance (FI) supervision for
Visual Question Answering (VQA) tasks persist even with random supervision,
suggesting that these methods do not meaningfully align model FI with human FI.
In this paper, we show that model FI supervision can meaningfully improve VQA
model accuracy as well as performance on several Right-for-the-Right-Reason
(RRR) metrics by optimizing for four key model objectives: (1) accurate
predictions given limited but sufficient information (Sufficiency); (2)
max-entropy predictions given no important information (Uncertainty); (3)
invariance of predictions to changes in unimportant features (Invariance); and
(4) alignment between model FI explanations and human FI explanations
(Plausibility). Our best performing method, Visual Feature Importance
Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in
terms of both in-distribution and out-of-distribution accuracy. While past work
suggests that the mechanism for improved accuracy is through improved
explanation plausibility, we show that this relationship depends crucially on
explanation faithfulness (whether explanations truly represent the model's
internal reasoning). Predictions are more accurate when explanations are
plausible and faithful, and not when they are plausible but not faithful.
Lastly, we show that, surprisingly, RRR metrics are not predictive of
out-of-distribution model accuracy when controlling for a model's
in-distribution accuracy, which calls into question the value of these metrics
for evaluating model reasoning. All supporting code is available at
https://github.com/zfying/visfis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Training a Graph Recurrent Network for Language Representation. (arXiv:2209.03834v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03834">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models have gained much advance in recent
years, becoming one of the most important backbones in natural language
processing. Recent work shows that the attention mechanism inside Transformer
may not be necessary, both convolutional neural networks and multi-layer
perceptron based models have also been investigated as Transformer
alternatives. In this paper, we consider a graph recurrent network for language
model pre-training, which builds a graph structure for each sequence with local
token-level communications, together with a sentence-level representation
decoupled from other tokens. The original model performs well in
domain-specific text classification under supervised training, however, its
potential in learning transfer knowledge by self-supervised way has not been
fully exploited. We fill this gap by optimizing the architecture and verifying
its effectiveness in more general language understanding tasks, for both
English and Chinese languages. As for model efficiency, instead of the
quadratic complexity in Transformer-based models, our model has linear
complexity and performs more efficiently during inference. Moreover, we find
that our model can generate more diverse outputs with less contextualized
feature redundancy than existing attention-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexER: Flexible Entity Resolution for Multiple Intents. (arXiv:2209.07569v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07569">
<div class="article-summary-box-inner">
<span><p>Entity resolution, a longstanding problem of data cleaning and integration,
aims at identifying data records that represent the same real-world entity.
Existing approaches treat entity resolution as a universal task, assuming the
existence of a single interpretation of a real-world entity and focusing only
on finding matched records, separating corresponding from non-corresponding
ones, with respect to this single interpretation. However, in real-world
scenarios, where entity resolution is part of a more general data project,
downstream applications may have varying interpretations of real-world entities
relating, for example, to various user needs. In what follows, we introduce the
problem of multiple intents entity resolution (MIER), an extension to the
universal (single intent) entity resolution task. As a solution, we propose
FlexER, utilizing contemporary solutions to universal entity resolution tasks
to solve multiple intents entity resolution. FlexER addresses the problem as a
multi-label classification problem. It combines intent-based representations of
tuple pairs using a multiplex graph representation that serves as an input to a
graph neural network (GNN). FlexER learns intent representations and improves
the outcome to multiple resolution problems. A large-scale empirical evaluation
introduces a new benchmark and, using also two well-known benchmarks, shows
that FlexER effectively solves the MIER problem and outperforms the
state-of-the-art for a universal entity resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who is GPT-3? An Exploration of Personality, Values and Demographics. (arXiv:2209.14338v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14338">
<div class="article-summary-box-inner">
<span><p>Language models such as GPT-3 have caused a furore in the research community.
Some studies found that GPT-3 has some creative abilities and makes mistakes
that are on par with human behaviour. This paper answers a related question:
Who is GPT-3? We administered two validated measurement tools to GPT-3 to
assess its personality, the values it holds and its self-reported demographics.
Our results show that GPT-3 scores similarly to human samples in terms of
personality and - when provided with a model response memory - in terms of the
values it holds. We provide the first evidence of psychological assessment of
the GPT-3 model and thereby add to our understanding of this language model. We
close with suggestions for future research that moves social science closer to
language models and vice versa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. (arXiv:2210.05035v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05035">
<div class="article-summary-box-inner">
<span><p>Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Utility of Self-supervised Models for Prosody-related Tasks. (arXiv:2210.07185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07185">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) from speech data has produced models that have
achieved remarkable performance in many tasks, and that are known to implicitly
represent many aspects of information latently present in speech signals.
However, relatively little is known about the suitability of such models for
prosody-related tasks or the extent to which they encode prosodic information.
We present a new evaluation framework, SUPERB-prosody, consisting of three
prosody-related downstream tasks and two pseudo tasks. We find that 13 of the
15 SSL models outperformed the baseline on all the prosody-related tasks. We
also show good performance on two pseudo tasks: prosody reconstruction and
future prosody prediction. We further analyze the layerwise contributions of
the SSL models. Overall we conclude that SSL speech models are highly effective
for prosody-related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. (arXiv:2210.08933v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08933">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models have emerged as a new paradigm for generative
models. Despite the success in domains using continuous signals such as vision
and audio, adapting diffusion models to natural language is difficult due to
the discrete nature of text. We tackle this challenge by proposing DiffuSeq: a
diffusion model designed for sequence-to-sequence (Seq2Seq) text generation
tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find
DiffuSeq achieving comparable or even better performance than six established
baselines, including a state-of-the-art model that is based on pre-trained
language models. Apart from quality, an intriguing property of DiffuSeq is its
high diversity during generation, which is desired in many Seq2Seq tasks. We
further include a theoretical analysis revealing the connection between
DiffuSeq and autoregressive/non-autoregressive models. Bringing together
theoretical analysis and empirical evidence, we demonstrate the great potential
of diffusion models in complex conditional language generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft-Labeled Contrastive Pre-training for Function-level Code Representation. (arXiv:2210.09597v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09597">
<div class="article-summary-box-inner">
<span><p>Code contrastive pre-training has recently achieved significant progress on
code-related tasks. In this paper, we present \textbf{SCodeR}, a
\textbf{S}oft-labeled contrastive pre-training framework with two positive
sample construction methods to learn functional-level \textbf{Code}
\textbf{R}epresentation. Considering the relevance between codes in a
large-scale code corpus, the soft-labeled contrastive pre-training can obtain
fine-grained soft-labels through an iterative adversarial manner and use them
to learn better code representation. The positive sample construction is
another key for contrastive pre-training. Previous works use
transformation-based methods like variable renaming to generate semantically
equal positive codes. However, they usually result in the generated code with a
highly similar surface form, and thus mislead the model to focus on superficial
code structure instead of code semantics. To encourage SCodeR to capture
semantic information from the code, we utilize code comments and abstract
syntax sub-trees of the code to build positive samples. We conduct experiments
on four code-related tasks over seven datasets. Extensive experimental results
show that SCodeR achieves new state-of-the-art performance on all of them,
which illustrates the effectiveness of the proposed pre-training method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Document Selection for Efficient Encoder Pretraining. (arXiv:2210.10951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10951">
<div class="article-summary-box-inner">
<span><p>Building pretrained language models is considered expensive and
data-intensive, but must we increase dataset size to achieve better
performance? We propose an alternative to larger training sets by automatically
identifying smaller yet domain-representative subsets. We extend Cynical Data
Selection, a statistical sentence scoring method that conditions on a
representative target domain corpus. As an example, we treat the OntoNotes
corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically
selected subset of the Pile. On both perplexity and across several downstream
tasks in the target domain, it consistently outperforms random selection with
20x less data, 3x fewer training iterations, and 2x less estimated cloud
compute cost, validating the recipe of automatic document selection for LM
pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation. (arXiv:2210.11109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11109">
<div class="article-summary-box-inner">
<span><p>Image-to-text tasks, such as open-ended image captioning and controllable
image description, have received extensive attention for decades. Here, we
further advance this line of work by presenting Visual Spatial Description
(VSD), a new perspective for image-to-text toward spatial semantics. Given an
image and two objects inside it, VSD aims to produce one description focusing
on the spatial perspective between the two objects. Accordingly, we manually
annotate a dataset to facilitate the investigation of the newly-introduced task
and build several benchmark encoder-decoder models by using VL-BART and VL-T5
as backbones. In addition, we investigate pipeline and joint end-to-end
architectures for incorporating visual spatial relationship classification
(VSRC) information into our model. Finally, we conduct experiments on our
benchmark dataset to evaluate all our models. Results show that our models are
impressive, providing accurate and human-like spatial-oriented text
descriptions. Meanwhile, VSRC has great potential for VSD, and the joint
end-to-end architecture is the better choice for their integration. We make the
dataset and codes public for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11715">
<div class="article-summary-box-inner">
<span><p>Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. (arXiv:2210.12409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12409">
<div class="article-summary-box-inner">
<span><p>Variational Auto-Encoder (VAE) has been widely adopted in text generation.
Among many variants, recurrent VAE learns token-wise latent variables with each
conditioned on the preceding ones, which captures sequential variability better
in the era of RNN. However, it is unclear how to incorporate such recurrent
dynamics into the recently dominant Transformer due to its parallelism. In this
work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE
imposes recurrence on segment-wise latent variables with arbitrarily separated
text segments and constructs the posterior distribution with residual
parameterization. Besides, we design an acceleration method by approximating
idempotent matrices, which allows parallelism while maintaining the conditional
dependence of latent variables. We demonstrate that TRACE could enhance the
entanglement of each segment and preceding latent variables and deduce a
non-zero lower bound of the KL term, providing a theoretical guarantee of
generation diversity. Experiments on two unconditional and one conditional
generation tasks show that TRACE achieves significantly improved diversity
while maintaining satisfactory generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts. (arXiv:2210.12467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12467">
<div class="article-summary-box-inner">
<span><p>Despite tremendous progress in automatic summarization, state-of-the-art
methods are predominantly trained to excel in summarizing short newswire
articles, or documents with strong layout biases such as scientific articles or
government reports. Efficient techniques to summarize financial documents,
including facts and figures, have largely been unexplored, majorly due to the
unavailability of suitable datasets. In this work, we present ECTSum, a new
dataset with transcripts of earnings calls (ECTs), hosted by publicly traded
companies, as documents, and short experts-written telegram-style bullet point
summaries derived from corresponding Reuters articles. ECTs are long
unstructured documents without any prescribed length limit or format. We
benchmark our dataset with state-of-the-art summarizers across various metrics
evaluating the content quality and factual consistency of the generated
summaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to
generate a set of bullet points that precisely capture the important facts
discussed in the calls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus Is What You Need For Chinese Grammatical Error Correction. (arXiv:2210.12692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12692">
<div class="article-summary-box-inner">
<span><p>Chinese Grammatical Error Correction (CGEC) aims to automatically detect and
correct grammatical errors contained in Chinese text. In the long term,
researchers regard CGEC as a task with a certain degree of uncertainty, that
is, an ungrammatical sentence may often have multiple references. However, we
argue that even though this is a very reasonable hypothesis, it is too harsh
for the intelligence of the mainstream models in this era. In this paper, we
first discover that multiple references do not actually bring positive gains to
model training. On the contrary, it is beneficial to the CGEC model if the
model can pay attention to small but essential data during the training
process. Furthermore, we propose a simple yet effective training strategy
called OneTarget to improve the focus ability of the CGEC models and thus
improve the CGEC performance. Extensive experiments and detailed analyses
demonstrate the correctness of our discovery and the effectiveness of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-level Sentiment Analysis in Contact Center Telephone Conversations. (arXiv:2210.13401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13401">
<div class="article-summary-box-inner">
<span><p>Entity-level sentiment analysis predicts the sentiment about entities
mentioned in a given text. It is very useful in a business context to
understand user emotions towards certain entities, such as products or
companies. In this paper, we demonstrate how we developed an entity-level
sentiment analysis system that analyzes English telephone conversation
transcripts in contact centers to provide business insight. We present two
approaches, one entirely based on the transformer-based DistilBERT model, and
another that uses a convolutional neural network supplemented with some
heuristic rules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapters for Enhanced Modeling of Multilingual Knowledge and Text. (arXiv:2210.13617v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13617">
<div class="article-summary-box-inner">
<span><p>Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Relation Classification via Efficient and Effective Prompting. (arXiv:2210.13838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13838">
<div class="article-summary-box-inner">
<span><p>Prompting pre-trained language models has achieved impressive performance on
various NLP tasks, especially in low data regimes. Despite the success of
prompting in monolingual settings, applying prompt-based methods in
multilingual scenarios has been limited to a narrow set of tasks, due to the
high cost of handcrafting multilingual prompts. In this paper, we present the
first work on prompt-based multilingual relation classification (RC), by
introducing an efficient and effective method that constructs prompts from
relation triples and involves only minimal translation for the class labels. We
evaluate its performance in fully supervised, few-shot and zero-shot scenarios,
and analyze its effectiveness across 14 languages, prompt variants, and
English-task training in cross-lingual settings. We find that in both fully
supervised and few-shot scenarios, our prompt method beats competitive
baselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the
random baseline by a large margin in zero-shot experiments. Our method requires
little in-language knowledge and can be used as a strong baseline for similar
multilingual classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Language Models for Secure Data Sharing. (arXiv:2210.13918v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13918">
<div class="article-summary-box-inner">
<span><p>To protect the privacy of individuals whose data is being shared, it is of
high importance to develop methods allowing researchers and companies to
release textual data while providing formal privacy guarantees to its
originators. In the field of NLP, substantial efforts have been directed at
building mechanisms following the framework of local differential privacy,
thereby anonymizing individual text samples before releasing them. In practice,
these approaches are often dissatisfying in terms of the quality of their
output language due to the strong noise required for local differential
privacy. In this paper, we approach the problem at hand using global
differential privacy, particularly by training a generative language model in a
differentially private manner and consequently sampling data from it. Using
natural language prompts and a new prompt-mismatch loss, we are able to create
highly accurate and fluent textual datasets taking on specific desired
attributes such as sentiment or topic and resembling statistical properties of
the training data. We perform thorough experiments indicating that our
synthetic datasets do not leak information from our original data and are of
high language quality and highly suitable for training models for further
analysis on real-world data. Notably, we also demonstrate that training
classifiers on private synthetic data outperforms directly training classifiers
on real data with DP-SGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14169">
<div class="article-summary-box-inner">
<span><p>Dialogue understanding tasks often necessitate abundant annotated data to
achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentation for dialogue
understanding by prompting large pre-trained language models and present a
novel approach that iterates on augmentation quality by applying
weakly-supervised filters. We evaluate our methods on the emotion and act
classification tasks in DailyDialog and the intent classification task in
Facebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our
augmented data mixed with few-shot ground truth data are able to approach or
surpass existing state-of-the-art performance on both datasets. For DailyDialog
specifically, using 10% of the ground truth data we outperform the current
state-of-the-art model which uses 100% of the data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-27 23:19:46.774633221 UTC">2022-10-27 23:19:46 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>