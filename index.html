<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-28T01:30:00Z">11-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaNet:Conditional Medical Report Generation for COVID-19 Diagnosis. (arXiv:2211.13229v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13229">
<div class="article-summary-box-inner">
<span><p>Fast screening and diagnosis are critical in COVID-19 patient treatment. In
addition to the gold standard RT-PCR, radiological imaging like X-ray and CT
also works as an important means in patient screening and follow-up. However,
due to the excessive number of patients, writing reports becomes a heavy burden
for radiologists. To reduce the workload of radiologists, we propose DeltaNet
to generate medical reports automatically. Different from typical image
captioning approaches that generate reports with an encoder and a decoder,
DeltaNet applies a conditional generation process. In particular, given a
medical image, DeltaNet employs three steps to generate a report: 1) first
retrieving related medical reports, i.e., the historical reports from the same
or similar patients; 2) then comparing retrieved images and current image to
find the differences; 3) finally generating a new report to accommodate
identified differences based on the conditional report. We evaluate DeltaNet on
a COVID-19 dataset, where DeltaNet outperforms state-of-the-art approaches.
Besides COVID-19, the proposed DeltaNet can be applied to other diseases as
well. We validate its generalization capabilities on the public IU-Xray and
MIMIC-CXR datasets for chest-related diseases. Code is available at
\url{https://github.com/LX-doctorAI1/DeltaNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Multimodal Model with Unlikelihood Training for Visual Dialog. (arXiv:2211.13235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13235">
<div class="article-summary-box-inner">
<span><p>The task of visual dialog requires a multimodal chatbot to answer sequential
questions from humans about image content. Prior work performs the standard
likelihood training for answer generation on the positive instances (involving
correct answers). However, the likelihood objective often leads to frequent and
dull outputs and fails to exploit the useful knowledge from negative instances
(involving incorrect answers). In this paper, we propose a Unified Multimodal
Model with UnLikelihood Training, named UniMM-UL, to tackle this problem.
First, to improve visual dialog understanding and generation by multi-task
learning, our model extends ViLBERT from only supporting answer discrimination
to holding both answer discrimination and answer generation seamlessly by
different attention masks. Specifically, in order to make the original
discriminative model compatible with answer generation, we design novel
generative attention masks to implement the autoregressive Masked Language
Modeling (autoregressive MLM) task. And to attenuate the adverse effects of the
likelihood objective, we exploit unlikelihood training on negative instances to
make the model less likely to generate incorrect answers. Then, to utilize
dense annotations, we adopt different fine-tuning methods for both generating
and discriminating answers, rather than just for discriminating answers as in
the prior work. Finally, on the VisDial dataset, our model achieves the best
generative results (69.23 NDCG score). And our model also yields comparable
discriminative results with the state-of-the-art in both single-model and
ensemble settings (75.92 and 76.17 NDCG scores).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction. (arXiv:2211.13252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13252">
<div class="article-summary-box-inner">
<span><p>Text error correction aims to correct the errors in text sequences such as
those typed by humans or generated by speech recognition models. Previous error
correction methods usually take the source (incorrect) sentence as encoder
input and generate the target (correct) sentence through the decoder. Since the
error rate of the incorrect sentence is usually low (e.g., 10\%), the
correction model can only learn to correct on limited error tokens but
trivially copy on most tokens (correct tokens), which harms the effective
training of error correction. In this paper, we argue that the correct tokens
should be better utilized to facilitate effective training and then propose a
simple yet effective masking strategy to achieve this goal. Specifically, we
randomly mask out a part of the correct tokens in the source sentence and let
the model learn to not only correct the original error tokens but also predict
the masked tokens based on their context information. Our method enjoys several
advantages: 1) it alleviates trivial copy; 2) it leverages effective training
signals from correct tokens; 3) it is a plug-and-play module and can be applied
to different models and tasks. Experiments on spelling error correction and
speech recognition error correction on Mandarin datasets and grammar error
correction on English datasets with both autoregressive and non-autoregressive
generation models show that our method improves the correction accuracy
consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Device Directedness with Contextual Cues for Spoken Dialog Systems. (arXiv:2211.13280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13280">
<div class="article-summary-box-inner">
<span><p>In this work, we define barge-in verification as a supervised learning task
where audio-only information is used to classify user spoken dialogue into true
and false barge-ins. Following the success of pre-trained models, we use
low-level speech representations from a self-supervised representation learning
model for our downstream classification task. Further, we propose a novel
technique to infuse lexical information directly into speech representations to
improve the domain-specific language information implicitly learned during
pre-training. Experiments conducted on spoken dialog data show that our
proposed model trained to validate barge-in entirely from speech
representations is faster by 38% relative and achieves 4.5% relative F1 score
improvement over a baseline LSTM model that uses both audio and Automatic
Speech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed
model with lexically infused representations along with contextual features
provides a further relative improvement of 5.7% in the F1 score but only 22%
faster than the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEAT: Stable and Explainable Attention. (arXiv:2211.13290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13290">
<div class="article-summary-box-inner">
<span><p>Currently, attention mechanism becomes a standard fixture in most
state-of-the-art natural language processing (NLP) models, not only due to
outstanding performance it could gain, but also due to plausible innate
explanation for the behaviors of neural architectures it provides, which is
notoriously difficult to analyze. However, recent studies show that attention
is unstable against randomness and perturbations during training or testing,
such as random seeds and slight perturbation of embedding vectors, which
impedes it from becoming a faithful explanation tool. Thus, a natural question
is whether we can find some substitute of the current attention which is more
stable and could keep the most important characteristics on explanation and
prediction of attention. In this paper, to resolve the problem, we provide a
first rigorous definition of such alternate namely SEAT (Stable and Explainable
Attention). Specifically, a SEAT should has the following three properties: (1)
Its prediction distribution is enforced to be close to the distribution based
on the vanilla attention; (2) Its top-k indices have large overlaps with those
of the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any
slight perturbation on SEAT will not change the prediction distribution too
much, which implicitly indicates that it is stable to randomness and
perturbations. Finally, through intensive experiments on various datasets, we
compare our SEAT with other baseline methods using RNN, BiLSTM and BERT
architectures via six different evaluation metrics for model interpretation,
stability and accuracy. Results show that SEAT is more stable against different
perturbations and randomness while also keeps the explainability of attention,
which indicates it is a more faithful explanation. Moreover, compared with
vanilla attention, there is almost no utility (accuracy) degradation for SEAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13308">
<div class="article-summary-box-inner">
<span><p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks, without the need for further fine-tuning.
However, existing benchmarks for evaluating these representations fail to
capture the diversity of relevant tasks. In response, we introduce SciRepEval,
the first comprehensive benchmark for training and evaluating scientific
document representations. It includes 25 challenging and realistic tasks, 11 of
which are new, across four formats: classification, regression, ranking and
search. We then use the benchmark to study and improve the generalization
ability of scientific document representation models. We show how
state-of-the-art models struggle to generalize across task formats, and that
simple multi-task training fails to improve them. However, a new approach that
learns multiple embeddings per document, each tailored to a different format,
can improve performance. We experiment with task-format-specific control codes
and adapters in a multi-task setting and find that they outperform the existing
single-embedding state-of-the-art by up to 1.5 points absolute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rank-One Editing of Encoder-Decoder Models. (arXiv:2211.13317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13317">
<div class="article-summary-box-inner">
<span><p>Large sequence to sequence models for tasks such as Neural Machine
Translation (NMT) are usually trained over hundreds of millions of samples.
However, training is just the origin of a model's life-cycle. Real-world
deployments of models require further behavioral adaptations as new
requirements emerge or shortcomings become known. Typically, in the space of
model behaviors, behavior deletion requests are addressed through model
retrainings whereas model finetuning is done to address behavior addition
requests, both procedures being instances of data-based model intervention. In
this work, we present a preliminary study investigating rank-one editing as a
direct intervention method for behavior deletion requests in encoder-decoder
transformer models. We propose four editing tasks for NMT and show that the
proposed editing algorithm achieves high efficacy, while requiring only a
single instance of positive example to fix an erroneous (negative) model
behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Report on the Euphemisms Detection Shared Task. (arXiv:2211.13327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13327">
<div class="article-summary-box-inner">
<span><p>This paper presents The Shared Task on Euphemism Detection for the Third
Workshop on Figurative Language Processing (FigLang 2022) held in conjunction
with EMNLP 2022. Participants were invited to investigate the euphemism
detection task: given input text, identify whether it contains a euphemism. The
input data is a corpus of sentences containing potentially euphemistic terms
(PETs) collected from the GloWbE corpus (Davies and Fuchs, 2015), and are
human-annotated as containing either a euphemistic or literal usage of a PET.
In this paper, we present the results and analyze the common themes, methods
and findings of the participating teams
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of Modulated Cross-Entropy in Natural Language Inference. (arXiv:2211.13331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13331">
<div class="article-summary-box-inner">
<span><p>There is no such thing as a perfect dataset. In some datasets, deep neural
networks discover underlying heuristics that allow them to take shortcuts in
the learning process, resulting in poor generalization capability. Instead of
using standard cross-entropy, we explore whether a modulated version of
cross-entropy called focal loss can constrain the model so as not to use
heuristics and improve generalization performance. Our experiments in natural
language inference show that focal loss has a regularizing impact on the
learning process, increasing accuracy on out-of-distribution data, but slightly
decreasing performance on in-distribution data. Despite the improved
out-of-distribution performance, we demonstrate the shortcomings of focal loss
and its inferiority in comparison to the performance of methods such as
unbiased focal loss and self-debiasing ensembles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring. (arXiv:2211.13373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13373">
<div class="article-summary-box-inner">
<span><p>In the prompt-specific holistic score prediction task for Automatic Essay
Scoring, the general approaches include pre-trained neural model, coherence
model, and hybrid model that incorporate syntactic features with neural model.
In this paper, we propose a novel approach to extract and represent essay
coherence features with prompt-learning NSP that shows to match the
state-of-the-art AES coherence model, and achieves the best performance for
long essays. We apply syntactic feature dense embedding to augment BERT-based
model and achieve the best performance for hybrid methodology for AES. In
addition, we explore various ideas to combine coherence, syntactic information
and semantic embeddings, which no previous study has done before. Our combined
model also performs better than the SOTA available for combined model, even
though it does not outperform our syntactic enhanced neural model. We further
offer analyses that can be useful for future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InDEX: Indonesian Idiom and Expression Dataset for Cloze Test. (arXiv:2211.13376v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13376">
<div class="article-summary-box-inner">
<span><p>We propose InDEX, an Indonesian Idiom and Expression dataset for cloze test.
The dataset contains 10438 unique sentences for 289 idioms and expressions for
which we generate 15 different types of distractors, resulting in a large
cloze-style corpus. Many baseline models of cloze test reading comprehension
apply BERT with random initialization to learn embedding representation. But
idioms and fixed expressions are different such that the literal meaning of the
phrases may or may not be consistent with their contextual meaning. Therefore,
we explore different ways to combine static and contextual representations for
a stronger baseline model. Experimentations show that combining definition and
random initialization will better support cloze test model performance for
idioms whether independently or mixed with fixed expressions. While for fixed
expressions with no special meaning, static embedding with random
initialization is sufficient for cloze test model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13437">
<div class="article-summary-box-inner">
<span><p>Cross-modal alignment is essential for vision-language pre-training (VLP)
models to learn the correct corresponding information across different
modalities. For this purpose, inspired by the success of masked language
modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling
tasks have been proposed for VLP to further promote cross-modal interactions.
The core idea of previous masked modeling tasks is to focus on reconstructing
the masked tokens based on visible context for learning local-to-local
alignment. However, most of them pay little attention to the global semantic
features generated for the masked data, resulting in the limited cross-modal
alignment ability of global representations. Therefore, in this paper, we
propose a novel Semantic Completion Learning (SCL) task, complementary to
existing masked modeling tasks, to facilitate global-to-local alignment.
Specifically, the SCL task complements the missing semantics of masked data by
capturing the corresponding information from the other modality, promoting
learning more representative global features which have a great impact on the
performance of downstream tasks. Moreover, we present a flexible vision
encoder, which enables our model to perform image-text and video-text
multimodal tasks simultaneously. Experimental results show that our proposed
method obtains state-of-the-art performance on various vision-language
benchmarks, such as visual question answering, image-text retrieval, and
video-text retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering. (arXiv:2211.13515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13515">
<div class="article-summary-box-inner">
<span><p>Unsupervised commonsense question answering requires mining effective
commonsense knowledge without the rely on the labeled task data. Previous
methods typically retrieved from traditional knowledge bases or used
pre-trained language models (PrLMs) to generate fixed types of knowledge, which
have poor generalization ability. In this paper, we aim to address the above
limitation by leveraging the implicit knowledge stored in PrLMs and propose a
two-stage prompt-based unsupervised commonsense question answering framework
(TSGP). Specifically, we first use knowledge generation prompts to generate the
knowledge required for questions with unlimited types and possible candidate
answers independent of specified choices. Then, we further utilize answer
generation prompts to generate possible candidate answers independent of
specified choices. Experimental results and analysis on three different
commonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA,
demonstrate that TSGP significantly improves the reasoning ability of language
models in unsupervised settings. Our code is available at:
https://github.com/Yueqing-Sun/TSGP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Mahalanobis-Based Scores for Textual OOD Detection. (arXiv:2211.13527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13527">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have boosted the adoption of NLP systems in real-life
applications. However, they turn out to be vulnerable to distribution shifts
over time which may cause severe dysfunctions in production systems, urging
practitioners to develop tools to detect out-of-distribution (OOD) samples
through the lens of the neural network. In this paper, we introduce TRUSTED, a
new OOD detector for classifiers based on Transformer architectures that meets
operational requirements: it is unsupervised and fast to compute. The
efficiency of TRUSTED relies on the fruitful idea that all hidden layers carry
relevant information to detect OOD examples. Based on this, for a given input,
TRUSTED consists in (i) aggregating this information and (ii) computing a
similarity score by exploiting the training distribution, leveraging the
powerful concept of data depth. Our extensive numerical experiments involve 51k
model configurations, including various checkpoints, seeds, and datasets, and
demonstrate that TRUSTED achieves state-of-the-art performances. In particular,
it improves previous AUROC over 3 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How "open" are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation. (arXiv:2211.13560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13560">
<div class="article-summary-box-inner">
<span><p>Open-domain chatbots are supposed to converse freely with humans without
being restricted to a topic, task or domain. However, the boundaries and/or
contents of open-domain conversations are not clear. To clarify the boundaries
of "openness", we conduct two studies: First, we classify the types of "speech
events" encountered in a chatbot evaluation data set (i.e., Meena by Google)
and find that these conversations mainly cover the "small talk" category and
exclude the other speech event categories encountered in real life human-human
communication. Second, we conduct a small-scale pilot study to generate online
conversations covering a wider range of speech event categories between two
humans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook).
A human evaluation of these generated conversations indicates a preference for
human-human conversations, since the human-chatbot conversations lack coherence
in most speech event categories. Based on these results, we suggest (a) using
the term "small talk" instead of "open-domain" for the current chatbots which
are not that "open" in terms of conversational abilities yet, and (b) revising
the evaluation methods to test the chatbot conversations against other speech
events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13613">
<div class="article-summary-box-inner">
<span><p>Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal, our proposed method offers a generic solution invariant
to the target Sign language. Our method gradually generates pose predictions
using transformer encoders that create meaningful representations of the text
and poses while considering their spatial and temporal information. We use weak
supervision for the training process and show that our method succeeds in
learning from partial and inaccurate data. Additionally, we offer a new
distance measurement for pose sequences, normalized Dynamic Time Warping
(nDTW), based on DTW over normalized keypoints trajectories, and validate its
correctness using AUTSL, a large-scale Sign language dataset. We show that it
measures the distance between pose sequences more accurately than existing
measurements and use it to assess the quality of our generated pose sequences.
Code for the data pre-processing, the model, and the distance measurement is
publicly released for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Fine-tuning: Towards Robust Performance Under Varying Data Sizes. (arXiv:2211.13638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13638">
<div class="article-summary-box-inner">
<span><p>In this paper, we move towards combining large parametric models with
non-parametric prototypical networks. We propose prototypical fine-tuning, a
novel prototypical framework for fine-tuning pretrained language models (LM),
which automatically learns a bias to improve predictive performance for varying
data sizes, especially low-resource settings. Our prototypical fine-tuning
approach can automatically adjust the model capacity according to the number of
data points and the model's inherent attributes. Moreover, we propose four
principles for effective prototype fine-tuning towards the optimal solution.
Experimental results across various datasets show that our work achieves
significant performance improvements under various low-resource settings, as
well as comparable and usually better performances in high-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Learning for Low Resource Spoken Language Understanding. (arXiv:2211.13703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13703">
<div class="article-summary-box-inner">
<span><p>We explore the benefits that multitask learning offer to speech processing as
we train models on dual objectives with automatic speech recognition and intent
classification or sentiment classification. Our models, although being of
modest size, show improvements over models trained end-to-end on intent
classification. We compare different settings to find the optimal disposition
of each task module compared to one another. Finally, we study the performance
of the models in low-resource scenario by training the models with as few as
one example per class. We show that multitask learning in these scenarios
compete with a baseline model trained on text features and performs
considerably better than a pipeline model. On sentiment classification, we
match the performance of an end-to-end model with ten times as many parameters.
We consider 4 tasks and 4 datasets in Dutch and English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13709">
<div class="article-summary-box-inner">
<span><p>As Natural Language Processing (NLP) technology rapidly develops and spreads
into daily life, it becomes crucial to anticipate how its use could harm
people. However, our ways of assessing the biases of NLP models have not kept
up. While especially the detection of English gender bias in such models has
enjoyed increasing research attention, many of the measures face serious
problems, as it is often unclear what they actually measure and how much they
are subject to measurement error. In this paper, we provide an
interdisciplinary approach to discussing the issue of NLP model bias by
adopting the lens of psychometrics -- a field specialized in the measurement of
concepts like bias that are not directly observable. We pair an introduction of
relevant psychometric concepts with a discussion of how they could be used to
evaluate and improve bias measures. We also argue that adopting psychometric
vocabulary and methodology can make NLP bias research more efficient and
transparent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion-guided Cross-domain Fake News Detection using Adversarial Domain Adaptation. (arXiv:2211.13718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13718">
<div class="article-summary-box-inner">
<span><p>Recent works on fake news detection have shown the efficacy of using emotions
as a feature or emotions-based features for improved performance. However, the
impact of these emotion-guided features for fake news detection in cross-domain
settings, where we face the problem of domain shift, is still largely
unexplored. In this work, we evaluate the impact of emotion-guided features for
cross-domain fake news detection, and further propose an emotion-guided,
domain-adaptive approach using adversarial learning. We prove the efficacy of
emotion-guided models in cross-domain settings for various combinations of
source and target datasets from FakeNewsAMT, Celeb, Politifact and Gossipcop
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-type Identification for Academic Questions in Online Learning Platform. (arXiv:2211.13727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13727">
<div class="article-summary-box-inner">
<span><p>Online learning platforms provide learning materials and answers to students'
academic questions by experts, peers, or systems. This paper explores
question-type identification as a step in content understanding for an online
learning platform. The aim of the question-type identifier is to categorize
question types based on their structure and complexity, using the question
text, subject, and structural features. We have defined twelve question-type
classes, including Multiple-Choice Question (MCQ), essay, and others. We have
compiled an internal dataset of students' questions and used a combination of
weak-supervision techniques and manual annotation. We then trained a BERT-based
ensemble model on this dataset and evaluated this model on a separate
human-labeled test set. Our experiments yielded an F1-score of 0.94 for MCQ
binary classification and promising results for 12-class multilabel
classification. We deployed the model in our online learning platform as a
crucial enabler for content understanding to enhance the student learning
experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">German Phoneme Recognition with Text-to-Phoneme Data Augmentation. (arXiv:2211.13776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13776">
<div class="article-summary-box-inner">
<span><p>In this study, we experimented to examine the effect of adding the most
frequent n phoneme bigrams to the basic vocabulary on the German phoneme
recognition model using the text-to-phoneme data augmentation strategy. As a
result, compared to the baseline model, the vowel30 model and the const20 model
showed an increased BLEU score of more than 1 point, and the total30 model
showed a significant decrease in the BLEU score of more than 20 points, showing
that the phoneme bigrams could have a positive or negative effect on the model
performance. In addition, we identified the types of errors that the models
repeatedly showed through error analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyTAIL: Interactive and Incremental Learning of NLP Models with Human in the Loop for Online Data. (arXiv:2211.13786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13786">
<div class="article-summary-box-inner">
<span><p>Online data streams make training machine learning models hard because of
distribution shift and new patterns emerging over time. For natural language
processing (NLP) tasks that utilize a collection of features based on lexicons
and rules, it is important to adapt these features to the changing data. To
address this challenge we introduce PyTAIL, a python library, which allows a
human in the loop approach to actively train NLP models. PyTAIL enhances
generic active learning, which only suggests new instances to label by also
suggesting new features like rules and lexicons to label. Furthermore, PyTAIL
is flexible enough for users to accept, reject, or update rules and lexicons as
the model is being trained. Finally, we simulate the performance of PyTAIL on
existing social media benchmark datasets for text classification. We compare
various active learning strategies on these benchmarks. The model closes the
gap with as few as 10% of the training data. Finally, we also highlight the
importance of tracking evaluation metric on remaining data (which is not yet
merged with active learning) alongside the test dataset. This highlights the
effectiveness of the model in accurately annotating the remaining dataset,
which is especially suitable for batch processing of large unlabelled corpora.
PyTAIL will be available at https://github.com/socialmediaie/pytail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Answering and Question Generation for Finnish. (arXiv:2211.13794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13794">
<div class="article-summary-box-inner">
<span><p>Recent advances in the field of language modeling have improved the
state-of-the-art in question answering (QA) and question generation (QG).
However, the development of modern neural models, their benchmarks, and
datasets for training them has mainly focused on English. Finnish, like many
other languages, faces a shortage of large QA/QG model training resources,
which has prevented experimenting with state-of-the-art QA/QG fine-tuning
methods. We present the first neural QA and QG models that work with Finnish.
To train the models, we automatically translate the SQuAD dataset and then use
normalization methods to reduce the amount of problematic data created during
the translation. Using the synthetic data, together with the Finnish partition
of the TyDi-QA dataset, we fine-tune several transformer-based models to both
QA and QG and evaluate their performance. To the best of our knowledge, the
resulting dataset is the first large-scale QA/QG resource for Finnish. This
paper also sets the initial benchmarks for Finnish-language QA and QG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Linguistic and Computational Requirements for Creating Face-to-Face Multimodal Human-Machine Interaction. (arXiv:2211.13804v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13804">
<div class="article-summary-box-inner">
<span><p>In this study, conversations between humans and avatars are linguistically,
organizationally, and structurally analyzed, focusing on what is necessary for
creating face-to-face multimodal interfaces for machines. We videorecorded
thirty-four human-avatar interactions, performed complete linguistic
microanalysis on video excerpts, and marked all the occurrences of multimodal
actions and events. Statistical inferences were applied to data, allowing us to
comprehend not only how often multimodal actions occur but also how multimodal
events are distributed between the speaker (emitter) and the listener
(recipient). We also observed the distribution of multimodal occurrences for
each modality. The data show evidence that double-loop feedback is established
during a face-to-face conversation. This led us to propose that knowledge from
Conversation Analysis (CA), cognitive science, and Theory of Mind (ToM), among
others, should be incorporated into the ones used for describing human-machine
multimodal interactions. Face-to-face interfaces require an additional control
layer to the multimodal fusion layer. This layer has to organize the flow of
conversation, integrate the social context into the interaction, as well as
make plans concerning 'what' and 'how' to progress on the interaction. This
higher level is best understood if we incorporate insights from CA and ToM into
the interface system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt. (arXiv:2211.13813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13813">
<div class="article-summary-box-inner">
<span><p>Automatic International Classification of Diseases (ICD) coding aims to
assign multiple ICD codes to a medical note with an average of 3,000+ tokens.
This task is challenging due to the high-dimensional space of multi-label
assignment (155,000+ ICD code candidates) and the long-tail challenge - Many
ICD codes are infrequently assigned yet infrequent ICD codes are important
clinically. This study addresses the long-tail challenge by transforming this
multi-label classification task into an autoregressive generation task.
Specifically, we first introduce a novel pretraining objective to generate free
text diagnoses and procedure using the SOAP structure, the medical logic
physicians use for note documentation. Second, instead of directly predicting
the high dimensional space of ICD codes, our model generates the lower
dimension of text descriptions, which then infer ICD codes. Third, we designed
a novel prompt template for multi-label classification. We evaluate our
Generation with Prompt model with the benchmark of all code assignment
(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark
(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with
a marco F1 30.2, which substantially outperforms the previous MIMIC-III-full
SOTA model (marco F1 4.3) and the model specifically designed for few/zero shot
setting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross
attention reranker with prompts, to integrate previous SOTA and our best
few-shot coding predictions. Experiments on MIMIC-III-full show that our
ensemble learner substantially improves both macro and micro F1, from 10.4 to
14.6 and from 58.2 to 59.1, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Selective Masking as a Bridge between Pre-training and Fine-tuning. (arXiv:2211.13815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13815">
<div class="article-summary-box-inner">
<span><p>Pre-training a language model and then fine-tuning it for downstream tasks
has demonstrated state-of-the-art results for various NLP tasks. Pre-training
is usually independent of the downstream task, and previous works have shown
that this pre-training alone might not be sufficient to capture the
task-specific nuances. We propose a way to tailor a pre-trained BERT model for
the downstream task via task-specific masking before the standard supervised
fine-tuning. For this, a word list is first collected specific to the task. For
example, if the task is sentiment classification, we collect a small sample of
words representing both positive and negative sentiments. Next, a word's
importance for the task, called the word's task score, is measured using the
word list. Each word is then assigned a probability of masking based on its
task score. We experiment with different masking functions that assign the
probability of masking based on the word's task score. The BERT model is
further trained on MLM objective, where masking is done using the above
strategy. Following this standard supervised fine-tuning is done for different
downstream tasks. Results on these tasks show that the selective masking
strategy outperforms random masking, indicating its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Entities in the Astrophysics Literature: A Comparison of Word-based and Span-based Entity Recognition Methods. (arXiv:2211.13819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13819">
<div class="article-summary-box-inner">
<span><p>Information Extraction from scientific literature can be challenging due to
the highly specialised nature of such text. We describe our entity recognition
methods developed as part of the DEAL (Detecting Entities in the Astrophysics
Literature) shared task. The aim of the task is to build a system that can
identify Named Entities in a dataset composed by scholarly articles from
astrophysics literature. We planned our participation such that it enables us
to conduct an empirical comparison between word-based tagging and span-based
classification methods. When evaluated on two hidden test sets provided by the
organizer, our best-performing submission achieved $F_1$ scores of 0.8307
(validation phase) and 0.7990 (testing phase).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13854">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pretraining (CLIP) has demonstrated great
zero-shot performance for image-text matching because of its holistic use of
natural language supervision that covers large-scale, open-world visual
concepts. However, it is still challenging to adapt CLIP to compositional image
and text matching -- a more challenging image and matching mask requiring the
model understanding of compositional word concepts and visual components.
Towards better compositional generalization in zero-shot image and text
matching, in this paper, we study the problem from a causal perspective: the
erroneous semantics of individual entities are essentially confounders that
cause the matching failure. Therefore, we propose a novel training-free
compositional CLIP model (ComCLIP). ComCLIP disentangles input images into
subjects, objects, and action sub-images and composes CLIP's vision encoder and
text encoder to perform evolving matching over compositional text embedding and
sub-image embeddings. In this way, ComCLIP can mitigate spurious correlations
introduced by the pretrained CLIP models and dynamically assess the
contribution of each entity when performing image and text matching.
Experiments on compositional image-text matching on SVO and ComVG and general
image-text retrieval on Flickr8K demonstrate the effectiveness of our
plug-and-play method, which boosts the zero-shot inference ability of CLIP even
without further training or fine-tuning of CLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?. (arXiv:2211.13865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13865">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) is often criticized for failures that happen
without awareness. The lack of competency awareness makes NMT untrustworthy.
This is in sharp contrast to human translators who give feedback or conduct
further investigations whenever they are in doubt about predictions. To fill
this gap, we propose a novel competency-aware NMT by extending conventional NMT
with a self-estimator, offering abilities to translate a source sentence and
estimate its competency. The self-estimator encodes the information of the
decoding procedure and then examines whether it can reconstruct the original
semantics of the source sentence. Experimental results on four translation
tasks demonstrate that the proposed method not only carries out translation
tasks intact but also delivers outstanding performance on quality estimation.
Without depending on any reference or annotated data typically required by
state-of-the-art metric and quality estimation methods, our model yields an
even higher correlation with human quality judgments than a variety of
aforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and
qualitative analyses show better robustness of competency awareness in our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13873">
<div class="article-summary-box-inner">
<span><p>Due to the absence of explicit connectives, implicit discourse relation
recognition (IDRR) remains a challenging task in discourse analysis. The
critical step for IDRR is to learn high-quality discourse relation
representations between two arguments. Recent methods tend to integrate the
whole hierarchical information of senses into discourse relation
representations for multi-level sense recognition. Nevertheless, they
insufficiently incorporate the static hierarchical structure containing all
senses (defined as global hierarchy), and ignore the hierarchical sense label
sequence corresponding to each instance (defined as local hierarchy). For the
purpose of sufficiently exploiting global and local hierarchies of senses to
learn better discourse relation representations, we propose a novel GLobal and
LOcal Hierarchy-aware Contrastive Framework (GLOF), to model two kinds of
hierarchies with the aid of contrastive learning. Experimental results on the
PDTB dataset demonstrate that our method remarkably outperforms the current
state-of-the-art model at all hierarchical levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Silver Standard Data for Zero-shot Relation Extraction. (arXiv:2211.13883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13883">
<div class="article-summary-box-inner">
<span><p>The superior performance of supervised relation extraction (RE) methods
heavily relies on a large amount of gold standard data. Recent zero-shot
relation extraction methods converted the RE task to other NLP tasks and used
off-the-shelf models of these NLP tasks to directly perform inference on the
test data without using a large amount of RE annotation data. A potentially
valuable by-product of these methods is the large-scale silver standard data.
However, there is no further investigation on the use of potentially valuable
silver standard data. In this paper, we propose to first detect a small amount
of clean data from silver standard data and then use the selected clean data to
finetune the pretrained model. We then use the finetuned model to infer
relation types. We also propose a class-aware clean data detection module to
consider class information when selecting clean data. The experimental results
show that our method can outperform the baseline by 12% and 11% on TACRED and
Wiki80 dataset in the zero-shot RE task. By using extra silver standard data of
different distributions, the performance can be further improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPA-Net: Generate A Dataset for Text to Physics-based Animation. (arXiv:2211.13887v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13887">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in Vision-Language (V&amp;L) joint research have achieved
remarkable results in various text-driven tasks. High-quality Text-to-video
(T2V), a task that has been long considered mission-impossible, was proven
feasible with reasonably good results in latest works. However, the resulting
videos often have undesired artifacts largely because the system is purely
data-driven and agnostic to the physical laws. To tackle this issue and further
push T2V towards high-level physical realism, we present an autonomous data
generation technique and a dataset, which intend to narrow the gap with a large
number of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the
dataset, we provide high-resolution 3D physical simulations for both solids and
fluids, along with textual descriptions of the physical phenomena. We take
advantage of state-of-the-art physical simulation methods (i) Incremental
Potential Contact (IPC) and (ii) Material Point Method (MPM) to simulate
diverse scenarios, including elastic deformations, material fractures,
collisions, turbulence, etc. Additionally, high-quality, multi-view rendering
videos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and
other communities. This work is the first step towards fully automated
Text-to-Video/Simulation (T2V/S). Live examples and subsequent work are at
https://sites.google.com/view/tpa-net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementary Explanations for Effective In-Context Learning. (arXiv:2211.13892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13892">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exhibited remarkable capabilities in
learning from explanations in prompts. Yet, there has been limited
understanding of what makes explanations effective for in-context learning.
This work aims to better understand the mechanisms by which explanations are
used for in-context learning. We first study the impact of two different
factors on prompting performance when using explanations: the computation trace
(the way the solution is decomposed) and the natural language of the prompt. By
perturbing explanations on three controlled tasks, we show that both factors
contribute to the effectiveness of explanations, indicating that LLMs do
faithfully follow the explanations to some extent. We further study how to form
maximally effective sets of explanations for solving a given test query. We
find that LLMs can benefit from the complementarity of the explanation set as
they are able to fuse different reasoning specified by individual exemplars in
prompts. Additionally, having relevant exemplars also contributes to more
effective prompts. Therefore, we propose a maximal-marginal-relevance-based
exemplar selection approach for constructing exemplar sets that are both
relevant as well as complementary, which successfully improves the in-context
learning performance across three real-world tasks on multiple LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts. (arXiv:2211.13896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13896">
<div class="article-summary-box-inner">
<span><p>Event detection (ED) identifies and classifies event triggers from
unstructured texts, serving as a fundamental task for information extraction.
Despite the remarkable progress achieved in the past several years, most
research efforts focus on detecting events from formal texts (e.g., news
articles, Wikipedia documents, financial announcements). Moreover, the texts in
each dataset are either from a single source or multiple yet relatively
homogeneous sources. With massive amounts of user-generated text accumulating
on the Web and inside enterprises, identifying meaningful events in these
informal texts, usually from multiple heterogeneous sources, has become a
problem of significant practical value. As a pioneering exploration that
expands event detection to the scenarios involving informal and heterogeneous
texts, we propose a new large-scale Chinese event detection dataset based on
user reviews, text conversations, and phone conversations in a leading
e-commerce platform for food service. We carefully investigate the proposed
dataset's textual informality and multi-source heterogeneity characteristics by
inspecting data samples quantitatively and qualitatively. Extensive experiments
with state-of-the-art event detection methods verify the unique challenges
posed by these characteristics, indicating that multi-source informal event
detection remains an open problem and requires further efforts. Our benchmark
and code are released at \url{https://github.com/myeclipse/MUSIED}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison Study Between Token Classification and Sequence Classification In Text Classification. (arXiv:2211.13899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13899">
<div class="article-summary-box-inner">
<span><p>Unsupervised Machine Learning techniques have been applied to Natural
Language Processing tasks and surpasses the benchmarks such as GLUE with great
success. Building language models approach achieves good results in one
language and it can be applied to multiple NLP task such as classification,
summarization, generation and etc as an out of box model. Among all the of the
classical approaches used in NLP, the masked language modeling is the most
used. In general, the only requirement to build a language model is presence of
the large corpus of textual data. Text classification engines uses a variety of
models from classical and state of art transformer models to classify texts for
in order to save costs. Sequence Classifiers are mostly used in the domain of
text classification. However Token classifiers also are viable candidate models
as well. Sequence Classifiers and Token Classifier both tend to improve the
classification predictions due to the capturing the context information
differently. This work aims to compare the performance of Sequence Classifier
and Token Classifiers and evaluate each model on the same set of data. In this
work, we are using a pre-trained model as the base model and Token Classifier
and Sequence Classier heads results of these two scoring paradigms with be
compared..
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Anomaly Detection Method in Textual Data. (arXiv:2211.13900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13900">
<div class="article-summary-box-inner">
<span><p>In this article, we propose using deep learning and transformer architectures
combined with classical machine learning algorithms to detect and identify text
anomalies in texts. Deep learning model provides a very crucial context
information about the textual data which all textual context are converted to a
numerical representation. We used multiple machine learning methods such as
Sentence Transformers, Auto Encoders, Logistic Regression and Distance
calculation methods to predict anomalies. The method are tested on the texts
data and we used syntactic data from different source injected into the
original text as anomalies or use them as target. Different methods and
algorithm are explained in the field of outlier detection and the results of
the best technique is presented. These results suggest that our algorithm could
potentially reduce false positive rates compared with other anomaly detection
methods that we are testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRAC: A Textual Benchmark for Reasoning about Actions and Change. (arXiv:2211.13930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13930">
<div class="article-summary-box-inner">
<span><p>Reasoning about actions and change (RAC) is essential to understand and
interact with the ever-changing environment. Previous AI research has shown the
importance of fundamental and indispensable knowledge of actions, i.e.,
preconditions and effects. However, traditional methods rely on logical
formalization which hinders practical applications. With recent
transformer-based language models (LMs), reasoning over text is desirable and
seemingly feasible, leading to the question of whether LMs can effectively and
efficiently learn to solve RAC problems. We propose four essential RAC tasks as
a comprehensive textual benchmark and generate problems in a way that minimizes
the influence of other linguistic requirements (e.g., grounding) to focus on
RAC. The resulting benchmark, TRAC, encompassing problems of various
complexities, facilitates a more granular evaluation of LMs, precisely
targeting the structural generalization ability much needed for RAC.
Experiments with three high-performing transformers indicates that additional
efforts are needed to tackle challenges raised by TRAC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10648">
<div class="article-summary-box-inner">
<span><p>Fake News on social media platforms has attracted a lot of attention in
recent times, primarily for events related to politics (2016 US Presidential
elections), healthcare (infodemic during COVID-19), to name a few. Various
methods have been proposed for detecting Fake News. The approaches span from
exploiting techniques related to network analysis, Natural Language Processing
(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose
DEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying
Fake News. Our approach is a combination of the NLP -- where we encode the news
content, and the GNN technique -- where we encode the Knowledge Graph (KG). A
variety of these encodings provides a complementary advantage to our detector.
We evaluate our framework using two publicly available datasets containing
articles from domains such as politics, business, technology, and healthcare.
As part of dataset pre-processing, we also remove the bias, such as the source
of the articles, which could impact the performance of the models. DEAP-FAKED
obtains an F1-score of 88% and 78% for the two datasets, which is an
improvement of 21%, and 3% respectively, which shows the effectiveness of the
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparison of latent semantic analysis and correspondence analysis of document-term matrices. (arXiv:2108.06197v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06197">
<div class="article-summary-box-inner">
<span><p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional
representations that capture relationships among documents and terms. In this
article, we present a theoretical analysis and comparison of the two techniques
in the context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins, i.e. sums
of row elements and column elements, arising from differing document-lengths
and term-frequencies are effectively eliminated, so that the CA solution is
optimally suited to focus on relationships among documents and terms. A
unifying framework is proposed that includes both CA and LSA as special cases.
We empirically compare CA to various LSA based methods on text categorization
in English and authorship attribution on historical Dutch texts, and find that
CA performs significantly better. We also apply CA to a long-standing question
regarding the authorship of the Dutch national anthem Wilhelmus and provide
further support that it can be attributed to the author Datheen, amongst
several contenders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misleading the Covid-19 vaccination discourse on Twitter: An exploratory study of infodemic around the pandemic. (arXiv:2108.10735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10735">
<div class="article-summary-box-inner">
<span><p>In this work, we collect a moderate-sized representative corpus of tweets
(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of
seven months (September 2020 - March 2021). Following a Transfer Learning
approach, we utilize the pre-trained Transformer-based XLNet model to classify
tweets as Misleading or Non-Misleading and validate against a random subset of
results manually. We build on this to study and contrast the characteristics of
tweets in the corpus that are misleading in nature against non-misleading ones.
This exploratory analysis enables us to design features (such as sentiments,
hashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying
tweets as (Non-)Misleading using various ML models in an explainable manner.
Specifically, several ML models are employed for prediction, with up to 90%
accuracy, and the importance of each feature is explained using SHAP
Explainable AI (XAI) tool. While the thrust of this work is principally
exploratory analysis in order to obtain insights on the online discourse on
Covid-19 vaccination, we conclude the paper by outlining how these insights
provide the foundations for a more actionable approach to mitigate
misinformation. The curated dataset and code is made available (Github
repository) so that the research community at large can reproduce, compare
against, or build upon this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Universal Intrinsic Task Subspace via Prompt Tuning. (arXiv:2110.07867v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07867">
<div class="article-summary-box-inner">
<span><p>Why can pre-trained language models (PLMs) learn universal representations
and effectively adapt to broad NLP tasks differing a lot superficially? In this
work, we empirically find evidence indicating that the adaptations of PLMs to
various few-shot tasks can be reparameterized as optimizing only a few free
parameters in a unified low-dimensional intrinsic task subspace, which may help
us understand why PLMs could easily adapt to various NLP tasks with small-scale
data. To find such a subspace and examine its universality, we propose an
analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort
to the recent success of prompt tuning and decompose the soft prompts of
multiple NLP tasks into the same low-dimensional nonlinear subspace, then we
learn to adapt the PLM to unseen data or tasks by only tuning parameters in
this subspace. In the experiments, we study diverse few-shot NLP tasks and
surprisingly find that in a 250-dimensional subspace found with 100 tasks, by
only tuning 250 free parameters, we can recover 97% and 83% of the full prompt
tuning performance for 100 seen tasks (using different training data) and 20
unseen tasks, respectively, showing great generalization ability of the found
intrinsic task subspace. Besides being an analysis tool, IPT could further help
us improve the prompt tuning stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge. (arXiv:2112.08544v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08544">
<div class="article-summary-box-inner">
<span><p>Claim detection and verification are crucial for news understanding and have
emerged as promising technologies for mitigating misinformation and
disinformation in the news. However, most existing work has focused on claim
sentence analysis while overlooking additional crucial attributes (e.g., the
claimer and the main object associated with the claim). In this work, we
present NewsClaims, a new benchmark for attribute-aware claim detection in the
news domain. We extend the claim detection problem to include extraction of
additional attributes related to each claim and release 889 claims annotated
over 143 news articles. NewsClaims aims to benchmark claim detection systems in
emerging scenarios, comprising unseen topics with little or no training data.
To this end, we see that zero-shot and prompt-based baselines show promising
performance on this benchmark, while still considerably behind human
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study. (arXiv:2202.00471v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00471">
<div class="article-summary-box-inner">
<span><p>Language data and models demonstrate various types of bias, be it ethnic,
religious, gender, or socioeconomic. AI/NLP models, when trained on the
racially biased dataset, AI/NLP models instigate poor model explainability,
influence user experience during decision making and thus further magnifies
societal biases, raising profound ethical implications for society. The
motivation of the study is to investigate how AI systems imbibe bias from data
and produce unexplainable discriminatory outcomes and influence an individual's
articulateness of system outcome due to the presence of racial bias features in
datasets. The design of the experiment involves studying the counterfactual
impact of racial bias features present in language datasets and its associated
effect on the model outcome. A mixed research methodology is adopted to
investigate the cross implication of biased model outcome on user experience,
effect on decision-making through controlled lab experimentation. The findings
provide foundation support for correlating the implication of carry-over an
artificial intelligence model solving NLP task due to biased concept presented
in the dataset. Further, the research outcomes justify the negative influence
on users' persuasiveness that leads to alter the decision-making quotient of an
individual when trying to rely on the model outcome to act. The paper bridges
the gap across the harm caused in establishing poor customer trustworthiness
due to an inequitable system design and provides strong support for
researchers, policymakers, and data scientists to build responsible AI
frameworks within organizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Failures of Large Language Models via Human Cognitive Biases. (arXiv:2202.12299v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12299">
<div class="article-summary-box-inner">
<span><p>Large language models generate complex, open-ended outputs: instead of
outputting a class label they write summaries, generate dialogue, or produce
working code. In order to asses the reliability of these open-ended generation
systems, we aim to identify qualitative categories of erroneous behavior,
beyond identifying individual errors. To hypothesize and test for such
qualitative errors, we draw inspiration from human cognitive biases --
systematic patterns of deviation from rational judgement. Specifically, we use
cognitive biases as motivation to (i) generate hypotheses for problems that
models may have, and (ii) develop experiments that elicit these problems. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to elicit high-impact errors such as incorrectly deleting files.
Our results indicate that experimental methodology from cognitive science can
help characterize how machine learning systems behave.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transition-based Semantic Role Labeling with Pointer Networks. (arXiv:2205.10023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10023">
<div class="article-summary-box-inner">
<span><p>Semantic role labeling (SRL) focuses on recognizing the predicate-argument
structure of a sentence and plays a critical role in many natural language
processing tasks such as machine translation and question answering.
Practically all available methods do not perform full SRL, since they rely on
pre-identified predicates, and most of them follow a pipeline strategy, using
specific models for undertaking one or several SRL subtasks. In addition,
previous approaches have a strong dependence on syntactic information to
achieve state-of-the-art performance, despite being syntactic trees equally
hard to produce. These simplifications and requirements make the majority of
SRL systems impractical for real-world applications. In this article, we
propose the first transition-based SRL approach that is capable of completely
processing an input sentence in a single left-to-right pass, with neither
leveraging syntactic information nor resorting to additional modules. Thanks to
our implementation based on Pointer Networks, full SRL can be accurately and
efficiently done in $O(n^2)$, achieving the best performance to date on the
majority of languages from the CoNLL-2009 shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI. (arXiv:2205.11029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11029">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems have been widely used by mobile phone
intelligent assistants to accomplish tasks such as calendar scheduling or hotel
reservation. Current TOD systems usually focus on multi-turn text/speech
interaction, then they would call back-end APIs designed for TODs to perform
the task. However, this API-based architecture greatly limits the
information-searching capability of intelligent assistants and may even lead to
task failure if TOD-specific APIs are not available or the task is too
complicated to be executed by the provided APIs. In this paper, we propose a
new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A
GUI-TOD system can directly perform GUI operations on real APPs and execute
tasks without invoking TOD-specific backend APIs. Furthermore, we release
META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile
GUI. We also propose a multi-model action prediction and response model, which
show promising results on META-GUI. The dataset, codes and leaderboard are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04510">
<div class="article-summary-box-inner">
<span><p>The academic literature of social sciences records human civilization and
studies human social problems. With its large-scale growth, the ways to quickly
find existing research on relevant issues have become an urgent demand for
researchers. Previous studies, such as SciBERT, have shown that pre-training
using domain-specific texts can improve the performance of natural language
processing tasks. However, the pre-trained language model for social sciences
is not available so far. In light of this, the present research proposes a
pre-trained model based on the abstracts published in the Social Science
Citation Index (SSCI) journals. The models, which are available on GitHub
(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent
performance on discipline classification, abstract structure-function
recognition, and named entity recognition tasks with the social sciences
literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08657">
<div class="article-summary-box-inner">
<span><p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose Bridge-Tower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, Bridge-Tower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, Bridge-Tower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, Bridge-Tower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
\url{https://github.com/microsoft/BridgeTower}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09059">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art vision-and-language models are evaluated on tasks
either individually or in a multi-task setting, overlooking the challenges of
continually learning (CL) tasks as they arrive. Existing CL benchmarks have
facilitated research on task adaptation and mitigating "catastrophic
forgetting", but are limited to vision-only and language-only tasks. We present
CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL
setting, and to systematically evaluate how upstream continual learning can
rapidly generalize to new multimodal and unimodal tasks. CLiMB includes
implementations of several CL algorithms and a modified Vision-Language
Transformer (ViLT) model that can be deployed on both multimodal and unimodal
tasks. We find that common CL methods can help mitigate forgetting during
multimodal task learning, but do not enable cross-task knowledge transfer. We
envision that CLiMB will facilitate research on a new class of CL algorithms
for this challenging multimodal setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. (arXiv:2207.01206v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01206">
<div class="article-summary-box-inner">
<span><p>Existing benchmarks for grounding language in interactive environments either
lack real-world linguistic elements, or prove difficult to scale up due to
substantial human involvement in the collection of data or feedback signals. To
bridge this gap, we develop WebShop -- a simulated e-commerce website
environment with $1.18$ million real-world products and $12,087$ crowd-sourced
text instructions. Given a text instruction specifying a product requirement,
an agent needs to navigate multiple types of webpages and issue diverse actions
to find, customize, and purchase an item. WebShop provides several challenges
for language grounding including understanding compositional instructions,
query (re-)formulation, comprehending and acting on noisy text in webpages, and
performing strategic exploration. We collect over $1,600$ human demonstrations
for the task, and train and evaluate a diverse range of agents using
reinforcement learning, imitation learning, and pre-trained image and language
models. Our best model achieves a task success rate of $29\%$, which
outperforms rule-based heuristics ($9.6\%$) but is far lower than human expert
performance ($59\%$). We also analyze agent and human trajectories and ablate
various model components to provide insights for developing future agents with
stronger language understanding and decision making abilities. Finally, we show
that agents trained on WebShop exhibit non-trivial sim-to-real transfer when
evaluated on amazon.com and ebay.com, indicating the potential value of WebShop
in developing practical web-based agents that can operate in the wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08562">
<div class="article-summary-box-inner">
<span><p>In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement. (arXiv:2208.08965v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08965">
<div class="article-summary-box-inner">
<span><p>Grounded Situation Recognition (GSR) aims to generate structured semantic
summaries of images for "human-like" event understanding. Specifically, GSR
task not only detects the salient activity verb (e.g. buying), but also
predicts all corresponding semantic roles (e.g. agent and goods). Inspired by
object detection and image captioning tasks, existing methods typically employ
a two-stage framework: 1) detect the activity verb, and then 2) predict
semantic roles based on the detected verb. Obviously, this illogical framework
constitutes a huge obstacle to semantic understanding. First, pre-detecting
verbs solely without semantic roles inevitably fails to distinguish many
similar daily activities (e.g., offering and giving, buying and selling).
Second, predicting semantic roles in a closed auto-regressive manner can hardly
exploit the semantic relations among the verb and roles. To this end, in this
paper we propose a novel two-stage framework that focuses on utilizing such
bidirectional relations within verbs and roles. In the first stage, instead of
pre-detecting the verb, we postpone the detection step and assume a pseudo
label, where an intermediate representation for each corresponding semantic
role is learned from images. In the second stage, we exploit transformer layers
to unearth the potential semantic relations within both verbs and semantic
roles. With the help of a set of support images, an alternate learning scheme
is designed to simultaneously optimize the results: update the verb using nouns
corresponding to the image, and update nouns using verbs from support images.
Extensive experimental results on challenging SWiG benchmarks show that our
renovated framework outperforms other state-of-the-art methods under various
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PreSTU: Pre-Training for Scene-Text Understanding. (arXiv:2209.05534v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05534">
<div class="article-summary-box-inner">
<span><p>The ability to recognize and reason about text embedded in visual inputs is
often lacking in vision-and-language (V&amp;L) models, perhaps because V&amp;L
pre-training methods have often failed to include such an ability as their
training objective. In this paper, we propose PreSTU, a novel pre-training
recipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware
pre-training objectives that encourage the model to recognize text from an
image and to connect what is recognized to the rest of the image content. We
implement PreSTU using a simple transformer-based encoder-decoder architecture,
combined with large-scale image-text datasets with scene text obtained from an
off-the-shelf OCR system. We empirically demonstrate the effectiveness of this
pre-training approach on four visual question answering and two image
captioning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. (arXiv:2210.01241v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01241">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of aligning pre-trained large language models (LMs)
with human preferences. If we view text generation as a sequential
decision-making problem, reinforcement learning (RL) appears to be a natural
conceptual framework. However, using RL for LM-based generation faces empirical
challenges, including training instability due to the combinatorial action
space, as well as a lack of open-source libraries and benchmarks customized for
LM alignment. Thus, a question rises in the research community: is RL a
practical paradigm for NLP?
</p>
<p>To help answer this, we first introduce an open-source modular library,
RL4LMs (Reinforcement Learning for Language Models), for optimizing language
generators with RL. The library consists of on-policy RL algorithms that can be
used to train any encoder or encoder-decoder LM in the HuggingFace library
(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE
(General Reinforced-language Understanding Evaluation) benchmark, a set of 6
language generation tasks which are supervised not by target strings, but by
reward functions which capture automated measures of human preference.GRUE is
the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,
we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language
Policy Optimization)} that learns to effectively reduce the combinatorial
action space in language generation. We show 1) that RL techniques are
generally better than supervised methods at aligning LMs to human preferences;
and 2) that NLPO exhibits greater stability and performance than previous
policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both
automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. (arXiv:2210.07783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07783">
<div class="article-summary-box-inner">
<span><p>Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD)
systems. To address the catastrophic forgetting issue of LL, generative replay
methods are widely employed to consolidate past knowledge with generated pseudo
samples. However, most existing generative replay methods use only a single
task-specific token to control their models. This scheme is usually not strong
enough to constrain the generative model due to insufficient information
involved. In this paper, we propose a novel method, prompt conditioned VAE for
lifelong learning (PCLL), to enhance generative replay by incorporating tasks'
statistics. PCLL captures task-specific distributions with a conditional
variational autoencoder, conditioned on natural language prompts to guide the
pseudo-sample generation. Moreover, it leverages a distillation process to
further consolidate past knowledge by alleviating the noise in pseudo samples.
Experiments on natural language understanding tasks of ToD systems demonstrate
that PCLL significantly outperforms competitive baselines in building LL
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU. (arXiv:2210.12499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12499">
<div class="article-summary-box-inner">
<span><p>Curriculum Learning (CL) is a technique of training models via ranking
examples in a typically increasing difficulty trend with the aim of
accelerating convergence and improving generalisability. Current approaches for
Natural Language Understanding (NLU) tasks use CL to improve in-distribution
data performance often via heuristic-oriented or task-agnostic difficulties. In
this work, instead, we employ CL for NLU by taking advantage of training
dynamics as difficulty metrics, i.e., statistics that measure the behavior of
the model at hand on specific task-data instances during training and propose
modifications of existing CL schedulers based on these statistics. Differently
from existing works, we focus on evaluating models on in-distribution (ID),
out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer
datasets. We show across several NLU tasks that CL with training dynamics can
result in better performance mostly on zero-shot cross-lingual transfer and OOD
settings with improvements up by 8.5% in certain cases. Overall, experiments
indicate that training dynamics can lead to better performing models with
smoother training compared to other difficulty metrics while being 20% faster
on average. In addition, through analysis we shed light on the correlations of
task-specific versus task-agnostic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Joint Training Really Help Cascaded Speech Translation?. (arXiv:2210.13700v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13700">
<div class="article-summary-box-inner">
<span><p>Currently, in speech translation, the straightforward approach - cascading a
recognition system with a translation system - delivers state-of-the-art
results. However, fundamental challenges such as error propagation from the
automatic speech recognition system still remain. To mitigate these problems,
recently, people turn their attention to direct data and propose various joint
training methods. In this work, we seek to answer the question of whether joint
training really helps cascaded speech translation. We review recent papers on
the topic and also investigate a joint training criterion by marginalizing the
transcription posterior probabilities. Our findings show that a strong cascaded
baseline can diminish any improvements obtained using joint training, and we
suggest alternatives to joint training. We hope this work can serve as a
refresher of the current speech translation landscape, and motivate research in
finding more efficient and creative ways to utilize the direct data for speech
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. (arXiv:2210.15212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15212">
<div class="article-summary-box-inner">
<span><p>We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to
improve the generalization ability of dense retrieval by combating the
distribution shifts between source training tasks and target scenarios. To
mitigate the impact of document differences, COCO-DR continues pretraining the
language model on the target corpora to adapt the model to target distributions
via COtinuous COtrastive learning. To prepare for unseen target queries,
COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to
reweight samples from different source query clusters for improving model
robustness over rare queries during fine-tuning. COCO-DR achieves superior
average performance on BEIR, the zero-shot retrieval benchmark. At BERT Base
scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At
BERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model
which has 500x more parameters. Our analysis show the correlation between
COCO-DR's effectiveness in combating distribution shifts and improving
zero-shot accuracy. Our code and model can be found at
\url{https://github.com/OpenMatch/COCO-DR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages. (arXiv:2211.03263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03263">
<div class="article-summary-box-inner">
<span><p>In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSCD-IME: Correcting Spelling Errors Generated by Pinyin IME. (arXiv:2211.08788v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08788">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Correction (CSC) is a task to detect and correct spelling
mistakes in texts. In fact, most of Chinese input is based on pinyin input
method, so the study of spelling errors in this process is more practical and
valuable. However, there is still no research dedicated to this essential
scenario. In this paper, we first present a Chinese Spelling Correction Dataset
for errors generated by pinyin IME (CSCD-IME), including 40,000 annotated
sentences from real posts of official media on Sina Weibo. Furthermore, we
propose a novel method to automatically construct large-scale and high-quality
pseudo data by simulating the input through pinyin IME. A series of analyses
and experiments on CSCD-IME show that spelling errors produced by pinyin IME
hold a particular distribution at pinyin level and semantic level and are
challenging enough. Meanwhile, our proposed pseudo-data construction method can
better fit this error distribution and improve the performance of CSC systems.
Finally, we provide a useful guide to using pseudo data, including the data
scale, the data source, and the training strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness. (arXiv:2211.11109v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11109">
<div class="article-summary-box-inner">
<span><p>Data-driven predictive solutions predominant in commercial applications tend
to suffer from biases and stereotypes, which raises equity concerns. Prediction
models may discover, use, or amplify spurious correlations based on gender or
other protected personal characteristics, thus discriminating against
marginalized groups. Mitigating gender bias has become an important research
focus in natural language processing (NLP) and is an area where annotated
corpora are available. Data augmentation reduces gender bias by adding
counterfactual examples to the training dataset. In this work, we show that
some of the examples in the augmented dataset can be not important or even
harmful for fairness. We hence propose a general method for pruning both the
factual and counterfactual examples to maximize the model's fairness as
measured by the demographic parity, equality of opportunity, and equality of
odds. The fairness achieved by our method surpasses that of data augmentation
on three text classification datasets, using no more than half of the examples
in the augmented dataset. Our experiments are conducted using models of varying
sizes and pre-training settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems. (arXiv:2211.11982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11982">
<div class="article-summary-box-inner">
<span><p>We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for
commercial text-based task-oriented dialog (TOD) systems. BotSIM consists of
three major components: 1) a Generator that can infer semantic-level dialog
acts and entities from bot definitions and generate user queries via
model-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to
simulate conversations with the dialog agents; 3) a Remediator to analyze the
simulated conversations, visualize the bot health reports and provide
actionable remediation suggestions for bot troubleshooting and improvement. We
demonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and
multi-intent dialog generation via case studies on two commercial bot
platforms. BotSIM's "generation-simulation-remediation" paradigm accelerates
the end-to-end bot evaluation and iteration process by: 1) reducing manual test
cases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU
and end-to-end performance via extensive dialog simulation; 3) improving the
bot troubleshooting process with actionable suggestions. A demo of our system
can be found at https://tinyurl.com/mryu74cd and a demo video at
https://youtu.be/qLi5iSoly30.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk. (arXiv:2211.12118v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12118">
<div class="article-summary-box-inner">
<span><p>One of the challenges of developing a summarization model arises from the
difficulty in measuring the factual inconsistency of the generated text. In
this study, we reinterpret the decoder overconfidence-regularizing objective
suggested in (Miao et al., 2021) as a hallucination risk measurement to better
estimate the quality of generated summaries. We propose a reference-free
metric, HaRiM+, which only requires an off-the-shelf summarization model to
compute the hallucination risk based on token likelihoods. Deploying it
requires no additional training of models or ad-hoc modules, which usually need
alignment to human judgments. For summary-quality estimation, HaRiM+ records
state-of-the-art correlation to human judgment on three summary-quality
annotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits
the use of summarization models, facilitates the progress of both automated
evaluation and generation of summary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12588">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in teaching language models to
perform step-by-step reasoning to solve complex numerical reasoning tasks.
Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these
tasks. CoT uses language models to perform both reasoning and computation in
the multi-step `thought' process. To disentangle computation from reasoning, we
propose `Program of Thoughts' (PoT), which uses language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to
an external computer, which executes the generated programs to derive the
answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,
TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)
for both few-shot and zero-shot setups. Under both few-shot and zero-shot
settings, PoT can show an average performance gain over CoT by around 12\%
across all the evaluated datasets. By combining PoT with self-consistency
decoding, we can achieve SoTA performance on all math problem datasets and
near-SoTA performance on financial datasets. All of our data and code are
released in
Github\footnote{\url{https://github.com/wenhuchen/Program-of-Thoughts}}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-28 23:14:06.182564698 UTC">2022-11-28 23:14:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>