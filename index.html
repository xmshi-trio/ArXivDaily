<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-23T01:30:00Z">01-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis for Measuring Hope and Fear from Reddit Posts During the 2022 Russo-Ukrainian Conflict. (arXiv:2301.08347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08347">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel lexicon-based unsupervised sentimental analysis
method to measure the $``\textit{hope}"$ and $``\textit{fear}"$ for the 2022
Ukrainian-Russian Conflict. $\textit{Reddit.com}$ is utilised as the main
source of human reactions to daily events during nearly the first three months
of the conflict. The top 50 $``hot"$ posts of six different subreddits about
Ukraine and news (Ukraine, worldnews, Ukraina, UkrainianConflict,
UkraineWarVideoReport, UkraineWarReports) and their relative comments are
scraped and a data set is created. On this corpus, multiple analyses such as
(1) public interest, (2) hope/fear score, (3) stock price interaction are
employed. We promote using a dictionary approach, which scores the hopefulness
of every submitted user post. The Latent Dirichlet Allocation (LDA) algorithm
of topic modelling is also utilised to understand the main issues raised by
users and what are the key talking points. Experimental analysis shows that the
hope strongly decreases after the symbolic and strategic losses of Azovstal
(Mariupol) and Severodonetsk. Spikes in hope/fear, both positives and
negatives, are present after important battles, but also some non-military
events, such as Eurovision and football games.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation for Accessible Multi-Language Text Analysis. (arXiv:2301.08416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08416">
<div class="article-summary-box-inner">
<span><p>English is the international standard of social research, but scholars are
increasingly conscious of their responsibility to meet the need for scholarly
insight into communication processes globally. This tension is as true in
computational methods as any other area, with revolutionary advances in the
tools for English language texts leaving most other languages far behind. In
this paper, we aim to leverage those very advances to demonstrate that
multi-language analysis is currently accessible to all computational scholars.
We show that English-trained measures computed after translation to English
have adequate-to-excellent accuracy compared to source-language measures
computed on original texts. We show this for three major analytics -- sentiment
analysis, topic analysis, and word embeddings -- over 16 languages, including
Spanish, Chinese, Hindi, and Arabic. We validate this claim by comparing
predictions on original language tweets and their backtranslations: double
translations from their source language to English and back to the source
language. Overall, our results suggest that Google Translate, a simple and
widely accessible tool, is effective in preserving semantic content across
languages and methods. Modern machine translation can thus help computational
scholars make more inclusive and general claims about human communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08427">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Agnostic Data-Driven Inverse Text Normalization. (arXiv:2301.08506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08506">
<div class="article-summary-box-inner">
<span><p>With the emergence of automatic speech recognition (ASR) models, converting
the spoken form text (from ASR) to the written form is in urgent need. This
inverse text normalization (ITN) problem attracts the attention of researchers
from various fields. Recently, several works show that data-driven ITN methods
can output high-quality written form text. Due to the scarcity of labeled
spoken-written datasets, the studies on non-English data-driven ITN are quite
limited. In this work, we propose a language-agnostic data-driven ITN framework
to fill this gap. Specifically, we leverage the data augmentation in
conjunction with neural machine translated data for low resource languages.
Moreover, we design an evaluation method for language agnostic ITN model when
only English data is available. Our empirical evaluation shows this language
agnostic modeling approach is effective for low resource languages while
preserving the performance for high resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Unstructured Text into Data with Context Rule Assisted Machine Learning (CRAML). (arXiv:2301.08549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08549">
<div class="article-summary-box-inner">
<span><p>We describe a method and new no-code software tools enabling domain experts
to build custom structured, labeled datasets from the unstructured text of
documents and build niche machine learning text classification models traceable
to expert-written rules. The Context Rule Assisted Machine Learning (CRAML)
method allows accurate and reproducible labeling of massive volumes of
unstructured text. CRAML enables domain experts to access uncommon constructs
buried within a document corpus, and avoids limitations of current
computational approaches that often lack context, transparency, and
interpetability. In this research methods paper, we present three use cases for
CRAML: we analyze recent management literature that draws from text data,
describe and release new machine learning models from an analysis of
proprietary job advertisement text, and present findings of social and economic
interest from a public corpus of franchise documents. CRAML produces
document-level coded tabular datasets that can be used for quantitative
academic research, and allows qualitative researchers to scale niche
classification schemes over massive text data. CRAML is a low-resource,
flexible, and scalable methodology for building training data for supervised
ML. We make available as open-source resources: the software, job advertisement
text classifiers, a novel corpus of franchise documents, and a fully replicable
start-to-finish trained example in the context of no poach clauses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. (arXiv:2301.08571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08571">
<div class="article-summary-box-inner">
<span><p>Current work on image-based story generation suffers from the fact that the
existing image sequence collections do not have coherent plots behind them. We
improve visual story generation by producing a new image-grounded dataset,
Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a
total of 12K stories which were collected via crowdsourcing given the image
sequences and a set of grounded characters from the corresponding image
sequence. Our new image sequence collection and filtering process has allowed
us to obtain stories that are more coherent and have more narrativity compared
to previous work. We also propose a character-based story generation model
driven by coherence as a strong baseline. Evaluations show that our generated
stories are more coherent, visually grounded, and have more narrativity than
stories generated with the current state-of-the-art model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Modeling Human Personality: The Dexter Machine. (arXiv:2301.08606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08606">
<div class="article-summary-box-inner">
<span><p>Modeling human personality is important for several AI challenges, from the
engineering of artificial psychotherapists to the design of persona bots.
However, the field of computational personality analysis heavily relies on
labeled data, which may be expensive, difficult or impossible to get. This
problem is amplified when dealing with rare personality types or disorders
(e.g., the anti-social psychopathic personality disorder). In this context, we
developed a text-based data augmentation approach for human personality
(PEDANT). PEDANT doesn't rely on the common type of labeled data but on the
generative pre-trained model (GPT) combined with domain expertise. Testing the
methodology on three different datasets, provides results that support the
quality of the generated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reversing The Twenty Questions Game. (arXiv:2301.08718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08718">
<div class="article-summary-box-inner">
<span><p>Twenty questions is a widely popular verbal game. In recent years, many
computerized versions of this game have been developed in which a user thinks
of an entity and a computer attempts to guess this entity by asking a series of
boolean-type (yes/no) questions. In this research, we aim to reverse this game
by making the computer choose an entity at random. The human aims to guess this
entity by quizzing the computer with natural language queries which the
computer will then attempt to parse using a boolean question answering model.
The game ends when the human is successfully able to guess the entity of the
computer's choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08721">
<div class="article-summary-box-inner">
<span><p>Performing inference on hundreds of thousands of samples with large language
models (LLMs) can be computationally and financially costly. We propose batch
prompting, a simple alternative prompting approach that enables the LLM to run
inference in batches, instead of one sample at a time. Our method reduces both
token and time costs while retaining downstream performance. We theoretically
demonstrate that under a few-shot in-context learning setting, the inference
costs decrease almost inverse linearly with the number of samples in each
batch. We extensively validate the effectiveness of batch prompting on ten
datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch
prompting significantly~(up to $5\times$ with six samples in batch) reduces the
LLM (Codex) inference token and time costs while achieving better or comparable
performance. Our analysis shows that the number of samples in each batch and
the complexity of tasks affect its performance. Further, batch prompting can be
applied across different LLMs and reasoning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Peanuts Fall in Love with Distributional Semantics?. (arXiv:2301.08731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08731">
<div class="article-summary-box-inner">
<span><p>The context in which a sentence appears can drastically alter our
expectations about upcoming words - for example, following a short story
involving an anthropomorphic peanut, experimental participants are more likely
to expect the sentence 'the peanut was in love' than 'the peanut was salted',
as indexed by N400 amplitude (Nieuwland &amp; van Berkum, 2006). This rapid and
dynamic updating of comprehenders' expectations about the kind of events that a
peanut may take part in based on context has been explained using the construct
of Situation Models - updated mental representations of key elements of an
event under discussion, in this case, the peanut protagonist. However, recent
work showing that N400 amplitude can be predicted based on distributional
information alone raises the question whether situation models are in fact
necessary for the kinds of contextual effects observed in previous work. To
investigate this question, we attempt to model the results of Nieuwland and van
Berkum (2006) using six computational language models and three sets of word
vectors, none of which have explicit situation models or semantic grounding. We
find that the effect found by Nieuwland and van Berkum (2006) can be fully
modeled by two language models and two sets of word vectors, with others
showing a reduced effect. Thus, at least some processing effects normally
explained through situation models may not in fact require explicit situation
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Dialogue Breakdown Detection with Semi-Supervised Learning. (arXiv:2011.00136v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00136">
<div class="article-summary-box-inner">
<span><p>Building user trust in dialogue agents requires smooth and consistent
dialogue exchanges. However, agents can easily lose conversational context and
generate irrelevant utterances. These situations are called dialogue breakdown,
where agent utterances prevent users from continuing the conversation. Building
systems to detect dialogue breakdown allows agents to recover appropriately or
avoid breakdown entirely. In this paper we investigate the use of
semi-supervised learning methods to improve dialogue breakdown detection,
including continued pre-training on the Reddit dataset and a manifold-based
data augmentation method. We demonstrate the effectiveness of these methods on
the Dialogue Breakdown Detection Challenge (DBDC) English shared task. Our
submissions to the 2020 DBDC5 shared task place first, beating baselines and
other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019,
our semi-supervised learning methods improve the performance of a baseline BERT
model by 2\% accuracy. These methods are applicable generally to any dialogue
task and provide a simple way to improve model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Data Augmentation Methods for Low-Resource Maltese ASR. (arXiv:2111.07793v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07793">
<div class="article-summary-box-inner">
<span><p>Recent years have seen an increased interest in the computational speech
processing of Maltese, but resources remain sparse. In this paper, we consider
data augmentation techniques for improving speech recognition for low-resource
languages, focusing on Maltese as a test case. We consider three different
types of data augmentation: unsupervised training, multilingual training and
the use of synthesized speech as training data. The goal is to determine which
of these techniques, or combination of them, is the most effective to improve
speech recognition for languages where the starting point is a small corpus of
approximately 7 hours of transcribed speech. Our results show that combining
the data augmentation techniques studied here lead us to an absolute WER
improvement of 15% without the use of a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. (arXiv:2206.10128v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10128">
<div class="article-summary-box-inner">
<span><p>The Differentiable Search Index (DSI) is an emerging paradigm for information
retrieval. Unlike traditional retrieval architectures where index and retrieval
are two different and separate components, DSI uses a single transformer model
to perform both indexing and retrieval.
</p>
<p>In this paper, we identify and tackle an important issue of current DSI
models: the data distribution mismatch that occurs between the DSI indexing and
retrieval processes. Specifically, we argue that, at indexing, current DSI
methods learn to build connections between the text of long documents and the
identifier of the documents, but then retrieval of document identifiers is
based on queries that are commonly much shorter than the indexed documents.
This problem is further exacerbated when using DSI for cross-lingual retrieval,
where document text and query text are in different languages.
</p>
<p>To address this fundamental problem of current DSI models, we propose a
simple yet effective indexing framework for DSI, called DSI-QG. When indexing,
DSI-QG represents documents with a number of potentially relevant queries
generated by a query generation model and re-ranked and filtered by a
cross-encoder ranker. The presence of these queries at indexing allows the DSI
models to connect a document identifier to a set of queries, hence mitigating
data distribution mismatches present between the indexing and the retrieval
phases. Empirical results on popular mono-lingual and cross-lingual passage
retrieval datasets show that DSI-QG significantly outperforms the original DSI
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08241">
<div class="article-summary-box-inner">
<span><p>Bootstrapping from pre-trained language models has been proven to be an
efficient approach for building vision-language models (VLM) for tasks such as
image captioning or visual question answering. However, outputs of these models
rarely align with user's rationales for specific answers. In order to improve
this alignment and reinforce commonsense reasons, we propose a tuning paradigm
based on human interactions with machine generated data. Our ILLUME executes
the following loop: Given an image-question-answer prompt, the VLM samples
multiple candidate rationales, and a human critic provides minimal feedback via
preference selection, used for fine-tuning. This loop increases the training
data and gradually carves out the VLM's rationalization capabilities that are
aligned with human intend. Our exhaustive experiments demonstrate that ILLUME
is competitive with standard supervised fine-tuning while using significantly
fewer training data and only requiring minimal feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state. (arXiv:2211.10247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10247">
<div class="article-summary-box-inner">
<span><p>Extracting summaries from long documents can be regarded as sentence
classification using the structural information of the documents. How to use
such structural information to summarize a document is challenging. In this
paper, we propose GoSum, a novel graph and reinforcement learning based
extractive model for long-paper summarization. In particular, GoSum encodes
sentence states in reinforcement learning by building a heterogeneous graph for
each input document at different discourse levels. An edge in the graph
reflects the discourse hierarchy of a document for restraining the semantic
drifts across section boundaries. We evaluate GoSum on two datasets of
scientific articles summarization: PubMed and arXiv. The experimental results
have demonstrated that GoSum achieve state-of-the-art results compared with
strong baselines of both extractive and abstractive models. The ablation
studies further validate that the performance of our GoSum benefits from the
use of discourse information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards continually learning new languages. (arXiv:2211.11703v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11703">
<div class="article-summary-box-inner">
<span><p>Multilingual speech recognition with neural networks is often implemented
with batch-learning, when all of the languages are available before training.
An ability to add new languages after the prior training sessions can be
economically beneficial, but the main challenge is catastrophic forgetting. In
this work, we combine the qualities of weight factorization and elastic weight
consolidation in order to counter catastrophic forgetting and facilitate
learning new languages quickly. Such combination allowed us to eliminate
catastrophic forgetting while still achieving performance for the new languages
comparable with having all languages at once, in experiments of learning from
an initial 10 languages to achieve 26 languages without catastrophic forgetting
and a reasonable performance compared to training all languages from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models. (arXiv:2212.01282v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01282">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) is a powerful technique for learning
representations from unlabeled data. Transformer based models such as HuBERT,
which consist a feature extractor and transformer layers, are leading the field
in the speech domain. SSL models are fine-tuned on a wide range of downstream
tasks, which involves re-training the majority of the model for each task.
Previous studies have introduced applying adapters, which are small lightweight
modules commonly used in Natural Language Processing (NLP) to adapt pre-trained
models to new tasks. However, such efficient tuning techniques only provide
adaptation at the transformer layer, but failed to perform adaptation at the
feature extractor. In this paper, we propose CHAPTER, an efficient tuning
method specifically designed for SSL speech model, by applying CNN adapters at
the feature extractor. Using this method, we can only fine-tune fewer than 5%
of parameters per task compared to fully fine-tuning and achieve better and
more stable performance. We empirically found that adding CNN adapters to the
feature extractor can help the adaptation on emotion and speaker tasks. For
instance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy
of ER is improved by 5%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-23 23:12:20.560045515 UTC">2023-01-23 23:12:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>