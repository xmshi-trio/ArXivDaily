<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-27T01:30:00Z">06-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13734">
<div class="article-summary-box-inner">
<span><p>The CHiME challenges have played a significant role in the development and
evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7
distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises
joint ASR and diarization in far-field settings with multiple, and possibly
heterogeneous, recording devices. Different from previous challenges, we
evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal
is for participants to devise a single system that can generalize across
different array geometries and use cases with no a-priori information. Another
departure from earlier CHiME iterations is that participants are allowed to use
open-source pre-trained models and datasets. In this paper, we describe the
challenge design, motivation, and fundamental research questions in detail. We
also present the baseline system, which is fully array-topology agnostic and
features multi-channel diarization, channel selection, guided source separation
and a robust ASR model that leverages self-supervised speech representations
(SSLR).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resume Information Extraction via Post-OCR Text Processing. (arXiv:2306.13775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13775">
<div class="article-summary-box-inner">
<span><p>Information extraction (IE), one of the main tasks of natural language
processing (NLP), has recently increased importance in the use of resumes. In
studies on the text to extract information from the CV, sentence classification
was generally made using NLP models. In this study, it is aimed to extract
information by classifying all of the text groups after pre-processing such as
Optical Character Recognition (OCT) and object recognition with the YOLOv8
model of the resumes. The text dataset consists of 286 resumes collected for 5
different (education, experience, talent, personal and language) job
descriptions in the IT industry. The dataset created for object recognition
consists of 1198 resumes, which were collected from the open-source internet
and labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were
used as models. F1 score variances were used to compare the model results. In
addition, the YOLOv8 model has also been reported comparatively in itself. As a
result of the comparison, DistilBERT was showed better results despite having a
lower number of parameters than other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13789">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) models have become increasingly popular in
real-world applications, such as text classification. However, they are
vulnerable to privacy attacks, including data reconstruction attacks that aim
to extract the data used to train the model. Most previous studies on data
reconstruction attacks have focused on LLM, while classification models were
assumed to be more secure. In this work, we propose a new targeted data
reconstruction attack called the Mix And Match attack, which takes advantage of
the fact that most classification models are based on LLM. The Mix And Match
attack uses the base model of the target model to generate candidate tokens and
then prunes them using the classification head. We extensively demonstrate the
effectiveness of the attack using both random and organic canaries. This work
highlights the importance of considering the privacy risks associated with data
reconstruction attacks in classification models and offers insights into
possible leakages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An analysis of vaccine-related sentiments from development to deployment of COVID-19 vaccines. (arXiv:2306.13797v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13797">
<div class="article-summary-box-inner">
<span><p>Anti-vaccine sentiments have been well-known and reported throughout the
history of viral outbreaks and vaccination programmes. The COVID-19 pandemic
had fear and uncertainty about vaccines which has been well expressed on social
media platforms such as Twitter. We analyse Twitter sentiments from the
beginning of the COVID-19 pandemic and study the public behaviour during the
planning, development and deployment of vaccines expressed in tweets worldwide
using a sentiment analysis framework via deep learning models. In this way, we
provide visualisation and analysis of anti-vaccine sentiments over the course
of the COVID-19 pandemic. Our results show a link between the number of tweets,
the number of cases, and the change in sentiment polarity scores during major
waves of COVID-19 cases. We also found that the first half of the pandemic had
drastic changes in the sentiment polarity scores that later stabilised which
implies that the vaccine rollout had an impact on the nature of discussions on
social media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13804">
<div class="article-summary-box-inner">
<span><p>Despite the recent progress in speech emotion recognition (SER),
state-of-the-art systems are unable to achieve improved performance in
cross-language settings. In this paper, we propose a Multimodal Dual Attention
Transformer (MDAT) model to improve cross-language SER. Our model utilises
pre-trained models for multimodal feature extraction and is equipped with a
dual attention mechanism including graph attention and co-attention to capture
complex dependencies across different modalities and achieve improved
cross-language SER results using minimal target language data. In addition, our
model also exploits a transformer encoder layer for high-level feature
representation to improve emotion classification accuracy. In this way, MDAT
performs refinement of feature representation at various stages and provides
emotional salient features to the classification layer. This novel approach
also ensures the preservation of modality-specific emotional information while
enhancing cross-modality and cross-language interactions. We assess our model's
performance on four publicly available SER datasets and establish its superior
effectiveness compared to recent approaches and baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13817">
<div class="article-summary-box-inner">
<span><p>We introduce a framework for analyzing various types of information in an NLP
Transformer. In this approach, we distinguish four layers of information:
positional, syntactic, semantic, and contextual. We also argue that the common
practice of adding positional information to semantic embedding is sub-optimal
and propose instead a Linear-and-Add approach. Our analysis reveals an
autogenetic separation of positional information through the deep layers. We
show that the distilled positional components of the embedding vectors follow
the path of a helix, both on the encoder side and on the decoder side. We
additionally show that on the encoder side, the conceptual dimensions generate
Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram
approach helps to reveal the PoS clusters of the next token. Our approach paves
a way to elucidate the processing of information through the deep layers of an
NLP Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13840">
<div class="article-summary-box-inner">
<span><p>Current trends to pre-train capable Large Language Models (LLMs) mostly focus
on scaling of model and dataset size. However, the quality of pre-training data
is an important factor for training powerful LLMs, yet it is a nebulous concept
that has not been fully characterized. Therefore, we use the recently proposed
Task2Vec diversity coefficient to ground and understand formal aspects of data
quality, to go beyond scale alone. Specifically, we measure the diversity
coefficient of publicly available pre-training datasets to demonstrate that
their formal diversity is high when compared to theoretical lower and upper
bounds. In addition, to build confidence in the diversity coefficient, we
conduct interpretability experiments and find that the coefficient aligns with
intuitive properties of diversity, e.g., it increases as the number of latent
concepts increases. We conclude the diversity coefficient is reliable, show
it's high for publicly available LLM datasets, and conjecture it can be used to
build useful diverse datasets for LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13841">
<div class="article-summary-box-inner">
<span><p>In the context of few-shot learning, it is currently believed that a fixed
pre-trained (PT) model, along with fine-tuning the final layer during
evaluation, outperforms standard meta-learning algorithms. We re-evaluate these
claims under an in-depth empirical examination of an extensive set of formally
diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike
previous work, we emphasize a fair comparison by using: the same architecture,
the same optimizer, and all models trained to convergence. Crucially, we use a
more rigorous statistical tool -- the effect size (Cohen's d) -- to determine
the practical significance of the difference between a model trained with PT
vs. a MAML. We then use a previously proposed metric -- the diversity
coefficient -- to compute the average formal diversity of a dataset. Using this
analysis, we demonstrate the following: 1. when the formal diversity of a data
set is low, PT beats MAML on average and 2. when the formal diversity is high,
MAML beats PT on average. The caveat is that the magnitude of the average
difference between a PT vs. MAML using the effect size is low (according to
classical statistical thresholds) -- less than 0.2. Nevertheless, this
observation is contrary to the currently held belief that a pre-trained model
is always better than a meta-learning model. Our extensive experiments consider
21 few-shot learning benchmarks, including the large-scale few-shot learning
dataset Meta-Data set. We also show no significant difference between a MAML
model vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a
pre-trained model does not always beat a meta-learned model and that the formal
diversity of a dataset is a driving factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations. (arXiv:2306.13865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13865">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) encode meanings of words in the form of
distributed semantics. Distributed semantics capture common statistical
patterns among language tokens (words, phrases, and sentences) from large
amounts of data. LLMs perform exceedingly well across General Language
Understanding Evaluation (GLUE) tasks designed to test a model's understanding
of the meanings of the input tokens. However, recent studies have shown that
LLMs tend to generate unintended, inconsistent, or wrong texts as outputs when
processing inputs that were seen rarely during training, or inputs that are
associated with diverse contexts (e.g., well-known hallucination phenomenon in
language generation tasks). Crowdsourced and expert-curated knowledge graphs
such as ConceptNet are designed to capture the meaning of words from a compact
set of well-defined contexts. Thus LLMs may benefit from leveraging such
knowledge contexts to reduce inconsistencies in outputs. We propose a novel
ensemble learning method, Interpretable Ensemble Representation Learning
(IERL), that systematically combines LLM and crowdsourced knowledge
representations of input tokens. IERL has the distinct advantage of being
interpretable by design (when was the LLM context used vs. when was the
knowledge context used?) over state-of-the-art (SOTA) methods, allowing
scrutiny of the inputs in conjunction with the parameters of the model,
facilitating the analysis of models' inconsistent or irrelevant outputs.
Although IERL is agnostic to the choice of LLM and crowdsourced knowledge, we
demonstrate our approach using BERT and ConceptNet. We report improved or
competitive results with IERL across GLUE tasks over current SOTA methods and
significantly enhanced model interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13888">
<div class="article-summary-box-inner">
<span><p>The exploration of sentiment analysis in low-resource languages, such as
Marathi, has been limited due to the availability of suitable datasets. In this
work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis
dataset, with four different domains - movie reviews, general tweets, TV show
subtitles, and political tweets. The dataset consists of around 60,000 manually
tagged samples covering 3 distinct sentiments - positive, negative, and
neutral. We create a sub-dataset for each domain comprising 15k samples. The
MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset
within the Indic sentiment landscape. We fine-tune different monolingual and
multilingual BERT models on these datasets and report the best accuracy with
the MahaBERT model. We also present an extensive in-domain and cross-domain
analysis thus highlighting the need for low-resource multi-domain datasets. The
data and models are available at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13891">
<div class="article-summary-box-inner">
<span><p>What is the effect of releasing a preprint of a paper before it is submitted
for peer review? No randomized controlled trial has been conducted, so we turn
to observational data to answer this question. We use data from the ICLR
conference (2018--2022) and apply methods from causal inference to estimate the
effect of arXiving a paper before the reviewing period (early arXiving) on its
acceptance to the conference. Adjusting for 18 confounders such as topic,
authors, and quality, we may estimate the causal effect. However, since quality
is a challenging construct to estimate, we use the negative outcome control
method, using paper citation count as a control variable to debias the quality
confounding effect. Our results suggest that early arXiving may have a small
effect on a paper's chances of acceptance. However, this effect (when existing)
does not differ significantly across different groups of authors, as grouped by
author citation count and institute rank. This suggests that early arXiving
does not provide an advantage to any particular group.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13899">
<div class="article-summary-box-inner">
<span><p>The art of mathematical reasoning stands as a fundamental pillar of
intellectual progress and is a central catalyst in cultivating human ingenuity.
Researchers have recently published a plethora of works centered around the
task of solving Math Word Problems (MWP) $-$ a crucial stride towards general
AI. These existing models are susceptible to dependency on shallow heuristics
and spurious correlations to derive the solution expressions. In order to
ameliorate this issue, in this paper, we propose a framework for MWP solvers
based on the generation of linguistic variants of the problem text. The
approach involves solving each of the variant problems and electing the
predicted expression with the majority of the votes. We use DeBERTa
(Decoding-enhanced BERT with disentangled attention) as the encoder to leverage
its rich textual representations and enhanced mask decoder to construct the
solution expressions. Furthermore, we introduce a challenging dataset,
$\mathrm{P\small{ARA}\normalsize{MAWPS}}$, consisting of paraphrased,
adversarial, and inverse variants of selectively sampled MWPs from the
benchmark $\mathrm{M\small{AWPS}}$ dataset. We extensively experiment on this
dataset along with other benchmark datasets using some baseline MWP solver
models. We show that training on linguistic variants of problem statements and
voting on candidate predictions improve the mathematical reasoning and
robustness of the model. We make our code and data publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13905">
<div class="article-summary-box-inner">
<span><p>In this paper, we lay out a vision for analysing semantic trajectory traces
and generating synthetic semantic trajectory data (SSTs) using generative
language model. Leveraging the advancements in deep learning, as evident by
progress in the field of natural language processing (NLP), computer vision,
etc. we intend to create intelligent models that can study the semantic
trajectories in various contexts, predicting future trends, increasing machine
understanding of the movement of animals, humans, goods, etc. enhancing
human-computer interactions, and contributing to an array of applications
ranging from urban-planning to personalized recommendation engines and business
strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?. (arXiv:2306.13906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13906">
<div class="article-summary-box-inner">
<span><p>We evaluated the capability of generative pre-trained transformers~(GPT-4) in
analysis of textual data in tasks that require highly specialized domain
expertise. Specifically, we focused on the task of analyzing court opinions to
interpret legal concepts. We found that GPT-4, prompted with annotation
guidelines, performs on par with well-trained law student annotators. We
observed that, with a relatively minor decrease in performance, GPT-4 can
perform batch predictions leading to significant cost reductions. However,
employing chain-of-thought prompting did not lead to noticeably improved
performance on this task. Further, we demonstrated how to analyze GPT-4's
predictions to identify and mitigate deficiencies in annotation guidelines, and
subsequently improve the performance of the model. Finally, we observed that
the model is quite brittle, as small formatting related changes in the prompt
had a high impact on the predictions. These findings can be leveraged by
researchers and practitioners who engage in semantic/pragmatic annotations of
texts in the context of the tasks requiring highly specialized domain
expertise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels. (arXiv:2306.13922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13922">
<div class="article-summary-box-inner">
<span><p>Deverbal nouns are nominal forms of verbs commonly used in written English
texts to describe events or actions, as well as their arguments. However, many
NLP systems, and in particular pattern-based ones, neglect to handle such
nominalized constructions. The solutions that do exist for handling arguments
of nominalized constructions are based on semantic annotation and require
semantic ontologies, making their applications restricted to a small set of
nouns. We propose to adopt instead a more syntactic approach, which maps the
arguments of deverbal nouns to the universal-dependency relations of the
corresponding verbal construction. We present an unsupervised mechanism --
based on contextualized word representations -- which allows to enrich
universal-dependency trees with dependency arcs denoting arguments of deverbal
nouns, using the same labels as the corresponding verbal cases. By sharing the
same label set as in the verbal case, patterns that were developed for verbs
can be applied without modification but with high accuracy also to the nominal
constructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13947">
<div class="article-summary-box-inner">
<span><p>Transformer based pre-trained models such as BERT and its variants, which are
trained on large corpora, have demonstrated tremendous success for natural
language processing (NLP) tasks. Most of academic works are based on the
English language; however, the number of multilingual and language specific
studies increase steadily. Furthermore, several studies claimed that language
specific models outperform multilingual models in various tasks. Therefore, the
community tends to train or fine-tune the models for the language of their case
study, specifically. In this paper, we focus on Turkish maps data and
thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT,
ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for
fine-tuning BERT in addition to the standard approach of one-layer fine-tuning.
For the dataset, a mid-sized Address Parsing corpus taken with a relatively
high quality is constructed. Conducted experiments on this dataset indicate
that Turkish language specific models with MLP fine-tuning yields slightly
better results when compared to the multilingual fine-tuned models. Moreover,
visualization of address tokens' representations further indicates the
effectiveness of BERT variants for classifying a variety of addresses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing the Emotion Carriers of COVID-19 Misinformation and Their Impact on Vaccination Outcomes in India and the United States. (arXiv:2306.13954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13954">
<div class="article-summary-box-inner">
<span><p>The COVID-19 Infodemic had an unprecedented impact on health behaviors and
outcomes at a global scale. While many studies have focused on a qualitative
and quantitative understanding of misinformation, including sentiment analysis,
there is a gap in understanding the emotion-carriers of misinformation and
their differences across geographies. In this study, we characterized emotion
carriers and their impact on vaccination rates in India and the United States.
A manually labelled dataset was created from 2.3 million tweets and collated
with three publicly available datasets (CoAID, AntiVax, CMU) to train deep
learning models for misinformation classification. Misinformation labelled
tweets were further analyzed for behavioral aspects by leveraging Plutchik
Transformers to determine the emotion for each tweet. Time series analysis was
conducted to study the impact of misinformation on spatial and temporal
characteristics. Further, categorical classification was performed using
transformer models to assign categories for the misinformation tweets.
Word2Vec+BiLSTM was the best model for misinformation classification, with an
F1-score of 0.92. The US had the highest proportion of misinformation tweets
(58.02%), followed by the UK (10.38%) and India (7.33%). Disgust, anticipation,
and anger were associated with an increased prevalence of misinformation
tweets. Disgust was the predominant emotion associated with misinformation
tweets in the US, while anticipation was the predominant emotion in India. For
India, the misinformation rate exhibited a lead relationship with vaccination,
while in the US it lagged behind vaccination. Our study deciphered that
emotions acted as differential carriers of misinformation across geography and
time. These carriers can be monitored to develop strategic interventions for
countering misinformation, leading to improved public health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13959">
<div class="article-summary-box-inner">
<span><p>In a conversational dialogue, speakers may have different emotional states
and their dynamics play an important role in understanding dialogue's emotional
discourse. However, simply detecting emotions is not sufficient to entirely
comprehend the speaker-specific changes in emotion that occur during a
conversation. To understand the emotional dynamics of speakers in an efficient
manner, it is imperative to identify the rationale or instigator behind any
changes or flips in emotion expressed by the speaker. In this paper, we explore
the task called Instigator based Emotion Flip Reasoning (EFR), which aims to
identify the instigator behind a speaker's emotion flip within a conversation.
For example, an emotion flip from joy to anger could be caused by an instigator
like threat. To facilitate this task, we present MELD-I, a dataset that
includes ground-truth EFR instigator labels, which are in line with emotional
psychology. To evaluate the dataset, we propose a novel neural architecture
called TGIF, which leverages Transformer encoders and stacked GRUs to capture
the dialogue context, speaker dynamics, and emotion sequence in a conversation.
Our evaluation demonstrates state-of-the-art performance (+4-12% increase in
F1-score) against five baselines used for the task. Further, we establish the
generalizability of TGIF on an unseen dataset in a zero-shot setting.
Additionally, we provide a detailed analysis of the competing models,
highlighting the advantages and limitations of our neural architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13968">
<div class="article-summary-box-inner">
<span><p>The realm of scientific text summarization has experienced remarkable
progress due to the availability of annotated brief summaries and ample data.
However, the utilization of multiple input modalities, such as videos and
audio, has yet to be thoroughly explored. At present, scientific
multimodal-input-based text summarization systems tend to employ longer target
summaries like abstracts, leading to an underwhelming performance in the task
of text summarization.
</p>
<p>In this paper, we deal with a novel task of extreme abstractive text
summarization (aka TL;DR generation) by leveraging multiple input modalities.
To this end, we introduce mTLDR, a first-of-its-kind dataset for the
aforementioned task, comprising videos, audio, and text, along with both
author-composed summaries and expert-annotated summaries. The mTLDR dataset
accompanies a total of 4,182 instances collected from various academic
conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present
mTLDRgen, an encoder-decoder-based model that employs a novel dual-fused
hyper-complex Transformer combined with a Wasserstein Riemannian Encoder
Transformer, to dexterously capture the intricacies between different
modalities in a hyper-complex latent geometric space. The hyper-complex
Transformer captures the intrinsic properties between the modalities, while the
Wasserstein Riemannian Encoder Transformer captures the latent structure of the
modalities in the latent space geometry, thereby enabling the model to produce
diverse sentences. mTLDRgen outperforms 20 baselines on mTLDR as well as
another non-scientific dataset (How2) across three Rouge-based evaluation
measures. Furthermore, based on the qualitative metrics, BERTScore and FEQA,
and human evaluations, we demonstrate that the summaries generated by mTLDRgen
are fluent and congruent to the original source material.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations. (arXiv:2306.13971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13971">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art NLP models have demonstrated excellent performance for
aspect based sentiment analysis (ABSA), substantial evidence has been presented
on their lack of robustness. This is especially manifested as significant
degradation in performance when faced with out-of-distribution data. Recent
solutions that rely on counterfactually augmented datasets show promising
results, but they are inherently limited because of the lack of access to
explicit causal structure. In this paper, we present an alternative approach
that relies on non-counterfactual data augmentation. Our proposal instead
relies on using noisy, cost-efficient data augmentations that preserve
semantics associated with the target aspect. Our approach then relies on
modelling invariances between different versions of the data to improve
robustness. A comprehensive suite of experiments shows that our proposal
significantly improves upon strong pre-trained baselines on both standard and
robustness-specific datasets. Our approach further establishes a new
state-of-the-art on the ABSA robustness benchmark and transfers well across
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Sous Chefs: Revising Recipes with GPT-3. (arXiv:2306.13986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13986">
<div class="article-summary-box-inner">
<span><p>With their remarkably improved text generation and prompting capabilities,
large language models can adapt existing written information into forms that
are easier to use and understand. In our work, we focus on recipes as an
example of complex, diverse, and widely used instructions. We develop a prompt
grounded in the original recipe and ingredients list that breaks recipes down
into simpler steps. We apply this prompt to recipes from various world
cuisines, and experiment with several large language models (LLMs), finding
best results with GPT-3.5. We also contribute an Amazon Mechanical Turk task
that is carefully designed to reduce fatigue while collecting human judgment of
the quality of recipe revisions. We find that annotators usually prefer the
revision over the original, demonstrating a promising application of LLMs in
serving as digital sous chefs for recipes and beyond. We release our prompt,
code, and MTurk template for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14003">
<div class="article-summary-box-inner">
<span><p>Instead of relying on human-annotated training samples to build a classifier,
weakly supervised scientific paper classification aims to classify papers only
using category descriptions (e.g., category names, category-indicative
keywords). Existing studies on weakly supervised paper classification are less
concerned with two challenges: (1) Papers should be classified into not only
coarse-grained research topics but also fine-grained themes, and potentially
into multiple themes, given a large and fine-grained label space; and (2) full
text should be utilized to complement the paper title and abstract for
classification. Moreover, instead of viewing the entire paper as a long linear
sequence, one should exploit the structural information such as citation links
across papers and the hierarchy of sections and paragraphs in each paper. To
tackle these challenges, in this study, we propose FUTEX, a framework that uses
the cross-paper network structure and the in-paper hierarchy structure to
classify full-text scientific papers under weak supervision. A network-aware
contrastive fine-tuning module and a hierarchy-aware aggregation module are
designed to leverage the two types of structural signals, respectively.
Experiments on two benchmark datasets demonstrate that FUTEX significantly
outperforms competitive baselines and is on par with fully supervised
classifiers that use 1,000 to 60,000 ground-truth training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14030">
<div class="article-summary-box-inner">
<span><p>The research on code-mixed data is limited due to the unavailability of
dedicated code-mixed datasets and pre-trained language models. In this work, we
focus on the low-resource Indian language Marathi which lacks any prior work in
code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English
(Mr-En) corpus with 5 million tweets for pretraining. We also release
L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models
pre-trained on MeCorpus. Furthermore, for benchmarking, we present three
supervised datasets MeHate, MeSent, and MeLID for downstream tasks like
code-mixed Mr-En hate speech detection, sentiment analysis, and language
identification respectively. These evaluation datasets individually consist of
manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations
show that the models trained on this novel corpus significantly outperform the
existing state-of-the-art BERT models. This is the first work that presents
artifacts for code-mixed Marathi research. All datasets and models are publicly
released at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14040">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks (RNNs) have achieved tremendous success in
processing sequential data, yet understanding and analyzing their behaviours
remains a significant challenge. To this end, many efforts have been made to
extract finite automata from RNNs, which are more amenable for analysis and
explanation. However, existing approaches like exact learning and compositional
approaches for model extraction have limitations in either scalability or
precision. In this paper, we propose a novel framework of Weighted Finite
Automata (WFA) extraction and explanation to tackle the limitations for natural
language tasks. First, to address the transition sparsity and context loss
problems we identified in WFA extraction for natural language tasks, we propose
an empirical method to complement missing rules in the transition diagram, and
adjust transition matrices to enhance the context-awareness of the WFA. We also
propose two data augmentation tactics to track more dynamic behaviours of RNN,
which further allows us to improve the extraction precision. Based on the
extracted model, we propose an explanation method for RNNs including a word
embedding method -- Transition Matrix Embeddings (TME) and TME-based task
oriented explanation for the target RNN. Our evaluation demonstrates the
advantage of our method in extraction precision than existing approaches, and
the effectiveness of TME-based explanation method in applications to
pretraining and adversarial example generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14050">
<div class="article-summary-box-inner">
<span><p>Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large
language models to verbalize rationalization for their predictions. While
chain-of-thought can lead to dramatic performance gains, benefits appear to
emerge only for sufficiently large models (beyond 50B parameters). We show that
orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit
from chain-of-thought prompting. To achieve this, we introduce Symbolic
Chain-of-Thought Distillation (SCoTD), a method to train a smaller student
model on rationalizations sampled from a significantly larger teacher model.
Experiments across several commonsense benchmarks show that: 1) SCoTD enhances
the performance of the student model in both supervised and few-shot settings,
and especially for challenge sets; 2) sampling many reasoning chains per
instance from the teacher is paramount; and 3) after distillation, student
chain-of-thoughts are judged by humans as comparable to the teacher, despite
orders of magnitude fewer parameters. We test several hypotheses regarding what
properties of chain-of-thought samples are important, e.g., diversity vs.
teacher likelihood vs. open-endedness. We release our corpus of
chain-of-thought samples and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14060">
<div class="article-summary-box-inner">
<span><p>Recent development in vision-language approaches has instigated a paradigm
shift in learning visual recognition models from language supervision. These
approaches align objects with language queries (e.g. "a photo of a cat") and
improve the models' adaptability to identify novel objects and domains.
Recently, several studies have attempted to query these models with complex
language expressions that include specifications of fine-grained semantic
details, such as attributes, shapes, textures, and relations. However, simply
incorporating language descriptions as queries does not guarantee accurate
interpretation by the models. In fact, our experiments show that GLIP, the
state-of-the-art vision-language model for object detection, often disregards
contextual information in the language descriptions and instead relies heavily
on detecting objects solely by their names. To tackle the challenges, we
propose a new description-conditioned (DesCo) paradigm of learning object
recognition models with rich language descriptions consisting of two major
innovations: 1) we employ a large language model as a commonsense knowledge
engine to generate rich language descriptions of objects based on object names
and the raw image-text caption; 2) we design context-sensitive queries to
improve the model's ability in deciphering intricate nuances embedded within
descriptions and enforce the model to focus on context rather than object names
alone. On two novel object detection benchmarks, LVIS and OminiLabel, under the
zero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and
29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models,
GLIP and FIBER, by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation. (arXiv:2306.14067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14067">
<div class="article-summary-box-inner">
<span><p>We describe the systems of the University of Alberta team for the
SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel
algorithm that leverages glosses retrieved from BabelNet, in combination with
text and image encoders. Furthermore, we compare language-specific encoders
against the application of English encoders to translated texts. As the
contexts given in the task datasets are extremely short, we also experiment
with augmenting these contexts with descriptions generated by a language model.
This yields substantial improvements in accuracy. We describe and evaluate
additional V-WSD methods which use image generation and text-conditioned image
segmentation. Overall, the results of our official submission rank us 18 out of
56 teams. Some of our unofficial results are even better than the official
ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14096">
<div class="article-summary-box-inner">
<span><p>Entity-level fine-grained sentiment analysis in the financial domain is a
crucial subtask of sentiment analysis and currently faces numerous challenges.
The primary challenge stems from the lack of high-quality and large-scale
annotated corpora specifically designed for financial text sentiment analysis,
which in turn limits the availability of data necessary for developing
effective text processing techniques. Recent advancements in large language
models (LLMs) have yielded remarkable performance in natural language
processing tasks, primarily centered around language pattern matching. In this
paper, we propose a novel and extensive Chinese fine-grained financial
sentiment analysis dataset, FinChina SA, for enterprise early warning. We
thoroughly evaluate and experiment with well-known existing open-source LLMs
using our dataset. We firmly believe that our dataset will serve as a valuable
resource to advance the exploration of real-world financial sentiment analysis
tasks, which should be the focus of future research. Our dataset and all code
to replicate the experimental results will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14115">
<div class="article-summary-box-inner">
<span><p>With recent advances in natural language processing, rationalization becomes
an essential self-explaining diagram to disentangle the black box by selecting
a subset of input texts to account for the major variation in prediction. Yet,
existing association-based approaches on rationalization cannot identify true
rationales when two or more snippets are highly inter-correlated and thus
provide a similar contribution to prediction accuracy, so-called spuriousness.
To address this limitation, we novelly leverage two causal desiderata,
non-spuriousness and efficiency, into rationalization from the causal inference
perspective. We formally define a series of probabilities of causation based on
a newly proposed structural causal model of rationalization, with its
theoretical identification established as the main component of learning
necessary and sufficient rationales. The superior performance of the proposed
causal rationalization is demonstrated on real-world review and medical
datasets with extensive experiments compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.02913">
<div class="article-summary-box-inner">
<span><p>Recent work in Dialogue Act (DA) classification approaches the task as a
sequence labeling problem, using neural network models coupled with a
Conditional Random Field (CRF) as the last layer. CRF models the conditional
probability of the target DA label sequence given the input utterance sequence.
However, the task involves another important input sequence, that of speakers,
which is ignored by previous work. To address this limitation, this paper
proposes a simple modification of the CRF layer that takes speaker-change into
account. Experiments on the SwDA corpus show that our modified CRF layer
outperforms the original one, with very wide margins for some DA labels.
Further, visualizations demonstrate that our CRF layer can learn meaningful,
sophisticated transition patterns between DA label pairs conditioned on
speaker-change in an end-to-end way. Code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01013">
<div class="article-summary-box-inner">
<span><p>Today's VQA models still tend to capture superficial linguistic correlations
in the training set and fail to generalize to the test set with different QA
distributions. To reduce these language biases, recent VQA works introduce an
auxiliary question-only model to regularize the training of targeted VQA model,
and achieve dominating performance on diagnostic benchmarks for
out-of-distribution testing. However, due to complex model design, these
ensemble-based methods are unable to equip themselves with two indispensable
characteristics of an ideal VQA model: 1) Visual-explainable: The model should
rely on the right visual regions when making decisions. 2) Question-sensitive:
The model should be sensitive to the linguistic variations in questions. To
this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing
and Training (CSST) strategy. After training with CSST, VQA models are forced
to focus on all critical objects and words, which significantly improves both
visual-explainable and question-sensitive abilities. Specifically, CSST is
composed of two parts: Counterfactual Samples Synthesizing (CSS) and
Counterfactual Samples Training (CST). CSS generates counterfactual samples by
carefully masking critical objects in images or words in questions and
assigning pseudo ground-truth answers. CST not only trains the VQA models with
both complementary samples to predict respective ground-truth answers, but also
urges the VQA models to further distinguish the original samples and
superficially similar counterfactual ones. To facilitate the CST training, we
propose two variants of supervised contrastive loss for VQA, and design an
effective positive and negative sample selection mechanism based on CSS.
Extensive experiments have shown the effectiveness of CSST. Particularly, by
building on top of model LMH+SAR, we achieve record-breaking performance on all
OOD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13398">
<div class="article-summary-box-inner">
<span><p>Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment
polarity towards an aspect. Because of the expensive and limited labelled data,
the pretraining strategy has become the de-facto standard for ABSA. However,
there always exists severe domain shift between the pretraining and downstream
ABSA datasets, hindering the effective knowledge transfer when directly
finetuning and making the downstream task performs sub-optimal. To mitigate
such domain shift, we introduce a unified alignment pretraining framework into
the vanilla pretrain-finetune pipeline with both instance- and knowledge-level
alignments. Specifically, we first devise a novel coarse-to-fine retrieval
sampling approach to select target domain-related instances from the
large-scale pretraining dataset, thus aligning the instances between
pretraining and target domains (First Stage). Then, we introduce a knowledge
guidance-based strategy to further bridge the domain gap at the knowledge
level. In practice, we formulate the model pretrained on the sampled instances
into a knowledge guidance model and a learner model, respectively. On the
target dataset, we design an on-the-fly teacher-student joint fine-tuning
approach to progressively transfer the knowledge from the knowledge guidance
model to the learner model (Second Stage). Thereby, the learner model can
maintain more domain-invariant knowledge when learning new knowledge from the
target dataset. In the Third Stage, the learner model is finetuned to better
adapt its learned knowledge to the target dataset. Extensive experiments and
analyses on several ABSA benchmarks demonstrate the effectiveness and
universality of our proposed pretraining framework. Our source code and models
are publicly available at https://github.com/WHU-ZQH/UIKA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12617">
<div class="article-summary-box-inner">
<span><p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding
entities and relations into continuous vector spaces. Existing methods are
mainly structure-based or description-based. Structure-based methods learn
representations that preserve the inherent structure of KGs. They cannot well
represent abundant long-tail entities in real-world KGs with limited structural
information. Description-based methods leverage textual information and
language models. Prior approaches in this direction barely outperform
structure-based ones, and suffer from problems like expensive negative sampling
and restrictive description demand. In this paper, we propose LMKE, which
adopts Language Models to derive Knowledge Embeddings, aiming at both enriching
representations of long-tail entities and solving problems of prior
description-based methods. We formulate description-based KE learning with a
contrastive learning framework to improve efficiency in training and
evaluation. Experimental results show that LMKE achieves state-of-the-art
performance on KE benchmarks of link prediction and triple classification,
especially for long-tail entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01079">
<div class="article-summary-box-inner">
<span><p>A crucial component in the curation of KB for a scientific domain is
information extraction from tables in the domain's published articles -- tables
carry important information (often numeric), which must be adequately extracted
for a comprehensive machine understanding of an article. Existing table
extractors assume prior knowledge of table structure and format, which may not
be known in scientific tables. We study a specific and challenging table
extraction problem: extracting compositions of materials (e.g., glasses,
alloys). We first observe that materials science researchers organize similar
compositions in a wide variety of table styles, necessitating an intelligent
model for table understanding and composition extraction. Consequently, we
define this novel task as a challenge for the ML community and create a
training dataset comprising 4,408 distantly supervised tables, along with 1,475
manually annotated dev and test tables. We also present DiSCoMaT, a strong
baseline geared towards this specific task, which combines multiple graph
neural networks with several task-specific regular expressions, features, and
constraints. We show that DiSCoMaT outperforms recent table processing
architectures by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretraining. (arXiv:2207.01772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01772">
<div class="article-summary-box-inner">
<span><p>With the burgeoning amount of data of image-text pairs and diversity of
Vision-and-Language (V\&amp;L) tasks, scholars have introduced an abundance of deep
learning models in this research domain. Furthermore, in recent years, transfer
learning has also shown tremendous success in Computer Vision for tasks such as
Image Classification, Object Detection, etc., and in Natural Language
Processing for Question Answering, Machine Translation, etc. Inheriting the
spirit of Transfer Learning, research works in V\&amp;L have devised multiple
pretraining techniques on large-scale datasets in order to enhance the
performance of downstream tasks. The aim of this article is to provide a
comprehensive revision of contemporary V\&amp;L pretraining models. In particular,
we categorize and delineate pretraining approaches, along with the summary of
state-of-the-art vision-and-language pretrained models. Moreover, a list of
training datasets and downstream tasks is supplied to further polish the
perspective into V\&amp;L pretraining. Lastly, we decided to take a further step to
discuss numerous directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11716">
<div class="article-summary-box-inner">
<span><p>Semantic similarity analysis and modeling is a fundamentally acclaimed task
in many pioneering applications of natural language processing today. Owing to
the sensation of sequential pattern recognition, many neural networks like RNNs
and LSTMs have achieved satisfactory results in semantic similarity modeling.
However, these solutions are considered inefficient due to their inability to
process information in a non-sequential manner, thus leading to the improper
extraction of context. Transformers function as the state-of-the-art
architecture due to their advantages like non-sequential data processing and
self-attention. In this paper, we perform semantic similarity analysis and
modeling on the U.S Patent Phrase to Phrase Matching Dataset using both
traditional and transformer-based techniques. We experiment upon four different
variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by
performing K-Fold Cross-Validation. The experimental results demonstrate our
methodology's enhanced performance compared to traditional techniques, with an
average Pearson correlation score of 0.79.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13335">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an effective way to transfer knowledge from a
strong teacher to an efficient student model. Ideally, we expect the better the
teacher is, the better the student. However, this expectation does not always
come true. It is common that a better teacher model results in a bad student
via distillation due to the nonnegligible gap between teacher and student. To
bridge the gap, we propose PROD, a PROgressive Distillation method, for dense
retrieval. PROD consists of a teacher progressive distillation and a data
progressive distillation to gradually improve the student. We conduct extensive
experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19,
TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves
the state-of-the-art within the distillation methods for dense retrieval. The
code and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04307">
<div class="article-summary-box-inner">
<span><p>Domain-specific language understanding requires integrating multiple pieces
of relevant contextual information. For example, we see both suicide and
depression-related behavior (multiple contexts) in the text ``I have a gun and
feel pretty bad about my life, and it wouldn't be the worst thing if I didn't
wake up tomorrow''. Domain specificity in self-attention architectures is
handled by fine-tuning on excerpts from relevant domain specific resources
(datasets and external knowledge - medical textbook chapters on mental health
diagnosis related to suicide and depression). We propose a modified
self-attention architecture Knowledge-infused Self Attention Transformer (KSAT)
that achieves the integration of multiple domain-specific contexts through the
use of external knowledge sources. KSAT introduces knowledge-guided biases in
dedicated self-attention layers for each knowledge source to accomplish this.
In addition, KSAT provides mechanics for controlling the trade-off between
learning from data and learning from knowledge. Our quantitative and
qualitative evaluations show that (1) the KSAT architecture provides novel
human-understandable ways to precisely measure and visualize the contributions
of the infused domain contexts, and (2) KSAT performs competitively with other
knowledge-infused baselines and significantly outperforms baselines that use
fine-tuning for domain-specific tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03358">
<div class="article-summary-box-inner">
<span><p>Words of estimative probability (WEP) are expressions of a statement's
plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...).
Multiple surveys demonstrate the agreement of human evaluators when assigning
numerical probability levels to WEP. For example, highly likely corresponds to
a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this
work, we measure the ability of neural language processing models to capture
the consensual probability level associated to each WEP. Firstly, we use the
UNLI dataset (Chen et al., 2020) which associates premises and hypotheses with
their perceived joint probability p, to construct prompts, e.g. "[PREMISE].
[WEP], [HYPOTHESIS]." and assess whether language models can predict whether
the WEP consensual probability level is close to p. Secondly, we construct a
dataset of WEP-based probabilistic reasoning, to test whether language models
can reason with WEP compositions. When prompted "[EVENTA] is likely. [EVENTB]
is impossible.", a causal language model should not express that [EVENTA&amp;B] is
likely. We show that both tasks are unsolved by off-the-shelf English language
models, but that fine-tuning leads to transferable improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09102">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) that have been trained on multilingual but not
parallel text exhibit a remarkable ability to translate between languages. We
probe this ability in an in-depth study of the pathways language model (PaLM),
which has demonstrated the strongest machine translation (MT) performance among
similarly-trained LLMs to date. We investigate various strategies for choosing
translation examples for few-shot prompting, concluding that example quality is
the most important factor. Using optimized prompts, we revisit previous
assessments of PaLM's MT capabilities with more recent test sets, modern MT
metrics, and human evaluation, and find that its performance, while impressive,
still lags that of state-of-the-art supervised systems. We conclude by
providing an analysis of PaLM's MT output which reveals some interesting
properties and prospects for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06800">
<div class="article-summary-box-inner">
<span><p>In-context learning has shown great success in i.i.d semantic parsing splits,
where the training and test sets are drawn from the same distribution. In this
setup, models are typically prompted with demonstrations that are similar to
the input utterance. However, in the setup of compositional generalization,
where models are tested on outputs with structures that are absent from the
training set, selecting similar demonstrations is insufficient, as often no
example will be similar enough to the input. In this work, we propose a method
to select diverse demonstrations that aims to collectively cover all of the
structures required in the output program, in order to encourage the model to
generalize to new structures from these demonstrations. We empirically show
that combining diverse demonstrations with in-context learning substantially
improves performance across three compositional generalization semantic parsing
datasets in the pure in-context learning setup and when combined with
finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10189">
<div class="article-summary-box-inner">
<span><p>When answering natural language questions over knowledge bases, missing
facts, incomplete schema and limited scope naturally lead to many questions
being unanswerable. While answerability has been explored in other QA settings,
it has not been studied for QA over knowledge bases (KBQA). We create
GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first
identifying various forms of KB incompleteness that make questions
unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset
with only answerable questions). Experimenting with three state-of-the-art KBQA
models, we find that all three models suffer a drop in performance even after
suitable adaptation for unanswerable questions. In addition, these often detect
unanswerability for wrong reasons and find specific forms of unanswerability
particularly difficult to handle. This underscores the need for further
research in making KBQA systems robust to unanswerability
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04388">
<div class="article-summary-box-inner">
<span><p>Recent work in the domain of speech enhancement has explored the use of
self-supervised speech representations to aid in the training of neural speech
enhancement models. However, much of this work focuses on using the deepest or
final outputs of self supervised speech representation models, rather than the
earlier feature encodings. The use of self supervised representations in such a
way is often not fully motivated. In this work it is shown that the distance
between the feature encodings of clean and noisy speech correlate strongly with
psychoacoustically motivated measures of speech quality and intelligibility, as
well as with human Mean Opinion Score (MOS) ratings. Experiments using this
distance as a loss function are performed and improved performance over the use
of STFT spectrogram distance based loss as well as other common loss functions
from speech enhancement literature is demonstrated using objective measures
such as perceptual evaluation of speech quality (PESQ) and short-time objective
intelligibility (STOI).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05895">
<div class="article-summary-box-inner">
<span><p>Discourse processing suffers from data sparsity, especially for dialogues. As
a result, we explore approaches to build discourse structures for dialogues,
based on attention matrices from Pre-trained Language Models (PLMs). We
investigate multiple tasks for fine-tuning and show that the dialogue-tailored
Sentence Ordering task performs best. To locate and exploit discourse
information in PLMs, we propose an unsupervised and a semi-supervised method.
Our proposals achieve encouraging results on the STAC corpus, with F1 scores of
57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When
restricted to projective trees, our scores improved to 63.3 and 68.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13114">
<div class="article-summary-box-inner">
<span><p>Complex Query Answering (CQA) is an important and fundamental task for
knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and
robust solution to CQA. In the encoding process, most existing QE methods first
parse the logical query into an executable computational direct-acyclic graph
(DAG), then use neural networks to parameterize the operators, and finally,
recursively execute these neuralized operators. However, the
parameterization-and-execution paradigm may be potentially over-complicated, as
it can be structurally simplified by a single neural network encoder.
Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective
for encoding semantic graphs in related tasks. Motivated by this, we propose
sequential query encoding (SQE) as an alternative to encode queries for CQA.
Instead of parameterizing and executing the computational graph, SQE first uses
a search-based algorithm to linearize the computational graph to a sequence of
tokens and then uses a sequence encoder to compute its vector representation.
Then this vector representation is used as a query embedding to retrieve
answers from the embedding space according to similarity scores. Despite its
simplicity, SQE demonstrates state-of-the-art neural query encoding performance
on FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine
types of in-distribution queries. Further experiment shows that SQE also
demonstrates comparable knowledge inference capability on out-of-distribution
queries, whose query types are not observed during the training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13939">
<div class="article-summary-box-inner">
<span><p>As the size of large language models continue to scale, so does the
computational resources required to run it. Spiking Neural Networks (SNNs) have
emerged as an energy-efficient approach to deep learning that leverage sparse
and event-driven activations to reduce the computational overhead associated
with model inference. While they have become competitive with non-spiking
models on many computer vision tasks, SNNs have also proven to be more
challenging to train. As a result, their performance lags behind modern deep
learning, and we are yet to see the effectiveness of SNNs in language
generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV)
language model, we successfully implement `SpikeGPT', a generative language
model with binary, event-driven spiking activation units. We train the proposed
model on two model variants: 45M and 216M parameters. To the best of our
knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,
rendering it suitable for both the generation and comprehension of natural
language. We achieve this by modifying the transformer block to replace
multi-head self attention to reduce quadratic computational complexity O(N^2)
to linear complexity O(N) with increasing sequence length. Input tokens are
instead streamed in sequentially to our attention mechanism (as with typical
SNNs). Our preliminary experiments show that SpikeGPT remains competitive with
non-spiking models on tested benchmarks, while maintaining 20x fewer operations
when processed on neuromorphic hardware that can leverage sparse, event-driven
activations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.06841">
<div class="article-summary-box-inner">
<span><p>The paper studies the capabilities of Recurrent-Neural-Network sequence to
sequence (RNN seq2seq) models in learning four transduction tasks: identity,
reversal, total reduplication, and quadratic copying. These transductions are
traditionally well studied under finite state transducers and attributed with
increasing complexity. We find that RNN seq2seq models are only able to
approximate a mapping that fits the training or in-distribution data, instead
of learning the underlying functions. Although attention makes learning more
efficient and robust, it does not overcome the out-of-distribution
generalization limitation. We establish a novel complexity hierarchy for
learning the four tasks for attention-less RNN seq2seq models, which may be
understood in terms of the complexity hierarchy of formal languages, instead of
string transductions. RNN variants also play a role in the results. In
particular, we show that Simple RNN seq2seq models cannot count the input
length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12816">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. (arXiv:2303.14070v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14070">
<div class="article-summary-box-inner">
<span><p>The primary aim of this research was to address the limitations observed in
the medical knowledge of prevalent large language models (LLMs) such as
ChatGPT, by creating a specialized language model with enhanced accuracy in
medical advice. We achieved this by adapting and refining the large language
model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues
sourced from a widely used online medical consultation platform. These
conversations were cleaned and anonymized to respect privacy concerns. In
addition to the model refinement, we incorporated a self-directed information
retrieval mechanism, allowing the model to access and utilize real-time
information from online sources like Wikipedia and data from curated offline
medical databases. The fine-tuning of the model with real-world patient-doctor
interactions significantly improved the model's ability to understand patient
needs and provide informed advice. By equipping the model with self-directed
information retrieval from reliable online and offline sources, we observed
substantial improvements in the accuracy of its responses. Our proposed
ChatDoctor, represents a significant advancement in medical LLMs, demonstrating
a significant improvement in understanding patient inquiries and providing
accurate advice. Given the high stakes and low error tolerance in the medical
field, such enhancements in providing accurate and reliable information are not
only beneficial but essential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01295">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD, a parallel and large-scale
multilingual conversation dataset that we created by translating the
English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into
105 other languages. XSGD contains approximately 330k utterances per language.
To facilitate aligned cross-lingual representations, we develop an efficient
prompt-tuning-based method for learning alignment prompts. We also investigate
two different classifiers: NLI-based and vanilla classifiers, and test
cross-lingual capability enabled by the aligned prompts. We evaluate our
model's cross-lingual generalization capabilities on two conversation tasks:
slot-filling and intent classification. Our results demonstrate the strong and
efficient modeling ability of NLI-based classifiers and the large cross-lingual
transfer improvements achieved by our aligned prompts, particularly in few-shot
settings. In addition, we highlight the nice results of our approach compared
to LLMs such as text-davinci-003 and ChatGPT in both zero-shot and few-shot
settings. While LLMs exhibit impressive performance in English, their
cross-lingual capabilities in other languages, particularly low-resource
languages, are limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06377">
<div class="article-summary-box-inner">
<span><p>The capacity to generate meaningful symbols and effectively employ them for
advanced cognitive processes, such as communication, reasoning, and planning,
constitutes a fundamental and distinctive aspect of human intelligence.
Existing deep neural networks still notably lag human capabilities in terms of
generating symbols for higher cognitive functions. Here, we propose a solution
(symbol emergence artificial network (SEA-net)) to endow neural networks with
the ability to create symbols, understand semantics, and achieve communication.
SEA-net generates symbols that dynamically configure the network to perform
specific tasks. These symbols capture compositional semantic information that
allows the system to acquire new functions purely by symbolic manipulation or
communication. In addition, these self-generated symbols exhibit an intrinsic
structure resembling that of natural language, suggesting a common framework
underlying the generation and understanding of symbols in both human brains and
artificial neural networks. We believe that the proposed framework will be
instrumental in producing more capable systems that can synergize the strengths
of connectionist and symbolic approaches for artificial intelligence (AI).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08453">
<div class="article-summary-box-inner">
<span><p>Various natural language processing (NLP) tasks necessitate models that are
efficient and small based on their ultimate application at the edge or in other
resource-constrained environments. While prior research has reduced the size of
these models, increasing computational efficiency without considerable
performance impacts remains difficult, especially for autoregressive tasks.
This paper proposes modular linearized attention (MLA), which combines multiple
efficient attention mechanisms, including cosFormer, to maximize inference
quality while achieving notable speedups. We validate this approach on several
autoregressive NLP tasks, including speech-to-text neural machine translation
(S2T NMT), speech-to-text simultaneous translation (SimulST), and
autoregressive text-to-spectrogram, noting efficiency gains on TTS and
competitive performance for NMT and SimulST during training and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>Making the contents generated by Large Language Model (LLM) such as ChatGPT,
accurate, credible and traceable is crucial, especially in complex
knowledge-intensive tasks that require multi-step reasoning and each of which
needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM
with external knowledge is good potential to solve this problem. However, where
and how to introduce IR into LLM is a big challenge. Previous work has the
disadvantage that the wrong knowledge retrieved by IR misleads the LLM or
breaks the reasoning chain of LLM. In this paper, we propose a novel framework
called Search-in-the-Chain (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the global reasoning chain called
Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the
answer to the query. Second, IR verifies the answer of each node of CoQ, it
corrects the answer that is not consistent with the retrieved information when
IR gives high confidence, which improves the credibility. Third, LLM can mark
its missing knowledge in CoQ and IR can provide this knowledge to LLM. These
three operations improve the accuracy of LLM for complex knowledge-intensive
tasks in terms of reasoning ability and knowledge. Finally, SearChain generates
the reasoning process and marks references to supporting documents for each
reasoning step, which improves traceability. SearChain transforms the topology
of reasoning from chain to tree, which can modify the reasoning direction.
Experiment shows that SearChain outperforms baselines on complex
knowledge-intensive tasks including multi-hop question-answering, slot filling,
fact checking, and long-form question-answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (arXiv:2305.01437v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01437">
<div class="article-summary-box-inner">
<span><p>With the advent of deep learning methods, Neural Machine Translation (NMT)
systems have become increasingly powerful. However, deep learning based systems
are susceptible to adversarial attacks, where imperceptible changes to the
input can cause undesirable changes at the output of the system. To date there
has been little work investigating adversarial attacks on sequence-to-sequence
systems, such as NMT models. Previous work in NMT has examined attacks with the
aim of introducing target phrases in the output sequence. In this work,
adversarial attacks for NMT systems are explored from an output perception
perspective. Thus the aim of an attack is to change the perception of the
output sequence, without altering the perception of the input sequence. For
example, an adversary may distort the sentiment of translated reviews to have
an exaggerated positive sentiment. In practice it is challenging to run
extensive human perception experiments, so a proxy deep-learning classifier
applied to the NMT output is used to measure perception changes. Experiments
demonstrate that the sentiment perception of NMT systems' output sequences can
be changed significantly with small imperceptible changes to input sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01505">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), consisting of 100 billion or more parameters,
have demonstrated remarkable ability in complex multi-step reasoning tasks.
However, the application of such generic advancements has been limited to a few
fields, such as clinical or legal, with the field of financial reasoning
remaining largely unexplored. To the best of our knowledge, the ability of LLMs
to solve financial reasoning problems has never been dealt with, and whether it
can be performed at any scale remains unknown. To address this knowledge gap,
this research presents a comprehensive investigation into the potential
application of LLMs in the financial domain. The investigation includes a
detailed exploration of a range of subjects, including task formulation,
synthetic data generation, prompting methods, and evaluation capability.
Furthermore, the study benchmarks various GPT variants with parameter scales
ranging from 2.8B to 13B, with and without instruction tuning, on diverse
dataset sizes. By analyzing the results, we reveal that the ability to generate
coherent financial reasoning first emerges at 6B parameters, and continues to
improve with better instruction-tuning or larger datasets. Additionally, the
study provides a publicly accessible dataset named sFIOG (Synthetic-Financial
Investment Opinion Generation), consisting of 11,802 synthetic investment
thesis samples, to support further research in the field of financial
reasoning. Overall, this research seeks to contribute to the understanding of
the efficacy of language models in the field of finance, with a particular
emphasis on their ability to engage in sophisticated reasoning and analysis
within the context of investment decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Taxonomy of Foundation Model based Systems for Responsible-AI-by-Design. (arXiv:2305.05352v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05352">
<div class="article-summary-box-inner">
<span><p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted significant attention on foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is little understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories:
foundation model pretraining and fine-tuning, architecture design of foundation
model based systems, and responsible-AI-by-design. This taxonomy provides
concrete guidance for making major design decisions when designing foundation
model based systems and highlights trade-offs arising from design decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05940">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) unfolds as large language models become capable of
inferring test labels conditioned on a few labeled samples without any gradient
update. ICL-enabled large language models provide a promising step forward
toward bypassing recurrent annotation costs in a low-resource setting. Yet,
only a handful of past studies have explored ICL in a cross-lingual setting, in
which the need for transferring label-knowledge from a high-resource language
to a low-resource one is immensely crucial. To bridge the gap, we provide the
first in-depth analysis of ICL for cross-lingual text classification. We find
that the prevalent mode of selecting random input-label pairs to construct the
prompt-context is severely limited in the case of cross-lingual ICL, primarily
due to the lack of alignment in the input as well as the output spaces. To
mitigate this, we propose a novel prompt construction strategy -- Cross-lingual
In-context Source-Target Alignment (X-InSTA). With an injected coherence in the
semantics of the input examples and a task-based alignment across the source
and target languages, X-InSTA is able to outperform random prompt selection by
a large margin across three different tasks using 44 different cross-lingual
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creative Data Generation: A Review Focusing on Text and Poetry. (arXiv:2305.08493v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08493">
<div class="article-summary-box-inner">
<span><p>The rapid advancement in machine learning has led to a surge in automatic
data generation, making it increasingly challenging to differentiate between
naturally or human-generated data and machine-generated data. Despite these
advancements, the generation of creative data remains a challenge. This paper
aims to investigate and comprehend the essence of creativity, both in general
and within the context of natural language generation. We review various
approaches to creative writing devices and tasks, with a specific focus on the
generation of poetry. We aim to shed light on the challenges and opportunities
in the field of creative data generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10276">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12493">
<div class="article-summary-box-inner">
<span><p>Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint. (arXiv:2305.13599v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13599">
<div class="article-summary-box-inner">
<span><p>A self-explaining rationalization model is generally constructed by a
cooperative game where a generator selects the most human-intelligible pieces
from the input text as rationales, followed by a predictor that makes
predictions based on the selected rationales. However, such a cooperative game
may incur the degeneration problem where the predictor overfits to the
uninformative pieces generated by a not yet well-trained generator and in turn,
leads the generator to converge to a sub-optimal model that tends to select
senseless pieces. In this paper, we theoretically bridge degeneration with the
predictor's Lipschitz continuity. Then, we empirically propose a simple but
effective method named DR, which can naturally and flexibly restrain the
Lipschitz constant of the predictor, to address the problem of degeneration.
The main idea of DR is to decouple the generator and predictor to allocate them
with asymmetric learning rates. A series of experiments conducted on two widely
used benchmarks have verified the effectiveness of the proposed method. Codes:
\href{https://github.com/jugechengzi/Rationalization-DR}{https://github.com/jugechengzi/Rationalization-DR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAIL: Search-Augmented Instruction Learning. (arXiv:2305.15225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15225">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been significantly improved by instruction
fine-tuning, but still lack transparency and the ability to utilize up-to-date
knowledge and information. In this work, we propose search-augmented
instruction learning (SAIL), which grounds the language generation and
instruction following abilities on complex search results generated by in-house
and external search engines. With an instruction tuning corpus, we collect
search results for each training case from different search APIs and domains,
and construct a new search-grounded training set containing
\textit{(instruction, grounding information, response)} triplets. We then
fine-tune the LLaMA-7B model on the constructed training set. Since the
collected results contain unrelated and disputing languages, the model needs to
learn to ground on trustworthy search results, filter out distracting passages,
and generate the target response. The search result-denoising process entails
explicit trustworthy information selection and multi-hop reasoning, since the
retrieved passages might be informative but not contain the
instruction-following answer. Experiments show that the fine-tuned SAIL-7B
model has a strong instruction-following ability, and it performs significantly
better on transparency-sensitive tasks, including open-ended question answering
and fact checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17760">
<div class="article-summary-box-inner">
<span><p>How do language models "think"? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18391">
<div class="article-summary-box-inner">
<span><p>Memes are a popular form of communicating trends and ideas in social media
and on the internet in general, combining the modalities of images and text.
They can express humor and sarcasm but can also have offensive content.
Analyzing and classifying memes automatically is challenging since their
interpretation relies on the understanding of visual elements, language, and
background knowledge. Thus, it is important to meaningfully represent these
sources and the interaction between them in order to classify a meme as a
whole. In this work, we propose to use scene graphs, that express images in
terms of objects and their visual relations, and knowledge graphs as structured
representations for meme classification with a Transformer-based architecture.
We compare our approach with ImgBERT, a multimodal model that uses only learned
(instead of structured) representations of the meme, and observe consistent
improvements. We further provide a dataset with human graph annotations that we
compare to automatically generated graphs and entity linking. Analysis shows
that automatic methods link more entities than human annotators and that
automatically generated graphs are better suited for hatefulness classification
in memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00013">
<div class="article-summary-box-inner">
<span><p>According to the World Health Organization (WHO), cancer is the second
leading cause of death globally. Scientific research on different types of
cancers grows at an ever-increasing rate, publishing large volumes of research
articles every year. The insight information and the knowledge of the drug,
diagnostics, risk, symptoms, treatments, etc., related to genes are significant
factors that help explore and advance the cancer research progression. Manual
screening of such a large volume of articles is very laborious and
time-consuming to formulate any hypothesis. The study uses the two most
non-trivial NLP, Natural Language Processing functions, Entity Recognition, and
text classification to discover knowledge from biomedical literature. Named
Entity Recognition (NER) recognizes and extracts the predefined entities
related to cancer from unstructured text with the support of a user-friendly
interface and built-in dictionaries. Text classification helps to explore the
insights into the text and simplifies data categorization, querying, and
article screening. Machine learning classifiers are also used to build the
classification model and Structured Query Languages (SQL) is used to identify
the hidden relations that may lead to significant predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00739">
<div class="article-summary-box-inner">
<span><p>One impressive emergent capability of large language models (LLMs) is
generation of code, including Structured Query Language (SQL) for databases.
For the task of converting natural language text to SQL queries, Text-to-SQL,
adaptation of LLMs is of paramount importance, both in in-context learning and
fine-tuning settings, depending on the amount of adaptation data used. In this
paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on
PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is
based on an execution-based self-consistency prompting approach designed for
Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our
best knowledge is the first to outperform previous state-of-the-art with
fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the
fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying
SQL-PaLM to real-world scenarios we further evaluate its robustness on other
challenging variants of Spider and demonstrate the superior generalization
capability of SQL-PaLM. In addition, via extensive case studies, we demonstrate
the impressive intelligent capabilities and various success enablers of
LLM-based Text-to-SQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01006">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive exploration of leveraging Large Language
Models (LLMs), specifically GPT-4, in the field of instructional design. With a
focus on scaling evidence-based instructional design expertise, our research
aims to bridge the gap between theoretical educational studies and practical
implementation. We discuss the benefits and limitations of AI-driven content
generation, emphasizing the necessity of human oversight in ensuring the
quality of educational materials. This work is elucidated through two detailed
case studies where we applied GPT-4 in creating complex higher-order
assessments and active learning components for different courses. From our
experiences, we provide best practices for effectively using LLMs in
instructional design tasks, such as utilizing templates, fine-tuning, handling
unexpected output, implementing LLM chains, citing references, evaluating
output, creating rubrics, grading, and generating distractors. We also share
our vision of a future recommendation system, where a customized GPT-4 extracts
instructional design principles from educational studies and creates
personalized, evidence-supported strategies for users' unique educational
contexts. Our research contributes to understanding and optimally harnessing
the potential of AI-driven language models in enhancing educational outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v3 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03763">
<div class="article-summary-box-inner">
<span><p>ChatGPT has demonstrated remarkable capabilities across various natural
language processing (NLP) tasks. However, its potential for inferring dynamic
network structures from temporal textual data, specifically financial news,
remains an unexplored frontier. In this research, we introduce a novel
framework that leverages ChatGPT's graph inference capabilities to enhance
Graph Neural Networks (GNN). Our framework adeptly extracts evolving network
structures from textual data, and incorporates these networks into graph neural
networks for subsequent predictive tasks. The experimental results from stock
movement forecasting indicate our model has consistently outperformed the
state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios
constructed based on our model's outputs demonstrate higher annualized
cumulative returns, alongside reduced volatility and maximum drawdown. This
superior performance highlights the potential of ChatGPT for text-based network
inferences and underscores its promising implications for the financial sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03774">
<div class="article-summary-box-inner">
<span><p>This paper presents the first comprehensive study on automatic readability
assessment of Turkish texts. We combine state-of-the-art neural network models
with linguistic features at lexical, morphosyntactic, syntactic and discourse
levels to develop an advanced readability tool. We evaluate the effectiveness
of traditional readability formulas compared to modern automated methods and
identify key linguistic features that determine the readability of Turkish
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07952">
<div class="article-summary-box-inner">
<span><p>We present MOFI, a new vision foundation model designed to learn image
representations from noisy entity annotated images. MOFI differs from previous
work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.
Regarding data, we introduce a new approach to automatically assign entity
labels to images from noisy image-text pairs. Our approach involves employing a
named entity recognition model to extract entities from the alt-text, and then
using a CLIP model to select the correct entities as labels of the paired
image. The approach is simple, does not require costly human annotation, and
can be readily scaled up to billions of image-text pairs mined from the web.
Through this method, we have created Image-to-Entities (I2E), a new large-scale
dataset with 1 billion images and 2 million distinct entities, covering rich
visual concepts in the wild. Building upon the I2E dataset, we study different
training recipes, including supervised pre-training, contrastive pre-training,
and multi-task learning. For constrastive pre-training, we treat entity names
as free-form text, and further enrich them with entity descriptions.
Experiments show that supervised pre-training with large-scale fine-grained
entity labels is highly effective for image retrieval tasks, and multi-task
training further improves the performance. The final MOFI model achieves 86.66%
mAP on the challenging GPR1200 dataset, surpassing the previous
state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further
experiments on zero-shot and linear probe image classification also show that
MOFI outperforms a CLIP model trained on the original image-text data,
demonstrating the effectiveness of the I2E dataset in learning strong image
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08997">
<div class="article-summary-box-inner">
<span><p>We curate a comprehensive dataset of 4,550 questions and solutions from
problem sets, midterm exams, and final exams across all MIT Mathematics and
Electrical Engineering and Computer Science (EECS) courses required for
obtaining a degree. We evaluate the ability of large language models to fulfill
the graduation requirements for any MIT major in Mathematics and EECS. Our
results demonstrate that GPT-3.5 successfully solves a third of the entire MIT
curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate
on a test set excluding questions based on images. We fine-tune an open-source
large language model on this dataset. We employ GPT-4 to automatically grade
model responses, providing a detailed performance breakdown by course,
question, and answer type. By embedding questions in a low-dimensional space,
we explore the relationships between questions, topics, and classes and
discover which questions and classes are required for solving other questions
and classes through few-shot learning. Our analysis offers valuable insights
into course prerequisites and curriculum design, highlighting language models'
potential for learning and improving Mathematics and EECS education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09869">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11222">
<div class="article-summary-box-inner">
<span><p>Transformer models have achieved remarkable results in various natural
language tasks, but they are often prohibitively large, requiring massive
memories and computational resources. To reduce the size and complexity of
these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel
model compression technique that approximates a weight matrix by the sum of a
low-rank matrix and a sparse matrix. Our method combines the advantages of both
low-rank approximations and pruning, while avoiding their limitations. Low-rank
approximation compresses the coherent and expressive parts in neurons, while
pruning removes the incoherent and non-expressive parts in neurons. Pruning
enhances the diversity of low-rank approximations, and low-rank approximation
prevents pruning from losing too many expressive neurons. We evaluate our
method on natural language understanding, question answering, and natural
language generation tasks. We show that it significantly outperforms existing
compression methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11260">
<div class="article-summary-box-inner">
<span><p>Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation
task, which analyze the emotional polarity of the evaluation aspects. However,
previous works only focus on the identification of opinion expressions, forget
that the diversity of opinion expressions also has great impacts on the ABSA
task. To mitigate this problem, we propose a novel counterfactual data
augmentation method to generate opinion expression with reversed sentiment
polarity. Specially, the integrated gradients are calculated to identify and
mask the opinion expression. Then, a prompt with the reverse label is combined
to the original text, and a pre-trained language model (PLM), T5, is finally
employed to retrieve the masks. The experimental results show the proposed
counterfactual data augmentation method perform better than current
augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11485">
<div class="article-summary-box-inner">
<span><p>Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems. (arXiv:2306.13307v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13307">
<div class="article-summary-box-inner">
<span><p>Current ASR systems are mainly trained and evaluated at the utterance level.
Long range cross utterance context can be incorporated. A key task is to derive
a suitable compact representation of the most relevant history contexts. In
contrast to previous researches based on either LSTM-RNN encoded histories that
attenuate the information from longer range contexts, or frame level
concatenation of transformer context embeddings, in this paper compact
low-dimensional cross utterance contextual features are learned in the
Conformer-Transducer Encoder using specially designed attention pooling layers
that are applied over efficiently cached preceding utterances history vectors.
Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed
contextualized streaming Conformer-Transducers outperform the baseline using
utterance internal context only with statistically significant WER reductions
of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-27 23:12:35.417487951 UTC">2023-06-27 23:12:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>