<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-07T01:30:00Z">03-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Data Augmentation Methods on Social Media Corpora. (arXiv:2303.02198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02198">
<div class="article-summary-box-inner">
<span><p>Data augmentation has proven widely effective in computer vision. In Natural
Language Processing (NLP) data augmentation remains an area of active research.
There is no widely accepted augmentation technique that works well across tasks
and model architectures. In this paper we explore data augmentation techniques
in the context of text classification using two social media datasets. We
explore popular varieties of data augmentation, starting with oversampling,
Easy Data Augmentation (Wei and Zou, 2019) and Back-Translation (Sennrich et
al., 2015). We also consider Greyscaling, a relatively unexplored data
augmentation technique that seeks to mitigate the intensity of adjectives in
examples. Finally, we consider a few-shot learning approach: Pattern-Exploiting
Training (PET) (Schick et al., 2020). For the experiments we use a BERT
transformer architecture. Results show that augmentation techniques provide
only minimal and inconsistent improvements. Synonym replacement provided
evidence of some performance improvement and adjective scales with Grayscaling
is an area where further exploration would be valuable. Few-shot learning
experiments show consistent improvement over supervised training, and seem very
promising when classes are easily separable but further exploration would be
valuable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Questions Over Knowledge Graphs Using Logic Programming Along with Language Models. (arXiv:2303.02206v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02206">
<div class="article-summary-box-inner">
<span><p>Question Answering over Knowledge Graphs (KGQA) is the task of answering
natural language questions over a knowledge graph (KG). This task requires a
model to reason over multiple edges of the KG to reach the right answer. In
this work, we present a method to equip large language models (LLMs) with
classic logical programming languages to provide an explainable solution to the
problem. Our goal is to extract the representation of the question in the form
of a Prolog query, which can then be used to answer the query programmatically.
To demonstrate the effectiveness of this approach, we use the MetaQA dataset
and show that our method finds the correct answer entities for all the
questions in the test dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrojText: Test-time Invisible Textual Trojan Insertion. (arXiv:2303.02242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02242">
<div class="article-summary-box-inner">
<span><p>In Natural Language Processing (NLP), intelligent neuron models can be
susceptible to textual Trojan attacks. Such attacks occur when Trojan models
behave normally for standard inputs but generate malicious output for inputs
that contain a specific trigger. Syntactic-structure triggers, which are
invisible, are becoming more popular for Trojan attacks because they are
difficult to detect and defend against. However, these types of attacks require
a large corpus of training data to generate poisoned samples with the necessary
syntactic structures for Trojan insertion. Obtaining such data can be difficult
for attackers, and the process of generating syntactic poisoned triggers and
inserting Trojans can be time-consuming. This paper proposes a solution called
TrojText, which aims to determine whether invisible textual Trojan attacks can
be performed more efficiently and cost-effectively without training data. The
proposed approach, called the Representation-Logit Trojan Insertion (RLI)
algorithm, uses smaller sampled test data instead of large training data to
achieve the desired attack. The paper also introduces two additional
techniques, namely the accumulated gradient ranking (AGR) and Trojan Weights
Pruning (TWP), to reduce the number of tuned parameters and the attack
overhead. The TrojText approach was evaluated on three datasets (AG's News,
SST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The
experiments demonstrated that the TrojText approach achieved a 98.35\%
classification accuracy for test sentences in the target class on the BERT
model for the AG's News dataset. The source code for TrojText is available at
https://github.com/UCF-ML-Research/TrojText.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to reason over visual objects. (arXiv:2303.02260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02260">
<div class="article-summary-box-inner">
<span><p>A core component of human intelligence is the ability to identify abstract
patterns inherent in complex, high-dimensional perceptual data, as exemplified
by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated
by the goal of designing AI systems with this capacity, recent work has focused
on evaluating whether neural networks can learn to solve RPM-like problems.
Previous work has generally found that strong performance on these problems
requires the incorporation of inductive biases that are specific to the RPM
problem format, raising the question of whether such models might be more
broadly useful. Here, we investigated the extent to which a general-purpose
mechanism for processing visual scenes in terms of objects might help promote
abstract visual reasoning. We found that a simple model, consisting only of an
object-centric encoder and a transformer reasoning module, achieved
state-of-the-art results on both of two challenging RPM-like benchmarks (PGM
and I-RAVEN), as well as a novel benchmark with greater visual complexity
(CLEVR-Matrices). These results suggest that an inductive bias for
object-centric processing may be a key component of abstract visual reasoning,
obviating the need for problem-specific inductive biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiTTO: A Feature Representation Imitation Approach for Improving Cross-Lingual Transfer. (arXiv:2303.02357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02357">
<div class="article-summary-box-inner">
<span><p>Zero-shot cross-lingual transfer is promising, however has been shown to be
sub-optimal, with inferior transfer performance across low-resource languages.
In this work, we envision languages as domains for improving zero-shot transfer
by jointly reducing the feature incongruity between the source and the target
language and increasing the generalization capabilities of pre-trained
multilingual transformers. We show that our approach, DiTTO, significantly
outperforms the standard zero-shot fine-tuning method on multiple datasets
across all languages using solely unlabeled instances in the target language.
Empirical results show that jointly reducing feature incongruity for multiple
target languages is vital for successful cross-lingual transfer. Moreover, our
model enables better cross-lingual transfer than standard fine-tuning methods,
even in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RweetMiner: Automatic identification and categorization of help requests on twitter during disasters. (arXiv:2303.02399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02399">
<div class="article-summary-box-inner">
<span><p>Catastrophic events create uncertain situations for humanitarian
organizations locating and providing aid to affected people. Many people turn
to social media during disasters for requesting help and/or providing relief to
others. However, the majority of social media posts seeking help could not
properly be detected and remained concealed because often they are noisy and
ill-formed. Existing systems lack in planning an effective strategy for tweet
preprocessing and grasping the contexts of tweets. This research, first of all,
formally defines request tweets in the context of social networking sites,
hereafter rweets, along with their different primary types and sub-types. Our
main contributions are the identification and categorization of rweets. For
rweet identification, we employ two approaches, namely a rule-based and
logistic regression, and show their high precision and F1 scores. The rweets
classification into sub-types such as medical, food, and shelter, using
logistic regression shows promising results and outperforms existing works.
Finally, we introduce an architecture to store intermediate data to accelerate
the development process of the machine learning classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges. (arXiv:2303.02411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02411">
<div class="article-summary-box-inner">
<span><p>Recent advancements in visiolinguistic (VL) learning have allowed the
development of multiple models and techniques that offer several impressive
implementations, able to currently resolve a variety of tasks that require the
collaboration of vision and language. Current datasets used for VL pre-training
only contain a limited amount of visual and linguistic knowledge, thus
significantly limiting the generalization capabilities of many VL models.
External knowledge sources such as knowledge graphs (KGs) and Large Language
Models (LLMs) are able to cover such generalization gaps by filling in missing
knowledge, resulting in the emergence of hybrid architectures. In the current
survey, we analyze tasks that have benefited from such hybrid approaches.
Moreover, we categorize existing knowledge sources and types, proceeding to
discussion regarding the KG vs LLM dilemma and its potential impact to future
hybrid approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02427">
<div class="article-summary-box-inner">
<span><p>We explore the possibility of meta-learning for the language-independent
unsupervised tokenization problem for English, Russian, and Chinese. We
implement the meta-learning approach for automatic determination of
hyper-parameters of the unsupervised tokenization model proposed in earlier
works, relying on various human-independent fitness functions such as
normalised anti-entropy, compression factor and cross-split F 1 score, as well
as additive and multiplicative composite combinations of the three metrics,
testing them against the conventional F1 tokenization score. We find a fairly
good correlation between the latter and the additive combination of the former
three metrics for English and Russian. In case of Chinese, we find a
significant correlation between the F 1 score and the compression factor. Our
results suggest the possibility of robust unsupervised tokenization of
low-resource and dead languages and allow us to think about human languages in
terms of the evolution of efficient symbolic communication codes with different
structural optimisation schemes that have evolved in different human cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lon-e{\aa} at SemEval-2023 Task 11: A Comparison of\\Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02468">
<div class="article-summary-box-inner">
<span><p>We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Quantum Classifiers for Natural-Language Text. (arXiv:2303.02469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02469">
<div class="article-summary-box-inner">
<span><p>As part of the recent research effort on quantum natural language processing
(QNLP), variational quantum sentence classifiers (VQSCs) have been implemented
and supported in lambeq / DisCoPy, based on the DisCoCat model of sentence
meaning. We discuss in some detail VQSCs, including category theory, DisCoCat
for modeling sentence as string diagram, and DisCoPy for encoding string
diagram as parameterized quantum circuit. Many NLP tasks, however, require the
handling of text consisting of multiple sentences, which is not supported in
lambeq / DisCoPy. A good example is sentiment classification of customer
feedback or product review. We discuss three potential approaches to
variational quantum text classifiers (VQTCs), in line with VQSCs. The first is
a weighted bag-of-sentences approach which treats text as a group of
independent sentences with task-specific sentence weighting. The second is a
coreference resolution approach which treats text as a consolidation of its
member sentences with coreferences among them resolved. Both approaches are
based on the DisCoCat model and should be implementable in lambeq / DisCoCat.
The third approach, on the other hand, is based on the DisCoCirc model which
considers both ordering of sentences and interaction of words in composing text
meaning from word and sentence meanings. DisCoCirc makes fundamental
modification of DisCoCat since a sentence in DisCoCirc updates meanings of
words, whereas all meanings are static in DisCoCat. It is not clear if
DisCoCirc can be implemented in lambeq / DisCoCat without breaking DisCoCat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02472">
<div class="article-summary-box-inner">
<span><p>Studies have shown that modern neural networks tend to be poorly calibrated
due to over-confident predictions. Traditionally, post-processing methods have
been used to calibrate the model after training. In recent years, various
trainable calibration measures have been proposed to incorporate them directly
into the training process. However, these methods all incorporate internal
hyperparameters, and the performance of these calibration objectives relies on
tuning these hyperparameters, incurring more computational costs as the size of
neural networks and datasets become larger. As such, we present Expected
Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable
calibration objective loss, where we view the calibration error from the
perspective of the squared difference between the two expectations. With
extensive experiments on several architectures (CNNs, Transformers) and
datasets, we demonstrate that (1) incorporating ESD into the training improves
model calibration in various batch size settings without the need for internal
hyperparameter tuning, (2) ESD yields the best-calibrated results compared with
previous approaches, and (3) ESD drastically improves the computational costs
required for calibration during training due to the absence of internal
hyperparameter. The code is publicly accessible at
https://github.com/hee-suk-yoon/ESD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09099">
<div class="article-summary-box-inner">
<span><p>Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logic Traps in Evaluating Attribution Scores. (arXiv:2109.05463v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05463">
<div class="article-summary-box-inner">
<span><p>Modern deep learning models are notoriously opaque, which has motivated the
development of methods for interpreting how deep models predict. This goal is
usually approached with attribution method, which assesses the influence of
features on model predictions. As an explanation method, the evaluation
criteria of attribution methods is how accurately it re-reflects the actual
reasoning process of the model (faithfulness). Meanwhile, since the reasoning
process of deep models is inaccessible, researchers design various evaluation
methods to demonstrate their arguments. However, some crucial logic traps in
these evaluation methods are ignored in most works, causing inaccurate
evaluation and unfair comparison. This paper systematically reviews existing
methods for evaluating attribution scores and summarizes the logic traps in
these methods. We further conduct experiments to demonstrate the existence of
each logic trap. Through both the theoretical and experimental analysis, we
hope to increase attention on the inaccurate evaluation of attribution scores.
Moreover, with this paper, we suggest stopping focusing on improving
performance under unreliable evaluation systems and starting efforts on
reducing the impact of proposed logic traps
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguisic Reasoning. (arXiv:2111.10756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10756">
<div class="article-summary-box-inner">
<span><p>Numerous visio-linguistic (V+L) representation learning methods have been
developed, yet existing datasets do not adequately evaluate the extent to which
they represent visual and linguistic concepts in a unified space. We propose
several novel evaluation settings for V+L models, including cross-modal
transfer. Furthermore, existing V+L benchmarks often report global accuracy
scores on the entire dataset, making it difficult to pinpoint the specific
reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic
dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows
us to constrain its training and testing distributions along task-relevant
dimensions, enabling the evaluation of out-of-distribution generalisation. Each
example in TraVLR redundantly encodes the scene in two modalities, allowing
either to be dropped or added during training or testing without losing
relevant information. We compare the performance of four state-of-the-art V+L
models, finding that while they perform well on test examples from the same
modality, they all fail at cross-modal transfer and have limited success
accommodating the addition or deletion of one modality. We release TraVLR as an
open challenge for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Memorization Across Neural Language Models. (arXiv:2202.07646v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07646">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
</p>
<p>We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes more complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Free Attentive Scoring for Speaker Verification. (arXiv:2203.05642v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05642">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel study of parameter-free attentive scoring for
speaker verification. Parameter-free scoring provides the flexibility of
comparing speaker representations without the need of an accompanying
parametric scoring model. Inspired by the attention component in Transformer
neural networks, we propose a variant of the scaled dot product attention
mechanism to compare enrollment and test segment representations. In addition,
this work explores the effect on performance of (i) different types of
normalization, (ii) independent versus tied query/key estimation, (iii) varying
the number of key-value pairs and (iv) pooling multiple enrollment utterance
statistics. Experimental results for a 4 task average show that a simple
parameter-free attentive scoring mechanism can improve the average EER by 10%
over the best cosine similarity baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03592">
<div class="article-summary-box-inner">
<span><p>Neural network language models can serve as computational hypotheses about
how humans process language. We compared the model-human consistency of diverse
language models using a novel experimental approach: controversial sentence
pairs. For each controversial sentence pair, two language models disagree about
which sentence is more likely to occur in natural text. Considering nine
language models (including n-gram, recurrent neural networks, and transformer
models), we created hundreds of such controversial sentence pairs by either
selecting sentences from a corpus or synthetically optimizing sentence pairs to
be highly controversial. Human subjects then provided judgments indicating for
each pair which of the two sentences is more likely. Controversial sentence
pairs proved highly effective at revealing model failures and identifying
models that aligned most closely with human judgments. The most
human-consistent model tested was GPT-2, although experiments also revealed
significant shortcomings of its alignment with human perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepStruct: Pretraining of Language Models for Structure Prediction. (arXiv:2205.10475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10475">
<div class="article-summary-box-inner">
<span><p>We introduce a method for improving the structural understanding abilities of
language models. Unlike previous approaches that finetune the models with
task-specific augmentation, we pretrain language models on a collection of
task-agnostic corpora to generate structures from text. Our structure
pretraining enables zero-shot transfer of the learned knowledge that models
have about the structure tasks. We study the performance of this approach on 28
datasets, spanning 10 structure prediction tasks including open information
extraction, joint entity and relation extraction, named entity recognition,
relation classification, semantic role labeling, event extraction, coreference
resolution, factual probe, intent detection, and dialogue state tracking. We
further enhance the pretraining with the task-specific training sets. We show
that a 10B parameter language model transfers non-trivially to most tasks and
obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations. (arXiv:2207.04154v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04154">
<div class="article-summary-box-inner">
<span><p>Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have become more complex, making them
harder to understand. To this end, researchers have proposed several techniques
to explain model predictions. However, practitioners struggle to use these
explainability techniques because they often do not know which one to choose
and how to interpret the results of the explanations. In this work, we address
these challenges by introducing TalkToModel: an interactive dialogue system for
explaining machine learning models through conversations. Specifically,
TalkToModel comprises of three key components: 1) a natural language interface
for engaging in conversations, making ML model explainability highly
accessible, 2) a dialogue engine that adapts to any tabular model and dataset,
interprets natural language, maps it to appropriate explanations, and generates
text responses, and 3) an execution component that constructs the explanations.
We carried out extensive quantitative and human subject evaluations of
TalkToModel. Overall, we found the conversational system understands user
inputs on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In real-world evaluations
with humans, 73% of healthcare workers (e.g., doctors and nurses) agreed they
would use TalkToModel over baseline point-and-click systems for explainability
in a disease prediction task, and 85% of ML professionals agreed TalkToModel
was easier to use for computing explanations. Our findings demonstrate that
TalkToModel is more effective for model explainability than existing systems,
introducing a new category of explainability tools for practitioners. Code &amp;
demo released here: https://github.com/dylan-slack/TalkToModel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation. (arXiv:2208.05309v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.05309">
<div class="article-summary-box-inner">
<span><p>Although the problem of hallucinations in neural machine translation (NMT)
has received some attention, research on this highly pathological phenomenon
lacks solid ground. Previous work has been limited in several ways: it often
resorts to artificial settings where the problem is amplified, it disregards
some (common) types of hallucinations, and it does not validate adequacy of
detection heuristics. In this paper, we set foundations for the study of NMT
hallucinations. First, we work in a natural setting, i.e., in-domain data
without artificial noise neither in training nor in inference. Next, we
annotate a dataset of over 3.4k sentences indicating different kinds of
critical errors and hallucinations. Then, we turn to detection methods and both
revisit methods used previously and propose using glass-box uncertainty-based
detectors. Overall, we show that for preventive settings, (i) previously used
methods are largely inadequate, (ii) sequence log-probability works best and
performs on par with reference-based methods. Finally, we propose
DeHallucinator, a simple method for alleviating hallucinations at test time
that significantly reduces the hallucinatory rate. To ease future research, we
release our annotated dataset for WMT18 German-English data, along with the
model, training data, and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AudioGen: Textually Guided Audio Generation. (arXiv:2209.15352v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15352">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of generating audio samples conditioned on descriptive
text captions. In this work, we propose AaudioGen, an auto-regressive
generative model that generates audio samples conditioned on text inputs.
AudioGen operates on a learnt discrete audio representation. The task of
text-to-audio generation poses multiple challenges. Due to the way audio
travels through a medium, differentiating ``objects'' can be a difficult task
(e.g., separating multiple people simultaneously speaking). This is further
complicated by real-world recording conditions (e.g., background noise,
reverberation, etc.). Scarce text annotations impose another constraint,
limiting the ability to scale models. Finally, modeling high-fidelity audio
requires encoding audio at high sampling rate, leading to extremely long
sequences. To alleviate the aforementioned challenges we propose an
augmentation technique that mixes different audio samples, driving the model to
internally learn to separate multiple sources. We curated 10 datasets
containing different types of audio and text annotations to handle the scarcity
of text-audio data points. For faster inference, we explore the use of
multi-stream modeling, allowing the use of shorter sequences while maintaining
a similar bitrate and perceptual quality. We apply classifier-free guidance to
improve adherence to text. Comparing to the evaluated baselines, AudioGen
outperforms over both objective and subjective metrics. Finally, we explore the
ability of the proposed method to generate audio continuation conditionally and
unconditionally. Samples: https://felixkreuk.github.io/audiogen
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparency Helps Reveal When Language Models Learn Meaning. (arXiv:2210.07468v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07468">
<div class="article-summary-box-inner">
<span><p>Many current NLP systems are built from language models trained to optimize
unsupervised objectives on large amounts of raw text. Under what conditions
might such a procedure acquire meaning? Our systematic experiments with
synthetic data reveal that, with languages where all expressions have
context-independent denotations (i.e., languages with strong transparency),
both autoregressive and masked language models successfully learn to emulate
semantic relations between expressions. However, when denotations are changed
to be context-dependent with the language otherwise unmodified, this ability
degrades. Turning to natural language, our experiments with a specific
phenomenon -- referential opacity -- add to the growing body of evidence that
current language models do not represent natural language semantics well. We
show this failure relates to the context-dependent nature of natural language
form-meaning mappings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss. (arXiv:2210.10305v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10305">
<div class="article-summary-box-inner">
<span><p>For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Segmentation in a Two-Pass Cascaded Encoder ASR Model. (arXiv:2211.15432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15432">
<div class="article-summary-box-inner">
<span><p>We explore unifying a neural segmenter with two-pass cascaded encoder ASR
into a single model. A key challenge is allowing the segmenter (which runs in
real-time, synchronously with the decoder) to finalize the 2nd pass (which runs
900 ms behind real-time) without introducing user-perceived latency or deletion
errors during inference. We propose a design where the neural segmenter is
integrated with the causal 1st pass decoder to emit a end-of-segment (EOS)
signal in real-time. The EOS signal is then used to finalize the non-causal 2nd
pass. We experiment with different ways to finalize the 2nd pass, and find that
a novel dummy frame injection strategy allows for simultaneous high quality 2nd
pass results and low finalization latency. On a real-world long-form captioning
task (YouTube), we achieve 2.4% relative WER and 140 ms EOS latency gains over
a baseline VAD-based segmenter with the same cascaded encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02676">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is important for language models to be
helpful and useful for humans, and to align with human and social values. Prior
work have achieved remarkable successes by learning from human feedback to
understand and follow instructions. Nonetheless, these methods are either
founded on hand-picked model generations that are favored by human annotators,
rendering them ineffective in terms of data utilization and challenging to
apply in general, or they depend on reward functions and reinforcement
learning, which are prone to imperfect reward function and extremely
challenging to optimize. In this work, we propose a novel technique, Chain of
Hindsight, that is easy to optimize and can learn from any form of feedback,
regardless of its polarity. Our idea is inspired by how humans learn from
extensive feedback presented in the form of languages. We convert all types of
feedback into sentences, which are then used to fine-tune the model, allowing
us to take advantage of the language comprehension capabilities of language
models. We condition the model on a sequence of model generations paired with
feedback. By doing so, models are trained to generate outputs based on
feedback, and models can learn to identify and correct negative attributes or
errors. Applying our method to large language models, we observed that Chain of
Hindsight significantly surpasses previous methods in aligning language models
with human preferences. We observed significant improvements on summarization
and dialogue tasks and our approach is markedly preferred in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03494">
<div class="article-summary-box-inner">
<span><p>Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Exemplars for In-context Learning. (arXiv:2302.05698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05698">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models (LMs) have shown impressive In-Context
Learning (ICL) ability, where the model learns to do an unseen task via a
prompt consisting of input-output examples as the demonstration, without any
parameter updates. The performance of ICL is highly dominated by the quality of
the selected in-context examples. However, previous selection methods are
mostly based on simple heuristics, leading to sub-optimal performance. In this
work, we formulate in-context example selection as a subset selection problem.
We propose CEIL (Compositional Exemplars for In-context Learning), which is
instantiated by Determinantal Point Processes (DPPs) to model the interaction
between the given input and in-context examples, and optimized through a
carefully-designed contrastive learning objective to obtain preference from
LMs. We validate CEIL on 12 classification and generation datasets from 7
distinct NLP tasks, including sentiment analysis, paraphrase detection, natural
language inference, commonsense reasoning, open-domain question answering, code
generation, and semantic parsing. Extensive experiments demonstrate not only
the state-of-the-art performance but also the transferability and
compositionality of CEIL, shedding new light on effective and efficient
in-context learning. Our code is released at
https://github.com/HKUNLP/icl-ceil.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sidecar Separator Can Convert a Single-Talker Speech Recognition System to a Multi-Talker One. (arXiv:2302.09908v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09908">
<div class="article-summary-box-inner">
<span><p>Although automatic speech recognition (ASR) can perform well in common
non-overlapping environments, sustaining performance in multi-talker
overlapping speech recognition remains challenging. Recent research revealed
that ASR model's encoder captures different levels of information with
different layers -- the lower layers tend to have more acoustic information,
and the upper layers more linguistic. This inspires us to develop a Sidecar
separator to empower a well-trained ASR model for multi-talker scenarios by
separating the mixed speech embedding between two suitable layers. We
experimented with a wav2vec 2.0-based ASR model with a Sidecar mounted. By
freezing the parameters of the original model and training only the Sidecar
(8.7 M, 8.4% of all parameters), the proposed approach outperforms the previous
state-of-the-art by a large margin for the 2-speaker mixed LibriMix dataset,
reaching a word error rate (WER) of 10.36%; and obtains comparable results
(7.56%) for LibriSpeechMix dataset when limited training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Semantics for Test Completion. (arXiv:2302.10166v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10166">
<div class="article-summary-box-inner">
<span><p>Writing tests is a time-consuming yet essential task during software
development. We propose to leverage recent advances in deep learning for text
and code generation to assist developers in writing tests. We formalize the
novel task of test completion to automatically complete the next statement in a
test method based on the context of prior statements and the code under test.
We develop TeCo -- a deep learning model using code semantics for test
completion. The key insight underlying TeCo is that predicting the next
statement in a test method requires reasoning about code execution, which is
hard to do with only syntax-level data that existing code completion models
use. TeCo extracts and uses six kinds of code semantics data, including the
execution result of prior statements and the execution context of the test
method. To provide a testbed for this new task, as well as to evaluate TeCo, we
collect a corpus of 130,934 test methods from 1,270 open-source Java projects.
Our results show that TeCo achieves an exact-match accuracy of 18, which is 29%
higher than the best baseline using syntax-level data only. When measuring
functional correctness of generated next statement, TeCo can generate runnable
code in 29% of the cases compared to 18% obtained by the best baseline.
Moreover, TeCo is significantly better than prior work on test oracle
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10866">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have relied heavily on the use of large
Transformers due to their ability to learn at scale. However, the core building
block of Transformers, the attention operator, exhibits quadratic cost in
sequence length, limiting the amount of context accessible. Existing
subquadratic methods based on low-rank and sparse approximations need to be
combined with dense attention layers to match Transformers, indicating a gap in
capability. In this work, we propose Hyena, a subquadratic drop-in replacement
for attention constructed by interleaving implicitly parametrized long
convolutions and data-controlled gating. In recall and reasoning tasks on
sequences of thousands to hundreds of thousands of tokens, Hyena improves
accuracy by more than 50 points over operators relying on state-spaces and
other implicit and explicit methods, matching attention-based models. We set a
new state-of-the-art for dense-attention-free architectures on language
modeling in standard datasets (WikiText103 and The Pile), reaching Transformer
quality with a 20% reduction in training compute required at sequence length
2K. Hyena operators are twice as fast as highly optimized attention at sequence
length 8K, and 100x faster at sequence length 64K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancements in Federated Learning: Models, Methods, and Privacy. (arXiv:2302.11466v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11466">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is a promising technique for addressing the rising
privacy and security issues. Its main ingredient is to cooperatively learn the
model among the distributed clients without uploading any sensitive data. In
this paper, we conducted a thorough review of the related works, following the
development context and deeply mining the key technologies behind FL from both
theoretical and practical perspectives. Specifically, we first classify the
existing works in FL architecture based on the network topology of FL systems
with detailed analysis and summarization. Next, we abstract the current
application problems, summarize the general techniques and frame the
application problems into the general paradigm of FL base models. Moreover, we
provide our proposed solutions for model training via FL. We have summarized
and analyzed the existing FedOpt algorithms, and deeply revealed the
algorithmic development principles of many first-order algorithms in depth,
proposing a more generalized algorithm design framework. Based on these
frameworks, we have instantiated FedOpt algorithms. As privacy and security is
the fundamental requirement in FL, we provide the existing attack scenarios and
the defense methods. To the best of our knowledge, we are among the first tier
to review the theoretical methodology and propose our strategies since there
are very few works surveying the theoretical approaches. Our survey targets
motivating the development of high-performance, privacy-preserving, and secure
methods to integrate FL into real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using Distant Supervision. (arXiv:2303.01912v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01912">
<div class="article-summary-box-inner">
<span><p>Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are
important to study ancient Chinese, but the amount of ancient Chinese WSG and
POS tagging data is still rare. In this paper, we propose a novel augmentation
method of ancient Chinese WSG and POS tagging data using distant supervision
over parallel corpus. However, there are still mislabeled and unlabeled ancient
Chinese words inevitably in distant supervision. To address this problem, we
take advantage of the memorization effects of deep neural networks and a small
amount of annotated data to get a model with much knowledge and a little noise,
and then we use this model to relabel the ancient Chinese sentences in parallel
corpus. Experiments show that the model trained over the relabeled data
outperforms the model trained over the data generated from distant supervision
and the annotated data. Our code is available at
https://github.com/farlit/ACDS.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-07 23:13:51.550404602 UTC">2023-03-07 23:13:51 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>