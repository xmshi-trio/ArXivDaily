<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-21T01:30:00Z">02-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning. (arXiv:2302.09068v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09068">
<div class="article-summary-box-inner">
<span><p>We conduct a pilot study selectively evaluating the cognitive abilities
(decision making and spatial reasoning) of two recently released generative
transformer models, ChatGPT and DALL-E 2. Input prompts were constructed
following neutral a priori guidelines, rather than adversarial intent. Post hoc
qualitative analysis of the outputs shows that DALL-E 2 is able to generate at
least one correct image for each spatial reasoning prompt, but most images
generated are incorrect (even though the model seems to have a clear
understanding of the objects mentioned in the prompt). Similarly, in evaluating
ChatGPT on the rationality axioms developed under the classical Von
Neumann-Morgenstern utility theorem, we find that, although it demonstrates
some level of rational decision-making, many of its decisions violate at least
one of the axioms even under reasonable constructions of preferences, bets, and
decision-making prompts. ChatGPT's outputs on such problems generally tended to
be unpredictable: even as it made irrational decisions (or employed an
incorrect reasoning process) for some simpler decision-making problems, it was
able to draw correct conclusions for more complex bet structures. We briefly
comment on the nuances and challenges involved in scaling up such a 'cognitive'
evaluation or conducting it with a closed set of answer keys ('ground truth'),
given that these models are inherently generative and open-ended in responding
to prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction. (arXiv:2302.09122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09122">
<div class="article-summary-box-inner">
<span><p>Creative writing is hard: Novelists struggle with writer's block daily. While
automatic story generation has advanced recently, it is treated as a "toy task"
for advancing artificial intelligence rather than helping people. In this
paper, we create a system that produces a short description that narrates a
predicted plot using existing story generation approaches. Our goal is to
assist writers in crafting a consistent and compelling story arc. We conducted
experiments on Amazon Mechanical Turk (AMT) to examine the quality of the
generated story plots in terms of consistency and storiability. The results
show that short descriptions produced by our frame-enhanced GPT-2 (FGPT-2) were
rated as the most consistent and storiable among all models; FGPT-2's outputs
even beat some random story snippets written by humans. Next, we conducted a
preliminary user study using a story continuation task where AMT workers were
given access to machine-generated story plots and asked to write a follow-up
story. FGPT-2 could positively affect the writing process, though people favor
other baselines more. Our study shed some light on the possibilities of future
creative writing support systems beyond the scope of completing sentences. Our
code is available at: https://github.com/appleternity/Story-Plot-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-Guided Label Generation in Extreme Multi-Label Classification. (arXiv:2302.09150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09150">
<div class="article-summary-box-inner">
<span><p>For extreme multi-label classification (XMC), existing classification-based
models poorly perform for tail labels and often ignore the semantic relations
among labels, like treating "Wikipedia" and "Wiki" as independent and separate
labels. In this paper, we cast XMC as a generation task (XLGen), where we
benefit from pre-trained text-to-text models. However, generating labels from
the extremely large label space is challenging without any constraints or
guidance. We, therefore, propose to guide label generation using label cluster
information to hierarchically generate lower-level labels. We also find that
frequency-based label ordering and using decoding ensemble methods are critical
factors for the improvements in XLGen. XLGen with cluster guidance
significantly outperforms the classification and generation baselines on tail
labels, and also generally improves the overall performance in four popular XMC
benchmarks. In human evaluation, we also find XLGen generates unseen but
plausible labels. Our code is now available at
https://github.com/alexa/xlgen-eacl-2023.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Med-EASi: Finely Annotated Dataset and Models for Controllable Simplification of Medical Texts. (arXiv:2302.09155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09155">
<div class="article-summary-box-inner">
<span><p>Automatic medical text simplification can assist providers with
patient-friendly communication and make medical texts more accessible, thereby
improving health literacy. But curating a quality corpus for this task requires
the supervision of medical experts. In this work, we present
$\textbf{Med-EASi}$ ($\underline{\textbf{Med}}$ical dataset for
$\underline{\textbf{E}}$laborative and $\underline{\textbf{A}}$bstractive
$\underline{\textbf{Si}}$mplification), a uniquely crowdsourced and finely
annotated dataset for supervised simplification of short medical texts. Its
$\textit{expert-layman-AI collaborative}$ annotations facilitate
$\textit{controllability}$ over text simplification by marking four kinds of
textual transformations: elaboration, replacement, deletion, and insertion. To
learn medical text simplification, we fine-tune T5-large with four different
styles of input-output combinations, leading to two control-free and two
controllable versions of the model. We add two types of
$\textit{controllability}$ into text simplification, by using a multi-angle
training approach: $\textit{position-aware}$, which uses in-place annotated
inputs and outputs, and $\textit{position-agnostic}$, where the model only
knows the contents to be edited, but not their positions. Our results show that
our fine-grained annotations improve learning compared to the unannotated
baseline. Furthermore, $\textit{position-aware}$ control generates better
simplification than the $\textit{position-agnostic}$ one. The data and code are
available at https://github.com/Chandrayee/CTRL-SIMP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KILM: Knowledge Injection into Encoder-Decoder Language Models. (arXiv:2302.09170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09170">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (PLMs) have been shown to retain implicit
knowledge within their parameters. To enhance this implicit knowledge, we
propose Knowledge Injection into Language Models (KILM), a novel approach that
injects entity-related knowledge into encoder-decoder PLMs, via a generative
knowledge infilling objective through continued pre-training. This is done
without architectural modifications to the PLMs or adding additional
parameters. Experimental results over a suite of knowledge-intensive tasks
spanning numerous datasets show that KILM enables models to retain more
knowledge and hallucinate less, while preserving their original performance on
general NLU and NLG tasks. KILM also demonstrates improved zero-shot
performances on tasks such as entity disambiguation, outperforming
state-of-the-art models having 30x more parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09173">
<div class="article-summary-box-inner">
<span><p>This work explores the problem of generating task graphs of real-world
activities. Different from prior formulations, we consider a setting where text
transcripts of instructional videos performing a real-world activity (e.g.,
making coffee) are provided and the goal is to identify the key steps relevant
to the task as well as the dependency relationship between these key steps. We
propose a novel task graph generation approach that combines the reasoning
capabilities of instruction-tuned language models along with clustering and
ranking components to generate accurate task graphs in a completely
unsupervised manner. We show that the proposed approach generates more accurate
task graphs compared to a supervised learning approach on tasks from the ProceL
and CrossTask datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints. (arXiv:2302.09185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09185">
<div class="article-summary-box-inner">
<span><p>The limits of open-ended generative models are unclear, yet increasingly
important. What causes them to succeed and what causes them to fail? In this
paper, we take a prompt-centric approach to analyzing and bounding the
abilities of open-ended generative models. We present a generic methodology of
analysis with two challenging prompt constraint types: structural and
stylistic. These constraint types are categorized into a set of well-defined
constraints that are analyzable by a single prompt. We then systematically
create a diverse set of simple, natural, and useful prompts to robustly analyze
each individual constraint. Using the GPT-3 text-davinci-002 model as a case
study, we generate outputs from our collection of prompts and analyze the
model's generative failures. We also show the generalizability of our proposed
method on other large models like BLOOM and OPT. Our results and our in-context
mitigation strategies reveal open challenges for future research. We have
publicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction of Constituent Factors of Digestion Efficiency in Information Transfer by Media Composed of Texts and Images. (arXiv:2302.09189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09189">
<div class="article-summary-box-inner">
<span><p>The development and spread of information and communication technologies have
increased and diversified information. However, the increase in the volume and
the selection of information does not necessarily promote understanding. In
addition, conventional evaluations of information transfer have focused only on
the arrival of information to the receivers. They need to sufficiently take
into account the receivers' understanding of the information after it has been
acquired, which is the original purpose of the evaluation. In this study, we
propose the concept of "information digestion," which refers to the receivers'
correct understanding of the acquired information, its contents, and its
purpose. In the experiment, we proposed an evaluation model of information
digestibility using hierarchical factor analysis and extracted factors that
constitute digestibility by four types of media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09207">
<div class="article-summary-box-inner">
<span><p>This paper describes RetVec, a resilient multilingual embedding scheme
designed for neural-based text processing, including small-text classification
and large-language models. RetVec combines a novel character encoding with an
optional small model to embed words into a 256-dimensional vector space. These
embeddings enable training competitive multilingual text models resilient to
typos and adversarial attacks. In this paper, we evaluate and compare RetVec to
state-of-the-art tokenizers and word embeddings on common model architectures.
These comparisons demonstrate that RetVec leads to competitive models that are
significantly more resilient to text perturbations across a variety of common
tasks. RetVec is available under Apache 2 license at
\url{https://github.com/[anonymized]}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation. (arXiv:2302.09210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09210">
<div class="article-summary-box-inner">
<span><p>Generative Pre-trained Transformer (GPT) models have shown remarkable
capabilities for natural language generation, but their performance for machine
translation has not been thoroughly investigated. In this paper, we present a
comprehensive evaluation of GPT models for machine translation, covering
various aspects such as quality of different GPT models in comparison with
state-of-the-art research and commercial systems, effect of prompting
strategies, robustness towards domain shifts and document-level translation. We
experiment with eighteen different translation directions involving high and
low resource languages, as well as non English-centric translations, and
evaluate the performance of three GPT models: ChatGPT, GPT3.5
(text-davinci-003), and text-davinci-002. Our results show that GPT models
achieve very competitive translation quality for high resource languages, while
having limited capabilities for low resource languages. We also show that
hybrid approaches, which combine GPT models with other translation systems, can
further enhance the translation quality. We perform comprehensive analysis and
human evaluation to further understand the characteristics of GPT translations.
We hope that our paper provides valuable insights for researchers and
practitioners in the field and helps to better understand the potential and
limitations of GPT models for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLN-Trans: Translator for the Vision and Language Navigation Agent. (arXiv:2302.09230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09230">
<div class="article-summary-box-inner">
<span><p>Language understanding is essential for the navigation agent to follow
instructions. We observe two kinds of issues in the instructions that can make
the navigation task challenging: 1. The mentioned landmarks are not
recognizable by the navigation agent due to the different vision abilities of
the instructor and the modeled agent. 2. The mentioned landmarks are applicable
to multiple targets, thus not distinctive for selecting the target among the
candidate viewpoints. To deal with these issues, we design a translator module
for the navigation agent to convert the original instructions into
easy-to-follow sub-instruction representations at each step. The translator
needs to focus on the recognizable and distinctive landmarks based on the
agent's visual abilities and the observed visual environment. To achieve this
goal, we create a new synthetic sub-instruction dataset and design specific
tasks to train the translator and the navigation agent. We evaluate our
approach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last)
datasets and achieve state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Prompt Generation for Semi-supervised Learning with Language Models. (arXiv:2302.09236v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09236">
<div class="article-summary-box-inner">
<span><p>Prompt-based learning methods in semi-supervised learning (SSL) settings have
been shown to be effective on multiple natural language understanding (NLU)
datasets and tasks in the literature. However, manually designing multiple
prompts and verbalizers requires domain knowledge and human effort, making it
difficult and expensive to scale across different datasets. In this paper, we
propose two methods to automatically design multiple prompts and integrate
automatic verbalizer in SSL settings without sacrificing performance. The first
method uses various demonstration examples with learnable continuous prompt
tokens to create diverse prompt models. The second method uses a varying number
of soft prompt tokens to encourage language models to learn different prompts.
For the verbalizer, we use the prototypical verbalizer to replace the manual
one. In summary, we obtained the best average accuracy of 73.2% (a relative
improvement of 2.52% over even the previous state-of-the-art SSL method with
manual prompts and verbalizers) in different few-shot learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Federated Approach for Hate Speech Detection. (arXiv:2302.09243v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09243">
<div class="article-summary-box-inner">
<span><p>Hate speech detection has been the subject of high research attention, due to
the scale of content created on social media. In spite of the attention and the
sensitive nature of the task, privacy preservation in hate speech detection has
remained under-studied. The majority of research has focused on centralised
machine learning infrastructures which risk leaking data. In this paper, we
show that using federated machine learning can help address privacy the
concerns that are inherent to hate speech detection while obtaining up to 6.81%
improvement in terms of F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Tricks for Effective Language Model Pretraining and Downstream Adaptation: A Case Study on GLUE. (arXiv:2302.09268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09268">
<div class="article-summary-box-inner">
<span><p>This technical report briefly describes our JDExplore d-team's submission
Vega v1 on the General Language Understanding Evaluation (GLUE) leaderboard,
where GLUE is a collection of nine natural language understanding tasks,
including question answering, linguistic acceptability, sentiment analysis,
text similarity, paraphrase detection, and natural language inference. [Method]
We investigate several effective strategies and choose their best combination
setting as the training recipes. As for model structure, we employ the vanilla
Transformer with disentangled attention as the basic block encoder. For
self-supervised training, we employ the representative denoising objective
(i.e., replaced token detection) in phase 1 and combine the contrastive
objective (i.e., sentence embedding contrastive learning) with it in phase 2.
During fine-tuning, several advanced techniques such as transductive
fine-tuning, self-calibrated fine-tuning, and adversarial fine-tuning are
adopted. [Results] According to our submission record (Jan. 2022), with our
optimized pretraining and fine-tuning strategies, our 1.3 billion model sets
new state-of-the-art on 4/9 tasks, achieving the best average score of 91.3.
Encouragingly, our Vega v1 is the first to exceed powerful human performance on
the two challenging tasks, i.e., SST-2 and WNLI. We believe our empirically
successful recipe with a bag of tricks could shed new light on developing
efficient discriminative large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-Engine-augmented Dialogue Response Generation with Cheaply Supervised Query Production. (arXiv:2302.09300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09300">
<div class="article-summary-box-inner">
<span><p>Knowledge-aided dialogue response generation aims at augmenting chatbots with
relevant external knowledge in the hope of generating more informative
responses. The majority of previous work assumes that the relevant knowledge is
given as input or retrieved from a static pool of knowledge. However, this
assumption violates the real-world situation, where knowledge is continually
updated and a chatbot has to dynamically retrieve useful knowledge. We propose
a dialogue model that can access the vast and dynamic information from any
search engine for response generation. As the core module, a query producer is
used to generate queries from a dialogue context to interact with a search
engine. We design a training algorithm using cheap noisy supervision for the
query producer, where the signals are obtained by comparing retrieved articles
with the next dialogue response. As the result, the query producer is adjusted
without any human annotation of gold queries, making it easily transferable to
other domains and search engines. Experiments show that our query producer can
achieve R@1 and R@5 rates of 62.4% and 74.8% for retrieving gold knowledge, and
the overall model generates better responses over strong knowledge-aided
baselines using BART and other typical systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension. (arXiv:2302.09301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09301">
<div class="article-summary-box-inner">
<span><p>Prompting has become an important mechanism by which users can more
effectively interact with many flavors of foundation model. Indeed, the last
several years have shown that well-honed prompts can sometimes unlock emergent
capabilities within such models. While there has been a substantial amount of
empirical exploration of prompting within the community, relatively few works
have studied prompting at a mathematical level. In this work we aim to take a
first step towards understanding basic geometric properties induced by prompts
in Stable Diffusion, focusing on the intrinsic dimension of internal
representations within the model. We find that choice of prompt has a
substantial impact on the intrinsic dimension of representations at both layers
of the model which we explored, but that the nature of this impact depends on
the layer being considered. For example, in certain bottleneck layers of the
model, intrinsic dimension of representations is correlated with prompt
perplexity (measured using a surrogate model), while this correlation is not
apparent in the latent layers. Our evidence suggests that intrinsic dimension
could be a useful tool for future studies of the impact of different prompts on
text-to-image models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge the Gap between Language models and Tabular Understanding. (arXiv:2302.09302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09302">
<div class="article-summary-box-inner">
<span><p>Table pretrain-then-finetune paradigm has been proposed and employed at a
rapid pace after the success of pre-training in the natural language domain.
Despite the promising findings in tabular pre-trained language models (TPLMs),
there is an input gap between pre-training and fine-tuning phases. For
instance, TPLMs jointly pre-trained with table and text input could be
effective for tasks also with table-text joint input like table question
answering, but it may fail for tasks with only tables or text as input such as
table retrieval. To this end, we propose UTP, an approach that dynamically
supports three types of multi-modal inputs: table-text, table, and text.
Specifically, UTP is pre-trained with two strategies: (1) We first utilize a
universal mask language modeling objective on each kind of input, enforcing the
model to adapt various inputs. (2) We then present Cross-Modal Contrastive
Regularization (CMCR), which utilizes contrastive learning to encourage the
consistency between table-text cross-modality representations via unsupervised
instance-wise training signals during pre-training. By these means, the
resulting model not only bridges the input gap between pre-training and
fine-tuning but also advances in the alignment of table and text. Extensive
results show UTP achieves superior results on uni-modal input tasks (e.g.,
table retrieval) and cross-modal input tasks (e.g., table question answering).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stress Test for BERT and Deep Models: Predicting Words from Italian Poetry. (arXiv:2302.09303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09303">
<div class="article-summary-box-inner">
<span><p>In this paper we present a set of experiments carried out with BERT on a
number of Italian sentences taken from poetry domain. The experiments are
organized on the hypothesis of a very high level of difficulty in
predictability at the three levels of linguistic complexity that we intend to
monitor: lexical, syntactic and semantic level. To test this hypothesis we ran
the Italian version of BERT with 80 sentences for a total of 900 tokens mostly
extracted from Italian poetry of the first half of last century. Then we
alternated canonical and noncanonical versions of the same sentence before
processing them with the same DL model. We used then sentences from the
newswire domain containing similar syntactic structures. The results show that
the DL model is highly sensitive to presence of noncanonical structures.
However, DLs are also very sensitive to word frequency and to local non literal
meaning compositional effect. This is also apparent by the preference for
predicting function vs content words, collocates vs infrequent word phrases. In
the paper, we focused our attention on the use of subword units done by BERT
for out of vocabulary words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretability in Activation Space Analysis of Transformers: A Focused Survey. (arXiv:2302.09304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09304">
<div class="article-summary-box-inner">
<span><p>The field of natural language processing has reached breakthroughs with the
advent of transformers. They have remained state-of-the-art since then, and
there also has been much research in analyzing, interpreting, and evaluating
the attention layers and the underlying embedding space. In addition to the
self-attention layers, the feed-forward layers in the transformer are a
prominent architectural component. From extensive research, we observe that its
role is under-explored. We focus on the latent space, known as the Activation
Space, that consists of the neuron activations from these feed-forward layers.
In this survey paper, we review interpretability methods that examine the
learnings that occurred in this activation space. Since there exists only
limited research in this direction, we conduct a detailed examination of each
work and point out potential future directions of research. We hope our work
provides a step towards strengthening activation space analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimising Human-Machine Collaboration for Efficient High-Precision Information Extraction from Text Documents. (arXiv:2302.09324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09324">
<div class="article-summary-box-inner">
<span><p>While humans can extract information from unstructured text with high
precision and recall, this is often too time-consuming to be practical.
Automated approaches, on the other hand, produce nearly-immediate results, but
may not be reliable enough for high-stakes applications where precision is
essential. In this work, we consider the benefits and drawbacks of various
human-only, human-machine, and machine-only information extraction approaches.
We argue for the utility of a human-in-the-loop approach in applications where
high precision is required, but purely manual extraction is infeasible. We
present a framework and an accompanying tool for information extraction using
weak-supervision labelling with human validation. We demonstrate our approach
on three criminal justice datasets. We find that the combination of computer
speed and human understanding yields precision comparable to manual annotation
while requiring only a fraction of time, and significantly outperforms fully
automated baselines in terms of precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformadores: Fundamentos teoricos y Aplicaciones. (arXiv:2302.09327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09327">
<div class="article-summary-box-inner">
<span><p>Transformers are a neural network architecture originally designed for
natural language processing that it is now a mainstream tool for solving a wide
variety of problems, including natural language processing, sound, image,
reinforcement learning, and other problems with heterogeneous input data. Its
distinctive feature is its self-attention system, based on attention to one's
own sequence, which derives from the previously introduced attention system.
This article provides the reader with the necessary context to understand the
most recent research articles and presents the mathematical and algorithmic
foundations of the elements that make up this type of network. The different
components that make up this architecture and the variations that may exist are
also studied, as well as some applications of the transformer models. This
article is in Spanish to bring this scientific knowledge to the
Spanish-speaking community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Front-End Adapter: Adapting Front-End Input of Speech based Self-Supervised Learning for Speech Recognition. (arXiv:2302.09331v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09331">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed a boom in self-supervised learning (SSL) in
various areas including speech processing. Speech based SSL models present
promising performance in a range of speech related tasks. However, the training
of SSL models is computationally expensive and a common practice is to
fine-tune a released SSL model on the specific task. It is essential to use
consistent front-end input during pre-training and fine-tuning. This
consistency may introduce potential issues when the optimal front-end is not
the same as that used in pre-training. In this paper, we propose a simple but
effective front-end adapter to address this front-end discrepancy. By
minimizing the distance between the outputs of different front-ends, the
filterbank feature (Fbank) can be compatible with SSL models which are
pre-trained with waveform. The experiment results demonstrate the effectiveness
of our proposed front-end adapter on several popular SSL models for the speech
recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Out-Of-Distribution Generalization Capability of Language Models: Counterfactually-Augmented Data is not Enough. (arXiv:2302.09345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09345">
<div class="article-summary-box-inner">
<span><p>Counterfactually-Augmented Data (CAD) has the potential to improve language
models' Out-Of-Distribution (OOD) generalization capability, as CAD induces
language models to exploit causal features and exclude spurious correlations.
However, the empirical results of OOD generalization on CAD are not as
efficient as expected. In this paper, we attribute the inefficiency to Myopia
Phenomenon caused by CAD: language models only focus on causal features that
are edited in the augmentation and exclude other non-edited causal features. As
a result, the potential of CAD is not fully exploited. Based on the structural
properties of CAD, we design two additional constraints to help language models
extract more complete causal features contained in CAD, thus improving the OOD
generalization capability. We evaluate our method on two tasks: Sentiment
Analysis and Natural Language Inference, and the experimental results
demonstrate that our method could unlock CAD's potential and improve language
models' OOD generalization capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT is not The Count: Learning to Match Mathematical Statements with Proofs. (arXiv:2302.09350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09350">
<div class="article-summary-box-inner">
<span><p>We introduce a task consisting in matching a proof to a given mathematical
statement. The task fits well within current research on Mathematical
Information Retrieval and, more generally, mathematical article analysis
(Mathematical Sciences, 2014). We present a dataset for the task (the MATcH
dataset) consisting of over 180k statement-proof pairs extracted from modern
mathematical research articles. We find this dataset highly representative of
our task, as it consists of relatively new findings useful to mathematicians.
We propose a bilinear similarity model and two decoding methods to match
statements to proofs effectively. While the first decoding method matches a
proof to a statement without being aware of other statements or proofs, the
second method treats the task as a global matching problem. Through a symbol
replacement procedure, we analyze the "insights" that pre-trained language
models have in such mathematical article analysis and show that while these
models perform well on this task with the best performing mean reciprocal rank
of 73.7, they follow a relatively shallow symbolic analysis and matching to
achieve that performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation. (arXiv:2302.09368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09368">
<div class="article-summary-box-inner">
<span><p>Natural Language-conditioned reinforcement learning (RL) enables the agents
to follow human instructions. Previous approaches generally implemented
language-conditioned RL by providing human instructions in natural language
(NL) and training a following policy. In this outside-in approach, the policy
needs to comprehend the NL and manage the task simultaneously. However, the
unbounded NL examples often bring much extra complexity for solving concrete RL
tasks, which can distract policy learning from completing the task. To ease the
learning burden of the policy, we investigate an inside-out scheme for natural
language-conditioned RL by developing a task language (TL) that is task-related
and unique. The TL is used in RL to achieve highly efficient and effective
policy training. Besides, a translator is trained to translate NL into TL. We
implement this scheme as TALAR (TAsk Language with predicAte Representation)
that learns multiple predicates to model object relationships as the TL.
Experiments indicate that TALAR not only better comprehends NL instructions but
also leads to a better instruction-following policy that improves 13.4% success
rate and adapts to unseen expressions of NL instruction. The TL can also be an
effective task abstraction, naturally compatible with hierarchical RL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker and Language Change Detection using Wav2vec2 and Whisper. (arXiv:2302.09381v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09381">
<div class="article-summary-box-inner">
<span><p>We investigate recent transformer networks pre-trained for automatic speech
recognition for their ability to detect speaker and language changes in speech.
We do this by simply adding speaker (change) or language targets to the labels.
For Wav2vec2 pre-trained networks, we also investigate if the representation
for the speaker change symbol can be conditioned to capture speaker identity
characteristics. Using a number of constructed data sets we show that these
capabilities are definitely there, with speaker recognition equal error rates
of the order of 10% and language detection error rates of a few percent. We
will publish the code for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-SENSE: Modeling Narrative Structure in Short Personal Narratives Using Protagonist's Mental Representations. (arXiv:2302.09418v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09418">
<div class="article-summary-box-inner">
<span><p>Narrative is a ubiquitous component of human communication. Understanding its
structure plays a critical role in a wide variety of applications, ranging from
simple comparative analyses to enhanced narrative retrieval, comprehension, or
reasoning capabilities. Prior research in narratology has highlighted the
importance of studying the links between cognitive and linguistic aspects of
narratives for effective comprehension. This interdependence is related to the
textual semantics and mental language in narratives, referring to characters'
motivations, feelings or emotions, and beliefs. However, this interdependence
is hardly explored for modeling narratives. In this work, we propose the task
of automatically detecting prominent elements of the narrative structure by
analyzing the role of characters' inferred mental state along with linguistic
information at the syntactic and semantic levels. We introduce a STORIES
dataset of short personal narratives containing manual annotations of key
elements of narrative structure, specifically climax and resolution. To this
end, we implement a computational model that leverages the protagonist's mental
state information obtained from a pre-trained model trained on social
commonsense knowledge and integrates their representations with contextual
semantic embed-dings using a multi-feature fusion approach. Evaluating against
prior zero-shot and supervised baselines, we find that our model is able to
achieve significant improvements in the task of identifying climax and
resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09419">
<div class="article-summary-box-inner">
<span><p>The Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A pretrained
foundation model, such as BERT, GPT-3, MAE, DALLE-E, and ChatGPT, is trained on
large-scale data which provides a reasonable parameter initialization for a
wide range of downstream applications. The idea of pretraining behind PFMs
plays an important role in the application of large models. Different from
previous methods that apply convolution and recurrent modules for feature
extractions, the generative pre-training (GPT) method applies Transformer as
the feature extractor and is trained on large datasets with an autoregressive
paradigm. Similarly, the BERT apples transformers to train on large datasets as
a contextual language model. Recently, the ChatGPT shows promising success on
large language models, which applies an autoregressive language model with zero
shot or few show prompting. With the extraordinary success of PFMs, AI has made
waves in a variety of fields over the past few years. Considerable methods,
datasets, and evaluation metrics have been proposed in the literature, the need
is raising for an updated survey. This study provides a comprehensive review of
recent research advancements, current and future challenges, and opportunities
for PFMs in text, image, graph, as well as other data modalities. We first
review the basic components and existing pretraining in natural language
processing, computer vision, and graph learning. We then discuss other advanced
PFMs for other data modalities and unified PFMs considering the data quality
and quantity. Besides, we discuss relevant research about the fundamentals of
the PFM, including model efficiency and compression, security, and privacy.
Finally, we lay out key implications, future research directions, challenges,
and open problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and Few-Shot Localization of Task-Oriented Dialogue Agents with a Distilled Representation. (arXiv:2302.09424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09424">
<div class="article-summary-box-inner">
<span><p>Task-oriented Dialogue (ToD) agents are mostly limited to a few widely-spoken
languages, mainly due to the high cost of acquiring training data for each
language. Existing low-cost approaches that rely on cross-lingual embeddings or
naive machine translation sacrifice a lot of accuracy for data efficiency, and
largely fail in creating a usable dialogue agent. We propose automatic methods
that use ToD training data in a source language to build a high-quality
functioning dialogue agent in another target language that has no training data
(i.e. zero-shot) or a small training set (i.e. few-shot). Unlike most prior
work in cross-lingual ToD that only focuses on Dialogue State Tracking (DST),
we build an end-to-end agent.
</p>
<p>We show that our approach closes the accuracy gap between few-shot and
existing full-shot methods for ToD agents. We achieve this by (1) improving the
dialogue data representation, (2) improving entity-aware machine translation,
and (3) automatic filtering of noisy translations.
</p>
<p>We evaluate our approach on the recent bilingual dialogue dataset BiToD. In
Chinese to English transfer, in the zero-shot setting, our method achieves
46.7% and 22.0% in Task Success Rate (TSR) and Dialogue Success Rate (DSR)
respectively. In the few-shot setting where 10% of the data in the target
language is used, we improve the state-of-the-art by 15.2% and 14.0%, coming
within 5% of full-shot training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark. (arXiv:2302.09432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09432">
<div class="article-summary-box-inner">
<span><p>To advance Chinese financial natural language processing (NLP), we introduce
BBT-FinT5, a new Chinese financial pre-training language model based on the T5
model. To support this effort, we have built BBT-FinCorpus, a large-scale
financial corpus with approximately 300GB of raw text from four different
sources. In general domain NLP, comprehensive benchmarks like GLUE and
SuperGLUE have driven significant advancements in language model pre-training
by enabling head-to-head comparisons among models. Drawing inspiration from
these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language
understanding and generation Evaluation Benchmark, which includes six datasets
covering both understanding and generation tasks. Our aim is to facilitate
research in the development of NLP within the Chinese financial domain. Our
model, corpus and benchmark are released at
https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the
Big Bang Transformer (BBT), a large-scale pre-trained language model project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Language Representations with Logical Inductive Bias. (arXiv:2302.09458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09458">
<div class="article-summary-box-inner">
<span><p>Transformer architectures have achieved great success in solving natural
language tasks, which learn strong language representations from large-scale
unlabeled texts. In this paper, we seek to go further beyond and explore a new
logical inductive bias for better language representation learning. Logic
reasoning is known as a formal methodology to reach answers from given
knowledge and facts. Inspired by such a view, we develop a novel neural
architecture named FOLNet (First-Order Logic Network), to encode this new
inductive bias. We construct a set of neural logic operators as learnable Horn
clauses, which are further forward-chained into a fully differentiable neural
architecture (FOLNet). Interestingly, we find that the self-attention module in
transformers can be composed by two of our neural logic operators, which
probably explains their strong reasoning performance. Our proposed FOLNet has
the same input and output interfaces as other pretrained models and thus could
be pretrained/finetuned by using similar losses. It also allows FOLNet to be
used in a plug-and-play manner when replacing other pretrained models. With our
logical inductive bias, the same set of ``logic deduction skills'' learned
through pretraining are expected to be equally capable of solving diverse
downstream tasks. For this reason, FOLNet learns language representations that
have much stronger transfer capabilities. Experimental results on several
language understanding tasks show that our pretrained FOLNet model outperforms
the existing strong transformer-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Text Retrieval by Supervised Multi-Space Multi-Grained Alignment. (arXiv:2302.09473v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09473">
<div class="article-summary-box-inner">
<span><p>While recent progress in video-text retrieval has been advanced by the
exploration of better representation learning, in this paper, we present a
novel multi-space multi-grained supervised learning framework, SUMA, to learn
an aligned representation space shared between the video and the text for
video-text retrieval. The shared aligned space is initialized with a finite
number of concept clusters, each of which refers to a number of basic concepts
(words). With the text data at hand, we are able to update the shared aligned
space in a supervised manner using the proposed similarity and alignment
losses. Moreover, to enable multi-grained alignment, we incorporate frame
representations for better modeling the video modality and calculating
fine-grained and coarse-grained similarity. Benefiting from learned shared
aligned space and multi-grained similarity, extensive experiments on several
video-text retrieval benchmarks demonstrate the superiority of SUMA over
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Classification in the Wild: a Large-scale Long-tailed Name Normalization Dataset. (arXiv:2302.09509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09509">
<div class="article-summary-box-inner">
<span><p>Real-world data usually exhibits a long-tailed distribution,with a few
frequent labels and a lot of few-shot labels. The study of institution name
normalization is a perfect application case showing this phenomenon. There are
many institutions worldwide with enormous variations of their names in the
publicly available literature. In this work, we first collect a large-scale
institution name normalization dataset LoT-insts1, which contains over 25k
classes that exhibit a naturally long-tailed distribution. In order to isolate
the few-shot and zero-shot learning scenarios from the massive many-shot
classes, we construct our test set from four different subsets: many-, medium-,
and few-shot sets, as well as a zero-shot open set. We also replicate several
important baseline methods on our data, covering a wide range from search-based
methods to neural network methods that use the pretrained BERT model. Further,
we propose our specially pretrained, BERT-based model that shows better
out-of-distribution generalization on few-shot and zero-shot test sets.
Compared to other datasets focusing on the long-tailed phenomenon, our dataset
has one order of magnitude more training data than the largest existing
long-tailed datasets and is naturally long-tailed rather than manually
synthesized. We believe it provides an important and different scenario to
study this problem. To our best knowledge, this is the first natural language
dataset that focuses on long-tailed and open-set classification problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes. (arXiv:2302.09527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09527">
<div class="article-summary-box-inner">
<span><p>We present a neural Sanskrit Natural Language Processing (NLP) toolkit named
SanskritShala (a school of Sanskrit) to facilitate computational linguistic
analyses for several tasks such as word segmentation, morphological tagging,
dependency parsing, and compound type identification. Our systems currently
report state-of-the-art performance on available benchmark datasets for all
tasks. SanskritShala is deployed as a web-based application, which allows a
user to get real-time analysis for the given input. It is built with
easy-to-use interactive data annotation features that allow annotators to
correct the system predictions when it makes mistakes. We publicly release the
source codes of the 4 modules included in the toolkit, 7 word embedding models
that have been trained on publicly available Sanskrit corpora and multiple
annotated datasets such as word similarity, relatedness, categorization,
analogy prediction to assess intrinsic properties of word embeddings. So far as
we know, this is the first neural-based Sanskrit NLP toolkit that has a
web-based interface and a number of NLP modules. We are sure that the people
who are willing to work with Sanskrit will find it useful for pedagogical and
annotative purposes. SanskritShala is available at:
https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be
accessed at: https://youtu.be/x0X31Y9k0mw4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Upvotes? Downvotes? No Votes? Understanding the relationship between reaction mechanisms and political discourse on Reddit. (arXiv:2302.09540v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09540">
<div class="article-summary-box-inner">
<span><p>A significant share of political discourse occurs online on social media
platforms. Policymakers and researchers try to understand the role of social
media design in shaping the quality of political discourse around the globe. In
the past decades, scholarship on political discourse theory has produced
distinct characteristics of different types of prominent political rhetoric
such as deliberative, civic, or demagogic discourse. This study investigates
the relationship between social media reaction mechanisms (i.e., upvotes,
downvotes) and political rhetoric in user discussions by engaging in an
in-depth conceptual analysis of political discourse theory. First, we analyze
155 million user comments in 55 political subforums on Reddit between 2010 and
2018 to explore whether users' style of political discussion aligns with the
essential components of deliberative, civic, and demagogic discourse. Second,
we perform a quantitative study that combines confirmatory factor analysis with
difference in differences models to explore whether different reaction
mechanism schemes (e.g., upvotes only, upvotes and downvotes, no reaction
mechanisms) correspond with political user discussion that is more or less
characteristic of deliberative, civic, or demagogic discourse. We produce three
main takeaways. First, despite being "ideal constructs of political rhetoric,"
we find that political discourse theories describe political discussions on
Reddit to a large extent. Second, we find that discussions in subforums with
only upvotes, or both up- and downvotes are associated with user discourse that
is more deliberate and civic. Third, social media discussions are most
demagogic in subreddits with no reaction mechanisms at all. These findings
offer valuable contributions for ongoing policy discussions on the relationship
between social media interface design and respectful political discussion among
users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Social Media Manipulation in Low-Resource Languages. (arXiv:2011.05367v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05367">
<div class="article-summary-box-inner">
<span><p>Social media have been deliberately used for malicious purposes, including
political manipulation and disinformation. Most research focuses on
high-resource languages. However, malicious actors share content across
countries and languages, including low-resource ones. Here, we investigate
whether and to what extent malicious actors can be detected in low-resource
language settings. We discovered that a high number of accounts posting in
Tagalog were suspended as part of Twitter's crackdown on interference
operations after the 2016 US Presidential election. By combining text embedding
and transfer learning, our framework can detect, with promising accuracy,
malicious users posting in Tagalog without any prior knowledge or training on
malicious content in that language. We first learn an embedding model for each
language, namely a high-resource language (English) and a low-resource one
(Tagalog), independently. Then, we learn a mapping between the two latent
spaces to transfer the detection model. We demonstrate that the proposed
approach significantly outperforms state-of-the-art models, including BERT, and
yields marked advantages in settings with very limited training data -- the
norm when dealing with detecting malicious activity in online platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding. (arXiv:2109.06466v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06466">
<div class="article-summary-box-inner">
<span><p>Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the
major semi-supervised approaches to improve natural language understanding
(NLU) tasks with massive amount of unlabeled data. However, it's unclear
whether they learn similar representations or they can be effectively combined.
In this paper, we show that TAPT and ST can be complementary with simple TFS
protocol by following TAPT -&gt; Finetuning -&gt; Self-training (TFS) process.
Experimental results show that TFS protocol can effectively utilize unlabeled
data to achieve strong combined gains consistently across six datasets covering
sentiment classification, paraphrase identification, natural language
inference, named entity recognition and dialogue slot classification. We
investigate various semi-supervised settings and consistently show that gains
from TAPT and ST can be strongly additive by following TFS procedure. We hope
that TFS could serve as an important semi-supervised baseline for future NLP
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) aims to determine the logical relationship
between two sentences, such as Entailment, Contradiction, and Neutral. In
recent years, deep learning models have become a prevailing approach to NLI,
but they lack interpretability and explainability. In this work, we address the
explainability of NLI by weakly supervised logical reasoning, and propose an
Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases
as the semantic unit and aligns corresponding phrases in the two sentences.
Then, the model predicts the NLI label for the aligned phrases, and induces the
sentence label by fuzzy logic formulas. Our EPR is almost everywhere
differentiable and thus the system can be trained end to end. In this way, we
are able to provide explicit explanations of phrasal logical relationships in a
weakly supervised manner. We further show that such reasoning results help
textual explanation generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semantic Parsing for Multilingual Task-Oriented Dialogues. (arXiv:2111.02574v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02574">
<div class="article-summary-box-inner">
<span><p>Robust state tracking for task-oriented dialogue systems currently remains
restricted to a few popular languages. This paper shows that given a
large-scale dialogue data set in one language, we can automatically produce an
effective semantic parser for other languages using machine translation. We
propose automatic translation of dialogue datasets with alignment to ensure
faithful translation of slot values and eliminate costly human supervision used
in previous benchmarks. We also propose a new contextual semantic parsing
model, which encodes the formal slots and values, and only the last agent and
user utterances. We show that the succinct representation reduces the
compounding effect of translation errors, without harming the accuracy in
practice.
</p>
<p>We evaluate our approach on several dialogue state tracking benchmarks. On
RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZ-ZH datasets we improve the state
of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a
comprehensive error analysis for all three datasets showing erroneous
annotations can lead to misguided judgments on the quality of the model.
</p>
<p>Finally, we present RiSAWOZ English and German datasets, created using our
translation methodology. On these datasets, accuracy is within 11% of the
original showing that high-accuracy multilingual dialogue datasets are possible
without relying on expensive human annotations. We release our datasets and
software open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration into Translation-Equivariant Image Quantization. (arXiv:2112.00384v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00384">
<div class="article-summary-box-inner">
<span><p>This is an exploratory study that discovers the current image quantization
(vector quantization) do not satisfy translation equivariance in the quantized
space due to aliasing. Instead of focusing on anti-aliasing, we propose a
simple yet effective way to achieve translation-equivariant image quantization
by enforcing orthogonality among the codebook embeddings. To explore the
advantages of translation-equivariant image quantization, we conduct three
proof-of-concept experiments with a carefully controlled dataset: (1)
text-to-image generation, where the quantized image indices are the target to
predict, (2) image-to-text generation, where the quantized image indices are
given as a condition, (3) using a smaller training set to analyze sample
efficiency. From the strictly controlled experiments, we empirically verify
that the translation-equivariant image quantizer improves not only sample
efficiency but also the accuracy over VQGAN up to +11.9% in text-to-image
generation and +3.9% in image-to-text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06009">
<div class="article-summary-box-inner">
<span><p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are
obvious to humans. For example, GPT-3 would mistakenly interpret "What word is
similar to good?" to mean a homophone, while the user intended a synonym. Our
goal is to effectively correct such errors via user interactions with the
system but without retraining, which will be prohibitively costly. We pair
GPT-3 with a growing memory of recorded cases where the model misunderstood the
user's intents, along with user feedback for clarification. Such a memory
allows our system to produce enhanced prompts for any new query based on the
user feedback for error correction on similar cases in the past. On four tasks
(two lexical tasks, two advanced ethical reasoning tasks), we show how a
(simulated) user can interactively teach a deployed GPT-3, substantially
increasing its accuracy over the queries with different kinds of
misunderstandings by the GPT-3. Our approach is a step towards the low-cost
utility enhancement for very large pre-trained LMs. Code, data, and
instructions to implement MEMPROMPT for a new task at
https://www.memprompt.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02225">
<div class="article-summary-box-inner">
<span><p>Unsupervised relation extraction aims to extract the relationship between
entities from natural language sentences without prior information on
relational scope or distribution. Existing works either utilize self-supervised
schemes to refine relational feature signals by iteratively leveraging adaptive
clustering and classification that provoke gradual drift problems, or adopt
instance-wise contrastive learning which unreasonably pushes apart those
sentence pairs that are semantically similar. To overcome these defects, we
propose a novel contrastive learning framework named HiURE, which has the
capability to derive hierarchical signals from relational feature space using
cross hierarchy attention and effectively optimize relation representation of
sentences under exemplar-wise contrastive learning. Experimental results on two
public datasets demonstrate the advanced effectiveness and robustness of HiURE
on unsupervised relation extraction when compared with state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04301">
<div class="article-summary-box-inner">
<span><p>We propose a synthetic reasoning task, LEGO (Learning Equality and Group
Operations), that encapsulates the problem of following a chain of reasoning,
and we study how the Transformer architectures learn this task. We pay special
attention to data effects such as pretraining (on seemingly unrelated NLP
tasks) and dataset composition (e.g., differing chain length at training and
test time), as well as architectural variants such as weight-tied layers or
adding convolutional components. We study how the trained models eventually
succeed at the task, and in particular, we manage to understand some of the
attention heads as well as how the information flows in the network. In
particular, we have identified a novel \emph{association} pattern that globally
attends only to identical tokens. Based on these observations we propose a
hypothesis that here pretraining helps for LEGO tasks due to certain structured
attention patterns, and we experimentally verify this hypothesis. We also
observe that in some data regime the trained transformer finds ``shortcut"
solutions to follow the chain of reasoning, which impedes the model's
robustness, and moreover we propose ways to prevent it. Motivated by our
findings on structured attention patterns, we propose the LEGO attention
module, a drop-in replacement for vanilla attention heads. This architectural
change significantly reduces Flops and maintains or even \emph{improves} the
model's performance at large-scale pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiViz: Towards Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00056">
<div class="article-summary-box-inner">
<span><p>The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocPrompting: Generating Code by Retrieving the Docs. (arXiv:2207.05987v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05987">
<div class="article-summary-box-inner">
<span><p>Publicly available source-code libraries are continuously growing and
changing. This makes it impossible for models of code to keep current with all
available APIs by simply training these models on existing code repositories.
Thus, existing models inherently cannot generalize to using unseen functions
and libraries, because these would never appear in the training data. In
contrast, when human programmers use functions and libraries for the first
time, they frequently refer to textual resources such as code manuals and
documentation, to explore and understand the available functionality. Inspired
by this observation, we introduce DocPrompting: a natural-language-to-code
generation approach that explicitly leverages documentation by (1) retrieving
the relevant documentation pieces given an NL intent, and (2) generating code
based on the NL intent and the retrieved documentation. DocPrompting is
general: it can be applied to any programming language and is agnostic to the
underlying neural model. We demonstrate that DocPrompting consistently improves
NL-to-code models: DocPrompting improves strong base models such as CodeT5 by
2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in
execution-based evaluation on the popular Python CoNaLa benchmark; on a new
Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to
absolute 6.9% exact match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Embedding Spaces by Conceptualization. (arXiv:2209.00445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00445">
<div class="article-summary-box-inner">
<span><p>One of the main methods for semantic interpretation of text is mapping it
into a vector in some embedding space. Such vectors can then be used for a
variety of text processing tasks. Recently, most embedding spaces are a product
of training large language models. One major drawback of this type of
representation is its incomprehensibility to humans. Understanding the
embedding space is crucial for several important needs, including the need to
explain the decision of a system that uses the embedding, the need to debug the
embedding method and compare it to alternatives, and the need to detect biases
hidden in the model. In this paper, we present a novel method of transforming
any embedding space into a comprehensible conceptual space. We first present an
algorithm for deriving a conceptual space with dynamic on-demand granularity.
We then show a method for transferring any vector in the original
incomprehensible space to an understandable vector in the conceptual space. We
combine human tests with cross-model tests to show that the conceptualized
vectors indeed represent the semantics of the original vectors. We also show
how the conceptualized vectors can be used for various tasks including
identifying weaknesses in the semantics underlying the original spaces and
differences in the semantics of alternative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions. (arXiv:2209.03430v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03430">
<div class="article-summary-box-inner">
<span><p>Multimodal machine learning is a vibrant multi-disciplinary research field
that aims to design computer agents with intelligent capabilities such as
understanding, reasoning, and learning through integrating multiple
communicative modalities, including linguistic, acoustic, visual, tactile, and
physiological messages. With the recent interest in video understanding,
embodied autonomous agents, text-to-image generation, and multisensor fusion in
application domains such as healthcare and robotics, multimodal machine
learning has brought unique computational and theoretical challenges to the
machine learning community given the heterogeneity of data sources and the
interconnections often found between modalities. However, the breadth of
progress in multimodal research has made it difficult to identify the common
themes and open questions in the field. By synthesizing a broad range of
application domains and theoretical frameworks from both historical and recent
perspectives, this paper is designed to provide an overview of the
computational and theoretical foundations of multimodal machine learning. We
start by defining three key principles of modality heterogeneity, connections,
and interactions that have driven subsequent innovations, and propose a
taxonomy of six core technical challenges: representation, alignment,
reasoning, generation, transference, and quantification covering historical and
recent trends. Recent technical achievements will be presented through the lens
of this taxonomy, allowing researchers to understand the similarities and
differences across new approaches. We end by motivating several open problems
for future research as identified by our taxonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05401">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) has been primarily studied through the lens
of the English language. Yet, tackling VQA in other languages in the same
manner would require a considerable amount of resources. In this paper, we
propose scalable solutions to multilingual visual question answering (mVQA), on
both data and modeling fronts. We first propose a translation-based framework
to mVQA data generation that requires much less human annotation efforts than
the conventional approach of directly collection questions and answers. Then,
we apply our framework to the multilingual captions in the Crossmodal-3600
dataset and develop an efficient annotation protocol to create MaXM, a
test-only VQA benchmark in 7 diverse languages. Finally, we propose an approach
to unified, extensible, open-ended, and end-to-end mVQA modeling and
demonstrate strong performance in 13 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07661">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05556">
<div class="article-summary-box-inner">
<span><p>We introduce ViLPAct, a novel vision-language benchmark for human activity
planning. It is designed for a task where embodied AI agents can reason and
forecast future actions of humans based on video clips about their initial
activities and intents in text. The dataset consists of 2.9k videos from
\charades extended with intents via crowdsourcing, a multi-choice question test
set, and four strong baselines. One of the baselines implements a neurosymbolic
approach based on a multi-modal knowledge base (MKB), while the other ones are
deep generative models adapted from recent state-of-the-art (SOTA) methods.
According to our extensive experiments, the key challenges are compositional
generalization and effective use of information from both modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Translation Memories into Non-Autoregressive Machine Translation. (arXiv:2210.06020v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06020">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive machine translation (NAT) has recently made great
progress. However, most works to date have focused on standard translation
tasks, even though some edit-based NAT models, such as the Levenshtein
Transformer (LevT), seem well suited to translate with a Translation Memory
(TM). This is the scenario considered here. We first analyze the vanilla LevT
model and explain why it does not do well in this setting. We then propose a
new variant, TM-LevT, and show how to effectively train this model. By
modifying the data presentation and introducing an extra deletion operation, we
obtain performance that are on par with an autoregressive approach, while
reducing the decoding load. We also show that incorporating TMs during training
dispenses to use knowledge distillation, a well-known trick used to mitigate
the multimodality issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Non-transferable Text Classification. (arXiv:2210.12651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12651">
<div class="article-summary-box-inner">
<span><p>Training a good deep learning model requires substantial data and computing
resources, which makes the resulting neural model a valuable intellectual
property. To prevent the neural network from being undesirably exploited,
non-transferable learning has been proposed to reduce the model generalization
ability in specific target domains. However, existing approaches require
labeled data for the target domain which can be difficult to obtain.
Furthermore, they do not have the mechanism to still recover the model's
ability to access the target domain. In this paper, we propose a novel
unsupervised non-transferable learning method for the text classification task
that does not require annotated target domain data. We further introduce a
secret key component in our approach for recovering the access to the target
domain, where we design both an explicit and an implicit method for doing so.
Extensive experiments demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEBERT: Efficient and Robust Binary Ensemble BERT. (arXiv:2210.15976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15976">
<div class="article-summary-box-inner">
<span><p>Pre-trained BERT models have achieved impressive accuracy on natural language
processing (NLP) tasks. However, their excessive amount of parameters hinders
them from efficient deployment on edge devices. Binarization of the BERT models
can significantly alleviate this issue but comes with a severe accuracy drop
compared with their full-precision counterparts. In this paper, we propose an
efficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.
To the best of our knowledge, this is the first work employing ensemble
techniques on binary BERTs, yielding BEBERT, which achieves superior accuracy
while retaining computational efficiency. Furthermore, we remove the knowledge
distillation procedures during ensemble to speed up the training process
without compromising accuracy. Experimental results on the GLUE benchmark show
that the proposed BEBERT significantly outperforms the existing binary BERT
models in accuracy and robustness with a 2x speedup on training time. Moreover,
our BEBERT has only a negligible accuracy loss of 0.3% compared to the
full-precision baseline while saving 15x and 13x in FLOPs and model size,
respectively. In addition, BEBERT also outperforms other compressed BERTs in
accuracy by up to 6.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Multimodal Entity-Relation Extraction Based on Edge-enhanced Graph Alignment Network and Word-pair Relation Tagging. (arXiv:2211.15028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15028">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition (MNER) and multimodal relation extraction
(MRE) are two fundamental subtasks in the multimodal knowledge graph
construction task. However, the existing methods usually handle two tasks
independently, which ignores the bidirectional interaction between them. This
paper is the first to propose jointly performing MNER and MRE as a joint
multimodal entity-relation extraction task (JMERE). Besides, the current MNER
and MRE models only consider aligning the visual objects with textual entities
in visual and textual graphs but ignore the entity-entity relationships and
object-object relationships. To address the above challenges, we propose an
edge-enhanced graph alignment network and a word-pair relation tagging (EEGA)
for JMERE task. Specifically, we first design a word-pair relation tagging to
exploit the bidirectional interaction between MNER and MRE and avoid the error
propagation. Then, we propose an edge-enhanced graph alignment network to
enhance the JMERE task by aligning nodes and edges in the cross-graph. Compared
with previous methods, the proposed method can leverage the edge information to
auxiliary alignment between objects and entities and find the correlations
between entity-entity relationships and object-object relationships.
Experiments are conducted to show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models. (arXiv:2212.07769v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07769">
<div class="article-summary-box-inner">
<span><p>Users often ask dialogue systems ambiguous questions that require
clarification. We show that current language models rarely ask users to clarify
ambiguous questions and instead provide incorrect answers. To address this, we
introduce CLAM: a framework for getting language models to selectively ask for
clarification about ambiguous user questions. In particular, we show that we
can prompt language models to detect whether a given question is ambiguous,
generate an appropriate clarifying question to ask the user, and give a final
answer after receiving clarification. We also show that we can simulate users
by providing language models with privileged information. This lets us
automatically evaluate multi-turn clarification dialogues. Finally, CLAM
significantly improves language models' accuracy on mixed ambiguous and
unambiguous questions relative to SotA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14548">
<div class="article-summary-box-inner">
<span><p>Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. (arXiv:2301.12867v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12867">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language-model (LLM) has significantly impacted businesses such as report
summarization softwares and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is no systematic examination and user study of the ethics
of current LLMs use. To further educate future efforts on constructing ethical
LLMs responsibly, we perform a qualitative research method on OpenAI's ChatGPT
to better understand the practical features of ethical dangers in recent LLMs.
We analyze ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)
\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on the AI
ethics of ChatGPT, as well as future problems and practical design
considerations for LLMs. We believe that our findings may give light on future
efforts to determine and mitigate the ethical hazards posed by machines in LLM
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02123">
<div class="article-summary-box-inner">
<span><p>Prior work has attempted to understand the internal structures and
functionalities of Transformer-based encoder-decoder architectures on the level
of multi-head attention and feed-forward sublayers. Interpretations have
focused on the encoder and decoder, along with the combinatorial possibilities
of the self-attention, cross-attention, and feed-forward sublayers. However,
without examining the low-level structures, one gains limited understanding of
the motivation behind sublayer reordering. Could we dive into the sublayer
abstraction and permute layer weight matrices to improve the quality of
translation? We propose AEIUOrder to greedily reorder layer weight matrices in
the encoder by their well-trainedness, as measured by Heavy-Tailed
Self-Regularization (HT-SR) metrics, and order the decoder matrices
correspondingly. Our results suggest that greedily reordering layer weight
matrices to maximize Total well-trainedness facilitates the model to learn
representations and generate translations more effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02676">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is important for language models to be
helpful and useful for humans, and to align with human and social values. Prior
work have achieved remarkable successes by learning from human feedback to
understand and follow instructions. They belong to two categories supervised
finetuning and RLHF. Supervised finetuning is based on curated model
generations that are preferred by human labelers, a key limitation of them is
that supervised finetuning cannot learn from negative ratings; models are only
trained on positive feedback, which makes it data inefficient and difficult to
generalize. While RLHF can learn from all feedback by learning a reward
function and RL optimization, it suffers from imperfect reward function and RL
is very hard to tune. In this work, we propose a novel technique that addresses
the limitations of both supervised finetuning and RLHF, our method, Chain of
Hindsight, aligns language models with all feedback without using reinforcement
learning. Our idea is motivated by how humans learn from hindsight experience,
and we turn all feedback into a sentence to finetune model in order to leverage
the language understanding abilities of language models. We condition the model
on a sequence of model generations paired with hindsight feedback, and finetune
the model to predict the most preferred output. By doing so, models can learn
to identify and correct negative attributes or errors. Applying our method to
GPT-J, we observe that it substantially outperforms both supervised finetuning
and RLHF on summarization and dialogue tasks and is significantly more
preferred in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences. (arXiv:2302.03106v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03106">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have led to a new state-of-the-art in many NLP
tasks. However, for topic modeling, statistical generative models such as LDA
are still prevalent, which do not easily allow incorporating contextual word
vectors. They might yield topics that do not align very well with human
judgment. In this work, we propose a novel topic modeling and inference
algorithm. We suggest a bag of sentences (BoS) approach using sentences as the
unit of analysis. We leverage pre-trained sentence embeddings by combining
generative process models with clustering. We derive a fast inference algorithm
based on expectation maximization, hard assignments, and an annealing process.
Our evaluation shows that our method yields state-of-the art results with
relatively little computational demands. Our methods is more flexible compared
to prior works leveraging word embeddings, since it provides the possibility to
customize topic-document distributions using priors. Code is at
\url{https://github.com/JohnTailor/BertSenClu}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic ambiguity analysis in ChatGPT. (arXiv:2302.06426v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06426">
<div class="article-summary-box-inner">
<span><p>Linguistic ambiguity is and has always been one of the main challenges in
Natural Language Processing (NLP) systems. Modern Transformer architectures
like BERT, T5 or more recently InstructGPT have achieved some impressive
improvements in many NLP fields, but there is still plenty of work to do.
Motivated by the uproar caused by ChatGPT, in this paper we provide an
introduction to linguistic ambiguity, its varieties and their relevance in
modern NLP, and perform an extensive empiric analysis. ChatGPT strengths and
weaknesses are revealed, as well as strategies to get the most of this model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Capacity for Moral Self-Correction in Large Language Models. (arXiv:2302.07459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07459">
<div class="article-summary-box-inner">
<span><p>We test the hypothesis that language models trained with reinforcement
learning from human feedback (RLHF) have the capability to "morally
self-correct" -- to avoid producing harmful outputs -- if instructed to do so.
We find strong evidence in support of this hypothesis across three different
experiments, each of which reveal different facets of moral self-correction. We
find that the capability for moral self-correction emerges at 22B model
parameters, and typically improves with increasing model size and RLHF
training. We believe that at this level of scale, language models obtain two
capabilities that they can use for moral self-correction: (1) they can follow
instructions and (2) they can learn complex normative concepts of harm like
stereotyping, bias, and discrimination. As such, they can follow instructions
to avoid certain kinds of morally harmful outputs. We believe our results are
cause for cautious optimism regarding the ability to train language models to
abide by ethical principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07748">
<div class="article-summary-box-inner">
<span><p>Narratives include a rich source of events unfolding over time and context.
Automatic understanding of these events may provide a summarised comprehension
of the narrative for further computation (such as reasoning). In this paper, we
study the Information Status (IS) of the events and propose a novel challenging
task: the automatic identification of new events in a narrative. We define an
event as a triplet of subject, predicate, and object. The event is categorized
as new with respect to the discourse context and whether it can be inferred
through commonsense reasoning. We annotated a publicly available corpus of
narratives with the new events at sentence level using human annotators. We
present the annotation protocol and a study aiming at validating the quality of
the annotation and the difficulty of the task. We publish the annotated
dataset, annotation materials, and machine learning baseline models for the
task of new event extraction for narrative understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08399">
<div class="article-summary-box-inner">
<span><p>Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of-Mind tasks. These tasks have shown both
successes and failures. We consider in particular a recent purported success
case, and show that small variations that maintain the principles of ToM turn
the results on their head. We argue that in general, the zero-hypothesis for
model evaluation in intuitive psychology should be skeptical, and that outlying
failure cases should outweigh average success rates. We also consider what
possible future successes on Theory-of-Mind tasks by more powerful LLMs would
mean for ToM tasks with people.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-21 23:13:09.250850430 UTC">2023-02-21 23:13:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>