<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-04T01:30:00Z">01-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01405">
<div class="article-summary-box-inner">
<span><p>Does Donald Trump speak differently from other presidents? If so, in what
ways? Are these differences confined to any single medium of communication? To
investigate these questions, this paper introduces a novel metric of uniqueness
based on large language models, develops a new lexicon for divisive speech, and
presents a framework for comparing the lexical features of political opponents.
Applying these tools to a variety of corpora of presidential speeches, we find
considerable evidence that Trump's speech patterns diverge from those of all
major party nominees for the presidency in recent history. Some notable
findings include Trump's employment of particularly divisive and antagonistic
language targeting of his political opponents and his patterns of repetition
for emphasis. Furthermore, Trump is significantly more distinctive than his
fellow Republicans, whose uniqueness values are comparably closer to those of
the Democrats. These differences hold across a variety of measurement
strategies, arise on both the campaign trail and in official presidential
addresses, and do not appear to be an artifact of secular time trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation. (arXiv:2401.01419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01419">
<div class="article-summary-box-inner">
<span><p>We conduct a large-scale fine-grained comparative analysis of machine
translations (MT) against human translations (HT) through the lens of
morphosyntactic divergence. Across three language pairs and two types of
divergence defined as the structural difference between the source and the
target, MT is consistently more conservative than HT, with less morphosyntactic
diversity, more convergent patterns, and more one-to-one alignments. Through
analysis on different decoding algorithms, we attribute this discrepancy to the
use of beam search that biases MT towards more convergent patterns. This bias
is most amplified when the convergent pattern appears around 50% of the time in
training data. Lastly, we show that for a majority of morphosyntactic
divergences, their presence in HT is correlated with decreased MT performance,
presenting a greater challenge for MT systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation. (arXiv:2401.01469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01469">
<div class="article-summary-box-inner">
<span><p>Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01472">
<div class="article-summary-box-inner">
<span><p>Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01487">
<div class="article-summary-box-inner">
<span><p>In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning. (arXiv:2401.01495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01495">
<div class="article-summary-box-inner">
<span><p>In terms of human-computer interaction, it is becoming more and more
important to correctly understand the user's emotional state in a conversation,
so the task of multimodal emotion recognition (MER) started to receive more
attention. However, existing emotion classification methods usually perform
classification only once. Sentences are likely to be misclassified in a single
round of classification. Previous work usually ignores the similarities and
differences between different morphological features in the fusion process. To
address the above issues, we propose a two-stage emotion recognition model
based on graph contrastive learning (TS-GCL). First, we encode the original
dataset with different preprocessing modalities. Second, a graph contrastive
learning (GCL) strategy is introduced for these three modal data with other
structures to learn similarities and differences within and between modalities.
Finally, we use MLP twice to achieve the final emotion classification. This
staged classification method can help the model to better focus on different
levels of emotional information, thereby improving the performance of the
model. Extensive experiments show that TS-GCL has superior performance on
IEMOCAP and MELD datasets compared with previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction. (arXiv:2401.01498v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01498">
<div class="article-summary-box-inner">
<span><p>We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01523">
<div class="article-summary-box-inner">
<span><p>The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models. (arXiv:2401.01572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01572">
<div class="article-summary-box-inner">
<span><p>Hallucinations are a type of output error produced by deep neural networks.
While this has been studied in natural language processing, they have not been
researched previously in automatic speech recognition. Here, we define
hallucinations in ASR as transcriptions generated by a model that are
semantically unrelated to the source utterance, yet still fluent and coherent.
The similarity of hallucinations to probable natural language outputs of the
model creates a danger of deception and impacts the credibility of the system.
We show that commonly used metrics, such as word error rates, cannot
differentiate between hallucinatory and non-hallucinatory models. To address
this, we propose a perturbation-based method for assessing the susceptibility
of an automatic speech recognition (ASR) model to hallucination at test time,
which does not require access to the training dataset. We demonstrate that this
method helps to distinguish between hallucinatory and non-hallucinatory models
that have similar baseline word error rates. We further explore the
relationship between the types of ASR errors and the types of dataset noise to
determine what types of noise are most likely to create hallucinatory outputs.
We devise a framework for identifying hallucinations by analysing their
semantic connection with the ground truth and their fluency. Finally, we
discover how to induce hallucinations with a random noise injection to the
utterance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries. (arXiv:2401.01596v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01596">
<div class="article-summary-box-inner">
<span><p>In the healthcare domain, summarizing medical questions posed by patients is
critical for improving doctor-patient interactions and medical decision-making.
Although medical data has grown in complexity and quantity, the current body of
research in this domain has primarily concentrated on text-based methods,
overlooking the integration of visual cues. Also prior works in the area of
medical question summarisation have been limited to the English language. This
work introduces the task of multimodal medical question summarization for
codemixed input in a low-resource setting. To address this gap, we introduce
the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which
combines Hindi-English codemixed medical queries with visual aids. This
integration enriches the representation of a patient's medical condition,
providing a more comprehensive perspective. We also propose a framework named
MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing
our MMCQS dataset, we demonstrate the value of integrating visual information
from images to improve the creation of medically detailed summaries. This
multimodal strategy not only improves healthcare decision-making but also
promotes a deeper comprehension of patient queries, paving the way for future
exploration in personalized and responsive medical care. Our dataset, code, and
pre-trained models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01600">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01614">
<div class="article-summary-box-inner">
<span><p>The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication. (arXiv:2401.01620v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01620">
<div class="article-summary-box-inner">
<span><p>We investigate whether general-domain large language models such as GPT-4
Turbo can perform risk stratification and predict post-operative outcome
measures using a description of the procedure and a patient's clinical notes
derived from the electronic health record. We examine predictive performance on
8 different tasks: prediction of ASA Physical Status Classification, hospital
admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1
duration, hospital duration, and ICU duration. Few-shot and chain-of-thought
prompting improves predictive performance for several of the tasks. We achieve
F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU
admission, and 0.86 for hospital mortality. Performance on duration prediction
tasks were universally poor across all prompt strategies. Current generation
large language models can assist clinicians in perioperative risk
stratification on classification tasks and produce high-quality natural
language summaries and explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01623">
<div class="article-summary-box-inner">
<span><p>Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Media Ready Caption Generation for Brands. (arXiv:2401.01637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01637">
<div class="article-summary-box-inner">
<span><p>Social media advertisements are key for brand marketing, aiming to attract
consumers with captivating captions and pictures or logos. While previous
research has focused on generating captions for general images, incorporating
brand personalities into social media captioning remains unexplored. Brand
personalities are shown to be affecting consumers' behaviours and social
interactions and thus are proven to be a key aspect of marketing strategies.
Current open-source multimodal LLMs are not directly suited for this task.
Hence, we propose a pipeline solution to assist brands in creating engaging
social media captions that align with the image and the brand personalities.
Our architecture is based on two parts: a the first part contains an image
captioning model that takes in an image that the brand wants to post online and
gives a plain English caption; b the second part takes in the generated caption
along with the target brand personality and outputs a catchy
personality-aligned social media caption. Along with brand personality, our
system also gives users the flexibility to provide hashtags, Instagram handles,
URLs, and named entities they want the caption to contain, making the captions
more semantically related to the social media handles. Comparative evaluations
against various baselines demonstrate the effectiveness of our approach, both
qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLPs Compass: What is learned when MLPs are combined with PLMs?. (arXiv:2401.01667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01667">
<div class="article-summary-box-inner">
<span><p>While Transformer-based pre-trained language models and their variants
exhibit strong semantic representation capabilities, the question of
comprehending the information gain derived from the additional components of
PLMs remains an open question in this field. Motivated by recent efforts that
prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture
capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims
to quantify whether simple MLPs can further enhance the already potent ability
of PLMs to capture linguistic information. Specifically, we design a simple yet
effective probing framework containing MLPs components based on BERT structure
and conduct extensive experiments encompassing 10 probing tasks spanning three
distinct linguistic levels. The experimental results demonstrate that MLPs can
indeed enhance the comprehension of linguistic structure by PLMs. Our research
provides interpretable and valuable insights into crafting variations of PLMs
utilizing MLPs for tasks that emphasize diverse linguistic structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches. (arXiv:2401.01692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01692">
<div class="article-summary-box-inner">
<span><p>Effective collaboration requires groups to strategically regulate themselves
to overcome challenges. Research has shown that groups may fail to regulate due
to differences in members' perceptions of challenges which may benefit from
external support. In this study, we investigated the potential of leveraging
three distinct natural language processing models: an expert knowledge
rule-based model, a supervised machine learning (ML) model and a Large Language
model (LLM), in challenge detection and challenge dimension identification
(cognitive, metacognitive, emotional and technical/other challenges) from
student discourse, was investigated. The results show that the supervised ML
and the LLM approaches performed considerably well in both tasks, in contrast
to the rule-based approach, whose efficacy heavily relies on the engineered
features by experts. The paper provides an extensive discussion of the three
approaches' performance for automated detection and support of students'
challenge moments in collaborative learning activities. It argues that,
although LLMs provide many advantages, they are unlikely to be the panacea to
issues of the detection and feedback provision of socially shared regulation of
learning due to their lack of reliability, as well as issues of validity
evaluation, privacy and confabulation. We conclude the paper with a discussion
on additional considerations, including model transparency to explore feasible
and meaningful analytical feedback for students and educators using LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patterns of Persistence and Diffusibility across World's Languages. (arXiv:2401.01698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01698">
<div class="article-summary-box-inner">
<span><p>Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e.~a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope. (arXiv:2401.01699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01699">
<div class="article-summary-box-inner">
<span><p>This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs. (arXiv:2401.01711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01711">
<div class="article-summary-box-inner">
<span><p>Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGA: Vision and Graph Fused Attention Network for Rumor Detection. (arXiv:2401.01759v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01759">
<div class="article-summary-box-inner">
<span><p>With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-target Stance Detection by Exploiting Target Analytical Perspectives. (arXiv:2401.01761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01761">
<div class="article-summary-box-inner">
<span><p>Cross-target stance detection (CTSD) is an important task, which infers the
attitude of the destination target by utilizing annotated data derived from the
source target. One important approach in CTSD is to extract domain-invariant
features to bridge the knowledge gap between multiple targets. However, the
analysis of informal and short text structure, and implicit expressions,
complicate the extraction of domain-invariant knowledge. In this paper, we
propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the
analysis perspective as a bridge to transfer knowledge. First, we develop a
two-stage instruct-based chain-of-thought method (TsCoT) to elicit target
analysis perspectives and provide natural language explanations (NLEs) from
multiple viewpoints by formulating instructions based on large language model
(LLM). Second, we propose a multi-perspective prompt-tuning framework
(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments
results demonstrate the superiority of MPPT against the state-of-the-art
baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering. (arXiv:2401.01780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01780">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLM) are able to accumulate and restore
knowledge, they are still prone to hallucination. Especially when faced with
factual questions, LLM cannot only rely on knowledge stored in parameters to
guarantee truthful and correct answers. Augmenting these models with the
ability to search on external information sources, such as the web, is a
promising approach to ground knowledge to retrieve information. However,
searching in a large collection of documents introduces additional
computational/time costs. An optimal behavior would be to query external
resources only when the LLM is not confident about answers. In this paper, we
propose a new LLM able to self-estimate if it is able to answer directly or
needs to request an external tool. We investigate a supervised approach by
introducing a hallucination masking mechanism in which labels are generated
using a close book question-answering task. In addition, we propose to leverage
parameter-efficient fine-tuning techniques to train our model on a small amount
of data. Our model directly provides answers for $78.2\%$ of the known queries
and opts to search for $77.2\%$ of the unknown ones. This results in the API
being utilized only $62\%$ of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physio: An LLM-Based Physiotherapy Advisor. (arXiv:2401.01825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01825">
<div class="article-summary-box-inner">
<span><p>The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01830">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01843">
<div class="article-summary-box-inner">
<span><p>Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01854">
<div class="article-summary-box-inner">
<span><p>As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01862">
<div class="article-summary-box-inner">
<span><p>What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01879">
<div class="article-summary-box-inner">
<span><p>A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11348">
<div class="article-summary-box-inner">
<span><p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Not Strong Abstract Reasoners. (arXiv:2305.19555v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19555">
<div class="article-summary-box-inner">
<span><p>Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing. (arXiv:2306.02052v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02052">
<div class="article-summary-box-inner">
<span><p>Despite increasing interest in the automatic detection of media frames in
NLP, the problem is typically simplified as single-label classification and
adopts a topic-like view on frames, evading modelling the broader
document-level narrative. In this work, we revisit a widely used
conceptualization of framing from the communication sciences which explicitly
captures elements of narratives, including conflict and its resolution, and
integrate it with the narrative framing of key entities in the story as heroes,
victims or villains. We adapt an effective annotation paradigm that breaks a
complex annotation task into a series of simpler binary questions, and present
an annotated data set of English news articles, and a case study on the framing
of climate change in articles from news outlets across the political spectrum.
Finally, we explore automatic multi-label prediction of our frames with
supervised and semi-supervised approaches, and present a novel retrieval-based
method which is both effective and transparent in its predictions. We conclude
with a discussion of opportunities and challenges for future work on
document-level models of narrative framing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17408">
<div class="article-summary-box-inner">
<span><p>As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05134">
<div class="article-summary-box-inner">
<span><p>The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Assessment Tests are Unreliable Measures of LLM Personality. (arXiv:2309.08163v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08163">
<div class="article-summary-box-inner">
<span><p>As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of "personality"
of LLMs using self-assessment personality tests developed to measure human
personality. Yet almost none of these works verify the applicability of these
tests on LLMs. In this paper, we analyze the reliability of LLM personality
scores obtained from self-assessment personality tests using two simple
experiments. We first introduce the property of prompt sensitivity, where three
semantically equivalent prompts representing three intuitive ways of
administering self-assessment tests on LLMs are used to measure the personality
of the same LLM. We find that all three prompts lead to very different
personality scores, a difference that is statistically significant for all
traits in a large majority of scenarios. We then introduce the property of
option-order symmetry for personality measurement of LLMs. Since most of the
self-assessment tests exist in the form of multiple choice question (MCQ)
questions, we argue that the scores should also be robust to not just the
prompt template but also the order in which the options are presented. This
test unsurprisingly reveals that the self-assessment test scores are not robust
to the order of the options. These simple tests, done on ChatGPT and three
Llama2 models of different sizes, show that self-assessment personality tests
created for humans are unreliable measures of personality in LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04444">
<div class="article-summary-box-inner">
<span><p>Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06452">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12072">
<div class="article-summary-box-inner">
<span><p>Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.16033">
<div class="article-summary-box-inner">
<span><p>Multimodal Large Language Models (MLLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether MLLMs can perceive details as well as larger
components in images. In particular, we show that their zero-shot accuracy in
answering visual questions is very sensitive to the size of the visual subject
related to the question, declining up to $45.91\%$ with size. Furthermore, we
show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. To scale up the usefulness of
human cropping, we propose ViCrop, a general framework that utilizes automatic
visual cropping to enhance zero-shot VQA of MLLMs. We construct five variants
of ViCrop leveraging either external localization models or the decision
process of the given MLLM itself. Our results show that ViCrop improves MLLMs'
zero-shot accuracy across different VQA datasets, for example, enhances
BLIP2-T5's performance by $32.23\%$ on the TextVQA test set. To facilitate
further investigation of MLLMs' behaviors, our code is publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19923">
<div class="article-summary-box-inner">
<span><p>Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
</p>
<p>To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models. (arXiv:2312.06281v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.06281">
<div class="article-summary-box-inner">
<span><p>We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of
emotional intelligence in Large Language Models (LLMs). We assess the ability
of LLMs to understand complex emotions and social interactions by asking them
to predict the intensity of emotional states of characters in a dialogue. The
benchmark is able to discriminate effectively between a wide range of models.
We find that EQ-Bench correlates strongly with comprehensive multi-domain
benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may
be capturing similar aspects of broad intelligence. Our benchmark produces
highly repeatable results using a set of 60 English-language questions. We also
provide open-source code for an automated benchmarking pipeline at
https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.07913">
<div class="article-summary-box-inner">
<span><p>Text watermarking algorithms play a crucial role in the copyright protection
of textual content, yet their capabilities and application scenarios have been
limited historically. The recent developments in large language models (LLMs)
have opened new opportunities for the advancement of text watermarking
techniques. LLMs not only enhance the capabilities of text watermarking
algorithms through their text understanding and generation abilities but also
necessitate the use of text watermarking algorithms for their own copyright
protection. This paper conducts a comprehensive survey of the current state of
text watermarking technology, covering four main aspects: (1) an overview and
comparison of different text watermarking techniques; (2) evaluation methods
for text watermarking algorithms, including their success rates, impact on text
quality, robustness, and unforgeability; (3) potential application scenarios
for text watermarking technology; (4) current challenges and future directions
for development. This survey aims to provide researchers with a thorough
understanding of text watermarking technology, thereby promoting its further
advancement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.10997">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever. (arXiv:2401.01076v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01076">
<div class="article-summary-box-inner">
<span><p>Recently, substantial advancements in pre-trained vision-language models have
greatly enhanced the capabilities of multi-modal dialog systems. These models
have demonstrated significant improvements by fine-tuning on downstream tasks.
However, the existing pre-trained models primarily focus on effectively
capturing the alignment between vision and language modalities, often ignoring
the intricate nature of dialog context. In this paper, we propose a
parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog
retrieval. Specifically, our approach introduces a multi-modal context prompt
generator to learn context features which are subsequently distilled into
prompts within the pre-trained vision-language model CLIP. Besides, we
introduce domain prompt to mitigate the disc repancy from the downstream dialog
data. To facilitate various types of retrieval, we also design multiple experts
to learn mappings from CLIP outputs to multi-modal representation space, with
each expert being responsible to one specific retrieval type. Extensive
experiments show that DialCLIP achieves state-of-the-art performance on two
widely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a
mere 0.04% of the total parameters. These results highlight the efficacy and
efficiency of our proposed approach, underscoring its potential to advance the
field of multi-modal dialog retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01078">
<div class="article-summary-box-inner">
<span><p>Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the "luc bat" genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01262">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01283">
<div class="article-summary-box-inner">
<span><p>Automatic machine translation metrics often use human translations to
determine the quality system translations. Common wisdom in the field dictates
that the human references should be of very high quality. However, there are no
cost-benefit analyses that could be used to guide practitioners who plan to
collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01313">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-04 23:12:22.051738023 UTC">2024-01-04 23:12:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>