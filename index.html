<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-24T01:30:00Z">05-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Dataset Towards Extracting Virus-Host Interactions. (arXiv:2305.13317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13317">
<div class="article-summary-box-inner">
<span><p>We describe a novel dataset for the automated recognition of named taxonomic
and other entities relevant to the association of viruses with their hosts. We
further describe some initial results using pre-trained models on the
named-entity recognition (NER) task on this novel dataset. We propose that our
dataset of manually annotated abstracts now offers a Gold Standard Corpus for
training future NER models in the automated extraction of host-pathogen
detection methods from scientific publications, and further explain how our
work makes first steps towards predicting the important human health-related
concept of viral spillover risk automatically from the scientific literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13330">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that it is possible to train an $\textit{unsupervised}$
automatic speech recognition (ASR) system using only unpaired audio and text.
Existing unsupervised ASR methods assume that no labeled data can be used for
training. We argue that even if one does not have any labeled audio for a given
language, there is $\textit{always}$ labeled data available for other
languages. We show that it is possible to use character-level acoustic models
(AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new
language. Here, "unsupervised" means no labeled audio is available for the
$\textit{target}$ language. Our approach is based on two key ingredients: (i)
generating pseudo-labels (PLs) of the $\textit{target}$ language using some
$\textit{other}$ language AM and (ii) constraining these PLs with a
$\textit{target language model}$. Our approach is effective on Common Voice:
e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms
character-based wav2vec-U 2.0 by 15% absolute WER on LJSpeech with 800h of
labeled German data instead of 60k hours of unlabeled English data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning. (arXiv:2305.13331v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13331">
<div class="article-summary-box-inner">
<span><p>Aphasia is a language disorder that affects the speaking ability of millions
of patients. This paper presents a new benchmark for Aphasia speech recognition
and detection tasks using state-of-the-art speech recognition techniques with
the AphsiaBank dataset. Specifically, we introduce two multi-task learning
methods based on the CTC/Attention architecture to perform both tasks
simultaneously. Our system achieves state-of-the-art speaker-level detection
accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia
patients. In addition, we demonstrate the generalizability of our approach by
applying it to another disordered speech database, the DementiaBank Pitt
corpus. We will make our all-in-one recipes and pre-trained model publicly
available to facilitate reproducibility. Our standardized data preprocessing
pipeline and open-source recipes enable researchers to compare results
directly, promoting progress in disordered speech processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13338">
<div class="article-summary-box-inner">
<span><p>Molecular biologists frequently interpret gene lists derived from
high-throughput experiments and computational analysis. This is typically done
as a statistical enrichment analysis that measures the over- or
under-representation of biological function terms associated with genes or
their properties, based on curated assertions from a knowledge base (KB) such
as the Gene Ontology (GO). Interpreting gene lists can also be framed as a
textual summarization task, enabling the use of Large Language Models (LLMs),
potentially utilizing scientific texts directly and avoiding reliance on a KB.
</p>
<p>We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language
Descriptions of Controlled Terms for Ontology Reporting), a method that uses
GPT models to perform gene set function summarization as a complement to
standard enrichment analysis. This method can use different sources of gene
functional information: (1) structured text derived from curated ontological KB
annotations, (2) ontology-free narrative gene summaries, or (3) direct model
retrieval.
</p>
<p>We demonstrate that these methods are able to generate plausible and
biologically valid summary GO term lists for gene sets. However, GPT-based
approaches are unable to deliver reliable scores or p-values and often return
terms that are not statistically significant. Crucially, these methods were
rarely able to recapitulate the most precise and informative term from standard
enrichment, likely due to an inability to generalize and reason using an
ontology. Results are highly nondeterministic, with minor variations in prompt
resulting in radically different term lists. Our results show that at this
point, LLM-based methods are unsuitable as a replacement for standard term
enrichment analysis and that manual curation of ontological assertions remains
necessary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Limitations of Simulating Active Learning. (arXiv:2305.13342v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13342">
<div class="article-summary-box-inner">
<span><p>Active learning (AL) is a human-and-model-in-the-loop paradigm that
iteratively selects informative unlabeled data for human annotation, aiming to
improve over random sampling. However, performing AL experiments with human
annotations on-the-fly is a laborious and expensive process, thus unrealistic
for academic research. An easy fix to this impediment is to simulate AL, by
treating an already labeled and publicly available dataset as the pool of
unlabeled data. In this position paper, we first survey recent literature and
highlight the challenges across all different steps within the AL loop. We
further unveil neglected caveats in the experimental setup that can
significantly affect the quality of AL research. We continue with an
exploration of how the simulation setting can govern empirical findings,
arguing that it might be one of the answers behind the ever posed question
``why do active learning algorithms sometimes fail to outperform random
sampling?''. We argue that evaluating AL algorithms on available labeled
datasets might provide a lower bound as to their effectiveness in real data. We
believe it is essential to collectively shape the best practices for AL
research, particularly as engineering advancements in LLMs push the research
focus towards data-driven approaches (e.g., data efficiency, alignment,
fairness). In light of this, we have developed guidelines for future work. Our
aim is to draw attention to these limitations within the community, in the hope
of finding ways to address them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLMs facilitate interpretation of pre-trained language models?. (arXiv:2305.13386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13386">
<div class="article-summary-box-inner">
<span><p>Work done to uncover the knowledge encoded within pre-trained language
models, rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
hierarchical clustering over contextualized representations and then annotate
these concepts using GPT annotations. Our findings demonstrate that ChatGPT
produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing framework and neuron interpretation. To facilitate further exploration
and experimentation in this field, we have made available a substantial
ConceptNet dataset comprising 39,000 annotated latent concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The neural dynamics of auditory word recognition and integration. (arXiv:2305.13388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13388">
<div class="article-summary-box-inner">
<span><p>Listeners recognize and integrate words in rapid and noisy everyday speech by
combining expectations about upcoming content with incremental sensory
evidence. We present a computational model of word recognition which formalizes
this perceptual process in Bayesian decision theory. We fit this model to
explain scalp EEG signals recorded as subjects passively listened to a
fictional story, revealing both the dynamics of the online auditory word
recognition process and the neural correlates of the recognition and
integration of words.
</p>
<p>The model reveals distinct neural processing of words depending on whether or
not they can be quickly recognized. While all words trigger a neural response
characteristic of probabilistic integration -- voltage modulations predicted by
a word's surprisal in context -- these modulations are amplified for words
which require more than roughly 100 ms of input to be recognized. We observe no
difference in the latency of these neural responses according to words'
recognition times.Our results support a two-part model of speech comprehension,
combining an eager and rapid process of word recognition with a temporally
independent process of word integration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance. (arXiv:2305.13395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13395">
<div class="article-summary-box-inner">
<span><p>Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical
literature is paramount for public safety, but involves slow and costly manual
labor. We set out to improve drug safety monitoring (pharmacovigilance, PV)
through the use of Natural Language Processing (NLP). We introduce BioDEX, a
large-scale resource for Biomedical adverse Drug Event Extraction, rooted in
the historical output of drug safety reporting in the U.S. BioDEX consists of
65k abstracts and 19k full-text biomedical papers with 256k associated
document-level safety reports created by medical experts. The core features of
these reports include the reported weight, age, and biological sex of a
patient, a set of drugs taken by the patient, the drug dosages, the reactions
experienced, and whether the reaction was life threatening. In this work, we
consider the task of predicting the core information of the report given its
originating paper. We estimate human performance to be 72.0% F1, whereas our
best model achieves 62.3% F1, indicating significant headroom on this task. We
also begin to explore ways in which these models could help professional PV
reviewers. Our code and data are available: https://github.com/KarelDO/BioDEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study of conceptual language similarity: comparison and evaluation. (arXiv:2305.13401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13401">
<div class="article-summary-box-inner">
<span><p>An interesting line of research in natural language processing (NLP) aims to
incorporate linguistic typology to bridge linguistic diversity and assist the
research of low-resource languages. While most works construct linguistic
similarity measures based on lexical or typological features, such as word
order and verbal inflection, recent work has introduced a novel approach to
defining language similarity based on how they represent basic concepts, which
is complementary to existing similarity measures. In this work, we study the
conceptual similarity in detail and evaluate it extensively on a binary
classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GATology for Linguistics: What Syntactic Dependencies It Knows. (arXiv:2305.13403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13403">
<div class="article-summary-box-inner">
<span><p>Graph Attention Network (GAT) is a graph neural network which is one of the
strategies for modeling and representing explicit syntactic knowledge and can
work with pre-trained models, such as BERT, in downstream tasks. Currently,
there is still a lack of investigation into how GAT learns syntactic knowledge
from the perspective of model structure. As one of the strategies for modeling
explicit syntactic knowledge, GAT and BERT have never been applied and
discussed in Machine Translation (MT) scenarios. We design a dependency
relation prediction task to study how GAT learns syntactic knowledge of three
languages as a function of the number of attention heads and layers. We also
use a paired t-test and F1-score to clarify the differences in syntactic
dependency prediction between GAT and BERT fine-tuned by the MT task (MT-B).
The experiments show that better performance can be achieved by appropriately
increasing the number of attention heads with two GAT layers. With more than
two layers, learning suffers. Moreover, GAT is more competitive in training
speed and syntactic dependency prediction than MT-B, which may reveal a better
incorporation of modeling explicit syntactic knowledge and the possibility of
combining GAT and BERT in the MT tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13406">
<div class="article-summary-box-inner">
<span><p>Existing large language models (LLMs) that mainly focus on Standard American
English (SAE) often lead to significantly worse performance when being applied
to other English dialects. While existing mitigations tackle discrepancies for
individual target dialects, they assume access to high-accuracy dialect
identification systems. The boundaries between dialects are inherently
flexible, making it difficult to categorize language into discrete predefined
categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic
Aggregation), a modular approach to imbue SAE-trained models with
multi-dialectal robustness by composing adapters which handle specific
linguistic features. The compositional architecture of DADA allows for both
targeted adaptation to specific dialect variants and simultaneous adaptation to
various dialects. We show that DADA is effective for both single task and
instruction finetuned language models, offering an extensible and interpretable
framework for adapting existing LLMs to different English dialects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular Domain Adaptation for Conformer-Based Streaming ASR. (arXiv:2305.13408v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13408">
<div class="article-summary-box-inner">
<span><p>Speech data from different domains has distinct acoustic and linguistic
characteristics. It is common to train a single multidomain model such as a
Conformer transducer for speech recognition on a mixture of data from all
domains. However, changing data in one domain or adding a new domain would
require the multidomain model to be retrained. To this end, we propose a
framework called modular domain adaptation (MDA) that enables a single model to
process multidomain data while keeping all parameters domain-specific, i.e.,
each parameter is only trained by data from one domain. On a streaming
Conformer transducer trained only on video caption data, experimental results
show that an MDA-based model can reach similar performance as the multidomain
model on other domains such as voice search and dictation by adding per-domain
adapters and per-domain feed-forward networks in the Conformer encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method. (arXiv:2305.13412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13412">
<div class="article-summary-box-inner">
<span><p>Automatic summarization generates concise summaries that contain key ideas of
source documents. As the most mainstream datasets for the news sub-domain,
CNN/DailyMail and BBC XSum have been widely used for performance benchmarking.
However, the reference summaries of those datasets turn out to be noisy, mainly
in terms of factual hallucination and information redundancy. To address this
challenge, we first annotate new expert-writing Element-aware test sets
following the "Lasswell Communication Model" proposed by Lasswell (1948),
allowing reference summaries to focus on more fine-grained news elements
objectively and comprehensively. Utilizing the new test sets, we observe the
surprising zero-shot summary ability of LLMs, which addresses the issue of the
inconsistent results between human preference and automatic evaluation metrics
of LLMs' zero-shot summaries in prior work. Further, we propose a Summary
Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step
by step, which helps them integrate more fine-grained details of source
documents into the final summaries that correlate with the human writing
mindset. Experimental results show our method outperforms state-of-the-art
fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two
datasets, respectively. Dataset and code are publicly available at
https://github.com/Alsace08/SumCoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Knowledge via Graph Attention with BERT in Machine Translation. (arXiv:2305.13413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13413">
<div class="article-summary-box-inner">
<span><p>Although the Transformer model can effectively acquire context features via a
self-attention mechanism, deeper syntactic knowledge is still not effectively
modeled. To alleviate the above problem, we propose Syntactic knowledge via
Graph attention with BERT (SGB) in Machine Translation (MT) scenarios. Graph
Attention Network (GAT) and BERT jointly represent syntactic dependency feature
as explicit knowledge of the source language to enrich source language
representations and guide target language generation. Our experiments use gold
syntax-annotation sentences and Quality Estimation (QE) model to obtain
interpretability of translation quality improvement regarding syntactic
knowledge without being limited to a BLEU score. Experiments show that the
proposed SGB engines improve translation quality across the three MT tasks
without sacrificing BLEU scores. We investigate what length of source sentences
benefits the most and what dependencies are better identified by the SGB
engines. We also find that learning of specific dependency relations by GAT can
be reflected in the translation quality containing such relations and that
syntax on the graph leads to new modeling of syntactic aspects of source
sentences in the middle and bottom layers of BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT. (arXiv:2305.13417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13417">
<div class="article-summary-box-inner">
<span><p>Recent advances in interpretability suggest we can project weights and hidden
states of transformer-based language models (LMs) to their vocabulary, a
transformation that makes them human interpretable and enables us to assign
semantics to what was seen only as numerical vectors. In this paper, we
interpret LM attention heads and memory values, the vectors the models
dynamically create and recall while processing a given input. By analyzing the
tokens they represent through this projection, we identify patterns in the
information flow inside the attention mechanism. Based on these discoveries, we
create a tool to visualize a forward pass of Generative Pre-trained
Transformers (GPTs) as an interactive flow graph, with nodes representing
neurons or hidden states and edges representing the interactions between them.
Our visualization simplifies huge amounts of data into easy-to-read plots that
reflect why models output their results. We demonstrate the utility of our
modeling by identifying the effect LM components have on the intermediate
processing in the model before outputting a prediction. For instance, we
discover that layer norms are used as semantic filters and find neurons that
act as regularization vectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13455">
<div class="article-summary-box-inner">
<span><p>Recent work has proposed a methodology for the systematic evaluation of
"Situated Language Understanding Agents"-agents that operate in rich linguistic
and non-linguistic contexts-through testing them in carefully constructed
interactive settings. Other recent work has argued that Large Language Models
(LLMs), if suitably set up, can be understood as (simulators of) such agents. A
connection suggests itself, which this paper explores: Can LLMs be evaluated
meaningfully by exposing them to constrained game-like settings that are built
to challenge specific capabilities? As a proof of concept, this paper
investigates five interaction settings, showing that current chat-optimised
LLMs are, to an extent, capable to follow game-play instructions. Both this
capability and the quality of the game play, measured by how well the
objectives of the different games are met, follows the development cycle, with
newer models performing better. The metrics even for the comparatively simple
example games are far from being saturated, suggesting that the proposed
instrument will remain to have diagnostic value. Our general framework for
implementing and evaluating games with LLMs is available at
https://github.com/clp-research/clembench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13469">
<div class="article-summary-box-inner">
<span><p>In this work, we present the first dataset, \dataset, for performing event
extraction from conversational email threads. To this end, we first proposed a
new taxonomy covering 10 event types and 76 arguments in the email domain. Our
final dataset includes $\sim$4K emails annotated with $\sim$9K event instances.
To understand the task challenges, we conducted a series of experiments
comparing two commonly-seen lines of approaches for event extraction, i.e.,
sequence labeling and generative end-to-end extraction (including few-shot
GPT-3.5). Our results showed that the task of email event extraction is far
from being addressed, due to challenges lying in, e.g., extracting
non-continuous, shared trigger spans, extracting non-named entity arguments,
and modeling the email conversational history. Our work thus suggests more
investigations in this domain-specific event extraction task in the
future.\footnote{The source code and dataset can be obtained from
\url{https://github.com/salokr/Email-Event-Extraction}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look-back Decoding for Open-Ended Text Generation. (arXiv:2305.13477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13477">
<div class="article-summary-box-inner">
<span><p>Given a prefix (context), open-ended generation aims to decode texts that are
coherent, which don't abruptly drift from previous topics, and informative,
which don't suffer from undesired repetitions. In this paper, we propose
Look-back, an improved decoding algorithm that leverages the Kullback-Leibler
divergence to track the distribution distance between current and historical
decoding steps. Thus Look-back can automatically predict potential repetitive
phrase and topic drift, and remove tokens that may cause the failure modes,
restricting the next token probability distribution within a plausible distance
to the history. We perform decoding experiments on document continuation and
story generation, and demonstrate that Look-back is able to generate more
fluent and coherent text, outperforming other strong decoding methods
significantly in both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Readability Assessment for Closely Related Languages. (arXiv:2305.13478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13478">
<div class="article-summary-box-inner">
<span><p>In recent years, the main focus of research on automatic readability
assessment (ARA) has shifted towards using expensive deep learning-based
methods with the primary goal of increasing models' accuracy. This, however, is
rarely applicable for low-resource languages where traditional handcrafted
features are still widely used due to the lack of existing NLP tools to extract
deeper linguistic representations. In this work, we take a step back from the
technical component and focus on how linguistic aspects such as mutual
intelligibility or degree of language relatedness can improve ARA in a
low-resource setting. We collect short stories written in three languages in
the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment
models and explore the interaction of data and features in various
cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel
specialized feature exploiting n-gram overlap applied to languages with high
mutual intelligibility, significantly improves the performance of ARA models
compared to the use of off-the-shelf large multilingual language models alone.
Consequently, when both linguistic representations are combined, we achieve
state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA
in Bikol.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13484">
<div class="article-summary-box-inner">
<span><p>In the rapidly evolving field of deep learning, the performance of model
inference has become a pivotal aspect as models become more complex and are
deployed in diverse applications. Among these, autoregressive models stand out
due to their state-of-the-art performance in numerous generative tasks. These
models, by design, harness a temporal dependency structure, where the current
token's probability distribution is conditioned on preceding tokens. This
inherently sequential characteristic, however, adheres to the Markov Chain
assumption and lacks temporal parallelism, which poses unique challenges.
Particularly in industrial contexts where inference requests, following a
Poisson time distribution, necessitate diverse response lengths, this absence
of parallelism is more profound. Existing solutions, such as dynamic batching
and concurrent model instances, nevertheless, come with severe overheads and a
lack of flexibility, these coarse-grained methods fall short of achieving
optimal latency and throughput. To address these shortcomings, we propose
Flavor -- a temporal fusion framework for efficient inference in autoregressive
models, eliminating the need for heuristic settings and applies to a wide range
of inference scenarios. By providing more fine-grained parallelism on the
temporality of requests and employing an efficient memory shuffle algorithm,
Flover achieves up to 11x faster inference on GPT models compared to the
cutting-edge solutions provided by NVIDIA Triton FasterTransformer. Crucially,
by leveraging the advanced tensor parallel technique, Flover proves efficacious
across diverse computational landscapes, from single-GPU setups to multi-node
scenarios, thereby offering robust performance optimization that transcends
hardware boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes. (arXiv:2305.13499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13499">
<div class="article-summary-box-inner">
<span><p>Many real-world applications require making multiple predictions from the
same text. Fine-tuning a large pre-trained language model for each downstream
task causes computational burdens in the inference time due to several times of
forward passes. To amortize the computational cost, freezing the language model
and building lightweight models for downstream tasks based on fixed text
representations are common solutions. Accordingly, how to learn fixed but
general text representations that can generalize well to unseen downstream
tasks becomes a challenge. Previous works have shown that the generalizability
of representations can be improved by fine-tuning the pre-trained language
model with some source tasks in a multi-tasking way. In this work, we propose a
prefix-based method to learn the fixed text representations with source tasks.
We learn a task-specific prefix for each source task independently and combine
them to get the final representations. Our experimental results show that
prefix-based training performs better than multi-tasking training and can
update the text representations at a smaller computational cost than
multi-tasking training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation for Code Generation. (arXiv:2305.13504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13504">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) methods developed for natural language
processing have been shown to be highly successful in automating translation
from one natural language to another. Recently, these NMT methods have been
adapted to the generation of program code. In NMT for code generation, the task
is to generate output source code that satisfies constraints expressed in the
input. In the literature, a variety of different input scenarios have been
explored, including generating code based on natural language description,
lower-level representations such as binary or assembly (neural decompilation),
partial representations of source code (code completion and repair), and source
code in another language (code translation). In this paper we survey the NMT
for code generation literature, cataloging the variety of methods that have
been explored according to input and output representations, model
architectures, optimization techniques used, data sets, and evaluation methods.
We discuss the limitations of existing methods and future research directions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13507">
<div class="article-summary-box-inner">
<span><p>Misinformation, i.e. factually incorrect information, is often conveyed in
multiple modalities, e.g. an image accompanied by a caption. It is perceived as
more credible by humans, and spreads faster and wider than its text-only
counterparts. While an increasing body of research investigates automated
fact-checking (AFC), previous surveys mostly focus on textual misinformation.
In this survey, we conceptualise a framework for AFC including subtasks unique
to multimodal misinformation. Furthermore, we discuss related terminological
developed in different communities in the context of our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13512">
<div class="article-summary-box-inner">
<span><p>Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13514">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive few-shot learning
capabilities, but they often underperform compared to fine-tuned models on
challenging tasks. Furthermore, their large size and restricted access only
through APIs make task-specific fine-tuning impractical. Moreover, LLMs are
sensitive to different aspects of prompts (e.g., the selection and order of
demonstrations) and can thus require time-consuming prompt engineering. In this
light, we propose a method to correct LLM outputs without relying on their
weights. First, we generate a pool of candidates by few-shot prompting an LLM.
Second, we refine the LLM-generated outputs using a smaller model, the
LM-corrector (LMCor), which is trained to rank, combine and rewrite the
candidates to produce the final target output. Our experiments demonstrate that
even a small LMCor model (250M) substantially improves the few-shot performance
of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor
exhibits robustness against different prompts, thereby minimizing the need for
extensive prompt engineering. Finally, we showcase that the LMCor can be
seamlessly integrated with different LLMs at inference time, serving as a
plug-and-play module to improve their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Speech Technology to 1,000+ Languages. (arXiv:2305.13516v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13516">
<div class="article-summary-box-inner">
<span><p>Expanding the language coverage of speech technology has the potential to
improve access to information for many more people. However, current speech
technology is restricted to about one hundred languages which is a small
fraction of the over 7,000 languages spoken around the world. The Massively
Multilingual Speech (MMS) project increases the number of supported languages
by 10-40x, depending on the task. The main ingredients are a new dataset based
on readings of publicly available religious texts and effectively leveraging
self-supervised learning. We built pre-trained wav2vec 2.0 models covering
1,406 languages, a single multilingual automatic speech recognition model for
1,107 languages, speech synthesis models for the same number of languages, as
well as a language identification model for 4,017 languages. Experiments show
that our multilingual speech recognition model more than halves the word error
rate of Whisper on 54 languages of the FLEURS benchmark while being trained on
a small fraction of the labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEO: Corpus-based Open-Domain Event Ontology Induction. (arXiv:2305.13521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13521">
<div class="article-summary-box-inner">
<span><p>Existing event-centric NLP models often only apply to the pre-defined
ontology, which significantly restricts their generalization capabilities. This
paper presents CEO, a novel Corpus-based Event Ontology induction model to
relax the restriction imposed by pre-defined event ontologies. Without direct
supervision, CEO leverages distant supervision from available summary datasets
to detect corpus-wise salient events and exploits external event knowledge to
force events within a short distance to have close embeddings. Experiments on
three popular event datasets show that the schema induced by CEO has better
coverage and higher accuracy than previous methods. Moreover, CEO is the first
event ontology induction model that can induce a hierarchical event ontology
with meaningful names on eleven open-domain corpora, making the induced schema
more trustworthy and easier to be further curated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Generative Large Language Model for Medical Research and Healthcare. (arXiv:2305.13523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13523">
<div class="article-summary-box-inner">
<span><p>There is enormous enthusiasm and concerns in using large language models
(LLMs) in healthcare, yet current assumptions are all based on general-purpose
LLMs such as ChatGPT. This study develops a clinical generative LLM,
GatorTronGPT, using 277 billion words of mixed clinical and English text with a
GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical
natural language processing for medical research. Synthetic NLP models trained
using GatorTronGPT generated text outperform NLP models trained using
real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)
scale shows that there is no significant difference in linguistic readability
(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical
relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that
physicians cannot differentiate them (p &lt; 0.001). This study provides insights
on the opportunities and challenges of LLMs for medical research and
healthcare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning the Norwegian UD Treebank with Entity and Coreference Information. (arXiv:2305.13527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13527">
<div class="article-summary-box-inner">
<span><p>This paper presents a merged collection of entity and coreference annotated
data grounded in the Universal Dependencies (UD) treebanks for the two written
forms of Norwegian: Bokm{\aa}l and Nynorsk. The aligned and converted corpora
are the \textit{Norwegian Named Entities} (NorNE) and \textit{Norwegian
Anaphora Resolution Corpus} (NARC). While NorNE is aligned with an older
version of the treebank, NARC is misaligned and requires extensive
transformation from the original annotations to the UD structure and CoNLL-U
format. We here demonstrate the conversion and alignment processes, along with
an analysis of discovered issues and errors in the data -- some of which
include data split overlaps in the original treebank. These procedures and the
developed system may prove helpful for future corpus alignment and coreference
annotation endeavors. The merged corpora comprise the first Norwegian UD
treebank enriched with named entities and coreference information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer-Free Data-Efficient Multilingual Slot Labeling. (arXiv:2305.13528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13528">
<div class="article-summary-box-inner">
<span><p>Slot labeling (SL) is a core component of task-oriented dialogue (ToD)
systems, where slots and corresponding values are usually language-, task- and
domain-specific. Therefore, extending the system to any new
language-domain-task configuration requires (re)running an expensive and
resource-intensive data annotation process. To mitigate the inherent data
scarcity issue, current research on multilingual ToD assumes that sufficient
English-language annotated data are always available for particular tasks and
domains, and thus operates in a standard cross-lingual transfer setup. In this
work, we depart from this often unrealistic assumption. We examine challenging
scenarios where such transfer-enabling English annotated data cannot be
guaranteed, and focus on bootstrapping multilingual data-efficient slot
labelers in transfer-free scenarios directly in the target languages without
any English-ready data. We propose a two-stage slot labeling approach (termed
TWOSL) which transforms standard multilingual sentence encoders into effective
slot labelers. In Stage 1, relying on SL-adapted contrastive learning with only
a handful of SL-annotated examples, we turn sentence encoders into
task-specific span encoders. In Stage 2, we recast SL from a token
classification into a simpler, less data-intensive span classification task.
Our results on two standard multilingual TOD datasets and across diverse
languages confirm the effectiveness and robustness of TWOSL. It is especially
effective for the most challenging transfer-free few-shot setups, paving the
way for quick and data-efficient bootstrapping of multilingual slot labelers
for ToD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian Language. (arXiv:2305.13530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13530">
<div class="article-summary-box-inner">
<span><p>This paper provides an overview of a text mining tool the StyloMetrix
developed initially for the Polish language and further extended for English
and recently for Ukrainian. The StyloMetrix is built upon various metrics
crafted manually by computational linguists and researchers from literary
studies to analyze grammatical, stylistic, and syntactic patterns. The idea of
constructing the statistical evaluation of syntactic and grammar features is
straightforward and familiar for the languages like English, Spanish, German,
and others; it is yet to be developed for low-resource languages like
Ukrainian. We describe the StyloMetrix pipeline and provide some experiments
with this tool for the text classification task. We also describe our package's
main limitations and the metrics' evaluation procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting. (arXiv:2305.13533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13533">
<div class="article-summary-box-inner">
<span><p>Open-world Relation Extraction (OpenRE) has recently garnered significant
attention. However, existing approaches tend to oversimplify the problem by
assuming that all unlabeled texts belong to novel classes, thereby limiting the
practicality of these methods. We argue that the OpenRE setting should be more
aligned with the characteristics of real-world data. Specifically, we propose
two key improvements: (a) unlabeled data should encompass known and novel
classes, including hard-negative instances; and (b) the set of novel classes
should represent long-tail relation types. Furthermore, we observe that popular
relations such as titles and locations can often be implicitly inferred through
specific patterns, while long-tail relations tend to be explicitly expressed in
sentences. Motivated by these insights, we present a novel method called KNoRD
(Known and Novel Relation Discovery), which effectively classifies explicitly
and implicitly expressed relations from known and novel classes within
unlabeled data. Experimental evaluations on several Open-world RE benchmarks
demonstrate that KNoRD consistently outperforms other existing methods,
achieving significant performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Language Model Hallucinations Can Snowball. (arXiv:2305.13534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13534">
<div class="article-summary-box-inner">
<span><p>A major risk of using language models in practical applications is their
tendency to hallucinate incorrect statements. Hallucinations are often
attributed to knowledge gaps in LMs, but we hypothesize that in some cases,
when justifying previously generated hallucinations, LMs output false claims
that they can separately recognize as incorrect. We construct three
question-answering datasets where ChatGPT and GPT-4 often state an incorrect
answer and offer an explanation with at least one incorrect claim. Crucially,
we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes,
respectively. We refer to this phenomenon as hallucination snowballing: an LM
over-commits to early mistakes, leading to more mistakes that it otherwise
would not make.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Classifier Robustness through Active Generation of Pairwise Counterfactuals. (arXiv:2305.13535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13535">
<div class="article-summary-box-inner">
<span><p>Counterfactual Data Augmentation (CDA) is a commonly used technique for
improving robustness in natural language classifiers. However, one fundamental
challenge is how to discover meaningful counterfactuals and efficiently label
them, with minimal human labeling cost. Most existing methods either completely
rely on human-annotated labels, an expensive process which limits the scale of
counterfactual data, or implicitly assume label invariance, which may mislead
the model with incorrect labels. In this paper, we present a novel framework
that utilizes counterfactual generative models to generate a large number of
diverse counterfactuals by actively sampling from regions of uncertainty, and
then automatically label them with a learned pairwise classifier. Our key
insight is that we can more correctly label the generated counterfactuals by
training a pairwise classifier that interpolates the relationship between the
original example and the counterfactual. We demonstrate that with a small
amount of human-annotated counterfactual data (10%), we can generate a
counterfactual augmentation dataset with learned labels, that provides an
18-20% improvement in robustness and a 14-21% reduction in errors on 6
out-of-domain datasets, comparable to that of a fully human-annotated
counterfactual dataset for both sentiment classification and question
paraphrase tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13547">
<div class="article-summary-box-inner">
<span><p>Text classification tasks often encounter few shot scenarios with limited
labeled data, and addressing data scarcity is crucial. Data augmentation with
mixup has shown to be effective on various text classification tasks. However,
most of the mixup methods do not consider the varying degree of learning
difficulty in different stages of training and generate new samples with one
hot labels, resulting in the model over confidence. In this paper, we propose a
self evolution learning (SE) based mixup approach for data augmentation in text
classification, which can generate more adaptive and model friendly pesudo
samples for the model training. SE focuses on the variation of the model's
learning ability. To alleviate the model confidence, we introduce a novel
instance specific label smoothing approach, which linearly interpolates the
model's output and one hot labels of the original samples to generate new soft
for label mixing up. Through experimental analysis, in addition to improving
classification accuracy, we demonstrate that SE also enhances the model's
generalize ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13551">
<div class="article-summary-box-inner">
<span><p>Entity names play an effective role in relation extraction (RE) and often
influence model performance. As a result, the entity names in the benchmarks'
test sets significantly influence the evaluation of RE models. In this work, we
find that the standard RE benchmarks' datasets have a large portion of
incorrect entity annotations, low entity name diversity, and are prone to have
shortcuts from entity names to ground-truth relations. These issues make the
standard benchmarks far from reflecting the real-world scenarios. Hence, in
this work, we present EntRED, a challenging RE benchmark with reduced shortcuts
and higher diversity of entities. To build EntRED, we propose an end-to-end
entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs
type-constrained replacements on entities to reduce the shortcuts from entity
bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting
the instances that need entity replacements, and 2) determining the candidate
entities for replacements. We apply ERIC on TACRED to produce EntRED. Our
EntRED evaluates whether the RE model can correctly extract the relations from
the text instead of relying on entity bias. Empirical results reveal that even
the strong RE model has a significant performance drop on EntRED, which
memorizes entity name patterns instead of reasoning from the textual context.
We release ERIC's source code and the EntRED benchmark at
https://github.com/wangywUST/ENTRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings. (arXiv:2305.13571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13571">
<div class="article-summary-box-inner">
<span><p>The use of positional embeddings in transformer language models is widely
accepted. However, recent research has called into question the necessity of
such embeddings. We further extend this inquiry by demonstrating that a
randomly initialized and frozen transformer language model, devoid of
positional embeddings, inherently encodes strong positional information through
the shrinkage of self-attention variance. To quantify this variance, we derive
the underlying distribution of each step within a transformer layer. Through
empirical validation using a fully pretrained model, we show that the variance
shrinkage effect still persists after extensive gradient updates. Our findings
serve to justify the decision to discard positional embeddings and thus
facilitate more efficient pretraining of transformer language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13582">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models have enabled significant
advancements in cross-lingual transfer. However, these models often exhibit a
performance disparity when transferring from high-resource languages to
low-resource languages, especially for languages that are underrepresented or
not in the pre-training data. Motivated by the superior performance of these
models on high-resource languages compared to low-resource languages, we
introduce a Translation-and-fusion framework, which translates low-resource
language text into a high-resource language for annotation using fully
supervised models before fusing the annotations back into the low-resource
language. Based on this framework, we present TransFusion, a model trained to
fuse predictions from a high-resource language to make robust predictions on
low-resource languages. We evaluate our methods on two low-resource named
entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25
languages, and show consistent improvement up to +16 F$_1$ over English
fine-tuning systems, achieving state-of-the-art performance compared to
Translate-train systems. Our analysis depicts the unique advantages of the
TransFusion method which is robust to translation errors and source language
prediction errors, and complimentary to adapted multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13583">
<div class="article-summary-box-inner">
<span><p>Fusing multiple modalities for affective computing tasks has proven effective
for performance improvement. However, how multimodal fusion works is not well
understood, and its use in the real world usually results in large model sizes.
In this work, on sentiment and emotion analysis, we first analyze how the
salient affective information in one modality can be affected by the other in
crossmodal attention. We find that inter-modal incongruity exists at the latent
level due to crossmodal attention. Based on this finding, we propose a
lightweight model via Hierarchical Crossmodal Transformer with Modality Gating
(HCT-MG), which determines a primary modality according to its contribution to
the target task and then hierarchically incorporates auxiliary modalities to
alleviate inter-modal incongruity and reduce information redundancy. The
experimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and
IEMOCAP verifies the efficacy of our approach, showing that it: 1) outperforms
major prior work by achieving competitive results and can successfully
recognize hard samples; 2) mitigates the inter-modal incongruity at the latent
level when modalities have mismatched affective tendencies; 3) reduces model
size to less than 1M parameters while outperforming existing models of similar
sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs. (arXiv:2305.13585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13585">
<div class="article-summary-box-inner">
<span><p>Logical reasoning over incomplete knowledge graphs to answer complex logical
queries is a challenging task. With the emergence of new entities and relations
in constantly evolving KGs, inductive logical reasoning over KGs has become a
crucial problem. However, previous PLMs-based methods struggle to model the
logical structures of complex queries, which limits their ability to generalize
within the same structure. In this paper, we propose a structure-modeled
textual encoding framework for inductive logical reasoning over KGs. It encodes
linearized query structures and entities using pre-trained language models to
find answers. For structure modeling of complex queries, we design stepwise
instructions that implicitly prompt PLMs on the execution order of geometric
operations in each query. We further separately model different geometric
operations (i.e., projection, intersection, and union) on the representation
space using a pre-trained encoder with additional attention and maxout layers
to enhance structured modeling. We conduct experiments on two inductive logical
reasoning datasets and three transductive datasets. The results demonstrate the
effectiveness of our method on logical reasoning over KGs in both inductive and
transductive settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases. (arXiv:2305.13589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13589">
<div class="article-summary-box-inner">
<span><p>Toxicity annotators and content moderators often default to mental shortcuts
when making decisions. This can lead to subtle toxicity being missed, and
seemingly toxic but harmless content being over-detected. We introduce BiasX, a
framework that enhances content moderation setups with free-text explanations
of statements' implied social biases, and explore its effectiveness through a
large-scale crowdsourced user study. We show that indeed, participants
substantially benefit from explanations for correctly identifying subtly
(non-)toxic content. The quality of explanations is critical: imperfect
machine-generated explanations (+2.4% on hard toxic examples) help less
compared to expert-written human explanations (+7.2%). Our results showcase the
promise of using free-text explanations to encourage more thoughtful toxicity
moderation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13592">
<div class="article-summary-box-inner">
<span><p>Semantic understanding of programs has attracted great attention in the
community. Inspired by recent successes of large language models (LLMs) in
natural language understanding, tremendous progress has been made by treating
programming language as another sort of natural language and training LLMs on
corpora of program code. However, programs are essentially different from texts
after all, in a sense that they are normally heavily structured and
syntax-strict. In particular, programs and their basic units (i.e., functions
and subroutines) are designed to demonstrate a variety of behaviors and/or
provide possible outputs, given different inputs. The relationship between
inputs and possible outputs/behaviors represents the functions/subroutines and
profiles the program as a whole. Therefore, we propose to incorporate such a
relationship into learning, for achieving a deeper semantic understanding of
programs. To obtain inputs that are representative enough to trigger the
execution of most part of the code, we resort to fuzz testing and propose fuzz
tuning to boost the performance of program understanding and code
representation learning, given a pre-trained LLM. The effectiveness of the
proposed method is verified on two program understanding tasks including code
clone detection and code classification, and it outperforms current
state-of-the-arts by large margins. Code is available at
https://github.com/rabbitjy/FuzzTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13599">
<div class="article-summary-box-inner">
<span><p>A self-explaining rationalization model is generally constructed by a
cooperative game where a generator selects the most human-intelligible pieces
from the input text as rationales, followed by a predictor that makes
predictions based on the selected rationales. However, such a cooperative game
may incur the degeneration problem where the predictor overfits to the
uninformative pieces generated by a not yet well-trained generator and in turn,
leads the generator to converge to a sub-optimal model that tends to select
senseless pieces. In this paper, we theoretically bridge degeneration with the
predictor's Lipschitz continuity. Then, we empirically propose a simple but
effective method named DR, which can naturally and flexibly restrain the
Lipschitz constant of the predictor, to address the problem of degeneration.
The main idea of DR is to decouple the generator and predictor to allocate them
with asymmetric learning rates. A series of experiments conducted on two widely
used benchmarks have verified the effectiveness of the proposed method. Codes:
\href{https://github.com/jugechengzi/Rationalization-DR}{https://github.com/jugechengzi/Rationalization-DR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue. (arXiv:2305.13602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13602">
<div class="article-summary-box-inner">
<span><p>Incorporating visual knowledge into text-only dialogue systems has become a
potential direction to imitate the way humans think, imagine, and communicate.
However, existing multimodal dialogue systems are either confined by the scale
and quality of available datasets or the coarse concept of visual knowledge. To
address these issues, we provide a new paradigm of constructing multimodal
dialogues as well as two datasets extended from text-only dialogues under such
paradigm (ReSee-WoW, ReSee-DD). We propose to explicitly split the visual
knowledge into finer granularity (``turn-level'' and ``entity-level''). To
further boost the accuracy and diversity of augmented visual information, we
retrieve them from the Internet or a large image dataset. To demonstrate the
superiority and universality of the provided visual knowledge, we propose a
simple but effective framework ReSee to add visual representation into vanilla
dialogue models by modality concatenations. We also conduct extensive
experiments and ablations w.r.t. different model configurations and visual
knowledge settings. Empirical, encouraging results not only demonstrate the
effectiveness of introducing visual knowledge at both entity and turn level but
also verify the proposed model ReSee outperforms several state-of-the-art
methods on automatic and human evaluations. By leveraging text and vision
knowledge, ReSee can produce informative responses with real-world visual
concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation. (arXiv:2305.13614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13614">
<div class="article-summary-box-inner">
<span><p>Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13617">
<div class="article-summary-box-inner">
<span><p>Event-centric structured prediction involves predicting structured outputs of
events. In most NLP cases, event structures are complex with manifold
dependency, and it is challenging to effectively represent these complicated
structured events. To address these issues, we propose Structured Prediction
with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex
dependency among event structured components with energy-based modeling, and
represents event classes with simple but effective hyperspheres. Experiments on
two unified-annotated event datasets indicate that SPEECH is predominant in
event detection and event-relation extraction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Validating Multimedia Content Moderation Software via Semantic Fusion. (arXiv:2305.13623v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13623">
<div class="article-summary-box-inner">
<span><p>The exponential growth of social media platforms, such as Facebook and
TikTok, has revolutionized communication and content publication in human
society. Users on these platforms can publish multimedia content that delivers
information via the combination of text, audio, images, and video. Meanwhile,
the multimedia content release facility has been increasingly exploited to
propagate toxic content, such as hate speech, malicious advertisements, and
pornography. To this end, content moderation software has been widely deployed
on these platforms to detect and blocks toxic content. However, due to the
complexity of content moderation models and the difficulty of understanding
information across multiple modalities, existing content moderation software
can fail to detect toxic content, which often leads to extremely negative
impacts.
</p>
<p>We introduce Semantic Fusion, a general, effective methodology for validating
multimedia content moderation software. Our key idea is to fuse two or more
existing single-modal inputs (e.g., a textual sentence and an image) into a new
input that combines the semantics of its ancestors in a novel manner and has
toxic nature by construction. This fused input is then used for validating
multimedia content moderation software. We realized Semantic Fusion as DUO, a
practical content moderation software testing tool. In our evaluation, we
employ DUO to test five commercial content moderation software and two
state-of-the-art models against three kinds of toxic content. The results show
that DUO achieves up to 100% error finding rate (EFR) when testing moderation
software. In addition, we leverage the test cases generated by DUO to retrain
the two models we explored, which largely improves model robustness while
maintaining the accuracy on the original test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration. (arXiv:2305.13626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13626">
<div class="article-summary-box-inner">
<span><p>Conversational systems based on Large Language Models (LLMs), such as
ChatGPT, show exceptional proficiency in context understanding and response
generation. However, despite their impressive capabilities, they still possess
limitations, such as providing randomly-guessed answers to ambiguous queries or
failing to refuse users' requests, both of which are considered aspects of a
conversational agent's proactivity. This raises the question of whether
LLM-based conversational systems are equipped to handle proactive dialogue
problems. In this work, we conduct a comprehensive analysis of LLM-based
conversational systems, specifically focusing on three aspects of proactive
dialogue systems: clarification, target-guided, and non-collaborative
dialogues. To trigger the proactivity of LLMs, we propose the Proactive
Chain-of-Thought prompting scheme, which augments LLMs with the goal planning
capability over descriptive reasoning chains. Empirical findings are discussed
to promote future studies on LLM-based proactive dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13627">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned large language models (LLMs) have shown remarkable
generalization capability over multiple tasks in multiple languages.
Nevertheless, their generalization towards different languages varies
especially to underrepresented languages or even to unseen languages. Prior
works on adapting new languages to LLMs find that naively adapting new
languages to instruction-tuned LLMs will result in catastrophic forgetting,
which in turn causes the loss of multitasking ability in these LLMs. To tackle
this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables
instruction-tuned LLMs to learn cross-lingual alignment between unseen and
previously learned languages via alignment-based cross-lingual
instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$
is able to learn a new language effectively with only a limited amount of
parallel data and at the same time prevent catastrophic forgetting by applying
continual instruction-tuning through experience replay. Our work contributes to
the progression of language adaptation methods for instruction-tuned LLMs and
opens up the possibility of adapting underrepresented low-resource languages
into existing instruction-tuned LLMs. Our code will be publicly released upon
acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13628">
<div class="article-summary-box-inner">
<span><p>In cross-lingual named entity recognition (NER), self-training is commonly
used to bridge the linguistic gap by training on pseudo-labeled target-language
data. However, due to sub-optimal performance on target languages, the pseudo
labels are often noisy and limit the overall performance. In this work, we aim
to improve self-training for cross-lingual NER by combining representation
learning and pseudo label refinement in one coherent framework. Our proposed
method, namely ContProto mainly comprises two components: (1) contrastive
self-training and (2) prototype-based pseudo-labeling. Our contrastive
self-training facilitates span classification by separating clusters of
different classes, and enhances cross-lingual transferability by producing
closely-aligned representations between the source and target language.
Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of
pseudo labels during training. We evaluate ContProto on multiple transfer
pairs, and experimental results show our method brings in substantial
improvements over current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13631">
<div class="article-summary-box-inner">
<span><p>Making image retrieval methods practical for real-world search applications
requires significant progress in dataset scales, entity comprehension, and
multimodal information fusion. In this work, we introduce
\textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a
challenging dataset for cross-modal image search in the news domain. EDIS
consists of 1 million web images from actual search engine results and curated
datasets, with each image paired with a textual description. Unlike datasets
that assume a small set of single-modality candidates, EDIS reflects real-world
web image search scenarios by including a million multimodal image-text pairs
as candidates. EDIS encourages the development of retrieval models that
simultaneously address cross-modal information fusion and matching. To achieve
accurate ranking results, a model must: 1) understand named entities and events
from text queries, 2) ground entities onto images or text descriptions, and 3)
effectively fuse textual and visual representations. Our experimental results
show that EDIS challenges state-of-the-art methods with dense entities and a
large-scale candidate set. The ablation study also proves that fusing textual
features with visual features is critical in improving retrieval results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13632">
<div class="article-summary-box-inner">
<span><p>Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IdEALS: Idiomatic Expressions for Advancement of Language Skills. (arXiv:2305.13637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13637">
<div class="article-summary-box-inner">
<span><p>Although significant progress has been made in developing methods for
Grammatical Error Correction (GEC), addressing word choice improvements has
been notably lacking and enhancing sentence expressivity by replacing phrases
with advanced expressions is an understudied aspect. In this paper, we focus on
this area and present our investigation into the task of incorporating the
usage of idiomatic expressions in student writing. To facilitate our study, we
curate extensive training sets and expert-annotated testing sets using
real-world data and evaluate various approaches and compare their performance
against human experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese. (arXiv:2305.13641v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13641">
<div class="article-summary-box-inner">
<span><p>Despite their successes in NLP, Transformer-based language models still
require extensive computing resources and suffer in low-resource or low-compute
settings. In this paper, we present AxomiyaBERTa, a novel BERT model for
Assamese, a morphologically-rich low-resource language (LRL) of Eastern India.
AxomiyaBERTa is trained only on the masked language modeling (MLM) task,
without the typical additional next sentence prediction (NSP) objective, and
our results show that in resource-scarce settings for very low-resource
languages like Assamese, MLM alone can be successfully leveraged for a range of
tasks. AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity
Recognition and also performs well on "longer-context" tasks like Cloze-style
QA and Wiki Title Prediction, with the assistance of a novel embedding
disperser and phonological signals respectively. Moreover, we show that
AxomiyaBERTa can leverage phonological signals for even more challenging tasks,
such as a novel cross-document coreference task on a translated version of the
ECB+ corpus, where we present a new SOTA result for an LRL. Our source code and
evaluation scripts may be found at https://github.com/csu-signal/axomiyaberta.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mPMR: A Multilingual Pre-trained Machine Reader at Scale. (arXiv:2305.13645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13645">
<div class="article-summary-box-inner">
<span><p>We present multilingual Pre-trained Machine Reader (mPMR), a novel method for
multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims
to guide multilingual pre-trained language models (mPLMs) to perform natural
language understanding (NLU) including both sequence classification and span
extraction in multiple languages. To achieve cross-lingual generalization when
only source-language fine-tuning data is available, existing mPLMs solely
transfer NLU capability from a source language to target languages. In
contrast, mPMR allows the direct inheritance of multilingual NLU capability
from the MRC-style pre-training to downstream tasks. Therefore, mPMR acquires
better NLU capability for target languages. mPMR also provides a unified solver
for tackling cross-lingual span extraction and sequence classification, thereby
enabling the extraction of rationales to explain the sentence-pair
classification process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation. (arXiv:2305.13648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13648">
<div class="article-summary-box-inner">
<span><p>Non-parametric, k-nearest-neighbor algorithms have recently made inroads to
assist generative models such as language models and machine translation
decoders. We explore whether such non-parametric models can improve machine
translation models at the fine-tuning stage by incorporating statistics from
the kNN predictions to inform the gradient updates for a baseline translation
model. There are multiple methods which could be used to incorporate kNN
statistics and we investigate gradient scaling by a gating mechanism, the kNN's
ground truth probability, and reinforcement learning. For four standard
in-domain machine translation datasets, compared with classic fine-tuning, we
report consistent improvements of all of the three methods by as much as 1.45
BLEU and 1.28 BLEU for German-English and English-German translations
respectively. Through qualitative analysis, we found particular improvements
when it comes to translating grammatical relations or function words, which
results in increased fluency of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Knowledge Transfer and Iterative Pseudo-labeling for Low-Resource Speech Recognition with Transducers. (arXiv:2305.13652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13652">
<div class="article-summary-box-inner">
<span><p>Voice technology has become ubiquitous recently. However, the accuracy, and
hence experience, in different languages varies significantly, which makes the
technology not equally inclusive. The availability of data for different
languages is one of the key factors affecting accuracy, especially in training
of all-neural end-to-end automatic speech recognition systems.
</p>
<p>Cross-lingual knowledge transfer and iterative pseudo-labeling are two
techniques that have been shown to be successful for improving the accuracy of
ASR systems, in particular for low-resource languages, like Ukrainian.
</p>
<p>Our goal is to train an all-neural Transducer-based ASR system to replace a
DNN-HMM hybrid system with no manually annotated training data. We show that
the Transducer system trained using transcripts produced by the hybrid system
achieves 18% reduction in terms of word error rate. However, using a
combination of cross-lingual knowledge transfer from related languages and
iterative pseudo-labeling, we are able to achieve 35% reduction of the error
rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13654">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that deep learning models are prone to exploit spurious
correlations that are present in the training set, yet may not hold true in
general. A sentiment classifier may erroneously learn that the token spielberg
is always tied to positive movie reviews. Relying on spurious correlations may
lead to significant degradation in generalizability and should be avoided. In
this paper, we propose a neighborhood analysis framework to explain how exactly
language models exploit spurious correlations. Driven by the analysis, we
propose a family of regularization methods, NFL (do Not Forget your Language)
to prevent the situation. Experiments on two text classification tasks show
that NFL brings a significant improvement over standard fine-tuning in terms of
robustness without sacrificing in-distribution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT as your Personal Data Scientist. (arXiv:2305.13657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13657">
<div class="article-summary-box-inner">
<span><p>The rise of big data has amplified the need for efficient, user-friendly
automated machine learning (AutoML) tools. However, the intricacy of
understanding domain-specific data and defining prediction tasks necessitates
human intervention making the process time-consuming while preventing full
automation. Instead, envision an intelligent agent capable of assisting users
in conducting AutoML tasks through intuitive, natural conversations without
requiring in-depth knowledge of the underlying machine learning (ML) processes.
This agent's key challenge is to accurately comprehend the user's prediction
goals and, consequently, formulate precise ML tasks, adjust data sets and model
parameters accordingly, and articulate results effectively. In this paper, we
take a pioneering step towards this ambitious goal by introducing a
ChatGPT-based conversational data-science framework to act as a "personal data
scientist". Precisely, we utilize Large Language Models (ChatGPT) to build a
natural interface between the users and the ML models (Scikit-Learn), which in
turn, allows us to approach this ambitious problem with a realistic solution.
</p>
<p>Our model pivots around four dialogue states: Data Visualization, Task
Formulation, Prediction Engineering, and Result Summary and Recommendation.
Each state marks a unique conversation phase, impacting the overall user-system
interaction. Multiple LLM instances, serving as "micro-agents", ensure a
cohesive conversation flow, granting us granular control over the
conversation's progression. In summary, we developed an end-to-end system that
not only proves the viability of the novel concept of conversational data
science but also underscores the potency of LLMs in solving complex tasks.
Interestingly, its development spotlighted several critical weaknesses in the
current LLMs (ChatGPT) and highlighted substantial opportunities for
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding compositional data augmentation in automatic morphological inflection. (arXiv:2305.13658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13658">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques are widely used in low-resource automatic
morphological inflection to address the issue of data sparsity. However, the
full implications of these techniques remain poorly understood. In this study,
we aim to shed light on the theoretical aspects of the data augmentation
strategy StemCorrupt, a method that generates synthetic examples by randomly
substituting stem characters in existing gold standard training examples. Our
analysis uncovers that StemCorrupt brings about fundamental changes in the
underlying data distribution, revealing inherent compositional concatenative
structure. To complement our theoretical analysis, we investigate the
data-efficiency of StemCorrupt. Through evaluation across a diverse set of
seven typologically distinct languages, we demonstrate that selecting a subset
of datapoints with both high diversity and high predictive uncertainty
significantly enhances the data-efficiency of StemCorrupt compared to
competitive baselines. Furthermore, we explore the impact of typological
features on the choice of augmentation strategy and find that languages
incorporating non-concatenativity, such as morphonological alternations, derive
less benefit from synthetic examples with high predictive uncertainty. We
attribute this effect to phonotactic violations induced by StemCorrupt,
emphasizing the need for further research to ensure optimal performance across
the entire spectrum of natural language morphology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning. (arXiv:2305.13660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13660">
<div class="article-summary-box-inner">
<span><p>Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often require
abundant annotated data, which creates challenges when faced with noisy
annotations or low-resource settings. We introduce GDP-Zero, an approach using
Open-Loop MCTS to perform goal-oriented dialogue policy planning without any
model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13661">
<div class="article-summary-box-inner">
<span><p>In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Non-Autoregressive Transformers with Contrastive Learning. (arXiv:2305.13667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13667">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive Transformers (NATs) reduce the inference latency of
Autoregressive Transformers (ATs) by predicting words all at once rather than
in sequential order. They have achieved remarkable progress in machine
translation as well as many other applications. However, a long-standing
challenge for NATs is the learning of multi-modality data distribution, which
is the main cause of the performance gap between NATs and ATs. In this paper,
we propose to ease the difficulty of modality learning via sampling from the
model distribution instead of the data distribution. We derive contrastive
constraints to stabilize the training process and integrate this resulting
objective with the state-of-the-art NAT architecture DA-Transformer. Our model
\method is examined on 3 different tasks, including machine translation, text
summarization, and paraphrasing with 5 benchmarks. Results show that our
approach outperforms previous non-autoregressive baselines by a significant
margin and establishes new state-of-the-art results for non-autoregressive
transformers on all the benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations. (arXiv:2305.13668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13668">
<div class="article-summary-box-inner">
<span><p>We present a novel method for using agent experiences gathered through an
embodied simulation to ground contextualized word vectors to object
representations. We use similarity learning to make comparisons between
different object types based on their properties when interacted with, and to
extract common features pertaining to the objects' behavior. We then use an
affine transformation to calculate a projection matrix that transforms
contextualized word vectors from different transformer-based language models
into this learned space, and evaluate whether new test instances of transformed
token vectors identify the correct concept in the object embedding space. Our
results expose properties of the embedding spaces of four different transformer
models and show that grounding object token vectors is usually more helpful to
grounding verb and attribute token vectors than the reverse, which reflects
earlier conclusions in the analogical reasoning and psycholinguistic
literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13669">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable recent advances in language models, they still
struggle with the hallucination problem and can generate misleading and
unsupported responses. A common approach to mitigate the hallucination issue is
retrieving and incorporating supporting evidence from a knowledge base.
However, user questions usually do not align well with the stored knowledge, as
they are unaware of the information available before asking questions. This
misalignment can limit the language model's ability to locate and utilize the
knowledge, potentially forcing it to hallucinate by ignoring or overriding the
retrieved evidence. To address this issue, we introduce MixAlign, a framework
that interacts with both the user and the knowledge base to obtain and
integrate clarifications on how the user question relates to the stored
information. MixAlign employs a language model to achieve automatic
question-knowledge alignment and, if necessary, further enhances this alignment
through human user clarifications. Experimental results demonstrate significant
improvements over state-of-the-art methods, showcasing the effectiveness of
MixAlign in mitigating language model hallucination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13673">
<div class="article-summary-box-inner">
<span><p>We design experiments to study $\textit{how}$ generative language models,
like GPT, learn context-free grammars (CFGs) -- diverse language systems with a
tree-like structure capturing many aspects of natural languages, programs, and
human logics. CFGs are as hard as pushdown automata, and can be ambiguous so
that verifying if a string satisfies the rules requires dynamic programming. We
construct synthetic data and demonstrate that even for very challenging CFGs,
pre-trained transformers can learn to generate sentences with near-perfect
accuracy and remarkable $\textit{diversity}$.
</p>
<p>More importantly, we delve into the $\textit{physical principles}$ behind how
transformers learns CFGs. We discover that the hidden states within the
transformer implicitly and $\textit{precisely}$ encode the CFG structure (such
as putting tree node information exactly on the subtree boundary), and learn to
form "boundary to boundary" attentions that resemble dynamic programming. We
also cover some extension of CFGs as well as the robustness aspect of
transformers against grammar mistakes. Overall, our research provides a
comprehensive and empirical understanding of how transformers learn CFGs, and
reveals the physical mechanisms utilized by transformers to capture the
structure and rules of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models. (arXiv:2305.13675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13675">
<div class="article-summary-box-inner">
<span><p>In this work, we evaluate the capacity for foundation models to retrieve
encyclopedic knowledge across a wide range of languages, topics, and contexts.
To support this effort, we 1) produce a new dataset containing 303k factual
associations in 20 different languages, 2) formulate a new counterfactual
knowledge assessment, Polyglot or Not, and 3) benchmark 5 foundation models in
a multilingual setting and a diverse set of 20 models in an English-only
setting. We observed significant accuracy differences in models of interest,
with Meta's LLaMA topping both the multilingual and English-only assessments.
Error analysis reveals a significant deficiency in LLaMA's ability to retrieve
facts in languages written in the Cyrillic script and gaps in its understanding
of facts based on the location and gender of entailed subjects. Ultimately, we
argue that the promise of utilizing foundation language models as bonafide
polyglots is greatly diminished when they are tasked with retrieving
information in languages other than English. Supporting code
(https://github.com/daniel-furman/Polyglot-or-Not) and dataset
(https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion) are openly
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Legally Enforceable Hate Speech Detection for Public Forums. (arXiv:2305.13677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13677">
<div class="article-summary-box-inner">
<span><p>Hate speech is a serious issue on public forums, and proper enforcement of
hate speech laws is key for protecting groups of people against harmful and
discriminatory language. However, determining what constitutes hate speech is a
complex task that is highly open to subjective interpretations. Existing works
do not align their systems with enforceable definitions of hate speech, which
can make their outputs inconsistent with the goals of regulators. Our work
introduces a new task for enforceable hate speech detection centred around
legal definitions, and a dataset annotated on violations of eleven possible
definitions by legal experts. Given the challenge of identifying clear, legally
enforceable instances of hate speech, we augment the dataset with
expert-generated samples and an automatically mined challenge set. We
experiment with grounding the model decision in these definitions using
zero-shot and few-shot prompting. We then report results on several large
language models (LLMs). With this task definition, automatic hate speech
detection can be more closely aligned to enforceable laws, and hence assist in
more rigorous enforcement of legal protections against harmful speech in public
forums.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error Detection for Text-to-SQL Semantic Parsing. (arXiv:2305.13683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13683">
<div class="article-summary-box-inner">
<span><p>Despite remarkable progress in text-to-SQL semantic parsing in recent years,
the performance of existing parsers is still far from perfect. At the same
time, modern deep learning based text-to-SQL parsers are often over-confident
and thus casting doubt on their trustworthiness when deployed for real use. To
that end, we propose to build a parser-independent error detection model for
text-to-SQL semantic parsing. The proposed model is based on pre-trained
language model of code and is enhanced with structural features learned by
graph neural networks. We train our model on realistic parsing errors collected
from a cross-domain setting. Experiments with three strong text-to-SQL parsers
featuring different decoding mechanisms show that our approach outperforms
parser-dependent uncertainty metrics and could effectively improve the
performance and usability of text-to-SQL semantic parsers regardless of their
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13684">
<div class="article-summary-box-inner">
<span><p>Recent multilingual pretrained language models (mPLMs) have been shown to
encode strong language-specific signals, which are not explicitly provided
during pretraining. It remains an open question whether it is feasible to
employ mPLMs to measure language similarity, and subsequently use the
similarity results to select source languages for boosting cross-lingual
transfer. To investigate this, we propose mPLM-Sim, a new language similarity
measure that induces the similarities across languages from mPLMs using
multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high
correlations with linguistic similarity measures, such as lexicostatistics,
genealogical language family, and geographical sprachbund. We also conduct a
case study on languages with low correlation and observe that mPLM-Sim yields
more accurate similarity results. Additionally, we find that similarity results
vary across different mPLMs and different layers within an mPLM. We further
investigate whether mPLM-Sim is effective for zero-shot cross-lingual transfer
by conducting experiments on both low-level syntactic tasks and high-level
semantic tasks. The experimental results demonstrate that mPLM-Sim is capable
of selecting better source languages than linguistic measures, resulting in a
1%-2% improvement in zero-shot cross-lingual transfer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Intervention for Abstractive Related Work Generation. (arXiv:2305.13685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13685">
<div class="article-summary-box-inner">
<span><p>Abstractive related work generation has attracted increasing attention in
generating coherent related work that better helps readers grasp the background
in the current research. However, most existing abstractive models ignore the
inherent causality of related work generation, leading to low quality of
generated related work and spurious correlations that affect the models'
generalizability. In this study, we argue that causal intervention can address
these limitations and improve the quality and coherence of the generated
related works. To this end, we propose a novel Causal Intervention Module for
Related Work Generation (CaM) to effectively capture causalities in the
generation process and improve the quality and coherence of the generated
related works. Specifically, we first model the relations among sentence order,
document relation, and transitional content in related work generation using a
causal graph. Then, to implement the causal intervention and mitigate the
negative impact of spurious correlations, we use do-calculus to derive ordinary
conditional probabilities and identify causal effects through CaM. Finally, we
subtly fuse CaM with Transformer to obtain an end-to-end generation model.
Extensive experiments on two real-world datasets show that causal interventions
in CaM can effectively promote the model to learn causal relations and produce
related work of higher quality and coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Asking Clarification Questions for Information Seeking on Task-Oriented Dialogues. (arXiv:2305.13690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13690">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems aim at providing users with task-specific
services. Users of such systems often do not know all the information about the
task they are trying to accomplish, requiring them to seek information about
the task. To provide accurate and personalized task-oriented information
seeking results, task-oriented dialogue systems need to address two potential
issues: 1) users' inability to describe their complex information needs in
their requests; and 2) ambiguous/missing information the system has about the
users. In this paper, we propose a new Multi-Attention Seq2Seq Network, named
MAS2S, which can ask questions to clarify the user's information needs and the
user's profile in task-oriented information seeking. We also extend an existing
dataset for task-oriented information seeking, leading to the \ourdataset which
contains about 100k task-oriented information seeking dialogues that are made
publicly available\footnote{Dataset and code is available at
\href{https://github.com/sweetalyssum/clarit}{https://github.com/sweetalyssum/clarit}.}.
Experimental results on \ourdataset show that MAS2S outperforms baselines on
both clarification question generation and answer prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis. (arXiv:2305.13691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13691">
<div class="article-summary-box-inner">
<span><p>Few-shot learning for open domain multi-hop question answering typically
relies on large language models (LLMs). While powerful, LLMs are inefficient at
the inference time. We propose a data synthesis framework for multi-hop
question answering that allows for improving smaller language models with less
than 10 human-annotated question answer pairs. The framework is built upon the
data generation functions parameterized by LLMs and prompts, which requires
minimal hand-crafted features. Empirically, we synthesize millions of multi-hop
questions and claims. After finetuning language models on the synthetic data,
we evaluate the models on popular benchmarks on multi-hop question answering
and fact verification. Our experimental results show that finetuning on the
synthetic data improves model performance significantly, allowing our finetuned
models to be competitive with prior models while being almost one-third the
size in terms of parameter counts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations. (arXiv:2305.13693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13693">
<div class="article-summary-box-inner">
<span><p>Evaluating multi-document summarization (MDS) quality is difficult. This is
especially true in the case of MDS for biomedical literature reviews, where
models must synthesize contradicting evidence reported across different
documents. Prior work has shown that rather than performing the task, models
may exploit shortcuts that are difficult to detect using standard n-gram
similarity metrics such as ROUGE. Better automated evaluation metrics are
needed, but few resources exist to assess metrics when they are proposed.
Therefore, we introduce a dataset of human-assessed summary quality facets and
pairwise preferences to encourage and support the development of better
automated evaluation methods for literature review MDS. We take advantage of
community submissions to the Multi-document Summarization for Literature Review
(MSLR) shared task to compile a diverse and representative sample of generated
summaries. We analyze how automated summarization evaluation metrics correlate
with lexical features of generated summaries, to other automated metrics
including several we propose in this work, and to aspects of human-assessed
summary quality. We find that not only do automated metrics fail to capture
aspects of quality as assessed by humans, in many cases the system rankings
produced by these metrics are anti-correlated with rankings according to human
annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstractive Text Summarization Using the BRIO Training Paradigm. (arXiv:2305.13696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13696">
<div class="article-summary-box-inner">
<span><p>Summary sentences produced by abstractive summarization models may be
coherent and comprehensive, but they lack control and rely heavily on reference
summaries. The BRIO training paradigm assumes a non-deterministic distribution
to reduce the model's dependence on reference summaries, and improve model
performance during inference. This paper presents a straightforward but
effective technique to improve abstractive summaries by fine-tuning pre-trained
language models, and training them with the BRIO paradigm. We build a text
summarization dataset for Vietnamese, called VieSum. We perform experiments
with abstractive summarization models trained with the BRIO paradigm on the
CNNDM and the VieSum datasets. The results show that the models, trained on
basic hardware, outperform all existing abstractive summarization models,
especially for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning. (arXiv:2305.13697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13697">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (VL) pre-training, which aims to learn a general
representation of image-text pairs that can be transferred to various
vision-and-language tasks. Compared with modeling uni-modal data, the main
challenge of the VL model is: how to learn the cross-modal interaction from
multimodal data, especially the fine-grained interaction. Existing works have
shown that fully transformer-based models that adopt attention mechanisms to
learn in-layer cross-model interaction can demonstrate impressive performance
on various cross-modal downstream tasks. However, they ignored that the
semantic information of the different modals at the same layer was not uniform,
which leads to the cross-modal interaction collapsing into a limited
multi-modal semantic information interaction. In this work, we propose the
UNIMO-3 model, which has the capacity to simultaneously learn the multimodal
in-layer interaction and cross-layer interaction. UNIMO-3 model can establish
effective connections between different layers in a cross-modal encoder, and
adaptively capture the interaction between two modalities at different levels.
The experimental results show that our model achieves state-of-the-art
performance in various downstream tasks, and through ablation study can prove
that effective cross-layer learning improves the model's ability of multimodal
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Large Language Models for Classical Philology. (arXiv:2305.13698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13698">
<div class="article-summary-box-inner">
<span><p>Recent advances in NLP have led to the creation of powerful language models
for many languages including Ancient Greek and Latin. While prior work on
Classical languages unanimously uses BERT, in this work we create four language
models for Ancient Greek that vary along two dimensions to study their
versatility for tasks of interest for Classical languages: we explore (i)
encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong
model types, and create for each of them (ii) a monolingual Ancient Greek and a
multilingual instance that includes Latin and English. We evaluate all models
on morphological and syntactic tasks, including lemmatization, which
demonstrates the added value of T5's decoding abilities. We further define two
probing tasks to investigate the knowledge acquired by models pre-trained on
Classical texts. Our experiments provide the first benchmarking analysis of
existing models of Ancient Greek. Results show that our models provide
significant improvements over the SoTA. The systematic analysis of model types
can inform future research in designing language models for Classical
languages, including the development of novel generative tasks. We make all our
models available as community resources, along with a large curated
pre-training corpus for Ancient Greek, to support the creation of a larger,
comparable model zoo for Classical Philology. Our models and resources are
available at https://github.com/Heidelberg-NLP/ancient-language-models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemeCap: A Dataset for Captioning and Interpreting Memes. (arXiv:2305.13703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13703">
<div class="article-summary-box-inner">
<span><p>Memes are a widely popular tool for web users to express their thoughts using
visual metaphors. Understanding memes requires recognizing and interpreting
visual metaphors with respect to the text inside or around the meme, often
while employing background knowledge and reasoning abilities. We present the
task of meme captioning and release a new dataset, MemeCap. Our dataset
contains 6.3K memes along with the title of the post containing the meme, the
meme captions, the literal image caption, and the visual metaphors. Despite the
recent success of vision and language (VL) models on tasks such as image
captioning and visual question answering, our extensive experiments using
state-of-the-art VL models show that they still struggle with visual metaphors,
and perform substantially worse than humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models. (arXiv:2305.13707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13707">
<div class="article-summary-box-inner">
<span><p>Language models have graduated from being research prototypes to
commercialized products offered as web APIs, and recent works have highlighted
the multilingual capabilities of these products. The API vendors charge their
users based on usage, more specifically on the number of ``tokens'' processed
or generated by the underlying language models. What constitutes a token,
however, is training data and model dependent with a large variance in the
number of tokens required to convey the same information in different
languages. In this work, we analyze the effect of this non-uniformity on the
fairness of an API's pricing policy across languages. We conduct a systematic
analysis of the cost and utility of OpenAI's language model API on multilingual
benchmarks in 22 typologically diverse languages. We show evidence that
speakers of a large number of the supported languages are overcharged while
obtaining poorer results. These speakers tend to also come from regions where
the APIs are less affordable to begin with. Through these analyses, we aim to
increase transparency around language model APIs' pricing policies and
encourage the vendors to make them more equitable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems. (arXiv:2305.13710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13710">
<div class="article-summary-box-inner">
<span><p>Traditional end-to-end task-oriented dialogue systems have been built with a
modularized design. However, such design often causes misalignment between the
agent response and external knowledge, due to inadequate representation of
information. Furthermore, its evaluation metrics emphasize assessing the
agent's pre-lexicalization response, neglecting the quality of the completed
response. In this work, we propose a novel paradigm that uses a textual
interface to align external knowledge and eliminate redundant processes. We
demonstrate our paradigm in practice through MultiWOZ-Remake, including an
interactive textual interface built for the MultiWOZ database and a
correspondingly re-processed dataset. We train an end-to-end dialogue system to
evaluate this new dataset. The experimental results show that our approach
generates more natural final responses and achieves a greater task success rate
compared to the previous models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. (arXiv:2305.13711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13711">
<div class="article-summary-box-inner">
<span><p>We propose LLM-Eval, a unified multi-dimensional automatic evaluation method
for open-domain conversations with large language models (LLMs). Existing
evaluation methods often rely on human annotations, ground-truth responses, or
multiple LLM prompts, which can be expensive and time-consuming. To address
these issues, we design a single prompt-based evaluation method that leverages
a unified evaluation schema to cover multiple dimensions of conversation
quality in a single model call. We extensively evaluate the performance of
LLM-Eval on various benchmark datasets, demonstrating its effectiveness,
efficiency, and adaptability compared to state-of-the-art evaluation methods.
Our analysis also highlights the importance of choosing suitable LLMs and
decoding strategies for accurate evaluation results. LLM-Eval offers a
versatile and robust solution for evaluating open-domain conversation systems,
streamlining the evaluation process and providing consistent performance across
diverse scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13712">
<div class="article-summary-box-inner">
<span><p>This paper investigates the capabilities of Large Language Models (LLMs) in
the context of understanding their own knowledge and measuring their
uncertainty. We argue this is an important feature for mitigating
hallucinations. Specifically, we focus on addressing \textit{known-unknown}
questions, characterized by high uncertainty due to the absence of definitive
answers. To facilitate our study, we collect a dataset with new Known-Unknown
Questions (KUQ) and propose a novel categorization scheme to elucidate the
sources of uncertainty. Subsequently, we assess the LLMs' ability to
differentiate between known and unknown questions and classify them
accordingly. Moreover, we evaluate the quality of their answers in an
Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we
create a semantic evaluation method that measures the model's accuracy in
expressing uncertainty between known vs unknown questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center. (arXiv:2305.13713v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13713">
<div class="article-summary-box-inner">
<span><p>We present CALLS, a Japanese speech corpus that considers phone calls in a
customer center as a new domain of empathetic spoken dialogue. The existing
STUDIES corpus covers only empathetic dialogue between a teacher and student in
a school. To extend the application range of empathetic dialogue speech
synthesis (EDSS), we designed our corpus to include the same female speaker as
the STUDIES teacher, acting as an operator in simulated phone calls. We
describe a corpus construction methodology and analyze the recorded speech. We
also conduct EDSS experiments using the CALLS and STUDIES corpora to
investigate the effect of domain differences. The results show that mixing the
two corpora during training causes biased improvements in the quality of
synthetic speech due to the different degrees of expressiveness. Our project
page of the corpus is <a href="http://sython.org/Corpus/STUDIES-2.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13716">
<div class="article-summary-box-inner">
<span><p>The recently proposed serialized output training (SOT) simplifies
multi-talker automatic speech recognition (ASR) by generating speaker
transcriptions separated by a special token. However, frequent speaker changes
can make speaker change prediction difficult. To address this, we propose
boundary-aware serialized output training (BA-SOT), which explicitly
incorporates boundary knowledge into the decoder via a speaker change detection
task and boundary constraint loss. We also introduce a two-stage connectionist
temporal classification (CTC) strategy that incorporates token-level SOT CTC to
restore temporal context information. Besides typical character error rate
(CER), we introduce utterance-dependent character error rate (UD-CER) to
further measure the precision of speaker change prediction. Compared to
original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a
pre-trained ASR model for BA-SOT model initialization further reduces
CER/UD-CER by 8.4%/19.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13718">
<div class="article-summary-box-inner">
<span><p>Existing efforts to improve logical reasoning ability of language models have
predominantly relied on supervised fine-tuning, hindering generalization to new
domains and/or tasks. The development of Large Langauge Models (LLMs) has
demonstrated the capacity of compressing abundant knowledge into a single
proxy, enabling them to tackle multiple tasks effectively. Our preliminary
experiments, nevertheless, show that LLMs do not show capability on logical
reasoning. The performance of LLMs on logical reasoning benchmarks is far
behind the existing state-of-the-art baselines. In this paper, we make the
first attempt to investigate the feasibility of incorporating logical knowledge
through self-supervised post-training, and activating it via in-context
learning, which we termed as LogicLLM. Specifically, we devise an
auto-regressive objective variant of MERIt and integrate it with two LLM
series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to
13 billion. The results on two challenging logical reasoning benchmarks
demonstrate the effectiveness of LogicLLM. Besides, we conduct extensive
ablation studies to analyze the key factors in designing logic-oriented proxy
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13721">
<div class="article-summary-box-inner">
<span><p>Dialogue systems are frequently updated to accommodate new services, but
naively updating them by continually training with data for new services in
diminishing performance on previously learnt services. Motivated by the insight
that dialogue state tracking (DST), a crucial component of dialogue systems
that estimates the user's goal as a conversation proceeds, is a simple natural
language understanding task, we propose reformulating it as a bundle of
granular example-guided question answering tasks to minimize the task shift
between services and thus benefit continual learning. Our approach alleviates
service-specific memorization and teaches a model to contextualize the given
question and example to extract the necessary information from the
conversation. We find that a model with just 60M parameters can achieve a
significant boost by learning to learn from in-context examples retrieved by a
retriever trained to identify turns with similar dialogue state changes.
Combining our method with dialogue-level memory replay, our approach attains
state of the art performance on DST continual learning metrics without relying
on any complex regularization or parameter expansion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training. (arXiv:2305.13723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13723">
<div class="article-summary-box-inner">
<span><p>Recently proposed weakly-supervised text classification settings train a
classifier using the label name of each target class as the only supervision.
Such weakly-supervised settings have been gaining increasing attention since
they can largely reduce human annotation efforts compared to fully-supervised
and semi-supervised settings. Most existing methods follow the strategy that
first uses the label names as static features to generate pseudo labels, which
are then used for classifier training. While reasonable, such a commonly
adopted framework suffers from two limitations: (1) words can have different
meanings in different contexts, so using label names for context-free matching
can induce very noisy pseudo labels; and (2) the errors made in the pseudo
label generation stage will directly propagate to the classifier training stage
without a chance of being corrected. In this paper, we propose a new method,
PromptClass, consisting of two modules: (1) a pseudo label acquisition module
that uses zero-shot prompting of pre-trained language models (PLM) to get
pseudo labels based on contextualized text understanding, and (2) a
noise-robust self-training module that iteratively trains the classifier and
updates pseudo labels by utilizing two PLM fine-tuning strategies that
regularize each other. Extensive experiments show that PromptClass achieves
overall better performance than existing strong baselines on four benchmark
datasets and even achieves similar performance to fully-supervised classifiers
on sentiment classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings. (arXiv:2305.13724v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13724">
<div class="article-summary-box-inner">
<span><p>We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS)
method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that
can deeply understand the content and purpose of an input prompt and
appropriately respond to the user's request. We focus on ChatGPT's reading
comprehension and introduce it to EDSS, a task of synthesizing speech that can
empathize with the interlocutor's emotion. Our method first gives chat history
to ChatGPT and asks it to generate three words representing the intention,
emotion, and speaking style for each line in the chat. Then, it trains an EDSS
model using the embeddings of ChatGPT-derived context words as the conditioning
features. The experimental results demonstrate that our method performs
comparably to ones using emotion labels or neural network-derived context
embeddings learned from chat histories. The collected ChatGPT-derived context
information is available at
https://sarulab-speech.github.io/demo_ChatGPT_EDSS/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Recommendation as Retrieval: A Simple, Strong Baseline. (arXiv:2305.13725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13725">
<div class="article-summary-box-inner">
<span><p>Conversational recommendation systems (CRS) aim to recommend suitable items
to users through natural language conversation. However, most CRS approaches do
not effectively utilize the signal provided by these conversations. They rely
heavily on explicit external knowledge e.g., knowledge graphs to augment the
models' understanding of the items and attributes, which is quite hard to
scale. To alleviate this, we propose an alternative information retrieval
(IR)-styled approach to the CRS item recommendation task, where we represent
conversations as queries and items as documents to be retrieved. We expand the
document representation used for retrieval with conversations from the training
set. With a simple BM25-based retriever, we show that our task formulation
compares favorably with much more complex baselines using complex external
knowledge on a popular CRS benchmark. We demonstrate further improvements using
user-centric modeling and data augmentation to counter the cold start problem
for CRSs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker. (arXiv:2305.13729v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13729">
<div class="article-summary-box-inner">
<span><p>Re-rankers, which order retrieved documents with respect to the relevance
score on the given query, have gained attention for the information retrieval
(IR) task. Rather than fine-tuning the pre-trained language model (PLM), the
large-scale language model (LLM) is utilized as a zero-shot re-ranker with
excellent results. While LLM is highly dependent on the prompts, the impact and
the optimization of the prompts for the zero-shot re-ranker are not explored
yet. Along with highlighting the impact of optimization on the zero-shot
re-ranker, we propose a novel discrete prompt optimization method, Constrained
Prompt generation (Co-Prompt), with the metric estimating the optimum for
re-ranking. Co-Prompt guides the generated texts from PLM toward optimal
prompts based on the metric without parameter update. The experimental results
demonstrate that Co-Prompt leads to outstanding re-ranking performance against
the baselines. Also, Co-Prompt generates more interpretable prompts for humans
against other prompt optimization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13733">
<div class="article-summary-box-inner">
<span><p>Numerous works are proposed to improve or evaluate the capabilities of Large
language models (LLMs) to fulfill user instructions. However, they neglect the
possibility that user inputs may inherently contain incorrect information due
to users' false beliefs or malicious intents. In this way, blindly adhering to
users' false content will cause deception and harm. To address this problem, we
propose a challenging benchmark consisting of Inductive Instructions (INDust)
to evaluate whether LLMs could resist these instructions. The INDust includes
15K instructions across three categories: Fact-Checking Instructions, Questions
based on False Premises, and Creative Instructions based on False Premises. Our
experiments on several strong LLMs reveal that current LLMs can be easily
deceived by INDust into generating misleading and malicious statements. Hence
we employ Self-Critique prompting to encourage LLMs to not only critique
themselves like in previous works but also the users, which show remarkable
improvement in handling inductive instructions under both zero-shot and
few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13735">
<div class="article-summary-box-inner">
<span><p>Aligning large language models (LLMs) to human values has become increasingly
important as it enables sophisticated steering of LLMs, e.g., making them
follow given instructions while keeping them less toxic. However, it requires a
significant amount of human demonstrations and feedback. Recently, open-sourced
models have attempted to replicate the alignment learning process by distilling
data from already aligned LLMs like InstructGPT or ChatGPT. While this process
reduces human efforts, constructing these datasets has a heavy dependency on
the teacher models. In this work, we propose a novel framework for alignment
learning with almost no human labor and no dependency on pre-aligned LLMs.
First, we perform reward modeling (RM) with synthetic feedback by contrasting
responses from vanilla LLMs with various sizes and prompts. Then, we use the RM
for simulating high-quality demonstrations to train a supervised policy and for
further optimizing the model with reinforcement learning. Our resulting model,
Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms
open-sourced models, including Alpaca, Dolly, and OpenAssistant, which are
trained on the outputs of InstructGPT or human-annotated instructions. Our
7B-sized model outperforms the 12-13B models in the A/B tests using GPT-4 as
the judge with about 75% winning rate on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">i-Code Studio: A Configurable and Composable Framework for Integrative AI. (arXiv:2305.13738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13738">
<div class="article-summary-box-inner">
<span><p>Artificial General Intelligence (AGI) requires comprehensive understanding
and generation capabilities for a variety of tasks spanning different
modalities and functionalities. Integrative AI is one important direction to
approach AGI, through combining multiple models to tackle complex multimodal
tasks. However, there is a lack of a flexible and composable platform to
facilitate efficient and effective model composition and coordination. In this
paper, we propose the i-Code Studio, a configurable and composable framework
for Integrative AI. The i-Code Studio orchestrates multiple pre-trained models
in a finetuning-free fashion to conduct complex multimodal tasks. Instead of
simple model composition, the i-Code Studio provides an integrative, flexible,
and composable setting for developers to quickly and easily compose
cutting-edge services and technologies tailored to their specific requirements.
The i-Code Studio achieves impressive results on a variety of zero-shot
multimodal tasks, such as video-to-text retrieval, speech-to-speech
translation, and visual question answering. We also demonstrate how to quickly
build a multimodal agent based on the i-Code Studio that can communicate and
personalize for users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation. (arXiv:2305.13740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13740">
<div class="article-summary-box-inner">
<span><p>Tense inconsistency frequently occurs in machine translation. However, there
are few criteria to assess the model's mastery of tense prediction from a
linguistic perspective. In this paper, we present a parallel tense test set,
containing French-English 552 utterances. We also introduce a corresponding
benchmark, tense prediction accuracy. With the tense test set and the
benchmark, researchers are able to measure the tense consistency performance of
machine translation systems for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goal-Driven Explainable Clustering via Language Descriptions. (arXiv:2305.13749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13749">
<div class="article-summary-box-inner">
<span><p>Unsupervised clustering is widely used to explore large corpora, but existing
formulations neither consider the users' goals nor explain clusters' meanings.
We propose a new task formulation, "Goal-Driven Clustering with Explanations"
(GoalEx), which represents both the goal and the explanations as free-form
language descriptions. For example, to categorize the errors made by a
summarization system, the input to GoalEx is a corpus of annotator-written
comments for system-generated summaries and a goal description "cluster the
comments based on why the annotators think the summary is imperfect.''; the
outputs are text clusters each with an explanation ("this cluster mentions that
the summary misses important context information."), which relates to the goal
and precisely explain which comments should (not) belong to a cluster. To
tackle GoalEx, we prompt a language model with "[corpus subset] + [goal] +
Brainstorm a list of explanations each representing a cluster."; then we
classify whether each sample belongs to a cluster based on its explanation;
finally, we use integer linear programming to select a subset of candidate
clusters to cover most samples while minimizing overlaps. We apply GoalEx
hierarchically to produce trees of progressively finer-grained clusters,
inducing taxonomies over debate arguments, customer complaints, and model
errors. We release our data and implementation at
https://github.com/ZihanWangKi/GoalEx.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Context-Aware Neural Machine Translation. (arXiv:2305.13751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13751">
<div class="article-summary-box-inner">
<span><p>Context-aware neural machine translation involves leveraging information
beyond sentence-level context to resolve inter-sentential discourse
dependencies and improve document-level translation quality, and has given rise
to a number of recent techniques. However, despite well-reasoned intuitions,
most context-aware translation models show only modest improvements over
sentence-level systems. In this work, we investigate several challenges that
impede progress within this field, relating to discourse phenomena, context
usage, model architectures, and document-level evaluation. To address these
problems, we propose a more realistic setting for document-level translation,
called paragraph-to-paragraph (para2para) translation, and collect a new
dataset of Chinese-English novels to promote future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-driven Distant Supervision Framework for Macro-level Discourse Parsing. (arXiv:2305.13755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13755">
<div class="article-summary-box-inner">
<span><p>Discourse parsing, the task of analyzing the internal rhetorical structure of
texts, is a challenging problem in natural language processing. Despite the
recent advances in neural models, the lack of large-scale, high-quality corpora
for training remains a major obstacle. Recent studies have attempted to
overcome this limitation by using distant supervision, which utilizes results
from other NLP tasks (e.g., sentiment polarity, attention matrix, and
segmentation probability) to parse discourse trees. However, these methods do
not take into account the differences between in-domain and out-of-domain
tasks, resulting in lower performance and inability to leverage the
high-quality in-domain data for further improvement. To address these issues,
we propose a distant supervision framework that leverages the relations between
topic structure and rhetorical structure. Specifically, we propose two
distantly supervised methods, based on transfer learning and the
teacher-student model, that narrow the gap between in-domain and out-of-domain
tasks through label mapping and oracle annotation. Experimental results on the
MCDTB and RST-DT datasets show that our methods achieve the best performance in
both distant-supervised and supervised scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-aware Training Improves In-context Learning Ability of Language Models. (arXiv:2305.13775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13775">
<div class="article-summary-box-inner">
<span><p>Many recent language models (LMs) of Transformers family exhibit so-called
in-context learning (ICL) ability, manifested in the LMs' ability to modulate
their function by a task described in a natural language input. Previous work
curating these models assumes that ICL emerges from vast over-parametrization
or the scale of multi-task training. However, a complementary branch of recent
theoretical work attributes ICL emergence to specific properties of training
data and creates functional in-context learners in small-scale, synthetic
settings.
</p>
<p>Inspired by recent findings on data properties driving the emergence of ICL,
we propose a method to create LMs able to better utilize the in-context
information, by constructing training scenarios where it is beneficial for the
LM to capture the analogical reasoning concepts. We measure that data sampling
of Concept-aware Training (CoAT) consistently improves models' reasoning
ability. As a result, the in-context learners trained with CoAT on only two
datasets of a single (QA) task perform comparably to larger models trained on
1600+ tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation. (arXiv:2305.13776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13776">
<div class="article-summary-box-inner">
<span><p>Counterspeech has been demonstrated to be an efficacious approach for
combating hate speech. While various conventional and controlled approaches
have been studied in recent years to generate counterspeech, a counterspeech
with a certain intent may not be sufficient in every scenario. Due to the
complex and multifaceted nature of hate speech, utilizing multiple forms of
counter-narratives with varying intents may be advantageous in different
circumstances. In this paper, we explore intent-conditioned counterspeech
generation. At first, we develop IntentCONAN, a diversified intent-specific
counterspeech dataset with 6831 counterspeeches conditioned on five intents,
i.e., informative, denouncing, question, positive, and humour. Subsequently, we
propose QUARC, a two-stage framework for intent-conditioned counterspeech
generation. QUARC leverages vector-quantized representations learned for each
intent category along with PerFuMe, a novel fusion module to incorporate
intent-specific information into the model. Our evaluation demonstrates that
QUARC outperforms several baselines by an average of 10% across evaluation
metrics. An extensive human evaluation supplements our hypothesis of better and
more appropriate responses than comparative systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks. (arXiv:2305.13782v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13782">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated robust performance on various
language tasks using zero-shot or few-shot learning paradigms. While being
actively researched, multimodal models that can additionally handle images as
input have yet to catch up in size and generality with language-only models. In
this work, we ask whether language-only models can be utilised for tasks that
require visual input -- but also, as we argue, often require a strong reasoning
component. Similar to some recent related work, we make visual information
accessible to the language model using separate verbalisation models.
Specifically, we investigate the performance of open-source, open-access
language models against GPT-3 on five vision-language tasks when given
textually-encoded visual information. Our results suggest that language models
are effective for solving vision-language tasks even with limited samples. This
approach also enhances the interpretability of a model's output by providing a
means of tracing the output back through the verbalised image content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13785">
<div class="article-summary-box-inner">
<span><p>Training or finetuning large-scale language models (LLMs) such as GPT-3
requires substantial computation resources, motivating recent efforts to
explore parameter-efficient adaptation to downstream tasks. One practical area
of research is to treat these models as black boxes and interact with them
through their inference APIs. In this paper, we investigate how to optimize
few-shot text classification without accessing the gradients of the LLMs. To
achieve this, we treat the black-box model as a feature extractor and train a
classifier with the augmented text data. Data augmentation is performed using
prompt-based finetuning on an auxiliary language model with a much smaller
parameter size than the black-box model. Through extensive experiments on eight
text classification datasets, we show that our approach, dubbed BT-Classifier,
significantly outperforms state-of-the-art black-box few-shot learners and
performs on par with methods that rely on full-model tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13788">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown stellar achievements in solving a
broad range of tasks. When generating text, it is common to sample tokens from
these models: whether LLMs closely align with the human disagreement
distribution has not been well-studied, especially within the scope of Natural
Language Inference (NLI). In this paper, we evaluate the performance and
alignment of LLM distribution with humans using two different techniques: Monte
Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a
result, we show LLMs exhibit limited ability in solving NLI tasks and
simultaneously fail to capture human disagreement distribution, raising
concerns about their natural language understanding (NLU) ability and their
representativeness of human users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Predictive ASR for Latency Reduction in Voice Assistants. (arXiv:2305.13794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13794">
<div class="article-summary-box-inner">
<span><p>Streaming Automatic Speech Recognition (ASR) in voice assistants can utilize
prefetching to partially hide the latency of response generation. Prefetching
involves passing a preliminary ASR hypothesis to downstream systems in order to
prefetch and cache a response. If the final ASR hypothesis after endpoint
detection matches the preliminary one, the cached response can be delivered to
the user, thus saving latency. In this paper, we extend this idea by
introducing predictive automatic speech recognition, where we predict the full
utterance from a partially observed utterance, and prefetch the response based
on the predicted utterance. We introduce two personalization approaches and
investigate the tradeoff between potential latency gains from successful
predictions and the cost increase from failed predictions. We evaluate our
methods on an internal voice assistant dataset as well as the public SLURP
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path. (arXiv:2305.13805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13805">
<div class="article-summary-box-inner">
<span><p>The rapid growth of web pages and the increasing complexity of their
structure poses a challenge for web mining models. Web mining models are
required to understand the semi-structured web pages, particularly when little
is known about the subject or template of a new page. Current methods migrate
language models to the web mining by embedding the XML source code into the
transformer or encoding the rendered layout with graph neural networks.
However, these approaches do not take into account the relationships between
text nodes within and across pages. In this paper, we propose a new approach,
ReXMiner, for zero-shot relation extraction in web mining. ReXMiner encodes the
shortest relative paths in the Document Object Model (DOM) tree which is a more
accurate and efficient signal for key-value pair extraction within a web page.
It also incorporates the popularity of each text node by counting the
occurrence of the same text node across different web pages. We use the
contrastive learning to address the issue of sparsity in relation extraction.
Extensive experiments on public benchmarks show that our method, ReXMiner,
outperforms the state-of-the-art baselines in the task of zero-shot relation
extraction in web mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Clarification Questions to Handle Ambiguity in Open-Domain QA. (arXiv:2305.13808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13808">
<div class="article-summary-box-inner">
<span><p>Ambiguous questions persist in open-domain question answering, because
formulating a precise question with a unique answer is often challenging.
Previously, Min et al. (2020) have tackled this issue by generating
disambiguated questions for all possible interpretations of the ambiguous
question. This can be effective, but not ideal for providing an answer to the
user. Instead, we propose to ask a clarification question, where the user's
response will help identify the interpretation that best aligns with the user's
intention. We first present CAMBIGNQ, a dataset consisting of 5,654 ambiguous
questions, each with relevant passages, possible answers, and a clarification
question. The clarification questions were efficiently created by generating
them using InstructGPT and manually revising them as necessary. We then define
a pipeline of tasks and design appropriate evaluation metrics. Lastly, we
achieve 61.3 F1 on ambiguity detection and 40.5 F1 on clarification-based QA,
providing strong baselines for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13812">
<div class="article-summary-box-inner">
<span><p>Contrastively trained vision-language models have achieved remarkable
progress in vision and language representation learning, leading to
state-of-the-art models for various downstream multimodal tasks. However,
recent research has highlighted severe limitations of these models in their
ability to perform compositional reasoning over objects, attributes, and
relations. Scene graphs have emerged as an effective way to understand images
compositionally. These are graph-structured semantic representations of images
that contain objects, their attributes, and relations with other objects in a
scene. In this work, we consider the scene graph parsed from text as a proxy
for the image scene graph and propose a graph decomposition and augmentation
framework along with a coarse-to-fine contrastive learning objective between
images and text that aligns sentences of various complexities to the same
image. Along with this, we propose novel negative mining techniques in the
scene graph space for improving attribute binding and relation understanding.
Through extensive experiments, we demonstrate the effectiveness of our approach
that significantly improves attribute binding, relation understanding,
systematic generalization, and productivity on multiple recently proposed
benchmarks (For example, improvements upto $18\%$ for systematic
generalization, $16.5\%$ for relation understanding over a strong baseline),
while achieving similar or better performance than CLIP on various general
multimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting automatically the layout of clinical documents to enhance the performances of downstream natural language processing. (arXiv:2305.13817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13817">
<div class="article-summary-box-inner">
<span><p>Objective:Develop and validate an algorithm for analyzing the layout of PDF
clinical documents to improve the performance of downstream natural language
processing tasks. Materials and Methods: We designed an algorithm to process
clinical PDF documents and extract only clinically relevant text. The algorithm
consists of several steps: initial text extraction using a PDF parser, followed
by classification into categories such as body text, left notes, and footers
using a Transformer deep neural network architecture, and finally an
aggregation step to compile the lines of a given label in the text. We
evaluated the technical performance of the body text extraction algorithm by
applying it to a random sample of documents that were annotated. Medical
performance was evaluated by examining the extraction of medical concepts of
interest from the text in their respective sections. Finally, we tested an
end-to-end system on a medical use case of automatic detection of acute
infection described in the hospital report. Results:Our algorithm achieved
per-line precision, recall, and F1 score of 98.4, 97.0, and 97.7, respectively,
for body line extraction. The precision, recall, and F1 score per document for
the acute infection detection algorithm were 82.54 (95CI 72.86-91.60), 85.24
(95CI 76.61-93.70), 83.87 (95CI 76, 92-90.08) with exploitation of the results
of the advanced body extraction algorithm, respectively. Conclusion:We have
developed and validated a system for extracting body text from clinical
documents in PDF format by identifying their layout. We were able to
demonstrate that this preprocessing allowed us to obtain better performances
for a common downstream task, i.e., the extraction of medical concepts in their
respective sections, thus proving the interest of this method on a clinical use
case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open Dataset and Model for Language Identification. (arXiv:2305.13820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13820">
<div class="article-summary-box-inner">
<span><p>Language identification (LID) is a fundamental step in many natural language
processing pipelines. However, current LID systems are far from perfect,
particularly on lower-resource languages. We present a LID model which achieves
a macro-average F1 score of 0.93 and a false positive rate of 0.033 across 201
languages, outperforming previous work. We achieve this by training on a
curated dataset of monolingual data, the reliability of which we ensure by
auditing a sample from each source and each language manually. We make both the
model and the dataset available to the research community. Finally, we carry
out detailed analysis into our model's performance, both in comparison to
existing open models and by language class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Is the Pope Catholic?" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures. (arXiv:2305.13826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13826">
<div class="article-summary-box-inner">
<span><p>Conversational implicatures are pragmatic inferences that require listeners
to deduce the intended meaning conveyed by a speaker from their explicit
utterances. Although such inferential reasoning is fundamental to human
communication, recent research indicates that large language models struggle to
comprehend these implicatures as effectively as the average human. This paper
demonstrates that by incorporating Grice's Four Maxims into the model through
chain-of-thought prompting, we can significantly enhance its performance,
surpassing even the average human performance on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13829">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated their ability to self-reflect and
refine their generation, which can further improve their performance. However,
this feedback mechanism faces challenges such as no guarantee of correctness
and the lack of global insight into the model's weaknesses. In this paper, we
propose a novel framework, Study Assistant for Large Language Model (SALAM), to
aid LLMs in the reflection and refinement process. Motivated by the human study
assistant, this framework grades previous responses with the ground truth and
collects mistakes in the training phase. During inference, it identifies common
misunderstandings based on the mistake collections and provides guidelines for
the model to help the model avoid similar mistakes during inference. SALAM is a
model-agnostic framework, focusing on providing general feedback and can adapt
to any base model. Our evaluation of SALAM on two challenging benchmarks
demonstrated a significant improvement over various baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models. (arXiv:2305.13831v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13831">
<div class="article-summary-box-inner">
<span><p>Emotional Text-To-Speech (TTS) is an important task in the development of
systems (e.g., human-like dialogue agents) that require natural and emotional
speech. Existing approaches, however, only aim to produce emotional TTS for
seen speakers during training, without consideration of the generalization to
unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive
emotion-controllable TTS model that allows users to synthesize any speaker's
emotional speech using only a short, neutral speech segment and the target
emotion label. Specifically, to enable a zero-shot adaptive TTS model to
synthesize emotional speech, we propose domain adversarial learning and
guidance methods on the diffusion model. Experimental results demonstrate that
ZET-Speech successfully synthesizes natural and emotional speech with the
desired emotion for both seen and unseen speakers. Samples are at
https://ZET-Speech.github.io/ZET-Speech-Demo/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Sensitivity on Speaker Names for Text Generation from Dialogues. (arXiv:2305.13833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13833">
<div class="article-summary-box-inner">
<span><p>Changing speaker names consistently throughout a dialogue should not affect
its meaning and corresponding outputs for text generation from dialogues.
However, pre-trained language models, serving as the backbone for
dialogue-processing tasks, have shown to be sensitive to nuances. This may
result in unfairness in real-world applications. No comprehensive analysis of
this problem has been done in the past. In this work, we propose to
quantitatively measure a model's sensitivity on speaker names, and
comprehensively evaluate a number of known methods for reducing speaker name
sensitivity, including a novel approach of our own. Extensive experiments on
multiple datasets provide a benchmark for this problem and show the favorable
performance of our approach in sensitivity reduction and quality of generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation. (arXiv:2305.13844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13844">
<div class="article-summary-box-inner">
<span><p>Geoparsing is a fundamental technique for analyzing geo-entity information in
text. We focus on document-level geoparsing, which considers geographic
relatedness among geo-entity mentions, and presents a Japanese travelogue
dataset designed for evaluating document-level geoparsing systems. Our dataset
comprises 200 travelogue documents with rich geo-entity information: 12,171
mentions, 6,339 coreference clusters, and 2,551 geo-entities linked to
geo-database entries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13850">
<div class="article-summary-box-inner">
<span><p>Visual relation extraction (VRE) aims to extract relations between entities
from visuallyrich documents. Existing methods usually predict relations for
each entity pair independently based on entity features but ignore the global
structure information, i.e., dependencies between entity pairs. The absence of
global structure information may make the model struggle to learn long-range
relations and easily predict conflicted results. To alleviate such limitations,
we propose a GlObal Structure knowledgeguided relation Extraction (GOSE)
framework, which captures dependencies between entity pairs in an iterative
manner. Given a scanned image of the document, GOSE firstly generates
preliminary relation predictions on entity pairs. Secondly, it mines global
structure knowledge based on prediction results of the previous iteration and
further incorporates global structure knowledge into entity representations.
This "generate-capture-incorporate" schema is performed multiple times so that
entity representations and global structure knowledge can mutually reinforce
each other. Extensive experiments show that GOSE not only outperforms previous
methods on the standard fine-tuning setting but also shows promising
superiority in cross-lingual learning; even yields stronger data-efficient
performance in the low-resource setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation. (arXiv:2305.13857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13857">
<div class="article-summary-box-inner">
<span><p>Most task-oriented dialogue (TOD) benchmarks assume users that know exactly
how to use the system by constraining the user behaviors within the system's
capabilities via strict user goals, namely "user familiarity" bias. This data
bias deepens when it combines with data-driven TOD systems, as it is impossible
to fathom the effect of it with existing static evaluations. Hence, we conduct
an interactive user study to unveil how vulnerable TOD systems are against
realistic scenarios. In particular, we compare users with 1) detailed goal
instructions that conform to the system boundaries (closed-goal) and 2) vague
goal instructions that are often unsupported but realistic (open-goal). Our
study reveals that conversations in open-goal settings lead to catastrophic
failures of the system, in which 92% of the dialogues had significant issues.
Moreover, we conduct a thorough analysis to identify distinctive features
between the two settings through error annotation. From this, we discover a
novel "pretending" behavior, in which the system pretends to handle the user
requests even though they are beyond the system's capabilities. We discuss its
characteristics and toxicity while emphasizing transparency and a fallback
strategy for robust TOD systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. (arXiv:2305.13860v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13860">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential
but also introduce challenges related to content constraints and potential
misuse. Our study investigates three key research questions: (1) the number of
different prompt types that can jailbreak LLMs, (2) the effectiveness of
jailbreak prompts in circumventing LLM constraints, and (3) the resilience of
ChatGPT against these jailbreak prompts. Initially, we develop a classification
model to analyze the distribution of existing prompts, identifying ten distinct
patterns and three categories of jailbreak prompts. Subsequently, we assess the
jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a
dataset of 3,120 jailbreak questions across eight prohibited scenarios.
Finally, we evaluate the resistance of ChatGPT against jailbreak prompts,
finding that the prompts can consistently evade the restrictions in 40 use-case
scenarios. The study underscores the importance of prompt structures in
jailbreaking LLMs and discusses the challenges of robust jailbreak prompt
generation and prevention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Trip Towards Fairness: Bias and De-Biasing in Large Language Models. (arXiv:2305.13862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13862">
<div class="article-summary-box-inner">
<span><p>An outbreak in the popularity of transformer-based Language Models (such as
GPT (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)) has opened the
doors to new Machine Learning applications. In particular, in Natural Language
Processing and how pre-training from large text, corpora is essential in
achieving remarkable results in downstream tasks. However, these Language
Models seem to have inherent biases toward certain demographics reflected in
their training data. While research has attempted to mitigate this problem,
existing methods either fail to remove bias altogether, degrade performance, or
are expensive. This paper examines the bias produced by promising Language
Models when varying parameters and pre-training data. Finally, we propose a
de-biasing technique that produces robust de-bias models that maintain
performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Brain Context-Sensitivity with Masked-Attention Generation. (arXiv:2305.13863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13863">
<div class="article-summary-box-inner">
<span><p>Two fundamental questions in neurolinguistics concerns the brain regions that
integrate information beyond the lexical level, and the size of their window of
integration. To address these questions we introduce a new approach named
masked-attention generation. It uses GPT-2 transformers to generate word
embeddings that capture a fixed amount of contextual information. We then
tested whether these embeddings could predict fMRI brain activity in humans
listening to naturalistic text. The results showed that most of the cortex
within the language network is sensitive to contextual information, and that
the right hemisphere is more sensitive to longer contexts than the left.
Masked-attention generation supports previous analyses of context-sensitivity
in the brain, and complements them by quantifying the window size of context
integration per voxel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Narrative XL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13877">
<div class="article-summary-box-inner">
<span><p>Despite their tremendous successes, most large language models do not have
any long-term memory mechanisms, which restricts their applications. Overcoming
this limitation would not only require changes to the typical transformer
architectures or training procedures, but also a dataset on which these new
models could be trained and evaluated. We argue that existing resources lack a
few key properties, and that at present, there are no naturalistic datasets of
sufficient scale to train (and not only evaluate) long-term memory language
models. We then present our solution that capitalizes on the advances in
short-term memory language models to create such a dataset. Using GPT 3.5, we
summarized each scene in 1500 hand-curated books from Project Gutenberg, which
resulted in approximately 150 scene-level summaries per book. We then created a
number of reading comprehension questions based on these summaries, including
three types of multiple-choice scene recognition questions, as well as
free-form narrative reconstruction questions. Each book is thus associated with
more than 500 reading comprehension questions. Crucially, most questions have a
known ``retention demand'', indicating how long-term of a memory is needed to
answer it, which should aid long-term memory performance evaluation. We
validate our data in three small-scale experiments: one with human labelers,
and two with existing language models. We show that our questions 1) adequately
represent the source material 2) can be used to diagnose the model's memory
capacity 3) are not trivial for modern language models even when the memory
demand does not exceed those models' context lengths. Lastly, we provide our
code which can be used to further expand the dataset in an automated manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaD: Program-aided Distillation Specializes Large Models in Reasoning. (arXiv:2305.13888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13888">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) excel in several natural language
processing tasks, their size and inaccessibility present challenges for
extensive practical application. Previous studies acquire specialized skills
through distillation on LLMs, which result in trading generic abilities, called
model specialization. As for reasoning ability, chain-of-thought was
synthesized to subsequent distillation. However, due to hallucination,
synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect
reasoning steps damage the reasoning capability. To tackle above issues, we
propose Program-aided Distillation (PaD), which distills LLMs to obtain
specialized small models in reasoning tasks. In PaD, we strengthen specialized
models with program-aided reasoning, and help them overcome faulty reasoning
steps with automated error checking. Experimental results demonstrate that, on
the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain
LLMs (e.g., LLaMA), but also achieves a 10% improvement over baselines with a
significantly smaller scale of parameters and data. Data pruning analysis
reveals that PaD possesses higher training efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13899">
<div class="article-summary-box-inner">
<span><p>The ability to learn new concepts sequentially is a major weakness for modern
neural networks, which hinders their use in non-stationary environments. Their
propensity to fit the current data distribution to the detriment of the past
acquired knowledge leads to the catastrophic forgetting issue. In this work we
tackle the problem of Spoken Language Understanding applied to a continual
learning setting. We first define a class-incremental scenario for the SLURP
dataset. Then, we propose three knowledge distillation (KD) approaches to
mitigate forgetting for a sequence-to-sequence transformer model: the first KD
method is applied to the encoder output (audio-KD), and the other two work on
the decoder output, either directly on the token-level (tok-KD) or on the
sequence-level (seq-KD) distributions. We show that the seq-KD substantially
improves all the performance metrics, and its combination with the audio-KD
further decreases the average WER and enhances the entity prediction metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction. (arXiv:2305.13903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13903">
<div class="article-summary-box-inner">
<span><p>Despite constituting 65% of all internet traffic in 2023, video content is
underrepresented in generative AI research. Meanwhile, recent large language
models (LLMs) have become increasingly integrated with capabilities in the
visual modality. Integrating video with LLMs is a natural next step, so how can
this gap be bridged? To advance video reasoning, we propose a new research
direction of VideoCOT on video keyframes, which leverages the multimodal
generative abilities of vision-language models to enhance video reasoning while
reducing the computational complexity of processing hundreds or thousands of
frames. We introduce VIP, an inference-time dataset that can be used to
evaluate VideoCOT, containing 1) a variety of real-life videos with keyframes
and corresponding unstructured and structured scene descriptions, and 2) two
new video reasoning tasks: video infilling and scene prediction. We benchmark
various vision-language models on VIP, demonstrating the potential to use
vision-language models and LLMs to enhance video chain of thought reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientSpeech: An On-Device Text to Speech Model. (arXiv:2305.13905v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13905">
<div class="article-summary-box-inner">
<span><p>State of the art (SOTA) neural text to speech (TTS) models can generate
natural-sounding synthetic voices. These models are characterized by large
memory footprints and substantial number of operations due to the long-standing
focus on speech quality with cloud inference in mind. Neural TTS models are
generally not designed to perform standalone speech syntheses on
resource-constrained and no Internet access edge devices. In this work, an
efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM
CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive
pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k
parameters and consumes 90 MFLOPS only or about 1% of the size and amount of
computation in modern compact models such as Mixer-TTS. EfficientSpeech
achieves an average mel generation real-time factor of 104.3 on an RPi4. Human
evaluation shows only a slight degradation in audio quality as compared to
FastSpeech2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13915">
<div class="article-summary-box-inner">
<span><p>Recent neural retrieval mainly focuses on ranking short texts and is
challenged with long documents. Existing work mainly evaluates either ranking
passages or whole documents. However, there are many cases where the users want
to find a relevant passage within a long document from a huge corpus, e.g.
legal cases, research papers, etc. In this scenario, the passage often provides
little document context and thus challenges the current approaches to finding
the correct document and returning accurate results. To fill this gap, we
propose and name this task Document-Aware Passage Retrieval (DAPR) and build a
benchmark including multiple datasets from various domains, covering both DAPR
and whole-document retrieval. In experiments, we extend the state-of-the-art
neural passage retrievers with document-level context via different approaches
including prepending document summary, pooling over passage representations,
and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,
can only improve on the DAPR tasks marginally while significantly improving on
the document-retrieval tasks. This motivates further research in developing
better retrieval systems for the new task. The code and the data are available
at https://github.com/kwang2049/dapr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Data for Symbolic Language with Large Language Models. (arXiv:2305.13917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13917">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) bring not only performance but also
complexity, recent work has started to turn LLMs into data generators rather
than task inferencers, where another affordable task model is trained for
efficient deployment and inference. However, such an approach has primarily
been applied to natural language tasks and has not yet been explored for
symbolic language tasks with complex structured outputs (e.g., semantic parsing
and code generation). In this paper, we propose SymGen which utilizes LLMs for
generating various annotation-expensive symbolic language data. SymGen consists
of an informative prompt to steer generation and an agreement-based verifier to
improve data correctness. We conduct extensive experiments on six symbolic
language tasks across various settings. Compared with the LLMs, we demonstrate
the 1\%-sized task model can achieve comparable or better performance, largely
cutting inference and deployment costs. We also show that generated data with
only a few human demonstrations can be as effective as over 10 times the amount
of human-annotated data when training the task model, saving a considerable
amount of annotation effort. SymGen sheds new light on data generation for
complex tasks, and we release the code at
\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction. (arXiv:2305.13944v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13944">
<div class="article-summary-box-inner">
<span><p>The semantic frame induction tasks are defined as a clustering of words into
the frames that they evoke, and a clustering of their arguments according to
the frame element roles that they should fill. In this paper, we address the
latter task of argument clustering, which aims to acquire frame element
knowledge, and propose a method that applies deep metric learning. In this
method, a pre-trained language model is fine-tuned to be suitable for
distinguishing frame element roles through the use of frame-annotated data, and
argument clustering is performed with embeddings obtained from the fine-tuned
model. Experimental results on FrameNet demonstrate that our method achieves
substantially better performance than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Instruction Optimization for Large Language Models with Distribution Shifts. (arXiv:2305.13954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13954">
<div class="article-summary-box-inner">
<span><p>Large Language Models have demonstrated significant ability in accomplishing
a wide range of Natural Language Processing (NLP) tasks. However, their
performance is highly sensitive to the even minor changes in the phrasing of
the task instructions, leading to a line of research in automatic instruction
optimization towards better performance for NLP tasks. Unfortunately, existing
methods for instruction optimization fail to consider the distribution shift
between the seen training data and the unseen test data, where testing on
unseen group of data with a different distribution could potentially lead to
performance drop. In this paper, we take an initial step of investigating the
problem of LLM instruction optimization across data groups with distribution
shifts. We find that the optimal instructions do encounter performance drops on
LLM under certain distribution shifts. To this end, we propose a framework to
derive more robust optimal instructions that improve the performance on the
unseen data group without large sacrifice on the seen data group. Experimental
results demonstrate the effectiveness of our proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13971">
<div class="article-summary-box-inner">
<span><p>LLMs have shown impressive few-shot performance across many tasks. However,
they still struggle when it comes to generating complex output structures, such
as those required for Information Extraction. This limitation stems from the
fact that LLMs, without finetuning, tend to generate free text rather than
precise structures that follow a specific grammar. In this work, we propose to
enrich the decoding step with formal grammar constraints. During beam search,
only valid token continuations compliant with the grammar production rules are
considered. This enforces the generation of valid sequences exclusively. Our
framework is highly general and flexible, allowing any Context-Free Grammar
(CFG) to be integrated into our custom constrained beam search implementation.
We demonstrate that the outputs of many NLP tasks can be represented as formal
languages, making them suitable for direct use in our framework. For task where
the output space is dependent on the input, we propose input-dependent grammars
to constrain the generation. We conducted experiments with two challenging
tasks involving large alphabets in their grammar (Wikidata entities and
relations): information extraction and entity disambiguation. Our results with
LLaMA models clearly indicate that grammar-constrained decoding outperforms
few-shot prompting without constraints, and even competes with task-specific
finetuned models. These findings suggest that integrating grammar-based
constraints during decoding holds great promise in making LLMs reliably produce
structured outputs, especially in setting where training data is scarce and
finetuning is expensive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make a Choice! Knowledge Base Question Answering with In-Context Learning. (arXiv:2305.13972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13972">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge bases (KBQA) aims to answer factoid
questions with a given knowledge base (KB). Due to the large scale of KB,
annotated data is impossible to cover all fact schemas in KB, which poses a
challenge to the generalization ability of methods that require a sufficient
amount of annotated data. Recently, LLMs have shown strong few-shot performance
in many NLP tasks. We expect LLM can help existing methods improve their
generalization ability, especially in low-resource situations. In this paper,
we present McL-KBQA, a framework that incorporates the few-shot ability of LLM
into the KBQA method via ICL-based multiple choice and then improves the
effectiveness of the QA tasks. Experimental results on two KBQA datasets
demonstrate the competitive performance of McL-KBQA with strong improvements in
generalization. We expect to explore a new way to QA tasks from KBQA in
conjunction with LLM, how to generate answers normatively and correctly with
strong generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effortless Integration of Memory Management into Open-Domain Conversation Systems. (arXiv:2305.13973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13973">
<div class="article-summary-box-inner">
<span><p>Open-domain conversation systems integrate multiple conversation skills into
a single system through a modular approach. One of the limitations of the
system, however, is the absence of management capability for external memory.
In this paper, we propose a simple method to improve BlenderBot3 by integrating
memory management ability into it. Since no training data exists for this
purpose, we propose an automating dataset creation for memory management. Our
method 1) requires little cost for data construction, 2) does not affect
performance in other tasks, and 3) reduces external memory. We show that our
proposed model BlenderBot3-M^3, which is multi-task trained with memory
management, outperforms BlenderBot3 with a relative 4% performance gain in
terms of F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13981">
<div class="article-summary-box-inner">
<span><p>The robustness to distribution changes ensures that NLP models can be
successfully applied in the realistic world, especially for information
extraction tasks. However, most prior evaluation benchmarks have been devoted
to validating pairwise matching correctness, ignoring the crucial measurement
of robustness. In this paper, we present the first benchmark that simulates the
evaluation of open information extraction models in the real world, where the
syntactic and expressive distributions under the same knowledge meaning may
drift variously. We design and annotate a large-scale testbed in which each
example is a knowledge-invariant clique that consists of sentences with
structured knowledge of the same meaning but with different syntactic and
expressive forms. By further elaborating the robustness metric, a model is
judged to be robust if its performance is consistently accurate on the overall
cliques. We perform experiments on typical models published in the last decade
as well as a popular large language model, the results show that the existing
successful models exhibit a frustrating degradation, with a maximum drop of
23.43 F1 score. Our resources and code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01176">
<div class="article-summary-box-inner">
<span><p>Overlapping frequently occurs in paired texts in natural language processing
tasks like text editing and semantic similarity evaluation. Better evaluation
of the semantic distance between the overlapped sentences benefits the language
system's understanding and guides the generation. Since conventional semantic
metrics are based on word representations, they are vulnerable to the
disturbance of overlapped components with similar representations. This paper
aims to address the issue with a mask-and-predict strategy. We take the words
in the longest common sequence (LCS) as neighboring words and use masked
language modeling (MLM) from pre-trained language models (PLMs) to predict the
distributions on their positions. Our metric, Neighboring Distribution
Divergence (NDD), represent the semantic distance by calculating the divergence
between distributions in the overlapped parts. Experiments on Semantic Textual
Similarity show NDD to be more sensitive to various semantic differences,
especially on highly overlapped paired texts. Based on the discovery, we
further implement an unsupervised and training-free method for text
compression, leading to a significant improvement on the previous
perplexity-based method. The high scalability of our method even enables NDD to
outperform the supervised state-of-the-art in domain adaption by a huge margin.
Further experiments on syntax and semantics analyses verify the awareness of
internal sentence structures, indicating the high potential of NDD for further
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Principled Paraphrase Generation with Parallel Corpora. (arXiv:2205.12213v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12213">
<div class="article-summary-box-inner">
<span><p>Round-trip Machine Translation (MT) is a popular choice for paraphrase
generation, which leverages readily available parallel corpora for supervision.
In this paper, we formalize the implicit similarity function induced by this
approach, and show that it is susceptible to non-paraphrase pairs sharing a
single ambiguous translation. Based on these insights, we design an alternative
similarity metric that mitigates this issue by requiring the entire translation
distribution to match, and implement a relaxation of it through the Information
Bottleneck method. Our approach incorporates an adversarial term into MT
training in order to learn representations that encode as much information
about the reference translation as possible, while keeping as little
information about the input as possible. Paraphrases can be generated by
decoding back to the source from this representation, without having to
generate pivot translations. In addition to being more principled and efficient
than round-trip MT, our approach offers an adjustable parameter to control the
fidelity-diversity trade-off, and obtains better results in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling. (arXiv:2209.07634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07634">
<div class="article-summary-box-inner">
<span><p>Transformer encoder-decoder models have achieved great performance in
dialogue generation tasks, however, their inability to process long dialogue
history often leads to truncation of the context To address this problem, we
propose a novel memory-augmented transformer that is compatible with existing
pre-trained encoder-decoder models and enables efficient preservation of the
dialogue history information. By incorporating a separate memory module
alongside the pre-trained transformer, the model can effectively interchange
information between the memory states and the current input context. We
evaluate our model on three dialogue datasets and two language modeling
datasets. Experimental results show that our method has achieved superior
efficiency and performance compared to other pre-trained Transformer baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks. (arXiv:2210.00185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00185">
<div class="article-summary-box-inner">
<span><p>Although large language models have achieved impressive zero-shot ability,
the huge model size generally incurs high cost. Recently, semi-parametric
language models, which augment a smaller language model with an external
retriever, have demonstrated promising language modeling capabilities. However,
it remains unclear whether such semi-parametric language models can perform
competitively well as their fully-parametric counterparts on zero-shot
generalization to downstream tasks. In this work, we introduce $\text{Zemi}$, a
zero-shot semi-parametric language model. To our best knowledge, this is the
first semi-parametric language model that can demonstrate strong zero-shot
performance on a wide range of held-out unseen tasks. We train $\text{Zemi}$
with a novel semi-parametric multitask prompted training paradigm, which shows
significant improvement compared with the parametric multitask training as
proposed by T0. Specifically, we augment the multitask training and zero-shot
evaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In
order to incorporate multiple potentially noisy retrieved augmentations, we
further propose a novel $\text{augmentation fusion}$ module leveraging
perceiver resampler and gated cross-attention. Notably, our proposed
$\text{Zemi}_\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation
tasks while being 3.9x smaller in model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Efficiency and Generalization of Light Hybrid Retrievers. (arXiv:2210.01371v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01371">
<div class="article-summary-box-inner">
<span><p>Hybrid retrievers can take advantage of both sparse and dense retrievers.
Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this
work, we study "Is it possible to reduce the indexing memory of hybrid
retrievers without sacrificing performance"? Driven by this question, we
leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a
LITE retriever that further reduces the memory of DrBoost. LITE is jointly
trained on contrastive learning and knowledge distillation from DrBoost. Then,
we integrate BM25, a sparse retriever, with either LITE or DrBoost to form
light hybrid retrievers. Our Hybrid-LITE retriever saves 13X memory while
maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In
addition, we study the generalization capacity of our light hybrid retrievers
on out-of-domain dataset and a set of adversarial attacks datasets. Experiments
showcase that light hybrid retrievers achieve better generalization performance
than individual sparse and dense retrievers. Nevertheless, our analysis shows
that there is a large room to improve the robustness of retrievers, suggesting
a new research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability. (arXiv:2210.01989v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01989">
<div class="article-summary-box-inner">
<span><p>Transformer and its variants are fundamental neural architectures in deep
learning. Recent works show that learning attention in the Fourier space can
improve the long sequence learning capability of Transformers. We argue that
wavelet transform shall be a better choice because it captures both position
and frequency information with linear time complexity. Therefore, in this
paper, we systematically study the synergy between wavelet transform and
Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates
attention learning in a learnable wavelet coefficient space which replaces the
attention in Transformers by (1) applying forward wavelet transform to project
the input sequences to multi-resolution bases, (2) conducting attention
learning in the wavelet coefficient space, and (3) reconstructing the
representation in input space via backward wavelet transform. Extensive
experiments on the Long Range Arena demonstrate that learning attention in the
wavelet space using either fixed or adaptive wavelets can consistently improve
Transformer's performance and also significantly outperform learning in Fourier
space. We further show our method can enhance Transformer's reasoning
extrapolation capability over distance on the LEGO chain-of-reasoning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Narrowing the Compositionality Gap in Language Models. (arXiv:2210.03350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03350">
<div class="article-summary-box-inner">
<span><p>We investigate the ability of language models to perform compositional
reasoning tasks where the overall solution depends on correctly composing the
answers to sub-problems. We measure how often models can correctly answer all
sub-problems but not generate the overall solution, a ratio we call the
compositionality gap. We evaluate this ratio by asking multi-hop questions with
answers that require composing multiple facts unlikely to have been observed
together during pretraining. In the GPT-3 family of models, as model size
increases we show that the single-hop question answering performance improves
faster than the multi-hop performance does, therefore the compositionality gap
does not decrease. This surprising result suggests that while more powerful
models memorize and recall more factual knowledge, they show no corresponding
improvement in their ability to perform this kind of compositional reasoning.
</p>
<p>We then demonstrate how elicitive prompting (such as chain of thought)
narrows the compositionality gap by reasoning explicitly instead of implicitly.
We present a new method, self-ask, that further improves on chain of thought.
In our method, the model explicitly asks itself (and then answers) follow-up
questions before answering the initial question. We finally show that
self-ask's structured prompting lets us easily plug in a search engine to
answer the follow-up questions, which additionally improves accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering. (arXiv:2210.05156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05156">
<div class="article-summary-box-inner">
<span><p>Given its effectiveness on knowledge-intensive natural language processing
tasks, dense retrieval models have become increasingly popular. Specifically,
the de-facto architecture for open-domain question answering uses two
isomorphic encoders that are initialized from the same pretrained model but
separately parameterized for questions and passages. This bi-encoder
architecture is parameter-inefficient in that there is no parameter sharing
between encoders. Further, recent studies show that such dense retrievers
underperform BM25 in various settings. We thus propose a new architecture,
Task-aware Specialization for dense Retrieval (TASER), which enables parameter
sharing by interleaving shared and specialized blocks in a single encoder. Our
experiments on five question answering datasets show that TASER can achieve
superior accuracy, surpassing BM25, while using about 60% of the parameters as
bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also
empirically more robust than bi-encoder dense retrievers. Our code is available
at https://github.com/microsoft/taser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Train and Test-Time Augmentations for Audio-Language Learning. (arXiv:2210.17143v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17143">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to unveil the impact of data augmentation in
audio-language multi-modal learning, which has not been explored despite its
importance. We explore various augmentation methods at not only train-time but
also test-time and find out that proper data augmentation can lead to
substantial improvements. Specifically, applying our proposed audio-language
paired augmentation PairMix, which is the first multi-modal audio-language
augmentation method, outperforms the baselines for both automated audio
captioning and audio-text retrieval tasks. To fully take advantage of data
augmentation, we also present multi-level test-time augmentation (Multi-TTA)
for the test-time. We successfully incorporate the two proposed methods and
uni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is
an 18.2% relative increase over the baseline. In audio-text retrieval, the
proposed methods also show an improvement in performance as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01335">
<div class="article-summary-box-inner">
<span><p>The tremendous success of CLIP (Radford et al., 2021) has promoted the
research and application of contrastive learning for vision-language
pretraining. In this work, we construct a large-scale dataset of image-text
pairs in Chinese, where most data are retrieved from publicly available
datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5
Chinese CLIP models of multiple sizes, spanning from 77 to 958 million
parameters. Furthermore, we propose a two-stage pretraining method, where the
model is first trained with the image encoder frozen and then trained with all
parameters being optimized, to achieve enhanced model performance. Our
comprehensive experiments demonstrate that Chinese CLIP can achieve the
state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups
of zero-shot learning and finetuning, and it is able to achieve competitive
performance in zero-shot image classification based on the evaluation on the
ELEVATER benchmark (Li et al., 2022). We have released our codes, models, and
demos in https://github.com/OFA-Sys/Chinese-CLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04928">
<div class="article-summary-box-inner">
<span><p>This paper presents miCSE, a mutual information-based contrastive learning
framework that significantly advances the state-of-the-art in few-shot sentence
embedding. The proposed approach imposes alignment between the attention
pattern of different views during contrastive learning. Learning sentence
embeddings with miCSE entails enforcing the structural consistency across
augmented views for every sentence, making contrastive self-supervised learning
more sample efficient. As a result, the proposed approach shows strong
performance in the few-shot learning domain. While it achieves superior results
compared to state-of-the-art methods on multiple benchmarks in few-shot
learning, it is comparable in the full-shot scenario. This study opens up
avenues for efficient self-supervised learning methods that are more robust
than current contrastive methods for sentence embedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UGIF: UI Grounded Instruction Following. (arXiv:2211.07615v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07615">
<div class="article-summary-box-inner">
<span><p>Smartphone users often find it difficult to navigate myriad menus to perform
common tasks such as "How to block calls from unknown numbers?". Currently,
help documents with step-by-step instructions are manually written to aid the
user. The user experience can be further enhanced by grounding the instructions
in the help document to the UI and overlaying a tutorial on the phone UI. To
build such tutorials, several natural language processing components including
retrieval, parsing, and grounding are necessary, but there isn't any relevant
dataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual,
multi-modal UI grounded dataset for step-by-step task completion on the
smartphone containing 4,184 tasks across 8 languages. As an initial approach to
this problem, we propose retrieving the relevant instruction steps based on the
user's query and parsing the steps using Large Language Models (LLMs) to
generate macros that can be executed on-device. The instruction steps are often
available only in English, so the challenge includes cross-modal, cross-lingual
retrieval of English how-to pages from user queries in many languages and
mapping English instruction steps to UI in a potentially different language. We
compare the performance of different LLMs including PaLM and GPT-3 and find
that the end-to-end task completion rate is 48% for English UI but the
performance drops to 32% for other languages. We analyze the common failure
modes of existing models on this task and point out areas for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-sample Curriculum Learning by Sequence Completion for Natural Language Generation. (arXiv:2211.11297v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11297">
<div class="article-summary-box-inner">
<span><p>Curriculum learning has shown promising improvements in multiple domains by
training machine learning models from easy samples to hard ones. Previous works
which either design rules or train models for scoring the difficulty highly
rely on task-specific expertise, and cannot generalize. Inspired by the
"easy-to-hard" intuition, we propose to do in-sample curriculum learning for
natural language generation tasks. Our learning strategy starts training the
model to generate the last few words, i.e., do sequence completion, and
gradually extends to generate the whole output sequence. Comprehensive
experiments show that it generalizes well to different tasks and achieves
significant improvements over strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompted Opinion Summarization with GPT-3.5. (arXiv:2211.15914v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15914">
<div class="article-summary-box-inner">
<span><p>Large language models have shown impressive performance across a wide variety
of tasks, including text summarization. In this paper, we show that this strong
performance extends to opinion summarization. We explore several pipeline
methods for applying GPT-3.5 to summarize a large collection of user reviews in
a prompted fashion. To handle arbitrarily large numbers of user reviews, we
explore recursive summarization as well as methods for selecting salient
content to summarize through supervised clustering or extraction. On two
datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and
a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show
that GPT-3.5 models achieve very strong performance in human evaluation. We
argue that standard evaluation metrics do not reflect this, and introduce three
new metrics targeting faithfulness, factuality, and genericity to contrast
these different methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning. (arXiv:2211.16773v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16773">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogs, an informative and successful system response needs
to include key information such as the phone number of a hotel. Therefore, we
hypothesize that a model can achieve better overall performance by focusing on
correctly generating key quantities. In this paper, we propose a new training
algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), that
utilizes Reinforcement Learning but avoids the time-consuming auto-regressive
generation, and a fine-grained per-token reward function to help the model
learn keywords generation more robustly. Empirical results show that the KRLS
algorithm can achieve state-of-the-art performance on the inform, success, and
combined score on the MultiWoZ benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness of Learning from Task Instructions. (arXiv:2212.03813v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03813">
<div class="article-summary-box-inner">
<span><p>Traditional supervised learning mostly works on individual tasks and requires
training on a large set of task-specific examples. This paradigm seriously
hinders the development of task generalization since preparing a task-specific
example set is costly. To build a system that can quickly and easily generalize
to new tasks, task instructions have been adopted as an emerging trend of
supervision recently. These instructions give the model the definition of the
task and allow the model to output the appropriate answer based on the
instructions and inputs. However, task instructions are often expressed in
different forms, which can be interpreted from two threads: first, some
instructions are short sentences and are pretrained language model (PLM)
oriented, such as prompts, while other instructions are paragraphs and are
human-oriented, such as those in Amazon MTurk; second, different end-users very
likely explain the same task with instructions of different textual
expressions. A robust system for task generalization should be able to handle
any new tasks regardless of the variability of instructions.
</p>
<p>However, the system robustness in dealing with instruction-driven task
generalization is still unexplored. This work investigates the system
robustness when the instructions of new tasks are (i) manipulated, (ii)
paraphrased, or (iii) from different levels of conciseness. To our knowledge,
this is the first work that systematically studies how robust a PLM is when it
is supervised by instructions with different factors of variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08158">
<div class="article-summary-box-inner">
<span><p>Vision and language models (VL) are known to exploit unrobust indicators in
individual modalities (e.g., introduced by distributional biases) instead of
focusing on relevant information in each modality. That a unimodal model
achieves similar accuracy on a VL task to a multimodal one, indicates that
so-called unimodal collapse occurred. However, accuracy-based tests fail to
detect e.g., when the model prediction is wrong, while the model used relevant
information from a modality. Instead, we propose MM-SHAP, a
performance-agnostic multimodality score based on Shapley values that reliably
quantifies in which proportions a multimodal model uses individual modalities.
We apply MM-SHAP in two ways: (1) to compare models for their average degree of
multimodality, and (2) to measure for individual models the contribution of
individual modalities for different tasks and datasets. Experiments with six VL
models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight
that unimodal collapse can occur to different degrees and in different
directions, contradicting the wide-spread assumption that unimodal collapse is
one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal
tasks, to diagnose and guide progress towards multimodal integration. Code
available at \url{https://github.com/Heidelberg-NLP/MM-SHAP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources in Natural Language Understanding Systems. (arXiv:2212.08192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08192">
<div class="article-summary-box-inner">
<span><p>Many state-of-the-art natural language understanding (NLU) models are based
on pretrained neural language models. These models often make inferences using
information from multiple sources. An important class of such inferences are
those that require both background knowledge, presumably contained in a model's
pretrained parameters, and instance-specific information that is supplied at
inference time. However, the integration and reasoning abilities of NLU models
in the presence of multiple knowledge sources have been largely understudied.
In this work, we propose a test suite of coreference resolution subtasks that
require reasoning over multiple facts. These subtasks differ in terms of which
knowledge sources contain the relevant facts. We also introduce subtasks where
knowledge is present only at inference time using fictional knowledge. We
evaluate state-of-the-art coreference resolution models on our dataset. Our
results indicate that several models struggle to reason on-the-fly over
knowledge observed both at pretrain time and at inference time. However, with
task-specific training, a subset of models demonstrates the ability to
integrate certain knowledge types from multiple sources. Still, even the best
performing models seem to have difficulties with reliably integrating knowledge
presented only at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Knowledge Distillation from Crowd Annotations for Out-of-Domain Generalization. (arXiv:2212.09409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09409">
<div class="article-summary-box-inner">
<span><p>Selecting an effective training signal for tasks in natural language
processing is difficult: expert annotations are expensive, and crowd-sourced
annotations may not be reliable. At the same time, recent work in NLP has
demonstrated that learning from a distribution over labels acquired from crowd
annotations can be effective. However, there are many ways to acquire such a
distribution, and the performance allotted by any one method can fluctuate
based on the task and the amount of available crowd annotations, making it
difficult to know a priori which distribution is best. This paper
systematically analyzes this in the out-of-domain setting, adding to the NLP
literature which has focused on in-domain evaluation, and proposes new methods
for acquiring soft-labels from crowd-annotations by aggregating the
distributions produced by existing methods. In particular, we propose to
aggregate multiple-views of crowd annotations via temperature scaling and
finding their Jensen-Shannon centroid. We demonstrate that these aggregation
methods lead to the most consistent performance across four NLP tasks on
out-of-domain test sets, mitigating fluctuations in performance from the
individual distributions. Additionally, aggregation results in the most
consistently well-calibrated uncertainty estimation. We argue that aggregating
different views of crowd-annotations is an effective and minimal intervention
to acquire soft-labels which induce robust classifiers despite the
inconsistency of the individual soft-labeling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Better Reasoners with Self-Verification. (arXiv:2212.09561v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09561">
<div class="article-summary-box-inner">
<span><p>Recently, with the chain of thought (CoT) prompting, large language models
(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural
language processing tasks such as arithmetic, commonsense, and logical
reasoning. However, LLMs with CoT require multi-step prompting and multi-token
prediction, which is highly sensitive to individual mistakes and vulnerable to
error accumulation. The above issues make the LLMs need the ability to verify
the answers. In fact, after inferring conclusions in some thinking decision
tasks, people often check them by re-verifying steps to avoid some mistakes. In
this paper, we propose and prove that LLMs also have similar self-verification
abilities. We take the conclusion obtained by CoT as one of the conditions for
solving the original problem. By taking turns masking the original conditions
and predicting their results, we calculate an explainable answer verification
score based on whether the re-predicted conditions are correct. Experimental
results demonstrate that the proposed method can improve the reasoning
performance on various arithmetic, commonsense, and logical reasoning datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications. (arXiv:2212.09676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09676">
<div class="article-summary-box-inner">
<span><p>Scholarly text is often laden with jargon, or specialized language that can
facilitate efficient in-group communication within fields but hinder
understanding for out-groups. In this work, we develop and validate an
interpretable approach for measuring scholarly jargon from text. Expanding the
scope of prior work which focuses on word types, we use word sense induction to
also identify words that are widespread but overloaded with different meanings
across fields. We then estimate the prevalence of these discipline-specific
words and senses across hundreds of subfields, and show that word senses
provide a complementary, yet unique view of jargon alongside word types. We
demonstrate the utility of our metrics for science of science and computational
sociolinguistics by highlighting two key social implications. First, though
most fields reduce their use of jargon when writing for general-purpose venues,
and some fields (e.g., biological sciences) do so less than others. Second, the
direction of correlation between jargon and citation rates varies among fields,
but jargon is nearly always negatively correlated with interdisciplinary
impact. Broadly, our findings suggest that though multidisciplinary venues
intend to cater to more general audiences, some fields' writing norms may act
as barriers rather than bridges, and thus impede the dispersion of scholarly
ideas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. (arXiv:2212.10474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10474">
<div class="article-summary-box-inner">
<span><p>State-of-the-art poetry generation systems are often complex. They either
consist of task-specific model pipelines, incorporate prior knowledge in the
form of manually created constraints, or both. In contrast, end-to-end models
would not suffer from the overhead of having to model prior knowledge and could
learn the nuances of poetry from data alone, reducing the degree of human
supervision required. In this work, we investigate end-to-end poetry generation
conditioned on styles such as rhyme, meter, and alliteration. We identify and
address lack of training data and mismatching tokenization algorithms as
possible limitations of past attempts. In particular, we successfully pre-train
ByGPT5, a new token-free decoder-only language model, and fine-tune it on a
large custom corpus of English and German quatrains annotated with our styles.
We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and
ChatGPT, while also being more parameter efficient and performing favorably
compared to humans. In addition, we analyze its runtime performance and
demonstrate that it is not prone to memorization. We make our code, models, and
datasets publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?. (arXiv:2212.10784v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10784">
<div class="article-summary-box-inner">
<span><p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of
annotations and the prevalence of instances without explicitly pre-defined
labels due to low annotation coverage. Existing approaches, which treat
biomedical RE as a multi-class classification task, often result in poor
generalization in low-resource settings and do not have the ability to make
selective prediction on unknown cases but give a guess from seen relations,
hindering the applicability of those approaches. We present NBR, which converts
biomedical RE as natural language inference formulation through indirect
supervision. By converting relations to natural language hypotheses, NBR is
capable of exploiting semantic cues to alleviate annotation scarcity. By
incorporating a ranking-based loss that implicitly calibrates abstinent
instances, NBR learns a clearer decision boundary and is instructed to abstain
on uncertain instances. Extensive experiments on three widely-used biomedical
RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in
both full-set and low-resource regimes. Our analysis demonstrates that indirect
supervision benefits biomedical RE even when a domain gap exists, and combining
NLI knowledge with biomedical knowledge leads to the best performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01015">
<div class="article-summary-box-inner">
<span><p>In this paper we explore the task of modeling semi-structured object
sequences; in particular, we focus our attention on the problem of developing a
structure-aware input representation for such sequences. Examples of such data
include user activity on websites, machine logs, and many others. This type of
data is often represented as a sequence of sets of key-value pairs over time
and can present modeling challenges due to an ever-increasing sequence length.
We propose a two-part approach, which first considers each key independently
and encodes a representation of its values over time; we then self-attend over
these value-aware key representations to accomplish a downstream task. This
allows us to operate on longer object sequences than existing methods. We
introduce a novel shared-attention-head architecture between the two modules
and present an innovative training schedule that interleaves the training of
both modules with shared weights for some attention heads. Our experiments on
multiple prediction tasks using real-world data demonstrate that our approach
outperforms a unified network with hierarchical encoding, as well as other
methods including a record-centric representation and a flattened
representation of the sequence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Replug: Retrieval-augmented black-box language models. (arXiv:2301.12652v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12652">
<div class="article-summary-box-inner">
<span><p>We introduce REPLUG, a retrieval-augmented language modeling framework that
treats the language model (LM) as a black box and augments it with a tuneable
retrieval model. Unlike prior retrieval-augmented LMs that train language
models with special cross attention mechanisms to encode the retrieved text,
REPLUG simply prepends retrieved documents to the input for the frozen
black-box LM. This simple design can be easily applied to any existing
retrieval and language models. Furthermore, we show that the LM can be used to
supervise the retrieval model, which can then find documents that help the LM
make better predictions. Our experiments demonstrate that REPLUG with the tuned
retriever significantly improves the performance of GPT-3 (175B) on language
modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by
5.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00871">
<div class="article-summary-box-inner">
<span><p>While large neural-based conversational models have become increasingly
proficient dialogue agents, recent work has highlighted safety issues with
these systems. For example, these systems can be goaded into generating toxic
content, which often perpetuates social biases or stereotypes. We investigate a
retrieval-based method for reducing bias and toxicity in responses from
chatbots. It uses in-context learning to steer a model towards safer
generations. Concretely, to generate a response to an unsafe dialogue context,
we retrieve demonstrations of safe responses to similar dialogue contexts. We
find our method performs competitively with strong baselines without requiring
training. For instance, using automatic evaluation, we find our best fine-tuned
baseline only generates safe responses to unsafe dialogue contexts from
DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking
procedure which can further improve response safeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model. (arXiv:2302.08150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08150">
<div class="article-summary-box-inner">
<span><p>We use both Bayesian and neural models to dissect a data set of Chinese
learners' pre- and post-interventional responses to two tests measuring their
understanding of English prepositions. The results mostly replicate previous
findings from frequentist analyses and newly reveal crucial interactions
between student ability, task type, and stimulus sentence. Given the sparsity
of the data as well as high diversity among learners, the Bayesian method
proves most useful; but we also see potential in using language model
probabilities as predictors of grammaticality and learnability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10724">
<div class="article-summary-box-inner">
<span><p>OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and
revolutionized the approach in artificial intelligence to human-model
interaction. Several publications on ChatGPT evaluation test its effectiveness
on well-known natural language processing (NLP) tasks. However, the existing
studies are mostly non-automated and tested on a very limited scale. In this
work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,
most of them subjective even to humans, such as sentiment analysis, emotion
recognition, offensiveness, and stance detection. In contrast, the other tasks
require more objective reasoning like word sense disambiguation, linguistic
acceptability, and question answering. We also evaluated GPT-4 model on five
selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process
and analyzed more than 49k responses. Our comparison of its results with
available State-of-the-Art (SOTA) solutions showed that the average loss in
quality of the ChatGPT model was about 25% for zero-shot and few-shot
evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower
than for ChatGPT. We showed that the more difficult the task (lower SOTA
performance), the higher the ChatGPT loss. It especially refers to pragmatic
NLP problems like emotion recognition. We also tested the ability to
personalize ChatGPT responses for selected subjective tasks via Random
Contextual Few-Shot Personalization, and we obtained significantly better
user-based predictions. Additional qualitative analysis revealed a ChatGPT
bias, most likely due to the rules imposed on human trainers by OpenAI. Our
results provide the basis for a fundamental discussion of whether the high
quality of recent predictive NLP models can indicate a tool's usefulness to
society and how the learning and validation procedures for such systems should
be established.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12246">
<div class="article-summary-box-inner">
<span><p>The increasing scale of large language models (LLMs) brings emergent
abilities to various complex tasks requiring reasoning, such as arithmetic and
commonsense reasoning. It is known that the effective design of task-specific
prompts is critical for LLMs' ability to produce high-quality answers. In
particular, an effective approach for complex question-and-answer tasks is
example-based prompting with chain-of-thought (CoT) reasoning, which
significantly improves the performance of LLMs. However, current CoT methods
rely on a fixed set of human-annotated exemplars, which are not necessarily the
most effective examples for different tasks. This paper proposes a new method,
Active-Prompt, to adapt LLMs to different tasks with task-specific example
prompts (annotated with human-designed CoT reasoning). For this purpose, we
propose a solution to the key problem of determining which questions are the
most important and helpful ones to annotate from a pool of task-specific
queries. By borrowing ideas from the related problem of uncertainty-based
active learning, we introduce several metrics to characterize the uncertainty
so as to select the most uncertain questions for annotation. Experimental
results demonstrate the superiority of our proposed method, achieving
state-of-the-art on eight complex reasoning tasks. Further analyses of
different uncertainty metrics, pool sizes, zero-shot learning, and
accuracy-uncertainty relationship demonstrate the effectiveness of our method.
Our code will be available at https://github.com/shizhediao/active-prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUX-PLMs: Data Multiplexing for High-throughput Language Models. (arXiv:2302.12441v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12441">
<div class="article-summary-box-inner">
<span><p>The widespread adoption of large language models such as ChatGPT and Bard has
led to unprecedented demand for these technologies. The burgeoning cost of
inference for ever-increasing model sizes coupled with hardware shortages has
limited affordable access and poses a pressing need for efficiency approaches
geared towards high throughput and performance. Multi-input multi-output (MIMO)
algorithms such as data multiplexing, offer a promising solution with a
many-fold increase in throughput by performing inference for multiple inputs at
the cost of a single input. Yet these approaches are not currently performant
enough to be deployed in modern systems. We change that by developing MUX-PLMs,
a class of high throughput pre-trained language models (PLMs) trained with data
multiplexing, that can be fine-tuned for any downstream task to yield
high-throughput high-performance. Our novel multiplexing and demultiplexing
modules proficiently entangle and disentangle inputs, and enable
high-performance high throughput \muxplms{} that are competitive with vanilla
PLMs while achieving 2x/5x inference speedup with only a $1-4\%$ drop on a
broad suite of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaScore: Story Evaluation with Perturbations. (arXiv:2303.08991v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08991">
<div class="article-summary-box-inner">
<span><p>Numerous evaluation metrics have been developed for natural language
generation tasks but their effectiveness in evaluating stories is limited as
they are not specifically tailored to assess intricate story aspects such as
fluency and interestingness. In this paper, we propose Deltascore, an approach
that utilizes perturbation to evaluate fine-grained story aspects. Our core
hypothesis is that the better the story performs in a specific aspect (e.g.,
fluency), the more it will be affected by a particular perturbation (e.g.,
introducing typos). To measure the impact, we calculate the \textit{likelihood
difference} between the pre- and post-perturbation using large pre-trained
language models. We evaluate Deltascore against a suite of current metrics
across two story domains, and investigate its correlation with human judgments
on five fine-grained story aspects: fluency, coherence, relatedness,
logicality, and interestingness. Deltascore performs very strongly, with a
surprise observation that one particular perturbation works very well for
capturing multiple aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16281">
<div class="article-summary-box-inner">
<span><p>Contrary to Google Search's mission of delivering information from "many
angles so you can form your own understanding of the world," we find that
Google and its most prominent returned results - Wikipedia and YouTube - simply
reflect a narrow set of cultural stereotypes tied to the search language for
complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and
"America." Simply stated, they present, to varying degrees, distinct
information across the same search in different languages, a phenomenon we call
'language bias.' This paper presents evidence and analysis of language bias and
discusses its larger social implications. Instead of presenting a global
picture of a complex topic, our online searches and emerging tools like ChatGPT
turn us into the proverbial blind person touching a small portion of an
elephant, ignorant of the existence of other cultural perspectives. Piecing
together a genuine depiction of the elephant is a challenging and important
endeavor that will require collaborative efforts from scholars in both the
humanities and technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. (arXiv:2304.01982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01982">
<div class="article-summary-box-inner">
<span><p>Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]
allow token-level interactions between queries and documents, and hence achieve
state of the art on many information retrieval benchmarks. However, their
non-linear scoring function cannot be scaled to millions of documents,
necessitating a three-stage process for inference: retrieving initial
candidates via token retrieval, accessing all token vectors, and scoring the
initial candidate documents. The non-linear scoring function is applied over
all token vectors of each candidate document, making the inference process
complicated and slow. In this paper, we aim to simplify the multi-vector
retrieval by rethinking the role of token retrieval. We present XTR,
ConteXtualized Token Retriever, which introduces a simple, yet novel, objective
function that encourages the model to retrieve the most important document
tokens first. The improvement to token retrieval allows XTR to rank candidates
only using the retrieved tokens rather than all tokens in the document, and
enables a newly designed scoring stage that is two-to-three orders of magnitude
cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the
state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis
confirms our decision to revisit the token retrieval stage, as XTR demonstrates
much better recall of the token retrieval stage compared to ColBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affect as a proxy for literary mood. (arXiv:2304.02894v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02894">
<div class="article-summary-box-inner">
<span><p>We propose to use affect as a proxy for mood in literary texts. In this
study, we explore the differences in computationally detecting tone versus
detecting mood. Methodologically we utilize affective word embeddings to look
at the affective distribution in different text segments. We also present a
simple yet efficient and effective method of enhancing emotion lexicons to take
both semantic shift and the domain of the text into account producing
real-world congruent results closely matching both contemporary and modern
qualitative analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03245">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are competitive with the state of the art on a
wide range of sentence-level translation datasets. However, their ability to
translate paragraphs and documents remains unexplored because evaluation in
these settings is costly and difficult. We show through a rigorous human
evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an
entire literary paragraph (e.g., from a novel) at once results in
higher-quality translations than standard sentence-by-sentence translation
across 18 linguistically-diverse language pairs (e.g., translating into and out
of Japanese, Polish, and English). Our evaluation, which took approximately 350
hours of effort for annotation and analysis, is conducted by hiring translators
fluent in both the source and target language and asking them to provide both
span-level error annotations as well as preference judgments of which system's
translations are better. We observe that discourse-level LLM translators commit
fewer mistranslations, grammar errors, and stylistic inconsistencies than
sentence-level approaches. With that said, critical errors still abound,
including occasional content omissions, and a human translator's intervention
remains necessary to ensure that the author's voice remains intact. We publicly
release our dataset and error annotations to spur future research on evaluation
of document-level literary translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11090">
<div class="article-summary-box-inner">
<span><p>The release of ChatGPT, Bard, and other large language model (LLM)-based
chatbots has drawn huge attention on foundations models worldwide. There is a
growing trend that foundation models will serve as the fundamental building
blocks for most of the future AI systems. However, incorporating foundation
models in AI systems raises significant concerns about responsible AI due to
their black box nature and rapidly advancing super-intelligence. Additionally,
the foundation model's growing capabilities can eventually absorb the other
components of AI systems, introducing the moving boundary and interface
evolution challenges in architecture design. To address these challenges, this
paper proposes a pattern-oriented responsible-AI-by-design reference
architecture for designing foundation model-based AI systems. Specially, the
paper first presents an architecture evolution of AI systems in the era of
foundation models, from "foundation-model-as-a-connector" to
"foundation-model-as-a-monolithic architecture". The paper then identifies the
key design decision points and proposes a pattern-oriented reference
architecture to provide reusable responsible-AI-by-design architectural
solutions to address the new architecture evolution and responsible AI
challenges. The patterns can be embedded as product features of foundation
model-based AI systems and can enable organisations to capitalise on the
potential of foundation models while minimising associated risks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v3 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11957">
<div class="article-summary-box-inner">
<span><p>The potential of large language models in medicine for education and decision
making purposes has been demonstrated as they achieve decent scores on medical
exams such as the United States Medical Licensing Exam (USMLE) and the MedQA
exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized
field of radiation oncology using the 38th American College of Radiology (ACR)
radiation oncology in-training (TXIT) exam and the 2022 Red Journal gray zone
cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of
63.65% and 74.57%, respectively, highlighting the advantage of the latest
ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in
radiation oncology are identified to some extent. Specifically, ChatGPT-4
demonstrates good knowledge of statistics, CNS &amp; eye, pediatrics, biology, and
physics but has limitations in bone &amp; soft tissue and gynecology, as per the
ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs well in
diagnosis, prognosis, and toxicity but lacks proficiency in topics related to
brachytherapy and dosimetry, as well as in-depth questions from clinical
trials. For the gray zone cases, ChatGPT-4 is able to suggest a personalized
treatment approach to each case with high correctness and comprehensiveness.
Most importantly, it provides novel treatment aspects for many cases, which are
not suggested by any human experts. Both evaluations demonstrate the potential
of ChatGPT-4 in medical education for the general public and cancer patients,
as well as the potential to aid clinical decision-making, while acknowledging
its limitations in certain domains. Because of the risk of hallucination, facts
provided by ChatGPT always need to be verified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13007">
<div class="article-summary-box-inner">
<span><p>Modern systems for multi-hop question answering (QA) typically break
questions into a sequence of reasoning steps, termed chain-of-thought (CoT),
before arriving at a final answer. Often, multiple chains are sampled and
aggregated through a voting mechanism over the final answers, but the
intermediate steps themselves are discarded. While such approaches improve
performance, they do not consider the relations between intermediate steps
across chains and do not provide a unified explanation for the predicted
answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts
large language models to meta-reason over multiple chains of thought, rather
than aggregating their answers. MCR examines different reasoning chains, mixes
information between them and selects the most relevant facts in generating an
explanation and predicting the answer. MCR outperforms strong baselines on 7
multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations
exhibit high quality, enabling humans to verify its answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02783">
<div class="article-summary-box-inner">
<span><p>The recent improvement in code generation capabilities due to the use of
large language models has mainly benefited general purpose programming
languages. Domain specific languages, such as the ones used for IT Automation,
have received far less attention, despite involving many active developers and
being an essential component of modern cloud platforms. This work focuses on
the generation of Ansible-YAML, a widely used markup language for IT
Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code
generation tool, aimed at improving IT automation productivity. Ansible Wisdom
is a transformer-based model, extended by training with a new dataset
containing Ansible-YAML. We also develop two novel performance metrics for YAML
and Ansible to capture the specific characteristics of this domain. Results
show that Ansible Wisdom can accurately generate Ansible script from natural
language prompts with performance comparable or better than existing state of
the art code generation models. In few-shot settings we asses the impact of
training with Ansible, YAML data and compare with different baselines including
Codex-Davinci-002. We also show that after finetuning, our Ansible specific
model (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4)
model, which was evaluated in few shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03695">
<div class="article-summary-box-inner">
<span><p>Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGR: Multi-generator based Rationalization. (arXiv:2305.04492v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04492">
<div class="article-summary-box-inner">
<span><p>Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting Natural in Natural Language Processing. (arXiv:2305.04572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04572">
<div class="article-summary-box-inner">
<span><p>Human language is firstly spoken and only secondarily written. Text, however,
is a very convenient and efficient representation of language, and modern
civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly
focused on processing written rather than spoken language. Work on spoken
language, on the other hand, has been siloed off within the largely separate
speech processing community which has been inordinately preoccupied with
transcribing speech into text. Recent advances in deep learning have led to a
fortuitous convergence in methods between speech processing and mainstream NLP.
Arguably, the time is ripe for a unification of these two fields, and for
starting to take spoken language seriously as the primary mode of human
communication. Truly natural language processing could lead to better
integration with the rest of language science and could lead to systems which
are more data-efficient and more human-like, and which can communicate beyond
the textual modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06349">
<div class="article-summary-box-inner">
<span><p>Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model's apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model's parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to use the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING's performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06626">
<div class="article-summary-box-inner">
<span><p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences in opinion across groups, not
noise. Thus, a crucial problem in hate speech detection is determining whether
a statement is offensive to the demographic group that it targets, when that
group may constitute a small fraction of the annotator pool. We construct a
model that predicts individual annotator ratings on potentially offensive text
and combines this information with the predicted target group of the text to
model the opinions of target group members. We show gains across a range of
metrics, including raising performance over the baseline by 22% at predicting
individual annotators' ratings and by 33% at predicting variance among
annotators, which provides a metric for model uncertainty downstream. We find
that annotator ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebCPM: Interactive Web Search for Chinese Long-form Question Answering. (arXiv:2305.06849v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06849">
<div class="article-summary-box-inner">
<span><p>Long-form question answering (LFQA) aims at answering complex, open-ended
questions with detailed, paragraph-length responses. The de facto paradigm of
LFQA necessitates two procedures: information retrieval, which searches for
relevant supporting facts, and information synthesis, which integrates these
facts into a coherent answer. In this paper, we introduce WebCPM, the first
Chinese LFQA dataset. One unique feature of WebCPM is that its information
retrieval is based on interactive web search, which engages with a search
engine in real time. Following WebGPT, we develop a web search interface. We
recruit annotators to search for relevant information using our interface and
then answer questions. Meanwhile, the web search behaviors of our annotators
would be recorded. In total, we collect 5,500 high-quality question-answer
pairs, together with 14,315 supporting facts and 121,330 web search actions. We
fine-tune pre-trained language models to imitate human behaviors for web search
and to generate answers based on the collected facts. Our LFQA pipeline, built
on these fine-tuned models, generates answers that are no worse than
human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07507">
<div class="article-summary-box-inner">
<span><p>In this work, we conduct a detailed analysis on the performance of
legal-oriented pre-trained language models (PLMs). We examine the interplay
between their original objective, acquired knowledge, and legal language
understanding capacities which we define as the upstream, probing, and
downstream performance, respectively. We consider not only the models' size but
also the pre-training corpora used as important dimensions in our study. To
this end, we release a multinational English legal corpus (LeXFiles) and a
legal knowledge probing benchmark (LegalLAMA) to facilitate training and
detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained
on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We
find that probing performance strongly correlates with upstream performance in
related legal topics. On the other hand, downstream performance is mainly
driven by the model's size and prior legal knowledge which can be estimated by
upstream and probing performance. Based on these findings, we can conclude that
both dimensions are important for those seeking the development of
domain-specific PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot. (arXiv:2305.09758v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09758">
<div class="article-summary-box-inner">
<span><p>Multimedia content, such as advertisements and story videos, exhibit a rich
blend of creativity and multiple modalities. They incorporate elements like
text, visuals, audio, and storytelling techniques, employing devices like
emotions, symbolism, and slogans to convey meaning. While previous research in
multimedia understanding has focused mainly on videos with specific actions
like cooking, there is a dearth of large annotated training datasets, hindering
the development of supervised learning models with satisfactory performance for
real-world applications. However, the rise of large language models (LLMs) has
witnessed remarkable zero-shot performance in various natural language
processing (NLP) tasks, such as emotion classification, question-answering, and
topic classification. To bridge this performance gap in multimedia
understanding, we propose verbalizing story videos to generate their
descriptions in natural language and then performing video-understanding tasks
on the generated story as opposed to the original video. Through extensive
experiments on five video-understanding tasks, we demonstrate that our method,
despite being zero-shot, achieves significantly better results than supervised
baselines for video understanding. Further, alleviating a lack of story
understanding benchmarks, we publicly release the first dataset on a crucial
task in computational social science, persuasion strategy identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10276">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning (NLP) composed of a set of
novel tasks: Brick World, NLVR-based Manipulations, and Natural Language
Navigation. We found that current popular LLMs such as ChatGPT still lack
abilities in complex planning. This arises a question -- do the LLMs have a
good understanding of the environments described in natural language, or maybe
other alternatives such as symbolic representations are neater and hence better
to be understood by LLMs? To this end, we propose a novel method called CoS
(Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10355">
<div class="article-summary-box-inner">
<span><p>Inspired by the superior language abilities of large language models (LLM),
large vision-language models (LVLM) have been recently explored by integrating
powerful LLMs for improving the performance on complex multimodal tasks.
Despite the promising progress on LVLMs, we find that LVLMs suffer from the
hallucination problem, i.e. they tend to generate objects that are inconsistent
with the target images in the descriptions. To investigate it, this work
presents the first systematic study on object hallucination of LVLMs. We
conduct the evaluation experiments on several representative LVLMs, and show
that they mostly suffer from severe object hallucination issue. We further
discuss that the visual instructions may influence the hallucination, and find
that: objects that frequently occur in the visual instructions or co-occur with
the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we
find that existing evaluation methods might be affected by the input
instructions and generation styles of LVLMs. Thus, we further design an
improved evaluation method for object hallucination by proposing a
polling-based query method called POPE. Experiment results demonstrate that our
POPE can evaluate the object hallucination in a more stable and flexible way.
Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10588">
<div class="article-summary-box-inner">
<span><p>Estimating the log-likelihood of a given sentence under an autoregressive
language model is straightforward: one can simply apply the chain rule and sum
the log-likelihood values for each successive token. However, for masked
language models (MLMs), there is no direct way to estimate the log-likelihood
of a sentence. To address this issue, Salazar et al. (2020) propose to estimate
sentence pseudo-log-likelihood (PLL) scores, computed by successively masking
each sentence token, retrieving its score using the rest of the sentence as
context, and summing the resulting values. Here, we demonstrate that the
original PLL method yields inflated scores for out-of-vocabulary words and
propose an adapted metric, in which we mask not only the target token, but also
all within-word tokens to the right of the target. We show that our adapted
metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric
in which all within-word tokens are masked. In particular, it better satisfies
theoretical desiderata and better correlates with scores from autoregressive
models. Finally, we show that the choice of metric affects even tightly
controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring
the importance of selecting an appropriate scoring metric for evaluating MLM
properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10626">
<div class="article-summary-box-inner">
<span><p>While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical elastic
weight consolidation (EWC) for selective weight updates, combined with low-rank
adapters (LoRA) for training efficiency. Extensive experiments show our
approach substantially improves base LMs on 18 downstream tasks by 64.28% on
average. In particular, the small LMs (1.3B and 6B) enhanced by our approach
match or even outperform much larger LMs (e.g., ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11038">
<div class="article-summary-box-inner">
<span><p>Named entity recognition in real-world applications suffers from the
diversity of entity types, the emergence of new entity types, and the lack of
high-quality annotations. To address the above problems, this paper proposes an
in-context learning-based NER approach, which can effectively inject in-context
NER ability into PLMs and recognize entities of novel types on-the-fly using
only a few demonstrative instances. Specifically, we model PLMs as a
meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}.
M}$, and a new entity extractor can be implicitly constructed by applying new
instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M)
}$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be
a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the
above in-context NER ability into PLMs, we propose a meta-function pre-training
algorithm, which pre-trains PLMs by comparing the (instruction,
demonstration)-initialized extractor with a surrogate golden extractor.
Experimental results on 4 few-shot NER datasets show that our method can
effectively inject in-context NER ability into PLMs and significantly
outperforms the PLMs+fine-tuning counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses. (arXiv:2305.11662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11662">
<div class="article-summary-box-inner">
<span><p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11853">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with in-context learning have demonstrated
remarkable capability in the text-to-SQL task. Previous research has prompted
LLMs with various demonstration-retrieval strategies and intermediate reasoning
steps to enhance the performance of LLMs. However, those works often employ
varied strategies when constructing the prompt text for text-to-SQL inputs,
such as databases and demonstration examples. This leads to a lack of
comparability in both the prompt constructions and their primary contributions.
Furthermore, selecting an effective prompt construction has emerged as a
persistent problem for future research. To address this limitation, we
comprehensively investigate the impact of prompt constructions across various
settings and provide insights for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hedges in Bidirectional Translations of Publicity-Oriented Documents. (arXiv:2305.12146v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12146">
<div class="article-summary-box-inner">
<span><p>Hedges are widely studied across registers and disciplines, yet research on
the translation of hedges in political texts is extremely limited. This
contrastive study is dedicated to investigating whether there is a diachronic
change in the frequencies of hedging devices in the target texts, to what
extent the changing frequencies of translated hedges through years are
attributed to the source texts, and what translation strategies are adopted to
deal with them. For the purposes of this research, two types of official
political texts and their translations from China and the United Nations were
collected to form three sub-corpora. Results show that hedges tend to appear
more frequently in English political texts, be it original English or
translated English. In addition, directionality seems to play an important role
in influencing both the frequencies and translation strategies regarding the
use of hedges. A noticeable diachronic increase of hedging devices is also
observed in our corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12474">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated remarkable performance across various
natural language processing tasks; however, their efficacy in more challenging
and domain-specific tasks remains less explored. This paper introduces the
GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions
from the Chinese Gaokao examination as test samples for evaluating large
language models.In order to align the evaluation results with humans as much as
possible, we designed a method based on zero-shot prompts to analyze the
accuracy and scoring rate of the model by dividing the questions into
subjective and objective types. We evaluated the ChatGPT model on
GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels
in tackling objective questions, while also shedding light on its shortcomings
and areas for improvement. To further scrutinize the model's responses, we
incorporate human evaluations.In conclusion, this research contributes a robust
evaluation benchmark for future large-scale language models and offers valuable
insights into the limitations of such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12532">
<div class="article-summary-box-inner">
<span><p>Automated text simplification aims to produce simple versions of complex
texts. This task is especially useful in the medical domain, where the latest
medical findings are typically communicated via complex and technical articles.
This creates barriers for laypeople seeking access to up-to-date medical
findings, consequently impeding progress on health literacy. Most existing work
on medical text simplification has focused on monolingual settings, with the
result that such evidence would be available only in just one language (most
often, English). This work addresses this limitation via multilingual
simplification, i.e., directly simplifying complex texts into simplified texts
in multiple languages. We introduce MultiCochrane, the first sentence-aligned
multilingual text simplification dataset for the medical domain in four
languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and
zero-shot models across these languages, with extensive human assessments and
analyses. Although models can now generate viable simplified texts, we identify
outstanding challenges that this dataset might be used to address.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Words: A Comprehensive Survey of Sentence Representations. (arXiv:2305.12641v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12641">
<div class="article-summary-box-inner">
<span><p>Sentence representations have become a critical component in natural language
processing applications, such as retrieval, question answering, and text
classification. They capture the semantics and meaning of a sentence, enabling
machines to understand and reason over human language. In recent years,
significant progress has been made in developing methods for learning sentence
representations, including unsupervised, supervised, and transfer learning
approaches. In this paper, we provide an overview of the different methods for
sentence representation learning, including both traditional and deep
learning-based techniques. We provide a systematic organization of the
literature on sentence representation learning, highlighting the key
contributions and challenges in this area. Overall, our review highlights the
progress made in sentence representation learning, the importance of this area
in natural language processing, and the challenges that remain. We conclude
with directions for future research, suggesting potential avenues for improving
the quality and efficiency of sentence representations in NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-SW3: An Autoregressive Language Model for the Nordic Languages. (arXiv:2305.12987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12987">
<div class="article-summary-box-inner">
<span><p>This paper details the process of developing the first native large
generative language model for the Nordic languages, GPT-SW3. We cover all parts
of the development process, from data collection and processing, training
configuration and instruction finetuning, to evaluation and considerations for
release strategies. We hope that this paper can serve as a guide and reference
for other researchers that undertake the development of large generative models
for smaller languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations. (arXiv:2305.13235v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13235">
<div class="article-summary-box-inner">
<span><p>Explaining the decisions of neural models is crucial for ensuring their
trustworthiness at deployment time. Using Natural Language Explanations (NLEs)
to justify a model's predictions has recently gained increasing interest.
However, this approach usually demands large datasets of human-written NLEs for
the ground-truth answers, which are expensive and potentially infeasible for
some applications. For models to generate high-quality NLEs when only a few
NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in
conjunction with prompt-based learning recently emerged. However, PLMs
typically have billions of parameters, making fine-tuning expensive. We propose
SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete
prompts to jointly generate predictions and NLEs. We experiment with SparseFit
on the T5 model and four datasets and compare it against state-of-the-art
parameter-efficient fine-tuning techniques. We perform automatic and human
evaluations to assess the quality of the model-generated NLEs, finding that
fine-tuning only 6.8% of the model parameters leads to competitive results for
both the task performance and the quality of the NLEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection. (arXiv:2305.13276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13276">
<div class="article-summary-box-inner">
<span><p>Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v3 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04325">
<div class="article-summary-box-inner">
<span><p>Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-24 23:11:32.688808253 UTC">2023-05-24 23:11:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>