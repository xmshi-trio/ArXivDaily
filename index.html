<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-13T01:30:00Z">01-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings. (arXiv:2301.04704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04704">
<div class="article-summary-box-inner">
<span><p>Adding interpretability to word embeddings represents an area of active
research in text representation. Recent work has explored thepotential of
embedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.
wrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,
and BiImp. Although these approaches provide interpretable dimensions for
words, they have not been designed to deal with polysemy, i.e. they can not
easily distinguish between different senses of words. To address this
limitation, we present SensePOLAR, an extension of the original POLAR framework
that enables word-sense aware interpretability for pre-trained contextual word
embeddings. The resulting interpretable word embeddings achieve a level of
performance that is comparable to original contextual word embeddings across a
variety of natural language processing tasks including the GLUE and SQuAD
benchmarks. Our work removes a fundamental limitation of existing approaches by
offering users sense aware interpretations for contextual word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Web Enabled Geographic Question Answering Framework: GeoTR. (arXiv:2301.04752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04752">
<div class="article-summary-box-inner">
<span><p>With the considerable growth of linked data, researchers have focused on how
to increase the availability of semantic web technologies to provide practical
usages for real life systems. Question answering systems are an example of
real-life systems that communicate directly with end users, understand user
intention and generate answers. End users do not care about the structural
query language or the vocabulary of the knowledge base where the point of a
problem arises. In this study, a question answering framework that converts
Turkish natural language input into SPARQL queries in the geographical domain
is proposed. Additionally, a novel Turkish ontology, which covers a 10th grade
geography lesson named Spatial Synthesis Turkey, has been developed to be used
as a linked data provider. Moreover, a gap in the literature on Turkish
question answering systems, which utilizes linked data in the geographical
domain, is addressed. A hybrid system architecture that combines natural
language processing techniques with linked data technologies to generate
answers is also proposed. Further related research areas are suggested.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04761">
<div class="article-summary-box-inner">
<span><p>Large-scale language model pretraining is a very successful form of
self-supervised learning in natural language processing, but it is increasingly
expensive to perform as the models and pretraining corpora have become larger
over time. We propose NarrowBERT, a modified transformer encoder that increases
the throughput for masked language model pretraining by more than $2\times$.
NarrowBERT sparsifies the transformer model such that the self-attention
queries and feedforward layers only operate on the masked tokens of each
sentence during pretraining, rather than all of the tokens as with the usual
transformer encoder. We also show that NarrowBERT increases the throughput at
inference time by as much as $3.5\times$ with minimal (or no) performance
degradation on sentence encoding tasks like MNLI. Finally, we examine the
performance of NarrowBERT on the IMDB and Amazon reviews classification and
CoNLL NER tasks and show that it is also comparable to standard BERT
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KAER: A Knowledge Augmented Pre-Trained Language Model for Entity Resolution. (arXiv:2301.04770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04770">
<div class="article-summary-box-inner">
<span><p>Entity resolution has been an essential and well-studied task in data
cleaning research for decades. Existing work has discussed the feasibility of
utilizing pre-trained language models to perform entity resolution and achieved
promising results. However, few works have discussed injecting domain knowledge
to improve the performance of pre-trained language models on entity resolution
tasks. In this study, we propose Knowledge Augmented Entity Resolution (KAER),
a novel framework named for augmenting pre-trained language models with
external knowledge for entity resolution. We discuss the results of utilizing
different knowledge augmentation and prompting methods to improve entity
resolution performance. Our model improves on Ditto, the existing
state-of-the-art entity resolution method. In particular, 1) KAER performs more
robustly and achieves better results on "dirty data", and 2) with more general
knowledge injection, KAER outperforms the existing baseline models on the
textual dataset and dataset from the online product domain. 3) KAER achieves
competitive results on highly domain-specific datasets, such as citation
datasets, requiring the injection of expert knowledge in future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Cognition and Language Computation -- Human and Machine Language Understanding. (arXiv:2301.04788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04788">
<div class="article-summary-box-inner">
<span><p>Language understanding is a key scientific issue in the fields of cognitive
and computer science. However, the two disciplines differ substantially in the
specific research questions. Cognitive science focuses on analyzing the
specific mechanism of the brain and investigating the brain's response to
language; few studies have examined the brain's language system as a whole. By
contrast, computer scientists focus on the efficiency of practical applications
when choosing research questions but may ignore the most essential laws of
language. Given these differences, can a combination of the disciplines offer
new insights for building intelligent language models and studying language
cognitive mechanisms? In the following text, we first review the research
questions, history, and methods of language understanding in cognitive and
computer science, focusing on the current progress and challenges. We then
compare and contrast the research of language understanding in cognitive and
computer sciences. Finally, we review existing work that combines insights from
language cognition and language computation and offer prospects for future
development trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Deep Learning. (arXiv:2301.04856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04856">
<div class="article-summary-box-inner">
<span><p>This book is the result of a seminar in which we reviewed multimodal
approaches and attempted to create a solid overview of the field, starting with
the current state-of-the-art approaches in the two subfields of Deep Learning
individually. Further, modeling frameworks are discussed where one modality is
transformed into the other, as well as models in which one modality is utilized
to enhance representation learning for the other. To conclude the second part,
architectures with a focus on handling both modalities simultaneously are
introduced. Finally, we also cover other modalities as well as general-purpose
multi-modal models, which are able to handle different tasks on different
modalities within one unified architecture. One interesting application
(Generative Art) eventually caps off this booklet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues. (arXiv:2301.04871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04871">
<div class="article-summary-box-inner">
<span><p>Maintaining engagement and consistency is particularly important in dialogue
systems. Existing works have improved the performance of dialogue systems by
intentionally learning interlocutor personas with sophisticated network
structures. One issue with this approach is that it requires more personal
corpora with annotations. Additionally, these models typically perform the next
utterance prediction to generate a response but neglect the discourse coherence
in the entire conversation. To address these issues, this study proposes a
method of learning to memorize entailment and discourse relations for
persona-consistent dialogue tasks. Entailment text pairs in natural language
inference dataset were applied to learn latent entailment relations as external
memories by premise-to-hypothesis generation task. Furthermore, an internal
memory with a similar architecture was applied to the discourse information in
the dialogue. Placing orthogonality restrictions on these two memory spaces
ensures that the latent entailment relations remain dialogue-independent. Both
memories collaborate to obtain entailment and discourse representation for the
generation, allowing a deeper understanding of both consistency and coherence.
Experiments on two large public datasets, PersonaChat and DSTC7-AVSD,
demonstrated the effectiveness of the proposed method. Both automatic and human
evaluations indicate that the proposed model outperforms several strong
baselines in terms of both persona consistency and response coherence. Our
source code is available at https://github.com/Chenrj233/LMEDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images. (arXiv:2301.04883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04883">
<div class="article-summary-box-inner">
<span><p>Visual question answering on document images that contain textual, visual,
and layout information, called document VQA, has received much attention
recently. Although many datasets have been proposed for developing document VQA
systems, most of the existing datasets focus on understanding the content
relationships within a single image and not across multiple images. In this
study, we propose a new multi-image document VQA dataset, SlideVQA, containing
2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a
slide deck. SlideVQA requires complex reasoning, including single-hop,
multi-hop, and numerical reasoning, and also provides annotated arithmetic
expressions of numerical answers for enhancing the ability of numerical
reasoning. Moreover, we developed a new end-to-end document VQA model that
treats evidence selection and question answering in a unified
sequence-to-sequence format. Experiments on SlideVQA show that our model
outperformed existing state-of-the-art QA models, but that it still has a large
gap behind human performance. We believe that our dataset will facilitate
research on document VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04907">
<div class="article-summary-box-inner">
<span><p>Towards human-like dialogue systems, current emotional dialogue approaches
jointly model emotion and semantics with a unified neural network. This
strategy tends to generate safe responses due to the mutual restriction between
emotion and semantics, and requires rare emotion-annotated large-scale dialogue
corpus. Inspired by the "think twice" behavior in human dialogue, we propose a
two-stage conversational agent for the generation of emotional dialogue.
Firstly, a dialogue model trained without the emotion-annotated dialogue corpus
generates a prototype response that meets the contextual semantics. Secondly,
the first-stage prototype is modified by a controllable emotion refiner with
the empathy hypothesis. Experimental results on the DailyDialog and
EmpatheticDialogues datasets demonstrate that the proposed conversational
outperforms the comparison models in emotion generation and maintains the
semantic performance in automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset of Kurdish (Sorani) Named Entities -- An Amendment to Kurdish-BLARK Named Entities. (arXiv:2301.04962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04962">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is one of the essential applications of
Natural Language Processing (NLP). It is also an instrument that plays a
significant role in many other NLP applications, such as Machine Translation
(MT), Information Retrieval (IR), and Part of Speech Tagging (POST). Kurdish is
an under-resourced language from the NLP perspective. Particularly, in all the
categories, the lack of NER resources hinders other aspects of Kurdish
processing. In this work, we present a data set that covers several categories
of NEs in Kurdish (Sorani). The dataset is a significant amendment to a
previously developed dataset in the Kurdish BLARK (Basic Language Resource
Kit). It covers 11 categories and 33261 entries in total. The dataset is
publicly available for non-commercial use under CC BY-NC-SA 4.0 license at
https://kurdishblark.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information. (arXiv:2301.05036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05036">
<div class="article-summary-box-inner">
<span><p>In NLP annotation, it is common to have multiple annotators label the text
and then obtain the ground truth labels based on the agreement of major
annotators. However, annotators are individuals with different backgrounds, and
minors' opinions should not be simply ignored. As annotation tasks become
subjective and topics are controversial in modern NLP tasks, we need NLP
systems that can represent people's diverse voices on subjective matters and
predict the level of diversity. This paper examines whether the text of the
task and annotators' demographic background information can be used to estimate
the level of disagreement among annotators. Particularly, we extract
disagreement labels from the annotators' voting histories in the five
subjective datasets, and then fine-tune language models to predict annotators'
disagreement. Our results show that knowing annotators' demographic
information, like gender, ethnicity, and education level, helps predict
disagreements. In order to distinguish the disagreement from the inherent
controversy from text content and the disagreement in the annotators' different
perspectives, we simulate everyone's voices with different combinations of
annotators' artificial demographics and examine its variance of the finetuned
disagreement predictor. Our paper aims to improve the annotation process for
more efficient and inclusive NLP systems through a novel disagreement
prediction mechanism. Our code and dataset are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cognitive Evaluation of Instruction Generation Agents tl;dr They Need Better Theory-of-Mind Capabilities. (arXiv:2301.05149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05149">
<div class="article-summary-box-inner">
<span><p>We mathematically characterize the cognitive capabilities that enable humans
to effectively guide others through natural language. We show that
neural-network-based instruction generation agents possess similar cognitive
capabilities, and design an evaluation scheme for probing those capabilities.
Our results indicate that these agents, while capable of effectively narrowing
the search space, poorly predict the listener's interpretations of their
instructions and thus often fail to select the best instructions even from a
small candidate set. We augment the agents with better theory-of-mind models of
the listener and obtain significant performance boost in guiding real humans.
Yet, there remains a considerable gap between our best agent and human guides.
We discuss the challenges in closing this gap, emphasizing the need to
construct better models of human behavior when interacting with AI-based
agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Question Duplicate and Related Questions Detection in e-learning platforms. (arXiv:2301.05150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05150">
<div class="article-summary-box-inner">
<span><p>Online learning platforms provide diverse questions to gauge the learners'
understanding of different concepts. The repository of questions has to be
constantly updated to ensure a diverse pool of questions to conduct assessments
for learners. However, it is impossible for the academician to manually skim
through the large repository of questions to check for duplicates when
onboarding new questions from external sources. Hence, we propose a tool QDup
in this paper that can surface near-duplicate and semantically related
questions without any supervised data. The proposed tool follows an
unsupervised hybrid pipeline of statistical and neural approaches for
incorporating different nuances in similarity for the task of question
duplicate detection. We demonstrate that QDup can detect near-duplicate
questions and also suggest related questions for practice with remarkable
accuracy and speed from a large repository of questions. The demo video of the
tool can be found at https://www.youtube.com/watch?v=loh0_-7XLW4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Adaptation for French Named Entity Recognition. (arXiv:2301.05220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05220">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is the task of identifying and classifying
named entities in large-scale texts into predefined classes. NER in French and
other relatively limited-resource languages cannot always benefit from
approaches proposed for languages like English due to a dearth of large, robust
datasets. In this paper, we present our work that aims to mitigate the effects
of this dearth of large, labeled datasets. We propose a Transformer-based NER
approach for French, using adversarial adaptation to similar domain or general
corpora to improve feature extraction and enable better generalization. Our
approach allows learning better features using large-scale unlabeled corpora
from the same domain or mixed domains to introduce more variations during
training and reduce overfitting. Experimental results on three labeled datasets
show that our adaptation framework outperforms the corresponding non-adaptive
models for various combinations of Transformer models, source datasets, and
target corpora. We also show that adversarial adaptation to large-scale
unlabeled corpora can help mitigate the performance dip incurred on using
Transformer models pre-trained on smaller corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning. (arXiv:2301.05226v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05226">
<div class="article-summary-box-inner">
<span><p>Large pre-trained vision and language models have demonstrated remarkable
capacities for various tasks. However, solving the knowledge-based visual
reasoning tasks remains challenging, which requires a model to comprehensively
understand image content, connect the external world knowledge, and perform
step-by-step reasoning to answer the questions correctly. To this end, we
propose a novel framework named Interactive Prompting Visual Reasoner (IPVR)
for few-shot knowledge-based visual reasoning. IPVR contains three stages, see,
think and confirm. The see stage scans the image and grounds the visual concept
candidates with a visual perception model. The think stage adopts a pre-trained
large language model (LLM) to attend to the key concepts from candidates
adaptively. It then transforms them into text context for prompting with a
visual captioning model and adopts the LLM to generate the answer. The confirm
stage further uses the LLM to generate the supporting rationale to the answer,
verify the generated rationale with a cross-modality classifier and ensure that
the rationale can infer the predicted output consistently. We conduct
experiments on a range of knowledge-based visual reasoning datasets. We found
our IPVR enjoys several benefits, 1). it achieves better performance than the
previous few-shot learning baselines; 2). it enjoys the total transparency and
trustworthiness of the whole reasoning process by providing rationales for each
reasoning step; 3). it is computation-efficient compared with other fine-tuning
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Transformers with Natural Language Explanations. (arXiv:2110.00125v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00125">
<div class="article-summary-box-inner">
<span><p>Transformers changed modern NLP in many ways. However, like many other neural
architectures, they are still weak on exploiting domain knowledge and on
interpretability. Unfortunately, the exploitation of external, structured
knowledge is notoriously prone to a knowledge acquisition bottleneck. We thus
propose a memory enhancement of transformer models that makes use of
unstructured knowledge. That, expressed in plain text, can be used to carry out
classification tasks and as a source of explanations for the model output. An
experimental evaluation conducted on two challenging datasets demonstrates that
our approach produces relevant explanations without losing in performance.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-15 23:12:49.879812743 UTC">2023-01-15 23:12:49 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>