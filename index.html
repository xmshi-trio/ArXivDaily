<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-02T01:30:00Z">08-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00002">
<div class="article-summary-box-inner">
<span><p>Temporal commonsense reasoning refers to the ability to understand the
typical temporal context of phrases, actions, and events, and use it to reason
over problems requiring such knowledge. This trait is essential in temporal
natural language processing tasks, with possible applications such as timeline
summarization, temporal question answering, and temporal natural language
inference. Recent research on the performance of large language models suggests
that, although they are adept at generating syntactically correct sentences and
solving classification tasks, they often take shortcuts in their reasoning and
fall prey to simple linguistic traps. This article provides an overview of
research in the domain of temporal commonsense reasoning, particularly focusing
on enhancing language model performance through a variety of augmentations and
their evaluation across a growing number of datasets. However, these augmented
models still struggle to approach human performance on reasoning tasks over
temporal common sense properties, such as the typical occurrence times,
orderings, or durations of events. We further emphasize the need for careful
interpretation of research to guard against overpromising evaluation results in
light of the shallow reasoning present in transformers. This can be achieved by
appropriately preparing datasets and suitable evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00014">
<div class="article-summary-box-inner">
<span><p>Which technological linkages affect the sector's ability to innovate? How do
these effects transmit through the technology space? This paper answers these
two key questions using novel methods of text mining and network analysis. We
examine technological interdependence across sectors over a period of half a
century (from 1976 to 2021) by analyzing the text of 6.5 million patents
granted by the United States Patent and Trademark Office (USPTO), and applying
network analysis to uncover the full spectrum of linkages existing across
technology areas. We demonstrate that patent text contains a wealth of
information often not captured by traditional innovation metrics, such as
patent citations. By using network analysis, we document that indirect linkages
are as important as direct connections and that the former would remain mostly
hidden using more traditional measures of indirect linkages, such as the
Leontief inverse matrix. Finally, based on an impulse-response analysis, we
illustrate how technological shocks transmit through the technology
(network-based) space, affecting the innovation capacity of the sectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00016">
<div class="article-summary-box-inner">
<span><p>One of the most important tasks in quantitative investment research is mining
new alphas (effective trading signals or factors). Traditional alpha mining
methods, either hand-crafted factor synthesizing or algorithmic factor mining
(e.g., search with genetic programming), have inherent limitations, especially
in implementing the ideas of quants. In this work, we propose a new alpha
mining paradigm by introducing human-AI interaction, and a novel prompt
engineering algorithmic framework to implement this paradigm by leveraging the
power of large language models. Moreover, we develop Alpha-GPT, a new
interactive alpha mining system framework that provides a heuristic way to
``understand'' the ideas of quant researchers and outputs creative, insightful,
and effective alphas. We demonstrate the effectiveness and advantage of
Alpha-GPT via a number of alpha mining experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00065">
<div class="article-summary-box-inner">
<span><p>Financial risk prediction plays a crucial role in the financial sector.
Machine learning methods have been widely applied for automatically detecting
potential risks and thus saving the cost of labor. However, the development in
this field is lagging behind in recent years by the following two facts: 1) the
algorithms used are somewhat outdated, especially in the context of the fast
advance of generative AI and large language models (LLMs); 2) the lack of a
unified and open-sourced financial benchmark has impeded the related research
for years. To tackle these issues, we propose FinPT and FinBench: the former is
a novel approach for financial risk prediction that conduct Profile Tuning on
large pretrained foundation models, and the latter is a set of high-quality
datasets on financial risks such as default, fraud, and churn. In FinPT, we
fill the financial tabular data into the pre-defined instruction template,
obtain natural-language customer profiles by prompting LLMs, and fine-tune
large foundation models with the profile text to make predictions. We
demonstrate the effectiveness of the proposed FinPT by experimenting with a
range of representative strong baselines on FinBench. The analytical studies
further deepen the understanding of LLMs for financial risk prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00071">
<div class="article-summary-box-inner">
<span><p>Given that language models are trained on vast datasets that may contain
inherent biases, there is a potential danger of inadvertently perpetuating
systemic discrimination. Consequently, it becomes essential to examine and
address biases in language models, integrating fairness into their development
to ensure these models are equitable and free from bias. In this work, we
demonstrate the importance of reasoning in zero-shot stereotype identification
based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from
13B to 33B, we show that the performance gain from reasoning significantly
exceeds the gain from scaling up. Our findings suggest that reasoning could be
a key factor that enables LLMs to trescend the scaling law on out-of-domain
tasks such as stereotype identification. Additionally, through a qualitative
analysis of select reasoning traces, we highlight how reasoning enhances not
just accuracy but also the interpretability of the decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00072">
<div class="article-summary-box-inner">
<span><p>OpenAI's ChatGPT language model has gained popularity as a powerful tool for
complex problem-solving and information retrieval. However, concerns arise
about the reproduction of biases present in the language-specific training
data. In this study, we address this issue in the context of the
Israeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed
an automated query procedure to inquire about casualties in specific
airstrikes, in both Hebrew and Arabic for the former conflict and Turkish and
Kurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\pm$11
percent lower fatality estimates when queried in the language of the attacker
than in the language of the targeted group. Evasive answers denying the
existence of such attacks further increase the discrepancy, creating a novel
bias mechanism not present in regular search engines. This language bias has
the potential to amplify existing media biases and contribute to information
bubbles, ultimately reinforcing conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trustworthiness of Children Stories Generated by Large Language Models. (arXiv:2308.00073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00073">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown a tremendous capacity for generating
literary text. However, their effectiveness in generating children's stories
has yet to be thoroughly examined. In this study, we evaluate the
trustworthiness of children's stories generated by LLMs using various measures,
and we compare and contrast our results with both old and new children's
stories to better assess their significance. Our findings suggest that LLMs
still struggle to generate children's stories at the level of quality and
nuance found in actual stories
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00081">
<div class="article-summary-box-inner">
<span><p>Embedding based Knowledge Graph (KG) Completion has gained much attention
over the past few years. Most of the current algorithms consider a KG as a
multidirectional labeled graph and lack the ability to capture the semantics
underlying the schematic information. In a separate development, a vast amount
of information has been captured within the Large Language Models (LLMs) which
has revolutionized the field of Artificial Intelligence. KGs could benefit from
these LLMs and vice versa. This vision paper discusses the existing algorithms
for KG completion based on the variations for generating KG embeddings. It
starts with discussing various KG completion algorithms such as transductive
and inductive link prediction and entity type prediction algorithms. It then
moves on to the algorithms utilizing type information within the KGs, LLMs, and
finally to algorithms capturing the semantics represented in different
description logic axioms. We conclude the paper with a critical reflection on
the current state of work in the community and give recommendations for future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00085">
<div class="article-summary-box-inner">
<span><p>Recent approaches to empathetic response generation try to incorporate
commonsense knowledge or reasoning about the causes of emotions to better
understand the user's experiences and feelings. However, these approaches
mainly focus on understanding the causalities of context from the user's
perspective, ignoring the system's perspective. In this paper, we propose a
commonsense-based causality explanation approach for diverse empathetic
response generation that considers both the user's perspective (user's desires
and reactions) and the system's perspective (system's intentions and
reactions). We enhance ChatGPT's ability to reason for the system's perspective
by integrating in-context learning with commonsense knowledge. Then, we
integrate the commonsense-based causality explanation with both ChatGPT and a
T5-based model. Experimental evaluations demonstrate that our method
outperforms other comparable methods on both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data. (arXiv:2308.00107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00107">
<div class="article-summary-box-inner">
<span><p>Objectives: To describe the development and validation of a zero-shot
learning natural language processing (NLP) tool for abstracting data from
unstructured text contained within PDF documents, such as those found within
electronic health records. Materials and Methods: A data abstraction tool based
on the GPT-3.5 model from OpenAI was developed and compared to three physician
human abstractors in terms of time to task completion and accuracy for
abstracting data on 14 unique variables from a set of 199 de-identified radical
prostatectomy pathology reports. The reports were processed by the software
tool in vectorized and scanned formats to establish the impact of optical
character recognition on data abstraction. The tool was assessed for
superiority for data abstraction speed and non-inferiority for accuracy.
Results: The human abstractors required a mean of 101s per report for data
abstraction, with times varying from 15 to 284 s. In comparison, the software
tool required a mean of 12.8 s to process the vectorized reports and a mean of
15.8 to process the scanned reports (P &lt; 0.001). The overall accuracies of the
three human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of
2786 datapoints. The software tool had an overall accuracy of 94.2% for the
vectorized reports, proving to be non-inferior to the human abstractors at a
margin of -10% ($\alpha$=0.025). The tool had a slightly lower accuracy of
88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3
human abstractors. Conclusion: The developed zero-shot learning NLP tool
affords researchers comparable levels of accuracy to that of human abstractors,
with significant time savings benefits. Because of the lack of need for
task-specific model training, the developed tool is highly generalizable and
can be used for a wide variety of data abstraction tasks, even outside the
field of medicine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00108">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models such as BERT have contributed
significantly to the development of NLP. However, those models require large
computational resources, making it difficult to be applied to mobile devices
where computing power is limited. In this paper we aim to address the weakness
of existing input-adaptive inference methods which fail to take full advantage
of the structure of BERT. We propose Dynamic Planning in BERT, a novel
fine-tuning strategy that can accelerate the inference process of BERT through
selecting a subsequence of transformer layers list of backbone as a
computational path for an input sample. To do this, our approach adds a
planning module to the original BERT model to determine whether a layer is
included or bypassed during inference. Experimental results on the GLUE
benchmark exhibit that our method reduces latency to 75\% while maintaining
98\% accuracy, yielding a better accuracy-speed trade-off compared to
state-of-the-art input-adaptive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?. (arXiv:2308.00109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00109">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence applications show great potential for
language-related tasks that rely on next-word prediction. The current
generation of large language models have been linked to claims about human-like
linguistic performance and their applications are hailed both as a key step
towards Artificial General Intelligence and as major advance in understanding
the cognitive, and even neural basis of human language. We analyze the
contribution of large language models as theoretically informative
representations of a target system vs. atheoretical powerful mechanistic tools,
and we identify the key abilities that are still missing from the current state
of development and exploitation of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00113">
<div class="article-summary-box-inner">
<span><p>The task of discerning between generated and natural texts is increasingly
challenging. In this context, watermarking emerges as a promising technique for
ascribing generated text to a specific model. It alters the sampling generation
process so as to leave an invisible trace in the generated output, facilitating
later detection. This research consolidates watermarks for large language
models based on three theoretical and empirical considerations. First, we
introduce new statistical tests that offer robust theoretical guarantees which
remain valid even at low false-positive rates (less than 10$^{\text{-6}}$).
Second, we compare the effectiveness of watermarks using classical benchmarks
in the field of natural language processing, gaining insights into their
real-world applicability. Third, we develop advanced detection schemes for
scenarios where access to the LLM is available, as well as multi-bit
watermarking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00116">
<div class="article-summary-box-inner">
<span><p>The Metadata Object Description Schema (MODS) was developed to describe
bibliographic concepts and metadata and is maintained by the Library of
Congress. Its authoritative version is given as an XML schema based on an XML
mindset which means that it has significant limitations for use in a knowledge
graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O)
which incorporates all elements and attributes of the MODS XML schema. In
designing the ontology, we adopt the recent Modular Ontology Design Methodology
(MOMo) with the intention to strike a balance between modularity and quality
ontology design on the one hand, and conservative backward compatibility with
MODS on the other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00121">
<div class="article-summary-box-inner">
<span><p>The field of software security testing, more specifically penetration
testing, is an activity that requires high levels of expertise and involves
many manual testing and analysis steps. This paper explores the potential usage
of large-language models, such as GPT3.5, to augment penetration testers with
AI sparring partners. We explore the feasibility of supplementing penetration
testers with AI models for two distinct use cases: high-level task planning for
security testing assignments and low-level vulnerability hunting within a
vulnerable virtual machine. For the latter, we implemented a closed-feedback
loop between LLM-generated low-level actions with a vulnerable virtual machine
(connected through SSH) and allowed the LLM to analyze the machine state for
vulnerabilities and suggest concrete attack vectors which were automatically
executed within the virtual machine. We discuss promising initial results,
detail avenues for improvement, and close deliberating on the ethics of
providing AI-based sparring partners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00129">
<div class="article-summary-box-inner">
<span><p>This thesis focuses on representation learning for sequence data over time or
space, aiming to improve downstream sequence prediction tasks by using the
learned representations. Supervised learning has been the most dominant
approach for training deep neural networks for learning good sequential
representations. However, one limiting factor to scale supervised learning is
the lack of enough annotated data. Motivated by this challenge, it is natural
to explore representation learning methods that can utilize large amounts of
unlabeled and weakly labeled data, as well as an additional data modality. I
describe my broad study of representation learning for speech data. Unlike most
other works that focus on a single learning setting, this thesis studies
multiple settings: supervised learning with auxiliary losses, unsupervised
learning, semi-supervised learning, and multi-view learning. Besides different
learning problems, I also explore multiple approaches for representation
learning. Though I focus on speech data, the methods described in this thesis
can also be applied to other domains. Overall, the field of representation
learning is developing rapidly. State-of-the-art results on speech related
tasks are typically based on Transformers pre-trained with large-scale
self-supervised learning, which aims to learn generic representations that can
benefit multiple downstream tasks. Since 2020, large-scale pre-training has
been the de facto choice to achieve good performance. This delayed thesis does
not attempt to summarize and compare with the latest results on speech
representation learning; instead, it presents a unique study on speech
representation learning before the Transformer era, that covers multiple
learning settings. Some of the findings in this thesis can still be useful
today.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts. (arXiv:2308.00157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00157">
<div class="article-summary-box-inner">
<span><p>Biomedical entity linking, also known as biomedical concept normalization,
has recently witnessed the rise to prominence of zero-shot contrastive models.
However, the pre-training material used for these models has, until now,
largely consisted of specialist biomedical content such as MIMIC-III clinical
notes (Johnson et al., 2016) and PubMed papers (Sayers et al., 2021; Gao et
al., 2020). While the resulting in-domain models have shown promising results
for many biomedical tasks, adverse drug event normalization on social media
texts has so far remained challenging for them (Portelli et al., 2022). In this
paper, we propose a new approach for adverse drug event normalization on social
media relying on general-purpose model initialization via BioLORD (Remy et al.,
2022) and a semantic-text-similarity fine-tuning named STS. Our experimental
results on several social media datasets demonstrate the effectiveness of our
proposed approach, by achieving state-of-the-art performance. Based on its
strong performance across all the tested datasets, we believe this work could
emerge as a turning point for the task of adverse drug event normalization on
social media and has the potential to serve as a benchmark for future research
in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00158">
<div class="article-summary-box-inner">
<span><p>Translation Quality Estimation (TQE) is an important step before deploying
the output translation into usage. TQE is also critical in assessing machine
translation (MT) and human translation (HT) quality without seeing the
reference translations. In this work, we examine if the state-of-the-art large
language models (LLMs) can be fine-tuned for the TQE task and their capability.
We take ChatGPT as one example and approach TQE as a binary classification
task. Using English-Italian and English-German training corpus, our
experimental results show that fine-tuned ChatGPT via its API can achieve a
relatively high score on predicting translation quality, i.e. if the
translation needs to be edited, but there is definitely space to improve the
accuracy. English-Italiano bilingual Abstract is available in the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00165">
<div class="article-summary-box-inner">
<span><p>Legal judgment prediction is the task of predicting the outcome of court
cases on a given text description of facts of cases. These tasks apply Natural
Language Processing (NLP) techniques to predict legal judgment results based on
facts. Recently, large-scale public datasets and NLP models have increased
research in areas related to legal judgment prediction systems. For such
systems to be practically helpful, they should be robust from adversarial
attacks. Previous works mainly focus on making a neural legal judgement system;
however, significantly less or no attention has been given to creating a robust
Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on
early existing LJP systems and found that none of them could handle attacks. In
this work, we proposed an approach for making robust LJP systems. Extensive
experiments on three legal datasets show significant improvements in our
approach over the state-of-the-art LJP system in handling adversarial attacks.
To the best of our knowledge, we are the first to increase the robustness of
early-existing LJP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00189">
<div class="article-summary-box-inner">
<span><p>Coaxing out desired behavior from pretrained models, while avoiding
undesirable ones, has redefined NLP and is reshaping how we interact with
computers. What was once a scientific engineering discipline-in which building
blocks are stacked one on top of the other-is arguably already a complex
systems science, in which emergent behaviors are sought out to support
previously unimagined use cases.
</p>
<p>Despite the ever increasing number of benchmarks that measure task
performance, we lack explanations of what behaviors language models exhibit
that allow them to complete these tasks in the first place. We argue for a
systematic effort to decompose language model behavior into categories that
explain cross-task performance, to guide mechanistic explanations and help
future-proof analytic research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00221">
<div class="article-summary-box-inner">
<span><p>This study aims to proactively tackle misuse of large language models beyond
identification of machine-generated text. While existing methods focus on
detection, some malicious misuses demand tracing the adversary user for
counteracting them. To address this, we propose "Multi-bit Watermark through
Color-listing" (COLOR), embedding traceable multi-bit information during
language model generation. Leveraging the benefits of zero-bit watermarking
(Kirchenbauer et al., 2023a), COLOR enables extraction without model access,
on-the-fly embedding, and maintains text quality, while allowing zero-bit
detection all at the same time. Preliminary experiments demonstrates successful
embedding of 32-bit messages with 91.9% accuracy in moderate-length texts
($\sim$500 tokens). This work advances strategies to counter language model
misuse effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation. (arXiv:2308.00240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00240">
<div class="article-summary-box-inner">
<span><p>Interpreting ancient Chinese has been the key to comprehending vast Chinese
literature, tradition, and civilization. In this paper, we propose Erya for
ancient Chinese translation. From a dataset perspective, we collect, clean, and
classify ancient Chinese materials from various sources, forming the most
extensive ancient Chinese resource to date. From a model perspective, we devise
Erya training method oriented towards ancient Chinese. We design two
jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked
language model (DMLM). From an evaluation perspective, we build a benchmark to
judge ancient Chinese translation quality in different scenarios and evaluate
the ancient Chinese translation capacities of various existing models. Our
model exhibits remarkable zero-shot performance across five domains, with over
+12.0 BLEU against GPT-3.5 models and better human evaluation results than
ERNIE Bot. Subsequent fine-tuning further shows the superior transfer
capability of Erya model with +6.2 BLEU gain. We release all the
above-mentioned resources at https://github.com/RUCAIBox/Erya.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00264">
<div class="article-summary-box-inner">
<span><p>In this work we investigate the optimal selection and fusion of features
across multiple modalities and combine these in a neural network to improve
emotion detection. We compare different fusion methods and examine the impact
of multi-loss training within the multi-modality fusion network, identifying
useful findings relating to subnet performance. Our best model achieves
state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and
CH-SIMS), and outperforms the other methods in most metrics. We have found that
training on multimodal features improves single modality testing and designing
fusion methods based on dataset annotation schema enhances model performance.
These results suggest a roadmap towards an optimized feature selection and
fusion approach for enhancing emotion detection in neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00295">
<div class="article-summary-box-inner">
<span><p>Text-based VQA aims at answering questions by reading the text present in the
images. It requires a large amount of scene-text relationship understanding
compared to the VQA task. Recent studies have shown that the question-answer
pairs in the dataset are more focused on the text present in the image but less
importance is given to visual features and some questions do not require
understanding the image. The models trained on this dataset predict biased
answers due to the lack of understanding of visual context. For example, in
questions like "What is written on the signboard?", the answer predicted by the
model is always "STOP" which makes the model to ignore the image. To address
these issues, we propose a method to learn visual features (making V matter in
TextVQA) along with the OCR features and question features using VQA dataset as
external knowledge for Text-based VQA. Specifically, we combine the TextVQA
dataset and VQA dataset and train the model on this combined dataset. Such a
simple, yet effective approach increases the understanding and correlation
between the image features and text present in the image, which helps in the
better answering of questions. We further test the model on different datasets
and compare their qualitative and quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00304">
<div class="article-summary-box-inner">
<span><p>We consider the problem of eliciting compositional generalization
capabilities in large language models (LLMs) with a novel type of prompting
strategy. Compositional generalization empowers the LLMs to solve problems that
are harder than the ones they have seen (i.e., easy-to-hard generalization),
which is a critical reasoning capability of human-like intelligence. However,
even the current state-of-the-art LLMs still struggle with this form of
reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,
which instructs LLMs how to compose basic skills to resolve more complex
problems. We find that it is crucial to demonstrate both the skills and the
compositional examples within the same prompting context. With as few as two
examplars, our SKiC prompting initiates strong synergies between skills and
their composition capabilities. Notably, it empowers LLMs to solve unseen
problems that require innovative skill compositions, achieving near-perfect
generalization on a broad range of challenging compositionality tasks.
Intriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling
them to leverage pre-existing internal skills acquired during earlier
pretraining and alignment stages, even when these skills are not explicitly
presented in the prompting context. This results in the capability of LLMs to
solve unseen complex problems by activating and composing these internal
competencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (arXiv:2308.00319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00319">
<div class="article-summary-box-inner">
<span><p>Natural language processing models are vulnerable to adversarial examples.
Previous textual adversarial attacks adopt gradients or confidence scores to
calculate word importance ranking and generate adversarial examples. However,
this information is unavailable in the real world. Therefore, we focus on a
more realistic and challenging setting, named hard-label attack, in which the
attacker can only query the model and obtain a discrete prediction label.
Existing hard-label attack algorithms tend to initialize adversarial examples
by random substitution and then utilize complex heuristic algorithms to
optimize the adversarial perturbation. These methods require a lot of model
queries and the attack success rate is restricted by adversary initialization.
In this paper, we propose a novel hard-label attack algorithm named LimeAttack,
which leverages a local explainable method to approximate word importance
ranking, and then adopts beam search to find the optimal solution. Extensive
experiments show that LimeAttack achieves the better attacking performance
compared with existing hard-label attack under the same query budget. In
addition, we evaluate the effectiveness of LimeAttack on large language models,
and results indicate that adversarial examples remain a significant threat to
large language models. The adversarial examples crafted by LimeAttack are
highly transferable and effectively improve model robustness in adversarial
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00364">
<div class="article-summary-box-inner">
<span><p>Deviations from the approved design or processes during mass production can
lead to unforeseen risks. However, these changes are sometimes necessary due to
changes in the product design characteristics or an adaptation in the
manufacturing process. A major challenge is to identify these risks early in
the workflow so that failures leading to warranty claims can be avoided. We
developed Fountain as a contextual assistant integrated in the deviation
management workflow that helps in identifying the risks based on the
description of the existing design and process criteria and the proposed
deviation. In the manufacturing context, it is important that the assistant
provides recommendations that are explainable and consistent. We achieve this
through a combination of the following two components 1) language models
finetuned for domain specific semantic similarity and, 2) knowledge
representation in the form of a property graph derived from the bill of
materials, Failure Modes and Effect Analysis (FMEA) and prior failures reported
by customers. Here, we present the nuances of selecting and adapting pretrained
language models for an engineering domain, continuous model updates based on
user interaction with the contextual assistant and creating the causal chain
for explainable recommendations based on the knowledge representation.
Additionally, we demonstrate that the model adaptation is feasible using
moderate computational infrastructure already available to most engineering
teams in manufacturing organizations and inference can be performed on standard
CPU only instances for integration with existing applications making these
methods easily deployable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00399">
<div class="article-summary-box-inner">
<span><p>Hallucinations in text generation occur when the system produces text that is
not grounded in the input. In this work, we tackle the problem of
hallucinations in neural chart summarization. Our analysis shows that the
target side of chart summarization training datasets often contains additional
information, leading to hallucinations. We propose a natural language inference
(NLI) based method to preprocess the training data and show through human
evaluation that our method significantly reduces hallucinations. We also found
that shortening long-distance dependencies in the input sequence and adding
chart-related information like title and legends improves the overall
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00400">
<div class="article-summary-box-inner">
<span><p>Image-grounded dialogue systems benefit greatly from integrating visual
information, resulting in high-quality response generation. However, current
models struggle to effectively utilize such information in zero-resource
scenarios, mainly due to the disparity between image and text modalities. To
overcome this challenge, we propose an innovative multimodal framework, called
ZRIGF, which assimilates image-grounded information for dialogue generation in
zero-resource situations. ZRIGF implements a two-stage learning strategy,
comprising contrastive pre-training and generative pre-training. Contrastive
pre-training includes a text-image matching module that maps images and texts
into a unified encoded vector space, along with a text-assisted masked image
modeling module that preserves pre-training visual features and fosters further
multimodal feature alignment. Generative pre-training employs a multimodal
fusion module and an information transfer module to produce insightful
responses based on harmonized multimodal representations. Comprehensive
experiments conducted on both text-based and image-grounded dialogue datasets
demonstrate ZRIGF's efficacy in generating contextually pertinent and
informative responses. Furthermore, we adopt a fully zero-resource scenario in
the image-grounded dialogue dataset to demonstrate our framework's robust
generalization capabilities in novel domains. The code is available at
https://github.com/zhangbo-nlp/ZRIGF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00425">
<div class="article-summary-box-inner">
<span><p>Sentences that present a complex syntax act as a major stumbling block for
downstream Natural Language Processing applications whose predictive quality
deteriorates with sentence length and complexity. The task of Text
Simplification (TS) may remedy this situation. It aims to modify sentences in
order to make them easier to process, using a set of rewriting operations, such
as reordering, deletion, or splitting. State-of-the-art syntactic TS approaches
suffer from two major drawbacks: first, they follow a very conservative
approach in that they tend to retain the input rather than transforming it, and
second, they ignore the cohesive nature of texts, where context spread across
clauses or sentences is needed to infer the true meaning of a statement. To
address these problems, we present a discourse-aware TS approach that splits
and rephrases complex English sentences within the semantic context in which
they occur. Based on a linguistically grounded transformation stage that uses
clausal and phrasal disembedding mechanisms, complex sentences are transformed
into shorter utterances with a simple canonical structure that can be easily
analyzed by downstream applications. With sentence splitting, we thus address a
TS task that has hardly been explored so far. Moreover, we introduce the notion
of minimality in this context, as we aim to decompose source sentences into a
set of self-contained minimal semantic units. To avoid breaking down the input
into a disjointed sequence of statements that is difficult to interpret because
important contextual information is missing, we incorporate the semantic
context between the split propositions in the form of hierarchical structures
and semantic relationships. In that way, we generate a semantic hierarchy of
minimal propositions that leads to a novel representation of complex assertions
that puts a semantic layer on top of the simplified sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00436">
<div class="article-summary-box-inner">
<span><p>The recent progress in large language models (LLMs), especially the invention
of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning
problems. However, even the strongest LLMs are still struggling with more
complicated problems that require non-linear thinking and multi-step reasoning.
In this work, we explore whether LLMs have the ability to recognize their own
errors, without resorting to external resources. In particular, we investigate
whether they can be used to identify individual errors within a step-by-step
reasoning. To this end, we propose a zero-shot verification scheme to recognize
such errors. We then use this verification scheme to improve question-answering
performance, by using it to perform weighted voting on different generated
answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and
find that it successfully recognizes errors and, in turn, increases final
predictive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00447">
<div class="article-summary-box-inner">
<span><p>It is evident that the current state of Large Language Models (LLMs)
necessitates the incorporation of external tools. The lack of straightforward
algebraic and logical reasoning is well documented and prompted researchers to
develop frameworks which allow LLMs to operate via external tools. The
ontological nature of tool utilization for a specific task can be well
formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is
to highlight the importance of graph based approaches to LLM-tool interaction
in near future. We propose an exemplary framework to guide the orchestration of
exponentially increasing numbers of external tools with LLMs,where objectives
and functionalities of tools are graph encoded hierarchically. Assuming that
textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as
defined here, the graph based framework can pave new avenues in that particular
direction as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00479">
<div class="article-summary-box-inner">
<span><p>Large Language Models are increasingly being used for various tasks including
content generation and as chatbots. Despite their impressive performances in
general tasks, LLMs need to be aligned when applying for domain specific tasks
to mitigate the problems of hallucination and producing harmful answers.
Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a
non-parametric knowledgebases to LLMs. Applications of RAG in the field of
medical education are discussed in this paper. A combined extractive and
abstractive summarization method for large unstructured textual data using
representative vectors is proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unimodal Intermediate Training for Multimodal Meme Sentiment Classification. (arXiv:2308.00528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00528">
<div class="article-summary-box-inner">
<span><p>Internet Memes remain a challenging form of user-generated content for
automated sentiment classification. The availability of labelled memes is a
barrier to developing sentiment classifiers of multimodal memes. To address the
shortage of labelled memes, we propose to supplement the training of a
multimodal meme classifier with unimodal (image-only and text-only) data. In
this work, we present a novel variant of supervised intermediate training that
uses relatively abundant sentiment-labelled unimodal data. Our results show a
statistically significant performance improvement from the incorporation of
unimodal text data. Furthermore, we show that the training set of labelled
memes can be reduced by 40% without reducing the performance of the downstream
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00624">
<div class="article-summary-box-inner">
<span><p>With the advancements in large language model technology, it has showcased
capabilities that come close to those of human beings across various tasks.
This achievement has garnered significant interest from companies and
scientific research institutions, leading to substantial investments in the
research and development of these models. While numerous large models have
emerged during this period, the majority of them have been trained primarily on
English data. Although they exhibit decent performance in other languages, such
as Chinese, their potential remains limited due to factors like vocabulary
design and training corpus. Consequently, their ability to fully express their
capabilities in Chinese falls short. To address this issue, we introduce the
model named JIANG (Chinese pinyin of ginger) specifically designed for the
Chinese language. We have gathered a substantial amount of Chinese corpus to
train the model and have also optimized its structure. The extensive
experimental results demonstrate the excellent performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00675">
<div class="article-summary-box-inner">
<span><p>Today, large language models (LLMs) are taught to use new tools by providing
a few demonstrations of the tool's usage. Unfortunately, demonstrations are
hard to acquire, and can result in undesirable biased usage if the wrong
demonstration is chosen. Even in the rare scenario that demonstrations are
readily available, there is no principled selection protocol to determine how
many and which ones to provide. As tasks grow more complex, the selection
search grows combinatorially and invariably becomes intractable. Our work
provides an alternative to demonstrations: tool documentation. We advocate the
use of tool documentation, descriptions for the individual tool usage, over
demonstrations. We substantiate our claim through three main empirical findings
on 6 tasks across both vision and language modalities. First, on existing
benchmarks, zero-shot prompts with only tool documentation are sufficient for
eliciting proper tool usage, achieving performance on par with few-shot
prompts. Second, on a newly collected realistic tool-use dataset with hundreds
of available tool APIs, we show that tool documentation is significantly more
valuable than demonstrations, with zero-shot documentation significantly
outperforming few-shot without documentation. Third, we highlight the benefits
of tool documentations by tackling image generation and video tracking using
just-released unseen state-of-the-art models as tools. Finally, we highlight
the possibility of using tool documentation to automatically enable new
applications: by using nothing more than the documentation of GroundingDino,
Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the
just-released Grounded-SAM and Track Anything models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00683">
<div class="article-summary-box-inner">
<span><p>Recent works have widely adopted large language model pretraining for source
code, suggested source code-specific pretraining objectives and investigated
the applicability of various Transformer-based language model architectures for
source code. This work investigates another important aspect of such models,
namely the effect of different subtokenization options, and aims at identifying
most effective and length-efficient subtokenizations, taking into account code
specifics. We propose subtokenziation that reduces average length by 17%
without downstream performance drop, and show that a carefully chosen
subtokenization may improve quality by 0.5-2%, possibly with some length
increase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Compositionality with Formal Languages. (arXiv:2208.08195v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08195">
<div class="article-summary-box-inner">
<span><p>Recombining known primitive concepts into larger novel combinations is a
quintessentially human cognitive capability. Whether large neural models in NLP
can acquire this ability while learning from data is an open question. In this
paper, we investigate this problem from the perspective of formal languages. We
use deterministic finite-state transducers to make an unbounded number of
datasets with controllable properties governing compositionality. By randomly
sampling over many transducers, we explore which of their properties contribute
to learnability of a compositional relation by a neural network. We find that
the models either learn the relations completely or not at all. The key is
transition coverage, setting a soft learnability limit at 400 examples per
transition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06607">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis has gained significant attention due to the
proliferation of multimodal content on social media. However, existing studies
in this area rely heavily on large-scale supervised data, which is
time-consuming and labor-intensive to collect. Thus, there is a need to address
the challenge of few-shot multimodal sentiment analysis. To tackle this
problem, we propose a novel method called Multimodal Probabilistic Fusion
Prompts (MultiPoint) that leverages diverse cues from different modalities for
multimodal sentiment detection in the few-shot scenario. Specifically, we start
by introducing a Consistently Distributed Sampling approach called CDS, which
ensures that the few-shot dataset has the same category distribution as the
full dataset. Unlike previous approaches primarily using prompts based on the
text modality, we design unified multimodal prompts to reduce discrepancies
between different modalities and dynamically incorporate multimodal
demonstrations into the context of each multimodal instance. To enhance the
model's robustness, we introduce a probabilistic fusion method to fuse output
predictions from multiple diverse prompts for each input. Our extensive
experiments on six datasets demonstrate the effectiveness of our approach.
First, our method outperforms strong baselines in the multimodal few-shot
setting. Furthermore, under the same amount of data (1% of the full dataset),
our CDS-based experimental results significantly outperform those based on
previously sampled datasets constructed from the same number of instances of
each class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08172">
<div class="article-summary-box-inner">
<span><p>Understanding geometric properties of natural language processing models'
latent spaces allows the manipulation of these properties for improved
performance on downstream tasks. One such property is the amount of data spread
in a model's latent space, or how fully the available latent space is being
used. In this work, we define data spread and demonstrate that the commonly
used measures of data spread, Average Cosine Similarity and a partition
function min/max ratio I(V), do not provide reliable metrics to compare the use
of latent space across models. We propose and examine eight alternative
measures of data spread, all but one of which improve over these current
metrics when applied to seven synthetic data distributions. Of our proposed
measures, we recommend one principal component-based measure and one
entropy-based measure that provide reliable, relative measures of spread and
can be used to compare models of different sizes and dimensionalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Context Windows for Large Language Models. (arXiv:2212.10947v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10947">
<div class="article-summary-box-inner">
<span><p>When applied to processing long text, Large Language Models (LLMs) are
limited by their context window. Existing efforts to address this limitation
involve training specialized architectures, and cannot be easily applied to
off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that
alleviates the context window restriction for any off-the-shelf LLM without
further training. The key to the approach is to carve a long context into
chunks (``windows''), restrict the attention mechanism to apply only within
each window, and re-use the positional embeddings across the windows. Our main
results test the PCW approach on in-context learning with models that range in
size between 750 million and 178 billion parameters, and show substantial
improvements for tasks with diverse input and output spaces. We show additional
benefits in other settings where long context windows may be beneficial:
multi-hop questions and retrieval-augmented question answering with multiple
retrieved documents. Our results highlight Parallel Context Windows as a
promising method for applying off-the-shelf LLMs in a range of settings that
require long text sequences. We make our code publicly available at
https://github.com/ai21labs/parallel-context-windows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00083">
<div class="article-summary-box-inner">
<span><p>Retrieval-Augmented Language Modeling (RALM) methods, which condition a
language model (LM) on relevant documents from a grounding corpus during
generation, were shown to significantly improve language modeling performance.
In addition, they can mitigate the problem of factually inaccurate text
generation and provide natural source attribution mechanism. Existing RALM
approaches focus on modifying the LM architecture in order to facilitate the
incorporation of external information, significantly complicating deployment.
This paper considers a simple alternative, which we dub In-Context RALM:
leaving the LM architecture unchanged and prepending grounding documents to the
input, without any further training of the LM. We show that In-Context RALM
that builds on off-the-shelf general purpose retrievers provides surprisingly
large LM gains across model sizes and diverse corpora. We also demonstrate that
the document retrieval and ranking mechanism can be specialized to the RALM
setting to further boost performance. We conclude that In-Context RALM has
considerable potential to increase the prevalence of LM grounding, particularly
in settings where a pretrained LM must be used without modification or even via
API access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12189">
<div class="article-summary-box-inner">
<span><p>Current captioning datasets focus on object-centric captions, describing the
visible objects in the image, e.g. "people eating food in a park". Although
these datasets are useful to evaluate the ability of Vision &amp; Language models
to recognize and describe visual content, they do not support controlled
experiments involving model testing or fine-tuning, with more high-level
captions, which humans find easy and natural to produce. For example, people
often describe images based on the type of scene they depict ('people at a
holiday resort') and the actions they perform ('people having a picnic'). Such
descriptions draw on personal experience and commonsense assumptions. We
present the High-Level Dataset a dataset extending 14997 images from the COCO
dataset, aligned with a new set of 134,973 human-annotated (high-level)
captions collected along three axes: scenes, actions, and rationales. We
further extend this dataset with confidence scores collected from an
independent set of readers, as well as a set of narrative captions generated
synthetically, by combining each of the three axes. We describe this dataset
and analyse it extensively. We also present baseline results for the High-Level
Captioning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05389">
<div class="article-summary-box-inner">
<span><p>Depression is a common disease worldwide. It is difficult to diagnose and
continues to be underdiagnosed. Because depressed patients constantly share
their symptoms, major life events, and treatments on social media, researchers
are turning to user-generated digital traces on social media for depression
detection. Such methods have distinct advantages in combating depression
because they can facilitate innovative approaches to fight depression and
alleviate its social and economic burden. However, most existing studies lack
effective means to incorporate established medical domain knowledge in
depression detection or suffer from feature extraction difficulties that impede
greater performance. Following the design science research paradigm, we propose
a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately
detect social media users at risk of depression and explain the critical
factors that contribute to such detection. Extensive empirical studies with
real-world data demonstrate that, by incorporating domain knowledge, our method
outperforms existing state-of-the-art methods. Our work has significant
implications for IS research in knowledge-aware machine learning, digital
traces utilization, and NLP research in IS. Practically, by providing early
detection and explaining the critical factors, DKDD can supplement clinical
depression screening and enable large-scale evaluations of a population's
mental health status.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat with the Environment: Interactive Multimodal Perception Using Large Language Models. (arXiv:2303.08268v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08268">
<div class="article-summary-box-inner">
<span><p>Programming robot behavior in a complex world faces challenges on multiple
levels, from dextrous low-level skills to high-level planning and reasoning.
Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning
ability in few-shot robotic planning. However, it remains challenging to ground
LLMs in multimodal sensory input and continuous action output, while enabling a
robot to interact with its environment and acquire novel information as its
policies unfold. We develop a robot interaction scenario with a partially
observable state, which necessitates a robot to decide on a range of epistemic
actions in order to sample sensory information among multiple modalities,
before being able to execute the task correctly. An interactive perception
framework is therefore proposed with an LLM as its backbone, whose ability is
exploited to instruct epistemic actions and to reason over the resulting
multimodal sensations (vision, sound, haptics, proprioception), as well as to
plan an entire task execution based on the interactively acquired information.
Our study demonstrates that LLMs can provide high-level planning and reasoning
skills and control interactive robot behavior in a multimodal environment,
while multimodal modules with the context of the environmental state help
ground the LLMs and extend their processing ability. The project website can be
found at
\href{https://matcha-model.github.io}{\textcolor{blue}{https://matcha-model.github.io/}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09901">
<div class="article-summary-box-inner">
<span><p>This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11082">
<div class="article-summary-box-inner">
<span><p>An important aspect in developing language models that interact with humans
is aligning their behavior to be useful and unharmful for their human users.
This is usually achieved by tuning the model in a way that enhances desired
behaviors and inhibits undesired ones, a process referred to as alignment. In
this paper, we propose a theoretical approach called Behavior Expectation
Bounds (BEB) which allows us to formally investigate several inherent
characteristics and limitations of alignment in large language models.
Importantly, we prove that for any behavior that has a finite probability of
being exhibited by the model, there exist prompts that can trigger the model
into outputting this behavior, with probability that increases with the length
of the prompt. This implies that any alignment process that attenuates
undesired behavior but does not remove it altogether, is not safe against
adversarial prompting attacks. Furthermore, our framework hints at the
mechanism by which leading alignment approaches such as reinforcement learning
from human feedback increase the LLM's proneness to being prompted into the
undesired behaviors. Moreover, we include the notion of personas in our BEB
framework, and find that behaviors which are generally very unlikely to be
exhibited by the model can be brought to the front by prompting the model to
behave as specific persona. This theoretical result is being experimentally
demonstrated in large scale by the so called contemporary "chatGPT jailbreaks",
where adversarial users trick the LLM into breaking its alignment guardrails by
triggering it into acting as a malicious persona. Our results expose
fundamental limitations in alignment of LLMs and bring to the forefront the
need to devise reliable mechanisms for ensuring AI safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04853">
<div class="article-summary-box-inner">
<span><p>With the explosive growth of textual information, summarization systems have
become increasingly important. This work aims to concisely indicate the current
state of the art in abstractive text summarization. As part of this, we outline
the current paradigm shifts towards pre-trained encoder-decoder models and
large autoregressive language models. Additionally, we delve further into the
challenges of evaluating summarization systems and the potential of
instruction-tuned models for zero-shot summarization. Finally, we provide a
brief overview of how summarization systems are currently being integrated into
commercial applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08698">
<div class="article-summary-box-inner">
<span><p>Multimodal Knowledge Graph Construction (MKGC) involves creating structured
representations of entities and relations using multiple modalities, such as
text and images. However, existing MKGC models face challenges in handling the
addition of new entities and relations in dynamic real-world scenarios. The
current continual setting for knowledge graph construction mainly focuses on
entity and relation extraction from text data, overlooking other multimodal
sources. Therefore, there arises the need to explore the challenge of continual
MKGC to address the phenomenon of catastrophic forgetting and ensure the
retention of past knowledge extracted from different forms of data. This
research focuses on investigating this complex topic by developing lifelong
MKGC benchmark datasets. Based on the empirical findings that several typical
MKGC models, when trained on multimedia data, might unexpectedly underperform
compared to those solely utilizing textual resources in a continual setting, we
propose a Lifelong MultiModal Consistent Transformer Framework (LMC) for
continual MKGC, which plays the strengths of the consistent multimodal
optimization in continual learning and leads to a better stability-plasticity
trade-off. Our experiments demonstrate the superior performance of our method
over prevailing continual learning techniques or multimodal approaches in
dynamic scenarios. Code and datasets can be found at
https://github.com/zjunlp/ContinueMKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13899">
<div class="article-summary-box-inner">
<span><p>The ability to learn new concepts sequentially is a major weakness for modern
neural networks, which hinders their use in non-stationary environments. Their
propensity to fit the current data distribution to the detriment of the past
acquired knowledge leads to the catastrophic forgetting issue. In this work we
tackle the problem of Spoken Language Understanding applied to a continual
learning setting. We first define a class-incremental scenario for the SLURP
dataset. Then, we propose three knowledge distillation (KD) approaches to
mitigate forgetting for a sequence-to-sequence transformer model: the first KD
method is applied to the encoder output (audio-KD), and the other two work on
the decoder output, either directly on the token-level (tok-KD) or on the
sequence-level (seq-KD) distributions. We show that the seq-KD substantially
improves all the performance metrics, and its combination with the audio-KD
further decreases the average WER and enhances the entity prediction metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14387">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17020">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence state-of-the-art systems for dialogue state tracking
(DST) use the full dialogue history as input, represent the current state as a
list with all the slots, and generate the entire state from scratch at each
dialogue turn. This approach is inefficient, especially when the number of
slots is large and the conversation is long. We propose Diable, a new task
formalisation that simplifies the design and implementation of efficient DST
systems and allows one to easily plug and play large language models. We
represent the dialogue state as a table and formalise DST as a table
manipulation task. At each turn, the system updates the previous state by
generating table operations based on the dialogue context. Extensive
experimentation on the MultiWoz datasets demonstrates that Diable (i)
outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient
than current state-of-the-art methods while retaining competitive Joint Goal
Accuracy, and (iii) is robust to noisy data annotations due to the table
operations approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17446">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) are known to be overly parameterized and
have significant redundancy, indicating a small degree of freedom of the PLMs.
Motivated by the observation, in this paper, we study the problem of
re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of
intrinsic task-specific subspace. Specifically, by exploiting the dynamics of
the fine-tuning process for a given task, the parameter optimization trajectory
is learned to uncover its intrinsic task-specific subspace. A key finding is
that PLMs can be effectively fine-tuned in the subspace with a small number of
free parameters. Beyond, we observe some outlier dimensions emerging during
fine-tuning in the subspace. Disabling these dimensions degrades the model
performance significantly. This suggests that these dimensions are crucial to
induce task-specific knowledge to downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18624">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MyCrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15551">
<div class="article-summary-box-inner">
<span><p>Scientific Machine Learning (SciML) has advanced recently across many
different areas in computational science and engineering. The objective is to
integrate data and physics seamlessly without the need of employing elaborate
and computationally taxing data assimilation schemes. However, preprocessing,
problem formulation, code generation, postprocessing and analysis are still
time consuming and may prevent SciML from wide applicability in industrial
applications and in digital twin frameworks. Here, we integrate the various
stages of SciML under the umbrella of ChatGPT, to formulate MyCrunchGPT, which
plays the role of a conductor orchestrating the entire workflow of SciML based
on simple prompts by the user. Specifically, we present two examples that
demonstrate the potential use of MyCrunchGPT in optimizing airfoils in
aerodynamics, and in obtaining flow fields in various geometries in interactive
mode, with emphasis on the validation stage. To demonstrate the flow of the
MyCrunchGPT, and create an infrastructure that can facilitate a broader vision,
we built a webapp based guided user interface, that includes options for a
comprehensive summary report. The overall objective is to extend MyCrunchGPT to
handle diverse problems in computational mechanics, design, optimization and
controls, and general scientific computing tasks involved in SciML, hence using
it as a research assistant tool but also as an educational tool. While here the
examples focus in fluid mechanics, future versions will target solid mechanics
and materials science, geophysics, systems biology and bioinformatics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17156">
<div class="article-summary-box-inner">
<span><p>Generative AI and large language models hold great promise in enhancing
computing education by powering next-generation educational technologies for
introductory programming. Recent works have studied these models for different
scenarios relevant to programming education; however, these works are limited
for several reasons, as they typically consider already outdated models or only
specific scenario(s). Consequently, there is a lack of a systematic study that
benchmarks state-of-the-art models for a comprehensive set of programming
education scenarios. In our work, we systematically evaluate two models,
ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human
tutors for a variety of scenarios. We evaluate using five introductory Python
programming problems and real-world buggy programs from an online platform, and
assess performance using expert-based annotations. Our results show that GPT-4
drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human
tutors' performance for several scenarios. These results also highlight
settings where GPT-4 still struggles, providing exciting future directions on
developing techniques to improve the performance of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02477">
<div class="article-summary-box-inner">
<span><p>The impressive performance of recent language models across a wide range of
tasks suggests that they possess a degree of abstract reasoning skills. Are
these skills general and transferable, or specialized to specific tasks seen
during pretraining? To disentangle these effects, we propose an evaluation
framework based on "counterfactual" task variants that deviate from the default
assumptions underlying standard tasks. Across a suite of 11 tasks, we observe
nontrivial performance on the counterfactual variants, but nevertheless find
that performance substantially and consistently degrades compared to the
default conditions. This suggests that while current LMs may possess abstract
task-solving skills to a degree, they often also rely on narrow,
non-transferable procedures for task-solving. These results motivate a more
careful interpretation of language model performance that teases apart these
aspects of behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.04192">
<div class="article-summary-box-inner">
<span><p>Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09009">
<div class="article-summary-box-inner">
<span><p>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)
opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating
code, 6) US Medical License tests, and 7) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was reasonable at identifying prime vs.
composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same
questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity
to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in
June than in March in this task. GPT-4 became less willing to answer sensitive
questions and opinion survey questions in June than in March. GPT-4 performed
better at multi-hop questions in June than in March, while GPT-3.5's
performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting
mistakes in code generation in June than in March. Overall, our findings show
that the behavior of the "same" LLM service can change substantially in a
relatively short amount of time, highlighting the need for continuous
monitoring of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11224">
<div class="article-summary-box-inner">
<span><p>Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating various textual inputs into numerical
representations, thereby capturing the semantic essence of the text. The models
excel in applications such as dense retrieval and semantic textual similarity.
This paper details the development of Jina Embeddings, starting with the
creation of high-quality pairwise and triplet datasets. It underlines the
crucial role of data cleaning in dataset preparation, gives in-depth insights
into the model training process, and concludes with a comprehensive performance
evaluation using the Massive Textual Embedding Benchmark (MTEB). To increase
the model's awareness of negations, we constructed a novel training and
evaluation dataset of negated and non-negated statements, which we make
publicly available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11760">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved significant performance in many
fields such as reasoning, language understanding, and math problem-solving, and
are regarded as a crucial step to artificial general intelligence (AGI).
However, the sensitivity of LLMs to prompts remains a major bottleneck for
their daily adoption. In this paper, we take inspiration from psychology and
propose EmotionPrompt to explore emotional intelligence to enhance the
performance of LLMs. EmotionPrompt operates on a remarkably straightforward
principle: the incorporation of emotional stimulus into prompts. Experimental
results demonstrate that our EmotionPrompt, using the same single prompt
templates, significantly outperforms original zero-shot prompt and
Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and
T5. Further, EmotionPrompt was observed to improve both truthfulness and
informativeness. We believe that EmotionPrompt heralds a novel avenue for
exploring interdisciplinary knowledge for humans-LLMs interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14522">
<div class="article-summary-box-inner">
<span><p>A clinical trial is a study that evaluates new biomedical interventions. To
design new trials, researchers draw inspiration from those current and
completed. In 2022, there were on average more than 100 clinical trials
submitted to ClinicalTrials.gov every day, with each trial having a mean of
approximately 1500 words [1]. This makes it nearly impossible to keep up to
date. To mitigate this issue, we have created a batch clinical trial summarizer
called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first
tool able to provide real-time, truthful, and comprehensive summaries of
clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions
(approximately 10,500 words) into a concise 200-word summary with references
and limited hallucinations. We have tested CliniDigest on its ability to
summarize 457 trials divided across 27 medical subdomains. For each field,
CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which
utilizes $\mu=54\%,\ \sigma=30\% $ of the sources. A more comprehensive
evaluation is planned and outlined in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15002">
<div class="article-summary-box-inner">
<span><p>The effectiveness of compression distance in KNN-based text classification
('gzip') has recently garnered lots of attention. In this note we show that
simpler means can also be effective, and compression may not be needed. Indeed,
a 'bag-of-words' matching can achieve similar or better results, and is more
efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15411">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable capacity for in-context
learning (ICL), where learning a new task from just a few training examples is
done without being explicitly pre-trained. However, despite the success of
LLMs, there has been little understanding of how ICL learns the knowledge from
the given prompts. In this paper, to make progress toward understanding the
learning behaviour of ICL, we train the same LLMs with the same demonstration
examples via ICL and supervised learning (SL), respectively, and investigate
their performance under label perturbations (i.e., noisy labels and label
imbalance) on a range of classification tasks. First, via extensive
experiments, we find that gold labels have significant impacts on the
downstream in-context performance, especially for large language models;
however, imbalanced labels matter little to ICL across all model sizes. Second,
when comparing with SL, we show empirically that ICL is less sensitive to label
perturbations than SL, and ICL gradually attains comparable performance to SL
as the model size increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records. (arXiv:2307.16573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16573">
<div class="article-summary-box-inner">
<span><p>Cultural heritage is an arena of international relations that interests all
states worldwide. The inscription process on the UNESCO World Heritage List and
the UNESCO Representative List of the Intangible Cultural Heritage of Humanity
often leads to tensions and conflicts among states. This research addresses
these challenges by developing automatic tools that provide valuable insights
into the decision-making processes regarding inscriptions to the two lists
mentioned above. We propose innovative topic modelling and tension detection
methods based on UNESCO's summary records. Our analysis achieved a commendable
accuracy rate of 72% in identifying tensions. Furthermore, we have developed an
application tailored for diplomats, lawyers, political scientists, and
international relations researchers that facilitates the efficient search of
paragraphs from selected documents and statements from specific speakers about
chosen topics. This application is a valuable resource for enhancing the
understanding of complex decision-making dynamics within international heritage
inscription procedures.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-02 23:10:28.843906483 UTC">2023-08-02 23:10:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>