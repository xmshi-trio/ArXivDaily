<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-29T01:30:00Z">12-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Text Simplification of News Articles in the Context of Public Broadcasting. (arXiv:2212.13317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13317">
<div class="article-summary-box-inner">
<span><p>This report summarizes the work carried out by the authors during the Twelfth
Montreal Industrial Problem Solving Workshop, held at Universit\'e de
Montr\'eal in August 2022. The team tackled a problem submitted by
CBC/Radio-Canada on the theme of Automatic Text Simplification (ATS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation. (arXiv:2212.13378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13378">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems frequently use a search-based
decoding strategy aiming to find the best attainable transcript by considering
multiple candidates. One prominent speech recognition decoding heuristic is
beam search, which seeks the transcript with the greatest likelihood computed
using the predicted distribution. While showing substantial performance gains
in various tasks, beam search loses some of its effectiveness when the
predicted probabilities are highly confident, i.e., the predicted distribution
is massed for a single or very few classes. We show that recently proposed
Self-Supervised Learning (SSL)-based ASR models tend to yield exceptionally
confident predictions that may hamper beam search from truly considering a
diverse set of candidates. We perform a layer analysis to reveal and visualize
how predictions evolve, and propose a decoding procedure that improves the
performance of fine-tuned ASR models. Our proposed approach does not require
further training beyond the original fine-tuning, nor additional model
parameters. In fact, we find that our proposed method requires significantly
less inference computation than current approaches. We propose aggregating the
top M layers, potentially leveraging useful information encoded in intermediate
layers, and relaxing model confidence. We demonstrate the effectiveness of our
approach by conducting an empirical study on varying amounts of labeled
resources and different model sizes, showing consistent improvements in
particular when applied to low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCuts: Single-Shot Interpretability based Pruning for BERT. (arXiv:2212.13392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13392">
<div class="article-summary-box-inner">
<span><p>As language models have grown in parameters and layers, it has become much
harder to train and infer with them on single GPUs. This is severely
restricting the availability of large language models such as GPT-3,
BERT-Large, and many others. A common technique to solve this problem is
pruning the network architecture by removing transformer heads, fully-connected
weights, and other modules. The main challenge is to discern the important
parameters from the less important ones. Our goal is to find strong metrics for
identifying such parameters. We thus propose two strategies: Cam-Cut based on
the GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for
calculating the importance scores. Through this work, we show that our scoring
functions are able to assign more relevant task-based scores to the network
parameters, and thus both our pruning approaches significantly outperform the
standard weight and gradient-based strategies, especially at higher compression
ratios in BERT-based models. We also analyze our pruning masks and find them to
be significantly different from the ones obtained using standard metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis. (arXiv:2212.13408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13408">
<div class="article-summary-box-inner">
<span><p>With the development of natural language processing techniques(NLP),
automatic diagnosis of eye diseases using ophthalmology electronic medical
records (OEMR) has become possible. It aims to evaluate the condition of both
eyes of a patient respectively, and we formulate it as a particular multi-label
classification task in this paper. Although there are a few related studies in
other diseases, automatic diagnosis of eye diseases exhibits unique
characteristics. First, descriptions of both eyes are mixed up in OEMR
documents, with both free text and templated asymptomatic descriptions,
resulting in sparsity and clutter of information. Second, OEMR documents
contain multiple parts of descriptions and have long document lengths. Third,
it is critical to provide explainability to the disease diagnosis model. To
overcome those challenges, we present an effective automatic eye disease
diagnosis framework, NEEDED. In this framework, a preprocessing module is
integrated to improve the density and quality of information. Then, we design a
hierarchical transformer structure for learning the contextualized
representations of each sentence in the OEMR document. For the diagnosis part,
we propose an attention-based predictor that enables traceable diagnosis by
obtaining disease-specific information. Experiments on the real dataset and
comparison with several baseline models show the advantage and explainability
of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Knowledge-Enhanced Pre-trained Language Models. (arXiv:2212.13428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13428">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) has been revolutionized by the use of
Pre-trained Language Models (PLMs) such as BERT. Despite setting new records in
nearly every NLP task, PLMs still face a number of challenges including poor
interpretability, weak reasoning capability, and the need for a lot of
expensive annotated data when applied to downstream tasks. By integrating
external knowledge into PLMs,
\textit{\underline{K}nowledge-\underline{E}nhanced \underline{P}re-trained
\underline{L}anguage \underline{M}odels} (KEPLMs) have the potential to
overcome the above-mentioned limitations. In this paper, we examine KEPLMs
systematically through a series of studies. Specifically, we outline the common
types and different formats of knowledge to be integrated into KEPLMs, detail
the existing methods for building and evaluating KEPLMS, present the
applications of KEPLMs in downstream tasks, and discuss the future research
directions. Researchers will benefit from this survey by gaining a quick and
comprehensive overview of the latest developments in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence. (arXiv:2212.13456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13456">
<div class="article-summary-box-inner">
<span><p>Creating an essay based on a few given topics is a challenging NLP task.
Although several effective methods for this problem, topic-to-essay generation,
have appeared recently, there is still much room for improvement, especially in
terms of the coverage of the given topics and the coherence of the generated
text. In this paper, we propose a novel approach called TegFormer which
utilizes the Transformer architecture where the encoder is enriched with
domain-specific contexts while the decoder is enhanced by a large-scale
pre-trained language model. Specifically, a \emph{Topic-Extension} layer
capturing the interaction between the given topics and their domain-specific
contexts is plugged into the encoder. Since the given topics are usually
concise and sparse, such an additional layer can bring more topic-related
semantics in to facilitate the subsequent natural language generation.
Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific
word embeddings learnt from the given corpus and the general-purpose word
embeddings provided by a GPT-2 model pre-trained on massive text data is
integrated into the decoder. Since GPT-2 is at a much larger scale, it contains
a lot more implicit linguistic knowledge which would help the decoder to
produce more grammatical and readable text. Extensive experiments have shown
that the pieces of text generated by TegFormer have better topic coverage and
higher text coherence than those from SOTA topic-to-essay techniques, according
to automatic and human evaluations. As revealed by ablation studies, both the
Topic-Extension layer and the Embedding-Fusion module contribute substantially
to TegFormer's performance advantage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions. (arXiv:2212.13465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13465">
<div class="article-summary-box-inner">
<span><p>Table-and-text hybrid question answering (HybridQA) is a widely used and
challenging NLP task commonly applied in the financial and scientific domain.
The early research focuses on migrating other QA task methods to HybridQA,
while with further research, more and more HybridQA-specific methods have been
present. With the rapid development of HybridQA, the systematic survey is still
under-explored to summarize the main techniques and advance further research.
So we present this work to summarize the current HybridQA benchmarks and
methods, then analyze the challenges and future directions of this task. The
contributions of this paper can be summarized in three folds: (1) first survey,
to our best knowledge, including benchmarks, methods and challenges for
HybridQA; (2) systematic investigation with the reasonable comparison of the
existing systems to articulate their advantages and shortcomings; (3) detailed
analysis of challenges in four important dimensions to shed light on future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing. (arXiv:2212.13492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13492">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL semantic parsing is an important NLP task, which greatly
facilitates the interaction between users and the database and becomes the key
component in many human-computer interaction systems. Much recent progress in
text-to-SQL has been driven by large-scale datasets, but most of them are
centered on English. In this work, we present MultiSpider, the largest
multilingual text-to-SQL dataset which covers seven languages (English, German,
French, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider, we
further identify the lexical and structural challenges of text-to-SQL (caused
by specific language properties and dialect sayings) and their intensity across
different languages. Experimental results under three typical settings
(zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in
accuracy in non-English languages. Qualitative and quantitative analyses are
conducted to understand the reason for the performance drop of each language.
Besides the dataset, we also propose a simple schema augmentation framework
SAVe (Schema-Augmentation-with-Verification), which significantly boosts the
overall performance by about 1.8% and closes the 29.5% performance gap across
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TempCLR: Temporal Alignment Representation with Contrastive Learning. (arXiv:2212.13738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13738">
<div class="article-summary-box-inner">
<span><p>Video representation learning has been successful in video-text pre-training
for zero-shot transfer, where each sentence is trained to be close to the
paired video clips in a common feature space. For long videos, given a
paragraph of description where the sentences describe different segments of the
video, by matching all sentence-clip pairs, the paragraph and the full video
are aligned implicitly. However, such unit-level similarity measure may ignore
the global temporal context over a long time span, which inevitably limits the
generalization ability. In this paper, we propose a contrastive learning
framework TempCLR to compare the full video and the paragraph explicitly. As
the video/paragraph is formulated as a sequence of clips/sentences, under the
constraint of their temporal order, we use dynamic time warping to compute the
minimum cumulative cost over sentence-clip pairs as the sequence-level
distance. To explore the temporal dynamics, we break the consistency of
temporal order by shuffling the video clips or sentences according to the
temporal granularity. In this way, we obtain the representations for
clips/sentences, which perceive the temporal information and thus facilitate
the sequence alignment. In addition to pre-training on the video and paragraph,
our approach can also generalize on the matching between different video
instances. We evaluate our approach on video retrieval, action step
localization, and few-shot action recognition, and achieve consistent
performance gain over all three tasks. Detailed ablation studies are provided
to justify the approach design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain. (arXiv:2212.13860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13860">
<div class="article-summary-box-inner">
<span><p>Future work sentences (FWS) are the particular sentences in academic papers
that contain the author's description of their proposed follow-up research
direction. This paper presents methods to automatically extract FWS from
academic papers and classify them according to the different future directions
embodied in the paper's content. FWS recognition methods will enable subsequent
researchers to locate future work sentences more accurately and quickly and
reduce the time and cost of acquiring the corpus. The current work on automatic
identification of future work sentences is relatively small, and the existing
research cannot accurately identify FWS from academic papers, and thus cannot
conduct data mining on a large scale. Furthermore, there are many aspects to
the content of future work, and the subdivision of the content is conducive to
the analysis of specific development directions. In this paper, Nature Language
Processing (NLP) is used as a case study, and FWS are extracted from academic
papers and classified into different types. We manually build an annotated
corpus with six different types of FWS. Then, automatic recognition and
classification of FWS are implemented using machine learning models, and the
performance of these models is compared based on the evaluation metrics. The
results show that the Bernoulli Bayesian model has the best performance in the
automatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT
model has the best performance in the automatic classification task, with the
weighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and
gain a deep understanding of the key content described in FWS, and we also
demonstrate that content determination in FWS will be reflected in the
subsequent research work by measuring the similarity between future work
sentences and the abstracts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Deep Neural Networks for Legal Document Retrieval. (arXiv:2212.13899v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13899">
<div class="article-summary-box-inner">
<span><p>Legal text retrieval serves as a key component in a wide range of legal text
processing tasks such as legal question answering, legal case entailment, and
statute law retrieval. The performance of legal text retrieval depends, to a
large extent, on the representation of text, both query and legal documents.
Based on good representations, a legal text retrieval model can effectively
match the query to its relevant documents. Because legal documents often
contain long articles and only some parts are relevant to queries, it is quite
a challenge for existing models to represent such documents. In this paper, we
study the use of attentive neural network-based text representation for statute
law document retrieval. We propose a general approach using deep neural
networks with attention mechanisms. Based on it, we develop two hierarchical
architectures with sparse attention to represent long sentences and articles,
and we name them Attentive CNN and Paraformer. The methods are evaluated on
datasets of different sizes and characteristics in English, Japanese, and
Vietnamese. Experimental results show that: i) Attentive neural methods
substantially outperform non-neural methods in terms of retrieval performance
across datasets and languages; ii) Pretrained transformer-based models achieve
better accuracy on small datasets at the cost of high computational complexity
while lighter weight Attentive CNN achieves better accuracy on large datasets;
and iii) Our proposed Paraformer outperforms state-of-the-art methods on COLIEE
dataset, achieving the highest recall and F2 scores in the top-N retrieval
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Emotion Recognition among Couples from Lab Settings to Daily Life using Smartwatches. (arXiv:2212.13917v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13917">
<div class="article-summary-box-inner">
<span><p>Couples generally manage chronic diseases together and the management takes
an emotional toll on both patients and their romantic partners. Consequently,
recognizing the emotions of each partner in daily life could provide an insight
into their emotional well-being in chronic disease management. The emotions of
partners are currently inferred in the lab and daily life using self-reports
which are not practical for continuous emotion assessment or observer reports
which are manual, time-intensive, and costly. Currently, there exists no
comprehensive overview of works on emotion recognition among couples.
Furthermore, approaches for emotion recognition among couples have (1) focused
on English-speaking couples in the U.S., (2) used data collected from the lab,
and (3) performed recognition using observer ratings rather than partner's
self-reported / subjective emotions. In this body of work contained in this
thesis (8 papers - 5 published and 3 currently under review in various
journals), we fill the current literature gap on couples' emotion recognition,
develop emotion recognition systems using 161 hours of data from a total of
1,051 individuals, and make contributions towards taking couples' emotion
recognition from the lab which is the status quo, to daily life. This thesis
contributes toward building automated emotion recognition systems that would
eventually enable partners to monitor their emotions in daily life and enable
the delivery of interventions to improve their emotional well-being.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Page Layout Analysis of Text-heavy Historical Documents: a Comparison of Textual and Visual Approaches. (arXiv:2212.13924v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13924">
<div class="article-summary-box-inner">
<span><p>Page layout analysis is a fundamental step in document processing which
enables to segment a page into regions of interest. With highly complex layouts
and mixed scripts, scholarly commentaries are text-heavy documents which remain
challenging for state-of-the-art models. Their layout considerably varies
across editions and their most important regions are mainly defined by semantic
rather than graphical characteristics such as position or appearance. This
setting calls for a comparison between textual, visual and hybrid approaches.
We therefore assess the performances of two transformers (LayoutLMv3 and
RoBERTa) and an objection-detection network (YOLOv5). If results show a clear
advantage in favor of the latter, we also list several caveats to this finding.
In addition to our experiments, we release a dataset of ca. 300 annotated pages
sampled from 19th century commentaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13939">
<div class="article-summary-box-inner">
<span><p>Learning models are highly dependent on data to work effectively, and they
give a better performance upon training on big datasets. Massive research
exists in the literature to address the dataset adequacy issue. One promising
approach for solving dataset adequacy issues is the data augmentation (DA)
approach. In DA, the amount of training data instances is increased by making
different transformations on the available data instances to generate new
correct and representative data instances. DA increases the dataset size and
its variability, which enhances the model performance and its prediction
accuracy. DA also solves the class imbalance problem in the classification
learning techniques. Few studies have recently considered DA in the Arabic
language. These studies rely on traditional augmentation approaches, such as
paraphrasing by using rules or noising-based techniques. In this paper, we
propose a new Arabic DA method that employs the recent powerful modeling
technique, namely the AraGPT-2, for the augmentation process. The generated
sentences are evaluated in terms of context, semantics, diversity, and novelty
using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT
transformer is used on sentiment classification tasks to evaluate the
classification performance of the augmented Arabic dataset. The experiments
were conducted on four sentiment Arabic datasets, namely AraSarcasm, ASTD, ATT,
and MOVIE. The selected datasets vary in size, label number, and unbalanced
classes. The results show that the proposed methodology enhanced the Arabic
sentiment text classification on all datasets with an increase in F1 score by
4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. (arXiv:2212.14024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14024">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented in-context learning has emerged as a powerful approach
for addressing knowledge-intensive tasks using frozen language models (LM) and
retrieval models (RM). Existing work has combined these in simple
"retrieve-then-read" pipelines in which the RM retrieves passages that are
inserted into the LM prompt. To begin to fully realize the potential of frozen
LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that
relies on passing natural language texts in sophisticated pipelines between an
LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware
demonstrations, search for relevant passages, and generate grounded
predictions, systematically breaking down problems into small transformations
that the LM and RM can handle more reliably. We have written novel DSP programs
for answering questions in open-domain, multi-hop, and conversational settings,
establishing in early evaluations new state-of-the-art in-context learning
results and delivering 37-200%, 8-40%, and 80-290% relative gains against
vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous
self-ask pipeline, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cramming: Training a Language Model on a Single GPU in One Day. (arXiv:2212.14034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14034">
<div class="article-summary-box-inner">
<span><p>Recent trends in language modeling have focused on increasing performance
through scaling, and have resulted in an environment where training language
models is out of reach for most researchers and practitioners. While most in
the community are asking how to push the limits of extreme computation, we ask
the opposite question: How far can we get with a single GPU in just one day?
</p>
<p>We investigate the downstream performance achievable with a transformer-based
language model trained completely from scratch with masked language modeling
for a single day on a single consumer GPU. Aside from re-analyzing nearly all
components of the pretraining pipeline for this scenario and providing a
modified pipeline with performance close to BERT, we investigate why scaling
down is hard, and which modifications actually improve performance in this
scenario. We provide evidence that even in this constrained setting,
performance closely follows scaling laws observed in large-compute settings.
Through the lens of scaling laws, we categorize a range of recent improvements
to training and architecture and discuss their merit and practical
applicability (or lack thereof) for the limited compute setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HateBR: A Large Expert Annotated Corpus of Brazilian Instagram Comments for Offensive Language and Hate Speech Detection. (arXiv:2103.14972v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14972">
<div class="article-summary-box-inner">
<span><p>Due to the severity of the social media offensive and hateful comments in
Brazil, and the lack of research in Portuguese, this paper provides the first
large-scale expert annotated corpus of Brazilian Instagram comments for hate
speech and offensive language detection. The HateBR corpus was collected from
the comment section of Brazilian politicians' accounts on Instagram and
manually annotated by specialists, reaching a high inter-annotator agreement.
The corpus consists of 7,000 documents annotated according to three different
layers: a binary classification (offensive versus non-offensive comments),
offensiveness-level classification (highly, moderately, and slightly
offensive), and nine hate speech groups (xenophobia, racism, homophobia,
sexism, religious intolerance, partyism, apology for the dictatorship,
antisemitism, and fatphobia). We also implemented baseline experiments for
offensive language and hate speech detection and compared them with a
literature baseline. Results show that the baseline experiments on our corpus
outperform the current state-of-the-art for the Portuguese language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02473">
<div class="article-summary-box-inner">
<span><p>In this research, we use user defined labels from three internet text sources
(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models
for the topic classification task of detecting cybersecurity discussions in
natural text. We analyze the false positive and false negative rates of each of
the 21 model's in a cross validation experiment. Then we present a
Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of
the 21 trained machine learning models as the decision mechanism for detecting
cybersecurity related text. We also show that the majority vote mechanism of
the CTC tool provides lower false negative and false positive rates on average
than any of the 21 individual models. We show that the CTC tool is scalable to
the hundreds of thousands of documents with a wall clock time on the order of
hours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture. (arXiv:2111.10974v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10974">
<div class="article-summary-box-inner">
<span><p>Supporting the current trend in the AI community, we present the AI Journey
2021 Challenge called Fusion Brain, the first competition which is targeted to
make the universal architecture which could process different modalities (in
this case, images, texts, and code) and solve multiple tasks for vision and
language. The Fusion Brain Challenge combines the following specific tasks:
Code2code Translation, Handwritten Text recognition, Zero-shot Object
Detection, and Visual Question Answering. We have created datasets for each
task to test the participants' submissions on it. Moreover, we have collected
and made publicly available a new handwritten dataset in both English and
Russian, which consists of 94,128 pairs of images and texts. We also propose a
multimodal and multitask architecture - a baseline solution, in the center of
which is a frozen foundation model and which has been trained in Fusion mode
along with Single-task mode. The proposed Fusion approach proves to be
competitive and more energy-efficient compared to the task-specific one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Self-learning End-to-End Task-Oriented Dialog Systems. (arXiv:2201.06849v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06849">
<div class="article-summary-box-inner">
<span><p>End-to-end task bots are typically learned over a static and usually
limited-size corpus. However, when deployed in dynamic, changing, and open
environments to interact with users, task bots tend to fail when confronted
with data that deviate from the training corpus, i.e., out-of-distribution
samples. In this paper, we study the problem of automatically adapting task
bots to changing environments by learning from human-bot interactions with
minimum or zero human annotations. We propose SL-AGENT, a novel self-learning
framework for building end-to-end task bots. SL-AGENT consists of a dialog
model and a pre-trained reward model to predict the quality of an agent
response. It enables task bots to automatically adapt to changing environments
by learning from the unlabeled human-bot dialog logs accumulated after
deployment via reinforcement learning with the incorporated reward model.
Experimental results on four well-studied dialog tasks show the effectiveness
of SL-AGENT to automatically adapt to changing environments, using both
automatic and human evaluations. We will release code and data for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Consistency for Zero-Shot Task Generalization. (arXiv:2205.00049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00049">
<div class="article-summary-box-inner">
<span><p>One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10828">
<div class="article-summary-box-inner">
<span><p>Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations. (arXiv:2205.11023v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11023">
<div class="article-summary-box-inner">
<span><p>In software development, it is common for programmers to copy-paste or port
code snippets and then adapt them to their use case. This scenario motivates
the code adaptation task -- a variant of program repair which aims to adapt
variable identifiers in a pasted snippet of code to the surrounding,
preexisting source code. However, no existing approach has been shown to
effectively address this task. In this paper, we introduce AdaptivePaste, a
learning-based approach to source code adaptation, based on transformers and a
dedicated dataflow-aware deobfuscation pre-training task to learn meaningful
representations of variable usage patterns. We evaluate AdaptivePaste on a
dataset of code snippets in Python. Results suggest that our model can learn to
adapt source code with 79.8% accuracy. To evaluate how valuable is
AdaptivePaste in practice, we perform a user study with 10 Python developers on
a hundred real-world copy-paste instances. The results show that AdaptivePaste
reduces the dwell time to nearly half the time it takes for manual code
adaptation, and helps to avoid bugs. In addition, we utilize the participant
feedback to identify potential avenues for improvement of AdaptivePaste.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using attention methods to predict judicial outcomes. (arXiv:2207.08823v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08823">
<div class="article-summary-box-inner">
<span><p>Legal Judgment Prediction is one of the most acclaimed fields for the
combined area of NLP, AI, and Law. By legal prediction we mean an intelligent
systems capable to predict specific judicial characteristics, such as judicial
outcome, a judicial class, predict an specific case. In this research, we have
used AI classifiers to predict judicial outcomes in the Brazilian legal system.
For this purpose, we developed a text crawler to extract data from the official
Brazilian electronic legal systems. These texts formed a dataset of
second-degree murder and active corruption cases. We applied different
classifiers, such as Support Vector Machines and Neural Networks, to predict
judicial outcomes by analyzing textual features from the dataset. Our research
showed that Regression Trees, Gated Recurring Units and Hierarchical Attention
Networks presented higher metrics for different subsets. As a final goal, we
explored the weights of one of the algorithms, the Hierarchical Attention
Networks, to find a sample of the most important words used to absolve or
convict defendants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13243">
<div class="article-summary-box-inner">
<span><p>The last decade of machine learning has seen drastic increases in scale and
capabilities. Deep neural networks (DNNs) are increasingly being deployed in
the real world. However, they are difficult to analyze, raising concerns about
using them without a rigorous understanding of how they function. Effective
tools for interpreting them will be important for building more trustworthy AI
by helping to identify problems, fix bugs, and improve basic understanding. In
particular, "inner" interpretability techniques, which focus on explaining the
internal components of DNNs, are well-suited for developing a mechanistic
understanding, guiding manual modifications, and reverse engineering solutions.
</p>
<p>Much recent work has focused on DNN interpretability, and rapid progress has
thus far made a thorough systematization of methods difficult. In this survey,
we review over 300 works with a focus on inner interpretability tools. We
introduce a taxonomy that classifies methods by what part of the network they
help to explain (weights, neurons, subnetworks, or latent representations) and
whether they are implemented during (intrinsic) or after (post hoc) training.
To our knowledge, we are also the first to survey a number of connections
between interpretability research and work in adversarial robustness, continual
learning, modularity, network compression, and studying the human visual
system. We discuss key challenges and argue that the status quo in
interpretability research is largely unproductive. Finally, we highlight the
importance of future work that emphasizes diagnostics, debugging, adversaries,
and benchmarking in order to make interpretability tools more useful to
engineers in practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Long-Text Understanding with Short-Text Models. (arXiv:2208.00748v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00748">
<div class="article-summary-box-inner">
<span><p>Transformer-based pretrained language models (LMs) are ubiquitous across
natural language understanding, but cannot be applied to long sequences such as
stories, scientific articles and long documents, due to their quadratic
complexity. While a myriad of efficient transformer variants have been
proposed, they are typically based on custom implementations that require
expensive pretraining from scratch. In this work, we propose SLED:
SLiding-Encoder and Decoder, a simple approach for processing long sequences
that re-uses and leverages battle-tested short-text pretrained LMs.
Specifically, we partition the input into overlapping chunks, encode each with
a short-text LM encoder and use the pretrained decoder to fuse information
across chunks (fusion-in-decoder). We illustrate through controlled experiments
that SLED offers a viable strategy for long text understanding and evaluate our
approach on SCROLLS, a benchmark with seven datasets across a wide range of
language understanding tasks. We find that SLED is competitive with specialized
models that are up to 50x larger and require a dedicated and expensive
pretraining step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10260">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a traditional task in natural language
processing. In particular, nested entity recognition receives extensive
attention for the widespread existence of the nesting scenario. The latest
research migrates the well-established paradigm of set prediction in object
detection to cope with entity nesting. However, the manual creation of query
vectors, which fail to adapt to the rich semantic information in the context,
limits these approaches. An end-to-end entity detection approach with proposer
and regressor is presented in this paper to tackle the issues. First, the
proposer utilizes the feature pyramid network to generate high-quality entity
proposals. Then, the regressor refines the proposals for generating the final
prediction. The model adopts encoder-only architecture and thus obtains the
advantages of the richness of query semantics, high precision of entity
localization, and easiness of model training. Moreover, we introduce the novel
spatially modulated attention and progressive refinement for further
improvement. Extensive experiments demonstrate that our model achieves advanced
performance in flat and nested NER, achieving a new state-of-the-art F1 score
of 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17114">
<div class="article-summary-box-inner">
<span><p>Limited computational budgets often prevent transformers from being used in
production and from having their high accuracy utilized. A knowledge
distillation approach addresses the computational efficiency by self-distilling
BERT into a smaller transformer representation having fewer layers and smaller
internal embedding. However, the performance of these models drops as we reduce
the number of layers, notably in advanced NLP tasks such as span question
answering. In addition, a separate model must be trained for each inference
scenario with its distinct computational budget. Dynamic-TinyBERT tackles both
limitations by partially implementing the Length Adaptive Transformer (LAT)
technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal
accuracy loss. In this work, we expand the Dynamic-TinyBERT approach to
generate a much more highly efficient model. We use MiniLM distillation jointly
with the LAT method, and we further enhance the efficiency by applying low-bit
quantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is
trained only once, dynamically fits any inference scenario, and achieves an
accuracy-efficiency trade-off superior to any other efficient approaches per
any computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with &lt;1%
accuracy loss). The code to reproduce this work is publicly available on
Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning. (arXiv:2212.03230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03230">
<div class="article-summary-box-inner">
<span><p>Discriminativeness is a desirable feature of image captions: captions should
describe the characteristic details of input images. However, recent
high-performing captioning models, which are trained with reinforcement
learning (RL), tend to generate overly generic captions despite their high
performance in various other criteria. First, we investigate the cause of the
unexpectedly low discriminativeness and show that RL has a deeply rooted side
effect of limiting the output words to high-frequency words. The limited
vocabulary is a severe bottleneck for discriminativeness as it is difficult for
a model to describe the details beyond its vocabulary. Then, based on this
identification of the bottleneck, we drastically recast discriminative image
captioning as a much simpler task of encouraging low-frequency word generation.
Hinted by long-tail classification and debiasing methods, we propose methods
that easily switch off-the-shelf RL models to discriminativeness-aware models
with only a single-epoch fine-tuning on the part of the parameters. Extensive
experiments demonstrate that our methods significantly enhance the
discriminativeness of off-the-shelf RL models and even outperform previous
discriminativeness-aware methods with much smaller computational costs.
Detailed analysis and human evaluation also verify that our methods boost the
discriminativeness without sacrificing the overall quality of captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans. (arXiv:2212.05546v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05546">
<div class="article-summary-box-inner">
<span><p>Importance: Social determinants of health (SDOH) are known to be associated
with increased risk of suicidal behaviors, but few studies utilized SDOH from
unstructured electronic health record (EHR) notes.
</p>
<p>Objective: To investigate associations between suicide and recent SDOH,
identified using structured and unstructured data.
</p>
<p>Design: Nested case-control study.
</p>
<p>Setting: EHR data from the US Veterans Health Administration (VHA).
</p>
<p>Participants: 6,122,785 Veterans who received care in the US VHA between
October 1, 2010, and September 30, 2015.
</p>
<p>Exposures: Occurrence of SDOH over a maximum span of two years compared with
no occurrence of SDOH.
</p>
<p>Main Outcomes and Measures: Cases of suicide deaths were matched with 4
controls on birth year, cohort entry date, sex, and duration of follow-up. We
developed an NLP system to extract SDOH from unstructured notes. Structured
data, NLP on unstructured data, and combining them yielded six, eight and nine
SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals
(CIs) were estimated using conditional logistic regression.
</p>
<p>Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382
person-years of follow-up (incidence rate 37.18/100,000 person-years). Our
cohort was mostly male (92.23%) and white (76.99%). Across the five common SDOH
as covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH
occurrences. All SDOH, measured by structured data and NLP, were significantly
associated with increased risk of suicide. The SDOH with the largest effects
was legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12,
95% CI=1.98-2.27). NLP-extracted and structured SDOH were also associated with
suicide.
</p>
<p>Conclusions and Relevance: NLP-extracted SDOH were always significantly
associated with increased risk of suicide among Veterans, suggesting the
potential of NLP in public health studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07428">
<div class="article-summary-box-inner">
<span><p>We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners. (arXiv:2212.10873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10873">
<div class="article-summary-box-inner">
<span><p>Through in-context learning (ICL), large-scale language models are effective
few-shot learners without additional model fine-tuning. However, the ICL
performance does not scale well with the number of available training samples
as it is limited by the inherent input length constraint of the underlying
language model. Meanwhile, many studies have revealed that language models are
also powerful feature extractors, allowing them to be utilized in a black-box
manner and enabling the linear probing paradigm, where lightweight
discriminators are trained on top of the pre-extracted input representations.
This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear
probing and ICL, which leverages the best of both worlds. PALP inherits the
scalability of linear probing and the capability of enforcing language models
to derive more meaningful representations via tailoring input into a more
conceivable form. Throughout in-depth investigations on various datasets, we
verified that PALP significantly enhances the input representations closing the
gap between ICL in the data-hungry scenario and fine-tuning in the
data-abundant scenario with little training overhead, potentially making PALP a
strong alternative in a black-box scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Safe and Usable Chatbots for Promoting Voter Participation. (arXiv:2212.11219v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.11219">
<div class="article-summary-box-inner">
<span><p>Chatbots, or bots for short, are multi-modal collaborative assistants that
can help people complete useful tasks. Usually, when chatbots are referenced in
connection with elections, they often draw negative reactions due to the fear
of mis-information and hacking. Instead, in this paper, we explore how chatbots
may be used to promote voter participation in vulnerable segments of society
like senior citizens and first-time voters. In particular, we build a system
that amplifies official information while personalizing it to users' unique
needs transparently. We discuss its design, build prototypes with frequently
asked questions (FAQ) election information for two US states that are low on an
ease-of-voting scale, and report on its initial evaluation in a focus group.
Our approach can be a win-win for voters, election agencies trying to fulfill
their mandate and democracy at large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. (arXiv:2212.12017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12017">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that fine-tuning large pre-trained language models on a
collection of tasks described via instructions, a.k.a. instruction-tuning,
improves their zero and few-shot generalization to unseen tasks. However, there
is a limited understanding of the performance trade-offs of different decisions
made during the instruction-tuning process. These decisions include the scale
and diversity of the instruction-tuning benchmark, different task sampling
strategies, fine-tuning with and without demonstrations, training using
specialized datasets for reasoning and dialogue, and finally, the fine-tuning
objectives themselves. In this paper, we characterize the effect of
instruction-tuning decisions on downstream task performance when scaling both
model and benchmark sizes. To this end, we create OPT-IML Bench: a large
benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated
into task categories from 8 existing benchmarks, and prepare an evaluation
framework to measure three types of model generalizations: to tasks from fully
held-out categories, to held-out tasks from seen categories, and to held-out
instances from seen tasks. Through the lens of this framework, we first present
insights about instruction-tuning decisions as applied to OPT-30B and further
exploit these insights to train OPT-IML 30B and 175B, which are
instruction-tuned versions of OPT. OPT-IML demonstrates all three
generalization abilities at both scales on four different evaluation benchmarks
with diverse tasks and input formats -- PromptSource, FLAN,
Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly
outperform OPT on all benchmarks but is also highly competitive with existing
models fine-tuned on each specific benchmark. We release OPT-IML at both
scales, together with the OPT-IML Bench evaluation framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework of Customer Review Analysis Using the Aspect-Based Opinion Mining Approach. (arXiv:2212.10051v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10051">
<div class="article-summary-box-inner">
<span><p>Opinion mining is the branch of computation that deals with opinions,
appraisals, attitudes, and emotions of people and their different aspects. This
field has attracted substantial research interest in recent years. Aspect-level
(called aspect-based opinion mining) is often desired in practical applications
as it provides detailed opinions or sentiments about different aspects of
entities and entities themselves, which are usually required for action. Aspect
extraction and entity extraction are thus two core tasks of aspect-based
opinion mining. his paper has presented a framework of aspect-based opinion
mining based on the concept of transfer learning. on real-world customer
reviews available on the Amazon website. The model has yielded quite
satisfactory results in its task of aspect-based opinion mining.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-29 23:13:22.478412502 UTC">2022-12-29 23:13:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>