<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-11T01:30:00Z">11-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Multi-Corpora Language Model Training for Speech Recognition. (arXiv:2211.05121v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05121">
<div class="article-summary-box-inner">
<span><p>Neural network language model (NNLM) plays an essential role in automatic
speech recognition (ASR) systems, especially in adaptation tasks when text-only
data is available. In practice, an NNLM is typically trained on a combination
of data sampled from multiple corpora. Thus, the data sampling strategy is
important to the adaptation performance. Most existing works focus on designing
static sampling strategies. However, each corpus may show varying impacts at
different NNLM training stages. In this paper, we introduce a novel adaptive
multi-corpora training algorithm that dynamically learns and adjusts the
sampling probability of each corpus along the training process. The algorithm
is robust to corpora sizes and domain relevance. Compared with static sampling
strategy baselines, the proposed approach yields remarkable improvement by
achieving up to relative 7% and 9% word error rate (WER) reductions on
in-domain and out-of-domain adaptation tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Zero-shot Event Extraction with Context-Definition Alignment. (arXiv:2211.05156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05156">
<div class="article-summary-box-inner">
<span><p>Event extraction (EE) is the task of identifying interested event mentions
from text. Conventional efforts mainly focus on the supervised setting.
However, these supervised models cannot generalize to event types out of the
pre-defined ontology. To fill this gap, many efforts have been devoted to the
zero-shot EE problem. This paper follows the trend of modeling event-type
semantics but moves one step further. We argue that using the static embedding
of the event type name might not be enough because a single word could be
ambiguous, and we need a sentence to define the type semantics accurately. To
model the definition semantics, we use two separate transformer models to
project the contextualized event mentions and corresponding definitions into
the same embedding space and then minimize their embedding distance via
contrastive learning. On top of that, we also propose a warming phase to help
the model learn the minor difference between similar definitions. We name our
approach Zero-shot Event extraction with Definition (ZED). Experiments on the
MAVEN dataset show that our model significantly outperforms all previous
zero-shot EE methods with fast inference speed due to the disjoint design.
Further experiments also show that ZED can be easily applied to the few-shot
setting when the annotation is available and consistently outperforms baseline
supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database. (arXiv:2211.05165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05165">
<div class="article-summary-box-inner">
<span><p>Parsing natural language questions into executable logical forms is a useful
and interpretable way to perform question answering on structured data such as
knowledge bases (KB) or databases (DB). However, existing approaches on
semantic parsing cannot adapt to both modalities, as they suffer from the
exponential growth of the logical form candidates and can hardly generalize to
unseen data. In this work, we propose Uni-Parser, a unified semantic parser for
question answering (QA) on both KB and DB. We introduce the primitive (relation
and entity in KB, and table name, column name and cell value in DB) as an
essential element in our framework. The number of primitives grows linearly
with the number of retrieved relations in KB and DB, preventing us from dealing
with exponential logic form candidates. We leverage the generator to predict
final logical forms by altering and composing topranked primitives with
different operations (e.g. select, where, count). With sufficiently pruned
search space by a contrastive primitive ranker, the generator is empowered to
capture the composition of primitives enhancing its generalization ability. We
achieve competitive results on multiple KB and DB QA benchmarks more
efficiently, especially in the compositional and zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05166">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Correction (GEC) is the task of automatically detecting and
correcting errors in text. The task not only includes the correction of
grammatical errors, such as missing prepositions and mismatched subject-verb
agreement, but also orthographic and semantic errors, such as misspellings and
word choice errors respectively. The field has seen significant progress in the
last decade, motivated in part by a series of five shared tasks, which drove
the development of rule-based methods, statistical classifiers, statistical
machine translation, and finally neural machine translation systems which
represent the current dominant state of the art. In this survey paper, we
condense the field into a single article and first outline some of the
linguistic challenges of the task, introduce the most popular datasets that are
available to researchers (for both English and other languages), and summarise
the various methods and techniques that have been developed with a particular
focus on artificial error generation. We next describe the many different
approaches to evaluation as well as concerns surrounding metric reliability,
especially in relation to subjective human judgements, before concluding with
an overview of recent progress and suggestions for future work and remaining
challenges. We hope that this survey will serve as comprehensive resource for
researchers who are new to the field or who want to be kept apprised of recent
developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech separation with large-scale self-supervised learning. (arXiv:2211.05172v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05172">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods such as WavLM have shown promising
speech separation (SS) results in small-scale simulation-based experiments. In
this work, we extend the exploration of the SSL-based SS by massively scaling
up both the pre-training data (more than 300K hours) and fine-tuning data (10K
hours). We also investigate various techniques to efficiently integrate the
pre-trained model with the SS network under a limited computation budget,
including a low frame rate SSL model training setup and a fine-tuning scheme
using only the part of the pre-trained model. Compared with a supervised
baseline and the WavLM-based SS model using feature embeddings obtained with
the previously released 94K hours trained WavLM, our proposed model obtains
15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for
a simulated far-field speech mixture test set. For conversation transcription
on real meeting recordings using continuous speech separation, the proposed
model achieves 6.8% and 10.6% of relative WER reductions over the purely
supervised baseline on AMI and ICSI evaluation sets, respectively, while
reducing the computational cost by 38%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reasoning-Aware Explainable VQA. (arXiv:2211.05190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05190">
<div class="article-summary-box-inner">
<span><p>The domain of joint vision-language understanding, especially in the context
of reasoning in Visual Question Answering (VQA) models, has garnered
significant attention in the recent past. While most of the existing VQA models
focus on improving the accuracy of VQA, the way models arrive at an answer is
oftentimes a black box. As a step towards making the VQA task more explainable
and interpretable, our method is built upon the SOTA VQA framework by
augmenting it with an end-to-end explanation generation module. In this paper,
we investigate two network architectures, including Long Short-Term Memory
(LSTM) and Transformer decoder, as the explanation generator. Our method
generates human-readable textual explanations while maintaining SOTA VQA
accuracy on the GQA-REX (77.49%) and VQA-E (71.48%) datasets. Approximately
65.16% of the generated explanations are approved by humans as valid. Roughly
60.5% of the generated explanations are valid and lead to the correct answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collateral facilitation in humans and language models. (arXiv:2211.05198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05198">
<div class="article-summary-box-inner">
<span><p>Are the predictions of humans and language models affected by similar things?
Research suggests that while comprehending language, humans make predictions
about upcoming words, with more predictable words being processed more easily.
However, evidence also shows that humans display a similar processing advantage
for highly anomalous words when these words are semantically related to the
preceding context or to the most probable continuation. Using stimuli from 3
psycholinguistic experiments, we find that this is also almost always also the
case for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa,
XLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of
this phenomenon for our understanding of both human language comprehension and
the predictions made by language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HilMeMe: A Human-in-the-Loop Machine Translation Evaluation Metric Looking into Multi-Word Expressions. (arXiv:2211.05201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05201">
<div class="article-summary-box-inner">
<span><p>With the fast development of Machine Translation (MT) systems, especially the
new boost from Neural MT (NMT) models, the MT output quality has reached a new
level of accuracy. However, many researchers criticised that the current
popular evaluation metrics such as BLEU can not correctly distinguish the
state-of-the-art NMT systems regarding quality differences. In this short
paper, we describe the design and implementation of a linguistically motivated
human-in-the-loop evaluation metric looking into idiomatic and terminological
Multi-word Expressions (MWEs). MWEs have played a bottleneck in many Natural
Language Processing (NLP) tasks including MT. MWEs can be used as one of the
main factors to distinguish different MT systems by looking into their
capabilities in recognising and translating MWEs in an accurate and meaning
equivalent manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Based Combination of Convolutional and Recurrent Neural Network for Indonesian Sentiment Analysis. (arXiv:2211.05273v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05273">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is the computational study of opinions and emotions
ex-pressed in text. Deep learning is a model that is currently producing
state-of-the-art in various application domains, including sentiment analysis.
Many researchers are using a hybrid approach that combines different deep
learning models and has been shown to improve model performance. In sentiment
analysis, input in text data is first converted into a numerical
representation. The standard method used to obtain a text representation is the
fine-tuned embedding method. However, this method does not pay attention to
each word's context in the sentence. Therefore, the Bidirectional Encoder
Representation from Transformer (BERT) model is used to obtain text
representations based on the context and position of words in sentences. This
research extends the previous hybrid deep learning using BERT representation
for Indonesian sentiment analysis. Our simulation shows that the BERT
representation improves the accuracies of all hybrid architectures. The
BERT-based LSTM-CNN also reaches slightly better accuracies than other
BERT-based hybrid architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information. (arXiv:2211.05284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05284">
<div class="article-summary-box-inner">
<span><p>Online forms are widely used to collect data from human and have a
multi-billion market. Many software products provide online services for
creating semi-structured forms where questions and descriptions are organized
by pre-defined structures. However, the design and creation process of forms is
still tedious and requires expert knowledge. To assist form designers, in this
work we present FormLM to model online forms (by enhancing pre-trained language
model with form structural information) and recommend form creation ideas
(including question / options recommendations and block type suggestion). For
model training and evaluation, we collect the first public online form dataset
with 62K online forms. Experiment results show that FormLM significantly
outperforms general-purpose language models on all tasks, with an improvement
by 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms
of ROUGE-1 and Macro-F1, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Classification with Hypersphere Modeling of Prototypes. (arXiv:2211.05319v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05319">
<div class="article-summary-box-inner">
<span><p>Metric-based meta-learning is one of the de facto standards in few-shot
learning. It composes of representation learning and metrics calculation
designs. Previous works construct class representations in different ways,
varying from mean output embedding to covariance and distributions. However,
using embeddings in space lacks expressivity and cannot capture class
information robustly, while statistical complex modeling poses difficulty to
metric designs. In this work, we use tensor fields (``areas'') to model classes
from the geometrical perspective for few-shot learning. We present a simple and
effective method, dubbed hypersphere prototypes (HyperProto), where class
information is represented by hyperspheres with dynamic sizes with two sets of
learnable parameters: the hypersphere's center and the radius. Extending from
points to areas, hyperspheres are much more expressive than embeddings.
Moreover, it is more convenient to perform metric-based classification with
hypersphere prototypes than statistical modeling, as we only need to calculate
the distance from a data point to the surface of the hypersphere. Following
this idea, we also develop two variants of prototypes under other measurements.
Extensive experiments and analysis on few-shot learning tasks across NLP and CV
and comparison with 20+ competitive baselines demonstrate the effectiveness of
our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling. (arXiv:2211.05343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05343">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) aims to identify semantic labels
among entities within a single document. One major challenge of DocRE is to dig
decisive details regarding a specific entity pair from long text. However, in
many cases, only a fraction of text carries required information, even in the
manually labeled supporting evidence. To better capture and exploit instructive
information, we propose a novel expLicit syntAx Refinement and Subsentence
mOdeliNg based framework (LARSON). By introducing extra syntactic information,
LARSON can model subsentences of arbitrary granularity and efficiently screen
instructive ones. Moreover, we incorporate refined syntax into text
representations which further improves the performance of LARSON. Experimental
results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that
LARSON significantly outperforms existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LERT: A Linguistically-motivated Pre-trained Language Model. (arXiv:2211.05344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05344">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Model (PLM) has become a representative foundation model
in the natural language processing field. Most PLMs are trained with
linguistic-agnostic pre-training tasks on the surface form of the text, such as
the masked language model (MLM). To further empower the PLMs with richer
linguistic features, in this paper, we aim to propose a simple but effective
way to learn linguistic features for pre-trained language models. We propose
LERT, a pre-trained language model that is trained on three types of linguistic
features along with the original MLM pre-training task, using a
linguistically-informed pre-training (LIP) strategy. We carried out extensive
experiments on ten Chinese NLU tasks, and the experimental results show that
LERT could bring significant improvements over various comparable baselines.
Furthermore, we also conduct analytical experiments in various linguistic
aspects, and the results prove that the design of LERT is valid and effective.
Resources are available at https://github.com/ymcui/LERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSDT: Masked Language Model Scoring Defense in Text Domain. (arXiv:2211.05371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05371">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models allowed us to process downstream tasks with the
help of fine-tuning, which aids the model to achieve fairly high accuracy in
various Natural Language Processing (NLP) tasks. Such easily-downloaded
language models from various websites empowered the public users as well as
some major institutions to give a momentum to their real-life application.
However, it was recently proven that models become extremely vulnerable when
they are backdoor attacked with trigger-inserted poisoned datasets by malicious
users. The attackers then redistribute the victim models to the public to
attract other users to use them, where the models tend to misclassify when
certain triggers are detected within the training sample. In this paper, we
will introduce a novel improved textual backdoor defense method, named MSDT,
that outperforms the current existing defensive algorithms in specific
datasets. The experimental results illustrate that our method can be effective
and constructive in terms of defending against backdoor attack in text domain.
Code is available at https://github.com/jcroh0508/MSDT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvEntS ReaLM: Event Reasoning of Entity States via Language Models. (arXiv:2211.05392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05392">
<div class="article-summary-box-inner">
<span><p>This paper investigates models of event implications. Specifically, how well
models predict entity state-changes, by targeting their understanding of
physical attributes. Nominally, Large Language models (LLM) have been exposed
to procedural knowledge about how objects interact, yet our benchmarking shows
they fail to reason about the world. Conversely, we also demonstrate that
existing approaches often misrepresent the surprising abilities of LLMs via
improper task encodings and that proper model prompting can dramatically
improve performance of reported baseline results across multiple tasks. In
particular, our results indicate that our prompting technique is especially
useful for unseen attributes (out-of-domain) or when only limited data is
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zebra: Deeply Integrating System-Level Provenance Search and Tracking for Efficient Attack Investigation. (arXiv:2211.05403v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05403">
<div class="article-summary-box-inner">
<span><p>System auditing has emerged as a key approach for monitoring system call
events and investigating sophisticated attacks. Based on the collected audit
logs, research has proposed to search for attack patterns or track the causal
dependencies of system events to reveal the attack sequence. However, existing
approaches either cannot reveal long-range attack sequences or suffer from the
dependency explosion problem due to a lack of focus on attack-relevant parts,
and thus are insufficient for investigating complex attacks.
</p>
<p>To bridge the gap, we propose Zebra, a system that synergistically integrates
attack pattern search and causal dependency tracking for efficient attack
investigation. With Zebra, security analysts can alternate between search and
tracking to reveal the entire attack sequence in a progressive, user-guided
manner, while mitigating the dependency explosion problem by prioritizing the
attack-relevant parts. To enable this, Zebra provides (1) an expressive and
concise domain-specific language, Tstl, for performing various types of search
and tracking analyses, and (2) an optimized language execution engine for
efficient execution over a big amount of auditing data. Evaluations on a broad
set of attack cases demonstrate the effectiveness of Zebra in facilitating a
timely attack investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VieCap4H - VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05405">
<div class="article-summary-box-inner">
<span><p>Image captioning is currently a challenging task that requires the ability to
both understand visual information and use human language to describe this
visual information in the image. In this paper, we propose an efficient way to
improve the image understanding ability of transformer-based method by
extending Object Relation Transformer architecture with Attention on Attention
mechanism. Experiments on the VieCap4H dataset show that our proposed method
significantly outperforms its original structure on both the public test and
private test of the Image Captioning shared task held by VLSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UIT-HWDB: Using Transferring Method to Construct A Novel Benchmark for Evaluating Unconstrained Handwriting Image Recognition in Vietnamese. (arXiv:2211.05407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05407">
<div class="article-summary-box-inner">
<span><p>Recognizing handwriting images is challenging due to the vast variation in
writing style across many people and distinct linguistic aspects of writing
languages. In Vietnamese, besides the modern Latin characters, there are accent
and letter marks together with characters that draw confusion to
state-of-the-art handwriting recognition methods. Moreover, as a low-resource
language, there are not many datasets for researching handwriting recognition
in Vietnamese, which makes handwriting recognition in this language have a
barrier for researchers to approach. Recent works evaluated offline handwriting
recognition methods in Vietnamese using images from an online handwriting
dataset constructed by connecting pen stroke coordinates without further
processing. This approach obviously can not measure the ability of recognition
methods effectively, as it is trivial and may be lack of features that are
essential in offline handwriting images. Therefore, in this paper, we propose
the Transferring method to construct a handwriting image dataset that
associates crucial natural attributes required for offline handwriting images.
Using our method, we provide a first high-quality synthetic dataset which is
complex and natural for efficiently evaluating handwriting recognition methods.
In addition, we conduct experiments with various state-of-the-art methods to
figure out the challenge to reach the solution for handwriting recognition in
Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADEPT: A DEbiasing PrompT Framework. (arXiv:2211.05414v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05414">
<div class="article-summary-box-inner">
<span><p>Several works have proven that finetuning is an applicable approach for
debiasing contextualized word embeddings. Similarly, discrete prompts with
semantic meanings have shown to be effective in debiasing tasks. With unfixed
mathematical representation at the token level, continuous prompts usually
surpass discrete ones at providing a pre-trained language model (PLM) with
additional task-specific information. Despite this, relatively few efforts have
been made to debias PLMs by prompt tuning with continuous prompts compared to
its discrete counterpart. Furthermore, for most debiasing methods that alter a
PLM's original parameters, a major problem is the need to not only decrease the
bias in the PLM but also to ensure that the PLM does not lose its
representation ability. Finetuning methods typically have a hard time
maintaining this balance, as they tend to violently remove meanings of
attribute words. In this paper, we propose ADEPT, a method to debias PLMs using
prompt tuning while maintaining the delicate balance between removing biases
and ensuring representation ability. To achieve this, we propose a new training
criterion inspired by manifold learning and equip it with an explicit debiasing
term to optimize prompt tuning. In addition, we conduct several experiments
with regard to the reliability, quality, and quantity of a previously proposed
attribute training corpus in order to obtain a clearer prototype of a certain
attribute, which indicates the attribute's position and relative distances to
other words on the manifold. We evaluate ADEPT on several widely acknowledged
debiasing benchmarks and downstream tasks, and find that it achieves
competitive results while maintaining (and in some cases even improving) the
PLM's representation ability. We further visualize words' correlation before
and after debiasing a PLM, and give some possible explanations for the visible
effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Transformers Reason in Fragments of Natural Language?. (arXiv:2211.05417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05417">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep-learning-based approaches to Natural Language
Processing (NLP) are credited with various capabilities that involve reasoning
with natural language texts. In this paper we carry out a large-scale empirical
study investigating the detection of formally valid inferences in controlled
fragments of natural language for which the satisfiability problem becomes
increasingly complex. We find that, while transformer-based language models
perform surprisingly well in these scenarios, a deeper analysis re-veals that
they appear to overfit to superficial patterns in the data rather than
acquiring the logical principles governing the reasoning in these fragments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Human-Centred Explainability Benchmarks For Text Classification. (arXiv:2211.05452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05452">
<div class="article-summary-box-inner">
<span><p>Progress on many Natural Language Processing (NLP) tasks, such as text
classification, is driven by objective, reproducible and scalable evaluation
via publicly available benchmarks. However, these are not always representative
of real-world scenarios where text classifiers are employed, such as sentiment
analysis or misinformation detection. In this position paper, we put forward
two points that aim to alleviate this problem. First, we propose to extend text
classification benchmarks to evaluate the explainability of text classifiers.
We review challenges associated with objectively evaluating the capabilities to
produce valid explanations which leads us to the second main point: We propose
to ground these benchmarks in human-centred applications, for example by using
social media, gamification or to learn explainability metrics from human
judgements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Text Classification Data and Models Using Aggregated Input Salience. (arXiv:2211.05485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05485">
<div class="article-summary-box-inner">
<span><p>Realizing when a model is right for a wrong reason is not trivial and
requires a significant effort by model developers. In some cases, an input
salience method, which highlights the most important parts of the input, may
reveal problematic reasoning. But scrutinizing highlights over many data
instances is tedious and often infeasible. Furthermore, analyzing examples in
isolation does not reveal general patterns in the data or in the model's
behavior.In this paper we aim to address these issues and go from understanding
single examples to understanding entire datasets and models. The methodology we
propose is based on aggregated salience maps. Using this methodology we address
multiple distinct but common model developer needs by showing how problematic
data and model behavior can be identified -- a necessary first step for
improving the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05503">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) aims to convert the dialogue history into
dialogue states which consist of slot-value pairs. As condensed structural
information memorizing all history information, the dialogue state in the last
turn is typically adopted as the input for predicting the current state by DST
models. However, these models tend to keep the predicted slot values unchanged,
which is defined as state momentum in this paper. Specifically, the models
struggle to update slot values that need to be changed and correct wrongly
predicted slot values in the last turn. To this end, we propose MoNET to tackle
state momentum via noise-enhanced training. First, the previous state of each
turn in the training data is noised via replacing some of its slot values.
Then, the noised previous state is used as the input to learn to predict the
current state, improving the model's ability to update and correct slot values.
Furthermore, a contrastive context matching framework is designed to narrow the
representation distance between a state and its corresponding noised variant,
which reduces the impact of noised state and makes the model better understand
the dialogue history. Experimental results on MultiWOZ datasets show that MoNET
outperforms previous DST methods. Ablations and analysis verify the
effectiveness of MoNET in alleviating state momentum and improving anti-noise
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05523">
<div class="article-summary-box-inner">
<span><p>Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of BERT-like language models. Our findings
suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveal that
the improved generalization is due to `more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GREENER: Graph Neural Networks for News Media Profiling. (arXiv:2211.05533v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05533">
<div class="article-summary-box-inner">
<span><p>We study the problem of profiling news media on the Web with respect to their
factuality of reporting and bias. This is an important but under-studied
problem related to disinformation and "fake news" detection, but it addresses
the issue at a coarser granularity compared to looking at an individual article
or an individual claim. This is useful as it allows to profile entire media
outlets in advance. Unlike previous work, which has focused primarily on text
(e.g.,~on the text of the articles published by the target website, or on the
textual description in their social media profiles or in Wikipedia), here our
main focus is on modeling the similarity between media outlets based on the
overlap of their audience. This is motivated by homophily considerations,
i.e.,~the tendency of people to have connections to people with similar
interests, which we extend to media, hypothesizing that similar types of media
would be read by similar kinds of users. In particular, we propose GREENER
(GRaph nEural nEtwork for News mEdia pRofiling), a model that builds a graph of
inter-media connections based on their audience overlap, and then uses graph
neural networks to represent each medium. We find that such representations are
quite useful for predicting the factuality and the bias of news media outlets,
yielding improvements over state-of-the-art results reported on two datasets.
When augmented with conventionally used representations obtained from news
articles, Twitter, YouTube, Facebook, and Wikipedia, prediction accuracy is
found to improve by 2.5-27 macro-F1 points for the two tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assistive Completion of Agrammatic Aphasic Sentences: A Transfer Learning Approach using Neurolinguistics-based Synthetic Dataset. (arXiv:2211.05557v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05557">
<div class="article-summary-box-inner">
<span><p>Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic
aphasia wherein patients, although able to comprehend, lack the ability to form
complete sentences. This inability leads to communication gaps which cause
difficulties in their daily lives. The usage of assistive devices can help in
mitigating these issues and enable the patients to communicate effectively.
However, due to lack of large scale studies of linguistic deficits in aphasia,
research on such assistive technology is relatively limited. In this work, we
present two contributions that aim to re-initiate research and development in
this field. Firstly, we propose a model that uses linguistic features from
small scale studies on aphasia patients and generates large scale datasets of
synthetic aphasic utterances from grammatically correct datasets. We show that
the mean length of utterance, the noun/verb ratio, and the simple/complex
sentence ratio of our synthetic datasets correspond to the reported features of
aphasic speech. Further, we demonstrate how the synthetic datasets may be
utilized to develop assistive devices for aphasia patients. The pre-trained T5
transformer is fine-tuned using the generated dataset to suggest 5 corrected
sentences given an aphasic utterance as input. We evaluate the efficacy of the
T5 model using the BLEU and cosine semantic similarity scores. Affirming
results with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00
were obtained. These results provide a strong foundation for the concept that a
synthetic dataset based on small scale studies on aphasia can be used to
develop effective assistive technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Soft Labels for Out-of-Domain Intent Detection. (arXiv:2211.05561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05561">
<div class="article-summary-box-inner">
<span><p>Out-of-Domain (OOD) intent detection is important for practical dialog
systems. To alleviate the issue of lacking OOD training samples, some works
propose synthesizing pseudo OOD samples and directly assigning one-hot OOD
labels to these pseudo samples. However, these one-hot labels introduce noises
to the training process because some hard pseudo OOD samples may coincide with
In-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo
labeling (ASoul) method that can estimate soft labels for pseudo OOD samples
when training OOD detectors. Semantic connections between pseudo OOD samples
and IND intents are captured using an embedding graph. A co-training framework
is further introduced to produce resulting soft labels following the smoothness
assumption, i.e., close samples are likely to have similar labels. Extensive
experiments on three benchmark datasets show that ASoul consistently improves
the OOD detection performance and outperforms various competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer-Aided Modelling of the Bilingual Word Indices to the Ninth-Century Uchitel'noe evangelie. (arXiv:2211.05579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05579">
<div class="article-summary-box-inner">
<span><p>The development of bilingual dictionaries to medieval translations presents
diverse difficulties. These result from two types of philological
circumstances: a) the asymmetry between the source language and the target
language; and b) the varying available sources of both the original and
translated texts. In particular, the full critical edition of Tihova of
Constantine of Preslav's Uchitel'noe evangelie ('Didactic Gospel') gives a
relatively good idea of the Old Church Slavonic translation but not of its
Greek source text. This is due to the fact that Cramer's edition of the catenae
- used as the parallel text in it - is based on several codices whose text does
not fully coincide with the Slavonic. This leads to the addition of the
newly-discovered parallels from Byzantine manuscripts and John Chrysostom's
homilies. Our approach to these issues is a step-wise process with two main
goals: a) to facilitate the philological annotation of input data and b) to
consider the manifestations of the mentioned challenges, first, separately in
order to simplify their resolution, and, then, in their combination. We
demonstrate how we model various types of asymmetric translation correlates and
the variability resulting from the pluralism of sources. We also demonstrate
how all these constructions are being modelled and processed into the final
indices. Our approach is designed with generalisation in mind and is intended
to be applicable also for other translations from Greek into Old Church
Slavonic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence. (arXiv:2211.05583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05583">
<div class="article-summary-box-inner">
<span><p>Developing Piping and Instrumentation Diagrams (P&amp;IDs) is a crucial step
during the development of chemical processes. Currently, this is a tedious,
manual, and time-consuming task. We propose a novel, completely data-driven
method for the prediction of control structures. Our methodology is inspired by
end-to-end transformer-based human language translation models. We cast the
control structure prediction as a translation task where Process Flow Diagrams
(PFDs) are translated to P&amp;IDs. To use established transformer-based language
translation models, we represent the P&amp;IDs and PFDs as strings using our
recently proposed SFILES 2.0 notation. Model training is performed in a
transfer learning approach. Firstly, we pre-train our model using generated
P&amp;IDs to learn the grammatical structure of the process diagrams. Thereafter,
the model is fine-tuned leveraging transfer learning on real P&amp;IDs. The model
achieved a top-5 accuracy of 74.8% on 10,000 generated P&amp;IDs and 89.2% on
100,000 generated P&amp;IDs. These promising results show great potential for
AI-assisted process engineering. The tests on a dataset of 312 real P&amp;IDs
indicate the need of a larger P&amp;IDs dataset for industry applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis. (arXiv:2211.05584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05584">
<div class="article-summary-box-inner">
<span><p>The invention of transformer-based models such as BERT, GPT, and RoBERTa has
enabled researchers and financial companies to finetune these powerful models
and use them in different downstream tasks to achieve state-of-the-art
performance. Recently, a lightweight alternative (approximately 0.1% - 3% of
the original model parameters) to fine-tuning, known as prefix tuning has been
introduced. This method freezes the model parameters and only updates the
prefix to achieve performance comparable to full fine-tuning. Prefix tuning
enables researchers and financial practitioners to achieve similar results with
much fewer parameters. In this paper, we explore the robustness of prefix
tuning when facing noisy data. Our experiments demonstrate that fine-tuning is
more robust to noise than prefix tuning -- the latter method faces a
significant decrease in performance on most corrupted data sets with increasing
noise levels. Furthermore, prefix tuning has high variances in the F1 scores
compared to fine-tuning in many corruption methods. We strongly advocate that
caution should be carefully taken when applying the state-of-the-art prefix
tuning method to noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Learning for Domain Adaptation in Task-Oriented Dialogue. (arXiv:2211.05596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05596">
<div class="article-summary-box-inner">
<span><p>Conversation designers continue to face significant obstacles when creating
production quality task-oriented dialogue systems. The complexity and cost
involved in schema development and data collection is often a major barrier for
such designers, limiting their ability to create natural, user-friendly
experiences. We frame the classification of user intent as the generation of a
canonical form, a lightweight semantic representation using natural language.
We show that canonical forms offer a promising alternative to traditional
methods for intent classification. By tuning soft prompts for a frozen large
language model, we show that canonical forms generalize very well to new,
unseen domains in a zero- or few-shot setting. The method is also
sample-efficient, reducing the complexity and effort of developing new
task-oriented dialogue domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using contradictions to improve QA systems. (arXiv:2211.05598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05598">
<div class="article-summary-box-inner">
<span><p>Ensuring the safety of question answering (QA) systems is critical for
deploying them in biomedical and scientific domains. One approach to improving
these systems uses natural language inference (NLI) to determine whether
answers are supported, or entailed, by some background context. However, these
systems are vulnerable to supporting an answer with a source that is wrong or
misleading. Our work proposes a critical approach by selecting answers based on
whether they have been contradicted by some background context. We evaluate
this system on multiple choice and extractive QA and find that while the
contradiction-based systems are competitive with and often better than
entailment-only systems, models that incorporate contradiction, entailment, and
QA model confidence scores together are the best. Based on this result, we
explore unique opportunities for leveraging contradiction-based approaches such
for improving interpretability and selecting better answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents. (arXiv:2211.05599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05599">
<div class="article-summary-box-inner">
<span><p>Topic models represent groups of documents as a list of words (the topic
labels). This work asks whether an alternative approach to topic labeling can
be developed that is closer to a natural language description of a topic than a
word list. To this end, we present an approach to generating human-like topic
labels using abstractive multi-document summarization (MDS). We investigate our
approach with an exploratory case study. We model topics in citation sentences
in order to understand what further research needs to be done to fully
operationalize MDS for topic labeling. Our case study shows that in addition to
more human-like topics there are additional advantages to evaluation by using
clustering and summarization measures instead of topic model measures. However,
we find that there are several developments needed before we can design a
well-powered study to evaluate MDS for topic modeling fully. Namely, improving
cluster cohesion, improving the factuality and faithfulness of MDS, and
increasing the number of documents that might be supported by MDS. We present a
number of ideas on how these can be tackled and conclude with some thoughts on
how topic modeling can also be used to improve MDS in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Inclusive Notion of Text. (arXiv:2211.05604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05604">
<div class="article-summary-box-inner">
<span><p>Natural language processing researchers develop models of grammar, meaning
and human communication based on written text. Due to task and data
differences, what is considered text can vary substantially across studies. A
conceptual framework for systematically capturing these differences is lacking.
We argue that clarity on the notion of text is crucial for reproducible and
generalizable NLP. Towards that goal, we propose common terminology to discuss
the production and transformation of textual data, and introduce a two-tier
taxonomy of linguistic and non-linguistic elements that are available in
textual sources and can be used in NLP modeling. We apply this taxonomy to
survey existing work that extends the notion of text beyond the conservative
language-centered view. We outline key desiderata and challenges of the
emerging inclusive approach to text in NLP, and suggest systematic
community-level reporting as a crucial next step to consolidate the discussion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning. (arXiv:2211.05610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05610">
<div class="article-summary-box-inner">
<span><p>Current pre-trained language models rely on large datasets for achieving
state-of-the-art performance. However, past research has shown that not all
examples in a dataset are equally important during training. In fact, it is
sometimes possible to prune a considerable fraction of the training set while
maintaining the test performance. Established on standard vision benchmarks,
two gradient-based scoring metrics for finding important examples are GraNd and
its estimated version, EL2N. In this work, we employ these two metrics for the
first time in NLP. We demonstrate that these metrics need to be computed after
at least one epoch of fine-tuning and they are not reliable in early steps.
Furthermore, we show that by pruning a small portion of the examples with the
highest GraNd/EL2N scores, we can not only preserve the test accuracy, but also
surpass it. This paper details adjustments and implementation choices which
enable GraNd and EL2N to be applied to NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey. (arXiv:2211.05617v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05617">
<div class="article-summary-box-inner">
<span><p>Despite being responsible for state-of-the-art results in several computer
vision and natural language processing tasks, neural networks have faced harsh
criticism due to some of their current shortcomings. One of them is that neural
networks are correlation machines prone to model biases within the data instead
of focusing on actual useful causal relationships. This problem is particularly
serious in application domains affected by aspects such as race, gender, and
age. To prevent models from incurring on unfair decision-making, the AI
community has concentrated efforts in correcting algorithmic biases, giving
rise to the research area now widely known as fairness in AI. In this survey
paper, we provide an in-depth overview of the main debiasing methods for
fairness-aware neural networks in the context of vision and language research.
We propose a novel taxonomy to better organize the literature on debiasing
methods for fairness, and we discuss the current challenges, trends, and
important future work directions for the interested researcher and
practitioner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. (arXiv:2211.05655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05655">
<div class="article-summary-box-inner">
<span><p>Question answering models commonly have access to two sources of "knowledge"
during inference time: (1) parametric knowledge - the factual knowledge encoded
in the model weights, and (2) contextual knowledge - external knowledge (e.g.,
a Wikipedia passage) given to the model to generate a grounded answer. Having
these two sources of knowledge entangled together is a core issue for
generative QA models as it is unclear whether the answer stems from the given
non-parametric knowledge or not. This unclarity has implications on issues of
trust, interpretability and factuality. In this work, we propose a new paradigm
in which QA models are trained to disentangle the two sources of knowledge.
Using counterfactual data augmentation, we introduce a model that predicts two
answers for a given question: one based on given contextual knowledge and one
based on parametric knowledge. Our experiments on the Natural Questions dataset
show that this approach improves the performance of QA models by making them
more robust to knowledge conflicts between the two knowledge sources, while
generating useful disentangled answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT in Plutarch's Shadows. (arXiv:2211.05673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05673">
<div class="article-summary-box-inner">
<span><p>The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea
(ca. 45-120 CE) also contains several texts which, according to current
scholarly opinion, did not originate with him and are therefore attributed to
an anonymous author Pseudo-Plutarch. These include, in particular, the work
Placita Philosophorum (Quotations and Opinions of the Ancient Philosophers),
which is extremely important for the history of ancient philosophy. Little is
known about the identity of that anonymous author and its relation to other
authors from the same period. This paper presents a BERT language model for
Ancient Greek. The model discovers previously unknown statistical properties
relevant to these literary, philosophical, and historical problems and can shed
new light on this authorship question. In particular, the Placita
Philosophorum, together with one of the other Pseudo-Plutarch texts, shows
similarities with the texts written by authors from an Alexandrian context
(2nd/3rd century CE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05705">
<div class="article-summary-box-inner">
<span><p>The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. In this work, we introduce a novel task
of conversational aspect-based sentiment quadruple analysis, namely DiaASQ,
aiming to detect the sentiment quadruple of target-aspect-opinion-sentiment in
a dialogue. DiaASQ bridges the gap between fine-grained sentiment analysis and
conversational opinion mining. We manually construct a large-scale,
high-quality Chinese dataset and also obtain the English version dataset via
manual translation. We deliberately propose a neural model to benchmark the
task. It advances in effectively performing end-to-end quadruple prediction and
manages to incorporate rich dialogue-specific and discourse feature
representations for better cross-utterance quadruple extraction. We finally
point out several potential future works to facilitate the follow-up research
of this new task. The DiaASQ data is open at https://github.com/unikcc/DiaASQ
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. (arXiv:2211.05719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05719">
<div class="article-summary-box-inner">
<span><p>Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 8x. Second, it contains massive topics to generalize the
open-domain. To build engaging dialogue system with this dataset, we propose
and normalize two response producing tasks based on retrieval and generative
scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05750">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have demonstrated extraordinary capabilities in
language generation. However, real-world tasks often require controlling the
distribution of generated text in order to mitigate bias, promote fairness, and
achieve personalization. Existing techniques for controlling the distribution
of generated text only work with quantified distributions, which require
pre-defined categories, proportions of the distribution, or an existing corpus
following the desired distributions. However, many important distributions,
such as personal preferences, are unquantified. In this work, we tackle the
problem of generating text following arbitrary distributions (quantified and
unquantified) by proposing Nano, a few-shot human-in-the-loop training
algorithm that continuously learns from human feedback. Nano achieves
state-of-the-art results on single topic/attribute as well as quantified
distribution control compared to previous works. We also show that Nano is able
to learn unquantified distributions, achieves personalization, and captures
differences between different individuals' personal preferences with high
sample efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities. (arXiv:2211.05756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05756">
<div class="article-summary-box-inner">
<span><p>End-to-end multilingual ASR has become more appealing because of several
reasons such as simplifying the training and deployment process and positive
performance transfer from high-resource to low-resource languages. However,
scaling up the number of languages, total hours, and number of unique tokens is
not a trivial task. This paper explores large-scale multilingual ASR models on
70 languages. We inspect two architectures: (1) Shared embedding and output and
(2) Multiple embedding and output model. In the shared model experiments, we
show the importance of tokenization strategy across different languages. Later,
we use our optimal tokenization strategy to train multiple embedding and output
model to further improve our result. Our multilingual ASR achieves 13.9%-15.6%
average WER relative improvement compared to monolingual models. We show that
our multilingual ASR generalizes well on an unseen dataset and domain,
achieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot
and finetuning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11909">
<div class="article-summary-box-inner">
<span><p>Employing paraphrasing tools to conceal plagiarized text is a severe threat
to academic integrity. To enable the detection of machine-paraphrased text, we
evaluate the effectiveness of five pre-trained word embedding models combined
with machine learning classifiers and state-of-the-art neural language models.
We analyze preprints of research papers, graduation theses, and Wikipedia
articles, which we paraphrased using different configurations of the tools
SpinBot and SpinnerChief. The best performing technique, Longformer, achieved
an average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for
SpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and
F1=65.6% for SpinnerChief cases. We show that the automated classification
alleviates shortcomings of widely-used text-matching systems, such as Turnitin
and PlagScan. To facilitate future research, all data, code, and two web
applications showcasing our contributions are openly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12450">
<div class="article-summary-box-inner">
<span><p>The rise of language models such as BERT allows for high-quality text
paraphrasing. This is a problem to academic integrity, as it is difficult to
differentiate between original and machine-generated content. We propose a
benchmark consisting of paraphrased articles using recent language models
relying on the Transformer architecture. Our contribution fosters future
research of paraphrase detection systems as it offers a large collection of
aligned original and paraphrased documents, a study regarding its structure,
classification experiments with state-of-the-art systems, and we make our
findings publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and straightforward attempts at applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained language models; (2) non-standard hyperparameters that suit DP
optimization; and (3) fine-tuning objectives which are aligned with the
pretraining procedure. With the above, we obtain NLP models that outperform
state-of-the-art DP-trained models under the same privacy budget and strong
non-private baselines -- by directly fine-tuning pretrained models with DP
optimization on moderately-sized corpora. To address the computational
challenge of running DP-SGD with large Transformers, we propose a memory saving
technique that allows clipping in DP-SGD to run without instantiating
per-example gradients for any linear layer in the model. The technique enables
privately training Transformers with almost the same memory cost as non-private
training at a modest run-time overhead. Contrary to conventional wisdom that DP
optimization fails at learning high-dimensional models (due to noise that
scales with dimension) empirical results reveal that private learning with
pretrained language models doesn't tend to suffer from dimension-dependent
performance degradation. Code to reproduce results can be found at
https://github.com/lxuechen/private-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07819">
<div class="article-summary-box-inner">
<span><p>A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods'
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Learning with Multilingual Language Models. (arXiv:2112.10668v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10668">
<div class="article-summary-box-inner">
<span><p>Large-scale generative language models such as GPT-3 are competitive few-shot
learners. While these models are known to be able to jointly represent many
different languages, their training data is dominated by English, potentially
limiting their cross-lingual generalization. In this work, we train
multilingual generative language models on a corpus covering a diverse set of
languages, and study their few- and zero-shot learning capabilities in a wide
range of tasks. Our largest model with 7.5 billion parameters sets new state of
the art in few-shot learning in more than 20 representative languages,
outperforming GPT-3 of comparable size in multilingual commonsense reasoning
(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in
4-shot settings) and natural language inference (+5.4% in each of 0-shot and
4-shot settings). On the FLORES-101 machine translation benchmark, our model
outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while
surpassing the official supervised baseline in 45 directions. We conduct an
in-depth analysis of different multilingual prompting approaches, showing in
particular that strong few-shot learning performance across languages can be
achieved via cross-lingual transfer through both templates and demonstration
examples. Finally, we evaluate our models in social value tasks such as hate
speech detection in five languages and find it has limitations similar to
comparable sized GPT-3 models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens. (arXiv:2202.01338v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01338">
<div class="article-summary-box-inner">
<span><p>Despite significant progress of generative models in the natural sciences,
their controllability remains chal-lenging. One fundamentally missing aspect of
molecular or protein generative models is an inductive bias that can reflect
continuous properties of interest. To that end, we propose the Regression
Transformer (RT), a novel method that abstracts regression as a conditional
sequence modeling problem. This introduces a new paradigm of multitask language
models which seamlessly bridge sequence regression and conditional sequence
generation.
</p>
<p>We thoroughly demonstrate that, despite using a nominal-scale training
objective, the RT matches or surpasses the performance of conventional
regression models in property prediction tasks of small molecules, proteins and
chemical reactions. Critically, priming the same model with continuous
properties yields a highly competitive conditional generative model that
outperforms specialized approaches in a substructure-constrained,
property-driven molecule generation benchmark. Our dichotomous approach is
facilitated by a novel, alternating training scheme that enables the model to
decorate seed sequences by desired properties, e.g., to optimize reaction
yield.
</p>
<p>In sum, the RT is the first report of a multitask model that concurrently
excels at predictive and generative tasks in biochemistry. This finds
particular application in property-driven, local exploration of the chemical or
protein space and could pave the road toward foundation models in material
design.
</p>
<p>The code to reproduce all experiments of the paper is available at:
https://github.com/IBM/ regression-transformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08009">
<div class="article-summary-box-inner">
<span><p>The General QA field has been developing the methodology referencing the
Stanford Question answering dataset (SQuAD) as the significant benchmark.
However, compiling factual questions is accompanied by time- and
labour-consuming annotation, limiting the training data's potential size. We
present the WikiOmnia dataset, a new publicly available set of QA-pairs and
corresponding Russian Wikipedia article summary sections, composed with a fully
automated generative pipeline. The dataset includes every available article
from Wikipedia for the Russian language. The WikiOmnia pipeline is available
open-source and is also tested for creating SQuAD-formatted QA on other
domains, like news texts, fiction, and social media. The resulting dataset
includes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs
with paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for
ruT5-large) and cleaned data with strict automatic verification (over 160,000
QA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with
paragraphs for ruT5-large).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction. (arXiv:2204.12674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12674">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment
analysis task that aims to extract triplets of aspect terms, sentiments, and
opinion terms from review sentences. Recently, span-level models achieve
gratifying results on ASTE task by taking advantage of the predictions of all
possible spans. Since all possible spans significantly increases the number of
potential aspect and opinion candidates, it is crucial and challenging to
efficiently extract the triplet elements among them. In this paper, we present
a span-level bidirectional network which utilizes all possible spans as input
and extracts triplets from spans bidirectionally. Specifically, we devise both
the aspect decoder and opinion decoder to decode the span representations and
extract triples from aspect-to-opinion and opinion-to-aspect directions. With
these two decoders complementing with each other, the whole network can extract
triplets from spans more comprehensively. Moreover, considering that mutual
exclusion cannot be guaranteed between the spans, we design a similar span
separation loss to facilitate the downstream task of distinguishing the correct
span by expanding the KL divergence of similar spans during the training
process; in the inference process, we adopt an inference strategy to remove
conflicting triplets from the results base on their confidence scores.
Experimental results show that our framework not only significantly outperforms
state-of-the-art methods, but achieves better performance in predicting
triplets with multi-token entities and extracting triplets in sentences contain
multi-triplets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v4 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13384">
<div class="article-summary-box-inner">
<span><p>DBLP is the largest open-access repository of scientific articles on computer
science and provides metadata associated with publications, authors, and
venues. We retrieved more than 6 million publications from DBLP and extracted
pertinent metadata (e.g., abstracts, author affiliations, citations) from the
publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to
identify trends in research activity, productivity, focus, bias, accessibility,
and impact of computer science research. We present an initial analysis focused
on the volume of computer science research (e.g., number of papers, authors,
research activity), trends in topics of interest, and citation patterns. Our
findings show that computer science is a growing research field (approx. 15%
annually), with an active and collaborative researcher community. While papers
in recent years present more bibliographical entries in comparison to previous
decades, the average number of citations has been declining. Investigating
papers' abstracts reveals that recent topic trends are clearly reflected in D3.
Finally, we list further applications of D3 and pose supplemental research
questions. The D3 dataset, our findings, and source code are publicly available
for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01663">
<div class="article-summary-box-inner">
<span><p>In the future, powerful AI systems may be deployed in high-stakes settings,
where a single failure could be catastrophic. One technique for improving AI
safety in high-stakes settings is adversarial training, which uses an adversary
to generate examples to train on in order to achieve better worst-case
performance.
</p>
<p>In this work, we used a safe language generation task (``avoid injuries'') as
a testbed for achieving high reliability through adversarial training. We
created a series of adversarial training techniques -- including a tool that
assists human adversaries -- to find and eliminate failures in a classifier
that filters text completions suggested by a generator. In our task, we
determined that we can set very conservative classifier thresholds without
significantly impacting the quality of the filtered outputs. We found that
adversarial training increased robustness to the adversarial attacks that we
trained on -- doubling the time for our contractors to find adversarial
examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44
minutes) -- without affecting in-distribution performance.
</p>
<p>We hope to see further work in the high-stakes reliability setting, including
more powerful tools for enhancing human adversaries and better ways to measure
high levels of reliability, until we can confidently rule out the possibility
of catastrophic deployment-time failures of powerful models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation. (arXiv:2205.12216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12216">
<div class="article-summary-box-inner">
<span><p>We present a new approach to perform zero-shot cross-modal transfer between
speech and text for translation tasks. Multilingual speech and text are encoded
in a joint fixed-size representation space. Then, we compare different
approaches to decode these multimodal and multilingual fixed-size
representations, enabling zero-shot translation between languages and
modalities. All our models are trained without the need of cross-modal labeled
translation data. Despite a fixed-size representation, we achieve very
competitive results on several text and speech translation tasks. In
particular, we significantly improve the state-of-the-art for zero-shot speech
translation on Must-C. Incorporating a speech decoder in our framework, we
introduce the first results for zero-shot direct speech-to-speech and
text-to-speech translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation Robustness to Natural Asemantic Variation. (arXiv:2205.12514v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12514">
<div class="article-summary-box-inner">
<span><p>Current Machine Translation (MT) models still struggle with more challenging
input, such as noisy data and tail-end words and phrases. Several works have
addressed this robustness issue by identifying specific categories of noise and
variation then tuning models to perform better on them. An important yet
under-studied category involves minor variations in nuance (non-typos) that
preserve meaning w.r.t. the target language. We introduce and formalize this
category as Natural Asemantic Variation (NAV) and investigate it in the context
of MT robustness. We find that existing MT models fail when presented with NAV
data, but we demonstrate strategies to improve performance on NAV by
fine-tuning them with human-generated variations. We also show that NAV
robustness can be transferred across languages and find that synthetic
perturbations can achieve some but not all of the benefits of organic NAV data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Large Language Models are Transforming Machine-Paraphrased Plagiarism. (arXiv:2210.03568v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03568">
<div class="article-summary-box-inner">
<span><p>The recent success of large language models for text generation poses a
severe threat to academic integrity, as plagiarists can generate realistic
paraphrases indistinguishable from original work. However, the role of large
autoregressive transformers in generating machine-paraphrased plagiarism and
their detection is still developing in the literature. This work explores T5
and GPT-3 for machine-paraphrase generation on scientific articles from arXiv,
student theses, and Wikipedia. We evaluate the detection performance of six
automated solutions and one commercial plagiarism detection software and
perform a human study with 105 participants regarding their detection
performance and the quality of generated examples. Our results suggest that
large models can rewrite text humans have difficulty identifying as
machine-paraphrased (53% mean acc.). Human experts rate the quality of
paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5,
fluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)
achieves a 66% F1-score in detecting paraphrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04284">
<div class="article-summary-box-inner">
<span><p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only
fine-tunes a few extra modules, becomes an appealing efficient alternative to
the full model fine-tuning. Although computationally efficient, the recent
Adapters often increase parameters (e.g. bottleneck dimension) for matching the
performance of full model fine-tuning, which we argue goes against their
original intention. In this work, we re-examine the parameter-efficiency of
Adapters through the lens of network pruning (we name such plug-in concept as
\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or
better performance than standard Adapters when the sparse ratio reaches up to
80\%. Based on our findings, we introduce an easy but effective setting
``\textit{Large-Sparse}'' to improve the model capacity of Adapters under the
same parameter budget. Experiments on five competitive Adapters upon three
advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.
40\%) SparseAdapter can consistently outperform their corresponding
counterpart. Encouragingly, with the \textit{Large-Sparse} setting, we can
obtain further appealing gains, even outperforming the full fine-tuning by a
large margin. Our code will be released at:
https://github.com/Shwai-He/SparseAdapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracing Semantic Variation in Slang. (arXiv:2210.08635v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08635">
<div class="article-summary-box-inner">
<span><p>The meaning of a slang term can vary in different communities. However, slang
semantic variation is not well understood and under-explored in the natural
language processing of slang. One existing view argues that slang semantic
variation is driven by culture-dependent communicative needs. An alternative
view focuses on slang's social functions suggesting that the desire to foster
semantic distinction may have led to the historical emergence of
community-specific slang senses. We explore these theories using computational
models and test them against historical slang dictionary entries, with a focus
on characterizing regularity in the geographical variation of slang usages
attested in the US and the UK over the past two centuries. We show that our
models are able to predict the regional identity of emerging slang word
meanings from historical slang records. We offer empirical evidence that both
communicative need and semantic distinction play a role in the variation of
slang meaning yet their relative importance fluctuates over the course of
history. Our work offers an opportunity for incorporating historical cultural
elements into the natural language processing of slang.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for a higher power in the human evaluation of MT. (arXiv:2210.11612v2 [stat.AP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11612">
<div class="article-summary-box-inner">
<span><p>In MT evaluation, pairwise comparisons are conducted to identify the better
system. In conducting the comparison, the experimenter must allocate a budget
to collect Direct Assessment (DA) judgments. We provide a cost effective way to
spend the budget, but show that typical budget sizes often do not allow for
solid comparison. Taking the perspective that the basis of solid comparison is
in achieving statistical significance, we study the power (rate of achieving
significance) on a large collection of pairwise DA comparisons. Due to the
nature of statistical estimation, power is low for differentiating less than
1-2 DA points, and to achieve a notable increase in power requires at least
2-3x more samples. Applying variance reduction alone will not yield these
gains, so we must face the reality of undetectable differences and spending
increases. In this context, we propose interim testing, an "early stopping"
collection procedure that yields more power per judgment collected, which
adaptively focuses the budget on pairs that are borderline significant. Interim
testing can achieve up to a 27% efficiency gain when spending 3x the current
budget, or 18% savings at the current evaluation power.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13952">
<div class="article-summary-box-inner">
<span><p>We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks. (arXiv:2210.13979v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13979">
<div class="article-summary-box-inner">
<span><p>Large pretrained Transformer-based language models like BERT and GPT have
changed the landscape of Natural Language Processing (NLP). However, fine
tuning such models still requires a large number of training examples for each
target task, thus annotating multiple datasets and training these models on
various downstream tasks becomes time consuming and expensive. In this work, we
propose a simple extension of the Prototypical Networks for few-shot text
classification. Our main idea is to replace the class prototypes by Gaussians
and introduce a regularization term that encourages the examples to be
clustered near the appropriate class centroids. Experimental results show that
our method outperforms various strong baselines on 13 public and 4 internal
datasets. Furthermore, we use the class distributions as a tool for detecting
potential out-of-distribution (OOD) data points during deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where to start? Analyzing the potential value of intermediate models. (arXiv:2211.00107v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00107">
<div class="article-summary-box-inner">
<span><p>Previous studies observed that finetuned models may be better base models
than the vanilla pretrained model. Such a model, finetuned on some source
dataset, may provide a better starting point for a new finetuning process on a
desired target dataset. Here, we perform a systematic analysis of this
intertraining scheme, over a wide range of English classification tasks.
Surprisingly, our analysis suggests that the potential intertraining gain can
be analyzed independently for the target dataset under consideration, and for a
base model being considered as a starting point. This is in contrast to current
perception that the alignment between the target dataset and the source dataset
used to generate the base model is a major factor in determining intertraining
success. We analyze different aspects that contribute to each. Furthermore, we
leverage our analysis to propose a practical and efficient approach to
determine if and how to select a base model in real-world settings. Last, we
release an updating ranking of best models in the HuggingFace hub per
architecture https://ibm.github.io/model-recycling/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer. (arXiv:2211.00974v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00974">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformers currently dominate most NLP tasks. They impose,
however, limits on the maximum input length (512 sub-words in BERT), which are
too restrictive in the legal domain. Even sparse-attention models, such as
Longformer and BigBird, which increase the maximum input length to 4,096
sub-words, severely truncate texts in three of the six datasets of LexGLUE.
Simpler linear classifiers with TF-IDF features can handle texts of any length,
require far less resources to train and deploy, but are usually outperformed by
pre-trained Transformers. We explore two directions to cope with long legal
texts: (i) modifying a Longformer warm-started from LegalBERT to handle even
longer texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use
TF-IDF representations. The first approach is the best in terms of performance,
surpassing a hierarchical version of LegalBERT, which was the previous state of
the art in LexGLUE. The second approach leads to computationally more efficient
models at the expense of lower performance, but the resulting models still
outperform overall a linear SVM with TF-IDF features in long legal document
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01482">
<div class="article-summary-box-inner">
<span><p>Existing metrics for evaluating the quality of automatically generated
questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
predicted questions, providing a high score when there is a considerable
lexical overlap or semantic similarity between the candidate and the reference
questions. This approach has two major shortcomings. First, we need expensive
human-provided reference questions. Second, it penalises valid questions that
may not have high lexical or semantic similarity to the reference questions. In
this paper, we propose a new metric, RQUGE, based on the answerability of the
candidate question given the context. The metric consists of a
question-answering and a span scorer module, in which we use pre-trained models
from the existing literature, and therefore, our metric can be used without
further training. We show that RQUGE has a higher correlation with human
judgment without relying on the reference question. RQUGE is shown to be
significantly more robust to several adversarial corruptions. Additionally, we
illustrate that we can significantly improve the performance of QA models on
out-of-domain datasets by fine-tuning on the synthetic data generated by a
question generation model and re-ranked by RQUGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions. (arXiv:2211.04971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04971">
<div class="article-summary-box-inner">
<span><p>Image captioning models tend to describe images in an object-centric way,
emphasising visible objects. But image descriptions can also abstract away from
objects and describe the type of scene depicted. In this paper, we explore the
potential of a state-of-the-art Vision and Language model, VinVL, to caption
images at the scene level using (1) a novel dataset which pairs images with
both object-centric and scene descriptions. Through (2) an in-depth analysis of
the effect of the fine-tuning, we show (3) that a small amount of curated data
suffices to generate scene descriptions without losing the capability to
identify object-level concepts in the scene; the model acquires a more holistic
view of the image compared to when object-centric descriptions are generated.
We discuss the parallels between these results and insights from computational
and cognitive science research on scene perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Wrong with Language Models that Can Not Tell a Story?. (arXiv:2211.05044v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05044">
<div class="article-summary-box-inner">
<span><p>This paper argues that a deeper understanding of narrative and the successful
generation of longer subjectively interesting texts is a vital bottleneck that
hinders the progress in modern Natural Language Processing (NLP) and may even
be in the whole field of Artificial Intelligence. We demonstrate that there are
no adequate datasets, evaluation methods, and even operational concepts that
could be used to start working on narrative processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing ASR Outputs in Joint Training for Speech Emotion Recognition. (arXiv:2110.15684v2 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15684">
<div class="article-summary-box-inner">
<span><p>Alongside acoustic information, linguistic features based on speech
transcripts have been proven useful in Speech Emotion Recognition (SER).
However, due to the scarcity of emotion labelled data and the difficulty of
recognizing emotional speech, it is hard to obtain reliable linguistic features
and models in this research area. In this paper, we propose to fuse Automatic
Speech Recognition (ASR) outputs into the pipeline for joint training SER. The
relationship between ASR and SER is understudied, and it is unclear what and
how ASR features benefit SER. By examining various ASR outputs and fusion
methods, our experiments show that in joint ASR-SER training, incorporating
both ASR hidden and text output using a hierarchical co-attention fusion
approach improves the SER performance the most. On the IEMOCAP corpus, our
approach achieves 63.4% weighted accuracy, which is close to the baseline
results achieved by combining ground-truth transcripts. In addition, we also
present novel word error rate analysis on IEMOCAP and layer-difference analysis
of the Wav2vec 2.0 model to better understand the relationship between ASR and
SER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps. (arXiv:2211.03988v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03988">
<div class="article-summary-box-inner">
<span><p>IR models using a pretrained language model significantly outperform lexical
approaches like BM25. In particular, SPLADE, which encodes texts to sparse
vectors, is an effective model for practical use because it shows robustness to
out-of-domain datasets. However, SPLADE still struggles with exact matching of
low-frequency words in training data. In addition, domain shifts in vocabulary
and word frequencies deteriorate the IR performance of SPLADE. Because
supervision data are scarce in the target domain, addressing the domain shifts
without supervision data is necessary. This paper proposes an unsupervised
domain adaptation method by filling vocabulary and word-frequency gaps. First,
we expand a vocabulary and execute continual pretraining with a masked language
model on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse
vectors by inverse document frequency weights to consider the importance of
documents with lowfrequency words. We conducted experiments using our method on
datasets with a large vocabulary gap from a source domain. We show that our
method outperforms the present stateof-the-art domain adaptation method. In
addition, our method achieves state-of-the-art results, combined with BM25.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-13 23:16:26.235219494 UTC">2022-11-13 23:16:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>