<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-04T01:30:00Z">05-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01710">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose document-level end-to-end sentiment analysis to
efficiently understand aspect and review sentiment expressed in online reviews
in a unified manner. In particular, we assume that star rating labels are a
"coarse-grained synthesis" of aspect ratings across in the review. We propose a
Distantly Supervised Pyramid Network (DSPN) to efficiently perform
Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating
Prediction using only document star rating labels for training. By performing
these three related sentiment subtasks in an end-to-end manner, DSPN can
extract aspects mentioned in the review, identify the corresponding sentiments,
and predict the star rating labels. We evaluate DSPN on multi-aspect review
datasets in English and Chinese and find that with only star rating labels for
supervision, DSPN can perform comparably well to a variety of benchmark models.
We also demonstrate the interpretability of DSPN's outputs on reviews to show
the pyramid structure inherent in document level end-to-end sentiment analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01711">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) trained on vast quantities of unlabelled data have
greatly advanced the field of natural language processing (NLP). In this study,
we re-visit the widely accepted notion in NLP that continued pre-training LMs
on task-related texts improves the performance of fine-tuning (FT) in
downstream tasks. Through experiments on eight single-sentence tasks and eight
sentence-pair tasks in both semi-supervised and fully-supervised settings, we
find that conventional continued pre-training does not consistently provide
benefits and can even be detrimental for sentence-pair tasks or when
prompt-based FT is used. To tackle these issues, we propose Prompt-based
Continued Pre-training (PCP), which combines the idea of instruction tuning
with conventional continued pre-training. Our approach aims to improve the
performance of prompt-based FT by presenting both task-related texts and prompt
templates to LMs through unsupervised pre-training objectives before
fine-tuning for the target task. Our empirical evaluations on 21 benchmarks
demonstrate that the PCP consistently improves the performance of
state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both
semi-supervised and fully-supervised settings, even with only hundreds of
unlabelled examples. Additionally, prompt-based FT with the PCP outperforms
state-of-the-art semi-supervised approaches with greater simplicity,
eliminating the need for an iterative process and extra data augmentation. Our
further analysis explores the performance lower bound of the PCP and reveals
that the advantages of PCP persist across different sizes of models and
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01713">
<div class="article-summary-box-inner">
<span><p>Disentangling sentence representations over continuous spaces can be a
critical process in improving interpretability and semantic control by
localising explicit generative factors. Such process confers to neural-based
language models some of the advantages that are characteristic of symbolic
models, while keeping their flexibility. This work presents a methodology for
disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it
into a more separable semantic space with the support of a flow-based
invertible neural network (INN). Experimental results indicate that the INN can
transform the distributed hidden space into a better semantically disentangled
latent space, resulting in better interpretability and controllability, when
compared to recent state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stance Detection With Supervised, Zero-Shot, and Few-Shot Applications. (arXiv:2305.01723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01723">
<div class="article-summary-box-inner">
<span><p>Stance detection is the identification of an author's beliefs about a subject
from a document. Researchers widely rely on sentiment analysis to accomplish
this. However, recent research has show that sentiment analysis is only loosely
correlated with stance, if at all. This paper advances methods in text analysis
by precisely defining the task of stance detection, providing a generalized
framework for the task, and then presenting three distinct approaches for
performing stance detection: supervised classification, zero-shot
classification with NLI classifiers, and in-context learning. In doing so, I
demonstrate how zero-shot and few-shot language classifiers can replace human
labelers for a variety of tasks and discuss how their application and
limitations differ from supervised classifiers. Finally, I demonstrate an
application of zero-shot stance detection by replicating Block Jr et al.
(2022).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01735">
<div class="article-summary-box-inner">
<span><p>Extractive summarization aims to form a summary by directly extracting
sentences from the source document. Existing works mostly formulate it as a
sequence labeling problem by making individual sentence label predictions. This
paper proposes DiffuSum, a novel paradigm for extractive summarization, by
directly generating the desired summary sentence representations with diffusion
models and extracting sentences based on sentence representation matching. In
addition, DiffuSum jointly optimizes a contrastive sentence encoder with a
matching loss for sentence representation alignment and a multi-class
contrastive loss for representation diversity. Experimental results show that
DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail
with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets
with different summary lengths also demonstrate the effectiveness of DiffuSum.
The strong performance of our framework shows the great potential of adapting
generative models for extractive summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01750">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge bases is considered a difficult problem due
to the challenge of generalizing to a wide variety of possible natural language
questions. Additionally, the heterogeneity of knowledge base schema items
between different knowledge bases often necessitates specialized training for
different knowledge base question-answering (KBQA) datasets. To handle
questions over diverse KBQA datasets with a unified training-free framework, we
propose KB-BINDER, which for the first time enables few-shot in-context
learning over KBQA tasks. Firstly, KB-BINDER leverages large language models
like Codex to generate logical forms as the draft for a specific question by
imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge
base to bind the generated draft to an executable one with BM25 score matching.
The experimental results on four public heterogeneous KBQA datasets show that
KB-BINDER can achieve a strong performance with only a few in-context
demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even
outperform the state-of-the-art trained models. On GrailQA and WebQSP, our
model is also on par with other fully-trained models. We believe KB-BINDER can
serve as an important baseline for future research. We plan to release all the
code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Speaker Anonymization on Emotional Speech. (arXiv:2305.01759v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01759">
<div class="article-summary-box-inner">
<span><p>Speech data carries a range of personal information, such as the speaker's
identity and emotional state. These attributes can be used for malicious
purposes. With the development of virtual assistants, a new generation of
privacy threats has emerged. Current studies have addressed the topic of
preserving speech privacy. One of them, the VoicePrivacy initiative aims to
promote the development of privacy preservation tools for speech technology.
The task selected for the VoicePrivacy 2020 Challenge (VPC) is about speaker
anonymization. The goal is to hide the source speaker's identity while
preserving the linguistic information. The baseline of the VPC makes use of a
voice conversion. This paper studies the impact of the speaker anonymization
baseline system of the VPC on emotional information present in speech
utterances. Evaluation is performed following the VPC rules regarding the
attackers' knowledge about the anonymization system. Our results show that the
VPC baseline system does not suppress speakers' emotions against informed
attackers. When comparing anonymized speech to original speech, the emotion
recognition performance is degraded by 15\% relative to IEMOCAP data, similar
to the degradation observed for automatic speech recognition used to evaluate
the preservation of the linguistic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01764">
<div class="article-summary-box-inner">
<span><p>NLP datasets are richer than just input-output pairs; rather, they carry
causal relations between the input and output variables. In this work, we take
sentiment classification as an example and look into the causal relations
between the review (X) and sentiment (Y). As psychology studies show that
language can affect emotion, different psychological processes are evoked when
a person first makes a rating and then self-rationalizes their feeling in a
review (where the sentiment causes the review, i.e., Y -&gt; X), versus first
describes their experience, and weighs the pros and cons to give a final rating
(where the review causes the sentiment, i.e., X -&gt; Y ). Furthermore, it is also
a completely different psychological process if an annotator infers the
original rating of the user by theory of mind (ToM) (where the review causes
the rating, i.e., X -ToM-&gt; Y ). In this paper, we verbalize these three causal
mechanisms of human psychological processes of sentiment classification into
three different causal prompts, and study (1) how differently they perform, and
(2) what nature of sentiment classification data leads to agreement or
diversity in the model responses elicited by the prompts. We suggest future
work raise awareness of different causal structures in NLP tasks. Our code and
data are at https://github.com/cogito233/psych-causal-prompt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLTUNET: A Simple Unified Model for Sign Language Translation. (arXiv:2305.01778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01778">
<div class="article-summary-box-inner">
<span><p>Despite recent successes with neural models for sign language translation
(SLT), translation quality still lags behind spoken languages because of the
data scarcity and modality gap between sign video and text. To address both
problems, we investigate strategies for cross-modality representation sharing
for SLT. We propose SLTUNET, a simple unified neural model designed to support
multiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and
sign-to-text translation. Jointly modeling different tasks endows SLTUNET with
the capability to explore the cross-task relatedness that could help narrow the
modality gap. In addition, this allows us to leverage the knowledge from
external resources, such as abundant parallel data used for spoken-language
machine translation (MT). We show in experiments that SLTUNET achieves
competitive and even state-of-the-art performance on PHOENIX-2014T and
CSL-Daily when augmented with MT data and equipped with a set of optimization
techniques. We further use the DGS Corpus for end-to-end SLT for the first
time. It covers broader domains with a significantly larger vocabulary, which
is more challenging and which we consider to allow for a more realistic
assessment of the current state of SLT than the former two. Still, SLTUNET
obtains improved results on the DGS Corpus. Code is available at
https://github.com/bzhangGo/sltunet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01788">
<div class="article-summary-box-inner">
<span><p>Visual Word Sense Disambiguation (VWSD) is a task to find the image that most
accurately depicts the correct sense of the target word for the given context.
Previously, image-text matching models often suffered from recognizing
polysemous words. This paper introduces an unsupervised VWSD approach that uses
gloss information of an external lexical knowledge-base, especially the sense
definitions. Specifically, we suggest employing Bayesian inference to
incorporate the sense definitions when sense information of the answer is not
provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we
propose a context-aware definition generation with GPT-3. Experimental results
show that the VWSD performance significantly increased with our Bayesian
inference-based approach. In addition, our context-aware definition generation
achieved prominent performance improvement in OOD examples exhibiting better
performance than the existing definition generation method. We will publish
source codes as soon as possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01795">
<div class="article-summary-box-inner">
<span><p>Embodied agents have achieved prominent performance in following human
instructions to complete tasks. However, the potential of providing
instructions informed by texts and images to assist humans in completing tasks
remains underexplored. To uncover this capability, we present the multimodal
procedural planning (MPP) task, in which models are given a high-level goal and
generate plans of paired text-image steps, providing more complementary and
informative guidance than unimodal plans. The key challenges of MPP are to
ensure the informativeness, temporal coherence,and accuracy of plans across
modalities. To tackle this, we propose Text-Image Prompting (TIP), a
dual-modality prompting method that jointly leverages zero-shot reasoning
ability in large language models (LLMs) and compelling text-to-image generation
ability from diffusion-based models. TIP improves the interaction in the dual
modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs
to guide the textual-grounded image plan generation and leveraging the
descriptions of image plans to ground the textual plan reversely. To address
the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed
for MPP. Our results show compelling human preferences and automatic scores
against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms
of informativeness, temporal coherence, and plan accuracy. Our code and data:
https://github.com/YujieLu10/MPP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01810">
<div class="article-summary-box-inner">
<span><p>In recent years, Pre-trained Language Models (PLMs) have shown their
superiority by pre-training on unstructured text corpus and then fine-tuning on
downstream tasks. On entity-rich textual resources like Wikipedia,
Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens
and mentioned entities in pre-training, and are thus more effective on
entity-centric tasks such as entity linking and relation classification.
Although exploiting Wikipedia's rich structures to some extent, conventional
KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is
around a topic entity (identified by the page URL and shown in the page title).
In this paper, we demonstrate that KEPLMs without incorporating the topic
entities will lead to insufficient entity interaction and biased (relation)
word semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained
LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET
identifies where to add the topic entity's information in a Wikipedia sentence,
fuses such information into token and mentioned entities representations, and
supervises the network learning, through which it takes topic entities back
into consideration. Experiments demonstrated the generality and superiority of
KEPLET which was applied to two representative KEPLMs, achieving significant
improvements on four entity-centric tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA. (arXiv:2305.01812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01812">
<div class="article-summary-box-inner">
<span><p>Despite remarkable progress made in natural language processing, even the
state-of-the-art models often make incorrect predictions. Such predictions
hamper the reliability of systems and limit their widespread adoption in
real-world applications. 'Selective prediction' partly addresses the above
concern by enabling models to abstain from answering when their predictions are
likely to be incorrect. While selective prediction is advantageous, it leaves
us with a pertinent question 'what to do after abstention'. To this end, we
present an explorative study on 'Post-Abstention', a task that allows
re-attempting the abstained instances with the aim of increasing 'coverage' of
the system without significantly sacrificing its 'accuracy'. We first provide
mathematical formulation of this task and then explore several methods to solve
it. Comprehensive experiments on 11 QA datasets show that these methods lead to
considerable risk improvements -- performance metric of the Post-Abstention
task -- both in the in-domain and the out-of-domain settings. We also conduct a
thorough analysis of these results which further leads to several interesting
findings. Finally, we believe that our work will encourage and facilitate
further research in this important area of addressing the reliability of NLP
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01863">
<div class="article-summary-box-inner">
<span><p>Learning new programming skills requires tailored guidance. With the
emergence of advanced Natural Language Generation models like the ChatGPT API,
there is now a possibility of creating a convenient and personalized tutoring
system with AI for computer science education. This paper presents GPTutor, a
ChatGPT-powered programming tool, which is a Visual Studio Code extension using
the ChatGPT API to provide programming code explanations. By integrating Visual
Studio Code API, GPTutor can comprehensively analyze the provided code by
referencing the relevant source codes. As a result, GPTutor can use designed
prompts to explain the selected code with a pop-up message. GPTutor is now
published at the Visual Studio Code Extension Marketplace, and its source code
is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor
delivers the most concise and accurate explanations compared to vanilla ChatGPT
and GitHub Copilot. Moreover, the feedback from students and teachers indicated
that GPTutor is user-friendly and can explain given codes satisfactorily.
Finally, we discuss possible future research directions for GPTutor. This
includes enhancing its performance and personalization via further prompt
programming, as well as evaluating the effectiveness of GPTutor with real
users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01876">
<div class="article-summary-box-inner">
<span><p>Concepts benefit natural language understanding but are far from complete in
existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs)
have been widely used in text-based concept extraction (CE). However, PLMs tend
to mine the co-occurrence associations from massive corpus as pre-trained
knowledge rather than the real causal effect between tokens.As a result, the
pre-trained knowledge confounds PLMs to extract biased concepts based on
spurious co-occurrence correlations, inevitably resulting in low precision. In
this paper, through the lens of a Structural Causal Model (SCM), we propose
equipping the PLM-based extractor with a knowledge-guided prompt as an
intervention to alleviate concept bias. The prompt adopts the topic of the
given entity from the existing knowledge in KGs to mitigate the spurious
co-occurrence correlations between entities and biased concepts. Our extensive
experiments on representative multilingual KG datasets justify that our
proposed prompt can effectively alleviate concept bias and improve the
performance of PLM-based CE models.The code has been released on
https://github.com/siyuyuan/KPCE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01879">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM's predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01901">
<div class="article-summary-box-inner">
<span><p>Few-shot event detection (ED) has been widely studied, while this brings
noticeable discrepancies, e.g., various motivations, tasks, and experimental
settings, that hinder the understanding of models for future progress. This
paper presents a thorough empirical study, a unified view of ED models, and a
better unified baseline. For fair evaluation, we choose two practical settings:
low-resource setting to assess generalization ability and class-transfer
setting for transferability. We compare ten representative methods on three
datasets, which are roughly grouped into prompt-based and prototype-based
models for detailed analysis. To investigate the superior performance of
prototype-based methods, we break down the design and build a unified
framework. Based on that, we not only propose a simple yet effective method
(e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable
research insights for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Natural Language Watermarking through Invariant Features. (arXiv:2305.01904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01904">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed a proliferation of valuable original natural
language contents found in subscription-based media outlets, web novel
platforms, and outputs of large language models. Without proper security
measures, however, these contents are susceptible to illegal piracy and
potential misuse. This calls for a secure watermarking system to guarantee
copyright protection through leakage tracing or ownership identification. To
effectively combat piracy and protect copyrights, a watermarking framework
should be able not only to embed adequate bits of information but also extract
the watermarks in a robust manner despite possible corruption. In this work, we
explore ways to advance both payload and robustness by following a well-known
proposition from image watermarking and identify features in natural language
that are invariant to minor corruption. Through a systematic analysis of the
possible sources of errors, we further propose a corruption-resistant infill
model. Our full method improves upon the previous work on robustness by +16.8%
point on average on four datasets, three corruption types, and two corruption
ratios. Code available at https://github.com/bangawayoo/nlp-watermarking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Interventions-based Few-Shot Named Entity Recognition. (arXiv:2305.01914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01914">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) systems aims at recognizing new
classes of entities based on a few labeled samples. A significant challenge in
the few-shot regime is prone to overfitting than the tasks with abundant
samples. The heavy overfitting in few-shot learning is mainly led by spurious
correlation caused by the few samples selection bias. To alleviate the problem
of the spurious correlation in the few-shot NER, in this paper, we propose a
causal intervention-based few-shot NER method. Based on the prototypical
network, the method intervenes in the context and prototype via backdoor
adjustment during training. In particular, intervening in the context of the
one-shot scenario is very difficult, so we intervene in the prototype via
incremental learning, which can also avoid catastrophic forgetting. Our
experiments on different benchmarks show that our approach achieves new
state-of-the-art results (achieving up to 29% absolute improvement and 12% on
average for all tasks).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01918">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Meta-Learning for Zero-Shot Relation Triplet Extraction. (arXiv:2305.01920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01920">
<div class="article-summary-box-inner">
<span><p>The zero-shot relation triplet extraction (ZeroRTE) task aims to extract
relation triplets from a piece of text with unseen relation types. The seminal
work adopts the pre-trained generative model to generate synthetic samples for
new relations. However, current generative models lack the optimization process
of model generalization on different tasks during training, and thus have
limited generalization capability. For this reason, we propose a novel
generative meta-learning framework which exploits the `learning-to-learn'
ability of meta-learning to boost the generalization capability of generative
models. Specifically, we first design a task-aware generative model which can
learn the general knowledge by forcing the optimization process to be conducted
across multiple tasks. Based on it, we then present three generative
meta-learning approaches designated for three typical meta-learning categories.
Extensive experimental results demonstrate that our framework achieves a new
state-of-the-art performance for the ZeroRTE task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01937">
<div class="article-summary-box-inner">
<span><p>Human evaluation is indispensable and inevitable for assessing the quality of
texts generated by machine learning models or written by humans. However, human
evaluation is very difficult to reproduce and its quality is notoriously
unstable, hindering fair comparisons among different natural language
processing (NLP) models and algorithms. Recently, large language models (LLMs)
have demonstrated exceptional performance on unseen tasks when only the task
instructions are provided. In this paper, we explore if such an ability of the
LLMs can be used as an alternative to human evaluation. We present the LLMs
with the exact same instructions, samples to be evaluated, and questions used
to conduct human evaluation, and then ask the LLMs to generate responses to
those questions; we dub this LLM evaluation. We use human evaluation and LLM
evaluation to evaluate the texts in two NLP tasks: open-ended story generation
and adversarial attacks. We show that the result of LLM evaluation is
consistent with the results obtained by expert human evaluation: the texts
rated higher by human experts are also rated higher by the LLMs. We also find
that the results of LLM evaluation are stable over different formatting of the
task instructions and the sampling algorithm used to generate the answer. We
are the first to show the potential of using LLMs to assess the quality of
texts and discuss the limitations and ethical considerations of LLM evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01938">
<div class="article-summary-box-inner">
<span><p>Discrete reasoning over table-text documents (e.g., financial reports) gains
increasing attention in recent two years. Existing works mostly simplify this
challenge by manually selecting and transforming document pages to structured
tables and paragraphs, hindering their practical application. In this work, we
explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer
the question over a visually-rich table-text document. Specifically, we propose
a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by
harnessing the differences and correlations among different elements (e.g.,
quantities, dates) of the given question and document with Semantic-oriented
hierarchical Graph structures. We conduct extensive experiments on TAT-DQA
dataset, and the results show that our proposed framework outperforms the best
baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score
respectively on the test set, achieving the new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01951">
<div class="article-summary-box-inner">
<span><p>Recent pre-trained language models (PLMs) achieve promising results in
existing abstractive summarization datasets. However, existing summarization
benchmarks overlap in time with the standard pre-training corpora and
finetuning datasets. Hence, the strong performance of PLMs may rely on the
parametric knowledge that is memorized during pre-training and fine-tuning.
Moreover, the knowledge memorized by PLMs may quickly become outdated, which
affects the generalization performance of PLMs on future data. In this work, we
propose TempoSum, a novel benchmark that contains data samples from 2010 to
2022, to understand the temporal generalization ability of abstractive
summarization models. Through extensive human evaluation, we show that
parametric knowledge stored in summarization models significantly affects the
faithfulness of the generated summaries on future data. Moreover, existing
faithfulness enhancement methods cannot reliably improve the faithfulness of
summarization models on future data. Finally, we discuss several
recommendations to the research community on how to evaluate and improve the
temporal generalization capability of text summarization models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01954">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a prevalent technique for improving performance in
various machine learning applications. We propose SeqAug, a modality-agnostic
augmentation method that is tailored towards sequences of extracted features.
The core idea of SeqAug is to augment the sequence by resampling from the
underlying feature distribution. Resampling is performed by randomly selecting
feature dimensions and permuting them along the temporal axis. Experiments on
CMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully
applied to a single modality or multiple modalities. We further verify its
compatibility with both recurrent and transformer architectures, and also
demonstrate comparable to state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NorQuAD: Norwegian Question Answering Dataset. (arXiv:2305.01957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01957">
<div class="article-summary-box-inner">
<span><p>In this paper we present NorQuAD: the first Norwegian question answering
dataset for machine reading comprehension. The dataset consists of 4,752
manually created question-answer pairs. We here detail the data collection
procedure and present statistics of the dataset. We also benchmark several
multilingual and Norwegian monolingual language models on the dataset and
compare them against human performance. The dataset will be made freely
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research. (arXiv:2305.01965v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01965">
<div class="article-summary-box-inner">
<span><p>Modelling of early language acquisition aims to understand how infants
bootstrap their language skills. The modelling encompasses properties of the
input data used for training the models, the cognitive hypotheses and their
algorithmic implementations being tested, and the evaluation methodologies to
compare models to human data. Recent developments have enabled the use of more
naturalistic training data for computational models. This also motivates
development of more naturalistic tests of model behaviour. A crucial step
towards such an aim is to develop representative speech datasets consisting of
speech heard by infants in their natural environments. However, a major
drawback of such recordings is that they are typically noisy, and it is
currently unclear how the sound quality could affect analyses and modelling
experiments conducted on such data. In this paper, we explore this aspect for
the case of infant-directed speech (IDS) and adult-directed speech (ADS)
analysis. First, we manually and automatically annotated audio quality of
utterances extracted from two corpora of child-centred long-form recordings (in
English and French). We then compared acoustic features of IDS and ADS in an
in-lab dataset and across different audio quality subsets of naturalistic data.
Finally, we assessed how the audio quality and recording environment may change
the conclusions of a modelling analysis using a recent self-supervised learning
model. Our results show that the use of modest and high audio quality
naturalistic speech data result in largely similar conclusions on IDS and ADS
in terms of acoustic analyses and modelling experiments. We also found that an
automatic sound quality assessment tool can be used to screen out useful parts
of long-form recordings for a closer analysis with comparable results to that
of manual quality annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural language processing on customer note data. (arXiv:2305.02029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02029">
<div class="article-summary-box-inner">
<span><p>Automatic analysis of customer data for businesses is an area that is of
interest to companies. Business to business data is studied rarely in academia
due to the sensitive nature of such information. Applying natural language
processing can speed up the analysis of prohibitively large sets of data. This
paper addresses this subject and applies sentiment analysis, topic modelling
and keyword extraction to a B2B data set. We show that accurate sentiment can
be extracted from the notes automatically and the notes can be sorted by
relevance into different topics. We see that without clear separation topics
can lack relevance to a business context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02031">
<div class="article-summary-box-inner">
<span><p>Modern Natural Language Generation (NLG) models come with massive
computational and storage requirements. In this work, we study the potential of
compressing them, which is crucial for real-world applications serving millions
of users. We focus on Knowledge Distillation (KD) techniques, in which a small
student model learns to imitate a large teacher model, allowing to transfer
knowledge from the teacher to the student. In contrast to much of the previous
work, our goal is to optimize the model for a specific NLG task and a specific
dataset. Typically, in real-world applications, in addition to labeled data
there is abundant unlabeled task-specific data, which is crucial for attaining
high compression rates via KD. In this work, we conduct a systematic study of
task-specific KD techniques for various NLG tasks under realistic assumptions.
We discuss the special characteristics of NLG distillation and particularly the
exposure bias problem. Following, we derive a family of Pseudo-Target (PT)
augmentation methods, substantially extending prior work on sequence-level KD.
We propose the Joint-Teaching method for NLG distillation, which applies
word-level KD to multiple PTs generated by both the teacher and the student.
Our study provides practical model design observations and demonstrates the
effectiveness of PT training for task-specific KD in NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Response-conditioned Turn-taking Prediction. (arXiv:2305.02036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02036">
<div class="article-summary-box-inner">
<span><p>Previous approaches to turn-taking and response generation in conversational
systems have treated it as a two-stage process: First, the end of a turn is
detected (based on conversation history), then the system generates an
appropriate response. Humans, however, do not take the turn just because it is
likely, but also consider whether what they want to say fits the position. In
this paper, we present a model (an extension of TurnGPT) that conditions the
end-of-turn prediction on both conversation history and what the next speaker
wants to say. We found that our model consistently outperforms the baseline
model in a variety of metrics. The improvement is most prominent in two
scenarios where turn predictions can be ambiguous solely from the conversation
history: 1) when the current utterance contains a statement followed by a
question; 2) when the end of the current utterance semantically matches the
response. Treating the turn-prediction and response-ranking as a one-stage
process, our findings suggest that our model can be used as an incremental
response ranker, which can be applied in various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Considerations for Ethical Speech Recognition Datasets. (arXiv:2305.02081v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02081">
<div class="article-summary-box-inner">
<span><p>Speech AI Technologies are largely trained on publicly available datasets or
by the massive web-crawling of speech. In both cases, data acquisition focuses
on minimizing collection effort, without necessarily taking the data subjects'
protection or user needs into consideration. This results to models that are
not robust when used on users who deviate from the dominant demographics in the
training set, discriminating individuals having different dialects, accents,
speaking styles, and disfluencies. In this talk, we use automatic speech
recognition as a case study and examine the properties that ethical speech
datasets should possess towards responsible AI applications. We showcase
diversity issues, inclusion practices, and necessary considerations that can
improve trained models, while facilitating model explainability and protecting
users and data subjects. We argue for the legal &amp; privacy protection of data
subjects, targeted data sampling corresponding to user demographics &amp; needs,
appropriate meta data that ensure explainability &amp; accountability in cases of
model failure, and the sociotechnical \&amp; situated model design. We hope this
talk can inspire researchers \&amp; practitioners to design and use more
human-centric datasets in speech technologies and other domains, in ways that
empower and respect users, while improving machine learning models' robustness
and utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What makes a good pause? Investigating the turn-holding effects of fillers. (arXiv:2305.02101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02101">
<div class="article-summary-box-inner">
<span><p>Filled pauses (or fillers), such as "uh" and "um", are frequent in
spontaneous speech and can serve as a turn-holding cue for the listener,
indicating that the current speaker is not done yet. In this paper, we use the
recently proposed Voice Activity Projection (VAP) model, which is a deep
learning model trained to predict the dynamics of conversation, to analyse the
effects of filled pauses on the expected turn-hold probability. The results
show that, while filled pauses do indeed have a turn-holding effect, it is
perhaps not as strong as could be expected, probably due to the redundancy of
other cues. We also find that the prosodic properties and position of the
filler has a significant effect on the turn-hold probability. However, contrary
to what has been suggested in previous work, there is no difference between
"uh" and "um" in this regard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries. (arXiv:2305.02104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02104">
<div class="article-summary-box-inner">
<span><p>Communication of scientific findings to the public is important for keeping
non-experts informed of developments such as life-saving medical treatments.
However, generating readable lay summaries from scientific documents is
challenging, and currently, these summaries suffer from critical factual
errors. One popular intervention for improving factuality is using additional
external knowledge to provide factual grounding. However, it is unclear how
these grounding sources should be retrieved, selected, or integrated, and how
supplementary grounding documents might affect the readability or relevance of
the generated summaries. We develop a simple method for selecting grounding
sources and integrating them with source documents. We then use the BioLaySum
summarization dataset to evaluate the effects of different grounding sources on
summary quality. We found that grounding source documents improves the
relevance and readability of lay summaries but does not improve factuality of
lay summaries. This continues to be true in zero-shot summarization settings
where we hypothesized that grounding might be even more important for factual
lay summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02105">
<div class="article-summary-box-inner">
<span><p>In spite of the potential for ground-breaking achievements offered by large
language models (LLMs) (e.g., GPT-3), they still lag significantly behind
fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).
This is due to the two major shortcomings of LLMs in RE: (1) low relevance
regarding entity and relation in retrieved demonstrations for in-context
learning; and (2) the strong inclination to wrongly classify NULL examples into
other pre-defined labels.
</p>
<p>In this paper, we propose GPT-RE to bridge the gap between LLMs and
fully-supervised baselines. GPT-RE successfully addresses the aforementioned
issues by (1) incorporating task-specific entity representations in
demonstration retrieval; and (2) enriching the demonstrations with gold
label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE
datasets, and observe that GPT-RE achieves improvements over not only existing
GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE
achieves SOTA performances on the Semeval and SciERC datasets, and competitive
performances on the TACRED and ACE05 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay More Attention to Relation Exploration for Knowledge Base Question Answering. (arXiv:2305.02118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02118">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) is a challenging task that aims to
retrieve correct answers from large-scale knowledge bases. Existing attempts
primarily focus on entity representation and final answer reasoning, which
results in limited supervision for this task. Moreover, the relations, which
empirically determine the reasoning path selection, are not fully considered in
recent advancements. In this study, we propose a novel framework, RE-KBQA, that
utilizes relations in the knowledge base to enhance entity representation and
introduce additional supervision. We explore guidance from relations in three
aspects, including (1) distinguishing similar entities by employing a
variational graph auto-encoder to learn relation importance; (2) exploring
extra supervision by predicting relation distributions as soft labels with a
multi-task scheme; (3) designing a relation-guided re-ranking algorithm for
post-processing. Experimental results on two benchmark datasets demonstrate the
effectiveness and superiority of our framework, improving the F1 score by 5.7%
from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par
with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02139">
<div class="article-summary-box-inner">
<span><p>Robust loss functions are designed to combat the adverse impacts of label
noise, whose robustness is typically supported by theoretical bounds agnostic
to the training dynamics. However, these bounds may fail to characterize the
empirical performance as it remains unclear why robust loss functions can
underfit. We show that most loss functions can be rewritten into a form with
the same class-score margin and different sample-weighting functions. The
resulting curriculum view provides a straightforward analysis of the training
dynamics, which helps attribute underfitting to diminished average sample
weights and noise robustness to larger weights for clean samples. We show that
simple fixes to the curriculums can make underfitting robust loss functions
competitive with the state-of-the-art, and training schedules can substantially
affect the noise robustness even with robust loss functions. Code is available
at \url{github}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02151">
<div class="article-summary-box-inner">
<span><p>Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Listwise Document Reranking with a Large Language Model. (arXiv:2305.02156v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02156">
<div class="article-summary-box-inner">
<span><p>Supervised ranking methods based on bi-encoder or cross-encoder architectures
have shown success in multi-stage text ranking tasks, but they require large
amounts of relevance judgments as training data. In this work, we propose
Listwise Reranker with a Large Language Model (LRL), which achieves strong
reranking effectiveness without using any task-specific training data.
Different from the existing pointwise ranking methods, where documents are
scored independently and ranked according to the scores, LRL directly generates
a reordered list of document identifiers given the candidate documents.
Experiments on three TREC web search datasets demonstrate that LRL not only
outperforms zero-shot pointwise methods when reranking first-stage retrieval
results, but can also act as a final-stage reranker to improve the top-ranked
results of a pointwise method for improved efficiency. Additionally, we apply
our approach to subsets of MIRACL, a recent multilingual retrieval dataset,
with results showing its potential to generalize across different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02160">
<div class="article-summary-box-inner">
<span><p>The emergence of large-scale pretrained language models has posed
unprecedented challenges in deriving explanations of why the model has made
some predictions. Stemmed from the compositional nature of languages, spurious
correlations have further undermined the trustworthiness of NLP systems,
leading to unreliable model explanations that are merely correlated with the
output predictions. To encourage fairness and transparency, there exists an
urgent demand for reliable explanations that allow users to consistently
understand the model's behavior. In this work, we propose a complete framework
for extending concept-based interpretability methods to NLP. Specifically, we
propose a post-hoc interpretability method for extracting predictive high-level
features (concepts) from the pretrained model's hidden layer activations. We
optimize for features whose existence causes the output predictions to change
substantially, \ie generates a high impact. Moreover, we devise several
evaluation metrics that can be universally applied. Extensive experiments on
real and synthetic tasks demonstrate that our method achieves superior results
on {predictive impact}, usability, and faithfulness compared to the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02170">
<div class="article-summary-box-inner">
<span><p>We present a pipeline for a statistical textual exploration, offering a
stylometry-based explanation and statistical validation of a hypothesized
partition of a text. Given a parameterization of the text, our pipeline: (1)
detects literary features yielding the optimal overlap between the hypothesized
and unsupervised partitions, (2) performs a hypothesis-testing analysis to
quantify the statistical significance of the optimal overlap, while conserving
implicit correlations between units of text that are more likely to be grouped,
and (3) extracts and quantifies the importance of features most responsible for
the classification, estimates their statistical stability and cluster-wise
abundance.
</p>
<p>We apply our pipeline to the first two books in the Bible, where one
stylistic component stands out in the eyes of biblical scholars, namely, the
Priestly component. We identify and explore statistically significant stylistic
differences between the Priestly and non-Priestly components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02176">
<div class="article-summary-box-inner">
<span><p>Mixture-of-experts (MoE) models that employ sparse activation have
demonstrated effectiveness in significantly increasing the number of parameters
while maintaining low computational requirements per token. However, recent
studies have established that MoE models are inherently parameter-inefficient
as the improvement in performance diminishes with an increasing number of
experts. We hypothesize this parameter inefficiency is a result of all experts
having equal capacity, which may not adequately meet the varying complexity
requirements of different tokens or tasks, e.g., in a multilingual setting,
languages based on their resource levels might require different capacities. In
light of this, we propose Stratified Mixture of Experts(SMoE) models, which
feature a stratified structure and can assign dynamic capacity to different
tokens. We demonstrate the effectiveness of SMoE on two multilingual machine
translation benchmarks, where it outperforms multiple state-of-the-art MoE
models. On a diverse 15-language dataset, SMoE improves the translation quality
over vanilla MoE by +0.93 BLEU points on average. Additionally, SMoE is
parameter-efficient, matching vanilla MoE performance with around 50\% fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02215">
<div class="article-summary-box-inner">
<span><p>The overwhelming success of transformers is a real conundrum stimulating a
compelling question: are these machines replicating some traditional linguistic
models or discovering radically new theories? In this paper, we propose a novel
standpoint to investigate this important question. Using typological
similarities among languages, we aim to layer-wise compare transformers for
different languages to observe whether these similarities emerge for particular
layers. For this investigation, we propose to use Centered kernel alignment to
measure similarity among weight matrices. We discovered that syntactic
typological similarity is consistent with the similarity among weights in the
middle layers. This finding confirms results obtained by syntactically probing
BERT and, thus, gives an important confirmation that BERT is replicating
traditional linguistic models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02220">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking. (arXiv:2305.02235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02235">
<div class="article-summary-box-inner">
<span><p>Annotating long-document question answering (long-document QA) pairs is
time-consuming and expensive. To alleviate the problem, it might be possible to
generate long-document QA pairs via unsupervised question answering (UQA)
methods. However, existing UQA tasks are based on short documents, and can
hardly incorporate long-range information. To tackle the problem, we propose a
new task, named unsupervised long-document question answering (ULQA), aiming to
generate high-quality long-document QA instances in an unsupervised manner.
Besides, we propose AttenWalker, a novel unsupervised method to aggregate and
generate answers with long-range dependency so as to construct long-document QA
pairs. Specifically, AttenWalker is composed of three modules, i.e., span
collector, span linker and answer aggregator. Firstly, the span collector takes
advantage of constituent parsing and reconstruction loss to select informative
candidate spans for constructing answers. Secondly, by going through the
attention graph of a pre-trained long-document model, potentially interrelated
text spans (that might be far apart) could be linked together via an
attention-walking algorithm. Thirdly, in the answer aggregator, linked spans
are aggregated into the final answer via the mask-filling ability of a
pre-trained model. Extensive experiments show that AttenWalker outperforms
previous methods on Qasper and NarrativeQA. In addition, AttenWalker also shows
strong performance in the few-shot learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02239">
<div class="article-summary-box-inner">
<span><p>Large language models have improved zero-shot text classification by allowing
the transfer of semantic knowledge from the training data in order to classify
among specific label sets in downstream tasks. We propose a simple way to
further improve zero-shot accuracies with minimal effort. We curate small
finetuning datasets intended to describe the labels for a task. Unlike typical
finetuning data, which has texts annotated with labels, our data simply
describes the labels in language, e.g., using a few related terms,
dictionary/encyclopedia entries, and short templates. Across a range of topic
and sentiment datasets, our method is more accurate than zero-shot by 15-17%
absolute. It is also more robust to choices required for zero-shot
classification, such as patterns for prompting the model to classify and
mappings from labels to tokens in the model's vocabulary. Furthermore, since
our data merely describes the labels but does not use input texts, finetuning
on it yields a model that performs strongly on multiple text domains for a
given label set, even improving over few-shot out-of-domain classification in
multiple settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Training and Decoding for Pivot-based Cascaded Translation Model. (arXiv:2305.02261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02261">
<div class="article-summary-box-inner">
<span><p>Utilizing pivot language effectively can significantly improve low-resource
machine translation. Usually, the two translation models, source-pivot and
pivot-target, are trained individually and do not utilize the limited (source,
target) parallel data. This work proposes an end-to-end training method for the
cascaded translation model and configures an improved decoding algorithm. The
input of the pivot-target model is modified to weighted pivot embedding based
on the probability distribution output by the source-pivot model. This allows
the model to be trained end-to-end. In addition, we mitigate the inconsistency
between tokens and probability distributions while using beam search in pivot
decoding. Experiments demonstrate that our method enhances the quality of
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02265">
<div class="article-summary-box-inner">
<span><p>Pretrained Vision-Language Models (VLMs) have achieved remarkable performance
in image retrieval from text. However, their performance drops drastically when
confronted with linguistically complex texts that they struggle to comprehend.
Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this
paper, we regard linguistically complex texts as compound proposition texts
composed of multiple simple proposition sentences and propose an end-to-end
Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three
main components: 1)Divide: a proposition generator divides the compound
proposition text into simple proposition sentences and produces their
corresponding representations, 2)Conquer: a pretrained VLMs-based
visual-linguistic interactor achieves the interaction between decomposed
proposition sentences and images, 3)Combine: a neural-symbolic reasoner
combines the above reasoning states to obtain the final solution via a neural
logic reasoning approach. According to the dual-process theory, the
visual-linguistic interactor and neural-symbolic reasoner could be regarded as
analogical reasoning System 1 and logical reasoning System 2. We conduct
extensive experiments on a challenging image retrieval from contextual
descriptions data set. Experimental results and analyses indicate NDCR
significantly improves performance in the complex image-text reasoning problem.
Code link: https://github.com/YunxinLi/NDCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis. (arXiv:2305.02269v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02269">
<div class="article-summary-box-inner">
<span><p>Conversational text-to-speech (TTS) aims to synthesize speech with proper
prosody of reply based on the historical conversation. However, it is still a
challenge to comprehensively model the conversation, and a majority of
conversational TTS systems only focus on extracting global information and omit
local prosody features, which contain important fine-grained information like
keywords and emphasis. Moreover, it is insufficient to only consider the
textual features, and acoustic features also contain various prosody
information. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal
conversational text-to-speech system, aiming to comprehensively utilize
historical conversation and enhance prosodic expression. More specifically, we
design a textual context module and an acoustic context module with both
coarse-grained and fine-grained modeling. Experimental results demonstrate that
our model mixed with fine-grained context information and additionally
considering acoustic features achieves better prosody performance and
naturalness in CMOS tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections. (arXiv:2305.02291v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02291">
<div class="article-summary-box-inner">
<span><p>The rapid growth of research publications has placed great demands on digital
libraries (DL) for advanced information management technologies. To cater to
these demands, techniques relying on knowledge-graph structures are being
advocated. In such graph-based pipelines, inferring semantic relations between
related scientific concepts is a crucial step. Recently, BERT-based pre-trained
models have been popularly explored for automatic relation classification.
Despite significant progress, most of them were evaluated in different
scenarios, which limits their comparability. Furthermore, existing methods are
primarily evaluated on clean texts, which ignores the digitization context of
early scholarly publications in terms of machine scanning and optical character
recognition (OCR). In such cases, the texts may contain OCR noise, in turn
creating uncertainty about existing classifiers' performances. To address these
limitations, we started by creating OCR-noisy texts based on three clean
corpora. Given these parallel corpora, we conducted a thorough empirical
evaluation of eight Bert-based classification models by focusing on three
factors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise
impacts. Experiments on clean data show that the domain-specific pre-trained
Bert is the best variant to identify scientific relations. The strategy of
predicting a single relation each time outperforms the one simultaneously
identifying multiple relations in general. The optimal classifier's performance
can decline by around 10% to 20% in F-score on the noisy corpora. Insights
discussed in this study can help DL stakeholders select techniques for building
optimal knowledge-graph-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Efficacy of Length-Controllable Machine Translation. (arXiv:2305.02300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02300">
<div class="article-summary-box-inner">
<span><p>Length-controllable machine translation is a type of constrained translation.
It aims to contain the original meaning as much as possible while controlling
the length of the translation. We can use automatic summarization or machine
translation evaluation metrics for length-controllable machine translation, but
this is not necessarily suitable and accurate. This work is the first attempt
to evaluate the automatic metrics for length-controllable machine translation
tasks systematically. We conduct a rigorous human evaluation on two translation
directions and evaluate 18 summarization or translation evaluation metrics. We
find that BLEURT and COMET have the highest correlation with human evaluation
and are most suitable as evaluation metrics for length-controllable machine
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02301">
<div class="article-summary-box-inner">
<span><p>Deploying large language models (LLMs) is challenging because they are memory
inefficient and compute-intensive for practical applications. In reaction,
researchers train smaller task-specific models by either finetuning with human
labels or distilling using LLM-generated labels. However, finetuning and
distillation require large amounts of training data to achieve comparable
performance to LLMs. We introduce Distilling step-by-step, a new mechanism that
(a) trains smaller models that outperform LLMs, and (b) achieves so by
leveraging less training data needed by finetuning or distillation. Our method
extracts LLM rationales as additional supervision for small models within a
multi-task training framework. We present three findings across 4 NLP
benchmarks: First, compared to both finetuning and distillation, our mechanism
achieves better performance with much fewer labeled/unlabeled training
examples. Second, compared to LLMs, we achieve better performance using
substantially smaller model sizes. Third, we reduce both the model size and the
amount of data required to outperform LLMs; our 770M T5 model outperforms the
540B PaLM model using only 80% of available data on a benchmark task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02317">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models elicit reasoning in a chain of
thought that allows models to decompose problems in a human-like fashion.
Though this paradigm improves multi-step reasoning ability in language models,
it is limited by being unimodal and applied mainly to question-answering tasks.
We claim that incorporating visual augmentation into reasoning is essential,
especially for complex, imaginative tasks. Consequently, we introduce VCoT, a
novel method that leverages chain of thought prompting with vision-language
grounding to recursively bridge the logical gaps within sequential data. Our
method uses visual guidance to generate synthetic multimodal infillings that
add consistent and novel information to reduce the logical gaps for downstream
tasks that can benefit from temporal reasoning, as well as provide
interpretability into models' multi-step reasoning. We apply VCoT to the Visual
Storytelling and WikiHow summarization datasets and demonstrate through human
evaluation that VCoT offers novel and consistent synthetic data augmentation
beating chain of thought baselines, which can be used to enhance downstream
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02321">
<div class="article-summary-box-inner">
<span><p>Growing literature has shown that powerful NLP systems may encode social
biases; however, the political bias of summarization models remains relatively
unknown. In this work, we use an entity replacement method to investigate the
portrayal of politicians in automatically generated summaries of news articles.
We develop a computational framework based on political entities and lexical
resources, and use it to assess biases about Donald Trump and Joe Biden in both
extractive and abstractive summarization models. We find consistent
differences, such as stronger associations of a collective US government (i.e.,
administration) with Biden than with Trump. These summary dissimilarities are
most prominent when the entity is heavily featured in the source article. Our
systematic characterization provides a framework for future studies of bias in
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01263">
<div class="article-summary-box-inner">
<span><p>Sample-and-rank is a key decoding strategy for modern generation-based
dialogue systems. It helps achieve diverse and high-quality responses by
selecting an answer from a small pool of generated candidates. The current
state-of-the-art ranking methods mainly use an encoding paradigm called
Cross-Encoder, which separately encodes each context-candidate pair and ranks
the candidates according to their fitness scores. However, Cross-Encoder
repeatedly encodes the same lengthy context for each candidate, resulting in
high computational costs. Poly-Encoder addresses the above problems by reducing
the interaction between context and candidates, but with a price of performance
drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps
the full attention over each pair as in Cross-Encoder while only encoding the
context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with
the context in one forward pass. We use the same positional embedding for all
candidates to ensure they are treated equally and design a new attention
mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking
paradigms using different attention and response concatenation methods.
Extensive experiments show that our proposed paradigm achieves new
state-of-the-art results on four benchmark datasets with high computational
efficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X
faster inference speed on the Ubuntu V2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00439">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are inherently opaque and challenging to interpret.
Unlike hand-crafted feature-based models, we struggle to comprehend the
concepts learned and how they interact within these models. This understanding
is crucial not only for debugging purposes but also for ensuring fairness in
ethical decision-making. In our study, we conduct a post-hoc functional
interpretability analysis of pretrained speech models using the probing
framework [1]. Specifically, we analyze utterance-level representations of
speech models trained for various tasks such as speaker recognition and dialect
identification. We conduct layer and neuron-wise analyses, probing for speaker,
language, and channel properties. Our study aims to answer the following
questions: i) what information is captured within the representations? ii) how
is it represented and distributed? and iii) can we identify a minimal subset of
the network that possesses this information?
</p>
<p>Our results reveal several novel findings, including: i) channel and gender
information are distributed across the network, ii) the information is
redundantly available in neurons with respect to a task, iii) complex
properties such as dialectal information are encoded only in the task-oriented
pretrained network, iv) and is localised in the upper layers, v) we can extract
a minimal subset of neurons encoding the pre-defined property, vi) salient
neurons are sometimes shared between properties, vii) our analysis highlights
the presence of biases (for example gender) in the network. Our
cross-architectural comparison indicates that: i) the pretrained models capture
speaker-invariant information, and ii) CNN models are competitive with
Transformer models in encoding various understudied properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation. (arXiv:2112.04539v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04539">
<div class="article-summary-box-inner">
<span><p>In relation triplet extraction (RTE), recognizing unseen (new) relations for
which there are no training instances is a challenging task. Efforts have been
made to recognize unseen relations based on question-answering models or
relation descriptions. However, these approaches miss the semantic information
about connections between seen and unseen relations. In this paper, We propose
a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize
unseen relations under the zero-shot setting. We present a new word-level
analogy-based sentence translation rule and generate augmented instances with
unseen relations from instances with seen relations using that new rule. We
design prompts with weighted virtual label construction based on an external
knowledge graph to integrate semantic knowledge information learned from seen
relations. Instead of using the actual label sets in the prompt template, we
construct weighted virtual label words. We learn the representations of both
seen and unseen relations with augmented instances and prompts. We then
calculate the distance between the generated representations using prototypical
networks to predict unseen relations. Extensive experiments conducted on three
public datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperforms
state-of-the-art methods under the zero-shot scenarios. Our experimental
results also demonstrate the effectiveness and robustness of ZS-SKA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07648">
<div class="article-summary-box-inner">
<span><p>Recent progress in representation and contrastive learning in NLP has not
widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning
in interaction within different language communities). To bridge this gap, we
propose a novel framework for learning task-agnostic representations
transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate
speech, humor, sarcasm). Our framework outperforms other contrastive learning
frameworks for both in-domain and out-of-domain data, across both the general
and few-shot settings. For example, compared to two popular pre-trained
language models, our method obtains an improvement of $11.66$ average $F_1$ on
$16$ datasets when fine-tuned on only $20$ training samples per dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Adversarial Purification as Defense against Adversarial Attacks. (arXiv:2203.14207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14207">
<div class="article-summary-box-inner">
<span><p>Adversarial purification is a successful defense mechanism against
adversarial attacks without requiring knowledge of the form of the incoming
attack. Generally, adversarial purification aims to remove the adversarial
perturbations therefore can make correct predictions based on the recovered
clean samples. Despite the success of adversarial purification in the computer
vision field that incorporates generative models such as energy-based models
and diffusion models, using purification as a defense strategy against textual
adversarial attacks is rarely explored. In this work, we introduce a novel
adversarial purification method that focuses on defending against textual
adversarial attacks. With the help of language models, we can inject noise by
masking input texts and reconstructing the masked texts based on the masked
language models. In this way, we construct an adversarial purification process
for textual models against the most widely used word-substitution adversarial
attacks. We test our proposed adversarial purification method on several strong
adversarial attack methods including Textfooler and BERT-Attack and
experimental results indicate that the purification algorithm can successfully
defend against strong word-substitution attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes. (arXiv:2205.05656v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05656">
<div class="article-summary-box-inner">
<span><p>Computational text phenotyping is the practice of identifying patients with
certain disorders and traits from clinical notes. Rare diseases are challenging
to be identified due to few cases available for machine learning and the need
for data annotation from domain experts. We propose a method using ontologies
and weak supervision, with recent pre-trained contextual representations from
Bi-directional Transformers (e.g. BERT). The ontology-based framework includes
two steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking
mentions to concepts in Unified Medical Language System (UMLS), with a Named
Entity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with
customised rules and contextual mention representation; (ii) UMLS-to-ORDO,
matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology
(ORDO). The weakly supervised approach is proposed to learn a phenotype
confirmation model to improve Text-to-UMLS linking, without annotated data from
domain experts. We evaluated the approach on three clinical datasets, MIMIC-III
discharge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging
reports from two institutions in the US and the UK, with annotations. The
improvements in the precision were pronounced (by over 30% to 50% absolute
score for Text-to-UMLS linking), with almost no loss of recall compared to the
existing NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and
NHS Tayside were consistent with the discharge summaries. The overall pipeline
processing clinical notes can extract rare disease cases, mostly uncaptured in
structured data (manually assigned ICD codes). We discuss the usefulness of the
weak supervision approach and propose directions for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11342">
<div class="article-summary-box-inner">
<span><p>Transformer-based masked language models such as BERT, trained on general
corpora, have shown impressive performance on downstream tasks. It has also
been demonstrated that the downstream task performance of such models can be
improved by pretraining larger models for longer on more data. In this work, we
empirically evaluate the extent to which these results extend to tasks in
science. We use 14 domain-specific transformer-based models (including
ScholarBERT, a new 770M-parameter science-focused masked language model
pretrained on up to 225B tokens) to evaluate the impact of training data, model
size, pretraining and finetuning time on 12 downstream scientific tasks.
Interestingly, we find that increasing model sizes, training data, or compute
time does not always lead to significant improvements (i.e., &gt;1% F1), if at
all, in scientific information extraction tasks and offered possible
explanations for the surprising performance differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15171">
<div class="article-summary-box-inner">
<span><p>Societal biases are reflected in large pre-trained language models and their
fine-tuned versions on downstream tasks. Common in-processing bias mitigation
approaches, such as adversarial training and mutual information removal,
introduce additional optimization criteria, and update the model to reach a new
debiased state. However, in practice, end-users and practitioners might prefer
to switch back to the original model, or apply debiasing only on a specific
subset of protected attributes. To enable this, we propose a novel modular bias
mitigation approach, consisting of stand-alone highly sparse debiasing
subnetworks, where each debiasing module can be integrated into the core model
on-demand at inference time. Our approach draws from the concept of \emph{diff}
pruning, and proposes a novel training regime adaptable to various
representation disentanglement optimizations. We conduct experiments on three
classification tasks with gender, race, and age as protected attributes. The
results show that our modular approach, while maintaining task performance,
improves (or at least remains on-par with) the effectiveness of bias mitigation
in comparison with baseline finetuning. Particularly on a two-attribute
dataset, our approach with separately learned debiasing subnetworks shows
effective utilization of either or both the subnetworks for selective bias
mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContraCLM: Contrastive Learning For Causal Language Model. (arXiv:2210.01185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01185">
<div class="article-summary-box-inner">
<span><p>Despite exciting progress in causal language models, the expressiveness of
the representations is largely limited due to poor discrimination ability. To
remedy this issue, we present ContraCLM, a novel contrastive learning framework
at both token-level and sequence-level. We assess ContraCLM on a variety of
downstream tasks. We show that ContraCLM enhances discrimination of the
representations and bridges the gap with the encoder-only models, which makes
causal language models better suited for tasks beyond language generation.
Specifically, we attain $44\%$ relative improvement on the Semantic Textual
Similarity tasks and $34\%$ on Code-to-Code Search tasks. Furthermore, by
improving the expressiveness of the representations, ContraCLM also boosts the
source code generation capability with $9\%$ relative improvement on execution
accuracy on the HumanEval benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Are We from Real Synonym Substitution Attacks?. (arXiv:2210.02844v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02844">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the following question: how far are we from real
synonym substitution attacks (SSAs). We approach this question by examining how
SSAs replace words in the original sentence and show that there are still
unresolved obstacles that make current SSAs generate invalid adversarial
samples. We reveal that four widely used word substitution methods generate a
large fraction of invalid substitution words that are ungrammatical or do not
preserve the original sentence's semantics. Next, we show that the semantic and
grammatical constraints used in SSAs for detecting invalid word replacements
are highly insufficient in detecting invalid adversarial samples. Our work is
an important stepping stone to constructing better SSAs in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05643">
<div class="article-summary-box-inner">
<span><p>It has become standard to solve NLP tasks by fine-tuning pre-trained language
models (LMs), especially in low-data settings. There is minimal theoretical
understanding of empirical success, e.g., why fine-tuning a model with $10^8$
or more parameters on a couple dozen training points does not result in
overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which
originated as a model to study the gradient descent dynamics of infinitely wide
networks with suitable random initialization - describes fine-tuning of
pre-trained LMs. This study was inspired by the decent performance of NTK for
computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam
and use Tensor Programs (Yang, 2020) to characterize conditions under which the
NTK lens may describe fine-tuning updates to pre-trained language models.
Extensive experiments on 14 NLP tasks validate our theory and show that
formulating the downstream task as a masked word prediction problem through
prompting often induces kernel-based dynamics during fine-tuning. Finally, we
use this kernel view to propose an explanation for the success of
parameter-efficient subspace-based fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14348">
<div class="article-summary-box-inner">
<span><p>Privacy concerns have attracted increasing attention in data-driven products
due to the tendency of machine learning models to memorize sensitive training
data. Generating synthetic versions of such data with a formal privacy
guarantee, such as differential privacy (DP), provides a promising path to
mitigating these privacy concerns, but previous approaches in this direction
have typically failed to produce synthetic data of high quality. In this work,
we show that a simple and practical recipe in the text domain is effective:
simply fine-tuning a pretrained generative language model with DP enables the
model to generate useful synthetic text with strong privacy protection. Through
extensive empirical analyses on both benchmark and private customer data, we
demonstrate that our method produces synthetic text that is competitive in
terms of utility with its non-private counterpart, meanwhile providing strong
protection against potential privacy leakages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Audio-Visual Noise Suppression. (arXiv:2211.03643v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03643">
<div class="article-summary-box-inner">
<span><p>This paper studies audio-visual noise suppression for egocentric videos --
where the speaker is not captured in the video. Instead, potential noise
sources are visible on screen with the camera emulating the off-screen
speaker's view of the outside world. This setting is different from prior work
in audio-visual speech enhancement that relies on lip and facial visuals. In
this paper, we first demonstrate that egocentric visual information is helpful
for noise suppression. We compare object recognition and action
classification-based visual feature extractors and investigate methods to align
audio and visual representations. Then, we examine different fusion strategies
for the aligned features, and locations within the noise suppression model to
incorporate visual information. Experiments demonstrate that visual features
are most helpful when used to generate additive correction masks. Finally, in
order to ensure that the visual features are discriminative with respect to
different noise types, we introduce a multi-task learning framework that
jointly optimizes audio-visual noise suppression and video-based acoustic event
detection. This proposed multi-task framework outperforms the audio-only
baseline on all metrics, including a 0.16 PESQ improvement. Extensive ablations
reveal the improved performance of the proposed model with multiple active
distractors, overall noise types, and across different SNRs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03760">
<div class="article-summary-box-inner">
<span><p>Recent studies have proposed unified user modeling frameworks that leverage
user behavior data from various applications. Many of them benefit from
utilizing users' behavior sequences as plain texts, representing rich
information in any domain or system without losing generality. Hence, a
question arises: Can language modeling for user history corpus help improve
recommender systems? While its versatile usability has been widely investigated
in many domains, its applications to recommender systems still remain
underexplored. We show that language modeling applied directly to task-specific
user histories achieves excellent results on diverse recommendation tasks.
Also, leveraging additional task-agnostic user histories delivers significant
performance benefits. We further demonstrate that our approach can provide
promising transfer learning capabilities for a broad spectrum of real-world
recommender systems, even on unseen domains and services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness of Learning from Task Instructions. (arXiv:2212.03813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03813">
<div class="article-summary-box-inner">
<span><p>Traditional supervised learning mostly works on individual tasks and requires
training on a large set of task-specific examples. This paradigm seriously
hinders the development of task generalization since preparing a task-specific
example set is costly. To build a system that can quickly and easily generalize
to new tasks, task instructions have been adopted as an emerging trend of
supervision recently. These instructions give the model the definition of the
task and allow the model to output the appropriate answer based on the
instructions and inputs. However, task instructions are often expressed in
different forms, which can be interpreted from two threads: first, some
instructions are short sentences and are pretrained language model (PLM)
oriented, such as prompts, while other instructions are paragraphs and are
human-oriented, such as those in Amazon MTurk; second, different end-users very
likely explain the same task with instructions of different textual
expressions. A robust system for task generalization should be able to handle
any new tasks regardless of the variability of instructions.
</p>
<p>However, the system robustness in dealing with instruction-driven task
generalization is still unexplored. This work investigates the system
robustness when the instructions of new tasks are (i) manipulated, (ii)
paraphrased, or (iii) from different levels of conciseness. To our knowledge,
this is the first work that systematically studies how robust a PLM is when it
is supervised by instructions with different factors of variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09651">
<div class="article-summary-box-inner">
<span><p>Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09683">
<div class="article-summary-box-inner">
<span><p>We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. Using our baseline system,
we show that human fact-checkers can identify 124 tweets per hour that violate
Twitter's policies on COVID-19 misinformation. We will make our code, data,
baseline models, and detailed annotation guidelines available to support the
evaluation of human-in-the-loop systems that identify novel misinformation
directly from raw user-generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09736">
<div class="article-summary-box-inner">
<span><p>A key missing capacity of current language models (LMs) is grounding to
real-world environments. Most existing work for grounded language understanding
uses LMs to directly generate plans that can be executed in the environment to
achieve the desired effects. It thereby casts the burden of ensuring
grammaticality, faithfulness, and controllability all on the LMs. We propose
Pangu, a generic framework for grounded language understanding that capitalizes
on the discriminative ability of LMs instead of their generative ability. Pangu
consists of a symbolic agent and a neural LM working in a concerted fashion:
The agent explores the environment to incrementally construct valid plans, and
the LM evaluates the plausibility of the candidate plans to guide the search
process. A case study on the challenging problem of knowledge base question
answering (KBQA), which features a massive environment, demonstrates the
remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient
for setting a new record on standard KBQA datasets, and larger LMs further
bring substantial gains. Pangu also enables, for the first time, effective
few-shot in-context learning for KBQA with large LMs such as Codex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10325">
<div class="article-summary-box-inner">
<span><p>Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10375">
<div class="article-summary-box-inner">
<span><p>Despite the surprising few-shot performance of in-context learning (ICL), it
is still a common practice to randomly sample examples to serve as context.
This paper advocates a new principle for ICL: self-adaptive in-context
learning. The self-adaption mechanism is introduced to help each sample find an
in-context example permutation (i.e., selection and ordering) that can derive
the correct prediction, thus maximizing performance. To validate the
effectiveness of self-adaptive ICL, we propose a general select-then-rank
framework and instantiate it with new selection and ranking algorithms. Upon
extensive evaluation on eight different NLP datasets, our self-adaptive ICL
method achieves a 40% relative improvement over the common practice setting.
Further analysis reveals the enormous potential of self-adaptive ICL that it
might be able to close the gap between ICL and finetuning given more advanced
algorithms. Our code is released to facilitate future research in this area:
https://github.com/Shark-NLP/self-adaptive-ICL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10562">
<div class="article-summary-box-inner">
<span><p>Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03488">
<div class="article-summary-box-inner">
<span><p>Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05658">
<div class="article-summary-box-inner">
<span><p>This paper introduces the DocILE benchmark with the largest dataset of
business documents for the tasks of Key Information Localization and Extraction
and Line Item Recognition. It contains 6.7k annotated business documents, 100k
synthetically generated documents, and nearly~1M unlabeled documents for
unsupervised pre-training. The dataset has been built with knowledge of domain-
and task-specific aspects, resulting in the following key features: (i)
annotations in 55 classes, which surpasses the granularity of previously
published key information extraction datasets by a large margin; (ii) Line Item
Recognition represents a highly practical information extraction task, where
key information has to be assigned to items in a table; (iii) documents come
from numerous layouts and the test set includes zero- and few-shot cases as
well as layouts commonly seen in the training set. The benchmark comes with
several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table
Transformer; applied to both tasks of the DocILE benchmark, with results shared
in this paper, offering a quick starting point for future work. The dataset,
baselines and supplementary material are available at
https://github.com/rossumai/docile.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Product Question Answering in E-Commerce: A Survey. (arXiv:2302.08092v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08092">
<div class="article-summary-box-inner">
<span><p>Product question answering (PQA), aiming to automatically provide instant
responses to customer's questions in E-Commerce platforms, has drawn increasing
attention in recent years. Compared with typical QA problems, PQA exhibits
unique challenges such as the subjectivity and reliability of user-generated
contents in E-commerce platforms. Therefore, various problem settings and novel
methods have been proposed to capture these special characteristics. In this
paper, we aim to systematically review existing research efforts on PQA.
Specifically, we categorize PQA studies into four problem settings in terms of
the form of provided answers. We analyze the pros and cons, as well as present
existing datasets and evaluation protocols for each setting. We further
summarize the most significant challenges that characterize PQA from general QA
applications and discuss their corresponding solutions. Finally, we conclude
this paper by providing the prospect on several future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09173">
<div class="article-summary-box-inner">
<span><p>This work explores the problem of generating task graphs of real-world
activities. Different from prior formulations, we consider a setting where text
transcripts of instructional videos performing a real-world activity (e.g.,
making coffee) are provided and the goal is to identify the key steps relevant
to the task as well as the dependency relationship between these key steps. We
propose a novel task graph generation approach that combines the reasoning
capabilities of instruction-tuned language models along with clustering and
ranking components to generate accurate task graphs in a completely
unsupervised manner. We show that the proposed approach generates more accurate
task graphs compared to a supervised learning approach on tasks from the ProceL
and CrossTask datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Social Media for Early Detection of Depression in COVID-19 Patients. (arXiv:2302.12044v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12044">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused substantial damage to global health. Even
though three years have passed, the world continues to struggle with the virus.
Concerns are growing about the impact of COVID-19 on the mental health of
infected individuals, who are more likely to experience depression, which can
have long-lasting consequences for both the affected individuals and the world.
Detection and intervention at an early stage can reduce the risk of depression
in COVID-19 patients. In this paper, we investigated the relationship between
COVID-19 infection and depression through social media analysis. Firstly, we
managed a dataset of COVID-19 patients that contains information about their
social media activity both before and after infection. Secondly,We conducted an
extensive analysis of this dataset to investigate the characteristic of
COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep
neural network for early prediction of depression risk. This model considers
daily mood swings as a psychiatric signal and incorporates textual and
emotional characteristics via knowledge distillation. Experimental results
demonstrate that our proposed framework outperforms baselines in detecting
depression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has
the potential to enable public health organizations to initiate prompt
intervention with high-risk patients
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Architext: Language-Driven Generative Architecture Design. (arXiv:2303.07519v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07519">
<div class="article-summary-box-inner">
<span><p>Architectural design is a highly complex practice that involves a wide
diversity of disciplines, technologies, proprietary design software, expertise,
and an almost infinite number of constraints, across a vast array of design
tasks. Enabling intuitive, accessible, and scalable design processes is an
important step towards performance-driven and sustainable design for all. To
that end, we introduce Architext, a novel semantic generation assistive tool.
Architext enables design generation with only natural language prompts, given
to large-scale Language Models, as input. We conduct a thorough quantitative
evaluation of Architext's downstream task performance, focusing on semantic
accuracy and diversity for a number of pre-trained language models ranging from
120 million to 6 billion parameters. Architext models are able to learn the
specific design task, generating valid residential layouts at a near 100% rate.
Accuracy shows great improvement when scaling the models, with the largest
model (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for
different prompt categories. We open source the finetuned Architext models and
our synthetic dataset, hoping to inspire experimentation in this exciting area
of design research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10475">
<div class="article-summary-box-inner">
<span><p>Task semantics can be expressed by a set of input-to-output examples or a
piece of textual instruction. Conventional machine learning approaches for
natural language processing (NLP) mainly rely on the availability of
large-scale sets of task-specific examples. Two issues arise: first, collecting
task-specific labeled examples does not apply to scenarios where tasks may be
too complicated or costly to annotate, or the system is required to handle a
new task immediately; second, this is not user-friendly since end-users are
probably more willing to provide task description rather than a set of examples
before using the system. Therefore, the community is paying increasing interest
in a new supervision-seeking paradigm for NLP: learning from task instructions.
Despite its impressive progress, there are some common issues that the
community struggles with. This survey paper tries to summarize and provide
insights into the current research on instruction learning, particularly by
answering the following questions: (i) What is task instruction, and what
instruction types exist? (ii) How to model instructions? (iii) What factors
influence and explain the instructions' performance? (iv) What challenges
remain in instruction learning? To our knowledge, this is the first
comprehensive survey about textual instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17579">
<div class="article-summary-box-inner">
<span><p>Automated generation of clinically accurate radiology reports can improve
patient care. Previous report generation methods that rely on image captioning
models often generate incoherent and incorrect text due to their lack of
relevant domain knowledge, while retrieval-based attempts frequently retrieve
reports that are irrelevant to the input image. In this work, we propose
Contrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology
report generation module that uses an image-text matching score to measure the
similarity of a chest X-ray image and radiology report for report retrieval. We
observe that computing the image-text matching score with a language-image
model can effectively capture the fine-grained interaction between image and
text that is often lost when using cosine similarity. X-REM outperforms
multiple prior radiology report generation modules in terms of both natural
language and clinical metrics. Human evaluation of the generated reports
suggests that X-REM increased the number of zero-error reports and decreased
the average error severity compared to the baseline retrieval approach. Our
code is available at: https://github.com/rajpurkarlab/X-REM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08448">
<div class="article-summary-box-inner">
<span><p>The 'Impression' section of a radiology report is a critical basis for
communication between radiologists and other physicians, and it is typically
written by radiologists based on the 'Findings' section. However, writing
numerous impressions can be laborious and error-prone for radiologists.
Although recent studies have achieved promising results in automatic impression
generation using large-scale medical text data for pre-training and fine-tuning
pre-trained language models, such models often require substantial amounts of
medical text data and have poor generalization performance. While large
language models (LLMs) like ChatGPT have shown strong generalization
capabilities and performance, their performance in specific domains, such as
radiology, remains under-investigated and potentially limited. To address this
limitation, we propose ImpressionGPT, which leverages the in-context learning
capability of LLMs by constructing dynamic contexts using domain-specific,
individualized data. This dynamic prompt approach enables the model to learn
contextual knowledge from semantically similar examples from existing data.
Additionally, we design an iterative optimization algorithm that performs
automatic evaluation on the generated impression results and composes the
corresponding instruction prompts to further optimize the model. The proposed
ImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and
OpenI datasets without requiring additional training data or fine-tuning the
LLMs. This work presents a paradigm for localizing LLMs that can be applied in
a wide range of similar application scenarios, bridging the gap between
general-purpose LLMs and the specific language processing needs of various
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers. (arXiv:2304.12730v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12730">
<div class="article-summary-box-inner">
<span><p>Citations in scientific papers not only help us trace the intellectual
lineage but also are a useful indicator of the scientific significance of the
work. Citation intents prove beneficial as they specify the role of the
citation in a given context. In this paper, we present CitePrompt, a framework
which uses the hitherto unexplored approach of prompt-based learning for
citation intent classification. We argue that with the proper choice of the
pretrained language model, the prompt template, and the prompt verbalizer, we
can not only get results that are better than or comparable to those obtained
with the state-of-the-art methods but also do it with much less exterior
information about the scientific document. We report state-of-the-art results
on the ACL-ARC dataset, and also show significant improvement on the SciCite
dataset over all baseline models except one. As suitably large labelled
datasets for citation intent classification can be quite hard to find, in a
first, we propose the conversion of this task to the few-shot and zero-shot
settings. For the ACL-ARC dataset, we report a 53.86% F1 score for the
zero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and
10-shot settings, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>With the wide application of Large Language Models (LLMs) such as ChatGPT,
how to make the contents generated by LLM accurate and credible becomes very
important, especially in complex knowledge-intensive tasks. In this paper, we
propose a novel framework called Search-in-the-Chain (SearChain) to improve the
accuracy, credibility and traceability of LLM-generated content for multi-hop
question answering, which is a typical complex knowledge-intensive task.
SearChain is a framework that deeply integrates LLM and information retrieval
(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition
of the multi-hop question. Each node of the chain is a query-answer pair
consisting of an IR-oriented query and the answer generated by LLM for this
query. IR verifies, completes, and traces the information of each node of the
chain, so as to guide LLM to construct the correct chain-of-query, and finally
answer the multi-hop question. SearChain makes LLM change from trying to give a
answer to trying to construct the chain-of-query when faced with the multi-hop
question, which can stimulate the knowledge-reasoning ability and provides the
interface for IR to be deeply involved in reasoning process of LLM. IR
interacts with each node of chain-of-query of LLM. It verifies the information
of the node and provides the unknown knowledge to LLM, which ensures the
accuracy of the whole chain in the process of LLM generating the answer.
Besides, the contents returned by LLM to the user include not only the final
answer but also the reasoning process for the question, that is, the
chain-of-query and the supporting documents retrieved by IR for each node of
the chain, which improves the credibility and traceability of the contents
generated by LLM. Experimental results show SearChain outperforms related
baselines on four multi-hop question-answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00969">
<div class="article-summary-box-inner">
<span><p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries, and the accompanying CryCeleb 2023 task - a public speaker
verification challenge based on infant cry sounds. We release for academic
usage more than 6 hours of manually segmented cry sounds from 786 newborns to
encourage research in infant cry analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01219">
<div class="article-summary-box-inner">
<span><p>The prompt-based learning paradigm, which bridges the gap between
pre-training and fine-tuning, achieves state-of-the-art performance on several
NLP tasks, particularly in few-shot settings. Despite being widely applied,
prompt-based learning is vulnerable to backdoor attacks. Textual backdoor
attacks are designed to introduce targeted vulnerabilities into models by
poisoning a subset of training samples through trigger injection and label
modification. However, they suffer from flaws such as abnormal natural language
expressions resulting from the trigger and incorrect labeling of poisoned
samples. In this study, we propose ProAttack, a novel and efficient method for
performing clean-label backdoor attacks based on the prompt, which uses the
prompt itself as a trigger. Our method does not require external triggers and
ensures correct labeling of poisoned samples, improving the stealthy nature of
the backdoor attack. With extensive experiments on rich-resource and few-shot
text classification tasks, we empirically validate ProAttack's competitive
performance in textual backdoor attacks. Notably, in the rich-resource setting,
ProAttack achieves state-of-the-art attack success rates in the clean-label
backdoor attack benchmark without external triggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01555">
<div class="article-summary-box-inner">
<span><p>Scaling language models have revolutionized widespread NLP tasks, yet little
comprehensively explored few-shot relation extraction with large language
models. In this paper, we investigate principal methodologies, in-context
learning and data generation, for few-shot relation extraction via GPT-3.5
through exhaustive experiments. To enhance few-shot performance, we further
propose task-related instructions and schema-constrained data generation. We
observe that in-context learning can achieve performance on par with previous
prompt learning approaches, and data generation with the large language model
can boost previous solutions to obtain new state-of-the-art few-shot results on
four widely-studied relation extraction datasets. We hope our work can inspire
future research for the capabilities of large language models in few-shot
relation extraction. Code is available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01645">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large models is highly effective, however, inference using these
models can be expensive and produces carbon emissions. Knowledge distillation
has been shown to be a practical solution to reduce inference costs, but the
distillation process itself requires significant computational resources.
Rather than buying or renting GPUs to fine-tune, then distill a large model, an
NLP practitioner who needs a compact model might also choose to simply allocate
an available budget to hire annotators and manually label additional
fine-tuning data. In this paper, we investigate how to most efficiently use a
fixed budget to build a compact model. Through our extensive experiments on six
diverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M)
leads to almost always a cost-efficient option compared to annotating more data
to directly train a compact model (T5-Small (60M)). We further demonstrate that
the optimal amount of distillation that maximizes utility varies across
different budgetary scenarios.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-04 23:12:15.820295814 UTC">2023-05-04 23:12:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>