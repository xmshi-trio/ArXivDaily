<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-25T01:30:00Z">04-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity. (arXiv:2304.11163v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11163">
<div class="article-summary-box-inner">
<span><p>The allure of emerging AI technologies is undoubtedly thrilling. However, the
promise that AI technologies will benefit all of humanity is empty so long as
we lack a nuanced understanding of what humanity is supposed to be in the face
of widening global inequality and pressing existential threats. Going forward,
it is crucial to invest in rigorous and collaborative AI safety and ethics
research. We also need to develop standards in a sustainable and equitable way
that differentiate between merely speculative and well-researched questions.
Only the latter enable us to co-construct and deploy the values that are
necessary for creating beneficial AI. Failure to do so could result in a future
in which our AI technological advancements outstrip our ability to navigate
their ethical and social implications. This path we do not want to go down.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. (arXiv:2304.11164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11164">
<div class="article-summary-box-inner">
<span><p>Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11220">
<div class="article-summary-box-inner">
<span><p>Conversational models that are generative and open-domain are particularly
susceptible to generating unsafe content since they are trained on web-based
social data. Prior approaches to mitigating this issue have drawbacks, such as
disrupting the flow of conversation, limited generalization to unseen toxic
input contexts, and sacrificing the quality of the dialogue for the sake of
safety. In this paper, we present a novel framework, named "LOT" (Learn NOT
to), that employs a contrastive loss to enhance generalization by learning from
both positive and negative training signals. Our approach differs from the
standard contrastive learning framework in that it automatically obtains
positive and negative signals from the safe and unsafe language distributions
that have been learned beforehand. The LOT framework utilizes divergence to
steer the generations away from the unsafe subspace and towards the safe
subspace while sustaining the flow of conversation. Our approach is memory and
time-efficient during decoding and effectively reduces toxicity while
preserving engagingness and fluency. Empirical results indicate that LOT
reduces toxicity by up to four-fold while achieving four to six-fold higher
rates of engagingness and fluency compared to baseline models. Our findings are
further corroborated by human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Group-Specific Approach to NLP for Hate Speech Detection. (arXiv:2304.11223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11223">
<div class="article-summary-box-inner">
<span><p>Automatic hate speech detection is an important yet complex task, requiring
knowledge of common sense, stereotypes of protected groups, and histories of
discrimination, each of which may constantly evolve. In this paper, we propose
a group-specific approach to NLP for online hate speech detection. The approach
consists of creating and infusing historical and linguistic knowledge about a
particular protected group into hate speech detection models, analyzing
historical data about discrimination against a protected group to better
predict spikes in hate speech against that group, and critically evaluating
hate speech detection models through lenses of intersectionality and ethics. We
demonstrate this approach through a case study on NLP for detection of
antisemitic hate speech. The case study synthesizes the current
English-language literature on NLP for antisemitism detection, introduces a
novel knowledge graph of antisemitic history and language from the 20th century
to the present, infuses information from the knowledge graph into a set of
tweets over Logistic Regression and uncased DistilBERT baselines, and suggests
that incorporating context from the knowledge graph can help models pick up
subtle stereotypes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis. (arXiv:2304.11256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11256">
<div class="article-summary-box-inner">
<span><p>We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared
task, where we tackle the task of sentiment analysis in 14 different African
languages. We develop both monolingual and multilingual models under a full
supervised setting (subtasks A and B). We also develop models for the zero-shot
setting (subtask C). Our approach involves experimenting with transfer learning
using six language models, including further pertaining of some of these models
as well as a final finetuning stage. Our best performing models achieve an
F1-score of 70.36 on development data and an F1-score of 66.13 on test data.
Unsurprisingly, our results demonstrate the effectiveness of transfer learning
and fine-tuning techniques for sentiment analysis across multiple languages.
Our approach can be applied to other sentiment analysis tasks in different
languages and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers. (arXiv:2304.11257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11257">
<div class="article-summary-box-inner">
<span><p>Written answers to open-ended questions can have a higher long-term effect on
learning than multiple-choice questions. However, it is critical that teachers
immediately review the answers, and ask to redo those that are incoherent. This
can be a difficult task and can be time-consuming for teachers. A possible
solution is to automate the detection of incoherent answers. One option is to
automate the review with Large Language Models (LLM). In this paper, we analyze
the responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,
and YOU. We used them with zero, one, two, three and four shots. We compared
their performance with the results of various classifiers trained with Machine
Learning (ML). We found that LLMs perform worse than MLs in detecting
incoherent answers. The difficulty seems to reside in recursive questions that
contain both questions and answers, and in responses from students with typical
fourth-grader misspellings. Upon closer examination, we have found that the
ChatGPT model faces the same challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students. (arXiv:2304.11276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11276">
<div class="article-summary-box-inner">
<span><p>The recent advancement in Natural Language Processing (NLP) capability has
led to the development of language models (e.g., ChatGPT) that is capable of
generating human-like language. In this study, we explore how language models
can be utilized to help the ideation aspect of creative writing. Our empirical
findings show that language models play different roles in helping student
writers to be more creative, such as the role of a collaborator, a provocateur,
etc
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11292">
<div class="article-summary-box-inner">
<span><p>The energy inefficiency of the apps can be a major issue for the app users
which is discussed on App Stores extensively. Previous research has shown the
importance of investigating the energy related app reviews to identify the
major causes or categories of energy related user feedback. However, there is
no study that efficiently extracts the energy related app reviews
automatically. In this paper, we empirically study different techniques for
automatic extraction of the energy related user feedback. We compare the
accuracy, F1-score and run time of numerous machine-learning models with
relevant feature combinations and relatively modern Neural Network-based
models. In total, 60 machine learning models are compared to 30 models that we
build using six neural network architectures and three word embedding models.
We develop a visualization tool for this study through which a developer can
traverse through this large-scale result set. The results show that neural
networks outperform the other machine learning techniques and can achieve the
highest F1-score of 0.935. To replicate the research results, we have open
sourced the interactive visualization tool. After identifying the best results
and extracting the energy related reviews, we further compare various
techniques to help the developers automatically investigate the emerging issues
that might be responsible for energy inefficiency of the apps. We experiment
the previously used string matching with results obtained from applying two of
the state-of-the-art topic modeling algorithms, OBTM and AOLDA. Finally, we run
a qualitative study performed in collaboration with developers and students
from different institutions to determine their preferences for identifying
necessary topics from previously categorized reviews, which shows OBTM produces
the most helpful results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Specialization for Knowledge-based Word Sense Disambiguation. (arXiv:2304.11340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11340">
<div class="article-summary-box-inner">
<span><p>A promising approach for knowledge-based Word Sense Disambiguation (WSD) is
to select the sense whose contextualized embeddings computed for its definition
sentence are closest to those computed for a target word in a given sentence.
This approach relies on the similarity of the \textit{sense} and
\textit{context} embeddings computed by a pre-trained language model. We
propose a semantic specialization for WSD where contextualized embeddings are
adapted to the WSD task using solely lexical knowledge. The key idea is, for a
given sense, to bring semantically related senses and contexts closer and send
different/unrelated senses farther away. We realize this idea as the joint
optimization of the Attract-Repel objective for sense pairs and the
self-training objective for context-sense pairs while controlling deviations
from the original embeddings. The proposed method outperformed previous studies
that adapt contextualized embeddings. It achieved state-of-the-art performance
on knowledge-based WSD when combined with the reranking heuristic that uses the
sense inventory. We found that the similarity characteristics of specialized
embeddings conform to the key idea. We also found that the (dis)similarity of
embeddings between the related/different/unrelated senses correlates well with
the performance of WSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition. (arXiv:2304.11350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11350">
<div class="article-summary-box-inner">
<span><p>Multiword expressions are a key ingredient for developing large-scale and
linguistically sound natural language processing technology. This paper
describes our improvements in automatically identifying Romanian multiword
expressions on the corpus released for the PARSEME v1.2 shared task. Our
approach assumes a multilingual perspective based on the recently introduced
lateral inhibition layer and adversarial training to boost the performance of
the employed multilingual language models. With the help of these two methods,
we improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseen
multiword expressions, the main task of the PARSEME 1.2 edition. In addition,
our results can be considered SOTA performance, as they outperform the previous
results on Romanian obtained by the participants in this competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. (arXiv:2304.11370v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11370">
<div class="article-summary-box-inner">
<span><p>Legal case retrieval, which aims to find relevant cases for a query case,
plays a core role in the intelligent legal system. Despite the success that
pre-training has achieved in ad-hoc retrieval tasks, effective pre-training
strategies for legal case retrieval remain to be explored. Compared with
general documents, legal case documents are typically long text sequences with
intrinsic logical structures. However, most existing language models have
difficulty understanding the long-distance dependencies between different
structures. Moreover, in contrast to the general retrieval, the relevance in
the legal domain is sensitive to key legal elements. Even subtle differences in
key legal elements can significantly affect the judgement of relevance.
However, existing pre-trained language models designed for general purposes
have not been equipped to handle legal elements.
</p>
<p>To address these issues, in this paper, we propose SAILER, a new
Structure-Aware pre-traIned language model for LEgal case Retrieval. It is
highlighted in the following three aspects: (1) SAILER fully utilizes the
structural information contained in legal case documents and pays more
attention to key legal elements, similar to how legal experts browse legal case
documents. (2) SAILER employs an asymmetric encoder-decoder architecture to
integrate several different pre-training objectives. In this way, rich semantic
information across tasks is encoded into dense vectors. (3) SAILER has powerful
discriminative ability, even without any legal annotation data. It can
distinguish legal cases with different charges accurately. Extensive
experiments over publicly available legal benchmarks demonstrate that our
approach can significantly outperform previous state-of-the-art methods in
legal case retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11389">
<div class="article-summary-box-inner">
<span><p>Recent psycholinguistic studies have drawn conflicting conclusions about the
relationship between the quality of a language model and the ability of its
surprisal estimates to predict human reading times, which has been speculated
to be due to the large gap in both the amount of training data and model
capacity across studies. The current work aims to consolidate these findings by
evaluating surprisal estimates from Transformer-based language model variants
that vary systematically in the amount of training data and model capacity on
their ability to predict human reading times. The results show that surprisal
estimates from most variants with contemporary model capacities provide the
best fit after seeing about two billion training tokens, after which they begin
to diverge from humanlike expectations. Additionally, newly-trained smaller
model variants reveal a 'tipping point' at convergence, after which the
decrease in language model perplexity begins to result in poorer fits to human
reading times. These results suggest that the massive amount of training data
is mainly responsible for the poorer fit achieved by surprisal from larger
pre-trained language models, and that a certain degree of model capacity is
necessary for Transformer-based language models to capture humanlike
expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11406">
<div class="article-summary-box-inner">
<span><p>This paper highlights the importance of personalization in the current state
of natural language understanding and generation and introduces the LaMP
benchmark -- a novel benchmark for training and evaluating language models for
producing personalized outputs. LaMP offers a comprehensive evaluation
framework with diverse language tasks and multiple entries for each user
profile. It consists of seven personalized tasks, spanning three classification
and four text generation tasks. We also propose a retrieval augmentation
approach that retrieves personalized items from user profiles to construct
personalized prompts for large language models. Our baseline zero-shot and
fine-tuned model results indicate that LMs utilizing profile augmentation
outperform their counterparts that do not factor in profile information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A bounded rationality account of dependency length minimization in Hindi. (arXiv:2304.11410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11410">
<div class="article-summary-box-inner">
<span><p>The principle of DEPENDENCY LENGTH MINIMIZATION, which seeks to keep
syntactically related words close in a sentence, is thought to universally
shape the structure of human languages for effective communication. However,
the extent to which dependency length minimization is applied in human language
systems is not yet fully understood. Preverbally, the placement of
long-before-short constituents and postverbally, short-before-long constituents
are known to minimize overall dependency length of a sentence. In this study,
we test the hypothesis that placing only the shortest preverbal constituent
next to the main-verb explains word order preferences in Hindi (a SOV language)
as opposed to the global minimization of dependency length. We characterize
this approach as a least-effort strategy because it is a cost-effective way to
shorten all dependencies between the verb and its preverbal dependencies. As
such, this approach is consistent with the bounded-rationality perspective
according to which decision making is governed by "fast but frugal" heuristics
rather than by a search for optimal solutions. Consistent with this idea, our
results indicate that actual corpus sentences in the Hindi-Urdu Treebank corpus
are better explained by the least effort strategy than by global minimization
of dependency lengths. Additionally, for the task of distinguishing corpus
sentences from counterfactual variants, we find that the dependency length and
constituent length of the constituent closest to the main verb are much better
predictors of whether a sentence appeared in the corpus than total dependency
length. Overall, our findings suggest that cognitive resource constraints play
a crucial role in shaping natural languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11434">
<div class="article-summary-box-inner">
<span><p>The multilingual Sentence-BERT (SBERT) models map different languages to
common representation space and are useful for cross-language similarity and
mining tasks. We propose a simple yet effective approach to convert vanilla
multilingual BERT models into multilingual sentence BERT models using synthetic
corpus. We simply aggregate translated NLI or STS datasets of the low-resource
target languages together and perform SBERT-like fine-tuning of the vanilla
multilingual BERT model. We show that multilingual BERT models are inherent
cross-lingual learners and this simple baseline fine-tuning approach without
explicit cross-lingual training yields exceptional cross-lingual properties. We
show the efficacy of our approach on 10 major Indic languages and also show the
applicability of our approach to non-Indic languages German and French. Using
this approach, we further present L3Cube-IndicSBERT, the first multilingual
sentence representation model specifically for Indian languages Hindi, Marathi,
Kannada, Telugu, Malayalam, Tamil, Gujarati, Odia, Bengali, and Punjabi. The
IndicSBERT exhibits strong cross-lingual capabilities and performs
significantly better than alternatives like LaBSE, LASER, and
paraphrase-multilingual-mpnet-base-v2 on Indic cross-lingual and monolingual
sentence similarity tasks. We also release monolingual SBERT models for each of
the languages and show that IndicSBERT performs competitively with its
monolingual counterparts. These models have been evaluated using embedding
similarity scores and classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey. (arXiv:2304.11461v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11461">
<div class="article-summary-box-inner">
<span><p>This is a tutorial paper on Recurrent Neural Network (RNN), Long Short-Term
Memory Network (LSTM), and their variants. We start with a dynamical system and
backpropagation through time for RNN. Then, we discuss the problems of gradient
vanishing and explosion in long-term dependencies. We explain close-to-identity
weight matrix, long delays, leaky units, and echo state networks for solving
this problem. Then, we introduce LSTM gates and cells, history and variants of
LSTM, and Gated Recurrent Units (GRU). Finally, we introduce bidirectional RNN,
bidirectional LSTM, and the Embeddings from Language Model (ELMo) network, for
processing a sequence in both directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11473">
<div class="article-summary-box-inner">
<span><p>As ecommerce continues growing, huge investments in ML and NLP for
Information Retrieval are following. While the vector space model dominated
retrieval modelling in product search - even as vectorization itself greatly
changed with the advent of deep learning -, our position paper argues in a
contrarian fashion that program synthesis provides significant advantages for
many queries and a significant number of players in the market. We detail the
industry significance of the proposed approach, sketch implementation details,
and address common objections drawing from our experience building a similar
system at Tooso.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11485">
<div class="article-summary-box-inner">
<span><p>Individuals involved in gang-related activity use mainstream social media
including Facebook and Twitter to express taunts and threats as well as grief
and memorializing. However, identifying the impact of gang-related activity in
order to serve community member needs through social media sources has a unique
set of challenges. This includes the difficulty of ethically identifying
training data of individuals impacted by gang activity and the need to account
for a non-standard language style commonly used in the tweets from these
individuals. Our study provides evidence of methods where natural language
processing tools can be helpful in efficiently identifying individuals who may
be in need of community care resources such as counselors, conflict mediators,
or academic/professional training programs. We demonstrate that our binary
logistic classifier outperforms baseline standards in identifying individuals
impacted by gang-related violence using a sample of gang-related tweets
associated with Chicago. We ultimately found that the language of a tweet is
highly relevant and that uses of ``big data'' methods or machine learning
models need to better understand how language impacts the model's performance
and how it discriminates among populations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11490">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translationese Reduction using Abstract Meaning Representation. (arXiv:2304.11501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11501">
<div class="article-summary-box-inner">
<span><p>Translated texts or utterances bear several hallmarks distinct from texts
originating in the language. This phenomenon, known as translationese, is
well-documented, and when found in training or test sets can affect model
performance. Still, work to mitigate the effect of translationese in human
translated text is understudied. We hypothesize that Abstract Meaning
Representation (AMR), a semantic representation which abstracts away from the
surface form, can be used as an interlingua to reduce the amount of
translationese in translated texts. By parsing English translations into an AMR
graph and then generating text from that AMR, we obtain texts that more closely
resemble non-translationese by macro-level measures. We show that across four
metrics, and qualitatively, using AMR as an interlingua enables the reduction
of translationese and we compare our results to two additional approaches: one
based on round-trip machine translation and one based on syntactically
controlled generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11520">
<div class="article-summary-box-inner">
<span><p>BERT-based neural architectures have established themselves as popular
state-of-the-art baselines for many downstream NLP tasks. However, these
architectures are data-hungry and consume a lot of memory and energy, often
hindering their deployment in many real-time, resource-constrained
applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT)
often cannot perform well on complex NLP tasks. More importantly, from a
designer's perspective, it is unclear what is the "right" BERT-based
architecture to use for a given NLP task that can strike the optimal trade-off
between the resources available and the minimum accuracy desired by the end
user. System engineers have to spend a lot of time conducting trial-and-error
experiments to find a suitable answer to this question. This paper presents an
exploratory study of BERT-based models under different resource constraints and
accuracy budgets to derive empirical observations about this resource/accuracy
trade-offs. Our findings can help designers to make informed choices among
alternative BERT-based architectures for embedded systems, thus saving
significant development time and effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11534">
<div class="article-summary-box-inner">
<span><p>Text Classification is the most essential and fundamental problem in Natural
Language Processing. While numerous recent text classification models applied
the sequential deep learning technique, graph neural network-based models can
directly deal with complex structured text data and exploit global information.
Many real text classification applications can be naturally cast into a graph,
which captures words, documents, and corpus global features. In this survey, we
bring the coverage of methods up to 2023, including corpus-level and
document-level graph neural networks. We discuss each of these methods in
detail, dealing with the graph construction mechanisms and the graph-based
learning process. As well as the technological survey, we look at issues behind
and future directions addressed in text classification using graph neural
networks. We also cover datasets, evaluation metrics, and experiment design and
present a summary of published performance on the publicly available
benchmarks. Note that we present a comprehensive comparison between different
techniques and identify the pros and cons of various evaluation metrics in this
survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide and Prompt: Chain of Thought Prompting for Text-to-SQL. (arXiv:2304.11556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11556">
<div class="article-summary-box-inner">
<span><p>Chain-of-thought (CoT) prompting combined with large language models (LLMs)
have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a
critical semantic parsing task that converts natural language questions into
SQL statements, involving a complex reasoning process. However, there is little
work about using CoT prompting to activate LLM's reasoning capabilities on
Text-to-SQL tasks. In this work, we propose a new paradigm for prompting
Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into
subtasks, and then approach each subtask through CoT. We present 3
prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments
show that these prompts guide LLMs to generate Text-to-SQL with higher
execution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiate ChatGPT-generated and Human-written Medical Texts. (arXiv:2304.11567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11567">
<div class="article-summary-box-inner">
<span><p>Background: Large language models such as ChatGPT are capable of generating
grammatically perfect and human-like text content, and a large number of
ChatGPT-generated texts have appeared on the Internet. However, medical texts
such as clinical notes and diagnoses require rigorous validation, and erroneous
medical content generated by ChatGPT could potentially lead to disinformation
that poses significant harm to healthcare and the general public.
</p>
<p>Objective: This research is among the first studies on responsible and
ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus
on analyzing the differences between medical texts written by human experts and
generated by ChatGPT, and designing machine learning workflows to effectively
detect and differentiate medical texts generated by ChatGPT.
</p>
<p>Methods: We first construct a suite of datasets containing medical texts
written by human experts and generated by ChatGPT. In the next step, we analyze
the linguistic features of these two types of content and uncover differences
in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally,
we design and implement machine learning methods to detect medical text
generated by ChatGPT.
</p>
<p>Results: Medical texts written by humans are more concrete, more diverse, and
typically contain more useful information, while medical texts generated by
ChatGPT pay more attention to fluency and logic, and usually express general
terminologies rather than effective information specific to the context of the
problem. A BERT-based model can effectively detect medical texts generated by
ChatGPT, and the F1 exceeds 95%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding. (arXiv:2304.11618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11618">
<div class="article-summary-box-inner">
<span><p>Negative sampling (NS) is widely used in knowledge graph embedding (KGE),
which aims to generate negative triples to make a positive-negative contrast
during training. However, existing NS methods are unsuitable when multi-modal
information is considered in KGE models. They are also inefficient due to their
complex design. In this paper, we propose Modality-Aware Negative Sampling
(MANS) for multi-modal knowledge graph embedding (MMKGE) to address the
mentioned problems. MANS could align structural and visual embeddings for
entities in KGs and learn meaningful embeddings to perform better in
multi-modal KGE while keeping lightweight and efficient. Empirical results on
two benchmarks demonstrate that MANS outperforms existing NS methods.
Meanwhile, we make further explorations about MANS to confirm its
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. (arXiv:2304.11633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11633">
<div class="article-summary-box-inner">
<span><p>The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11657">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can achieve highly effective performance on
various reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting as demonstrations. However, the reasoning chains of demonstrations
generated by LLMs are prone to errors, which can subsequently lead to incorrect
reasoning during inference. Furthermore, inappropriate exemplars (overly
simplistic or complex), can affect overall performance among varying levels of
difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts
Prompting), an iterative bootstrapping approach for selecting exemplars and
generating reasoning chains. By utilizing iterative bootstrapping, our approach
enables LLMs to autonomously rectify errors, resulting in more precise and
comprehensive reasoning chains. Simultaneously, our approach selects
challenging yet answerable questions accompanied by reasoning chains as
exemplars with a moderate level of difficulty, which enhances the LLMs'
generalizability across varying levels of difficulty. Experimental results
indicate that Iter-CoT exhibits superiority, achieving competitive performance
across three distinct reasoning tasks on eleven datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources. (arXiv:2304.11664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11664">
<div class="article-summary-box-inner">
<span><p>Nowadays, one of the main challenges for Question Answering Systems is to
answer complex questions using various sources of information. Multi-hop
questions are a type of complex questions that require multi-step reasoning to
answer. In this article, the IslamicPCQA dataset is introduced. This is the
first Persian dataset for answering complex questions based on non-structured
information sources and consists of 12,282 question-answer pairs extracted from
9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA
English dataset approach, which was customized to suit the complexities of the
Persian language. Answering questions in this dataset requires more than one
paragraph and reasoning. The questions are not limited to any prior knowledge
base or ontology, and to provide robust reasoning ability, the dataset also
includes supporting facts and key sentences. The prepared dataset covers a wide
range of Islamic topics and aims to facilitate answering complex Persian
questions within this subject matter
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hold the Suspect! : An Analysis on Media Framing of Itaewon Halloween Crowd Crush. (arXiv:2304.11666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11666">
<div class="article-summary-box-inner">
<span><p>Based on the 10.9K articles from top 40 news providers of South Korea, this
paper analyzed the media framing of Itaewon Halloween Crowd Crush during the
first 72 hours after the incident. By adopting word-vector embedding and
clustering, we figured out that conservative media focused on political
parties' responses and the suspect's identity while the liberal media covered
the responsibility of the government and possible unequal spillover effect on
the low-income industry workers. Although the social tragedy was not directly
connected to institutional politics, the media clearly exhibited political bias
in the coverage process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11679">
<div class="article-summary-box-inner">
<span><p>Domain knowledge refers to the in-depth understanding, expertise, and
familiarity with a specific subject, industry, field, or area of special
interest. The existing benchmarks are all lack of an overall design for domain
knowledge evaluation. Holding the belief that the real ability of domain
language understanding can only be fairly evaluated by an comprehensive and
in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa
targets at testing Large Language Models (LLMs) on their domain knowledge
understanding, it features extensive domain coverage, large data volume, and a
continually updated data set based on Chinese 112 first-level subject
classifications. DomMa consist of 100,000 questions in both Chinese and English
sourced from graduate entrance examinations and undergraduate exams in Chinese
college. We have also propose designs to make benchmark and evaluation process
more suitable to LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow. (arXiv:2304.11693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11693">
<div class="article-summary-box-inner">
<span><p>Semi-cooperative behaviors are intrinsic properties of human drivers and
should be considered for autonomous driving. In addition, new autonomous
planners can consider the social value orientation (SVO) of human drivers to
generate socially-compliant trajectories. Yet the overall impact on traffic
flow for this new class of planners remain to be understood. In this work, we
present study of implicit semi-cooperative driving where agents deploy a
game-theoretic version of iterative best response assuming knowledge of the
SVOs of other agents. We simulate nominal traffic flow and investigate whether
the proportion of prosocial agents on the road impact individual or system-wide
driving performance. Experiments show that the proportion of prosocial agents
has a minor impact on overall traffic flow and that benefits of
semi-cooperation disproportionally affect egoistic and high-speed drivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization. (arXiv:2107.05799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05799">
<div class="article-summary-box-inner">
<span><p>The computational principles underlying attention allocation in complex
goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a
passage to answer a question in mind, is a common real-world task that strongly
engages attention. Here, we investigate what computational models can explain
attention distribution in this complex task. We show that the reading time on
each word is predicted by the attention weights in transformer-based deep
neural networks (DNNs) optimized to perform the same reading task. Eye-tracking
further reveals that readers separately attend to basic text features and
question-relevant information during first-pass reading and rereading,
respectively. Similarly, text features and question relevance separately
modulate attention weights in shallow and deep DNN layers. Furthermore, when
readers scan a passage without a question in mind, their reading time is
predicted by DNNs optimized for a word prediction task. Therefore, attention
during real-world reading can be interpreted as the consequence of task
optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-based Sentiment Analysis in Document -- FOMC Meeting Minutes on Economic Projection. (arXiv:2108.04080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04080">
<div class="article-summary-box-inner">
<span><p>The Federal Open Market Committee within the Federal Reserve System is
responsible for managing inflation, maximizing employment, and stabilizing
interest rates. Meeting minutes play an important role for market movements
because they provide the birds eye view of how this economic complexity is
constantly re-weighed. Therefore, There has been growing interest in analyzing
and extracting sentiments on various aspects from large financial texts for
economic projection. However, Aspect-based Sentiment Analysis is not widely
used on financial data due to the lack of large labeled dataset. In this paper,
I propose a model to train ABSA on financial documents under weak supervision
and analyze its predictive power on various macroeconomic indicators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06027">
<div class="article-summary-box-inner">
<span><p>Recently, dense passage retrieval has become a mainstream approach to finding
relevant information in various natural language processing tasks. A number of
studies have been devoted to improving the widely adopted dual-encoder
architecture. However, most of the previous studies only consider query-centric
similarity relation when learning the dual-encoder retriever. In order to
capture more comprehensive similarity relations, we propose a novel approach
that leverages both query-centric and PAssage-centric sImilarity Relations
(called PAIR) for dense passage retrieval. To implement our approach, we make
three major technical contributions by introducing formal formulations of the
two kinds of similarity relations, generating high-quality pseudo labeled data
via knowledge distillation, and designing an effective two-stage training
procedure that incorporates passage-centric similarity relation constraint.
Extensive experiments show that our approach significantly outperforms previous
state-of-the-art models on both MSMARCO and Natural Questions datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07367">
<div class="article-summary-box-inner">
<span><p>In various natural language processing tasks, passage retrieval and passage
re-ranking are two key procedures in finding and ranking relevant information.
Since both the two procedures contribute to the final performance, it is
important to jointly optimize them in order to achieve mutual improvement. In
this paper, we propose a novel joint training approach for dense passage
retrieval and passage re-ranking. A major contribution is that we introduce the
dynamic listwise distillation, where we design a unified listwise training
approach for both the retriever and the re-ranker. During the dynamic
distillation, the retriever and the re-ranker can be adaptively improved
according to each other's relevance information. We also propose a hybrid data
augmentation strategy to construct diverse training instances for listwise
training approach. Extensive experiments show the effectiveness of our approach
on both MSMARCO and Natural Questions datasets. Our code is available at
https://github.com/PaddlePaddle/RocketQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02797">
<div class="article-summary-box-inner">
<span><p>Automated medical coding, an essential task for healthcare operation and
delivery, makes unstructured data manageable by predicting medical codes from
clinical documents. Recent advances in deep learning and natural language
processing have been widely applied to this task. However, deep learning-based
medical coding lacks a unified view of the design of neural network
architectures. This review proposes a unified framework to provide a general
understanding of the building blocks of medical coding models and summarizes
recent advanced models under the proposed framework. Our unified framework
decomposes medical coding into four main components, i.e., encoder modules for
text feature extraction, mechanisms for building deep encoder architectures,
decoder modules for transforming hidden representations into medical codes, and
the usage of auxiliary information. Finally, we introduce the benchmarks and
real-world usage and discuss key research challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-Answer Sentence Graph for Joint Modeling Answer Selection. (arXiv:2203.03549v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03549">
<div class="article-summary-box-inner">
<span><p>This research studies graph-based approaches for Answer Sentence Selection
(AS2), an essential component for retrieval-based Question Answering (QA)
systems. During offline learning, our model constructs a small-scale relevant
training graph per question in an unsupervised manner, and integrates with
Graph Neural Networks. Graph nodes are question sentence to answer sentence
pairs. We train and integrate state-of-the-art (SOTA) models for computing
scores between question-question, question-answer, and answer-answer pairs, and
use thresholding on relevance scores for creating graph edges. Online inference
is then performed to solve the AS2 task on unseen queries. Experiments on two
well-known academic benchmarks and a real-world dataset show that our approach
consistently outperforms SOTA QA baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03574">
<div class="article-summary-box-inner">
<span><p>We introduce compositional soft prompting (CSP), a parameter-efficient
learning technique to improve the zero-shot compositionality of large-scale
pretrained vision-language models (VLMs) like CLIP. We develop CSP for
compositional zero-shot learning, the task of predicting unseen
attribute-object compositions (e.g., old cat and young tiger). VLMs have a
flexible text encoder that can represent arbitrary classes as natural language
prompts but they often underperform task-specific architectures on the
compositional zero-shot benchmark datasets. CSP treats the attributes and
objects that define classes as learnable tokens of vocabulary. During training,
the vocabulary is tuned to recognize classes that compose tokens in multiple
ways (e.g., old cat and white cat). At test time, we recompose the learned
attribute-object vocabulary in new combinations to recognize novel classes. We
show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9
percentage points on AUC. CSP also outperforms CoOp, a soft prompting method
that fine-tunes the prefix context tokens, by an average of 5.8 percentage
points on AUC. We perform additional experiments to show that CSP improves
generalization to higher-order attribute-attribute-object compositions (e.g.,
old white cat) and combinations of pretrained attributes and fine-tuned
objects. The code is available at https://github.com/BatsResearch/csp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08952">
<div class="article-summary-box-inner">
<span><p>Prior studies in privacy policies frame the question answering (QA) task as
identifying the most relevant text segment or a list of sentences from a policy
document given a user query. Existing labeled datasets are heavily imbalanced
(only a few relevant segments), limiting the QA performance in this domain. In
this paper, we develop a data augmentation framework based on ensembling
retriever models that captures the relevant text segments from unlabeled policy
documents and expand the positive examples in the training set. In addition, to
improve the diversity and quality of the augmented data, we leverage multiple
pre-trained language models (LMs) and cascade them with noise reduction filter
models. Using our augmented data on the PrivacyQA benchmark, we elevate the
existing baseline by a large margin (10\% F1) and achieve a new
state-of-the-art F1 score of 50\%. Our ablation studies provide further
insights into the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12755">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the significant advance in dense retrieval (DR)
based on powerful pre-trained language models (PLM). DR models have achieved
excellent performance in several benchmark datasets, while they are shown to be
not as competitive as traditional sparse retrieval models (e.g., BM25) in a
zero-shot retrieval setting. However, in the related literature, there still
lacks a detailed and comprehensive study on zero-shot retrieval. In this paper,
we present the first thorough examination of the zero-shot capability of DR
models. We aim to identify the key factors and analyze how they affect
zero-shot retrieval performance. In particular, we discuss the effect of
several key factors related to source training set, analyze the potential bias
from the target dataset, and review and compare existing zero-shot DR models.
Our findings provide important evidence to better understand and develop
zero-shot DR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding EFL Student Idea Generation Strategies for Creative Writing with NLG Tools. (arXiv:2207.01484v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01484">
<div class="article-summary-box-inner">
<span><p>Natural language generation (NLG) is a process within artificial intelligence
where computer systems produce human-comprehensible language texts from
information. English as a foreign language (EFL) students' use of NLG tools
might facilitate their idea generation, which is fundamental to creative
writing. However, little is known about how EFL students interact with NLG
tools to generate ideas. This study explores strategies adopted by EFL students
when searching for ideas using NLG tools, evaluating ideas generated by NLG
tools and selecting NLG tools for ideas generation. Four Hong Kong secondary
school students attended workshops where they learned to write stories
comprising their own words and words generated by NLG tools. After the
workshops, they answered questions to reflect on their writing experience with
NLG tools. In a thematic analysis of the written reflections, we found students
may have existing ideas when searching for ideas and evaluating ideas with NLG
tools. Students showed some aversion to ideas generated by NLG tools and
selected NLG tools that generated a greater quantity of ideas. The findings
inform our understanding of EFL students' concerns when using NLG tools for
idea generation and can inform educators' instruction to implement NLG tools
for classroom creative writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Translation with Compiler Representations. (arXiv:2207.03578v5 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03578">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage low-level compiler intermediate representations
(IR) to improve code translation. Traditional transpilers rely on syntactic
information and handcrafted rules, which limits their applicability and
produces unnatural-looking code. Applying neural machine translation (NMT)
approaches to code has successfully broadened the set of programs on which one
can get a natural-looking translation. However, they treat the code as
sequences of text tokens, and still do not differentiate well enough between
similar pieces of code which have different semantics in different languages.
The consequence is low quality translation, reducing the practicality of NMT,
and stressing the need for an approach significantly increasing its accuracy.
Here we propose to augment code translation with IRs, specifically LLVM IR,
with results on the C++, Java, Rust, and Go languages. Our method improves upon
the state of the art for unsupervised code translation, increasing the number
of correct translations by 11% on average, and up to 79% for the Java -&gt; Rust
pair with greedy decoding. We extend previous test sets for code translation,
by adding hundreds of Go and Rust functions. Additionally, we train models with
high performance on the problem of IR decompilation, generating programming
source code from IR, and study using IRs as intermediary pivot for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12261">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00068">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Tetris, a new task of Goal-Oriented Script
Completion. Unlike previous work, it considers a more realistic and general
setting, where the input includes not only the goal but also additional user
context, including preferences and history. To address this problem, we propose
a novel approach, which uses two techniques to improve performance: (1) concept
prompting, and (2) script-oriented contrastive learning that addresses step
repetition and hallucination problems. On our WikiHow-based dataset, we find
that both methods improve performance. The dataset, repository, and models will
be publicly available to facilitate further research on this new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01338">
<div class="article-summary-box-inner">
<span><p>Humans tend to decompose a sentence into different parts like \textsc{sth do
sth at someplace} and then fill each part with certain content. Inspired by
this, we follow the \textit{principle of modular design} to propose a novel
image captioner: learning to Collocate Visual-Linguistic Neural Modules
(CVLNM). Unlike the \re{widely used} neural module networks in VQA, where the
language (\ie, question) is fully observable, \re{the task of collocating
visual-linguistic modules is more challenging.} This is because the language is
only partially observable, for which we need to dynamically collocate the
modules during the process of image captioning. To sum up, we make the
following technical contributions to design and train our CVLNM: 1)
\textit{distinguishable module design} -- \re{four modules in the encoder}
including one linguistic module for function words and three visual modules for
different content words (\ie, noun, adjective, and verb) and another linguistic
one in the decoder for commonsense reasoning, 2) a self-attention based
\textit{module controller} for robustifying the visual reasoning, 3) a
part-of-speech based \textit{syntax loss} imposed on the module controller for
further regularizing the training of our CVLNM. Extensive experiments on the
MS-COCO dataset show that our CVLNM is more effective, \eg, achieving a new
state-of-the-art 129.5 CIDEr-D, and more robust, \eg, being less likely to
overfit to dataset bias and suffering less when fewer training samples are
available. Codes are available at \url{https://github.com/GCYZSL/CVLMN}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03273">
<div class="article-summary-box-inner">
<span><p>Entities, as important carriers of real-world knowledge, play a key role in
many NLP tasks. We focus on incorporating entity knowledge into an
encoder-decoder framework for informative text generation. Existing approaches
tried to index, retrieve, and read external documents as evidence, but they
suffered from a large computational overhead. In this work, we propose an
encoder-decoder framework with an entity memory, namely EDMem. The entity
knowledge is stored in the memory as latent representations, and the memory is
pre-trained on Wikipedia along with encoder-decoder parameters. To precisely
generate entity names, we design three decoding methods to constrain entity
generation by linking entities in the memory. EDMem is a unified framework that
can be used on various entity-intensive question answering and generation
tasks. Extensive experimental results show that EDMem outperforms both
memory-based auto-encoder models and non-memory encoder-decoder models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12061">
<div class="article-summary-box-inner">
<span><p>This article presents a dataset of 10,917 news articles with hierarchical
news categories collected between 1 January 2019 and 31 December 2019. We
manually labeled the articles based on a hierarchical taxonomy with 17
first-level and 109 second-level categories. This dataset can be used to train
machine learning models for automatically classifying news articles by topic.
This dataset can be helpful for researchers working on news structuring,
classification, and predicting future events based on released news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14453">
<div class="article-summary-box-inner">
<span><p>The ability to jointly learn from multiple modalities, such as text, audio,
and visual data, is a defining feature of intelligent systems. While there have
been promising advances in designing neural networks to harness multimodal
data, the enormous success of data augmentation currently remains limited to
single-modality tasks like image classification. Indeed, it is particularly
difficult to augment each modality while preserving the overall semantic
structure of the data; for example, a caption may no longer be a good
description of an image after standard augmentations have been applied, such as
translation. Moreover, it is challenging to specify reasonable transformations
that are not tailored to a particular modality. In this paper, we introduce
LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that
automatically learns to jointly augment multimodal data in feature space, with
no constraints on the identities of the modalities or the relationship between
modalities. We show that LeMDA can (1) profoundly improve the performance of
multimodal deep learning architectures, (2) apply to combinations of modalities
that have not been previously considered, and (3) achieve state-of-the-art
results on a wide range of applications comprised of image, text, and tabular
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08956">
<div class="article-summary-box-inner">
<span><p>Africa is home to over 2000 languages from over six language families and has
the highest linguistic diversity among all continents. This includes 75
languages with at least one million speakers each. Yet, there is little NLP
research conducted on African languages. Crucial in enabling such research is
the availability of high-quality annotated datasets. In this paper, we
introduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets
in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,
Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,
Tigrinya, Twi, Xitsonga, and Yor\`ub\'a) from four language families annotated
by native speakers. The data is used in SemEval 2023 Task 12, the first
Afro-centric SemEval shared task. We describe the data collection methodology,
annotation process, and related challenges when curating each of the datasets.
We conduct experiments with different sentiment classification baselines and
discuss their usefulness. We hope AfriSenti enables new work on
under-represented languages. The dataset is available at
https://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be
loaded as a huggingface datasets
(https://huggingface.co/datasets/shmuhammad/AfriSenti).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01194">
<div class="article-summary-box-inner">
<span><p>This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9
"Multilingual Tweet Intimacy Analysis". We achieved second-best results in all
10 languages according to the official Pearson's correlation regression
evaluation measure. Our cross-lingual transfer learning approach explores the
benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates
only the regression head parameters and then also updates the pre-trained
transformer encoder parameters at a reduced learning rate. Additionally, we
study the impact of using a small set of automatically generated examples (in
our case, from ChatGPT) for low-resource settings where no human-labeled data
is available. Our study shows that HeFiT stabilizes training and consistently
improves results for pre-trained models that lack domain adaptation to tweets.
Our study also shows a noticeable performance increase in cross-lingual
learning when synthetic data is used, confirming the usefulness of current text
generation systems to improve zero-shot baseline results. Finally, we examine
how possible inconsistencies in the annotated data contribute to cross-lingual
interference issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02468">
<div class="article-summary-box-inner">
<span><p>We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaScore: Story Evaluation with Perturbations. (arXiv:2303.08991v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08991">
<div class="article-summary-box-inner">
<span><p>Various evaluation metrics exist for natural language generation tasks, but
they have limited utility for story generation since they generally do not
correlate well with human judgments and are not designed to evaluate
fine-grained story aspects, such as fluency and relatedness. In this paper, we
propose deltascore, an approach that utilizes perturbation to evaluate
fine-grained story aspects. Our core idea is based on the hypothesis that the
better the story performs in a specific aspect (e.g., fluency), the more it
will be affected by a particular perturbation (e.g., introducing typos). To
measure the impact, we calculate the likelihood difference between the pre- and
post-perturbation stories using large pre-trained language models. We evaluate
deltascore against state-of-the-art model-based and traditional
similarity-based metrics across two story domains, and investigate its
correlation with human judgments on five fine-grained story aspects: fluency,
coherence, relatedness, logicality, and interestingness. The findings of our
study indicate that the deltascore approach exhibits exceptional performance in
evaluating intricate story aspects. An unexpected discovery was made in our
experiment, where a single perturbation method was found to effectively capture
a majority of these aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09314">
<div class="article-summary-box-inner">
<span><p>Multimodal hate detection, which aims to identify harmful content online such
as memes, is crucial for building a wholesome internet environment. Previous
work has made enlightening exploration in detecting explicit hate remarks.
However, most of their approaches neglect the analysis of implicit harm, which
is particularly challenging as explicit text markers and demographic visual
cues are often twisted or missing. The leveraged cross-modal attention
mechanisms also suffer from the distributional modality gap and lack logical
interpretability. To address these semantic gaps issues, we propose TOT: a
topology-aware optimal transport framework to decipher the implicit harm in
memes scenario, which formulates the cross-modal aligning problem as solutions
for optimal transportation plans. Specifically, we leverage an optimal
transport kernel method to capture complementary information from multiple
modalities. The kernel embedding provides a non-linear transformation ability
to reproduce a kernel Hilbert space (RKHS), which reflects significance for
eliminating the distributional modality gap. Moreover, we perceive the topology
information based on aligned representations to conduct bipartite graph path
reasoning. The newly achieved state-of-the-art performance on two publicly
available benchmark datasets, together with further visual analysis,
demonstrate the superiority of TOT in capturing implicit cross-modal alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13099">
<div class="article-summary-box-inner">
<span><p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14334">
<div class="article-summary-box-inner">
<span><p>Scholarly publications are key to the transfer of knowledge from scholars to
others. However, research papers are information-dense, and as the volume of
the scientific literature grows, the need for new technology to support the
reading process grows. In contrast to the process of finding papers, which has
been transformed by Internet technology, the experience of reading research
papers has changed little in decades. The PDF format for sharing research
papers is widely used due to its portability, but it has significant downsides
including: static content, poor accessibility for low-vision readers, and
difficulty reading on mobile devices. This paper explores the question "Can
recent advances in AI and HCI power intelligent, interactive, and accessible
reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader
Project, a collaborative effort across multiple institutions to explore
automatic creation of dynamic reading interfaces for research papers. Through
this project, we've developed ten research prototype interfaces and conducted
usability studies with more than 300 participants and real-world users showing
improved reading experiences for scholars. We've also released a production
reading interface for research papers that will incorporate the best features
as they mature. We structure this paper around challenges scholars and the
public face when reading research papers -- Discovery, Efficiency,
Comprehension, Synthesis, and Accessibility -- and present an overview of our
progress and remaining open challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\varepsilon$ K\'U <MASK>: Integrating Yor\`ub\'a cultural greetings into machine translation. (arXiv:2303.17972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17972">
<div class="article-summary-box-inner">
<span><p>This paper investigates the performance of massively multilingual neural
machine translation (NMT) systems in translating Yor\`ub\'a greetings
($\varepsilon$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and
culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a
Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings,
and sample use cases. We analysed the performance of different multilingual NMT
systems including Google and NLLB and show that these models struggle to
accurately translate Yor\`ub\'a greetings into English. In addition, we trained
a Yor\`ub\'a-English model by finetuning an existing NMT model on the training
split of IkiniYor\`ub\'a and this achieved better performance when compared to
the pre-trained multilingual NMT models, although they were trained on a large
volume of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03518">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to Task 10 at SemEval 2023-Explainable
Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise
in social media platforms has seen an increase in disproportionate levels of
sexism experienced by women on social media platforms. This has made detecting
and explaining online sexist content more important than ever to make social
media safer and more accessible for women. Our approach consists of
experimenting and finetuning BERT-based models and using a Majority Voting
ensemble model that outperforms individual baseline model scores. Our system
achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319
for Task C.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04099">
<div class="article-summary-box-inner">
<span><p>Unsupervised discovery of stories with correlated news articles in real-time
helps people digest massive news streams without expensive human annotations. A
common approach of the existing studies for unsupervised online story discovery
is to represent news articles with symbolic- or graph-based embedding and
incrementally cluster them into stories. Recent large language models are
expected to improve the embedding further, but a straightforward adoption of
the models by indiscriminately encoding all information in articles is
ineffective to deal with text-rich and evolving news streams. In this work, we
propose a novel thematic embedding with an off-the-shelf pretrained sentence
encoder to dynamically represent articles and stories by considering their
shared temporal themes. To realize the idea for unsupervised online story
discovery, a scalable framework USTORY is introduced with two main techniques,
theme- and time-aware dynamic embedding and novelty-aware adaptive clustering,
fueled by lightweight story summaries. A thorough evaluation with real news
data sets demonstrates that USTORY achieves higher story discovery performances
than baselines while being robust and scalable to various streaming settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDFVQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06447">
<div class="article-summary-box-inner">
<span><p>Document-based Visual Question Answering examines the document understanding
of document images in conditions of natural language questions. We proposed a
new document-based VQA dataset, PDF-VQA, to comprehensively examine the
document understanding from various aspects, including document element
recognition, document layout structural understanding as well as contextual
understanding and key information extraction. Our PDF-VQA dataset extends the
current scale of document understanding that limits on the single document page
to the new scale that asks questions over the full document of multiple pages.
We also propose a new graph-based VQA model that explicitly integrates the
spatial and hierarchically structural relationships between different document
elements to boost the document structural understanding. The performances are
compared with several baselines over different question types and
tasks\footnote{The full dataset will be released after paper acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v2 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07619">
<div class="article-summary-box-inner">
<span><p>We examine the potential of ChatGPT, and other large language models, in
predicting stock market returns using sentiment analysis of news headlines. We
use ChatGPT to indicate whether a given headline is good, bad, or irrelevant
news for firms' stock prices. We then compute a numerical score and document a
positive correlation between these ChatGPT scores and subsequent daily stock
market returns. Further, ChatGPT outperforms traditional sentiment analysis
methods. We find that more basic models such as GPT-1, GPT-2, and BERT cannot
accurately forecast returns, indicating return predictability is an emerging
capacity of complex models. Our results suggest that incorporating advanced
language models into the investment decision-making process can yield more
accurate predictions and enhance the performance of quantitative trading
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09548">
<div class="article-summary-box-inner">
<span><p>In populous countries, pending legal cases have been growing exponentially.
There is a need for developing NLP-based techniques for processing and
automatically understanding legal documents. To promote research in the area of
Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at
SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles
Labeling) is about automatically structuring legal documents into semantically
coherent units, Task-B (Legal Named Entity Recognition) deals with identifying
relevant entities in a legal document and Task-C (Court Judgement Prediction
with Explanation) explores the possibility of automatically predicting the
outcome of a legal case along with providing an explanation for the prediction.
In total 26 teams (approx. 100 participants spread across the world) submitted
systems paper. In each of the sub-tasks, the proposed systems outperformed the
baselines; however, there is a lot of scope for improvement. This paper
describes the tasks, and analyzes techniques proposed by various teams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information. (arXiv:2304.09667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09667">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have been successfully applied to various
tasks, they still face challenges with hallucinations and generating erroneous
content. Augmenting LLMs with domain-specific tools such as database utilities
has the potential to facilitate more precise and straightforward access to
specialized knowledge. In this paper, we present GeneGPT, a novel method for
teaching LLMs to use the Web Application Programming Interfaces (APIs) of the
National Center for Biotechnology Information (NCBI) and answer genomics
questions. Specifically, we prompt Codex (code-davinci-002) to solve the
GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations
for in-context learning. During inference, we stop the decoding once a call
request is detected and make the API call with the generated URL. We then
append the raw execution results returned by NCBI APIs to the generated texts
and continue the generation until the answer is found or another API call is
detected. Our preliminary results show that GeneGPT achieves state-of-the-art
results on three out of four one-shot tasks and four out of five zero-shot
tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average
score of 0.76, which is much higher than retrieval-augmented LLMs such as the
New Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as
well as other LLMs such as GPT-3 (0.16) and ChatGPT (0.12).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09960">
<div class="article-summary-box-inner">
<span><p>Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10145">
<div class="article-summary-box-inner">
<span><p>The release of ChatGPT has uncovered a range of possibilities whereby large
language models (LLMs) can substitute human intelligence. In this paper, we
seek to understand whether ChatGPT has the potential to reproduce
human-generated label annotations in social computing tasks. Such an
achievement could significantly reduce the cost and complexity of social
computing research. As such, we use ChatGPT to relabel five seminal datasets
covering stance detection (2x), sentiment analysis, hate speech, and bot
detection. Our results highlight that ChatGPT does have the potential to handle
these data annotation tasks, although a number of challenges remain. ChatGPT
obtains an average accuracy 0.609. Performance is highest for the sentiment
analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we
show that performance varies substantially across individual labels. We believe
this work can open up new lines of analysis and act as a basis for future
research into the exploitation of ChatGPT for human annotation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Program with Natural Language. (arXiv:2304.10464v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10464">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown remarkable performance in various
basic natural language tasks, which raises hopes for achieving Artificial
General Intelligence. To better complete complex tasks, we need LLMs to program
for the task and then follow the program to generate a specific solution for
the test sample. We propose using natural language as a new programming
language to describe task procedures, making them easily understandable to both
humans and LLMs. LLMs are capable of directly generating natural language
programs, but these programs may still contain factual errors or incomplete
steps. Therefore, we further propose the Learning to Program (LP) method to ask
LLMs themselves to learn natural language programs from the training dataset of
complex tasks and then use the learned program to guide inference. Our
experiments on the AMPS (high school math) and Math (competition mathematics
problems) datasets demonstrate the effectiveness of our approach. When testing
ChatGPT on 10 tasks from the AMPS dataset, our LP method's average performance
outperformed the direct zero-shot test performance by 18.3$\%$. We release our
code at \url{https://github.com/microsoft/NaturalLanguageProgram}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10637">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a core natural language processing task in
which pre-trained language models have shown remarkable performance. However,
standard benchmarks like CoNLL 2003 do not address many of the challenges that
deployed NER systems face, such as having to classify emerging or complex
entities in a fine-grained way. In this paper we present a novel NER cascade
approach comprising three steps: first, identifying candidate entities in the
input sentence; second, linking the each candidate to an existing knowledge
base; third, predicting the fine-grained category for each entity candidate. We
empirically demonstrate the significance of external knowledge bases in
accurately classifying fine-grained and emerging entities. Our system exhibits
robust performance in the MultiCoNER2 shared task, even in the low-resource
language setting where we leverage knowledge bases of high-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Time: Transformer-based Article Time Period Prediction. (arXiv:2304.10859v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10859">
<div class="article-summary-box-inner">
<span><p>The task of predicting the publication period of text documents, such as news
articles, is an important but less studied problem in the field of natural
language processing. Predicting the year of a news article can be useful in
various contexts, such as historical research, sentiment analysis, and media
monitoring. In this work, we investigate the problem of predicting the
publication period of a text document, specifically a news article, based on
its textual content. In order to do so, we created our own extensive labeled
dataset of over 350,000 news articles published by The New York Times over six
decades. In our approach, we use a pretrained BERT model fine-tuned for the
task of text classification, specifically for time period prediction.This model
exceeds our expectations and provides some very impressive results in terms of
accurately classifying news articles into their respective publication decades.
The results beat the performance of the baseline model for this relatively
unexplored task of time prediction from text.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-25 23:11:30.913856001 UTC">2023-04-25 23:11:30 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>