<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-18T01:30:00Z">10-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">TestAug: A Framework for Augmenting Capability-based NLP Tests. (arXiv:2210.08097v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08097">
<div class="article-summary-box-inner">
<span><p>The recently proposed capability-based NLP testing allows model developers to
test the functional capabilities of NLP models, revealing functional failures
that cannot be detected by the traditional heldout mechanism. However, existing
work on capability-based testing requires extensive manual efforts and domain
expertise in creating the test cases. In this paper, we investigate a low-cost
approach for the test case generation by leveraging the GPT-3 engine. We
further propose to use a classifier to remove the invalid outputs from GPT-3
and expand the outputs into templates to generate more test cases. Our
experiments show that TestAug has three advantages over the existing work on
behavioral testing: (1) TestAug can find more bugs than existing work; (2) The
test cases in TestAug are more diverse; and (3) TestAug largely saves the
manual efforts in creating the test suites. The code and data for TestAug can
be found at our project website (https://guanqun-yang.github.io/testaug/) and
GitHub (https://github.com/guanqun-yang/testaug).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TweetNERD -- End to End Entity Linking Benchmark for Tweets. (arXiv:2210.08129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08129">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition and Disambiguation (NERD) systems are foundational
for information retrieval, question answering, event detection, and other
natural language processing (NLP) applications. We introduce TweetNERD, a
dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on
Tweets. This is the largest and most temporally diverse open sourced dataset
benchmark for NERD on Tweets and can be used to facilitate research in this
area. We describe evaluation setup with TweetNERD for three NERD tasks: Named
Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End
Entity Linking (End2End); and provide performance of existing publicly
available methods on specific TweetNERD splits. TweetNERD is available at:
https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0
International (CC BY 4.0) license. Check out more details at
https://github.com/twitter-research/TweetNERD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Repetition in Abstractive Neural Summarizers. (arXiv:2210.08145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08145">
<div class="article-summary-box-inner">
<span><p>We provide a quantitative and qualitative analysis of self-repetition in the
output of neural summarizers. We measure self-repetition as the number of
n-grams of length four or longer that appear in multiple outputs of the same
system. We analyze the behavior of three popular architectures (BART, T5, and
Pegasus), fine-tuned on five datasets. In a regression analysis, we find that
the three architectures have different propensities for repeating content
across output summaries for inputs, with BART being particularly prone to
self-repetition. Fine-tuning on more abstractive data, and on data featuring
formulaic language, is associated with a higher rate of self-repetition. In
qualitative analysis we find systems produce artefacts such as ads and
disclaimers unrelated to the content being summarized, as well as formulaic
phrases common in the fine-tuning domain. Our approach to corpus-level analysis
of self-repetition may help practitioners clean up training data for
summarizers and ultimately support methods for minimizing the amount of
self-repetition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Synthetic Speech from SpokenVocab for Speech Translation. (arXiv:2210.08174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08174">
<div class="article-summary-box-inner">
<span><p>Training end-to-end speech translation (ST) systems requires sufficiently
large-scale data, which is unavailable for most language pairs and domains. One
practical solution to the data scarcity issue is to convert machine translation
data (MT) to ST data via text-to-speech (TTS) systems. Yet, using TTS systems
can be tedious and slow, as the conversion needs to be done for each MT
dataset. In this work, we propose a simple, scalable and effective data
augmentation technique, i.e., SpokenVocab, to convert MT data to ST data
on-the-fly. The idea is to retrieve and stitch audio snippets from a
SpokenVocab bank according to words in an MT sequence. Our experiments on
multiple language pairs from Must-C show that this method outperforms strong
baselines by an average of 1.83 BLEU scores, and it performs equally well as
TTS-generated speech. We also showcase how SpokenVocab can be applied in
code-switching ST for which often no TTS systems exit. Our code is available at
https://github.com/mingzi151/SpokenVocab
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation. (arXiv:2210.08182v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08182">
<div class="article-summary-box-inner">
<span><p>Unsupervised representation learning for speech audios attained impressive
performances for speech recognition tasks, particularly when annotated speech
is limited. However, the unsupervised paradigm needs to be carefully designed
and little is known about what properties these representations acquire. There
is no guarantee that the model learns meaningful representations for valuable
information for recognition. Moreover, the adaptation ability of the learned
representations to other domains still needs to be estimated. In this work, we
explore learning domain-invariant representations via a direct mapping of
speech representations to their corresponding high-level linguistic
informations. Results prove that the learned latents not only capture the
articulatory feature of each phoneme but also enhance the adaptation ability,
outperforming the baseline largely on accented benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Word Meaning Disambiguation using TimeLMs. (arXiv:2210.08207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08207">
<div class="article-summary-box-inner">
<span><p>Meaning of words constantly changes given the events in modern civilization.
Large Language Models use word embeddings, which are often static and thus
cannot cope with this semantic change. Thus,it is important to resolve
ambiguity in word meanings. This paper is an effort in this direction, where we
explore methods for word sense disambiguation for the EvoNLP shared task. We
conduct rigorous ablations for two solutions to this problem. We see that an
approach using time-aware language models helps this task. Furthermore, we
explore possible future directions to this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Multi-label Propaganda Detection. (arXiv:2210.08209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08209">
<div class="article-summary-box-inner">
<span><p>The spread of propaganda through the internet has increased drastically over
the past years. Lately, propaganda detection has started gaining importance
because of the negative impact it has on society. In this work, we describe our
approach for the WANLP 2022 shared task which handles the task of propaganda
detection in a multi-label setting. The task demands the model to label the
given text as having one or more types of propaganda techniques. There are a
total of 22 propaganda techniques to be detected. We show that an ensemble of
five models performs the best on the task, scoring a micro-F1 score of 59.73%.
We also conduct comprehensive ablations and propose various future directions
for this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Few-Shot Relation Extraction Pipeline Based on Adaptive Prototype Fusion. (arXiv:2210.08242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08242">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) aims at recognizing unseen relations by
learning with merely a handful of annotated instances. To more effectively
generalize to new relations, this paper proposes a novel pipeline for the FSRE
task based on adaptive prototype fusion. Specifically, for each relation class,
the pipeline fully explores the relation information by concatenating two types
of embedding, and then elaborately combine the relation representation with the
adaptive prototype fusion mechanism. The whole framework can be effectively and
efficiently optimized in an end-to-end fashion. Experiments on the benchmark
dataset FewRel 1.0 show a significant improvement of our method against
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation. (arXiv:2210.08249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08249">
<div class="article-summary-box-inner">
<span><p>Question answering requiring discrete reasoning, e.g., arithmetic computing,
comparison, and counting, over knowledge is a challenging task. In this paper,
we propose UniRPG, a semantic-parsing-based approach advanced in
interpretability and scalability, to perform unified discrete reasoning over
heterogeneous knowledge resources, i.e., table and text, as program generation.
Concretely, UniRPG consists of a neural programmer and a symbolic program
executor, where a program is the composition of a set of pre-defined general
atomic and higher-order operations and arguments extracted from table and text.
First, the programmer parses a question into a program by generating operations
and copying arguments, and then the executor derives answers from table and
text based on the program. To alleviate the costly program annotation issue, we
design a distant supervision approach for programmer learning, where pseudo
programs are automatically constructed without annotated derivations. Extensive
experiments on the TAT-QA dataset show that UniRPG achieves tremendous
improvements and enhances interpretability and scalability compared with
state-of-the-art methods, even without derivation annotation. Moreover, it
achieves promising performance on the textual dataset DROP without derivations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraLegal-BERT: A pretrained language model for Arabic Legal text. (arXiv:2210.08284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08284">
<div class="article-summary-box-inner">
<span><p>The effectiveness of the BERT model on multiple linguistic tasks has been
well documented. On the other hand, its potentials for narrow and specific
domains such as Legal, have not been fully explored. In this paper, we examine
how BERT can be used in the Arabic legal domain and try customizing this
language model for several downstream tasks using several different
domain-relevant training and testing datasets to train BERT from scratch. We
introduce the AraLegal-BERT, a bidirectional encoder Transformer-based model
that have been thoroughly tested and carefully optimized with the goal to
amplify the impact of NLP-driven solution concerning jurisprudence, legal
documents, and legal practice. We fine-tuned AraLegal-BERT and evaluated it
against three BERT variations for Arabic language in three natural languages
understanding (NLU) tasks. The results show that the base version of
AraLegal-BERT achieve better accuracy than the general and original BERT over
the Legal text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08303">
<div class="article-summary-box-inner">
<span><p>The impression is crucial for the referring physicians to grasp key
information since it is concluded from the findings and reasoning of
radiologists. To alleviate the workload of radiologists and reduce repetitive
human labor in impression writing, many researchers have focused on automatic
impression generation. However, recent works on this task mainly summarize the
corresponding findings and pay less attention to the radiology images. In
clinical, radiographs can provide more detailed valuable observations to
enhance radiologists' impression writing, especially for complicated cases.
Besides, each sentence in findings usually focuses on single anatomy, so they
only need to be matched to corresponding anatomical regions instead of the
whole image, which is beneficial for textual and visual features alignment.
Therefore, we propose a novel anatomy-enhanced multimodal model to promote
impression generation. In detail, we first construct a set of rules to extract
anatomies and put these prompts into each sentence to highlight anatomy
characteristics. Then, two separate encoders are applied to extract features
from the radiograph and findings. Afterward, we utilize a contrastive learning
module to align these two representations at the overall level and use a
co-attention to fuse them at the sentence level with the help of
anatomy-enhanced sentence representation. Finally, the decoder takes the fused
information as the input to generate impressions. The experimental results on
two benchmark datasets confirm the effectiveness of the proposed method, which
achieves state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction Repetition Reduces Information Rate in Dialogue. (arXiv:2210.08321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08321">
<div class="article-summary-box-inner">
<span><p>Speakers repeat constructions frequently in dialogue. Due to their peculiar
information-theoretic properties, repetitions can be thought of as a strategy
for cost-effective communication. In this study, we focus on the repetition of
lexicalised constructions -- i.e., recurring multi-word units -- in English
open-domain spoken dialogues. We hypothesise that speakers use construction
repetition to mitigate information rate, leading to an overall decrease in
utterance information content over the course of a dialogue. We conduct a
quantitative analysis, measuring the information content of constructions and
that of their containing utterances, estimating information content with an
adaptive neural language model. We observe that construction usage lowers the
information content of utterances. This facilitating effect (i) increases
throughout dialogues, (ii) is boosted by repetition, (iii) grows as a function
of repetition frequency and density, and (iv) is stronger for repetitions of
referential constructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combination Of Convolution Neural Networks And Deep Neural Networks For Fake News Detection. (arXiv:2210.08331v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08331">
<div class="article-summary-box-inner">
<span><p>Nowadays, People prefer to follow the latest news on social media, as it is
cheap, easily accessible, and quickly disseminated. However, it can spread fake
or unreliable, low-quality news that intentionally contains false information.
The spread of fake news can have a negative effect on people and society. Given
the seriousness of such a problem, researchers did their best to identify
patterns and characteristics that fake news may exhibit to design a system that
can detect fake news before publishing. In this paper, we have described the
Fake News Challenge stage #1 (FNC-1) dataset and given an overview of the
competitive attempts to build a fake news detection system using the FNC-1
dataset. The proposed model was evaluated with the FNC-1 dataset. A competitive
dataset is considered an open problem and a challenge worldwide. This system's
procedure implies processing the text in the headline and body text columns
with different natural language processing techniques. After that, the
extracted features are reduced using the elbow truncated method, finding the
similarity between each pair using the soft cosine similarity method. The new
feature is entered into CNN and DNN deep learning approaches. The proposed
system detects all the categories with high accuracy except the disagree
category. As a result, the system achieves up to 84.6 % accuracy, classifying
it as the second ranking based on other competitive studies regarding this
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Affect Intensities. (arXiv:1704.08798v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.08798">
<div class="article-summary-box-inner">
<span><p>Words often convey affect -- emotions, feelings, and attitudes. Further,
different words can convey affect to various degrees (intensities). However,
existing manually created lexicons for basic emotions (such as anger and fear)
indicate only coarse categories of affect association (for example, associated
with anger or not associated with anger). Automatic lexicons of affect provide
fine degrees of association, but they tend not to be accurate as human-created
lexicons. Here, for the first time, we present a manually created affect
intensity lexicon with real-valued scores of intensity for four basic emotions:
anger, fear, joy, and sadness. (We will subsequently add entries for more
emotions such as disgust, anticipation, trust, and surprise.) We refer to this
dataset as the NRC Affect Intensity Lexicon, or AIL for short. AIL has entries
for close to 6,000 English words. We used a technique called best-worst scaling
(BWS) to create the lexicon. BWS improves annotation consistency and obtains
reliable fine-grained scores (split-half reliability &gt; 0.91). We also compare
the entries in AIL with the entries in the NRC VAD Lexicon, which has valence,
arousal, and dominance (VAD) scores for 20K English words. We find that anger,
fear, and sadness words, on average, have very similar VAD scores. However,
sadness words tend to have slightly lower dominance scores than fear and anger
words. The Affect Intensity Lexicon has applications in automatic emotion
analysis in a number of domains such as commerce, education, intelligence, and
public health. AIL is also useful in the building of natural language
generation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model. (arXiv:2102.11008v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11008">
<div class="article-summary-box-inner">
<span><p>We propose InsNet, an expressive insertion-based text generator with
efficient training and flexible decoding (parallel or sequential). Unlike most
existing insertion-based text generation works that require re-encoding of the
context after each insertion operation and thus are inefficient to train,
InsNet only requires one pass of context encoding for the entire sequence
during training by introducing a novel insertion-oriented position encoding and
a light-weighted slot representation strategy to enable computation sharing.
Furthermore, we propose an algorithm InsNet-Dinic to better determine the
parallelization of insertion operations that provides a controllable switch
between parallel and sequential decoding, making it flexible to handle more
parallelizable tasks such as machine translation with efficient decoding, or
less parallelizable tasks such as open-domain text generation to guarantee
high-quality outputs. Experiments on two lexically constrained text generation
datasets and three machine translation datasets demonstrate InsNet's advantages
over previous insertion-based methods in terms of training speed, inference
efficiency, and generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Picard understanding Darmok: A Dataset and Model for Metaphor-Rich Translation in a Constructed Language. (arXiv:2107.08146v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08146">
<div class="article-summary-box-inner">
<span><p>Tamarian, a fictional language introduced in the Star Trek episode Darmok,
communicates meaning through utterances of metaphorical references, such as
"Darmok and Jalad at Tanagra" instead of "We should work together." This work
assembles a Tamarian-English dictionary of utterances from the original episode
and several follow-on novels, and uses this to construct a parallel corpus of
456 English-Tamarian utterances. A machine translation system based on a large
language model (T5) is trained using this parallel corpus, and is shown to
produce an accuracy of 76% when translating from English to Tamarian on known
utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05781">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) allows transcribing the communications
between air traffic controllers (ATCOs) and aircraft pilots. The transcriptions
are used later to extract ATC named entities, e.g., aircraft callsigns. One
common challenge is speech activity detection (SAD) and speaker diarization
(SD). In the failure condition, two or more segments remain in the same
recording, jeopardizing the overall performance. We propose a system that
combines SAD and a BERT model to perform speaker change detection and speaker
role detection (SRD) by chunking ASR transcripts, i.e., SD with a defined
number of speakers together with SRD. The proposed model is evaluated on
real-life public ATC databases. Our BERT SD model baseline reaches up to 10%
and 20% token-based Jaccard error rate (JER) in public and private ATC
databases. We also achieved relative improvements of 32% and 7.7% in JERs and
SD error rate (DER), respectively, compared to VBx, a well-known SD system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07566">
<div class="article-summary-box-inner">
<span><p>In attempts to develop sample-efficient and interpretable algorithms,
researcher have explored myriad mechanisms for collecting and exploiting
feature feedback (or rationales) auxiliary annotations provided for training
(but not test) instances that highlight salient evidence. Examples include
bounding boxes around objects and salient spans in text. Despite its intuitive
appeal, feature feedback has not delivered significant gains in practical
problems as assessed on iid holdout sets. However, recent works on
counterfactually augmented data suggest an alternative benefit of supplemental
annotations, beyond interpretability: lessening sensitivity to spurious
patterns and consequently delivering gains in out-of-domain evaluations. We
speculate that while existing methods for incorporating feature feedback have
delivered negligible in-sample performance gains, they may nevertheless provide
out-of-domain benefits. Our experiments addressing sentiment analysis, show
that feature feedback methods perform significantly better on various natural
out-of-domain datasets despite comparable in-domain evaluations. By contrast,
performance on natural language inference remains comparable. Finally, we
compare those tasks where feature feedback does (and does not) help.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00701">
<div class="article-summary-box-inner">
<span><p>While there has been substantial progress in text comprehension through
simple factoid question answering, more holistic comprehension of a discourse
still presents a major challenge (Dunietz et al., 2020). Someone critically
reflecting on a text as they read it will pose curiosity-driven, often
open-ended questions, which reflect deep understanding of the content and
require complex reasoning to answer (Ko et al., 2020; Westera et al., 2020). A
key challenge in building and evaluating models for this type of discourse
comprehension is the lack of annotated data, especially since collecting
answers to such questions requires high cognitive load for annotators. This
paper presents a novel paradigm that enables scalable data collection targeting
the comprehension of news documents, viewing these questions through the lens
of discourse. The resulting corpus, DCQA (Discourse Comprehension by Question
Answering), captures both discourse and semantic links between sentences in the
form of free-form, open-ended questions. On an evaluation set that we annotated
on questions from Ko et al. (2020), we show that DCQA provides valuable
supervision for answering open-ended questions. We additionally design
pre-training methods utilizing existing question-answering resources, and use
synthetic data to accommodate unanswerable questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10962">
<div class="article-summary-box-inner">
<span><p>Knowledge-enhanced language representation learning has shown promising
results across various knowledge-intensive NLP tasks. However, prior methods
are limited in efficient utilization of multilingual knowledge graph (KG) data
for language model (LM) pretraining. They often train LMs with KGs in indirect
ways, relying on extra entity/relation embeddings to facilitate knowledge
injection. In this work, we explore methods to make better use of the
multilingual annotation and language agnostic property of KG triples, and
present novel knowledge based multilingual language models (KMLMs) trained
directly on the knowledge triples. We first generate a large amount of
multilingual synthetic sentences using the Wikidata KG triples. Then based on
the intra- and inter-sentence structures of the generated data, we design
pretraining tasks to enable the LMs to not only memorize the factual knowledge
but also learn useful logical patterns. Our pretrained KMLMs demonstrate
significant performance improvements on a wide range of knowledge-intensive
cross-lingual tasks, including named entity recognition (NER), factual
knowledge retrieval, relation classification, and a newly designed logical
reasoning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding. (arXiv:2112.01047v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01047">
<div class="article-summary-box-inner">
<span><p>Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained
models with relation triples injecting from knowledge graphs to improve
language understanding abilities. To guarantee effective knowledge injection,
previous studies integrate models with knowledge encoders for representing
knowledge retrieved from knowledge graphs. The operations for knowledge
retrieval and encoding bring significant computational burdens, restricting the
usage of such models in real-world applications that require high inference
speed. In this paper, we propose a novel KEPLM named DKPLM that Decomposes
Knowledge injection process of the Pre-trained Language Models in pre-training,
fine-tuning and inference stages, which facilitates the applications of KEPLMs
in real-world scenarios. Specifically, we first detect knowledge-aware
long-tail entities as the target for knowledge injection, enhancing the KEPLMs'
semantic understanding abilities and avoiding injecting redundant information.
The embeddings of long-tail entities are replaced by "pseudo token
representations" formed by relevant knowledge triples. We further design the
relational knowledge decoding task for pre-training to force the models to
truly understand the injected knowledge by relation triple reconstruction.
Experiments show that our model outperforms other KEPLMs significantly over
zero-shot knowledge probing tasks and multiple knowledge-aware language
understanding tasks. We further show that DKPLM has a higher inference speed
than other competing models due to the decomposing mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Context-Word Biases in Lexical Semantic Datasets. (arXiv:2112.06733v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06733">
<div class="article-summary-box-inner">
<span><p>State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks
such as WiC and WSD to evaluate their word-in-context representations. This
inherently assumes that performance in these tasks reflect how well a model
represents the coupled word and context semantics. We question this assumption
by presenting the first quantitative analysis on the context-word interaction
being tested in major contextual lexical semantic tasks. To achieve this, we
run probing baselines on masked input, and propose measures to calculate and
visualize the degree of context or word biases in existing datasets. The
analysis was performed on both models and humans. Our findings demonstrate that
models are usually not being tested for word-in-context semantics in the same
way as humans are in these tasks, which helps us better understand the
model-human gap. Specifically, to PCMs, most existing datasets fall into the
extreme ends (the retrieval-based tasks exhibit strong target word bias while
WiC-style tasks and WSD show strong context bias); In comparison, humans are
less biased and achieve much better performance when both word and context are
available than with masked input. We recommend our framework for understanding
and controlling these biases for model interpretation and future task design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilled Dual-Encoder Model for Vision-Language Understanding. (arXiv:2112.08723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08723">
<div class="article-summary-box-inner">
<span><p>We propose a cross-modal attention distillation framework to train a
dual-encoder model for vision-language understanding tasks, such as visual
reasoning and visual question answering. Dual-encoder models have a faster
inference speed than fusion-encoder models and enable the pre-computation of
images and text during inference. However, the shallow interaction module used
in dual-encoder models is insufficient to handle complex vision-language
understanding tasks. In order to learn deep interactions of images and text, we
introduce cross-modal attention distillation, which uses the image-to-text and
text-to-image attention distributions of a fusion-encoder model to guide the
training of our dual-encoder model. In addition, we show that applying the
cross-modal attention distillation for both pre-training and fine-tuning stages
achieves further improvements. Experimental results demonstrate that the
distilled dual-encoder model achieves competitive performance for visual
reasoning, visual entailment and visual question answering tasks while enjoying
a much faster inference speed than fusion-encoder models. Our code and models
will be publicly available at https://github.com/kugwzk/Distilled-DualEncoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05575">
<div class="article-summary-box-inner">
<span><p>Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they struggle to reason rare or emerging unseen entities. In this paper, we
propose kNN-KGE, a new knowledge graph embedding approach with pre-trained
language models, by linearly interpolating its entity distribution with
k-nearest neighbors. We compute the nearest neighbors based on the distance in
the entity embedding space from the knowledge store. Our approach can allow
rare or emerging entities to be memorized explicitly rather than implicitly in
model parameters. Experimental results demonstrate that our approach can
improve inductive and transductive link prediction results and yield better
performance for low-resource settings with only a few triples, which might be
easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CsFEVER and CTKFacts: Acquiring Czech data for fact verification. (arXiv:2201.11115v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11115">
<div class="article-summary-box-inner">
<span><p>In this paper, we examine several methods of acquiring Czech data for
automated fact-checking, which is a task commonly modeled as a classification
of textual claim veracity w.r.t. a corpus of trusted ground truths. We attempt
to collect sets of data in form of a factual claim, evidence within the ground
truth corpus, and its veracity label (supported, refuted or not enough info).
As a first attempt, we generate a Czech version of the large-scale FEVER
dataset built on top of Wikipedia corpus. We take a hybrid approach of machine
translation and document alignment; the approach and the tools we provide can
be easily applied to other languages. We discuss its weaknesses and
inaccuracies, propose a future approach for their cleaning and publish the 127k
resulting translations, as well as a version of such dataset reliably
applicable for the Natural Language Inference task - the CsFEVER-NLI.
Furthermore, we collect a novel dataset of 3,097 claims, which is annotated
using the corpus of 2.2M articles of Czech News Agency. We present its extended
annotation methodology based on the FEVER approach, and, as the underlying
corpus is kept a trade secret, we also publish a standalone version of the
dataset for the task of Natural Language Inference we call CTKFactsNLI. We
analyze both acquired datasets for spurious cues - annotation patterns leading
to model overfitting. CTKFacts is further examined for inter-annotator
agreement, thoroughly cleaned, and a typology of common annotator errors is
extracted. Finally, we provide baseline models for all stages of the
fact-checking pipeline and publish the NLI datasets, as well as our annotation
platform and other experimental data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POTATO: exPlainable infOrmation exTrAcTion framewOrk. (arXiv:2201.13230v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13230">
<div class="article-summary-box-inner">
<span><p>We present POTATO, a task- and languageindependent framework for
human-in-the-loop (HITL) learning of rule-based text classifiers using
graph-based features. POTATO handles any type of directed graph and supports
parsing text into Abstract Meaning Representations (AMR), Universal
Dependencies (UD), and 4lang semantic graphs. A streamlit-based user interface
allows users to build rule systems from graph patterns, provides real-time
evaluation based on ground truth data, and suggests rules by ranking graph
features using interpretable machine learning models. Users can also provide
patterns over graphs using regular expressions, and POTATO can recommend
refinements of such rules. POTATO is applied in projects across domains and
languages, including classification tasks on German legal text and English
social media data. All components of our system are written in Python, can be
installed via pip, and are released under an MIT License on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Typical Sampling. (arXiv:2202.00666v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00666">
<div class="article-summary-box-inner">
<span><p>Today's probabilistic language generators fall short when it comes to
producing coherent and fluent text despite the fact that the underlying models
perform well under standard metrics, e.g., perplexity. This discrepancy has
puzzled the language generation community for the last few years. In this work,
we posit that the abstraction of natural language generation as a discrete
stochastic process--which allows for an information-theoretic analysis--can
provide new insights into the behavior of probabilistic language generators,
e.g., why high-probability texts can be dull or repetitive. Humans use language
as a means of communicating information, aiming to do so in a simultaneously
efficient and error-minimizing manner; in fact, psycholinguistics research
suggests humans choose each word in a string with this subconscious goal in
mind. We formally define the set of strings that meet this criterion: those for
which each word has an information content close to the expected information
content, i.e., the conditional entropy of our model. We then propose a simple
and efficient procedure for enforcing this criterion when generating from
probabilistic models, which we call locally typical sampling. Automatic and
human evaluations show that, in comparison to nucleus and top-k sampling,
locally typical sampling offers competitive performance (in both abstractive
summarization and story generation) in terms of quality while consistently
reducing degenerate repetitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization. (arXiv:2202.05599v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05599">
<div class="article-summary-box-inner">
<span><p>We present ClidSum, a benchmark dataset for building cross-lingual
summarization systems on dialogue documents. It consists of 67k+ dialogue
documents from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated
summaries in different target languages. Based on the proposed ClidSum, we
introduce two benchmark settings for supervised and semi-supervised scenarios,
respectively. We then build various baseline systems in different paradigms
(pipeline and end-to-end) and conduct extensive experiments on ClidSum to
provide deeper analyses. Furthermore, we propose mDialBART which extends
mBART-50 (a multi-lingual BART) via further pre-training. The multiple
objectives used in the further pre-training stage help the pre-trained model
capture the structural characteristics as well as important content in
dialogues and the transformation from source to the target language.
Experimental results show the superiority of mDialBART, as an end-to-end model,
outperforms strong pipeline models on ClidSum. Finally, we discuss specific
challenges that current approaches faced with this task and give multiple
promising directions for future research. We have released the dataset and code
at https://github.com/krystalan/ClidSum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06574">
<div class="article-summary-box-inner">
<span><p>Image Captioning is a popular vision-and-language task to generate the
language description of an image. Recent advances focus on scaling up the model
size and the number of training data, significantly increasing the cost of
training. As an alternative to these heavy-cost models, we introduce I-Tuning,
a lightweight image captioning framework, which contains only a small number of
trainable parameters. The novel I-Tuning cross-attention module connects the
non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT.
Since most parameters are not updated during training, our framework is
lightweight and fast. Experimental results on three image captioning benchmarks
reveal that our framework achieves comparable or better performance than the
large-scale baseline systems. At the same time, our models require up to 10
times fewer trainable parameters and much fewer training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt. (arXiv:2202.11451v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11451">
<div class="article-summary-box-inner">
<span><p>Prompt-based tuning has been proven effective for pretrained language models
(PLMs). While most of the existing work focuses on the monolingual prompts, we
study the multilingual prompts for multilingual PLMs, especially in the
zero-shot cross-lingual setting. To alleviate the effort of designing different
prompts for multiple languages, we propose a novel model that uses a unified
prompt for all languages, called UniPrompt. Different from the discrete prompts
and soft prompts, the unified prompt is model-based and language-agnostic.
Specifically, the unified prompt is initialized by a multilingual PLM to
produce language-independent representation, after which is fused with the text
input. During inference, the prompts can be pre-computed so that no extra
computation cost is needed. To collocate with the unified prompt, we propose a
new initialization method for the target label word to further improve the
model's transferability across languages. Extensive experiments show that our
proposed methods can significantly outperform the strong baselines across
different languages. We release data and code to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these multimodal harms due to
lacking measurement robustness and feature degradation. To address these
challenges, we investigate bias measures and apply ranking metrics for
image-text representations. We then investigate debiasing methods and show that
prepending learned embeddings to text queries that are jointly trained with
adversarial debiasing and a contrastive loss reduces various bias measures with
minimal degradation to the image-text representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16822">
<div class="article-summary-box-inner">
<span><p>Recent work on self-supervised pre-training focus on leveraging large-scale
unlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)
that can be later fine-tuned on downstream tasks e.g., automatic speech
recognition (ASR). Yet, few works investigated the impact on performance when
the data properties substantially differ between the pre-training and
fine-tuning phases, termed domain shift. We target this scenario by analyzing
the robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a
completely unseen domain, air traffic control (ATC) communications. We
benchmark these two models on several open-source and challenging ATC databases
with signal-to-noise ratio between 5 and 20 dB. Relative word error rate (WER)
reductions between 20% to 40% are obtained in comparison to hybrid-based ASR
baselines by only fine-tuning E2E acoustic models with a smaller fraction of
labeled data. We analyze WERs on the low-resource scenario and gender bias
carried by one ATC dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue. (arXiv:2204.04327v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04327">
<div class="article-summary-box-inner">
<span><p>Building universal dialogue systems that operate across multiple domains/APIs
and generalize to new ones with minimal overhead is a critical challenge.
Recent works have leveraged natural language descriptions of schema elements to
enable such systems; however, descriptions only indirectly convey schema
semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq
models with a labeled example dialogue to show the semantics of schema elements
rather than tell the model through descriptions. While requiring similar effort
from service developers as generating descriptions, we show that using short
examples as schema representations with large language models results in
state-of-the-art performance on two popular dialogue state tracking benchmarks
designed to measure zero-shot generalization - the Schema-Guided Dialogue
dataset and the MultiWOZ leave-one-out benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimental Standards for Deep Learning in Natural Language Processing Research. (arXiv:2204.06251v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06251">
<div class="article-summary-box-inner">
<span><p>The field of Deep Learning (DL) has undergone explosive growth during the
last decade, with a substantial impact on Natural Language Processing (NLP) as
well. Yet, compared to more established disciplines, a lack of common
experimental standards remains an open challenge to the field at large.
Starting from fundamental scientific principles, we distill ongoing discussions
on experimental standards in NLP into a single, widely-applicable methodology.
Following these best practices is crucial to strengthen experimental evidence,
improve reproducibility and support scientific progress. These standards are
further collected in a public repository to help them transparently adapt to
future needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08152">
<div class="article-summary-box-inner">
<span><p>Multi-turn dialogue modeling as a challenging branch of natural language
understanding (NLU), aims to build representations for machines to understand
human dialogues, which provides a solid foundation for multiple downstream
tasks. Recent studies of dialogue modeling commonly employ pre-trained language
models (PrLMs) to encode the dialogue history as successive tokens, which is
insufficient in capturing the temporal characteristics of dialogues. Therefore,
we propose Bidirectional Information Decoupling Network (BiDeN) as a universal
dialogue encoder, which explicitly incorporates both the past and future
contexts and can be generalized to a wide range of dialogue-related tasks.
Experimental results on datasets of different downstream tasks demonstrate the
universality and effectiveness of our BiDeN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fast Post-Training Pruning Framework for Transformers. (arXiv:2204.09656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09656">
<div class="article-summary-box-inner">
<span><p>Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining &lt; 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05435">
<div class="article-summary-box-inner">
<span><p>Performance of text classification models tends to drop over time due to
changes in data, which limits the lifetime of a pretrained model. Therefore an
ability to predict a model's ability to persist over time can help design
models that can be effectively used over a longer period of time. In this
paper, we look at this problem from a practical perspective by assessing the
ability of a wide range of language models and classification algorithms to
persist over time, as well as how dataset characteristics can help predict the
temporal stability of different models. We perform longitudinal classification
experiments on three datasets spanning between 6 and 19 years, and involving
diverse tasks and types of data. We find that one can estimate how a model will
retain its performance over time based on (i) how well the model performs over
a restricted time period and its extrapolation to a longer time period, and
(ii) the linguistic characteristics of the dataset, such as the familiarity
score between subsets from different years.Findings from these experiments have
important implications for the design of text classification models with the
aim of preserving performance over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts. (arXiv:2205.10762v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10762">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation systems built on top of Transformer-based
architectures are routinely improving the state-of-the-art in translation
quality according to word-overlap metrics. However, a growing number of studies
also highlight the inherent gender bias that these models incorporate during
training, which reflects poorly in their translations. In this work, we
investigate whether these models can be instructed to fix their bias during
inference using targeted, guided instructions as contexts. By translating
relevant contextual sentences during inference along with the input, we observe
large improvements in reducing the gender bias in translations, across three
popular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric
to assess several large pre-trained models (OPUS-MT, M2M-100) on their
sensitivity towards using contexts during translation to correct their biases.
Our approach requires no fine-tuning and thus can be used easily in production
systems to de-bias translations from stereotypical gender-occupation bias 1. We
hope our method, along with our metric, can be used to build better, bias-free
translation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10852">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
semantic and structural information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the globally semantic information among
sub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm
for knowledge graph representation learning. We apply Relphormer to three
tasks, namely, knowledge graph completion, KG-based question answering and
KG-based recommendation for evaluation. Experimental results show that
Relphormer can obtain better performance on benchmark datasets compared with
baselines. Code is available in https://github.com/zjunlp/Relphormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder. (arXiv:2205.12035v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12035">
<div class="article-summary-box-inner">
<span><p>Despite pre-training's progress in many important NLP tasks, it remains to
explore effective pre-training strategies for dense retrieval. In this paper,
we propose RetroMAE, a new retrieval oriented pre-training paradigm based on
Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs.
1) A novel MAE workflow, where the input sentence is polluted for encoder and
decoder with different masks. The sentence embedding is generated from the
encoder's masked input; then, the original sentence is recovered based on the
sentence embedding and the decoder's masked input via masked language modeling.
2) Asymmetric model structure, with a full-scale BERT like transformer as
encoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios,
with a moderate ratio for encoder: 15~30%, and an aggressive ratio for decoder:
50~70%. Our framework is simple to realize and empirically competitive: the
pre-trained models dramatically improve the SOTA performances on a wide range
of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and
pre-trained models are made publicly available at
https://github.com/staoxiao/RetroMAE so as to inspire more interesting
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12331">
<div class="article-summary-box-inner">
<span><p>Deep learning models have achieved great success in many fields, yet they are
vulnerable to adversarial examples. This paper follows a causal perspective to
look into the adversarial vulnerability and proposes Causal Intervention by
Semantic Smoothing (CISS), a novel framework towards robustness against natural
language attacks. Instead of merely fitting observational data, CISS learns
causal effects p(y|do(x)) by smoothing in the latent semantic space to make
robust predictions, which scales to deep architectures and avoids tedious
construction of noise customized for specific attacks. CISS is provably robust
against word substitution attacks, as well as empirically robust even when
perturbations are strengthened by unknown attack algorithms. For example, on
YELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness
against word substitutions, and achieves 79.4% empirical robustness when
syntactic attacks are integrated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLUTE: Figurative Language Understanding through Textual Explanations. (arXiv:2205.12404v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12404">
<div class="article-summary-box-inner">
<span><p>Figurative language understanding has been recently framed as a recognizing
textual entailment (RTE) task (a.k.a. natural language inference, or NLI).
However, similar to classical RTE/NLI datasets, the current benchmarks suffer
from spurious correlations and annotation artifacts. To tackle this problem,
work on NLI has built explanation-based datasets such as e-SNLI, allowing us to
probe whether language models are right for the right reasons.Yet no such data
exists for figurative language, making it harder to assess genuine
understanding of such expressions. To address this issue, we release FLUTE, a
dataset of 9,000 figurative NLI instances with explanations, spanning four
categories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through
a model-in-the-loop framework based on GPT-3, crowd workers, and expert
annotators. We show how utilizing GPT-3 in conjunction with human annotators
(novices and experts) can aid in scaling up the creation of datasets even for
such complex linguistic phenomena as figurative language. The baseline
performance of the T5 model fine-tuned on FLUTE shows that our dataset can
bring us a step closer to developing models that understand figurative language
through textual explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization. (arXiv:2205.14522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14522">
<div class="article-summary-box-inner">
<span><p>Sentence summarization aims at compressing a long sentence into a short one
that keeps the main gist, and has extensive real-world applications such as
headline generation. In previous work, researchers have developed various
approaches to improve the ROUGE score, which is the main evaluation metric for
summarization, whereas controlling the summary length has not drawn much
attention. In our work, we address a new problem of explicit character-level
length control for summarization, and propose a dynamic programming algorithm
based on the Connectionist Temporal Classification (CTC) model. Results show
that our approach not only achieves higher ROUGE scores but also yields more
complete sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Squeezeformer: An Efficient Transformer for Automatic Speech Recognition. (arXiv:2206.00888v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00888">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer model has become the de facto backbone model
for various downstream speech tasks based on its hybrid attention-convolution
architecture that captures both local and global features. However, through a
series of systematic studies, we find that the Conformer architecture's design
choices are not optimal. After re-examining the design choices for both the
macro and micro-architecture of Conformer, we propose Squeezeformer which
consistently outperforms the state-of-the-art ASR models under the same
training schemes. In particular, for the macro-architecture, Squeezeformer
incorporates (i) the Temporal U-Net structure which reduces the cost of the
multi-head attention modules on long sequences, and (ii) a simpler block
structure of multi-head attention or convolution modules followed up by
feed-forward module instead of the Macaron structure proposed in Conformer.
Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the
activations in the convolutional block, (ii) removes redundant Layer
Normalization operations, and (iii) incorporates an efficient depthwise
down-sampling layer to efficiently sub-sample the input signal. Squeezeformer
achieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate (WER)
on LibriSpeech test-other without external language models, which are 3.1%,
1.4%, and 0.6% better than Conformer-CTC with the same number of FLOPs. Our
code is open-sourced and available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07669">
<div class="article-summary-box-inner">
<span><p>While language tasks are naturally expressed in a single, unified, modeling
framework, i.e., generating sequences of tokens, this has not been the case in
computer vision. As a result, there is a proliferation of distinct
architectures and loss functions for different vision tasks. In this work we
show that a diverse set of "core" computer vision tasks can also be unified if
formulated in terms of a shared pixel-to-sequence interface. We focus on four
tasks, namely, object detection, instance segmentation, keypoint detection, and
image captioning, all with diverse types of outputs, e.g., bounding boxes or
dense masks. Despite that, by formulating the output of each task as a sequence
of discrete tokens with a unified interface, we show that one can train a
neural network with a single model architecture and loss function on all these
tasks, with no task-specific customization. To solve a specific task, we use a
short prompt as task description, and the sequence output adapts to the prompt
so it can produce task-specific output. We show that such a model can achieve
competitive performance compared to well-established task-specific models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation Identification Through a Prompt-based Few-shot Approach. (arXiv:2209.03895v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03895">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our participation in the subtask 1 of CASE-2022,
Event Causality Identification with Casual News Corpus. We address the Causal
Relation Identification (CRI) task by exploiting a set of simple yet
complementary techniques for fine-tuning language models (LMs) on a small
number of annotated examples (i.e., a few-shot configuration). We follow a
prompt-based prediction approach for fine-tuning LMs in which the CRI task is
treated as a masked language modeling problem (MLM). This approach allows LMs
natively pre-trained on MLM problems to directly generate textual responses to
CRI-specific prompts. We compare the performance of this method against
ensemble techniques trained on the entire dataset. Our best-performing
submission was fine-tuned with only 256 instances per class, 15.7% of the all
available data, and yet obtained the second-best precision (0.82), third-best
accuracy (0.82), and an F1-score (0.85) very close to what was reported by the
winner team (0.86).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. (arXiv:2209.09513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09513">
<div class="article-summary-box-inner">
<span><p>When answering a question, humans utilize the information available across
different modalities to synthesize a consistent and complete chain of thought
(CoT). This process is normally a black box in the case of deep learning models
like large-scale language models. Recently, science question benchmarks have
been used to diagnose the multi-hop reasoning ability and interpretability of
an AI system. However, existing datasets fail to provide annotations for the
answers, or are restricted to the textual-only modality, small scales, and
limited domain diversity. To this end, we present Science Question Answering
(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice
questions with a diverse set of science topics and annotations of their answers
with corresponding lectures and explanations. We further design language models
to learn to generate lectures and explanations as the chain of thought (CoT) to
mimic the multi-hop reasoning process when answering ScienceQA questions.
ScienceQA demonstrates the utility of CoT in language models, as CoT improves
the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in
fine-tuned UnifiedQA. We also explore the upper bound for models to leverage
explanations by feeding those in the input; we observe that it improves the
few-shot performance of GPT-3 by 18.96%. Our analysis further shows that
language models, similar to humans, benefit from explanations to learn from
fewer data and achieve the same performance with just 40% of the data. The data
and code are available at https://scienceqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12226">
<div class="article-summary-box-inner">
<span><p>Recent research has revealed undesirable biases in NLP data and models.
However, these efforts focus of social disparities in West, and are not
directly portable to other geo-cultural contexts. In this paper, we focus on
NLP fair-ness in the context of India. We start with a brief account of the
prominent axes of social disparities in India. We build resources for fairness
evaluation in the Indian context and use them to demonstrate prediction biases
along some of the axes. We then delve deeper into social stereotypes for Region
andReligion, demonstrating its prevalence in corpora and models. Finally, we
outline a holistic research agenda to re-contextualize NLP fairness research
for the Indian context, ac-counting for Indian societal context, bridging
technological gaps in NLP capabilities and re-sources, and adapting to Indian
cultural values. While we focus on India, this framework can be generalized to
other geo-cultural contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment is all you need to win US Presidential elections. (arXiv:2209.13487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13487">
<div class="article-summary-box-inner">
<span><p>Election speeches play an integral role in communicating the vision and
mission of the candidates. From lofty promises to mud-slinging, the electoral
candidate accounts for all. However, there remains an open question about what
exactly wins over the voters. In this work, we used state-of-the-art natural
language processing methods to study the speeches and sentiments of the
Republican candidate, Donald Trump, and Democratic candidate, Joe Biden,
fighting for the 2020 US Presidential election. Comparing the racial dichotomy
of the United States, we analyze what led to the victory and defeat of the
different candidates. We believe this work will inform the election campaigning
strategy and provide a basis for communicating to diverse crowds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer. (arXiv:2209.14008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14008">
<div class="article-summary-box-inner">
<span><p>The paper explores the relevance of the Text-To-Text Transfer Transformer
language model (T5) for Polish (plT5) to the task of intrinsic and extrinsic
keyword extraction from short text passages. The evaluation is carried out on
the new Polish Open Science Metadata Corpus (POSMAC), which is released with
this paper: a collection of 216,214 abstracts of scientific publications
compiled in the CURLICAT project. We compare the results obtained by four
different methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that
the plT5kw model yields particularly promising results for both frequent and
sparsely represented keywords. Furthermore, a plT5kw keyword generation model
trained on the POSMAC also seems to produce highly useful results in
cross-domain text labelling scenarios. We discuss the performance of the model
on news stories and phone-based dialog transcripts which represent text genres
and domains extrinsic to the dataset of scientific abstracts. Finally, we also
attempt to characterize the challenges of evaluating a text-to-text model on
both intrinsic and extrinsic keyword extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Communication: Generalization and Overfitting in Lewis Games. (arXiv:2209.15342v2 [cs.MA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15342">
<div class="article-summary-box-inner">
<span><p>Lewis signaling games are a class of simple communication games for
simulating the emergence of language. In these games, two agents must agree on
a communication protocol in order to solve a cooperative task. Previous work
has shown that agents trained to play this game with reinforcement learning
tend to develop languages that display undesirable properties from a linguistic
point of view (lack of generalization, lack of compositionality, etc). In this
paper, we aim to provide better understanding of this phenomenon by
analytically studying the learning problem in Lewis games. As a core
contribution, we demonstrate that the standard objective in Lewis games can be
decomposed in two components: a co-adaptation loss and an information loss.
This decomposition enables us to surface two potential sources of overfitting,
which we show may undermine the emergence of a structured communication
protocol. In particular, when we control for overfitting on the co-adaptation
loss, we recover desired properties in the emergent languages: they are more
compositional and generalize better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement. (arXiv:2210.03501v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03501">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a linguistic phenomenon indicating a discrepancy between literal
meanings and implied intentions. Due to its sophisticated nature, it is usually
challenging to be detected from the text itself. As a result, multi-modal
sarcasm detection has received more attention in both academia and industries.
However, most existing techniques only modeled the atomic-level inconsistencies
between the text input and its accompanying image, ignoring more complex
compositions for both modalities. Moreover, they neglected the rich information
contained in external knowledge, e.g., image captions. In this paper, we
propose a novel hierarchical framework for sarcasm detection by exploring both
the atomic-level congruity based on multi-head cross attention mechanism and
the composition-level congruity based on graph neural networks, where a post
with low congruity can be identified as sarcasm. In addition, we exploit the
effect of various knowledge resources for sarcasm detection. Evaluation results
on a public multi-modal sarcasm detection dataset based on Twitter demonstrate
the superiority of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03923">
<div class="article-summary-box-inner">
<span><p>Recent advances in distilling pretrained language models have discovered
that, besides the expressiveness of knowledge, the student-friendliness should
be taken into consideration to realize a truly knowledgable teacher. Based on a
pilot study, we find that over-parameterized teachers can produce expressive
yet student-unfriendly knowledge and are thus limited in overall
knowledgableness. To remove the parameters that result in
student-unfriendliness, we propose a sparse teacher trick under the guidance of
an overall knowledgable score for each teacher parameter. The knowledgable
score is essentially an interpolation of the expressiveness and
student-friendliness scores. The aim is to ensure that the expressive
parameters are retained while the student-unfriendly ones are removed.
Extensive experiments on the GLUE benchmark show that the proposed sparse
teachers can be dense with knowledge and lead to students with compelling
performance in comparison with a series of competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection. (arXiv:2210.04267v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04267">
<div class="article-summary-box-inner">
<span><p>Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04982">
<div class="article-summary-box-inner">
<span><p>Free-text rationales are a promising step towards explainable AI, yet their
evaluation remains an open research problem. While existing metrics have mostly
focused on measuring the direct association between the rationale and a given
label, we argue that an ideal metric should also be able to focus on the new
information uniquely provided in the rationale that is otherwise not provided
in the input or the label. We investigate this research problem from an
information-theoretic perspective using the conditional V-information. More
concretely, we propose a metric called REV (Rationale Evaluation with
conditional V-information), that can quantify the new information in a
rationale supporting a given label beyond the information already available in
the input or the label. Experiments on reasoning tasks across four benchmarks,
including few-shot prompting with GPT-3, demonstrate the effectiveness of REV
in evaluating different types of rationale-label pairs, compared to existing
metrics. Through several quantitative comparisons, we demonstrate the
capability of REV in providing more sensitive measurements of new information
in free-text rationales with respect to a label. Furthermore, REV is consistent
with human judgments on rationale evaluations. Overall, when used alongside
traditional performance metrics, REV provides deeper insights into a models'
reasoning and prediction processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Language Maps for Robot Navigation. (arXiv:2210.05714v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05714">
<div class="article-summary-box-inner">
<span><p>Grounding language to the visual observations of a navigating agent can be
performed using off-the-shelf visual-language models pretrained on
Internet-scale data (e.g., image captions). While this is useful for matching
images to natural language descriptions of object goals, it remains disjoint
from the process of mapping the environment, so that it lacks the spatial
precision of classic geometric maps. To address this problem, we propose
VLMaps, a spatial map representation that directly fuses pretrained
visual-language features with a 3D reconstruction of the physical world. VLMaps
can be autonomously built from video feed on robots using standard exploration
approaches and enables natural language indexing of the map without additional
labeled data. Specifically, when combined with large language models (LLMs),
VLMaps can be used to (i) translate natural language commands into a sequence
of open-vocabulary navigation goals (which, beyond prior work, can be spatial
by construction, e.g., "in between the sofa and TV" or "three meters to the
right of the chair") directly localized in the map, and (ii) can be shared
among multiple robots with different embodiments to generate new obstacle maps
on-the-fly (by using a list of obstacle categories). Extensive experiments
carried out in simulated and real world environments show that VLMaps enable
navigation according to more complex language instructions than existing
methods. Videos are available at https://vlmaps.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05916">
<div class="article-summary-box-inner">
<span><p>Hateful memes are a growing menace on social media. While the image and its
corresponding text in a meme are related, they do not necessarily convey the
same meaning when viewed individually. Hence, detecting hateful memes requires
careful consideration of both visual and textual information. Multimodal
pre-training can be beneficial for this task because it effectively captures
the relationship between the image and the text by representing them in a
similar feature space. Furthermore, it is essential to model the interactions
between the image and text features through intermediate fusion. Most existing
methods either employ multimodal pre-training or intermediate fusion, but not
both. In this work, we propose the Hate-CLIPper architecture, which explicitly
models the cross-modal interactions between the image and text representations
obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a
feature interaction matrix (FIM). A simple classifier based on the FIM
representation is able to achieve state-of-the-art performance on the Hateful
Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the
human performance of 82.65. Experiments on other meme datasets such as
Propaganda Memes and TamilMemes also demonstrate the generalizability of the
proposed approach. Finally, we analyze the interpretability of the FIM
representation and show that cross-modal interactions can indeed facilitate the
learning of meaningful concepts. The code for this work is available at
https://github.com/gokulkarthik/hateclipper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06346">
<div class="article-summary-box-inner">
<span><p>The number of clinical citations received from clinical guidelines or
clinical trials has been considered as one of the most appropriate indicators
for quantifying the clinical impact of biomedical papers. Therefore, the early
prediction of the clinical citation count of biomedical papers is critical to
scientific activities in biomedicine, such as research evaluation, resource
allocation, and clinical translation. In this study, we designed a four-layer
multilayer perceptron neural network (MPNN) model to predict the clinical
citation count of biomedical papers in the future by using 9,822,620 biomedical
papers published from 1985 to 2005. We extracted ninety-one paper features from
three dimensions as the input of the model, including twenty-one features in
the paper dimension, thirty-five in the reference dimension, and thirty-five in
the citing paper dimension. In each dimension, the features can be classified
into three categories, i.e., the citation-related features, the clinical
translation-related features, and the topic-related features. Besides, in the
paper dimension, we also considered the features that have previously been
demonstrated to be related to the citation counts of research papers. The
results showed that the proposed MPNN model outperformed the other five
baseline models, and the features in the reference dimension were the most
important.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06694">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06774">
<div class="article-summary-box-inner">
<span><p>We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Ambiguity, Grammaticality and Complexity Probes. (arXiv:2210.06928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06928">
<div class="article-summary-box-inner">
<span><p>It is unclear whether, how and where large pre-trained language models
capture subtle linguistic traits like ambiguity, grammaticality and sentence
complexity. We present results of automatic classification of these traits and
compare their viability and patterns across representation types. We
demonstrate that template-based datasets with surface-level artifacts should
not be used for probing, careful comparisons with baselines should be done and
that t-SNE plots should not be used to determine the presence of a feature
among dense vectors representations. We also show how features might be highly
localized in the layers for these models and get lost in the upper layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition. (arXiv:2210.07032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07032">
<div class="article-summary-box-inner">
<span><p>Due to the absence of connectives, implicit discourse relation recognition
(IDRR) is still a challenging and crucial task in discourse analysis. Most of
the current work adopted multi-task learning to aid IDRR through explicit
discourse relation recognition (EDRR) or utilized dependencies between
discourse relation labels to constrain model predictions. But these methods
still performed poorly on fine-grained IDRR and even utterly misidentified on
most of the few-shot discourse relation classes. To address these problems, we
propose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our
method instructs large-scale pre-trained models to use knowledge relevant to
discourse relation and utilizes the strong correlation between connectives and
discourse relation to help the model recognize implicit discourse relations.
Experimental results show that our method surpasses the current
state-of-the-art model and achieves significant improvements on those
fine-grained few-shot discourse relation. Moreover, our approach is able to be
transferred to EDRR and obtain acceptable results. Our code is released in
https://github.com/zh-i9/PCP-for-IDRR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of "Subjectivity" and "Identity Terms". (arXiv:2109.02691v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
<div class="article-summary-box-inner">
<span><p>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
"Muslim" and "black". Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-18 23:21:37.575644742 UTC">2022-10-18 23:21:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>