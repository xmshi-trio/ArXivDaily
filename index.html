<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-23T01:30:00Z">11-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts. (arXiv:2211.11772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11772">
<div class="article-summary-box-inner">
<span><p>Classroom discourse is a core medium of instruction -- analyzing it can
provide a window into teaching and learning as well as driving the development
of new tools for improving instruction. We introduce the largest dataset of
mathematics classroom transcripts available to researchers, and demonstrate how
this data can help improve instruction. The dataset consists of 1,660 45-60
minute long 4th and 5th grade elementary mathematics observations collected by
the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The
anonymized transcripts represent data from 317 teachers across 4 school
districts that serve largely historically marginalized students. The
transcripts come with rich metadata, including turn-level annotations for
dialogic discourse moves, classroom observation scores, demographic
information, survey responses and student test scores. We demonstrate that our
natural language processing model, trained on our turn-level annotations, can
learn to identify dialogic discourse moves and these moves are correlated with
better classroom observation scores and learning outcomes. This dataset opens
up several possibilities for researchers, educators and policymakers to learn
about and improve K-12 instruction. The data and its terms of use can be
accessed here: https://github.com/ddemszky/classroom-transcript-analysis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can You Label Less by Using Out-of-Domain Data? Active & Transfer Learning with Few-shot Instructions. (arXiv:2211.11798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11798">
<div class="article-summary-box-inner">
<span><p>Labeling social-media data for custom dimensions of toxicity and social bias
is challenging and labor-intensive. Existing transfer and active learning
approaches meant to reduce annotation effort require fine-tuning, which suffers
from over-fitting to noise and can cause domain shift with small sample sizes.
In this work, we propose a novel Active Transfer Few-shot Instructions (ATF)
approach which requires no fine-tuning. ATF leverages the internal linguistic
knowledge of pre-trained language models (PLMs) to facilitate the transfer of
information from existing pre-labeled datasets (source-domain task) with
minimum labeling effort on unlabeled target data (target-domain task). Our
strategy can yield positive transfer achieving a mean AUC gain of 10.5%
compared to no transfer with a large 22b parameter PLM. We further show that
annotation of just a few target-domain samples via active learning can be
beneficial for transfer, but the impact diminishes with more annotation effort
(26% drop in gain between 100 and 2000 annotated examples). Finally, we find
that not all transfer scenarios yield a positive gain, which seems related to
the PLMs initial performance on the target-domain task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised extraction, labelling and clustering of segments from clinical notes. (arXiv:2211.11799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11799">
<div class="article-summary-box-inner">
<span><p>This work is motivated by the scarcity of tools for accurate, unsupervised
information extraction from unstructured clinical notes in computationally
underrepresented languages, such as Czech. We introduce a stepping stone to a
broad array of downstream tasks such as summarisation or integration of
individual patient records, extraction of structured information for national
cancer registry reporting or building of semi-structured semantic patient
representations for computing patient embeddings. More specifically, we present
a method for unsupervised extraction of semantically-labelled textual segments
from clinical notes and test it out on a dataset of Czech breast cancer
patients, provided by Masaryk Memorial Cancer Institute (the largest Czech
hospital specialising in oncology). Our goal was to extract, classify (i.e.
label) and cluster segments of the free-text notes that correspond to specific
clinical features (e.g., family background, comorbidities or toxicities). The
presented results demonstrate the practical relevance of the proposed approach
for building more sophisticated extraction and analytical pipelines deployed on
Czech clinical notes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference. (arXiv:2211.11875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11875">
<div class="article-summary-box-inner">
<span><p>While large pre-trained language models are powerful, their predictions often
lack logical consistency across test inputs. For example, a state-of-the-art
Macaw question-answering (QA) model answers 'Yes' to 'Is a sparrow a bird?' and
'Does a bird have feet?' but answers 'No' to 'Does a sparrow have feet?'. To
address this failure mode, we propose a framework, Consistency Correction
through Relation Detection, or ConCoRD, for boosting the consistency and
accuracy of pre-trained NLP models using pre-trained natural language inference
(NLI) models without fine-tuning or re-training. Given a batch of test inputs,
ConCoRD samples several candidate outputs for each input and instantiates a
factor graph that accounts for both the model's belief about the likelihood of
each answer choice in isolation and the NLI model's beliefs about pair-wise
answer choice compatibility. We show that a weighted MaxSAT solver can
efficiently compute high-quality answer choices under this factor graph,
improving over the raw model's predictions. Our experiments demonstrate that
ConCoRD consistently boosts accuracy and consistency of off-the-shelf
closed-book QA and VQA models using off-the-shelf NLI models, notably
increasing accuracy of LXMERT on ConVQA by 5% absolute. See
https://ericmitchell.ai/emnlp-2022-concord/ for code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEMPERA: Test-Time Prompting via Reinforcement Learning. (arXiv:2211.11890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11890">
<div class="article-summary-box-inner">
<span><p>Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Knowledge Dependency of Questions. (arXiv:2211.11902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11902">
<div class="article-summary-box-inner">
<span><p>The automatic generation of Multiple Choice Questions (MCQ) has the potential
to reduce the time educators spend on student assessment significantly.
However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,
and METEOR, focus on the n-gram based similarity of the generated MCQ to the
gold sample in the dataset and disregard their educational value. They fail to
evaluate the MCQ's ability to assess the student's knowledge of the
corresponding target fact. To tackle this issue, we propose a novel automatic
evaluation metric, coined Knowledge Dependent Answerability (KDA), which
measures the MCQ's answerability given knowledge of the target fact.
Specifically, we first show how to measure KDA based on student responses from
a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and
KDA_cont, that approximate KDA by leveraging pre-trained language models to
imitate students' problem-solving behavior. Through our human studies, we show
that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)
usability in an actual classroom setting, labeled by experts. Furthermore, when
combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown
to have a strong predictive power for various expert-labeled MCQ quality
measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best-$k$ Search Algorithm for Neural Text Generation. (arXiv:2211.11924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11924">
<div class="article-summary-box-inner">
<span><p>Modern natural language generation paradigms require a good decoding strategy
to obtain quality sequences out of the model. Beam search yields high-quality
but low diversity outputs; stochastic approaches suffer from high variance and
sometimes low quality, but the outputs tend to be more natural and creative. In
this work, we propose a deterministic search algorithm balancing both quality
and diversity. We first investigate the vanilla best-first search (BFS)
algorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we
greedily expand the top $k$ nodes, instead of only the first node, to boost
efficiency and diversity. Upweighting recently discovered nodes accompanied by
heap pruning ensures the completeness of the search procedure. Experiments on
four NLG tasks, including question generation, commonsense generation, text
summarization, and translation, show that best-$k$ search yields more diverse
and natural outputs compared to strong baselines, while our approach maintains
high text quality. The proposed algorithm is parameter-free, lightweight,
efficient, and easy to use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Backdoor Attack and Defense in Natural Language Processing. (arXiv:2211.11958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11958">
<div class="article-summary-box-inner">
<span><p>Deep learning is becoming increasingly popular in real-life applications,
especially in natural language processing (NLP). Users often choose training
outsourcing or adopt third-party data and models due to data and computation
resources being limited. In such a situation, training data and models are
exposed to the public. As a result, attackers can manipulate the training
process to inject some triggers into the model, which is called backdoor
attack. Backdoor attack is quite stealthy and difficult to be detected because
it has little inferior influence on the model's performance for the clean
samples. To get a precise grasp and understanding of this problem, in this
paper, we conduct a comprehensive review of backdoor attacks and defenses in
the field of NLP. Besides, we summarize benchmark datasets and point out the
open issues to design credible systems to defend against backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems. (arXiv:2211.11982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11982">
<div class="article-summary-box-inner">
<span><p>We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for
commercial text-based task-oriented dialog (TOD) systems. BotSIM consists of
three major components: 1) a Generator that can infer semantic-level dialog
acts and entities from bot definitions and generate user queries via
model-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to
simulate conversations with the dialog agents; 3) a Remediator to analyze the
simulated conversations, visualize the bot health reports and provide
actionable remediation suggestions for bot troubleshooting and improvement. We
demonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and
multi-intent dialog generation via case studies on two commercial bot
platforms. BotSIM's "generation-simulation-remediation" paradigm accelerates
the end-to-end bot evaluation and iteration process by: 1) reducing manual test
cases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU
and end-to-end performance via extensive dialog simulation; 3) improving the
bot troubleshooting process with actionable suggestions. A demo of our system
can be found at https://tinyurl.com/mryu74cd and a demo video at
https://youtu.be/qLi5iSoly30.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English. (arXiv:2211.12000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12000">
<div class="article-summary-box-inner">
<span><p>We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -
English Speech Translation Corpus. This corpus is an extension of the ArzEn
speech corpus, which was collected through informal interviews with bilingual
speakers. In this work, we collect translations in both directions, monolingual
Egyptian Arabic and monolingual English, forming a three-way speech translation
corpus. We make the translation guidelines and corpus publicly available. We
also report results for baseline systems for machine translation and speech
translation tasks. We believe this is a valuable resource that can motivate and
facilitate further research studying the code-switching phenomenon from a
linguistic perspective and can be used to train and evaluate NLP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12054">
<div class="article-summary-box-inner">
<span><p>Large-scale commonsense knowledge bases empower a broad range of AI
applications, where the automatic extraction of commonsense knowledge (CKE) is
a fundamental and challenging problem. CKE from text is known for suffering
from the inherent sparsity and reporting bias of commonsense in text. Visual
perception, on the other hand, contains rich commonsense knowledge about
real-world entities, e.g., (person, can_hold, bottle), which can serve as
promising sources for acquiring grounded commonsense knowledge. In this work,
we present CLEVER, which formulates CKE as a distantly supervised
multi-instance learning problem, where models learn to summarize commonsense
relations from a bag of images about an entity pair without any human
annotation on image instances. To address the problem, CLEVER leverages
vision-language pre-training models for deep understanding of each image in the
bag, and selects informative instances from the bag to summarize commonsense
entity relations via a novel contrastive attention mechanism. Comprehensive
experimental results in held-out and human evaluation show that CLEVER can
extract commonsense knowledge in promising quality, outperforming pre-trained
language model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted
commonsense scores show strong correlation with human judgment with a 0.78
Spearman coefficient. Moreover, the extracted commonsense can also be grounded
into images with reasonable interpretability. The data and codes can be
obtained at https://github.com/thunlp/CLEVER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models. (arXiv:2211.12092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12092">
<div class="article-summary-box-inner">
<span><p>The simplest way to obtain continuous interpolation between two points in
high dimensional space is to draw a line between them. While previous works
focused on the general connectivity between model parameters, we explored
linear interpolation for parameters of pre-trained models after fine-tuning.
Surprisingly, we could perform linear interpolation without a performance drop
in intermediate points for fine-tuned models. For controllable text generation,
such interpolation could be seen as moving a model towards or against the
desired text attribute (e.g., positive sentiment), which could be used as
grounds for further methods for controllable text generation without inference
speed overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk. (arXiv:2211.12118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12118">
<div class="article-summary-box-inner">
<span><p>One of the challenges of developing a summarization model arises from the
difficulty in measuring the factual inconsistency of the generated text. In
this study, we reinterpret the decoder overconfidence-regularizing objective
suggested in (Miao et al., 2021) as a hallucination risk measurement to better
estimate the quality of generated summaries. We propose a reference-free
metric, HaRiM+, which only requires an off-the-shelf summarization model to
compute the hallucination risk based on token likelihoods. Deploying it
requires no additional training of models or ad-hoc modules, which usually need
alignment to human judgments. For summary-quality estimation, HaRiM+ records
state-of-the-art correlation to human judgment on three summary-quality
annotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits
the use of summarization models, facilitates the progress of both automated
evaluation and generation of summary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Dataset for Biomedical Keyphrase Generation. (arXiv:2211.12124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12124">
<div class="article-summary-box-inner">
<span><p>Keyphrase generation is the task consisting in generating a set of words or
phrases that highlight the main topics of a document. There are few datasets
for keyphrase generation in the biomedical domain and they do not meet the
expectations in terms of size for training generative models. In this paper, we
introduce kp-biomed, the first large-scale biomedical keyphrase generation
dataset with more than 5M documents collected from PubMed abstracts. We train
and release several generative models and conduct a series of experiments
showing that using large scale datasets improves significantly the performances
for present and absent keyphrase generation. The dataset is available under
CC-BY-NC v4.0 license at https://huggingface.co/ datasets/taln-ls2n/kpbiomed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12130">
<div class="article-summary-box-inner">
<span><p>Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coreference Resolution through a seq2seq Transition-Based System. (arXiv:2211.12142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12142">
<div class="article-summary-box-inner">
<span><p>Most recent coreference resolution systems use search algorithms over
possible spans to identify mentions and resolve coreference. We instead present
a coreference resolution system that uses a text-to-text (seq2seq) paradigm to
predict mentions and links jointly. We implement the coreference system as a
transition system and use multilingual T5 as an underlying language model. We
obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score
for English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))
using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than
previous work) and 74.3 F1-score for Chinese (+5.3). In addition we use the
SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot
setting, and supervised setting using all available training data. We get
substantially higher zero-shot F1-scores for 3 out of 4 languages than previous
approaches and significantly exceed previous supervised state-of-the-art
results for all five tested languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022. (arXiv:2211.12154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12154">
<div class="article-summary-box-inner">
<span><p>The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PESE: Event Structure Extraction using Pointer Network based Encoder-Decoder Architecture. (arXiv:2211.12157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12157">
<div class="article-summary-box-inner">
<span><p>The task of event extraction (EE) aims to find the events and event-related
argument information from the text and represent them in a structured format.
Most previous works try to solve the problem by separately identifying multiple
substructures and aggregating them to get the complete event structure. The
problem with the methods is that it fails to identify all the interdependencies
among the event participants (event-triggers, arguments, and roles). In this
paper, we represent each event record in a unique tuple format that contains
trigger phrase, trigger type, argument phrase, and corresponding role
information. Our proposed pointer network-based encoder-decoder model generates
an event tuple in each time step by exploiting the interactions among event
participants and presenting a truly end-to-end solution to the EE task. We
evaluate our model on the ACE2005 dataset, and experimental results demonstrate
the effectiveness of our model by achieving competitive performance compared to
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type. (arXiv:2211.12164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12164">
<div class="article-summary-box-inner">
<span><p>Machine generation of Arithmetic Word Problems (AWPs) is challenging as they
express quantities and mathematical relationships and need to be consistent.
ML-solvers require a large annotated training set of consistent problems with
language variations. Exploiting domain-knowledge is needed for consistency
checking whereas LSTM-based approaches are good for producing text with
language variations. Combining these we propose a system, OLGA, to generate
consistent word problems of TC (Transfer-Case) type, involving object transfers
among agents. Though we provide a dataset of consistent 2-agent TC-problems for
training, only about 36% of the outputs of an LSTM-based generator are found
consistent. We use an extension of TC-Ontology, proposed by us previously, to
determine the consistency of problems. Among the remaining 64%, about 40% have
minor errors which we repair using the same ontology. To check consistency and
for the repair process, we construct an instance-specific representation (ABox)
of an auto-generated problem. We use a sentence classifier and BERT models for
this task. The training set for these LMs is problem-texts where sentence-parts
are annotated with ontology class-names. As three-agent problems are longer,
the percentage of consistent problems generated by an LSTM-based approach drops
further. Hence, we propose an ontology-based method that extends consistent
2-agent problems into consistent 3-agent problems. Overall, our approach
generates a large number of consistent TC-type AWPs involving 2 or 3 agents. As
ABox has all the information of a problem, any annotations can also be
generated. Adopting the proposed approach to generate other types of AWPs is
interesting future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptTTS: Controllable Text-to-Speech with Text Descriptions. (arXiv:2211.12171v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12171">
<div class="article-summary-box-inner">
<span><p>Using a text description as prompt to guide the generation of text or images
(e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and
image generation, in this work, we explore the possibility of utilizing text
descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS)
system (dubbed as PromptTTS) that takes a prompt with both style and content
descriptions as input to synthesize the corresponding speech. Specifically,
PromptTTS consists of a style encoder and a content encoder to extract the
corresponding representations from the prompt, and a speech decoder to
synthesize speech according to the extracted style and content representations.
Compared with previous works in controllable TTS that require users to have
acoustic knowledge to understand style factors such as prosody and pitch,
PromptTTS is more user-friendly since text descriptions are a more natural way
to express speech style (e.g., ''A lady whispers to her friend slowly''). Given
that there is no TTS dataset with prompts, to benchmark the task of PromptTTS,
we construct and release a dataset containing prompts with style and content
information and the corresponding speech. Experiments show that PromptTTS can
generate speech with precise style control and high speech quality. Audio
samples and our dataset are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding. (arXiv:2211.12220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12220">
<div class="article-summary-box-inner">
<span><p>Multi-Intent Spoken Language Understanding (SLU), a novel and more complex
scenario of SLU, is attracting increasing attention. Unlike traditional SLU,
each intent in this scenario has its specific scope. Semantic information
outside the scope even hinders the prediction, which tremendously increases the
difficulty of intent detection. More seriously, guiding slot filling with these
inaccurate intent labels suffers error propagation problems, resulting in
unsatisfied overall performance. To solve these challenges, in this paper, we
propose a novel Scope-Sensitive Result Attention Network (SSRAN) based on
Transformer, which contains a Scope Recognizer (SR) and a Result Attention
Network (RAN). Scope Recognizer assignments scope information to each token,
reducing the distraction of out-of-scope tokens. Result Attention Network
effectively utilizes the bidirectional interaction between results of slot
filling and intent detection, mitigating the error propagation problem.
Experiments on two public datasets indicate that our model significantly
improves SLU performance (5.4\% and 2.1\% on Overall accuracy) over the
state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-dependent Contrastive Learning with Cluster Sampling for Inductive Relation Prediction. (arXiv:2211.12266v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12266">
<div class="article-summary-box-inner">
<span><p>Relation prediction is a task designed for knowledge graph completion which
aims to predict missing relationships between entities. Recent subgraph-based
models for inductive relation prediction have received increasing attention,
which can predict relation for unseen entities based on the extracted subgraph
surrounding the candidate triplet. However, they are not completely inductive
because of their disability of predicting unseen relations. Moreover, they fail
to pay sufficient attention to the role of relation as they only depend on the
model to learn parameterized relation embedding, which leads to inaccurate
prediction on long-tail relations. In this paper, we introduce
Relation-dependent Contrastive Learning (ReCoLe) for inductive relation
prediction, which adapts contrastive learning with a novel sampling method
based on clustering algorithm to enhance the role of relation and improve the
generalization ability to unseen relations. Instead of directly learning
embedding for relations, ReCoLe allocates a pre-trained GNN-based encoder to
each relation to strengthen the influence of relation. The GNN-based encoder is
optimized by contrastive learning, which ensures satisfactory performance on
long-tail relations. In addition, the cluster sampling method equips ReCoLe
with the ability to handle both unseen relations and entities. Experimental
results suggest that ReCoLe outperforms state-of-the-art methods on commonly
used inductive datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12316">
<div class="article-summary-box-inner">
<span><p>Despite the widespread success of Transformers on NLP tasks, recent works
have found that they struggle to model several formal languages when compared
to recurrent models. This raises the question of why Transformers perform well
in practice and whether they have any properties that enable them to generalize
better than recurrent models. In this work, we conduct an extensive empirical
study on Boolean functions to demonstrate the following: (i) Random
Transformers are relatively more biased towards functions of low sensitivity.
(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize
learning functions of low sensitivity, with Transformers ultimately converging
to functions of lower sensitivity. (iii) On sparse Boolean functions which have
low sensitivity, we find that Transformers generalize near perfectly even in
the presence of noisy labels whereas LSTMs overfit and achieve poor
generalization accuracy. Overall, our results provide strong quantifiable
evidence that suggests differences in the inductive biases of Transformers and
recurrent models which may help explain Transformer's effective generalization
performance despite relatively limited expressiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GDPR Compliant Collection of Therapist-Patient-Dialogues. (arXiv:2211.12360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12360">
<div class="article-summary-box-inner">
<span><p>According to the Global Burden of Disease list provided by the World Health
Organization (WHO), mental disorders are among the most debilitating
disorders.To improve the diagnosis and the therapy effectiveness in recent
years, researchers have tried to identify individual biomarkers. Gathering
neurobiological data however, is costly and time-consuming. Another potential
source of information, which is already part of the clinical routine, are
therapist-patient dialogues. While there are some pioneering works
investigating the role of language as predictors for various therapeutic
parameters, for example patient-therapist alliance, there are no large-scale
studies. A major obstacle to conduct these studies is the availability of
sizeable datasets, which are needed to train machine learning models. While
these conversations are part of the daily routine of clinicians, gathering them
is usually hindered by various ethical (purpose of data usage), legal (data
privacy) and technical (data formatting) limitations. Some of these limitations
are particular to the domain of therapy dialogues, like the increased
difficulty in anonymisation, or the transcription of the recordings. In this
paper, we elaborate on the challenges we faced in starting our collection of
therapist-patient dialogues in a psychiatry clinic under the General Data
Privacy Regulation of the European Union with the goal to use the data for
Natural Language Processing (NLP) research. We give an overview of each step in
our procedure and point out the potential pitfalls to motivate further research
in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Method for Determining the Similarity of Text Documents for the Kazakh language, Taking Into Account Synonyms: Extension to TF-IDF. (arXiv:2211.12364v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12364">
<div class="article-summary-box-inner">
<span><p>The task of determining the similarity of text documents has received
considerable attention in many areas such as Information Retrieval, Text
Mining, Natural Language Processing (NLP) and Computational Linguistics.
Transferring data to numeric vectors is a complex task where algorithms such as
tokenization, stopword filtering, stemming, and weighting of terms are used.
The term frequency - inverse document frequency (TF-IDF) is the most widely
used term weighting method to facilitate the search for relevant documents. To
improve the weighting of terms, a large number of TF-IDF extensions are made.
In this paper, another extension of the TF-IDF method is proposed where
synonyms are taken into account. The effectiveness of the method is confirmed
by experiments on functions such as Cosine, Dice and Jaccard to measure the
similarity of text documents for the Kazakh language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Emotion-Aware Multi-Task Approach to Fake News and Rumour Detection using Transfer Learning. (arXiv:2211.12374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12374">
<div class="article-summary-box-inner">
<span><p>Social networking sites, blogs, and online articles are instant sources of
news for internet users globally. However, in the absence of strict regulations
mandating the genuineness of every text on social media, it is probable that
some of these texts are fake news or rumours. Their deceptive nature and
ability to propagate instantly can have an adverse effect on society. This
necessitates the need for more effective detection of fake news and rumours on
the web. In this work, we annotate four fake news detection and rumour
detection datasets with their emotion class labels using transfer learning. We
show the correlation between the legitimacy of a text with its intrinsic
emotion for fake news and rumour detection, and prove that even within the same
emotion class, fake and real news are often represented differently, which can
be used for improved feature extraction. Based on this, we propose a multi-task
framework for fake news and rumour detection, predicting both the emotion and
legitimacy of the text. We train a variety of deep learning models in
single-task and multi-task settings for a more comprehensive comparison. We
further analyze the performance of our multi-task approach for fake news
detection in cross-domain settings to verify its efficacy for better
generalization across datasets, and to verify that emotions act as a
domain-independent feature. Experimental results verify that our multi-task
models consistently outperform their single-task counterparts in terms of
accuracy, precision, recall, and F1 score, both for in-domain and cross-domain
settings. We also qualitatively analyze the difference in performance in
single-task and multi-task learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12402">
<div class="article-summary-box-inner">
<span><p>Vision language pre-training aims to learn alignments between vision and
language from a large amount of data. We proposed multi-grained vision language
pre-training, a unified approach which can learn vision language alignments in
multiple granularity. This paper advances the proposed method by unifying image
and video encoding in one model and scaling up the model with large-scale data.
We present X$^2$-VLM, a pre-trained VLM with a modular architecture for both
image-text tasks and video-text tasks. Experiment results show that X$^2$-VLM
performs the best on base and large scale for both image-text and video-text
tasks, making a good trade-off between performance and model scale. Moreover,
we show that the modular design of X$^2$-VLM results in high transferability
for X$^2$-VLM to be utilized in any language or domain. For example, by simply
replacing the text encoder with XLM-R, X$^2$-VLM outperforms state-of-the-art
multilingual multi-modal pre-trained models without any multilingual
pre-training. The code and pre-trained models will be available at
github.com/zengyan-97/X2-VLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Narrative Information and the Distillation of Stories. (arXiv:2211.12423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12423">
<div class="article-summary-box-inner">
<span><p>The act of telling stories is a fundamental part of what it means to be
human. This work introduces the concept of narrative information, which we
define to be the overlap in information space between a story and the items
that compose the story. Using contrastive learning methods, we show how modern
artificial neural networks can be leveraged to distill stories and extract a
representation of the narrative information. We then demonstrate how
evolutionary algorithms can leverage this to extract a set of narrative
templates and how these templates -- in tandem with a novel curve-fitting
algorithm we introduce -- can reorder music albums to automatically induce
stories in them. In the process of doing so, we give strong statistical
evidence that these narrative information templates are present in existing
albums. While we experiment only with music albums here, the premises of our
work extend to any form of (largely) independent media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTuning: Toward Adapting Large Language Models without Back-propagation. (arXiv:2211.12485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12485">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large language models for different tasks can be costly and
inefficient, and even methods that reduce the number of tuned parameters still
require full gradient-based optimization. We propose HyperTuning, a novel
approach to model adaptation that uses a hypermodel to generate task-specific
parameters for a fixed downstream model. We demonstrate a simple setup for
hypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or
LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5
in two stages: first, hyperpretraining with a modified conditional language
modeling objective that trains a hypermodel to generate parameters; second,
multi-task fine-tuning (MTF) on a large number of diverse language tasks. We
evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and
show that it can effectively generate parameters for unseen tasks. Moreover, we
show that using hypermodel-generated parameters as initializations for further
parameter-efficient fine-tuning improves performance. HyperTuning can thus be a
flexible and efficient way to leverage large language models for diverse
downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified MRC Framework for Named Entity Recognition. (arXiv:1910.11476v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.11476">
<div class="article-summary-box-inner">
<span><p>The task of named entity recognition (NER) is normally divided into nested
NER and flat NER depending on whether named entities are nested or not. Models
are usually separately developed for the two tasks, since sequence labeling
models, the most widely used backbone for flat NER, are only able to assign a
single label to a particular token, which is unsuitable for nested NER where a
token may be assigned several labels.
</p>
<p>In this paper, we propose a unified framework that is capable of handling
both flat and nested NER tasks. Instead of treating the task of NER as a
sequence labeling problem, we propose to formulate it as a machine reading
comprehension (MRC) task. For example, extracting entities with the
\textsc{per} label is formalized as extracting answer spans to the question
"{\it which person is mentioned in the text?}". This formulation naturally
tackles the entity overlapping issue in nested NER: the extraction of two
overlapping entities for different categories requires answering two
independent questions. Additionally, since the query encodes informative prior
knowledge, this strategy facilitates the process of entity extraction, leading
to better performances for not only nested NER, but flat NER.
</p>
<p>We conduct experiments on both {\em nested} and {\em flat} NER datasets.
Experimental results demonstrate the effectiveness of the proposed formulation.
We are able to achieve vast amount of performance boost over current SOTA
models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively
on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets,
i.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English
OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Nearest Neighbor Machine Translation. (arXiv:2105.14528v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14528">
<div class="article-summary-box-inner">
<span><p>Though nearest neighbor Machine Translation ($k$NN-MT)
\citep{khandelwal2020nearest} has proved to introduce significant performance
boosts over standard neural MT systems, it is prohibitively slow since it uses
the entire reference corpus as the datastore for the nearest neighbor search.
This means each step for each beam in the beam search has to search over the
entire reference corpus. $k$NN-MT is thus two-orders slower than vanilla MT
models, making it hard to be applied to real-world applications, especially
online services. In this work, we propose Fast $k$NN-MT to address this issue.
Fast $k$NN-MT constructs a significantly smaller datastore for the nearest
neighbor search: for each word in a source sentence, Fast $k$NN-MT first
selects its nearest token-level neighbors, which is limited to tokens that are
the same as the query token. Then at each decoding step, in contrast to using
the entire corpus as the datastore, the search space is limited to target
tokens corresponding to the previously selected reference source tokens. This
strategy avoids search through the whole datastore for nearest neighbors and
drastically improves decoding efficiency. Without loss of performance, Fast
$k$NN-MT is two-orders faster than $k$NN-MT, and is only two times slower than
the standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT
systems in real-world MT applications. The code is available at
\url{https://github.com/ShannonAI/fast-knn-nmt}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Evaluation of Cross-document Coreference Resolution Models Using Datasets with Diverse Annotation Schemes. (arXiv:2109.05250v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05250">
<div class="article-summary-box-inner">
<span><p>Established cross-document coreference resolution (CDCR) datasets contain
event-centric coreference chains of events and entities with identity
relations. These datasets establish strict definitions of the coreference
relations across related tests but typically ignore anaphora with more vague
context-dependent loose coreference relations. In this paper, we qualitatively
and quantitatively compare the annotation schemes of ECB+, a CDCR dataset with
identity coreference relations, and NewsWCL50, a CDCR dataset with a mix of
loose context-dependent and strict coreference relations. We propose a phrasing
diversity metric (PD) that encounters for the diversity of full phrases unlike
the previously proposed metrics and allows to evaluate lexical diversity of the
CDCR datasets in a higher precision. The analysis shows that coreference chains
of NewsWCL50 are more lexically diverse than those of ECB+ but annotating of
NewsWCL50 leads to the lower inter-coder reliability. We discuss the different
tasks that both CDCR datasets create for the CDCR models, i.e., lexical
disambiguation and lexical diversity. Finally, to ensure generalizability of
the CDCR models, we propose a direction for CDCR evaluation that combines CDCR
datasets with multiple annotation schemes that focus of various properties of
the coreference chains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Intrinsic Exploration with Language Abstractions. (arXiv:2202.08938v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08938">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) agents are particularly hard to train when
rewards are sparse. One common solution is to use intrinsic rewards to
encourage agents to explore their environment. However, recent intrinsic
exploration methods often use state-based novelty measures which reward
low-level exploration and may not scale to domains requiring more abstract
skills. Instead, we explore natural language as a general medium for
highlighting relevant abstractions in an environment. Unlike previous work, we
evaluate whether language can improve over existing exploration methods by
directly extending (and comparing to) competitive intrinsic exploration
baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These
language-based variants outperform their non-linguistic forms by 47-85% across
13 challenging tasks from the MiniGrid and MiniHack environment suites.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models. (arXiv:2205.11169v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11169">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training (VLP) has shown impressive performance on a wide
range of cross-modal tasks, where VLP models without reliance on object
detectors are becoming the mainstream due to their superior computation
efficiency and competitive performance. However, the removal of object
detectors also deprives the capability of VLP models in explicit object
modeling, which is essential to various position-sensitive vision-language (VL)
tasks, such as referring expression comprehension and visual commonsense
reasoning. To address the challenge, we introduce PEVL that enhances the
pre-training and prompt tuning of VLP models with explicit object position
modeling. Specifically, PEVL reformulates discretized object positions and
language in a unified language modeling framework, which facilitates explicit
VL alignment during pre-training, and also enables flexible prompt tuning for
various downstream tasks. We show that PEVL enables state-of-the-art
performance of detector-free VLP models on position-sensitive tasks such as
referring expression comprehension and phrase grounding, and also improves the
performance on position-insensitive tasks with grounded inputs. We make the
data and code for this paper publicly available at
https://github.com/thunlp/PEVL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts. (arXiv:2205.12701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12701">
<div class="article-summary-box-inner">
<span><p>Recent works suggest that transformer models are capable of multi-tasking on
diverse NLP tasks and adapting to new tasks efficiently. However, the potential
of these multi-task models may be limited as they use the same set of
parameters for all tasks. In contrast, humans tackle tasks in a more flexible
way, by making proper presumptions on what skills and knowledge are relevant
and executing only the necessary computations. Inspired by this, we propose to
use task-level mixture-of-expert models, which has a collection of transformer
layers (i.e., experts) and a router component that chooses from these experts
dynamically and flexibly. We find that these models help improve the average
performance gain (ARG) metric by 2.6% when adapting to unseen tasks in the
few-shot setting and by 5.6% in the zero-shot generalization setting. Further,
we show that the learned routing decisions partly rediscover human
categorization of NLP tasks -- certain experts are strongly associated with
extractive tasks, some with classification tasks, and some with tasks requiring
world knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08853">
<div class="article-summary-box-inner">
<span><p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite, knowledge bases, algorithm
implementation, and pretrained models (https://minedojo.org) to promote
research towards the goal of generally capable embodied agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BabelBERT: Massively Multilingual Transformers Meet a Massively Multilingual Lexical Resource. (arXiv:2208.01018v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01018">
<div class="article-summary-box-inner">
<span><p>While pretrained language models (PLMs) primarily serve as general purpose
text encoders that can be fine-tuned for a wide variety of downstream tasks,
recent work has shown that they can also be rewired to produce high-quality
word representations (i.e., static word embeddings) and yield good performance
in type-level lexical tasks. While existing work primarily focused on lexical
specialization of PLMs in monolingual and bilingual settings, in this work we
expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to
multilingual lexical knowledge at scale, leveraging BabelNet as the readily
available rich source of multilingual and cross-lingual type-level lexical
knowledge. Concretely, we leverage BabelNet's multilingual synsets to create
synonym pairs across $50$ languages and then subject the MMTs (mBERT and XLM-R)
to a lexical specialization procedure guided by a contrastive objective. We
show that such massively multilingual lexical specialization brings massive
gains in two standard cross-lingual lexical tasks, bilingual lexicon induction
and cross-lingual word similarity, as well as in cross-lingual sentence
retrieval. Crucially, we observe gains for languages unseen in specialization,
indicating that the multilingual lexical specialization enables generalization
to languages with no lexical constraints. In a series of subsequent controlled
experiments, we demonstrate that the pretraining quality of word
representations in the MMT for languages involved in specialization has a much
larger effect on performance than the linguistic diversity of the set of
constraints. Encouragingly, this suggests that lexical tasks involving
low-resource languages benefit the most from lexical knowledge of resource-rich
languages, generally much more available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention. (arXiv:2209.03126v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03126">
<div class="article-summary-box-inner">
<span><p>There is increasing interest in the use of multimodal data in various web
applications, such as digital advertising and e-commerce. Typical methods for
extracting important information from multimodal data rely on a mid-fusion
architecture that combines the feature representations from multiple encoders.
However, as the number of modalities increases, several potential problems with
the mid-fusion model structure arise, such as an increase in the dimensionality
of the concatenated multimodal features and missing modalities. To address
these problems, we propose a new concept that considers multimodal inputs as a
set of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our
set-aware concept consists of three components that capture the relationships
among multiple modalities: (a) a BERT-based encoder to handle the inter- and
intra-order of elements in the sequences, (b) intra-modality residual attention
(IntraMRA) to capture the importance of the elements in a modality, and (c)
inter-modality residual attention (InterMRA) to enhance the importance of
elements with modality-level granularity further. Our concept exhibits
performance that is comparable to or better than the previous set-aware models.
Furthermore, we demonstrate that the visualization of the learned InterMRA and
IntraMRA weights can provide an interpretation of the prediction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvNext Based Neural Network for Audio Anti-Spoofing. (arXiv:2209.06434v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06434">
<div class="article-summary-box-inner">
<span><p>Automatic speaker verification (ASV) has been widely used in the real life
for identity authentication. However, with the rapid development of speech
conversion, speech synthesis algorithms, ASV systems are vulnerable for spoof
attacks. In recent years, there have many works about synthetic speech
detection, researchers had proposed a number of anti-spoofing methods based on
hand-crafted features to improve the detection accuracy and robustness of ASV
systems. However, using hand-crafted features rather than raw waveform would
lose certain information for anti-spoofing, which will reduce the detection
performance of the system. Inspired by the promising performance of ConvNeXt in
image classification tasks, we revise the ConvNeXt network architecture
accordingly for spoof attacks detection task and propose a light weight
end-to-end anti-spoofing model. By integrating the revised architecture with
the channel attention block and using the focal loss function, the proposed
model can focus on the most informative sub-bands of speech representations to
improve the anti-spoofing performance and the difficult samples that are hard
for models to classify. Experiments show that our proposed best single system
could achieve an equal error rate of 0.75% and min-tDCF of 0.0212 for the
ASVSpoof2019 LA evaluation dataset, which outperform the state-of-the-art
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selection Induced Collider Bias: A Gender Pronoun Uncertainty Case Study. (arXiv:2210.00131v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00131">
<div class="article-summary-box-inner">
<span><p>In this paper, we cast the problem of task underspecification in causal
terms, and develop a method for empirical measurement of spurious associations
between gender and gender-neutral entities for unmodified large language
models, detecting previously unreported spurious correlations. We then describe
a lightweight method to exploit the resulting spurious associations for
prediction task uncertainty classification, achieving over 90% accuracy on a
Winogender Schemas challenge set. Finally, we generalize our approach to
address a wider range of prediction tasks and provide open-source demos for
each method described here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13952">
<div class="article-summary-box-inner">
<span><p>We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues. (arXiv:2211.11049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11049">
<div class="article-summary-box-inner">
<span><p>Conversations emerge as the primary media for exchanging ideas and
conceptions. From the listener's perspective, identifying various affective
qualities, such as sarcasm, humour, and emotions, is paramount for
comprehending the true connotation of the emitted utterance. However, one of
the major hurdles faced in learning these affect dimensions is the presence of
figurative language, viz. irony, metaphor, or sarcasm. We hypothesize that any
detection system constituting the exhaustive and explicit presentation of the
emitted utterance would improve the overall comprehension of the dialogue. To
this end, we explore the task of Sarcasm Explanation in Dialogues, which aims
to unfold the hidden irony behind sarcastic utterances. We propose MOSES, a
deep neural network, which takes a multimodal (sarcastic) dialogue instance as
an input and generates a natural language sentence as its explanation.
Subsequently, we leverage the generated explanation for various natural
language understanding tasks in a conversational dialogue setup, such as
sarcasm detection, humour identification, and emotion recognition. Our
evaluation shows that MOSES outperforms the state-of-the-art system for SED by
an average of ~2% on different evaluation metrics, such as ROUGE, BLEU, and
METEOR. Further, we observe that leveraging the generated explanation advances
three downstream tasks for affect classification - an average improvement of
~14% F1-score in the sarcasm detection task and ~2% in the humour
identification and emotion recognition task. We also perform extensive analyses
to assess the quality of the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi. (arXiv:2211.11187v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11187">
<div class="article-summary-box-inner">
<span><p>Sentence representation from vanilla BERT models does not work well on
sentence similarity tasks. Sentence-BERT models specifically trained on STS or
NLI datasets are shown to provide state-of-the-art performance. However,
building these models for low-resource languages is not straightforward due to
the lack of these specialized datasets. This work focuses on two low-resource
Indian languages, Hindi and Marathi. We train sentence-BERT models for these
languages using synthetic NLI and STS datasets prepared using machine
translation. We show that the strategy of NLI pre-training followed by STSb
fine-tuning is effective in generating high-performance sentence-similarity
models for Hindi and Marathi. The vanilla BERT models trained using this simple
strategy outperform the multilingual LaBSE trained using a complex training
strategy. These models are evaluated on downstream text classification and
similarity tasks. We evaluate these models on real text classification datasets
to show embeddings obtained from synthetic data training are generalizable to
real datasets as well and thus represent an effective training strategy for
low-resource languages. We also provide a comparative analysis of sentence
embeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,
xlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and
monolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release
L3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for
Marathi and Hindi respectively. Our work also serves as a guide to building
low-resource sentence embedding models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11419">
<div class="article-summary-box-inner">
<span><p>This paper presents an in-depth study on a Sequentially Sampled Chunk
Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer
first demonstrates the significant performance gains from using the
sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the
Conformer encoder by allowing efficient cross-chunk interactions while keeping
linear complexities. Furthermore, it explores taking advantage of chunked
convolution to make use of the chunk-wise future context and integrates with
casual convolution in the convolution layers to further reduce CER. We verify
the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results
show that a state-of-the-art performance for streaming E2E ASR is achieved with
CER 5.33% without LM rescoring. And, owing to its linear complexity, the
SSC-Conformer can train with large batch sizes and infer more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training. (arXiv:2211.11446v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11446">
<div class="article-summary-box-inner">
<span><p>Video-language pre-training is crucial for learning powerful multi-modal
representation. However, it typically requires a massive amount of computation.
In this paper, we develop SMAUG, an efficient pre-training framework for
video-language models. The foundation component in SMAUG is masked
autoencoders. Different from prior works which only mask textual inputs, our
masking strategy considers both visual and textual modalities, providing a
better cross-modal alignment and saving more pre-training costs. On top of
that, we introduce a space-time token sparsification module, which leverages
context information to further select only "important" spatial regions and
temporal frames for pre-training. Coupling all these designs allows our method
to enjoy both competitive performances on text-to-video retrieval and video
question answering tasks, and much less pre-training costs by 1.9X or more. For
example, our SMAUG only needs about 50 NVIDIA A6000 GPU hours for pre-training
to attain competitive performances on these two video-language tasks across six
popular benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Vision-Language Prompt Tuning. (arXiv:2211.11720v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11720">
<div class="article-summary-box-inner">
<span><p>Prompt Tuning, conditioning on task-specific learned prompt vectors, has
emerged as a data-efficient and parameter-efficient method for adapting large
pretrained vision-language models to multiple downstream tasks. However,
existing approaches usually consider learning prompt vectors for each task
independently from scratch, thereby failing to exploit the rich shareable
knowledge across different vision-language tasks. In this paper, we propose
multitask vision-language prompt tuning (MVLPT), which incorporates cross-task
knowledge into prompt tuning for vision-language models. Specifically, (i) we
demonstrate the effectiveness of learning a single transferable prompt from
multiple source tasks to initialize the prompt for each target task; (ii) we
show many target tasks can benefit each other from sharing prompt vectors and
thus can be jointly learned via multitask prompt tuning. We benchmark the
proposed MVLPT using three representative prompt tuning methods, namely text
prompt tuning, visual prompt tuning, and the unified vision-language prompt
tuning. Results in 20 vision tasks demonstrate that the proposed approach
outperforms all single-task baseline prompt tuning methods, setting the new
state-of-the-art on the few-shot ELEVATER benchmarks and cross-task
generalization benchmarks. To understand where the cross-task knowledge is most
effective, we also conduct a large-scale study on task transferability with 20
vision tasks in 400 combinations for each prompt tuning method. It shows that
the most performant MVLPT for each prompt tuning method prefers different task
combinations and many tasks can benefit each other, depending on their visual
similarity and label similarity. Code is available at
https://github.com/sIncerass/MVLPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convexifying Transformers: Improving optimization and understanding of transformer networks. (arXiv:2211.11052v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11052">
<div class="article-summary-box-inner">
<span><p>Understanding the fundamental mechanism behind the success of transformer
networks is still an open problem in the deep learning literature. Although
their remarkable performance has been mostly attributed to the self-attention
mechanism, the literature still lacks a solid analysis of these networks and
interpretation of the functions learned by them. To this end, we study the
training problem of attention/transformer networks and introduce a novel convex
analytic approach to improve the understanding and optimization of these
networks. Particularly, we first introduce a convex alternative to the
self-attention mechanism and reformulate the regularized training problem of
transformer networks with our alternative convex attention. Then, we cast the
reformulation as a convex optimization problem that is interpretable and easier
to optimize. Moreover, as a byproduct of our convex analysis, we reveal an
implicit regularization mechanism, which promotes sparsity across tokens.
Therefore, we not only improve the optimization of attention/transformer
networks but also provide a solid theoretical understanding of the functions
learned by them. We also demonstrate the effectiveness of our theory through
several numerical experiments.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-23 23:13:32.814478664 UTC">2022-11-23 23:13:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>