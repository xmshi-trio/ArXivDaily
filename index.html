<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-05T01:30:00Z">12-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Math Word Problem Solvers via Solution Diversification. (arXiv:2212.00833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00833">
<div class="article-summary-box-inner">
<span><p>Current math word problem (MWP) solvers are usually Seq2Seq models trained by
the (one-problem; one-solution) pairs, each of which is made of a problem
description and a solution showing reasoning flow to get the correct answer.
However, one MWP problem naturally has multiple solution equations. The
training of an MWP solver with (one-problem; one-solution) pairs excludes other
correct solutions, and thus limits the generalizability of the MWP solver. One
feasible solution to this limitation is to augment multiple solutions to a
given problem. However, it is difficult to collect diverse and accurate augment
solutions through human efforts. In this paper, we design a new training
framework for an MWP solver by introducing a solution buffer and a solution
discriminator. The buffer includes solutions generated by an MWP solver to
encourage the training data diversity. The discriminator controls the quality
of buffered solutions to participate in training. Our framework is flexibly
applicable to a wide setting of fully, semi-weakly and weakly supervised
training for all Seq2Seq MWP solvers. We conduct extensive experiments on a
benchmark dataset Math23k and a new dataset named Weak12k, and show that our
framework improves the performance of various MWP solvers under different
settings by generating correct and diverse solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogical Math Word Problems Solving with Enhanced Problem-Solution Association. (arXiv:2212.00837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00837">
<div class="article-summary-box-inner">
<span><p>Math word problem (MWP) solving is an important task in question answering
which requires human-like reasoning ability. Analogical reasoning has long been
used in mathematical education, as it enables students to apply common
relational structures of mathematical situations to solve new problems. In this
paper, we propose to build a novel MWP solver by leveraging analogical MWPs,
which advance the solver's generalization ability across different kinds of
MWPs. The key idea, named analogy identification, is to associate the
analogical MWP pairs in a latent space, i.e., encoding an MWP close to another
analogical MWP, while moving away from the non-analogical ones. Moreover, a
solution discriminator is integrated into the MWP solver to enhance the
association between the representations of MWPs and their true solutions. The
evaluation results verify that our proposed analogical learning strategy
promotes the performance of MWP-BERT on Math23k over the state-of-the-art model
Generate2Rank, with 5 times fewer parameters in the encoder. We also find that
our model has a stronger generalization ability in solving difficult MWPs due
to the analogical learning from easy MWPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus! Relevant and Sufficient Context Selection for News Image Captioning. (arXiv:2212.00843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00843">
<div class="article-summary-box-inner">
<span><p>News Image Captioning requires describing an image by leveraging additional
context from a news article. Previous works only coarsely leverage the article
to extract the necessary context, which makes it challenging for models to
identify relevant events and named entities. In our paper, we first demonstrate
that by combining more fine-grained context that captures the key named
entities (obtained via an oracle) and the global context that summarizes the
news, we can dramatically improve the model's ability to generate accurate news
captions. This begs the question, how to automatically extract such key
entities from an image? We propose to use the pre-trained vision and language
retrieval model CLIP to localize the visually grounded entities in the news
article and then capture the non-visual entities via an open relation
extraction model. Our experiments demonstrate that by simply selecting a better
context from the article, we can significantly improve the performance of
existing models and achieve new state-of-the-art performance on multiple
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLD: Sinhala Offensive Language Dataset. (arXiv:2212.00851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00851">
<div class="article-summary-box-inner">
<span><p>The widespread of offensive content online, such as hate speech and
cyber-bullying, is a global phenomenon. This has sparked interest in the
artificial intelligence (AI) and natural language processing (NLP) communities,
motivating the development of various systems trained to detect potentially
harmful content automatically. These systems require annotated datasets to
train the machine learning (ML) models. However, with a few notable exceptions,
most datasets on this topic have dealt with English and a few other
high-resource languages. As a result, the research in offensive language
identification has been limited to these languages. This paper addresses this
gap by tackling offensive language identification in Sinhala, a low-resource
Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce
the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments
on this dataset. SOLD is a manually annotated dataset containing 10,000 posts
from Twitter annotated as offensive and not offensive at both sentence-level
and token-level, improving the explainability of the ML models. SOLD is the
first large publicly available offensive language dataset compiled for Sinhala.
We also introduce SemiSOLD, a larger dataset containing more than 145,000
Sinhala tweets, annotated following a semi-supervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">a survey on GPT-3. (arXiv:2212.00857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00857">
<div class="article-summary-box-inner">
<span><p>This paper provides an introductory survey to GPT-3. We cover some of the
historical development behind this technology, some of the key features of
GPT-3, and discuss the machine learning model and the datasets used. We survey
both academic and commercial efforts applying GPT-3 in diverse domains such as
developing conversational AI chatbots, software development, creative work,
domain knowledge, and business productivity. We discuss some of the challenges
that GPT-3 faces such as the problems of training complexity, bias, and
hallucination/incorrect answers. We also discuss the future research
opportunities in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization. (arXiv:2212.00921v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00921">
<div class="article-summary-box-inner">
<span><p>Models trained via empirical risk minimization (ERM) are known to rely on
spurious correlations between labels and task-independent input features,
resulting in poor generalization to distributional shifts. Group
distributionally robust optimization (G-DRO) can alleviate this problem by
minimizing the worst-case loss over a set of pre-defined groups over training
data. G-DRO successfully improves performance of the worst-group, where the
correlation does not hold. However, G-DRO assumes that the spurious
correlations and associated worst groups are known in advance, making it
challenging to apply it to new tasks with potentially multiple unknown spurious
correlations. We propose AGRO -- Adversarial Group discovery for
Distributionally Robust Optimization -- an end-to-end approach that jointly
identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO
with an adversarial slicing model to find a group assignment for training
examples which maximizes worst-case loss over the discovered groups. On the
WILDS benchmark, AGRO results in 8% higher model performance on average on
known worst-groups, compared to prior group discovery approaches used with
G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and
MS-COCO -- datasets where potential spurious correlations are as yet
uncharacterized. Human evaluation of ARGO groups shows that they contain
well-defined, yet previously unstudied spurious correlations that lead to model
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Nested Named Entity Recognition. (arXiv:2212.00953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00953">
<div class="article-summary-box-inner">
<span><p>While Named Entity Recognition (NER) is a widely studied task, making
inferences of entities with only a few labeled data has been challenging,
especially for entities with nested structures. Unlike flat entities, entities
and their nested entities are more likely to have similar semantic feature
representations, drastically increasing difficulties in classifying different
entity categories in the few-shot setting. Although prior work has briefly
discussed nested structures in the context of few-shot learning, to our best
knowledge, this paper is the first one specifically dedicated to studying the
few-shot nested NER task. Leveraging contextual dependency to distinguish
nested entities, we propose a Biaffine-based Contrastive Learning (BCL)
framework. We first design a Biaffine span representation module for learning
the contextual span dependency representation for each entity span rather than
only learning its semantic representation. We then merge these two
representations by the residual connection to distinguish nested entities.
Finally, we build a contrastive learning framework to adjust the representation
distribution for larger margin boundaries and more generalized domain transfer
learning ability. We conducted experimental studies on three English, German,
and Russian nested NER datasets. The results show that the BCL outperformed
three baseline models on the 1-shot and 5-shot tasks in terms of F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. (arXiv:2212.00959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00959">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it firstly retrieves a relatively small subgraph related to the question and
then performs the reasoning on the subgraph to accurately find the answer
entities. Although these two stages are highly related, previous work employs
very different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the edges on KGs. For parameter learning, we design a shared pre-training
task based on question-relation matching for both retrieval and reasoning
models, and then propose retrieval- and reasoning-oriented fine-tuning
strategies. Compared with previous studies, our approach is more unified,
tightly relating the retrieval and reasoning stages. Extensive experiments on
three benchmark datasets have demonstrated the effectiveness of our method on
the multi-hop KGQA task. Our codes and data are publicly available at
https://github.com/RUCAIBox/UniKGQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00975">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) is a task that entails reasoning over natural
language contexts, and many relevant works augment language models (LMs) with
graph neural networks (GNNs) to encode the Knowledge Graph (KG) information.
However, most existing GNN-based modules for QA do not take advantage of rich
relational information of KGs and depend on limited information interaction
between the LM and the KG. To address these issues, we propose Question
Answering Transformer (QAT), which is designed to jointly reason over language
and graphs with respect to entity relations in a unified manner. Specifically,
QAT constructs Meta-Path tokens, which learn relation-centric embeddings based
on diverse structural and semantic relations. Then, our Relation-Aware
Self-Attention module comprehensively integrates different modalities via the
Cross-Modal Relative Position Bias, which guides information exchange between
relevant entities of different modalities. We validate the effectiveness of QAT
on commonsense question answering datasets like CommonsenseQA and OpenBookQA,
and on a medical question answering dataset, MedQA-USMLE. On all the datasets,
our method achieves state-of-the-art performance. Our code is available at
<a href="http://github.com/mlvlab/QAT.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Contrastive Pre-Training for Efficient Video-Text Retrieval. (arXiv:2212.00986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00986">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective end-to-end Video-language Pre-training
(VidLP) framework, Masked Contrastive Video-language Pretraining (MAC), for
video-text retrieval tasks. Our MAC aims to reduce video representation's
spatial and temporal redundancy in the VidLP model by a mask sampling mechanism
to improve pre-training efficiency. Comparing conventional temporal sparse
sampling, we propose to randomly mask a high ratio of spatial regions and only
feed visible regions into the encoder as sparse spatial sampling. Similarly, we
adopt the mask sampling technique for text inputs for consistency. Instead of
blindly applying the mask-then-prediction paradigm from MAE, we propose a
masked-then-alignment paradigm for efficient video-text alignment. The
motivation is that video-text retrieval tasks rely on high-level alignment
rather than low-level reconstruction, and multimodal alignment with masked
modeling encourages the model to learn a robust and general multimodal
representation from incomplete and unstable inputs. Coupling these designs
enables efficient end-to-end pre-training: reduce FLOPs (60% off), accelerate
pre-training (by 3x), and improve performance. Our MAC achieves
state-of-the-art results on various video-text retrieval datasets, including
MSR-VTT, DiDeMo, and ActivityNet. Our approach is omnivorous to input
modalities. With minimal modifications, we achieve competitive results on
image-text retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGO: Boosting Mobile AI Inference Performance by Removing Constraints on Graph Optimization. (arXiv:2212.01005v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01005">
<div class="article-summary-box-inner">
<span><p>Traditional deep learning compilers rely on heuristics for subgraph
generation, which impose extra constraints on graph optimization, e.g., each
subgraph can only contain at most one complex operator. In this paper, we
propose AGO, a framework for graph optimization with arbitrary structures to
boost the inference performance of deep models by removing such constraints. To
create new optimization opportunities for complicated subgraphs, we propose
intensive operator fusion, which can effectively stitch multiple complex
operators together for better performance. Further, we design a graph
partitioning scheme that allows an arbitrary structure for each subgraph while
guaranteeing the acyclic property among all generated subgraphs. Additionally,
to enable efficient performance tuning on complicated subgraphs, we devise a
novel divide-and-conquer tuning mechanism to orchestrate different system
components. Through extensive experiments on various neural networks and mobile
devices, we show that our system can improve the inference performance by up to
3.3x when compared with state-of-the-art deep compilers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Framework for Self-Supervised Model Priming for Parameter-Efficient Fine-tuning. (arXiv:2212.01032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01032">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient methods (like Prompt or Adapters) for adapting
pre-trained language models to downstream tasks have been popular recently.
However, hindrances still prevent these methods from reaching their full
potential. For example, two significant challenges are few-shot adaptation and
cross-task generalization ability. To tackle these issues, we propose a general
framework to enhance the few-shot adaptation and cross-domain generalization
ability of parameter-efficient methods. In our framework, we prime the
self-supervised model for parameter-efficient methods to rapidly adapt to
various downstream few-shot tasks. To evaluate the authentic generalization
ability of these parameter-efficient methods, we conduct experiments on a
few-shot cross-domain benchmark containing 160 diverse NLP tasks. The
experiment result reveals that priming by tuning PLM only with extra training
tasks leads to the best performance. Also, we perform a comprehensive analysis
of various parameter-efficient methods under few-shot cross-domain scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition. (arXiv:2212.01039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01039">
<div class="article-summary-box-inner">
<span><p>Error correction in automatic speech recognition (ASR) aims to correct those
incorrect words in sentences generated by ASR models. Since recent ASR models
usually have low word error rate (WER), to avoid affecting originally correct
tokens, error correction models should only modify incorrect words, and
therefore detecting incorrect words is important for error correction. Previous
works on error correction either implicitly detect error words through
target-source attention or CTC (connectionist temporal classification) loss, or
explicitly locate specific deletion/substitution/insertion errors. However,
implicit error detection does not provide clear signal about which tokens are
incorrect and explicit error detection suffers from low detection accuracy. In
this paper, we propose SoftCorrect with a soft error detection mechanism to
avoid the limitations of both explicit and implicit error detection.
Specifically, we first detect whether a token is correct or not through a
probability produced by a dedicatedly designed language model, and then design
a constrained CTC loss that only duplicates the detected incorrect tokens to
let the decoder focus on the correction of error tokens. Compared with implicit
error detection with CTC loss, SoftCorrect provides explicit signal about which
words are incorrect and thus does not need to duplicate every token but only
incorrect tokens; compared with explicit error detection, SoftCorrect does not
detect specific deletion/substitution/insertion errors but just leaves it to
CTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that
SoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming
previous works by a large margin, while still enjoying fast speed of parallel
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Faithful Rationale for Multi-hop Fact Verification via Salience-Aware Graph Learning. (arXiv:2212.01060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01060">
<div class="article-summary-box-inner">
<span><p>The opaqueness of the multi-hop fact verification model imposes imperative
requirements for explainability. One feasible way is to extract rationales, a
subset of inputs, where the performance of prediction drops dramatically when
being removed. Though being explainable, most rationale extraction methods for
multi-hop fact verification explore the semantic information within each piece
of evidence individually, while ignoring the topological information
interaction among different pieces of evidence. Intuitively, a faithful
rationale bears complementary information being able to extract other
rationales through the multi-hop reasoning process. To tackle such
disadvantages, we cast explainable multi-hop fact verification as subgraph
extraction, which can be solved based on graph convolutional network (GCN) with
salience-aware graph learning. In specific, GCN is utilized to incorporate the
topological interaction information among multiple pieces of evidence for
learning evidence representation. Meanwhile, to alleviate the influence of
noisy evidence, the salience-aware graph perturbation is induced into the
message passing of GCN. Moreover, the multi-task model with three diagnostic
properties of rationale is elaborately designed to improve the quality of an
explanation without any explicit annotations. Experimental results on the
FEVEROUS benchmark show significant gains over previous state-of-the-art
methods for both rationale extraction and fact verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Mutual Learning for Cued Speech Recognition. (arXiv:2212.01083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01083">
<div class="article-summary-box-inner">
<span><p>Automatic Cued Speech Recognition (ACSR) provides an intelligent
human-machine interface for visual communications, where the Cued Speech (CS)
system utilizes lip movements and hand gestures to code spoken language for
hearing-impaired people. Previous ACSR approaches often utilize direct feature
concatenation as the main fusion paradigm. However, the asynchronous modalities
(\textit{i.e.}, lip, hand shape and hand position) in CS may cause interference
for feature concatenation. To address this challenge, we propose a transformer
based cross-modal mutual learning framework to prompt multi-modal interaction.
Compared with the vanilla self-attention, our model forces modality-specific
information of different modalities to pass through a modality-invariant
codebook, collating linguistic representations for tokens of each modality.
Then the shared linguistic knowledge is used to re-synchronize multi-modal
sequences. Moreover, we establish a novel large-scale multi-speaker CS dataset
for Mandarin Chinese. To our knowledge, this is the first work on ACSR for
Mandarin Chinese. Extensive experiments are conducted for different languages
(\textit{i.e.}, Chinese, French, and British English). Results demonstrate that
our model exhibits superior recognition performance to the state-of-the-art by
a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures. (arXiv:2212.01094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01094">
<div class="article-summary-box-inner">
<span><p>One of the common traits of past and present approaches for Semantic Role
Labeling (SRL) is that they rely upon discrete labels drawn from a predefined
linguistic inventory to classify predicate senses and their arguments. However,
we argue this need not be the case. In this paper, we present an approach that
leverages Definition Modeling to introduce a generalized formulation of SRL as
the task of describing predicate-argument structures using natural language
definitions instead of discrete labels. Our novel formulation takes a first
step towards placing interpretability and flexibility foremost, and yet our
experiments and analyses on PropBank-style and FrameNet-style, dependency-based
and span-based SRL also demonstrate that a flexible model with an interpretable
output does not necessarily come at the expense of performance. We release our
software for research purposes at https://github.com/SapienzaNLP/dsrl.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01117">
<div class="article-summary-box-inner">
<span><p>The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22. (arXiv:2212.01140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01140">
<div class="article-summary-box-inner">
<span><p>This paper describes the system developed at the Universitat Polit\`ecnica de
Catalunya for the Workshop on Machine Translation 2022 Sign Language
Translation Task, in particular, for the sign-to-text direction. We use a
Transformer model implemented with the Fairseq modeling toolkit. We have
experimented with the vocabulary size, data augmentation techniques and
pretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50
BLEU score for the test set, improving the organizers' baseline by 0.38 BLEU.
We remark the poor results for both the baseline and our system, and thus, the
unreliability of our findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables. (arXiv:2212.01145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01145">
<div class="article-summary-box-inner">
<span><p>Conditional variational models, using either continuous or discrete latent
variables, are powerful for open-domain dialogue response generation. However,
previous works show that continuous latent variables tend to reduce the
coherence of generated responses. In this paper, we also found that discrete
latent variables have difficulty capturing more diverse expressions. To tackle
these problems, we combine the merits of both continuous and discrete latent
variables and propose a Hybrid Latent Variable (HLV) method. Specifically, HLV
constrains the global semantics of responses through discrete latent variables
and enriches responses with continuous latent variables. Thus, we diversify the
generated responses while maintaining relevance and coherence. In addition, we
propose Conditional Hybrid Variational Transformer (CHVT) to construct and to
utilize HLV with transformers for dialogue generation. Through fine-grained
symbolic-level semantic information and additive Gaussian mixing, we construct
the distribution of continuous variables, prompting the generation of diverse
expressions. Meanwhile, to maintain the relevance and coherence, the discrete
latent variable is optimized by self-separation training. Experimental results
on two dialogue generation datasets (DailyDialog and Opensubtitles) show that
CHVT is superior to traditional transformer-based variational mechanism w.r.t.
diversity, relevance and coherence metrics. Moreover, we also demonstrate the
benefit of applying HLV to fine-tuning two pre-trained dialogue models (PLATO
and BART-base).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SumREN: Summarizing Reported Speech about Events in News. (arXiv:2212.01146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01146">
<div class="article-summary-box-inner">
<span><p>A primary objective of news articles is to establish the factual record for
an event, frequently achieved by conveying both the details of the specified
event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and
how people reacted to it (i.e., reported statements). However, existing work on
news summarization almost exclusively focuses on the event details. In this
work, we propose the novel task of summarizing the reactions of different
speakers, as expressed by their reported statements, to a given event. To this
end, we create a new multi-document summarization benchmark, SUMREN, comprising
745 summaries of reported statements from various public figures obtained from
633 news articles discussing 132 events. We propose an automatic silver
training data generation approach for our task, which helps smaller models like
BART achieve GPT-3 level performance on this task. Finally, we introduce a
pipeline-based framework for summarizing reported speech, which we empirically
show to generate summaries that are more abstractive and factual than baseline
query-focused summarization approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition. (arXiv:2212.01187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01187">
<div class="article-summary-box-inner">
<span><p>Compared to conventional artificial neurons that produce dense and
real-valued responses, biologically-inspired spiking neurons transmit sparse
and binary information, which can also lead to energy-efficient
implementations. Recent research has shown that spiking neural networks can be
trained like standard recurrent neural networks using the surrogate gradient
method. They have shown promising results on speech command recognition tasks.
Using the same technique, we show that they are scalable to large vocabulary
continuous speech recognition, where they are capable of replacing LSTMs in the
encoder with only minor loss of performance. This suggests that they may be
applicable to more involved sequence-to-sequence tasks. Moreover, in contrast
to their recurrent non-spiking counterparts, they show robustness to exploding
gradient problems without the need to use gates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Simultaneous Machine Translation with Monolingual Data. (arXiv:2212.01188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01188">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) is usually done via sequence-level
knowledge distillation (Seq-KD) from a full-sentence neural machine translation
(NMT) model. However, there is still a significant performance gap between NMT
and SiMT. In this work, we propose to leverage monolingual data to improve
SiMT, which trains a SiMT student on the combination of bilingual data and
external monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh
and En-Ja news domain corpora demonstrate that monolingual data can
significantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired
by the behavior of human simultaneous interpreters, we propose a novel
monolingual sampling strategy for SiMT, considering both chunk length and
monotonicity. Experimental results show that our sampling strategy consistently
outperforms the random sampling strategy (and other conventional typical NMT
monolingual sampling strategies) by avoiding the key problem of SiMT --
hallucination, and has better scalability. We achieve +0.72 BLEU improvements
on average against random sampling on En-Zh and En-Ja. Data and codes can be
found at https://github.com/hexuandeng/Mono4SiMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Open Knowledge Base Canonicalization and Linking. (arXiv:2212.01207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01207">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OIE) methods extract a large number of OIE
triples (noun phrase, relation phrase, noun phrase) from text, which compose
large Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation
phrases (RPs) in OKBs are not canonicalized and often appear in different
paraphrased textual variants, which leads to redundant and ambiguous facts. To
address this problem, there are two related tasks: OKB canonicalization (i.e.,
convert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and
RPs with their corresponding entities and relations in a curated Knowledge Base
(e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit
significantly from the other. However, they have been studied in isolation so
far. In this paper, we explore the task of joint OKB canonicalization and
linking for the first time, and propose a novel framework JOCL based on factor
graph model to make them reinforce each other. JOCL is flexible enough to
combine different signals from both tasks, and able to extend to fit any new
signals. A thorough experimental study over two large scale OIE triple data
sets shows that our framework outperforms all the baseline methods for the task
of OKB canonicalization (OKB linking) in terms of average F1 (accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Pre-Trained Language Model to Assist FDA in Premarket Medical Device. (arXiv:2212.01217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01217">
<div class="article-summary-box-inner">
<span><p>This paper proposes a possible method using natural language processing that
might assist in the FDA medical device marketing process. Actual device
descriptions are taken and matched with the device description in FDA Title 21
of CFR to determine their corresponding device type. Both pre-trained word
embeddings such as FastText and large pre-trained sentence embedding models
such as sentence transformers are evaluated on their accuracy in characterizing
a piece of device description. An experiment is also done to test whether these
models can identify the devices wrongly classified in the FDA database. The
result shows that sentence transformer with T5 and MPNet and GPT-3 semantic
search embedding show high accuracy in identifying the correct classification
by narrowing down the correct label to be contained in the first 15 most likely
results, as compared to 2585 types of device descriptions that must be manually
searched through. On the other hand, all methods demonstrate high accuracy in
identifying completely incorrectly labeled devices, but all fail to identify
false device classifications that are wrong but closely related to the true
label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answer ranking in Community Question Answering: a deep learning approach. (arXiv:2212.01218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01218">
<div class="article-summary-box-inner">
<span><p>Community Question Answering is the field of computational linguistics that
deals with problems derived from the questions and answers posted to websites
such as Quora or Stack Overflow. Among some of these problems we find the issue
of ranking the multiple answers posted in reply to each question by how
informative they are in the attempt to solve the original question. This work
tries to advance the state of the art on answer ranking for community Question
Answering by proceeding with a deep learning approach. We started off by
creating a large data set of questions and answers posted to the Stack Overflow
website.
</p>
<p>We then leveraged the natural language processing capabilities of dense
embeddings and LSTM networks to produce a prediction for the accepted answer
attribute, and present the answers in a ranked form ordered by how likely they
are to be marked as accepted by the question asker. We also produced a set of
numerical features to assist with the answer ranking task. These numerical
features were either extracted from metadata found in the Stack Overflow posts
or derived from the questions and answers texts. We compared the performance of
our deep learning models against a set of forest and boosted trees ensemble
methods and found that our models could not improve the best baseline results.
We speculate that this lack of performance improvement versus the baseline
models may be caused by the large number of out of vocabulary words present in
the programming code snippets found in the questions and answers text. We
conclude that while a deep learning approach may be helpful in answer ranking
problems new methods should be developed to assist with the large number of out
of vocabulary words present in the programming code snippets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models. (arXiv:2212.01282v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01282">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) is a powerful technique for learning
representations from unlabeled data. Transformer based models such as HuBERT,
which consist a feature extractor and transformer layers, are leading the field
in the speech domain. SSL models are fine-tuned on a wide range of downstream
tasks, which involves re-training the majority of the model for each task.
Previous studies have introduced applying adapters, which are small lightweight
modules commonly used in Natural Language Processing (NLP) to adapt pre-trained
models to new tasks. However, such efficient tuning techniques only provide
adaptation at the transformer layer, but failed to perform adaptation at the
feature extractor. In this paper, we propose CHAPTER, an efficient tuning
method specifically designed for SSL speech model, by applying CNN adapters at
the feature extractor. Using this method, we can only fine-tune fewer than 5%
of parameters per task compared to fully fine-tuning and achieve better and
more stable performance. We empirically found that adding CNN adapters to the
feature extractor can help the adaptation on emotion and speaker tasks. For
instance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy
of ER is improved by 5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subword-Delimited Downsampling for Better Character-Level Translation. (arXiv:2212.01304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01304">
<div class="article-summary-box-inner">
<span><p>Subword-level models have been the dominant paradigm in NLP. However,
character-level models have the benefit of seeing each character individually,
providing the model with more detailed information that ultimately could lead
to better models. Recent works have shown character-level models to be
competitive with subword models, but costly in terms of time and computation.
Character-level models with a downsampling component alleviate this, but at the
cost of quality, particularly for machine translation. This work analyzes the
problems of previous downsampling methods and introduces a novel downsampling
method which is informed by subwords. This new downsampling method not only
outperforms existing downsampling methods, showing that downsampling characters
can be done without sacrificing quality, but also leads to promising
performance compared to subword models for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Legal Prompting: Teaching a Language Model to Think Like a Lawyer. (arXiv:2212.01326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01326">
<div class="article-summary-box-inner">
<span><p>Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fair with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking. (arXiv:2212.01340v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01340">
<div class="article-summary-box-inner">
<span><p>Neural information retrieval (IR) systems have progressed rapidly in recent
years, in large part due to the release of publicly available benchmarking
tasks. Unfortunately, some dimensions of this progress are illusory: the
majority of the popular IR benchmarks today focus exclusively on downstream
task accuracy and thus conceal the costs incurred by systems that trade away
efficiency for quality. Latency, hardware cost, and other efficiency
considerations are paramount to the deployment of IR systems in user-facing
settings. We propose that IR benchmarks structure their evaluation methodology
to include not only metrics of accuracy, but also efficiency considerations
such as a query latency and the corresponding cost budget for a reproducible
hardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show
how the best choice of IR system varies according to how these efficiency
considerations are chosen and weighed. We hope that future benchmarks will
adopt these guidelines toward more holistic IR evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonparametric Masked Language Modeling. (arXiv:2212.01349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01349">
<div class="article-summary-box-inner">
<span><p>Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 9 closed-set tasks and 7 open-set tasks demonstrates that NPM
outperforms significantly larger parametric models, either with or without a
retrieve-and-generate approach. It is particularly better on dealing with rare
patterns (word senses or facts), and predicting rare or nearly unseen words
(e.g., non-Latin script). We release the model and code at
github.com/facebookresearch/NPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks. (arXiv:2212.01350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01350">
<div class="article-summary-box-inner">
<span><p>Iterative text revision improves text quality by fixing grammatical errors,
rephrasing for better readability or contextual appropriateness, or
reorganizing sentence structures throughout a document. Most recent research
has focused on understanding and classifying different types of edits in the
iterative revision process from human-written text instead of building accurate
and robust systems for iterative text revision. In this work, we aim to build
an end-to-end text revision system that can iteratively generate helpful edits
by explicitly detecting editable spans (where-to-edit) with their corresponding
edit intents and then instructing a revision model to revise the detected edit
spans. Leveraging datasets from other related text editing NLP tasks, combined
with the specification of editable spans, leads our system to more accurately
model the process of iterative text refinement, as evidenced by empirical
results and human evaluations. Our system significantly outperforms previous
baselines on our text revision tasks and other standard text revision tasks,
including grammatical error correction, text simplification, sentence fusion,
and style transfer. Through extensive qualitative and quantitative analysis, we
make vital connections between edit intentions and writing quality, and better
computational modeling of iterative text revisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01378">
<div class="article-summary-box-inner">
<span><p>Pretraining has been shown to scale well with compute, data size and data
diversity. Multitask learning trains on a mixture of supervised datasets and
produces improved performance compared to self-supervised pretraining. Until
now, massively multitask learning required simultaneous access to all datasets
in the mixture and heavy compute resources that are only available to
well-resourced teams.
</p>
<p>In this paper, we propose ColD Fusion, a method that provides the benefits of
multitask learning but leverages distributed computation and requires limited
communication and no sharing of data. Consequentially, ColD Fusion can create a
synergistic loop, where finetuned models can be recycled to continually improve
the pretrained model they are based on. We show that ColD Fusion yields
comparable benefits to multitask pretraining by producing a model that (a)
attains strong performance on all of the datasets it was multitask trained on
and (b) is a better starting point for finetuning on unseen datasets. We find
ColD Fusion outperforms RoBERTa and even previous multitask models.
Specifically, when training and testing on 35 diverse datasets, ColD
Fusion-based model outperforms RoBERTa by 2.45 points in average without any
changes to the architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10936">
<div class="article-summary-box-inner">
<span><p>In today's era of digital misinformation, we are increasingly faced with new
threats posed by video falsification techniques. Such falsifications range from
cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,
sophisticated AI media synthesis methods), which are becoming perceptually
indistinguishable from real videos. To tackle this challenge, we propose a
multi-modal semantic forensic approach to discover clues that go beyond
detecting discrepancies in visual quality, thereby handling both simpler
cheapfakes and visually persuasive deepfakes. In this work, our goal is to
verify that the purported person seen in the video is indeed themselves by
detecting anomalous facial movements corresponding to the spoken words. We
leverage the idea of attribution to learn person-specific biometric patterns
that distinguish a given speaker from others. We use interpretable Action Units
(AUs) to capture a person's face and head movement as opposed to deep CNN
features, and we are the first to use word-conditioned facial motion analysis.
We further demonstrate our method's effectiveness on a range of fakes not seen
in training including those without video manipulation, that were not addressed
in prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06674">
<div class="article-summary-box-inner">
<span><p>Recent improvements in KG-to-text generation are due to additional auxiliary
pre-training tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-training
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show our models are competitive while involving fewer
parameters and no additional pre-training tasks. By formulating the problem as
a framework, we can interchange the various proposed components and begin
interpreting KG-to-text generative models based on the topological and type
information found in a graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking. (arXiv:2205.11245v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11245">
<div class="article-summary-box-inner">
<span><p>This paper describes the PASH participation in TREC 2021 Deep Learning Track.
In the recall stage, we adopt a scheme combining sparse and dense retrieval
method. In the multi-stage ranking phase, point-wise and pair-wise ranking
strategies are used one after another based on model continual pre-trained on
general knowledge and document-level data. Compared to TREC 2020 Deep Learning
Track, we have additionally introduced the generative model T5 to further
enhance the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11401">
<div class="article-summary-box-inner">
<span><p>Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConTextual Masked Auto-Encoder for Dense Passage Retrieval. (arXiv:2208.07670v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07670">
<div class="article-summary-box-inner">
<span><p>Dense passage retrieval aims to retrieve the relevant passages of a query
from a large corpus based on dense representations (i.e., vectors) of the query
and the passages. Recent studies have explored improving pre-trained language
models to boost dense retrieval performance. This paper proposes CoT-MAE
(ConTextual Masked Auto-Encoder), a simple yet effective generative
pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric
encoder-decoder architecture that learns to compress the sentence semantics
into a dense vector through self-supervised and context-supervised masked
auto-encoding. Precisely, self-supervised masked auto-encoding learns to model
the semantics of the tokens inside a text span, and context-supervised masked
auto-encoding learns to model the semantical correlation between the text
spans. We conduct experiments on large-scale passage retrieval benchmarks and
show considerable improvements over strong baselines, demonstrating the high
efficiency of CoT-MAE. Our code is available at
https://github.com/caskcsg/ir/tree/main/cotmae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09723">
<div class="article-summary-box-inner">
<span><p>Textual entailment recognition is one of the basic natural language
understanding(NLU) tasks. Understanding the meaning of sentences is a
prerequisite before applying any natural language processing(NLP) techniques to
automatically recognize the textual entailment. A text entails a hypothesis if
and only if the true value of the hypothesis follows the text. Classical
approaches generally utilize the feature value of each word from word embedding
to represent the sentences. In this paper, we propose a novel approach to
identifying the textual entailment relationship between text and hypothesis,
thereby introducing a new semantic feature focusing on empirical
threshold-based semantic text representation. We employ an element-wise
Manhattan distance vector-based feature that can identify the semantic
entailment relationship between the text-hypothesis pair. We carried out
several experiments on a benchmark entailment classification(SICK-RTE) dataset.
We train several machine learning(ML) algorithms applying both semantic and
lexical features to classify the text-hypothesis pair as entailment, neutral,
or contradiction. Our empirical sentence representation technique enriches the
semantic information of the texts and hypotheses found to be more efficient
than the classical ones. In the end, our approach significantly outperforms
known methods in understanding the meaning of the sentences for the textual
entailment classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye-tracking based classification of Mandarin Chinese readers with and without dyslexia using neural sequence models. (arXiv:2210.09819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09819">
<div class="article-summary-box-inner">
<span><p>Eye movements are known to reflect cognitive processes in reading, and
psychological reading research has shown that eye gaze patterns differ between
readers with and without dyslexia. In recent years, researchers have attempted
to classify readers with dyslexia based on their eye movements using Support
Vector Machines (SVMs). However, these approaches (i) are based on highly
aggregated features averaged over all words read by a participant, thus
disregarding the sequential nature of the eye movements, and (ii) do not
consider the linguistic stimulus and its interaction with the reader's eye
movements. In the present work, we propose two simple sequence models that
process eye movements on the entire stimulus without the need of aggregating
features across the sentence. Additionally, we incorporate the linguistic
stimulus into the model in two ways -- contextualized word embeddings and
manually extracted linguistic features. The models are evaluated on a Mandarin
Chinese dataset containing eye movements from children with and without
dyslexia. Our results show that (i) even for a logographic script such as
Chinese, sequence models are able to classify dyslexia on eye gaze sequences,
reaching state-of-the-art performance, and (ii) incorporating the linguistic
stimulus does not help to improve classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12130">
<div class="article-summary-box-inner">
<span><p>Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14769">
<div class="article-summary-box-inner">
<span><p>Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information. (arXiv:2211.15848v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15848">
<div class="article-summary-box-inner">
<span><p>ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10
billion web pages affiliated with rich information. Its design was influenced
by the need for a high quality, large scale web corpus to support a range of
academic and industry research, for example, in information systems,
retrieval-augmented AI systems, and model pretraining. Compared with earlier
ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of
higher-quality, and aligned with the document distributions in commercial web
search. Besides raw HTML, ClueWeb22 includes rich information about the web
pages provided by industry-standard document understanding systems, including
the visual representation of pages rendered by a web browser, parsed HTML
structure information from a neural network parser, and pre-processed cleaned
document text to lower the barrier to entry. Many of these signals have been
widely used in industry but are available to the research community for the
first time at this scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological Data Analysis for Speech Processing. (arXiv:2211.17223v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.17223">
<div class="article-summary-box-inner">
<span><p>We apply topological data analysis (TDA) to speech classification problems
and to the introspection of a pretrained speech model, HuBERT. To this end, we
introduce a number of topological and algebraic features derived from
Transformer attention maps and embeddings. We show that a simple linear
classifier built on top of such features outperforms a fine-tuned
classification head. In particular, we achieve an improvement of about $9\%$
accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed
feature set reaches a new state of the art performance with accuracy $80.155$.
We also show that topological features are able to reveal functional roles of
speech Transformer heads; e.g., we find the heads capable to distinguish
between pairs of sample sources (natural/synthetic) or voices without any
downstream fine-tuning. Our results demonstrate that TDA is a promising new
approach for speech analysis, especially for tasks that require structural
prediction. Appendices, an introduction to TDA, and other additional materials
are available here - https://topohubert.github.io/speech-topology-webpages/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference of Media Bias and Content Quality Using Natural-Language Processing. (arXiv:2212.00237v1 [physics.soc-ph] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00237">
<div class="article-summary-box-inner">
<span><p>Media bias can significantly impact the formation and development of opinions
and sentiments in a population. It is thus important to study the emergence and
development of partisan media and political polarization. However, it is
challenging to quantitatively infer the ideological positions of media outlets.
In this paper, we present a quantitative framework to infer both political bias
and content quality of media outlets from text, and we illustrate this
framework with empirical experiments with real-world data. We apply a
bidirectional long short-term memory (LSTM) neural network to a data set of
more than 1 million tweets to generate a two-dimensional ideological-bias and
content-quality measurement for each tweet. We then infer a ``media-bias
chart'' of (bias, quality) coordinates for the media outlets by integrating the
(bias, quality) measurements of the tweets of the media outlets. We also apply
a variety of baseline machine-learning methods, such as a naive-Bayes method
and a support-vector machine (SVM), to infer the bias and quality values for
each tweet. All of these baseline approaches are based on a bag-of-words
approach. We find that the LSTM-network approach has the best performance of
the examined methods. Our results illustrate the importance of leveraging word
order into machine-learning methods in text analysis.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-05 23:13:16.161846753 UTC">2022-12-05 23:13:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>