<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-18T01:30:00Z">11-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Question Answering in Slovene. (arXiv:2211.09159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09159">
<div class="article-summary-box-inner">
<span><p>Question answering is one of the most challenging tasks in language
understanding. Most approaches are developed for English, while less-resourced
languages are much less researched. We adapt a successful English
question-answering approach, called UnifiedQA, to the less-resourced Slovene
language. Our adaptation uses the encoder-decoder transformer SloT5 and mT5
models to handle four question-answering formats: yes/no, multiple-choice,
abstractive, and extractive. We use existing Slovene adaptations of four
datasets, and machine translate the MCTest dataset. We show that a general
model can answer questions in different formats at least as well as specialized
models. The results are further improved using cross-lingual transfer from
English. While we produce state-of-the-art results for Slovene, the performance
still lags behind English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09172">
<div class="article-summary-box-inner">
<span><p>While Emotion Recognition in Conversations (ERC) has seen a tremendous
advancement in the last few years, new applications and implementation
scenarios present novel challenges and opportunities. These range from
leveraging the conversational context, speaker and emotion dynamics modelling,
to interpreting common sense expressions, informal language and sarcasm,
addressing challenges of real time ERC and recognizing emotion causes. This
survey starts by introducing ERC, elaborating on the challenges and
opportunities pertaining to this task. It proceeds with a description of the
main emotion taxonomies and methods to deal with subjectivity in annotations.
It then describes Deep Learning methods relevant for ERC, word embeddings, and
elaborates on the use of performance metrics for the task and methods to deal
with the typically unbalanced ERC datasets. This is followed by a description
and benchmark of key ERC works along with comprehensive tables comparing
several works regarding their methods and performance across different
datasets. The survey highlights the advantage of leveraging techniques to
address unbalanced data, the exploration of mixed emotions and the benefits of
incorporating annotation subjectivity in the learning phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Graph-Based Context-Aware Model to Understand Online Conversations. (arXiv:2211.09207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09207">
<div class="article-summary-box-inner">
<span><p>Online forums that allow for participatory engagement between users have been
transformative for the public discussion of many important issues. However,
such conversations can sometimes escalate into full-blown exchanges of hate and
misinformation. Existing approaches in natural language processing (NLP), such
as deep learning models for classification tasks, use as inputs only a single
comment or a pair of comments depending upon whether the task concerns the
inference of properties of the individual comments or the replies between pairs
of comments, respectively. But in online conversations, comments and replies
may be based on external context beyond the immediately relevant information
that is input to the model. Therefore, being aware of the conversations'
surrounding contexts should improve the model's performance for the inference
task at hand.
</p>
<p>We propose GraphNLI, a novel graph-based deep learning architecture that uses
graph walks to incorporate the wider context of a conversation in a principled
manner. Specifically, a graph walk starts from a given comment and samples
"nearby" comments in the same or parallel conversation threads, which results
in additional embeddings that are aggregated together with the initial
comment's embedding. We then use these enriched embeddings for downstream NLP
prediction tasks that are important for online conversations. We evaluate
GraphNLI on two such tasks - polarity prediction and misogynistic hate speech
detection - and found that our model consistently outperforms all relevant
baselines for both tasks. Specifically, GraphNLI with a biased root-seeking
random walk performs with a macro-F1 score of 3 and 6 percentage points better
than the best-performing BERT-based baselines for the polarity prediction and
hate speech detection tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Disfluency Detection, Uh No, Disfluency Generation for the Masses. (arXiv:2211.09235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09235">
<div class="article-summary-box-inner">
<span><p>Existing approaches for disfluency detection typically require the existence
of large annotated datasets. However, current datasets for this task are
limited, suffer from class imbalance, and lack some types of disfluencies that
can be encountered in real-world scenarios. This work proposes LARD, a method
for automatically generating artificial disfluencies from fluent text. LARD can
simulate all the different types of disfluencies (repetitions, replacements and
restarts) based on the reparandum/interregnum annotation scheme. In addition,
it incorporates contextual embeddings into the disfluency generation to produce
realistic context-aware artificial disfluencies. Since the proposed method
requires only fluent text, it can be used directly for training, bypassing the
requirement of annotated disfluent data. Our empirical evaluation demonstrates
that LARD can indeed be effectively used when no or only a few data are
available. Furthermore, our detailed analysis suggests that the proposed method
generates realistic disfluencies and increases the accuracy of existing
disfluency detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-aware Retrieval with Instructions. (arXiv:2211.09260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09260">
<div class="article-summary-box-inner">
<span><p>We study the problem of retrieval with instructions, where users of a
retrieval system explicitly describe their intent along with their queries,
making the system task-aware. We aim to develop a general-purpose task-aware
retrieval systems using multi-task instruction tuning that can follow
human-written instructions to find the best documents for a given query. To
this end, we introduce the first large-scale collection of approximately 40
retrieval datasets with instructions, and present TART, a multi-task retrieval
system trained on the diverse retrieval tasks with instructions. TART shows
strong capabilities to adapt to a new task via instructions and advances the
state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE,
outperforming models up to three times larger. We further introduce a new
evaluation setup to better reflect real-world scenarios, pooling diverse
documents and tasks. In this setup, TART significantly outperforms competitive
baselines, further demonstrating the effectiveness of guiding retrieval with
instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality. (arXiv:2211.09267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09267">
<div class="article-summary-box-inner">
<span><p>Human communication relies on common ground (CG), the mutual knowledge and
beliefs shared by participants, to produce coherent and interesting
conversations. In this paper, we demonstrate that current response generation
(RG) models produce generic and dull responses in dialogues because they act
reflexively, failing to explicitly model CG, both due to the lack of CG in
training data and the standard RG training procedure. We introduce Reflect, a
dataset that annotates dialogues with explicit CG (materialized as inferences
approximating shared knowledge and beliefs) and solicits 9k diverse
human-generated responses each following one common ground. Using Reflect, we
showcase the limitations of current dialogue data and RG models: less than half
of the responses in current data are rated as high quality (sensible, specific,
and interesting) and models trained using this data have even lower quality,
while most Reflect responses are judged high quality. Next, we analyze whether
CG can help models produce better-quality responses by using Reflect CG to
guide RG models. Surprisingly, we find that simply prompting GPT3 to "think"
about CG generates 30% more quality responses, showing promising benefits to
integrating CG into the RG process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Execution-based Evaluation for Data Science Code Generation Models. (arXiv:2211.09374v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09374">
<div class="article-summary-box-inner">
<span><p>Code generation models can benefit data scientists' productivity by
automatically generating code from context and text descriptions. An important
measure of the modeling progress is whether a model can generate code that can
correctly execute to solve the task. However, due to the lack of an evaluation
dataset that directly supports execution-based model evaluation, existing work
relies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model
selection, which can be inaccurate.
</p>
<p>To remedy this, we introduce ExeDS, an evaluation dataset for execution
evaluation for data science code generation tasks. ExeDS contains a set of 534
problems from Jupyter Notebooks, each consisting of code context, task
description, reference program, and the desired execution output. With ExeDS,
we evaluate the execution performance of five state-of-the-art code generation
models that have achieved high surface-form evaluation scores. Our experiments
show that models with high surface-form scores do not necessarily perform well
on execution metrics, and execution-based metrics can better capture model code
generation errors. Source code and data can be found at
https://github.com/Jun-jie-Huang/ExeDS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training with Purpose Preserving Augmentation Improves Few-shot Generative Dialogue State Tracking. (arXiv:2211.09379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09379">
<div class="article-summary-box-inner">
<span><p>In dialogue state tracking (DST), labeling the dataset involves considerable
human labor. We propose a new self-training framework for few-shot generative
DST that utilize unlabeled data. Our self-training method iteratively improves
the model by pseudo labeling and employs Purpose Preserving Augmentation
(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by
approximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen
values compared to baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient Autoregressive Document Retrieval for Fact Verification. (arXiv:2211.09388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09388">
<div class="article-summary-box-inner">
<span><p>Document retrieval is a core component of many knowledge-intensive natural
language processing task formulations such as fact verification and question
answering. Sources of textual knowledge, such as Wikipedia articles, condition
the generation of answers from the models. Recent advances in retrieval use
sequence-to-sequence models to incrementally predict the title of the
appropriate Wikipedia page given a query. However, this method requires
supervision in the form of human annotation to label which Wikipedia pages
contain appropriate context. This paper introduces a distant-supervision method
that does not require any annotation to train autoregressive retrievers that
attain competitive R-Precision and Recall in a zero-shot setting. Furthermore
we show that with task-specific supervised fine-tuning, autoregressive
retrieval performance for two Wikipedia-based fact verification tasks can
approach or even exceed full supervision using less than $1/4$ of the annotated
data indicating possible directions for data-efficient autoregressive
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConNER: Consistency Training for Cross-lingual Named Entity Recognition. (arXiv:2211.09394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09394">
<div class="article-summary-box-inner">
<span><p>Cross-lingual named entity recognition (NER) suffers from data scarcity in
the target languages, especially under zero-shot settings. Existing
translate-train or knowledge distillation methods attempt to bridge the
language gap, but often introduce a high level of noise. To solve this problem,
consistency training methods regularize the model to be robust towards
perturbations on data or hidden states. However, such methods are likely to
violate the consistency hypothesis, or mainly focus on coarse-grain
consistency. We propose ConNER as a novel consistency training framework for
cross-lingual NER, which comprises of: (1) translation-based consistency
training on unlabeled target-language data, and (2) dropoutbased consistency
training on labeled source-language data. ConNER effectively leverages
unlabeled target-language data and alleviates overfitting on the source
language to enhance the cross-lingual adaptability. Experimental results show
our ConNER achieves consistent improvement over various baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain Conversational Question Answering with Historical Answers. (arXiv:2211.09401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09401">
<div class="article-summary-box-inner">
<span><p>Open-domain conversational question answering can be viewed as two tasks:
passage retrieval and conversational question answering, where the former
relies on selecting candidate passages from a large corpus and the latter
requires better understanding of a question with contexts to predict the
answers. This paper proposes ConvADR-QA that leverages historical answers to
boost retrieval performance and further achieves better answering performance.
In our proposed framework, the retrievers use a teacher-student framework to
reduce noises from previous turns. Our experiments on the benchmark dataset,
OR-QuAC, demonstrate that our model outperforms existing baselines in both
extractive and generative reader settings, well justifying the effectiveness of
historical answers for open-domain conversational question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongFNT: Long-form Speech Recognition with Factorized Neural Transducer. (arXiv:2211.09412v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09412">
<div class="article-summary-box-inner">
<span><p>Traditional automatic speech recognition~(ASR) systems usually focus on
individual utterances, without considering long-form speech with useful
historical information, which is more practical in real scenarios. Simply
attending longer transcription history for a vanilla neural transducer model
shows no much gain in our preliminary experiments, since the prediction network
is not a pure language model. This motivates us to leverage the factorized
neural transducer structure, containing a real language model, the vocabulary
predictor. We propose the {LongFNT-Text} architecture, which fuses the
sentence-level long-form features directly with the output of the vocabulary
predictor and then embeds token-level long-form features inside the vocabulary
predictor, with a pre-trained contextual encoder RoBERTa to further boost the
performance. Moreover, we propose the {LongFNT} architecture by extending the
long-form speech to the original speech input and achieve the best performance.
The effectiveness of our LongFNT approach is validated on LibriSpeech and
GigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impaired. (arXiv:2211.09427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09427">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-augmented Machine Reading Comprehension with Auxiliary Tasks. (arXiv:2211.09438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09438">
<div class="article-summary-box-inner">
<span><p>While most successful approaches for machine reading comprehension rely on
single training objective, it is assumed that the encoder layer can learn great
representation through the loss function we define in the predict layer, which
is cross entropy in most of time, in the case that we first use neural networks
to encode the question and paragraph, then directly fuse the encoding result of
them. However, due to the distantly loss backpropagating in reading
comprehension, the encoder layer cannot learn effectively and be directly
supervised. Thus, the encoder layer can not learn the representation well at
any time. Base on this, we propose to inject multi granularity information to
the encoding layer. Experiments demonstrate the effect of adding multi
granularity information to the encoding layer can boost the performance of
machine reading comprehension system. Finally, empirical study shows that our
approach can be applied to many existing MRC models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consultation Checklists: Standardising the Human Evaluation of Medical Note Generation. (arXiv:2211.09455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09455">
<div class="article-summary-box-inner">
<span><p>Evaluating automatically generated text is generally hard due to the
inherently subjective nature of many aspects of the output quality. This
difficulty is compounded in automatic consultation note generation by differing
opinions between medical experts both about which patient statements should be
included in generated notes and about their respective importance in arriving
at a diagnosis. Previous real-world evaluations of note-generation systems saw
substantial disagreement between expert evaluators. In this paper we propose a
protocol that aims to increase objectivity by grounding evaluations in
Consultation Checklists, which are created in a preliminary step and then used
as a common point of reference during quality assessment. We observed good
levels of inter-annotator agreement in a first evaluation study using the
protocol; further, using Consultation Checklists produced in the study as
reference for automatic metrics such as ROUGE or BERTScore improves their
correlation with human judgements compared to using the original human note.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstractive Summarization Guided by Latent Hierarchical Document Structure. (arXiv:2211.09458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09458">
<div class="article-summary-box-inner">
<span><p>Sequential abstractive neural summarizers often do not use the underlying
structure in the input article or dependencies between the input sentences.
This structure is essential to integrate and consolidate information from
different parts of the text. To address this shortcoming, we propose a
hierarchy-aware graph neural network (HierGNN) which captures such dependencies
through three main steps: 1) learning a hierarchical document structure through
a latent structure tree learned by a sparse matrix-tree computation; 2)
propagating sentence information over this structure using a novel
message-passing node propagation mechanism to identify salient information; 3)
using graph-level attention to concentrate the decoder on salient information.
Experiments confirm HierGNN improves strong sequence models such as BART, with
a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further
human evaluation demonstrates that summaries produced by our model are more
relevant and less redundant than the baselines, into which HierGNN is
incorporated. We also find HierGNN synthesizes summaries by fusing multiple
source sentences more, rather than compressing a single source sentence, and
that it processes long inputs more effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation. (arXiv:2211.09495v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09495">
<div class="article-summary-box-inner">
<span><p>Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review. (arXiv:2211.09511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09511">
<div class="article-summary-box-inner">
<span><p>Speech is the fundamental means of communication between humans. The advent
of AI and sophisticated speech technologies have led to the rapid proliferation
of human-to-computer-based interactions, fueled primarily by Automatic Speech
Recognition (ASR) systems. ASR systems normally take human speech in the form
of audio and convert it into words, but for some users, it cannot decode the
speech, and any output text is filled with errors that are incomprehensible to
the human reader. These systems do not work equally for everyone and actually
hinder the productivity of some users. In this paper, we present research that
addresses ASR biases against gender, race, and the sick and disabled, while
exploring studies that propose ASR debiasing techniques for mitigating these
discriminations. We also discuss techniques for designing a more accessible and
inclusive ASR technology. For each approach surveyed, we also provide a summary
of the investigation and methods applied, the ASR systems and corpora used, and
the research findings, and highlight their strengths and/or weaknesses.
Finally, we propose future opportunities for Natural Language Processing
researchers to explore in the next level creation of ASR technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ignore Previous Prompt: Attack Techniques For Language Models. (arXiv:2211.09527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09527">
<div class="article-summary-box-inner">
<span><p>Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at https://github.com/agencyenterprise/PromptInject.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Building Text-To-Speech Systems for the Next Billion Users. (arXiv:2211.09536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09536">
<div class="article-summary-box-inner">
<span><p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Adapter for Text-Video Retrieval. (arXiv:2211.09623v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09623">
<div class="article-summary-box-inner">
<span><p>Text-video retrieval is an important multi-modal learning task, where the
goal is to retrieve the most relevant video for a given text query. Recently,
pre-trained models, e.g., CLIP, show great potential on this task. However, as
pre-trained models are scaling up, fully fine-tuning them on text-video
retrieval datasets has a high risk of overfitting. Moreover, in practice, it
would be costly to train and store a large model for each task. To overcome the
above issues, we present a novel $\textbf{Cross-Modal Adapter}$ for
parameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust
the pre-trained model with a few parameterization layers. However, there are
two notable differences. First, our method is designed for the multi-modal
domain. Secondly, it allows early cross-modal interactions between CLIP's two
encoders. Although surprisingly simple, our approach has three notable
benefits: (1) reduces $\textbf{99.6}\%$ of fine-tuned parameters, and
alleviates the problem of overfitting, (2) saves approximately 30% of training
time, and (3) allows all the pre-trained parameters to be fixed, enabling the
pre-trained model to be shared across datasets. Extensive experiments
demonstrate that, without bells and whistles, it achieves superior or
comparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,
VATEX, ActivityNet, and DiDeMo datasets. The code will be available at
\url{https://github.com/LeapLabTHU/Cross-Modal-Adapter}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyse der Entwicklungstreiber milit\"arischer Schwarmdrohnen durch Natural Language Processing. (arXiv:2211.09680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09680">
<div class="article-summary-box-inner">
<span><p>Military drones are taking an increasingly prominent role in armed conflict,
and the use of multiple drones in a swarm can be useful. Who the drivers of the
research are and what sub-domains exist is analyzed and visually presented in
this research using NLP techniques based on 946 studies. Most research is
conducted in the Western world, led by the United States, the United Kingdom,
and Germany. Through Tf-idf scoring, it is shown that countries have
significant differences in the subdomains studied. Overall, 2019 and 2020 saw
the most works published, with significant interest in military swarm drones as
early as 2008. This study provides a first glimpse into research in this area
and prompts further investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effectiveness of Bidirectional Generative Patent Language Models. (arXiv:2211.09690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09690">
<div class="article-summary-box-inner">
<span><p>Generative patent language models can assist humans to write patent text more
effectively. The question is how to measure effectiveness from a human-centric
perspective and how to improve effectiveness. In this manuscript, a simplified
design of the autocomplete function is proposed to increase effectiveness by
more than 10%. With the new design, the effectiveness of autocomplete can reach
more than 60%, which means that more than 60% of keystrokes can be saved by
autocomplete. Since writing patent text does not necessarily start from the
beginning to the end, a question is whether the generative model can assist a
user no matter where to start writing. To answer the question, the generative
models in this manuscript are pre-trained with training data in both
directions. The generative models become bidirectional. Since text generation
is bidirectional, the calculation of autocomplete effectiveness can be
bidirectional and starts from anywhere in the text. After thorough experiments,
a key finding is that the autocomplete effectiveness of a model for the same
text remains similar no matter where the calculation starts. The finding
indicates that such bidirectional models can assist a user at a similar level,
no matter where the user starts to write.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptCap: Prompt-Guided Task-Aware Image Captioning. (arXiv:2211.09699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09699">
<div class="article-summary-box-inner">
<span><p>Image captioning aims to describe an image with a natural language sentence,
allowing powerful language models to understand images. The framework of
combining image captioning with language models has been successful on various
vision-language tasks. However, an image contains much more information than a
single sentence, leading to underspecification of which visual entities should
be described in the caption sentence. For example, when performing visual
questioning answering (VQA), generic image captions often miss visual details
that are essential for the language model to answer correctly. To address this
challenge, we propose PromptCap, a captioning model that takes a
natural-language prompt to control the contents of the generated caption. The
prompt contains a question that the caption should help to answer, and also
supports taking auxiliary text inputs such as scene text within the image
itself. To finetune a general image caption model for prompt-guided captioning,
we propose a pipeline to synthesize and filter training examples with GPT-3 and
existing VQA datasets. For evaluation, we start with an existing pipeline in
which a language model is prompted with image captions to carry out VQA. With
the same language model, a higher QA accuracy shows that our generated captions
are more relevant to the question prompts. PromptCap outperforms generic
captions by a large margin on a variety of VQA tasks and achieves the
state-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot
experiments on WebQA show that PromptCap generalizes well to unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09710">
<div class="article-summary-box-inner">
<span><p>Midrash collections are complex rabbinic works that consist of text in
multiple languages, which evolved through long processes of unstable oral and
written transmission. Determining the origin of a given passage in such a
compilation is not always straightforward and is often a matter of dispute
among scholars, yet it is essential for scholars' understanding of the passage
and its relationship to other texts in the rabbinic corpus.
</p>
<p>To help solve this problem, we propose a system for classification of
rabbinic literature based on its style, leveraging recently released pretrained
Transformer models for Hebrew. Additionally, we demonstrate how our method can
be applied to uncover lost material from Midrash Tanhuma.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design Considerations For Hypothesis Rejection Modules In Spoken Language Understanding Systems. (arXiv:2211.09711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09711">
<div class="article-summary-box-inner">
<span><p>Spoken Language Understanding (SLU) systems typically consist of a set of
machine learning models that operate in conjunction to produce an SLU
hypothesis. The generated hypothesis is then sent to downstream components for
further action. However, it is desirable to discard an incorrect hypothesis
before sending it downstream. In this work, we present two designs for SLU
hypothesis rejection modules: (i) scheme R1 that performs rejection on domain
specific SLU hypothesis and, (ii) scheme R2 that performs rejection on
hypothesis generated from the overall SLU system. Hypothesis rejection modules
in both schemes reject/accept a hypothesis based on features drawn from the
utterance directed to the SLU system, the associated SLU hypothesis and SLU
confidence score. Our experiments suggest that both the schemes yield similar
results (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with
the best performing systems using all the available features. We argue that
while either of the rejection schemes can be chosen over the other, they carry
some inherent differences which need to be considered while making this choice.
Additionally, we incorporate ASR features in the rejection module (obtaining an
1.9% FRR @ 3.8% FAR) and analyze the improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Numerical Optimizations for Weighted Low-rank Estimation on Language Model. (arXiv:2211.09718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09718">
<div class="article-summary-box-inner">
<span><p>Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Multilingual Models for Medical Transcript Analysis. (arXiv:2211.09722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09722">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) is a novel machine learning approach that allows the
model trainer to access more data samples, by training the model across
multiple decentralized data sources, while data access constraints are in
place. Such trained models can achieve significantly higher performance beyond
what can be done when trained on a single data source. As part of FL's
promises, none of the training data is ever transmitted to any central
location, ensuring that sensitive data remains local and private. These
characteristics make FL perfectly suited for large-scale applications in
healthcare, where a variety of compliance constraints restrict how data may be
handled, processed, and stored. Despite the apparent benefits of federated
learning, the heterogeneity in the local data distributions pose significant
challenges, and such challenges are even more pronounced in the case of
multilingual data providers. In this paper we present a federated learning
system for training a large-scale multi-lingual model suitable for fine-tuning
on downstream tasks such as medical entity tagging. Our work represents one of
the first such production-scale systems, capable of training across multiple
highly heterogeneous data providers, and achieving levels of accuracy that
could not be otherwise achieved by using central training with public data.
Finally, we show that the global model performance can be further improved by a
training step performed locally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Training Can Improve Neural Language Models. (arXiv:2211.09728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09728">
<div class="article-summary-box-inner">
<span><p>While deep learning in the form of recurrent neural networks (RNNs) has
caused a significant improvement in neural language modeling, the fact that
they are extremely prone to overfitting is still a mainly unresolved issue. In
this paper we propose a regularization method based on generative adversarial
networks (GANs) and adversarial training (AT), that can prevent overfitting in
neural language models. Unlike common adversarial training methods such as the
fast gradient sign method (FGSM) that require a second back-propagation through
time, and therefore effectively require at least twice the amount of time for
regular training, the overhead of our method does not exceed more than 20% of
the training of the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered Speech. (arXiv:2211.09731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09731">
<div class="article-summary-box-inner">
<span><p>Stuttering is a speech disorder where the natural flow of speech is
interrupted by blocks, repetitions or prolongations of syllables, words and
phrases. The majority of existing automatic speech recognition (ASR) interfaces
perform poorly on utterances with stutter, mainly due to lack of matched
training data. Synthesis of speech with stutter thus presents an opportunity to
improve ASR for this type of speech. We describe Stutter-TTS, an end-to-end
neural text-to-speech model capable of synthesizing diverse types of stuttering
utterances. We develop a simple, yet effective prosody-control strategy whereby
additional tokens are introduced into source text during training to represent
specific stuttering characteristics. By choosing the position of the stutter
tokens, Stutter-TTS allows word-level control of where stuttering occurs in the
synthesized utterance. We are able to synthesize stutter events with high
accuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By
fine-tuning an ASR model on synthetic stuttered speech we are able to reduce
word error by 5.7% relative on stuttered utterances, with only minor (&lt;0.2%
relative) degradation for fluent utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Logic Explained Networks to Text Classification. (arXiv:2211.09732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09732">
<div class="article-summary-box-inner">
<span><p>Recently, Logic Explained Networks (LENs) have been proposed as
explainable-by-design neural models providing logic explanations for their
predictions. However, these models have only been applied to vision and tabular
data, and they mostly favour the generation of global explanations, while local
ones tend to be noisy and verbose. For these reasons, we propose LENp,
improving local explanations by perturbing input words, and we test it on text
classification. Our results show that (i) LENp provides better local
explanations than LIME in terms of sensitivity and faithfulness, and (ii) logic
explanations are more useful and user-friendly than feature scoring provided by
LIME as attested by a human survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets. (arXiv:2211.09733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09733">
<div class="article-summary-box-inner">
<span><p>The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dynamic Quantization for Transformer Inference. (arXiv:2211.09744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09744">
<div class="article-summary-box-inner">
<span><p>We introduce a novel run-time method for significantly reducing the accuracy
loss associated with quantizing BERT-like models to 8-bit integers. Existing
methods for quantizing models either modify the training procedure,or they
require an additional calibration step to adjust parameters that also requires
a selected held-out dataset. Our method permits taking advantage of
quantization without the need for these adjustments. We present results on
several NLP tasks demonstrating the usefulness of this technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for Incremental Parse States in Autoregressive Language Models. (arXiv:2211.09748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09748">
<div class="article-summary-box-inner">
<span><p>Next-word predictions from autoregressive neural language models show
remarkable sensitivity to syntax. This work evaluates the extent to which this
behavior arises as a result of a learned ability to maintain implicit
representations of incremental syntactic structures. We extend work in
syntactic probing to the incremental setting and present several probes for
extracting incomplete syntactic structure (operationalized through parse states
from a stack-based parser) from autoregressive language models. We find that
our probes can be used to predict model preferences on ambiguous sentence
prefixes and causally intervene on model representations and steer model
behavior. This suggests implicit incremental syntactic inferences underlie
next-word predictions in autoregressive neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09761">
<div class="article-summary-box-inner">
<span><p>Transformers achieve unrivalled performance in modelling language, but remain
inefficient in terms of memory and time complexity. A possible remedy is to
reduce the sequence length in the intermediate layers by pooling fixed-length
segments of tokens. Nevertheless, natural units of meaning, such as words or
phrases, display varying sizes. To address this mismatch, we equip language
models with a dynamic-pooling mechanism, which predicts segment boundaries in
an autoregressive fashion. We compare several methods to infer boundaries,
including end-to-end learning through stochastic re-parameterisation,
supervised learning (based on segmentations from subword tokenizers or spikes
in conditional entropy), as well as linguistically motivated boundaries. We
perform character-level evaluation on texts from multiple datasets and
morphologically diverse languages. The results demonstrate that dynamic
pooling, which jointly segments and models language, is often both faster and
more accurate than vanilla Transformers and fixed-length pooling within the
same computational budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09783">
<div class="article-summary-box-inner">
<span><p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructPix2Pix: Learning to Follow Image Editing Instructions. (arXiv:2211.09800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09800">
<div class="article-summary-box-inner">
<span><p>We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07210">
<div class="article-summary-box-inner">
<span><p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully
applied in text-to-speech (TTS) to synthesize speech for single-language text.
To synthesize speech for multiple languages usually requires multi-lingual
speech from the target speaker. However, it is both laborious and expensive to
collect high-quality multi-lingual TTS data for the target speakers. In this
paper, we proposed to use low-quality code-switched found data from the
non-target speakers to achieve cross-lingual voice cloning for the target
speakers. Experiments show that our proposed method can generate high-quality
code-switched speech in the target voices in terms of both naturalness and
speaker consistency. More importantly, we find that our method can achieve a
comparable result to the state-of-the-art (SOTA) performance in cross-lingual
voice cloning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Pretrained Models of Source Code. (arXiv:2202.08975v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08975">
<div class="article-summary-box-inner">
<span><p>Deep learning models are widely used for solving challenging code processing
tasks, such as code generation or code summarization. Traditionally, a specific
model architecture was carefully built to solve a particular code processing
task. However, recently general pretrained models such as CodeBERT or CodeT5
have been shown to outperform task-specific models in many applications. While
pretrained models are known to learn complex patterns from data, they may fail
to understand some properties of source code. To test diverse aspects of code
understanding, we introduce a set of diagnosting probing tasks. We show that
pretrained models of code indeed contain information about code syntactic
structure and correctness, the notions of identifiers, data flow and
namespaces, and natural language naming. We also investigate how probing
results are affected by using code-specific pretraining objectives, varying the
model size, or finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors. (arXiv:2203.04443v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04443">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is a key attribute for artificial intelligence systems
that need to naturally interact with humans. However, the task definition is
still an open problem due to the inherent ambiguity of emotions. In this paper,
a novel Bayesian training loss based on per-utterance Dirichlet prior
distributions is proposed for verbal emotion recognition, which models the
uncertainty in one-hot labels created when human annotators assign the same
utterance to different emotion classes. An additional metric is used to
evaluate the performance by detection test utterances with high labelling
uncertainty. This removes a major limitation that emotion classification
systems only consider utterances with labels where the majority of annotators
agree on the emotion class. Furthermore, a frequentist approach is studied to
leverage the continuous-valued "soft" labels obtained by averaging the one-hot
labels. We propose a two-branch model structure for emotion classification on a
per-utterance basis, which achieves state-of-the-art classification results on
the widely used IEMOCAP dataset. Based on this, uncertainty estimation
experiments were performed. The best performance in terms of the area under the
precision-recall curve when detecting utterances with high uncertainty was
achieved by interpolating the Bayesian training loss with the Kullback-Leibler
divergence training loss for the soft labels. The generality of the proposed
approach was verified using the MSP-Podcast dataset which yielded the same
pattern of results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-ASC: Implicit Aspect Representation Learning through Auxiliary-Sentence Construction for Sentiment Analysis. (arXiv:2203.11702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11702">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) task aim at associating a piece of
text with a set of aspects and meanwhile infer their respective sentimental
polarities. The state-of-the-art approaches are built upon fine-tuning of
various pre-trained language models. They commonly attempt to learn
aspect-specific representation from the corpus. Unfortunately, the aspect is
often expressed implicitly through a set of representatives and thus renders
implicit mapping process unattainable unless sufficient labeled examples are
available. However, high-quality labeled examples may not be readily available
in real-world scenarios. In this paper, we propose to jointly address aspect
categorization and aspect-based sentiment subtasks in a unified framework.
Specifically, we first introduce a simple but effective mechanism to construct
an auxiliary-sentence for the implicit aspect based on the semantic information
in the corpus. Then, we encourage BERT to learn the aspect-specific
representation in response to the automatically constructed auxiliary-sentence
instead of the aspect itself. Finally, we empirically evaluate the performance
of the proposed solution by a comparative study on real benchmark datasets for
both ABSA and Targeted-ABSA tasks. Our extensive experiments show that it
consistently achieves state-of-the-art performance in terms of aspect
categorization and aspect-based sentiment across all datasets and the
improvement margins are considerable. The code of BERT-ASC is available in
GitHub: https://github.com/amurtadha/BERT-ASC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v9 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition (ASR) system would not help since
they are pre-trained on voices that differ from children's in terms of
frequency and amplitude. Because most of these are pre-trained with data in a
specific range of amplitude, their objectives do not make them ready for voices
in different amplitudes. To overcome this issue, we added a new objective to
the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch
(RFP). In addition, we used our newly introduced dataset to fine-tune our model
for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using
masking in concatenation with RFP outperforms the masking objective of Wav2Vec
2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER
of 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our
novel methodology produces positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03783">
<div class="article-summary-box-inner">
<span><p>In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en-&gt;{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08110">
<div class="article-summary-box-inner">
<span><p>English pretrained language models, which make up the backbone of many modern
NLP systems, require huge amounts of unlabeled training data. These models are
generally presented as being trained only on English text but have been found
to transfer surprisingly well to other languages. We investigate this
phenomenon and find that common English pretraining corpora actually contain
significant amounts of non-English text: even when less than 1% of data is not
English (well within the error rate of strong language classifiers), this leads
to hundreds of millions of foreign language tokens in large-scale datasets. We
then demonstrate that even these small percentages of non-English data
facilitate cross-lingual transfer for models trained on them, with target
language performance strongly correlated to the amount of in-language data seen
during pretraining. In light of these findings, we argue that no model is truly
monolingual when pretrained at scale, which should be considered when
evaluating cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Language Feedback. (arXiv:2204.14146v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14146">
<div class="article-summary-box-inner">
<span><p>Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05055">
<div class="article-summary-box-inner">
<span><p>Large transformer-based models are able to perform in-context few-shot
learning, without being explicitly trained for it. This observation raises the
question: what aspects of the training regime lead to this emergent behavior?
Here, we show that this behavior is driven by the distributions of the training
data itself. In-context learning emerges when the training data exhibits
particular distributional properties such as burstiness (items appear in
clusters rather than being uniformly distributed over time) and having large
numbers of rarely occurring classes. In-context learning also emerges more
strongly when item meanings or interpretations are dynamic rather than fixed.
These properties are exemplified by natural language, but are also inherent to
naturalistic data in a wide range of other domains. They also depart
significantly from the uniform, i.i.d. training distributions typically used
for standard supervised learning. In our initial experiments, we found that
in-context learning traded off against more conventional weight-based learning,
and models were unable to achieve both simultaneously. However, our later
experiments uncovered that the two modes of learning could co-exist in a single
model when it was trained on data following a skewed Zipfian distribution --
another common property of naturalistic data, including language. In further
experiments, we found that naturalistic data distributions were only able to
elicit in-context learning in transformers, and not in recurrent models. In
sum, our findings indicate how the transformer architecture works together with
particular properties of the training data to drive the intriguing emergent
in-context learning behaviour of large language models, and how future work
might encourage both in-context and in-weights learning in domains beyond
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of Hierarchical Conversation Structure. (arXiv:2205.12244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12244">
<div class="article-summary-box-inner">
<span><p>Human conversations can evolve in many different ways, creating challenges
for automatic understanding and summarization. Goal-oriented conversations
often have meaningful sub-dialogue structure, but it can be highly
domain-dependent. This work introduces an unsupervised approach to learning
hierarchical conversation structure, including turn and sub-dialogue segment
labels, corresponding roughly to dialogue acts and sub-tasks, respectively. The
decoded structure is shown to be useful in enhancing neural models of language
for three conversation-level understanding tasks. Further, the learned
finite-state sub-dialogue network is made interpretable through automatic
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quark: Controllable Text Generation with Reinforced Unlearning. (arXiv:2205.13636v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13636">
<div class="article-summary-box-inner">
<span><p>Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NatiQ: An End-to-end Text-to-Speech System for Arabic. (arXiv:2206.07373v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07373">
<div class="article-summary-box-inner">
<span><p>NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer
uses an encoder-decoder architecture with attention. We used both
tacotron-based models (tacotron-1 and tacotron-2) and the faster transformer
model for generating mel-spectrograms from characters. We concatenated
Tacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and
ESPnet transformer with the parallel wavegan vocoder to synthesize waveforms
from the spectrograms. We used in-house speech data for two voices: 1) neutral
male "Hamza"- narrating general content and news, and 2) expressive female
"Amina"- narrating children story books to train our models. Our best systems
achieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and
Hamza respectively. The objective evaluation of the systems using word and
character error rate (WER and CER) as well as the response time measured by
real-time factor favored the end-to-end architecture ESPnet. NatiQ demo is
available on-line at https://tts.qcri.org
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not Cheating on the Turing Test: Towards Grounded Language Learning in Artificial Intelligence. (arXiv:2206.14672v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14672">
<div class="article-summary-box-inner">
<span><p>Recent hype surrounding the increasing sophistication of language processing
models has renewed optimism regarding machines achieving a human-like command
of natural language. Research in the area of natural language understanding
(NLU) in artificial intelligence claims to have been making great strides in
this area, however, the lack of conceptual clarity/consistency in how
'understanding' is used in this and other disciplines makes it difficult to
discern how close we actually are. In this interdisciplinary research thesis, I
integrate insights from cognitive science/psychology, philosophy of mind, and
cognitive linguistics, and evaluate it against a critical review of current
approaches in NLU to explore the basic requirements--and remaining
challenges--for developing artificially intelligent systems with human-like
capacities for language use and comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems. (arXiv:2208.08191v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08191">
<div class="article-summary-box-inner">
<span><p>Vision-Transformers are widely used in various vision tasks. Meanwhile, there
is another line of works starting with the MLP-mixer trying to achieve similar
performance using mlp-based architectures. Interestingly, until now those
mlp-based architectures have not been adapted for NLP tasks. Additionally,
until now, mlp-based architectures have failed to achieve state-of-the-art
performance in vision tasks. In this paper, we analyze the expressive power of
mlp-based architectures in modeling dependencies between multiple different
inputs simultaneously, and show an exponential gap between the attention and
the mlp-based mechanisms. Our results suggest a theoretical explanation for the
mlp inability to compete with attention-based mechanisms in NLP problems, they
also suggest that the performance gap in vision tasks may be due to the mlp
relative weakness in modeling dependencies between multiple different
locations, and that combining smart input permutations with mlp architectures
may not be enough to close the performance gap alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Image Generation: Leaving no Language Behind. (arXiv:2208.09333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09333">
<div class="article-summary-box-inner">
<span><p>One of the latest applications of Artificial Intelligence (AI) is to generate
images from natural language descriptions. These generators are now becoming
available and achieve impressive results that have been used for example in the
front cover of magazines. As the input to the generators is in the form of a
natural language text, a question that arises immediately is how these models
behave when the input is written in different languages. In this paper we
perform an initial exploration of how the performance of three popular
text-to-image generators depends on the language. The results show that there
is a significant performance degradation when using languages other than
English, especially for languages that are not widely used. This observation
leads us to discuss different alternatives on how text-to-image generators can
be improved so that performance is consistent across different languages. This
is fundamental to ensure that this new technology can be used by non-native
English speakers and to preserve linguistic diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dodging the Data Bottleneck: Automatic Subtitling with Automatically Segmented ST Corpora. (arXiv:2209.10608v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10608">
<div class="article-summary-box-inner">
<span><p>Speech translation for subtitling (SubST) is the task of automatically
translating speech data into well-formed subtitles by inserting subtitle breaks
compliant to specific displaying guidelines. Similar to speech translation
(ST), model training requires parallel data comprising audio inputs paired with
their textual translations. In SubST, however, the text has to be also
annotated with subtitle breaks. So far, this requirement has represented a
bottleneck for system development, as confirmed by the dearth of publicly
available SubST corpora. To fill this gap, we propose a method to convert
existing ST corpora into SubST resources without human intervention. We build a
segmenter model that automatically segments texts into proper subtitles by
exploiting audio and text in a multimodal fashion, achieving high segmentation
quality in zero-shot conditions. Comparative experiments with SubST systems
respectively trained on manual and automatic segmentations result in similar
performance, showing the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparency Helps Reveal When Language Models Learn Meaning. (arXiv:2210.07468v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07468">
<div class="article-summary-box-inner">
<span><p>Many current NLP systems are built from language models trained to optimize
unsupervised objectives on large amounts of raw text. Under what conditions
might such a procedure acquire meaning? Our systematic experiments with
synthetic data reveal that, with languages where all expressions have
context-independent denotations (i.e., languages with strong transparency),
both autoregressive and masked language models successfully learn to emulate
semantic relations between expressions. However, when denotations are changed
to be context-dependent with the language otherwise unmodified, this ability
degrades. Turning to natural language, our experiments with a specific
phenomenon -- referential opacity -- add to the growing body of evidence that
current language models do not well-represent natural language semantics. We
show this failure relates to the context-dependent nature of natural language
form-meaning mappings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Word Meaning Disambiguation using TimeLMs. (arXiv:2210.08207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08207">
<div class="article-summary-box-inner">
<span><p>Meaning of words constantly changes given the events in modern civilization.
Large Language Models use word embeddings, which are often static and thus
cannot cope with this semantic change. Thus,it is important to resolve
ambiguity in word meanings. This paper is an effort in this direction, where we
explore methods for word sense disambiguation for the EvoNLP shared task. We
conduct rigorous ablations for two solutions to this problem. We see that an
approach using time-aware language models helps this task. Furthermore, we
explore possible future directions to this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models. (arXiv:2210.11670v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11670">
<div class="article-summary-box-inner">
<span><p>This paper describes the Stevens Institute of Technology's submission for the
WMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task
consisted of two subtasks, subtask $1$ Hindi/English to Hinglish and subtask
$2$ Hinglish to English translation. Our findings lie in the improvements made
through the use of large pre-trained multilingual NMT models and in-domain
datasets, as well as back-translation and ensemble techniques. The translation
output is automatically evaluated against the reference translations using
ROUGE-L and WER. Our system achieves the $1^{st}$ position on subtask $2$
according to ROUGE-L, WER, and human evaluation, $1^{st}$ position on subtask
$1$ according to WER and human evaluation, and $3^{rd}$ position on subtask $1$
with respect to ROUGE-L metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Structured Prediction with Language Models. (arXiv:2210.14698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14698">
<div class="article-summary-box-inner">
<span><p>Recent years have seen a paradigm shift in NLP towards using pretrained
language models ({PLM}) for a wide range of tasks.
</p>
<p>However, there are many difficult design decisions to represent structures
(e.g. tagged text, coreference chains) in a way such that they can be captured
by PLMs.
</p>
<p>Prior work on structured prediction with PLMs typically flattens the
structured output into a sequence, which limits the quality of structural
information being learned and leads to inferior performance compared to classic
discriminative models.
</p>
<p>In this work, we describe an approach to model structures as sequences of
actions in an autoregressive manner with PLMs, allowing in-structure
dependencies to be learned without any loss.
</p>
<p>Our approach achieves the new state-of-the-art on all the structured
prediction tasks we looked at, namely, named entity recognition, end-to-end
relation extraction, and coreference resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chronic pain patient narratives allow for the estimation of current pain intensity. (arXiv:2210.17473v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17473">
<div class="article-summary-box-inner">
<span><p>Chronic pain is a multi-dimensional experience, and pain intensity plays an
important part, impacting the patients emotional balance, psychology, and
behaviour. Standard self-reporting tools, such as the Visual Analogue Scale for
pain, fail to capture this burden. Moreover, this type of tools is susceptible
to a degree of subjectivity, dependent on the patients clear understanding of
how to use it, social biases, and their ability to translate a complex
experience to a scale. To overcome these and other self-reporting challenges,
pain intensity estimation has been previously studied based on facial
expressions, electroencephalograms, brain imaging, and autonomic features.
However, to the best of our knowledge, it has never been attempted to base this
estimation on the patient narratives of the personal experience of chronic
pain, which is what we propose in this work. Indeed, in the clinical assessment
and management of chronic pain, verbal communication is essential to convey
information to physicians that would otherwise not be easily accessible through
standard reporting tools, since language, sociocultural, and psychosocial
variables are intertwined. We show that language features from patient
narratives indeed convey information relevant for pain intensity estimation,
and that our computational models can take advantage of that. Specifically, our
results show that patients with mild pain focus more on the use of verbs,
whilst moderate and severe pain patients focus on adverbs, and nouns and
adjectives, respectively, and that these differences allow for the distinction
between these three pain classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge-Enhanced Pre-trained Language Models. (arXiv:2211.05994v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05994">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) which are trained on large text corpus via
self-supervised learning method, have yielded promising performance on various
tasks in Natural Language Processing (NLP). However, though PLMs with huge
parameters can effectively possess rich knowledge learned from massive training
text and benefit downstream tasks at the fine-tuning stage, they still have
some limitations such as poor reasoning ability due to the lack of external
knowledge. Research has been dedicated to incorporating knowledge into PLMs to
tackle these issues. In this paper, we present a comprehensive review of
Knowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear
insight into this thriving field. We introduce appropriate taxonomies
respectively for Natural Language Understanding (NLU) and Natural Language
Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide
the types of knowledge into four categories: linguistic knowledge, text
knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are
categorized into KG-based and retrieval-based methods. Finally, we point out
some promising future directions of KE-PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07443">
<div class="article-summary-box-inner">
<span><p>Task-oriented semantic parsing is increasingly being used in user-facing
applications, making measuring the calibration of parsing models especially
important. We examine the calibration characteristics of six models across
three model families on two common English semantic parsing datasets, finding
that many models are reasonably well-calibrated and that there is a trade-off
between calibration and performance. Based on confidence scores across three
models, we propose and release new challenge splits of the two datasets we
examine. We then illustrate the ways a calibrated model can be useful in
balancing common trade-offs in task-oriented parsing. In a simulated
annotator-in-the-loop experiment, we show that using model confidence allows us
to improve performance by 9.6% (absolute) with interactions on only 2.2% of
tokens. Using sequence-level confidence scores, we then examine how we can
optimize trade-off between a parser's usability and safety. We show that
confidence-based thresholding can reduce the number of incorrect low-confidence
programs executed by 76%; however, this comes at a cost to usability. We
propose the DidYouMean system which balances usability and safety. We conclude
by calling for calibration to be included in the evaluation of semantic parsing
systems, and release a library for computing calibration metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches. (arXiv:2211.08029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08029">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is one of the machine learning applications which can be
done using text, speech, or image data gathered from social media spaces.
Detecting emotion can help us in different fields, including opinion mining.
With the spread of social media, different platforms like Twitter have become
data sources, and the language used in these platforms is informal, making the
emotion detection task difficult. EmoPars and ArmanEmo are two new
human-labeled emotion datasets for the Persian language. These datasets,
especially EmoPars, are suffering from inequality between several samples
between two classes. In this paper, we evaluate EmoPars and compare them with
ArmanEmo. Throughout this analysis, we use data augmentation techniques, data
re-sampling, and class-weights with Transformer-based Pretrained Language
Models(PLMs) to handle the imbalance problem of these datasets. Moreover,
feature selection is used to enhance the models' performance by emphasizing the
text's specific features. In addition, we provide a new policy for selecting
data from EmoPars, which selects the high-confidence samples; as a result, the
model does not see samples that do not have specific emotion during training.
Our model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and
EmoPars, respectively, which are new state-of-the-art results in these
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An FNet based Auto Encoder for Long Sequence News Story Generation. (arXiv:2211.08295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08295">
<div class="article-summary-box-inner">
<span><p>In this paper, we design an auto encoder based off of Google's FNet
Architecture in order to generate text from a subset of news stories contained
in Google's C4 dataset. We discuss previous attempts and methods to generate
text from autoencoders and non LLM Models. FNET poses multiple advantages to
BERT based encoders in the realm of efficiency which train 80% faster on GPUs
and 70% faster on TPUs. We then compare outputs of how this autencoder perfroms
on different epochs. Finally, we analyze what outputs the encoder produces with
different seed text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08447">
<div class="article-summary-box-inner">
<span><p>The goal of sexism detection is to mitigate negative online content targeting
certain gender groups of people. However, the limited availability of labeled
sexism-related datasets makes it problematic to identify online sexism for
low-resource languages. In this paper, we address the task of automatic sexism
detection in social media for one low-resource language -- Chinese. Rather than
collecting new sexism data or building cross-lingual transfer learning models,
we develop a cross-lingual domain-aware semantic specialisation system in order
to make the most of existing data. Semantic specialisation is a technique for
retrofitting pre-trained distributional word vectors by integrating external
linguistic knowledge (such as lexico-semantic relations) into the specialised
feature space. To do this, we leverage semantic resources for sexism from a
high-resource language (English) to specialise pre-trained word vectors in the
target language (Chinese) to inject domain knowledge. We demonstrate the
benefit of our sexist word embeddings (SexWEs) specialised by our framework via
intrinsic evaluation of word similarity and extrinsic evaluation of sexism
detection. Compared with other specialisation approaches and Chinese baseline
word vectors, our SexWEs shows an average score improvement of 0.033 and 0.064
in both intrinsic and extrinsic evaluations, respectively. The ablative results
and visualisation of SexWEs also prove the effectiveness of our framework on
retrofitting word vectors in low-resource languages. Our code and
sexism-related word vectors will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Carriers of the Diffuse Interstellar Bands Across Disciplines, using Natural Language Processing. (arXiv:2211.08513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08513">
<div class="article-summary-box-inner">
<span><p>The explosion of scientific publications overloads researchers with
information. This is even more dramatic for interdisciplinary studies, where
several fields need to be explored. A tool to help researchers overcome this is
Natural Language Processing (NLP): a machine-learning (ML) technique that
allows scientists to automatically synthesize information from many articles.
As a practical example, we have used NLP to conduct an interdisciplinary search
for compounds that could be carriers for Diffuse Interstellar Bands (DIBs), a
long-standing open question in astrophysics. We have trained a NLP model on a
corpus of 1.5 million cross-domain articles in open access, and fine-tuned this
model with a corpus of astrophysical publications about DIBs. Our analysis
points us toward several molecules, studied primarily in biology, having
transitions at the wavelengths of several DIBs and composed of abundant
interstellar atoms. Several of these molecules contain chromophores, small
molecular groups responsible for the molecule's colour, that could be promising
candidate carriers. Identifying viable carriers demonstrates the value of using
NLP to tackle open scientific questions, in an interdisciplinary manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lesion Guided Explainable Few Weak-shot Medical Report Generation. (arXiv:2211.08732v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08732">
<div class="article-summary-box-inner">
<span><p>Medical images are widely used in clinical practice for diagnosis.
Automatically generating interpretable medical reports can reduce radiologists'
burden and facilitate timely care. However, most existing approaches to
automatic report generation require sufficient labeled data for training. In
addition, the learned model can only generate reports for the training classes,
lacking the ability to adapt to previously unseen novel diseases. To this end,
we propose a lesion guided explainable few weak-shot medical report generation
framework that learns correlation between seen and novel classes through visual
and semantic feature alignment, aiming to generate medical reports for diseases
not observed in training. It integrates a lesion-centric feature extractor and
a Transformer-based report generation module. Concretely, the lesion-centric
feature extractor detects the abnormal regions and learns correlations between
seen and novel classes with multi-view (visual and lexical) embeddings. Then,
features of the detected regions and corresponding embeddings are concatenated
as multi-view input to the report generation module for explainable report
generation, including text descriptions and corresponding abnormal regions
detected in the images. We conduct experiments on FFA-IR, a dataset providing
explainable annotations, showing that our framework outperforms others on
report generation for novel diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08481">
<div class="article-summary-box-inner">
<span><p>Visual grounding, i.e., localizing objects in images according to natural
language queries, is an important topic in visual language understanding. The
most effective approaches for this task are based on deep learning, which
generally require expensive manually labeled image-query or patch-query pairs.
To eliminate the heavy dependence on human annotations, we present a novel
method, named Pseudo-Q, to automatically generate pseudo language queries for
supervised training. Our method leverages an off-the-shelf object detector to
identify visual objects from unlabeled images, and then language queries for
these objects are obtained in an unsupervised fashion with a pseudo-query
generation module. Then, we design a task-related query prompt module to
specifically tailor generated pseudo language queries for visual grounding
tasks. Further, in order to fully capture the contextual relationships between
images and language queries, we develop a visual-language model equipped with
multi-level cross-modality attention mechanism. Extensive experimental results
demonstrate that our method has two notable benefits: (1) it can reduce human
annotation costs significantly, e.g., 31% on RefCOCO without degrading original
model's performance under the fully supervised setting, and (2) without bells
and whistles, it achieves superior or comparable performance compared to
state-of-the-art weakly-supervised visual grounding methods on all the five
datasets we have experimented. Code is available at
https://github.com/LeapLabTHU/Pseudo-Q.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-20 23:14:57.494782563 UTC">2022-11-20 23:14:57 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>