<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-19T01:30:00Z">10-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Bidirectional Language-Knowledge Graph Pretraining. (arXiv:2210.09338v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09338">
<div class="article-summary-box-inner">
<span><p>Pretraining a language model (LM) on text has been shown to help various
downstream NLP tasks. Recent works show that a knowledge graph (KG) can
complement text data, offering structured background knowledge that provides a
useful scaffold for reasoning. However, these works are not pretrained to learn
a deep fusion of the two modalities at scale, limiting the potential to acquire
fully joint representations of text and KG. Here we propose DRAGON (Deep
Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach
to pretraining a deeply joint language-knowledge foundation model from text and
KG at scale. Specifically, our model takes pairs of text segments and relevant
KG subgraphs as input and bidirectionally fuses information from both
modalities. We pretrain this model by unifying two self-supervised reasoning
tasks, masked language modeling and KG link prediction. DRAGON outperforms
existing LM and LM+KG models on diverse downstream tasks including question
answering across general and biomedical domains, with +5% absolute gain on
average. In particular, DRAGON achieves notable performance on complex
reasoning about language and knowledge (+10% on questions involving long
contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and
RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our
code and trained models are available at
https://github.com/michiyasunaga/dragon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Knowledge via Neighborhood-Aware Optimal Transport for Low-Resource Hate Speech Detection. (arXiv:2210.09340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09340">
<div class="article-summary-box-inner">
<span><p>The concerning rise of hateful content on online platforms has increased the
attention towards automatic hate speech detection, commonly formulated as a
supervised classification task. State-of-the-art deep learning-based approaches
usually require a substantial amount of labeled resources for training.
However, annotating hate speech resources is expensive, time-consuming, and
often harmful to the annotators. This creates a pressing need to transfer
knowledge from the existing labeled resources to low-resource hate speech
corpora with the goal of improving system performance. For this,
neighborhood-based frameworks have been shown to be effective. However, they
have limited flexibility. In our paper, we propose a novel training strategy
that allows flexible modeling of the relative proximity of neighbors retrieved
from a resource-rich corpus to learn the amount of transfer. In particular, we
incorporate neighborhood information with Optimal Transport, which permits
exploiting the geometry of the data embedding space. By aligning the joint
embedding and label distributions of neighbors, we demonstrate substantial
improvements over strong baselines, in low-resource scenarios, on different
publicly available hate speech corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossRE: A Cross-Domain Dataset for Relation Extraction. (arXiv:2210.09345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09345">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) has attracted increasing attention, but current RE
evaluation is limited to in-domain evaluation setups. Little is known on how
well a RE system fares in challenging, but realistic out-of-distribution
evaluation setups. To address this gap, we propose CrossRE, a new,
freely-available cross-domain benchmark for RE, which comprises six distinct
text domains and includes multi-label annotations. An additional innovation is
that we release meta-data collected during annotation, to include explanations
and flags of difficult instances. We provide an empirical evaluation with a
state-of-the-art model for relation classification. As the meta-data enables us
to shed new light on the state-of-the-art model, we provide a comprehensive
analysis on the impact of difficult cases and find correlations between model
and human annotations. Overall, our empirical investigation highlights the
difficulty of cross-domain RE. We release our dataset, to spur more research in
this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Potrika: Raw and Balanced Newspaper Datasets in the Bangla Language with Eight Topics and Five Attributes. (arXiv:2210.09389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09389">
<div class="article-summary-box-inner">
<span><p>Knowledge is central to human and scientific developments. Natural Language
Processing (NLP) allows automated analysis and creation of knowledge. Data is a
crucial NLP and machine learning ingredient. The scarcity of open datasets is a
well-known problem in machine and deep learning research. This is very much the
case for textual NLP datasets in English and other major world languages. For
the Bangla language, the situation is even more challenging and the number of
large datasets for NLP research is practically nil. We hereby present Potrika,
a large single-label Bangla news article textual dataset curated for NLP
research from six popular online news portals in Bangladesh (Jugantor,
Jaijaidin, Ittefaq, Kaler Kontho, Inqilab, and Somoyer Alo) for the period
2014-2020. The articles are classified into eight distinct categories
(National, Sports, International, Entertainment, Economy, Education, Politics,
and Science \&amp; Technology) providing five attributes (News Article, Category,
Headline, Publication Date, and Newspaper Source). The raw dataset contains
185.51 million words and 12.57 million sentences contained in 664,880 news
articles. Moreover, using NLP augmentation techniques, we create from the raw
(unbalanced) dataset another (balanced) dataset comprising 320,000 news
articles with 40,000 articles in each of the eight news categories. Potrika
contains both the datasets (raw and balanced) to suit a wide range of NLP
research. By far, to the best of our knowledge, Potrika is the largest and the
most extensive dataset for news classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Idiosyncratic Responses to Music. (arXiv:2210.09396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09396">
<div class="article-summary-box-inner">
<span><p>Affective responses to music are highly personal. Despite consensus that
idiosyncratic factors play a key role in regulating how listeners emotionally
respond to music, precisely measuring the marginal effects of these variables
has proved challenging. To address this gap, we develop computational methods
to measure affective responses to music from over 403M listener comments on a
Chinese social music platform. Building on studies from music psychology in
systematic and quasi-causal analyses, we test for musical, lyrical, contextual,
demographic, and mental health effects that drive listener affective responses.
Finally, motivated by the social phenomenon known as w\v{a}ng-y\`i-y\'un, we
identify influencing factors of platform user self-disclosures, the social
support they receive, and notable differences in discloser user activity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Text Detection: Limitations and Opportunities. (arXiv:2210.09421v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09421">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative models for language have enabled the creation
of convincing synthetic text or deepfake text. Prior work has demonstrated the
potential for misuse of deepfake text to mislead content consumers. Therefore,
deepfake text detection, the task of discriminating between human and
machine-generated text, is becoming increasingly critical. Several defenses
have been proposed for deepfake text detection. However, we lack a thorough
understanding of their real-world applicability. In this paper, we collect
deepfake text from 4 online services powered by Transformer-based tools to
evaluate the generalization ability of the defenses on content in the wild. We
develop several low-cost adversarial attacks, and investigate the robustness of
existing defenses against an adaptive attacker. We find that many defenses show
significant degradation in performance under our evaluation scenarios compared
to their original claimed performance. Our evaluation shows that tapping into
the semantic information in the text content is a promising approach for
improving the robustness and generalization performance of deepfake text
detection schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization. (arXiv:2210.09428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09428">
<div class="article-summary-box-inner">
<span><p>We present Expected Statistic Regularization (ESR), a novel regularization
technique that utilizes low-order multi-task structural statistics to shape
model distributions for semi-supervised learning on low-resource datasets. We
study ESR in the context of cross-lingual transfer for syntactic analysis (POS
tagging and labeled dependency parsing) and present several classes of
low-order statistic functions that bear on model behavior. Experimentally, we
evaluate the proposed statistics with ESR for unsupervised transfer on 5
diverse target languages and show that all statistics, when estimated
accurately, yield improvements to both POS and LAS, with the best statistic
improving POS by +7.0 and LAS by +8.5 on average. We also present
semi-supervised transfer and learning curve experiments that show ESR provides
significant gains over strong cross-lingual-transfer-plus-fine-tuning baselines
for modest amounts of label data. These results indicate that ESR is a
promising and complementary approach to model-transfer approaches for
cross-lingual parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling Emotion Dynamics in Song Lyrics with State Space Models. (arXiv:2210.09434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09434">
<div class="article-summary-box-inner">
<span><p>Most previous work in music emotion recognition assumes a single or a few
song-level labels for the whole song. While it is known that different emotions
can vary in intensity within a song, annotated data for this setup is scarce
and difficult to obtain. In this work, we propose a method to predict emotion
dynamics in song lyrics without song-level supervision. We frame each song as a
time series and employ a State Space Model (SSM), combining a sentence-level
emotion predictor with an Expectation-Maximization (EM) procedure to generate
the full emotion dynamics. Our experiments show that applying our method
consistently improves the performance of sentence-level baselines without
requiring any annotated songs, making it ideal for limited training data
scenarios. Further analysis through case studies shows the benefits of our
method while also indicating the limitations and pointing to future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09440">
<div class="article-summary-box-inner">
<span><p>Processing information locked within clinical health records is a challenging
task that remains an active area of research in biomedical NLP. In this work,
we evaluate a broad set of machine learning techniques ranging from simple RNNs
to specialised transformers such as BioBERT on a dataset containing clinical
notes along with a set of annotations indicating whether a sample is
cancer-related or not.
</p>
<p>Furthermore, we specifically employ efficient fine-tuning methods from NLP,
namely, bottleneck adapters and prompt tuning, to adapt the models to our
specialised task. Our evaluations suggest that fine-tuning a frozen BERT model
pre-trained on natural language and with bottleneck adapters outperforms all
other strategies, including full fine-tuning of the specialised BioBERT model.
Based on our findings, we suggest that using bottleneck adapters in
low-resource situations with limited access to labelled data or processing
capacity could be a viable strategy in biomedical text mining. The code used in
the experiments are going to be made available at
https://github.com/omidrohanian/bottleneck-adapters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-granularity Argument Mining in Legal Texts. (arXiv:2210.09472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09472">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore legal argument mining using multiple levels of
granularity. Argument mining has usually been conceptualized as a sentence
classification problem. In this work, we conceptualize argument mining as a
token-level (i.e., word-level) classification problem. We use a Longformer
model to classify the tokens. Results show that token-level text classification
identifies certain legal argument elements more accurately than sentence-level
text classification. Token-level classification also provides greater
flexibility to analyze legal texts and to gain more insight into what the model
focuses on when processing a large amount of input data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Non-dialogue Summaries for Dialogue Summarization. (arXiv:2210.09474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09474">
<div class="article-summary-box-inner">
<span><p>To mitigate the lack of diverse dialogue summarization datasets in academia,
we present methods to utilize non-dialogue summarization data for enhancing
dialogue summarization systems. We apply transformations to document
summarization data pairs to create training data that better befit dialogue
summarization. The suggested transformations also retain desirable properties
of non-dialogue datasets, such as improved faithfulness to the source text. We
conduct extensive experiments across both English and Korean to verify our
approach. Although absolute gains in ROUGE naturally plateau as more dialogue
summarization samples are introduced, utilizing non-dialogue data for training
significantly improves summarization performance in zero- and few-shot settings
and enhances faithfulness across all training regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematicity in GPT-3's Interpretation of Novel English Noun Compounds. (arXiv:2210.09492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09492">
<div class="article-summary-box-inner">
<span><p>Levin et al. (2019) show experimentally that the interpretations of novel
English noun compounds (e.g., stew skillet), while not fully compositional, are
highly predictable based on whether the modifier and head refer to artifacts or
natural kinds. Is the large language model GPT-3 governed by the same
interpretive principles? To address this question, we first compare Levin et
al.'s experimental data with GPT-3 generations, finding a high degree of
similarity. However, this evidence is consistent with GPT3 reasoning only about
specific lexical items rather than the more abstract conceptual categories of
Levin et al.'s theory. To probe more deeply, we construct prompts that require
the relevant kind of conceptual reasoning. Here, we fail to find convincing
evidence that GPT-3 is reasoning about more than just individual lexical items.
These results highlight the importance of controlling for low-level
distributional regularities when assessing whether a large language model
latently encodes a deeper theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalization of CTC Speech Recognition Models. (arXiv:2210.09510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09510">
<div class="article-summary-box-inner">
<span><p>End-to-end speech recognition models trained using joint Connectionist
Temporal Classification (CTC)-Attention loss have gained popularity recently.
In these models, a non-autoregressive CTC decoder is often used at inference
time due to its speed and simplicity. However, such models are hard to
personalize because of their conditional independence assumption that prevents
output tokens from previous time steps to influence future predictions. To
tackle this, we propose a novel two-way approach that first biases the encoder
with attention over a predefined list of rare long-tail and out-of-vocabulary
(OOV) words and then uses dynamic boosting and phone alignment network during
decoding to further bias the subword predictions. We evaluate our approach on
open-source VoxPopuli and in-house medical datasets to showcase a 60%
improvement in F1 score on domain-specific rare words over a strong CTC
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team Flow at DRC2022: Pipeline System for Travel Destination Recommendation Task in Spoken Dialogue. (arXiv:2210.09518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09518">
<div class="article-summary-box-inner">
<span><p>To improve the interactive capabilities of a dialogue system, e.g., to adapt
to different customers, the Dialogue Robot Competition (DRC2022) was held. As
one of the teams, we built a dialogue system with a pipeline structure
containing four modules. The natural language understanding (NLU) and natural
language generation (NLG) modules were GPT-2 based models, and the dialogue
state tracking (DST) and policy modules were designed on the basis of
hand-crafted rules. After the preliminary round of the competition, we found
that the low variation in training examples for the NLU and failed
recommendation due to the policy used were probably the main reasons for the
limited performance of the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Simplifying Feature Extractors Prevents Overfitting for Neural Discourse Parsing Models. (arXiv:2210.09537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09537">
<div class="article-summary-box-inner">
<span><p>Complex feature extractors are widely employed for text representation
building. However, these complex feature extractors can lead to severe
overfitting problems especially when the training datasets are small, which is
especially the case for several discourse parsing tasks. Thus, we propose to
remove additional feature extractors and only utilize self-attention mechanism
to exploit pretrained neural language models in order to mitigate the
overfitting problem. Experiments on three common discourse parsing tasks (News
Discourse Profiling, Rhetorical Structure Theory based Discourse Parsing and
Penn Discourse Treebank based Discourse Parsing) show that powered by recent
pretrained language models, our simplied feature extractors obtain better
generalizabilities and meanwhile achieve comparable or even better system
performance. The simplified feature extractors have fewer learnable parameters
and less processing time. Codes will be released and this simple yet effective
model can serve as a better baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models. (arXiv:2210.09545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09545">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.
In Natural Language Processing (NLP), DNNs are often backdoored during the
fine-tuning process of a large-scale Pre-trained Language Model (PLM) with
poisoned samples. Although the clean weights of PLMs are readily available,
existing methods have ignored this information in defending NLP models against
backdoor attacks. In this work, we take the first step to exploit the
pre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language
models. Specifically, we leverage the clean pre-trained weights via two
complementary techniques: (1) a two-step Fine-mixing technique, which first
mixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained
weights, then fine-tunes the mixed weights on a small subset of clean data; (2)
an Embedding Purification (E-PUR) technique, which mitigates potential
backdoors existing in the word embeddings. We compare Fine-mixing with typical
backdoor mitigation methods on three single-sentence sentiment classification
tasks and two sentence-pair classification tasks and show that it outperforms
the baselines by a considerable margin in all scenarios. We also show that our
E-PUR method can benefit existing mitigation methods. Our work establishes a
simple but strong baseline defense for secure fine-tuned NLP models against
backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Cross-modal Semantics Alignment Capability from the Textual Perspective. (arXiv:2210.09550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09550">
<div class="article-summary-box-inner">
<span><p>In recent years, vision and language pre-training (VLP) models have advanced
the state-of-the-art results in a variety of cross-modal downstream tasks.
Aligning cross-modal semantics is claimed to be one of the essential
capabilities of VLP models. However, it still remains unclear about the inner
working mechanism of alignment in VLP models. In this paper, we propose a new
probing method that is based on image captioning to first empirically study the
cross-modal semantics alignment of VLP models. Our probing method is built upon
the fact that given an image-caption pair, the VLP models will give a score,
indicating how well two modalities are aligned; maximizing such scores will
generate sentences that VLP models believe are of good alignment. Analyzing
these sentences thus will reveal in what way different modalities are aligned
and how well these alignments are in VLP models. We apply our probing method to
five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,
and provide a comprehensive analysis of the generated captions guided by these
models. Our results show that VLP models (1) focus more on just aligning
objects with visual words, while neglecting global semantics; (2) prefer fixed
sentence patterns, thus ignoring more important textual information including
fluency and grammar; and (3) deem the captions with more visual words are
better aligned with images. These findings indicate that VLP models still have
weaknesses in cross-modal semantics alignment and we hope this work will draw
researchers' attention to such problems when designing a new VLP model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation. (arXiv:2210.09551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09551">
<div class="article-summary-box-inner">
<span><p>Prompt learning with immensely large Casual Language Models (CLMs) has been
shown promising for attribute-controllable text generation (CTG). However,
vanilla prompt tuning tends to imitate training corpus characteristics beyond
the control attributes, resulting in a poor generalization ability. Moreover,
it is less able to capture the relationship between different attributes,
further limiting the control performance. In this paper, we propose a new CTG
approach, namely DisCup, which incorporates the attribute knowledge of
discriminator to optimize the control-prompts, steering a frozen CLM to produce
attribute-specific texts. Specifically, the frozen CLM model, capable of
producing multitudinous texts, is first used to generate the next-token
candidates based on the context, so as to ensure the diversity of tokens to be
predicted. Then, we leverage an attribute-discriminator to select
desired/undesired tokens from those candidates, providing the inter-attribute
knowledge. Finally, we bridge the above two traits by an unlikelihood objective
for prompt-tuning. Extensive experimental results show that DisCup can achieve
a new state-of-the-art control performance while maintaining an efficient and
high-quality text generation, only relying on around 10 virtual tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation. (arXiv:2210.09556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09556">
<div class="article-summary-box-inner">
<span><p>End-to-end Speech Translation (ST) aims at translating the source language
speech into target language text without generating the intermediate
transcriptions. However, the training of end-to-end methods relies on parallel
ST data, which are difficult and expensive to obtain. Fortunately, the
supervised data for automatic speech recognition (ASR) and machine translation
(MT) are usually more accessible, making zero-shot speech translation a
potential direction. Existing zero-shot methods fail to align the two
modalities of speech and text into a shared semantic space, resulting in much
worse performance compared to the supervised ST methods. In order to enable
zero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method
that employs a shared discrete vocabulary space to accommodate and match both
modalities of speech and text. Specifically, we introduce a vector quantization
module to discretize the continuous representations of speech and text into a
finite set of virtual tokens, and use ASR data to map corresponding speech and
text to the same virtual token in a shared codebook. This way, source language
speech can be embedded in the same semantic space as the source language text,
which can be then transformed into target language text with an MT module.
Experiments on multiple language pairs demonstrate that our zero-shot ST method
significantly improves the SOTA, and even performers on par with the strong
supervised ST baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder. (arXiv:2210.09559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09559">
<div class="article-summary-box-inner">
<span><p>With a growing need for robust and general discourse structures in many
downstream tasks and real-world applications, the current lack of high-quality,
high-quantity discourse trees poses a severe shortcoming. In order the
alleviate this limitation, we propose a new strategy to generate tree
structures in a task-agnostic, unsupervised fashion by extending a latent tree
induction framework with an auto-encoding objective. The proposed approach can
be applied to any tree-structured objective, such as syntactic parsing,
discourse parsing and others. However, due to the especially difficult
annotation process to generate discourse trees, we initially develop such
method to complement task-specific models in generating much larger and more
diverse discourse treebanks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Domain-Independent Supervised Discourse Parsing Through Gradient Boosting. (arXiv:2210.09565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09565">
<div class="article-summary-box-inner">
<span><p>Discourse analysis and discourse parsing have shown great impact on many
important problems in the field of Natural Language Processing (NLP). Given the
direct impact of discourse annotations on model performance and
interpretability, robustly extracting discourse structures from arbitrary
documents is a key task to further improve computational models in NLP. To this
end, we present a new, supervised paradigm directly tackling the domain
adaptation issue in discourse parsing. Specifically, we introduce the first
fully supervised discourse parser designed to alleviate the domain dependency
through a staged model of weak classifiers by introducing the gradient boosting
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADI 2022: The Third Nuanced Arabic Dialect Identification Shared Task. (arXiv:2210.09582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09582">
<div class="article-summary-box-inner">
<span><p>We describe findings of the third Nuanced Arabic Dialect Identification
Shared Task (NADI 2022). NADI aims at advancing state of the art Arabic NLP,
including on Arabic dialects. It does so by affording diverse datasets and
modeling opportunities in a standardized context where meaningful comparisons
between models and approaches are possible. NADI 2022 targeted both dialect
identification (Subtask 1) and dialectal sentiment analysis (Subtask 2) at the
country level. A total of 41 unique teams registered for the shared task, of
whom 21 teams have actually participated (with 105 valid submissions). Among
these, 19 teams participated in Subtask 1 and 10 participated in Subtask 2. The
winning team achieved 27.06 F1 on Subtask 1 and F1=75.16 on Subtask 2,
reflecting that the two subtasks remain challenging and motivating future work
in this area. We describe methods employed by participating teams and offer an
outlook for NADI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary Workbench: Unifying Application and Evaluation of Text Summarization Models. (arXiv:2210.09587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09587">
<div class="article-summary-box-inner">
<span><p>This paper presents Summary Workbench, a new tool for developing and
evaluating text summarization models. New models and evaluation measures can be
easily integrated as Docker-based plugins, allowing to examine the quality of
their summaries against any input and to evaluate them using various evaluation
measures. Visual analyses combining multiple measures provide insights into the
models' strengths and weaknesses. The tool is hosted at
\url{https://tldr.demo.webis.de} and also supports local deployment for private
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks. (arXiv:2210.09588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09588">
<div class="article-summary-box-inner">
<span><p>Translation has played a crucial role in improving the performance on
multilingual tasks: (1) to generate the target language data from the source
language data for training and (2) to generate the source language data from
the target language data for inference. However, prior works have not
considered the use of both translations simultaneously. This paper shows that
combining them can synergize the results on various multilingual sentence
classification tasks. We empirically find that translation artifacts stylized
by translators are the main factor of the performance gain. Based on this
analysis, we adopt two training methods, SupCon and MixUp, considering
translation artifacts. Furthermore, we propose a cross-lingual fine-tuning
algorithm called MUSC, which uses SupCon and MixUp jointly and improves the
performance. Our code is available at https://github.com/jongwooko/MUSC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft-Labeled Contrastive Pre-training for Function-level Code Representation. (arXiv:2210.09597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09597">
<div class="article-summary-box-inner">
<span><p>Code contrastive pre-training has recently achieved significant progress on
code-related tasks. In this paper, we present \textbf{SCodeR}, a
\textbf{S}oft-labeled contrastive pre-training framework with two positive
sample construction methods to learn functional-level \textbf{Code}
\textbf{R}epresentation. Considering the relevance between codes in a
large-scale code corpus, the soft-labeled contrastive pre-training can obtain
fine-grained soft-labels through an iterative adversarial manner and use them
to learn better code representation. The positive sample construction is
another key for contrastive pre-training. Previous works use
transformation-based methods like variable renaming to generate semantically
equal positive codes. However, they usually result in the generated code with a
highly similar surface form, and thus mislead the model to focus on superficial
code structure instead of code semantics. To encourage SCodeR to capture
semantic information from the code, we utilize code comments and abstract
syntax sub-trees of the code to build positive samples. We conduct experiments
on four code-related tasks over seven datasets. Extensive experimental results
show that SCodeR achieves new state-of-the-art performance on all of them,
which illustrates the effectiveness of the proposed pre-training method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Enhanced Distantly Supervised Ultrafine Entity Typing. (arXiv:2210.09599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09599">
<div class="article-summary-box-inner">
<span><p>Recently, the task of distantly supervised (DS) ultra-fine entity typing has
received significant attention. However, DS data is noisy and often suffers
from missing or wrong labeling issues resulting in low precision and low
recall. This paper proposes a novel ultra-fine entity typing model with
denoising capability. Specifically, we build a noise model to estimate the
unknown labeling noise distribution over input contexts and noisy type labels.
With the noise model, more trustworthy labels can be recovered by subtracting
the estimated noise from the input. Furthermore, we propose an entity typing
model, which adopts a bi-encoder architecture, is trained on the denoised data.
Finally, the noise model and entity typing model are trained iteratively to
enhance each other. We conduct extensive experiments on the Ultra-Fine entity
typing dataset as well as OntoNotes dataset and demonstrate that our approach
significantly outperforms other baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages. (arXiv:2210.09644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09644">
<div class="article-summary-box-inner">
<span><p>This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROSE: Robust Selective Fine-tuning for Pre-trained Language Models. (arXiv:2210.09658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09658">
<div class="article-summary-box-inner">
<span><p>Even though the large-scale language models have achieved excellent
performances, they suffer from various adversarial attacks. A large body of
defense methods has been proposed. However, they are still limited due to
redundant attack search spaces and the inability to defend against various
types of attacks. In this work, we present a novel fine-tuning approach called
\textbf{RO}bust \textbf{SE}letive fine-tuning (\textbf{ROSE}) to address this
issue. ROSE conducts selective updates when adapting pre-trained models to
downstream tasks, filtering out invaluable and unrobust updates of parameters.
Specifically, we propose two strategies: the first-order and second-order ROSE
for selecting target robust parameters. The experimental results show that ROSE
achieves significant improvements in adversarial robustness on various
downstream NLP tasks, and the ensemble method even surpasses both variants
above. Furthermore, ROSE can be easily incorporated into existing fine-tuning
methods to improve their adversarial robustness further. The empirical analysis
confirms that ROSE eliminates unrobust spurious updates during fine-tuning,
leading to solutions corresponding to flatter and wider optima than the
conventional method. Code is available at
\url{https://github.com/jiangllan/ROSE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task. (arXiv:2210.09683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09683">
<div class="article-summary-box-inner">
<span><p>In this report, we present our submission to the WMT 2022 Metrics Shared
Task. We build our system based on the core idea of UNITE (Unified Translation
Evaluation), which unifies source-only, reference-only, and
source-reference-combined evaluation scenarios into one single model.
Specifically, during the model pre-training phase, we first apply the
pseudo-labeled data examples to continuously pre-train UNITE. Notably, to
reduce the gap between pre-training and fine-tuning, we use data cropping and a
ranking-based score normalization strategy. During the fine-tuning phase, we
use both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data
from past years' WMT competitions. Specially, we collect the results from
models with different pre-trained language model backbones, and use different
ensembling strategies for involved translation directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Analysis of Acknowledgement Texts in Web of Science: a case study on four scientific domains. (arXiv:2210.09716v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09716">
<div class="article-summary-box-inner">
<span><p>Analysis of acknowledgments is particularly interesting as acknowledgments
may give information not only about funding, but they are also able to reveal
hidden contributions to authorship and the researcher's collaboration patterns,
context in which research was conducted, and specific aspects of the academic
work. The focus of the present research is the analysis of a large sample of
acknowledgement texts indexed in the Web of Science (WoS) Core Collection.
Record types 'article' and 'review' from four different scientific domains,
namely social sciences, economics, oceanography and computer science, published
from 2014 to 2019 in a scientific journal in English were considered. Six types
of acknowledged entities, i.e., funding agency, grant number, individuals,
university, corporation and miscellaneous, were extracted from the
acknowledgement texts using a Named Entity Recognition (NER) tagger and
subsequently examined. A general analysis of the acknowledgement texts showed
that indexing of funding information in WoS is incomplete. The analysis of the
automatically extracted entities revealed differences and distinct patterns in
the distribution of acknowledged entities of different types between different
scientific domains. A strong association was found between acknowledged entity
and scientific domain and acknowledged entity and entity type. Only negligible
correlation was found between the number of citations and the number of
acknowledged entities. Generally, the number of words in the acknowledgement
texts positively correlates with the number of acknowledged funding
organizations, universities, individuals and miscellaneous entities. At the
same time, acknowledgement texts with the larger number of sentences have more
acknowledged individuals and miscellaneous categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09723">
<div class="article-summary-box-inner">
<span><p>Textual entailment recognition is one of the basic natural language
understanding(NLU) tasks. Understanding the meaning of sentences is a
prerequisite before applying any natural language processing(NLP) techniques to
automatically recognize the textual entailment. A text entails a hypothesis if
and only if the true value of the hypothesis follows the text. Classical
approaches generally utilize the feature value of each word from word embedding
to represent the sentences. In this paper, we propose a novel approach to
identifying the textual entailment relationship between text and hypothesis,
thereby introducing a new semantic feature focusing on empirical
threshold-based semantic text representation. We employ an element-wise
Manhattan distance vector-based feature that can identify the semantic
entailment relationship between the text-hypothesis pair. We carried out
several experiments on a benchmark entailment classification(SICK-RTE) dataset.
We train several machine learning(ML) algorithms applying both semantic and
lexical features to classify the text-hypothesis pair as entailment, neutral,
or contradiction. Our empirical sentence representation technique enriches the
semantic information of the texts and hypotheses found to be more efficient
than the classical ones. In the end, our approach significantly outperforms
known methods in understanding the meaning of the sentences for the textual
entailment classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Translation for Unsegmented Input: A Sliding Window Approach. (arXiv:2210.09754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09754">
<div class="article-summary-box-inner">
<span><p>In the cascaded approach to spoken language translation (SLT), the ASR output
is typically punctuated and segmented into sentences before being passed to MT,
since the latter is typically trained on written text. However, erroneous
segmentation, due to poor sentence-final punctuation by the ASR system, leads
to degradation in translation quality, especially in the simultaneous (online)
setting where the input is continuously updated. To reduce the influence of
automatic segmentation, we present a sliding window approach to translate raw
ASR outputs (online or offline) without needing to rely on an automatic
segmenter. We train translation models using parallel windows (instead of
parallel sentences) extracted from the original training data. At test time, we
translate at the window level and join the translated windows using a simple
approach to generate the final translation. Experiments on English-to-German
and English-to-Czech show that our approach improves 1.3--2.0 BLEU points over
the usual ASR-segmenter pipeline, and the fixed-length window considerably
reduces flicker compared to a baseline retranslation-based online SLT system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventGraph at CASE 2021 Task 1: A General Graph-based Approach to Protest Event Extraction. (arXiv:2210.09770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09770">
<div class="article-summary-box-inner">
<span><p>This paper presents our submission to the 2022 edition of the CASE 2021
shared task 1, subtask 4. The EventGraph system adapts an end-to-end,
graph-based semantic parser to the task of Protest Event Extraction and more
specifically subtask 4 on event trigger and argument extraction. We experiment
with various graphs, encoding the events as either "labeled-edge" or
"node-centric" graphs. We show that the "node-centric" approach yields best
results overall, performing well across the three languages of the task, namely
English, Spanish, and Portuguese. EventGraph is ranked 3rd for English and
Portuguese, and 4th for Spanish. Our code is available at:
https://github.com/huiling-y/eventgraph_at_case
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation. (arXiv:2210.09773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09773">
<div class="article-summary-box-inner">
<span><p>We introduce a new method to improve existing multilingual sentence
embeddings with Abstract Meaning Representation (AMR). Compared with the
original textual input, AMR is a structured semantic representation that
presents the core concepts and relations in a sentence explicitly and
unambiguously. It also helps reduce surface variations across different
expressions and languages. Unlike most prior work that only evaluates the
ability to measure semantic similarity, we present a thorough evaluation of
existing multilingual sentence embeddings and our improved versions, which
include a collection of five transfer tasks in different downstream
applications. Experiment results show that retrofitting multilingual sentence
embeddings with AMR leads to better state-of-the-art performance on both
semantic textual similarity and transfer tasks. Our codebase and evaluation
scripts can be found at \url{https://github.com/jcyk/MSE-AMR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis. (arXiv:2210.09803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09803">
<div class="article-summary-box-inner">
<span><p>Most existing pre-trained language representation models (PLMs) are
sub-optimal in sentiment analysis tasks, as they capture the sentiment
information from word-level while under-considering sentence-level information.
In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained
language model with combined Word-level and Sentence-level Pre-training tasks.
The word level pre-training task detects replaced sentiment words, via a
generator-discriminator framework, to enhance the PLM's knowledge about
sentiment words. The sentence level pre-training task further strengthens the
discriminator via a contrastive learning framework, with similar sentences as
negative samples, to encode sentiments in a sentence. Extensive experimental
results show that SentiWSP achieves new state-of-the-art performance on various
sentence-level and aspect-level sentiment classification benchmarks. We have
made our code and model publicly available at
https://github.com/XMUDM/SentiWSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specific Sub-network for Multi-Domain Neural Machine Translation. (arXiv:2210.09805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09805">
<div class="article-summary-box-inner">
<span><p>This paper presents Domain-Specific Sub-network (DoSS). It uses a set of
masks obtained through pruning to define a sub-network for each domain and
finetunes the sub-network parameters on domain data. This performs very closely
and drastically reduces the number of parameters compared to finetuning the
whole network on each domain. Also a method to make masks unique per domain is
proposed and shown to greatly improve the generalization to unseen domains. In
our experiments on German to English machine translation the proposed method
outperforms the strong baseline of continue training on multi-domain (medical,
tech and religion) data by 1.47 BLEU points. Also continue training DoSS on new
domain (legal) outperforms the multi-domain (medical, tech, religion, legal)
baseline by 1.52 BLEU points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye-tracking based classification of Mandarin Chinese readers with and without dyslexia using neural sequence models. (arXiv:2210.09819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09819">
<div class="article-summary-box-inner">
<span><p>Eye movements are known to reflect cognitive processes in reading, and
psychological reading research has shown that eye gaze patterns differ between
readers with and without dyslexia. In recent years, researchers have attempted
to classify readers with dyslexia based on their eye movements using Support
Vector Machines (SVMs). However, these approaches (i) are based on highly
aggregated features averaged over all words read by a participant, thus
disregarding the sequential nature of the eye movements, and (ii) do not
consider the linguistic stimulus and its interaction with the reader's eye
movements. In the present work, we propose two simple sequence models that
process eye movements on the entire stimulus without the need of aggregating
features across the sentence. Additionally, we incorporate the linguistic
stimulus into the model in two ways -- contextualized word embeddings and
manually extracted linguistic features. The models are evaluated on a Mandarin
Chinese dataset containing eye movements from children with and without
dyslexia. Our results show that (i) even for a logographic script such as
Chinese, sequence models are able to classify dyslexia on eye gaze sequences,
reaching state-of-the-art performance, and (ii) incorporating the linguistic
stimulus does not help to improve classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging. (arXiv:2210.09840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09840">
<div class="article-summary-box-inner">
<span><p>Part-of-Speech (POS) tagging is an important component of the NLP pipeline,
but many low-resource languages lack labeled data for training. An established
method for training a POS tagger in such a scenario is to create a labeled
training set by transferring from high-resource languages. In this paper, we
propose a novel method for transferring labels from multiple high-resource
source to low-resource target languages. We formalize POS tag projection as
graph-based label propagation. Given translations of a sentence in multiple
languages, we create a graph with words as nodes and alignment links as edges
by aligning words for all language pairs. We then propagate node labels from
source to target using a Graph Neural Network augmented with transformer
layers. We show that our propagation creates training sets that allow us to
train POS taggers for a diverse set of languages. When combined with enhanced
contextualized embeddings, our method achieves a new state-of-the-art for
unsupervised POS tagging of low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions. (arXiv:2210.09894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09894">
<div class="article-summary-box-inner">
<span><p>Abstractive dialogue summarization is to generate a concise and fluent
summary covering the salient information in a dialogue among two or more
interlocutors. It has attracted great attention in recent years based on the
massive emergence of social communication platforms and an urgent requirement
for efficient dialogue information understanding and digestion. Different from
news or articles in traditional document summarization, dialogues bring unique
characteristics and additional challenges, including different language styles
and formats, scattered information, flexible discourse structures and unclear
topic boundaries. This survey provides a comprehensive investigation on
existing work for abstractive dialogue summarization from scenarios, approaches
to evaluations. It categorizes the task into two broad categories according to
the type of input dialogues, i.e., open-domain and task-oriented, and presents
a taxonomy of existing techniques in three directions, namely, injecting
dialogue features, designing auxiliary training tasks and using additional
data.A list of datasets under different scenarios and widely-accepted
evaluation metrics are summarized for completeness. After that, the trends of
scenarios and techniques are summarized, together with deep insights on
correlations between extensively exploited features and different scenarios.
Based on these analyses, we recommend future directions including more
controlled and complicated scenarios, technical innovations and comparisons,
publicly available datasets in special domains, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAPO: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction. (arXiv:2210.09926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09926">
<div class="article-summary-box-inner">
<span><p>Bilingual lexicon induction induces the word translations by aligning
independently trained word embeddings in two languages. Existing approaches
generally focus on minimizing the distances between words in the aligned pairs,
while suffering from low discriminative capability to distinguish the relative
orders between positive and negative candidates. In addition, the mapping
function is globally shared by all words, whose performance might be hindered
by the deviations in the distributions of different languages. In this work, we
propose a novel ranking-oriented induction model RAPO to learn personalized
mapping function for each word. RAPO is capable of enjoying the merits from the
unique characteristics of a single word and the cross-language isomorphism
simultaneously. Extensive experimental results on public datasets including
both rich-resource and low-resource languages demonstrate the superiority of
our proposal. Our code is publicly available in
\url{https://github.com/Jlfj345wf/RAPO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. (arXiv:2210.09932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09932">
<div class="article-summary-box-inner">
<span><p>Lay summarisation aims to jointly summarise and simplify a given text, thus
making its content more comprehensible to non-experts. Automatic approaches for
lay summarisation can provide significant value in broadening access to
scientific literature, enabling a greater degree of both interdisciplinary
knowledge sharing and public understanding when it comes to research findings.
However, current corpora for this task are limited in their size and scope,
hindering the development of broadly applicable data-driven approaches. Aiming
to rectify these issues, we present two novel lay summarisation datasets, PLOS
(large-scale) and eLife (medium-scale), each of which contains biomedical
journal articles alongside expert-written lay summaries. We provide a thorough
characterisation of our lay summaries, highlighting differing levels of
readability and abstractiveness between datasets that can be leveraged to
support the needs of different applications. Finally, we benchmark our datasets
using mainstream summarisation approaches and perform a manual evaluation with
domain experts, demonstrating their utility and casting light on the key
challenges of this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning. (arXiv:2210.09934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09934">
<div class="article-summary-box-inner">
<span><p>Existing zero-shot cross-lingual transfer methods rely on parallel corpora or
bilingual dictionaries, which are expensive and impractical for low-resource
languages. To disengage from these dependencies, researchers have explored
training multilingual models on English-only resources and transferring them to
low-resource languages. However, its effect is limited by the gap between
embedding clusters of different languages. To address this issue, we propose
Embedding-Push, Attention-Pull, and Robust targets to transfer English
embeddings to virtual multilingual embeddings without semantic loss, thereby
improving cross-lingual transferability. Experimental results on mBERT and
XLM-R demonstrate that our method significantly outperforms previous works on
the zero-shot cross-lingual text classification task and can obtain a better
multilingual alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer-learning for video classification: Video Swin Transformer on multiple domains. (arXiv:2210.09969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09969">
<div class="article-summary-box-inner">
<span><p>The computer vision community has seen a shift from convolutional-based to
pure transformer architectures for both image and video tasks. Training a
transformer from zero for these tasks usually requires a lot of data and
computational resources. Video Swin Transformer (VST) is a pure-transformer
model developed for video classification which achieves state-of-the-art
results in accuracy and efficiency on several datasets. In this paper, we aim
to understand if VST generalizes well enough to be used in an out-of-domain
setting. We study the performance of VST on two large-scale datasets, namely
FCVID and Something-Something using a transfer learning approach from
Kinetics-400, which requires around 4x less memory than training from scratch.
We then break down the results to understand where VST fails the most and in
which scenarios the transfer-learning approach is viable. Our experiments show
an 85\% top-1 accuracy on FCVID without retraining the whole model which is
equal to the state-of-the-art for the dataset and a 21\% accuracy on
Something-Something. The experiments also suggest that the performance of the
VST decreases on average when the video duration increases which seems to be a
consequence of a design choice of the model. From the results, we conclude that
VST generalizes well enough to classify out-of-domain videos without retraining
when the target classes are from the same type as the classes used to train the
model. We observed this effect when we performed transfer-learning from
Kinetics-400 to FCVID, where most datasets target mostly objects. On the other
hand, if the classes are not from the same type, then the accuracy after the
transfer-learning approach is expected to be poor. We observed this effect when
we performed transfer-learning from Kinetics-400, where the classes represent
mostly objects, to Something-Something, where the classes represent mostly
actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Information Content of Predictions in Word Analogy Tests. (arXiv:2210.09972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09972">
<div class="article-summary-box-inner">
<span><p>An approach is proposed to quantify, in bits of information, the actual
relevance of analogies in analogy tests. The main component of this approach is
a softaccuracy estimator that also yields entropy estimates with compensated
biases. Experimental results obtained with pre-trained GloVe 300-D vectors and
two public analogy test sets show that proximity hints are much more relevant
than analogies in analogy tests, from an information content perspective.
Accordingly, a simple word embedding model is used to predict that analogies
carry about one bit of information, which is experimentally corroborated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making a MIRACL: Multilingual Information Retrieval Across a Continuum of Languages. (arXiv:2210.09984v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09984">
<div class="article-summary-box-inner">
<span><p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)
is a multilingual dataset we have built for the WSDM 2023 Cup challenge that
focuses on ad hoc retrieval across 18 different languages, which collectively
encompass over three billion native speakers around the world. These languages
have diverse typologies, originate from many different language families, and
are associated with varying amounts of available resources -- including what
researchers typically characterize as high-resource as well as low-resource
languages. Our dataset is designed to support the creation and evaluation of
models for monolingual retrieval, where the queries and the corpora are in the
same language. In total, we have gathered over 700k high-quality relevance
judgments for around 77k queries over Wikipedia in these 18 languages, where
all assessments have been performed by native speakers hired by our team. Our
goal is to spur research that will improve retrieval across a continuum of
languages, thus enhancing information access capabilities for diverse
populations around the world, particularly those that have been traditionally
underserved. This overview paper describes the dataset and baselines that we
share with the community. The MIRACL website is live at <a href="http://miracl.ai/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-hoc analysis of Arabic transformer models. (arXiv:2210.09990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09990">
<div class="article-summary-box-inner">
<span><p>Arabic is a Semitic language which is widely spoken with many dialects. Given
the success of pre-trained language models, many transformer models trained on
Arabic and its dialects have surfaced. While there have been an extrinsic
evaluation of these models with respect to downstream NLP tasks, no work has
been carried out to analyze and compare their internal representations. We
probe how linguistic information is encoded in the transformer models, trained
on different Arabic dialects. We perform a layer and neuron analysis on the
models using morphological tagging tasks for different dialects of Arabic and a
dialectal identification task. Our analysis enlightens interesting findings
such as: i) word morphology is learned at the lower and middle layers, ii)
while syntactic dependencies are predominantly captured at the higher layers,
iii) despite a large overlap in their vocabulary, the MSA-based models fail to
capture the nuances of Arabic dialects, iv) we found that neurons in embedding
layers are polysemous in nature, while the neurons in middle layers are
exclusive to specific properties
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Guardedness and its Implications. (arXiv:2210.10012v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10012">
<div class="article-summary-box-inner">
<span><p>Previous work on concept identification in neural representations has focused
on linear concept subspaces and their neutralization. In this work, we
formulate the notion of linear guardedness -- the inability to directly predict
a given concept from the representation -- and study its implications. We show
that, in the binary case, the neutralized concept cannot be recovered by an
additional linear layer. However, we point out that -- contrary to what was
implicitly argued in previous works -- multiclass softmax classifiers can be
constructed that indirectly recover the concept. Thus, linear guardedness does
not guarantee that linear classifiers do not utilize the neutralized concepts,
shedding light on theoretical limitations of linear information removal
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ULN: Towards Underspecified Vision-and-Language Navigation. (arXiv:2210.10020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10020">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) is a task to guide an embodied agent
moving to a target position using language instructions. Despite the
significant performance improvement, the wide use of fine-grained instructions
fails to characterize more practical linguistic variations in reality. To fill
in this gap, we introduce a new setting, namely Underspecified
vision-and-Language Navigation (ULN), and associated evaluation datasets. ULN
evaluates agents using multi-level underspecified instructions instead of
purely fine-grained or coarse-grained, which is a more realistic and general
setting. As a primary step toward ULN, we propose a VLN framework that consists
of a classification module, a navigation agent, and an
Exploitation-to-Exploration (E2E) module. Specifically, we propose to learn
Granularity Specific Sub-networks (GSS) for the agent to ground multi-level
instructions with minimal additional parameters. Then, our E2E module estimates
grounding uncertainty and conducts multi-step lookahead exploration to improve
the success rate further. Experimental results show that existing VLN models
are still brittle to multi-level language underspecification. Our framework is
more robust and outperforms the baselines on ULN by ~10% relative success rate
across all levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR. (arXiv:2210.10027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10027">
<div class="article-summary-box-inner">
<span><p>Training state-of-the-art Automated Speech Recognition (ASR) models typically
requires a substantial amount of transcribed speech. In this work, we
demonstrate that a modality-matched joint speech and text model can be
leveraged to train a massively multilingual ASR model without any supervised
(manually transcribed) speech for some languages. This paper explores the use
of jointly learnt speech and text representations in a massively multilingual,
zero supervised speech, real-world setting to expand the set of languages
covered by ASR with only unlabeled speech and text in the target languages.
Using the FLEURS dataset, we define the task to cover $102$ languages, where
transcribed speech is available in $52$ of these languages and can be used to
improve end-to-end ASR quality on the remaining $50$. First, we show that by
combining speech representations with byte-level text representations and use
of language embeddings, we can dramatically reduce the Character Error Rate
(CER) on languages with no supervised speech from 64.8\% to 30.8\%, a relative
reduction of 53\%. Second, using a subset of South Asian languages we show that
Maestro-U can promote knowledge transfer from languages with supervised speech
even when there is limited to no graphemic overlap. Overall, Maestro-U closes
the gap to oracle performance by 68.5\% relative and reduces the CER of 19
languages below 15\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision. (arXiv:2210.10031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10031">
<div class="article-summary-box-inner">
<span><p>In the age of social media, where billions of internet users share
information and opinions, the negative impact of pandemics is not limited to
the physical world. It provokes a surge of incomplete, biased, and incorrect
information, also known as an infodemic. This global infodemic jeopardizes
measures to control the pandemic by creating panic, vaccine hesitancy, and
fragmented social response. Platforms like Facebook allow advertisers to adapt
their messaging to target different demographics and help alleviate or
exacerbate the infodemic problem depending on their content. In this paper, we
propose a minimally supervised multi-task learning framework for understanding
messaging on Facebook related to the covid vaccine by identifying ad themes and
moral foundations. Furthermore, we perform a more nuanced thematic analysis of
messaging tactics of vaccine campaigns on social media so that policymakers can
make better decisions on pandemic control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10040">
<div class="article-summary-box-inner">
<span><p>How reliably can we trust the scores obtained from social bias benchmarks as
faithful indicators of problematic social biases in a given language model? In
this work, we study this question by contrasting social biases with non-social
biases stemming from choices made during dataset construction that might not
even be discernible to the human eye. To do so, we empirically simulate various
alternative constructions for a given benchmark based on innocuous
modifications (such as paraphrasing or random-sampling) that maintain the
essence of their social bias. On two well-known social bias benchmarks
(Winogender and BiasNLI) we observe that these shallow modifications have a
surprising effect on the resulting degree of bias across various models. We
hope these troubling observations motivate more robust measures of social
biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. (arXiv:2210.10041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10041">
<div class="article-summary-box-inner">
<span><p>While transferring a pretrained language model, common approaches
conventionally attach their task-specific classifiers to the top layer and
adapt all the pretrained layers. We investigate whether one could make a
task-specific selection on which subset of the layers to adapt and where to
place the classifier. The goal is to reduce the computation cost of transfer
learning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its
performance.
</p>
<p>We propose to select layers based on the variability of their hidden states
given a task-specific corpus. We say a layer is already ``well-specialized'' in
a task if the within-class variability of its hidden states is low relative to
the between-class variability. Our variability metric is cheap to compute and
doesn't need any training or hyperparameter tuning. It is robust to data
imbalance and data scarcity. Extensive experiments on the GLUE benchmark
demonstrate that selecting layers based on our metric can yield significantly
stronger performance than using the same number of top layers and often match
the performance of fine-tuning or adapter-tuning the entire language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SafeText: A Benchmark for Exploring Physical Safety in Language Models. (arXiv:2210.10045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10045">
<div class="article-summary-box-inner">
<span><p>Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisit Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06658">
<div class="article-summary-box-inner">
<span><p>Humans can systematically generalize to novel compositions of existing
concepts. Recent studies argue that neural networks appear inherently
ineffective in such cognitive capacity, leading to a pessimistic view and a
lack of attention to optimistic results. We revisit this controversial topic
from the perspective of meaningful learning, an exceptional capability of
humans to learn novel concepts by connecting them with known ones. We reassess
the compositional skills of sequence-to-sequence models conditioned on the
semantic links between new and old concepts. Our observations suggest that
models can successfully one-shot generalize to novel concepts and compositions
through semantic linking, either inductively or deductively. We demonstrate
that prior knowledge plays a key role as well. In addition to synthetic tests,
we further conduct proof-of-concept experiments in machine translation and
semantic parsing, showing the benefits of meaningful learning in applications.
We hope our positive findings will encourage excavating modern neural networks'
potential in systematic generalization through more advanced learning schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic structures and the general Markov models. (arXiv:2104.08462v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08462">
<div class="article-summary-box-inner">
<span><p>We study phylogenetic signal present in syntactic information by considering
the syntactic structures data from Longobardi (2017b), Collins (2010), Ceolin
et al. (2020) and Koopman (2011). Focusing first on the general Markov models,
we explore how well the the syntactic structures data conform to the hypothesis
required by these models. We do this by comparing derived phylogenetic trees
against trees agreed on by the linguistics community. We then interpret the
methods of Ceolin et al. (2020) as an infinite sites evolutionary model and
compare the consistency of the data with this alternative. The ideas and
methods discussed in the present paper are more generally applicable than to
the specific setting of syntactic structures, and can be used in other
contexts, when analyzing consistency of data with against hypothesized
evolutionary models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NSP-BERT: A Prompt-based Few-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03564">
<div class="article-summary-box-inner">
<span><p>Using prompts to utilize language models to perform various downstream tasks,
also known as prompt-based learning or prompt-learning, has lately gained
significant success in comparison to the pre-train and fine-tune paradigm.
Nonetheless, virtually all prompt-based methods are token-level, meaning they
all utilize GPT's left-to-right language model or BERT's masked language model
to perform cloze-style tasks. In this paper, we attempt to accomplish several
NLP tasks in the zero-shot scenario using a BERT original pre-training task
abandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike
token-level techniques, our sentence-level prompt-based method NSP-BERT does
not need to fix the length of the prompt or the position to be predicted,
allowing it to handle tasks such as entity linking with ease. Based on the
characteristics of NSP-BERT, we offer several quick building templates for
various downstream tasks. We suggest a two-stage prompt method for word sense
disambiguation tasks in particular. Our strategies for mapping the labels
significantly enhance the model's performance on sentence pair tasks. On the
FewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of
these tasks and comes close to the few-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Contextual Toxicity Detection in Conversations. (arXiv:2111.12447v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12447">
<div class="article-summary-box-inner">
<span><p>Understanding toxicity in user conversations is undoubtedly an important
problem. Addressing "covert" or implicit cases of toxicity is particularly hard
and requires context. Very few previous studies have analysed the influence of
conversational context in human perception or in automated detection models. We
dive deeper into both these directions. We start by analysing existing
contextual datasets and come to the conclusion that toxicity labelling by
humans is in general influenced by the conversational structure, polarity and
topic of the context. We then propose to bring these findings into
computational detection models by introducing and evaluating (a) neural
architectures for contextual toxicity detection that are aware of the
conversational structure, and (b) data augmentation strategies that can help
model contextual toxicity detection. Our results have shown the encouraging
potential of neural architectures that are aware of the conversation structure.
We have also demonstrated that such models can benefit from synthetic data,
especially in the social media domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05966">
<div class="article-summary-box-inner">
<span><p>Structured knowledge grounding (SKG) leverages structured knowledge to
complete user requests, such as semantic parsing over databases and question
answering over knowledge bases. Since the inputs and outputs of SKG tasks are
heterogeneous, they have been studied separately by different communities,
which limits systematic and compatible research on SKG. In this paper, we
overcome this limitation by proposing the UnifiedSKG framework, which unifies
21 SKG tasks into a text-to-text format, aiming to promote systematic SKG
research, instead of being exclusive to a single task, domain, or dataset. We
use UnifiedSKG to benchmark T5 with different sizes and show that T5, with
simple modifications when necessary, achieves state-of-the-art performance on
almost all of the 21 tasks. We further demonstrate that multi-task
prefix-tuning improves the performance on most tasks, largely improving the
overall performance. UnifiedSKG also facilitates the investigation of zero-shot
and few-shot learning, and we show that T0, GPT-3, and Codex struggle in
zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a
series of controlled experiments on structured knowledge encoding variants
across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is
open-sourced at https://github.com/hkunlp/unifiedskg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments. (arXiv:2202.06387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06387">
<div class="article-summary-box-inner">
<span><p>Neural scaling laws define a predictable relationship between a model's
parameter count and its performance after training in the form of a power law.
However, most research to date has not explicitly investigated whether scaling
laws can be used to accelerate model development. In this work, we perform such
an empirical investigation across a wide range of language understanding tasks,
starting from models with as few as 10K parameters, and evaluate downstream
performance across 9 language understanding tasks. We find that scaling laws
emerge at finetuning time in some NLP tasks, and that they can also be
exploited for debugging convergence when training large models. Moreover, for
tasks where scaling laws exist, they can be used to predict the performance of
larger models, which enables effective model selection. However, revealing
scaling laws requires careful hyperparameter tuning and multiple runs for the
purpose of uncertainty estimation, which incurs additional overhead, partially
offsetting the computational benefits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models. (arXiv:2203.02094v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02094">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture is ubiquitously used as the building block of
large-scale autoregressive language models. However, finding architectures with
the optimal trade-off between task performance (perplexity) and hardware
constraints like peak memory utilization and latency is non-trivial. This is
exacerbated by the proliferation of various hardware. We leverage the somewhat
surprising empirical observation that the number of decoder parameters in
autoregressive Transformers has a high rank correlation with task performance,
irrespective of the architecture topology. This observation organically induces
a simple Neural Architecture Search (NAS) algorithm that uses decoder
parameters as a proxy for perplexity without need for any model training. The
search phase of our training-free algorithm, dubbed Lightweight Transformer
Search (LTS), can be run directly on target devices since it does not require
GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of
perplexity versus any hardware performance cost. We evaluate LTS on diverse
devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer
backbones: GPT-2 and Transformer-XL. Results show that the perplexity of
16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster
runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero
and one-shot settings, LTS Pareto-frontier models achieve higher average
accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x
lower latency. LTS extracts the Pareto-frontier in under 3 hours while running
on a commodity laptop. We effectively remove the carbon footprint of hundreds
of GPU hours of training during search, offering a strong simple baseline for
future NAS methods in autoregressive language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models. (arXiv:2203.07259v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07259">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models have become a key building block for
natural language processing. While these models are extremely accurate, they
can be too large and computationally intensive to run on standard deployments.
A variety of compression methods, including distillation, quantization,
structured and unstructured pruning are known to decrease model size and
increase inference speed, with low accuracy loss. In this context, this paper's
contributions are two-fold. We perform an in-depth study of the
accuracy-compression trade-off for unstructured weight pruning of BERT models.
We introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight
pruning method based on approximate second-order information, which we show to
yield state-of-the-art results in both stages of language tasks: pre-training
and fine-tuning. Specifically, oBERT extends existing work on unstructured
second-order pruning by allowing for pruning blocks of weights, and by being
applicable at the BERT scale. Second, we investigate the impact of this pruning
method when compounding compression approaches to obtain highly compressed but
accurate models for deployment on edge devices. These models significantly push
boundaries of the current state-of-the-art sparse BERT models with respect to
all metrics: model size, inference speed and task accuracy. For example,
relative to the dense BERT-base, we obtain 10x model size compression (in MB)
with &lt; 1% accuracy drop, 10x CPU-inference speedup with &lt; 2% accuracy drop, and
29x CPU-inference speedup with &lt; 7.5% accuracy drop. Our code, fully integrated
with Transformers and SparseML, is available at
https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperdecoders: Instance-specific decoders for multi-task NLP. (arXiv:2203.08304v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08304">
<div class="article-summary-box-inner">
<span><p>We investigate input-conditioned hypernetworks for multi-tasking in NLP,
generating parameter-efficient adaptations for a decoder using a hypernetwork
conditioned on the output of an encoder. This approach produces a unique
decoder adaptation for every input instance, allowing the network a larger
degree of flexibility than prior work that only produces one decoder adaptation
per task. We apply our method to sequence classification tasks, extractive QA,
and summarisation and find that it surpasses previous parameter efficient
fine-tuning methods and often outperforms fully finetuning the underlying
model. An analysis of the embeddings used by our hypernetwork shows that they
are sensitive to output label and type, suggesting that our approach better
maps from encoder representations to output labels. Our code is publicly
available at https://github.com/allenai/hyperdecoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning. (arXiv:2204.06487v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06487">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cross-Task Generalization via Retrieval Augmentation. (arXiv:2204.07937v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07937">
<div class="article-summary-box-inner">
<span><p>Humans can perform unseen tasks by recalling relevant skills acquired
previously and then generalizing them to the target tasks, even if there is no
supervision at all. In this paper, we aim to improve this kind of cross-task
generalization ability of massive multi-task language models, such as T0 and
FLAN, in an unsupervised setting. We propose a retrieval-augmentation method
named ReCross that takes a few unlabelled examples as queries to retrieve a
small subset of upstream data and uses them to update the multi-task model for
better generalization. ReCross is a straightforward yet effective retrieval
method that combines both efficient dense retrieval and effective pair-wise
reranking. Our results and analysis show that it significantly outperforms both
non-retrieval methods and other baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Climate Awareness in NLP Research. (arXiv:2205.05071v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05071">
<div class="article-summary-box-inner">
<span><p>The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering Private Text in Federated Learning of Language Models. (arXiv:2205.08514v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08514">
<div class="article-summary-box-inner">
<span><p>Federated learning allows distributed users to collaboratively train a model
while keeping each user's data private. Recently, a growing body of work has
demonstrated that an eavesdropping attacker can effectively recover image data
from gradients transmitted during federated learning. However, little progress
has been made in recovering text data. In this paper, we present a novel attack
method FILM for federated learning of language models (LMs). For the first
time, we show the feasibility of recovering text from large batch sizes of up
to 128 sentences. Unlike image-recovery methods that are optimized to match
gradients, we take a distinct approach that first identifies a set of words
from gradients and then directly reconstructs sentences based on beam search
and a prior-based reordering strategy. We conduct the FILM attack on several
large-scale datasets and show that it can successfully reconstruct single
sentences with high fidelity for large batch sizes and even multiple sentences
if applied iteratively. We evaluate three defense methods: gradient pruning,
DPSGD, and a simple approach to freeze word embeddings that we propose. We show
that both gradient pruning and DPSGD lead to a significant drop in utility.
However, if we fine-tune a public pre-trained LM on private text without
updating word embeddings, it can effectively defend the attack with minimal
data utility loss. Together, we hope that our results can encourage the
community to rethink the privacy concerns of LM training and its standard
practices in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition. (arXiv:2205.11998v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11998">
<div class="article-summary-box-inner">
<span><p>The choice of modeling units is crucial for automatic speech recognition
(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning
but are not directly related to the pronunciation. Thus only considering the
writing of Chinese characters as modeling units is insufficient to capture
speech features. In this paper, we present a novel method involves with
multi-level modeling units, which integrates multi-level information for
mandarin speech recognition. Specifically, the encoder block considers
syllables as modeling units and the decoder block deals with character-level
modeling units. To facilitate the incremental conversion from syllable features
to character features, we design an auxiliary task that applies cross-entropy
(CE) loss to intermediate decoder layers. During inference, the input feature
sequences are converted into syllable sequences by the encoder block and then
converted into Chinese characters by the decoder block. Experiments on the
widely used AISHELL-1 corpus demonstrate that our method achieves promising
results with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the
Transformer backbones respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. (arXiv:2205.12215v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12215">
<div class="article-summary-box-inner">
<span><p>We introduce DivEMT, the first publicly available post-editing study of
Neural Machine Translation (NMT) over a typologically diverse set of target
languages. Using a strictly controlled setup, 18 professional translators were
instructed to translate or post-edit the same set of English documents into
Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,
their edits, keystrokes, editing times and pauses were recorded, enabling an
in-depth, cross-lingual evaluation of NMT quality and post-editing
effectiveness. Using this new dataset, we assess the impact of two
state-of-the-art NMT systems, Google Translate and the multilingual mBART-50
model, on translation productivity. We find that post-editing is consistently
faster than translation from scratch. However, the magnitude of productivity
gains varies widely across systems and languages, highlighting major
disparities in post-editing effectiveness for languages at different degrees of
typological relatedness to English, even when controlling for system
architecture and training data size. We publicly release the complete dataset
including all collected behavioral data, to foster new research on the
translation capabilities of NMT systems for typologically diverse languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Natural Language Proofs with Verifier-Guided Search. (arXiv:2205.12443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12443">
<div class="article-summary-box-inner">
<span><p>Deductive reasoning over natural language is a challenging problem in NLP. In
this work, we focus on proof generation: Given a hypothesis and a set of
supporting facts, the model generates a proof tree indicating how to deduce the
hypothesis from supporting facts. Compared to generating the entire proof in
one shot, stepwise generation can better exploit the compositionality and
generalize to longer proofs but has achieved limited success on real-world
data. Existing stepwise methods struggle to generate proof steps that are both
logically valid and relevant to the hypothesis. Instead, they tend to
hallucinate invalid steps given the hypothesis. In this paper, we present a
novel stepwise method, NLProofS (Natural Language Proof Search), which learns
to generate relevant steps conditioning on the hypothesis. At the core of our
approach, we train an independent verifier to check the validity of the proof
steps to prevent hallucination. Instead of generating steps greedily, we search
for proofs maximizing a global proof score judged by the verifier. NLProofS
achieves state-of-the-art performance on EntailmentBank and RuleTaker.
Specifically, it improves the correctness of predicted proofs from 27.7% to
33.3% in the distractor setting of EntailmentBank, demonstrating the
effectiveness of NLProofS in generating challenging human-authored proofs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts. (arXiv:2206.12469v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12469">
<div class="article-summary-box-inner">
<span><p>We present Burst2Vec, our multi-task learning approach to predict emotion,
age, and origin (i.e., native country/language) from vocal bursts. Burst2Vec
utilises pre-trained speech representations to capture acoustic information
from raw waveforms and incorporates the concept of model debiasing via
adversarial training. Our models achieve a relative 30 % performance gain over
baselines using pre-extracted features and score the highest amongst all
participants in the ICML ExVo 2022 Multi-Task Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Translation with Compiler Representations. (arXiv:2207.03578v3 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03578">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage low-level compiler intermediate representations
(IR) to improve code translation. Traditional transpilers rely on syntactic
information and handcrafted rules, which limits their applicability and
produces unnatural-looking code. Applying neural machine translation (NMT)
approaches to code has successfully broadened the set of programs on which one
can get a natural-looking translation. However, they treat the code as
sequences of text tokens, and still do not differentiate well enough between
similar pieces of code which have different semantics in different languages.
The consequence is low quality translation, reducing the practicality of NMT,
and stressing the need for an approach significantly increasing its accuracy.
Here we propose to augment code translation with IRs, specifically LLVM IR,
with results on the C++, Java, Rust, and Go languages. Our method improves upon
the state of the art for unsupervised code translation, increasing the number
of correct translations by 11% on average, and up to 79% for the Java -&gt; Rust
pair with greedy decoding. With beam search, it increases the number of correct
translations by 5.5% in average. We extend previous test sets for code
translation, by adding hundreds of Go and Rust functions. Additionally, we
train models with high performance on the problem of IR decompilation,
generating programming source code from IR, and study using IRs as intermediary
pivot for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10643">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) predicts intent directly from
audio using a single model. It promises to improve the performance of assistant
systems by leveraging acoustic information lost in the intermediate textual
representation and preventing cascading errors from Automatic Speech
Recognition (ASR). Further, having one unified model has efficiency advantages
when deploying assistant systems on-device. However, the limited number of
public audio datasets with semantic parse labels hinders the research progress
in this area. In this paper, we release the Spoken Task-Oriented semantic
Parsing (STOP) dataset, the largest and most complex SLU dataset to be publicly
available. Additionally, we define low-resource splits to establish a benchmark
for improving SLU when limited labeled data is available. Furthermore, in
addition to the human-recorded audio, we are releasing a TTS-generated version
to benchmark the performance for low-resource domain adaptation of end-to-end
SLU systems. Initial experimentation show end-to-end SLU models performing
slightly worse than their cascaded counterparts, which we hope encourages
future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where's the Learning in Representation Learning for Compositional Semantics and the Case of Thematic Fit. (arXiv:2208.04749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.04749">
<div class="article-summary-box-inner">
<span><p>Observing that for certain NLP tasks, such as semantic role prediction or
thematic fit estimation, random embeddings perform as well as pretrained
embeddings, we explore what settings allow for this and examine where most of
the learning is encoded: the word embeddings, the semantic role embeddings, or
``the network''. We find nuanced answers, depending on the task and its
relation to the training objective. We examine these representation learning
aspects in multi-task learning, where role prediction and role-filling are
supervised tasks, while several thematic fit tasks are outside the models'
direct supervision. We observe a non-monotonous relation between some tasks'
quality score and the training data size. In order to better understand this
observation, we analyze these results using easier, per-verb versions of these
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation. (arXiv:2209.03316v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03316">
<div class="article-summary-box-inner">
<span><p>Pre-Training (PT) of text representations has been successfully applied to
low-resource Neural Machine Translation (NMT). However, it usually fails to
achieve notable gains (sometimes, even worse) on resource-rich NMT on par with
its Random-Initialization (RI) counterpart. We take the first step to
investigate the complementarity between PT and RI in resource-rich scenarios
via two probing analyses, and find that: 1) PT improves NOT the accuracy, but
the generalization by achieving flatter loss landscapes than that of RI; 2) PT
improves NOT the confidence of lexical choice, but the negative diversity by
assigning smoother lexical probability distributions than that of RI. Based on
these insights, we propose to combine their complementarities with a model
fusion algorithm that utilizes optimal transport to align neurons between PT
and RI. Experiments on two resource-rich translation benchmarks, WMT'17
English-Chinese (20M) and WMT'19 English-German (36M), show that PT and RI
could be nicely complementary to each other, achieving substantial improvements
considering both translation accuracy, generalization, and negative diversity.
Probing tools and code are released at: https://github.com/zanchangtong/PTvsRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Extraction and Human-Robot Dialogue towards Real-life Tasks: A Baseline Study with the MobileCS Dataset. (arXiv:2209.13464v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13464">
<div class="article-summary-box-inner">
<span><p>Recently, there have merged a class of task-oriented dialogue (TOD) datasets
collected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data
are in fact simulated data and thus are fundamentally different from real-life
conversations, which are more noisy and casual. Recently, the SereTOD challenge
is organized and releases the MobileCS dataset, which consists of real-world
dialog transcripts between real users and customer-service staffs from China
Mobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not
only evaluating the construction of the dialogue system itself, but also
examining information extraction from dialog transcripts, which is crucial for
building the knowledge base for TOD. This paper mainly presents a baseline
study of the two tasks with the MobileCS dataset. We introduce how the two
baselines are constructed, the problems encountered, and the results. We
anticipate that the baselines can facilitate exciting future research to build
human-robot dialogue systems for real-life tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. (arXiv:2210.01478v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01478">
<div class="article-summary-box-inner">
<span><p>AI systems are becoming increasingly intertwined with human life. In order to
effectively collaborate with humans and ensure safety, AI systems need to be
able to understand, interpret and predict human moral judgments and decisions.
Human moral judgments are often guided by rules, but not always. A central
challenge for AI safety is capturing the flexibility of the human moral mind --
the ability to determine when a rule should be broken, especially in novel or
unusual situations. In this paper, we present a novel challenge set consisting
of rule-breaking question answering (RBQA) of cases that involve potentially
permissible rule-breaking -- inspired by recent moral psychology studies. Using
a state-of-the-art large language model (LLM) as a basis, we propose a novel
moral chain of thought (MORALCOT) prompting strategy that combines the
strengths of LLMs with theories of moral reasoning developed in cognitive
science to predict human moral judgments. MORALCOT outperforms seven existing
LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to
capture the flexibility of the human moral mind. We also conduct a detailed
error analysis to suggest directions for future work to improve AI safety using
RBQA. Our data and code are available at https://github.com/feradauto/MoralCoT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Entity Typing with Curriculum Learning. (arXiv:2210.02914v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02914">
<div class="article-summary-box-inner">
<span><p>Entity typing aims to assign types to the entity mentions in given texts. The
traditional classification-based entity typing paradigm has two unignorable
drawbacks: 1) it fails to assign an entity to the types beyond the predefined
type set, and 2) it can hardly handle few-shot and zero-shot situations where
many long-tail types only have few or even no training instances. To overcome
these drawbacks, we propose a novel generative entity typing (GET) paradigm:
given a text with an entity mention, the multiple types for the role that the
entity plays in the text are generated with a pre-trained language model (PLM).
However, PLMs tend to generate coarse-grained types after fine-tuning upon the
entity typing dataset. Besides, we only have heterogeneous training data
consisting of a small portion of human-annotated data and a large portion of
auto-generated but low-quality data. To tackle these problems, we employ
curriculum learning (CL) to train our GET model upon the heterogeneous data,
where the curriculum could be self-adjusted with the self-paced learning
according to its comprehension of the type granularity and data heterogeneity.
Our extensive experiments upon the datasets of different languages and
downstream tasks justify the superiority of our GET model over the
state-of-the-art entity typing models. The code has been released on
https://github.com/siyuyuan/GET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03221">
<div class="article-summary-box-inner">
<span><p>With careful manipulation, malicious agents can reverse engineer private
information encoded in pre-trained language models. Security concerns motivate
the development of quantum pre-training. In this work, we propose a highly
portable quantum language model (PQLM) that can be easily transferred to
downstream tasks on classical machines. The framework consists of a cloud PQLM
built with random Variational Quantum Classifiers (VQC) and local models for
downstream applications. We demonstrate the portability of the quantum model by
extracting only the word embeddings and effectively applying them to downstream
tasks on classical machines. Our PQLM exhibits comparable performance to its
classical counterpart on both intrinsic evaluation (loss, perplexity) and
extrinsic evaluation (multilingual sentiment analysis accuracy) metrics and
achieves an accuracy of 93.4%, outperforming the classical model. We also
perform ablation studies on the factors affecting PQLM performance to analyze
model stability. Our work establishes a theoretical foundation for a portable
quantum pre-trained language model that could be trained on private data and
made available for public use with privacy protection guarantees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Factual Knowledge in Pretrained Language Models. (arXiv:2210.03329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03329">
<div class="article-summary-box-inner">
<span><p>Previous literature has proved that Pretrained Language Models (PLMs) can
store factual knowledge. However, we find that facts stored in the PLMs are not
always correct. It motivates us to explore a fundamental question: How do we
calibrate factual knowledge in PLMs without re-training from scratch? In this
work, we propose a simple and lightweight method CaliNet to achieve this goal.
To be specific, we first detect whether PLMs can learn the right facts via a
contrastive score between right and fake facts. If not, we then use a
lightweight method to add and adapt new parameters to specific factual texts.
Experiments on the knowledge probing task show the calibration effectiveness
and efficiency. In addition, through closed-book question answering, we find
that the calibrated PLM possesses knowledge generalization ability after
fine-tuning. Beyond the calibration performance, we further investigate and
visualize the knowledge calibration mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04284">
<div class="article-summary-box-inner">
<span><p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only
fine-tunes a few extra modules, becomes an appealing efficient alternative to
the full model fine-tuning. Although computationally efficient, the recent
Adapters often increase parameters (e.g. bottleneck dimension) for matching the
performance of full model fine-tuning, which we argue goes against their
original intention. In this work, we re-examine the parameter-efficiency of
Adapters through the lens of network pruning (we name such plug-in concept as
\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or
better performance than standard Adapters when the sparse ratio reaches up to
80\%. Based on our findings, we introduce an easy but effective setting
``\textit{Large-Sparse}'' to improve the model capacity of Adapters under the
same parameter budget. Experiments on five competitive Adapters upon three
advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.
40\%) SparseAdapter can consistently outperform their corresponding
counterpart. Encouragingly, with the \textit{Large-Sparse} setting, we can
obtain further appealing gains, even outperforming the full fine-tuning by a
large margin. Our code will be released at:
https://github.com/Shwai-He/SparseAdapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks. (arXiv:2210.04834v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04834">
<div class="article-summary-box-inner">
<span><p>Teacher-student knowledge distillation is a popular technique for compressing
today's prevailing large language models into manageable sizes that fit
low-latency downstream applications. Both the teacher and the choice of
transfer set used for distillation are crucial ingredients in creating a high
quality student. Yet, the generic corpora used to pretrain the teacher and the
corpora associated with the downstream target domain are often significantly
different, which raises a natural question: should the student be distilled
over the generic corpora, so as to learn from high-quality teacher predictions,
or over the downstream task corpora to align with finetuning? Our study
investigates this trade-off using Domain Classification (DC) and Intent
Classification/Named Entity Recognition (ICNER) as downstream tasks. We distill
several multilingual students from a larger multilingual LM with varying
proportions of generic and task-specific datasets, and report their performance
after finetuning on DC and ICNER. We observe significant improvements across
tasks and test sets when only task-specific corpora is used. We also report on
how the impact of adding task-specific data to the transfer set correlates with
the similarity between generic and task-specific data. Our results clearly
indicate that, while distillation from a generic LM benefits downstream tasks,
students learn better using target domain data even if it comes at the price of
noisier teacher predictions. In other words, target domain data still trumps
teacher knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence estimation of classification based on the distribution of the neural network output layer. (arXiv:2210.07745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07745">
<div class="article-summary-box-inner">
<span><p>One of the most common problems preventing the application of prediction
models in the real world is lack of generalization: The accuracy of models,
measured in the benchmark does repeat itself on future data, e.g. in the
settings of real business. There is relatively little methods exist that
estimate the confidence of prediction models. In this paper, we propose novel
methods that, given a neural network classification model, estimate uncertainty
of particular predictions generated by this model. Furthermore, we propose a
method that, given a model and a confidence level, calculates a threshold that
separates prediction generated by this model into two subsets, one of them
meets the given confidence level. In contrast to other methods, the proposed
methods do not require any changes on existing neural networks, because they
simply build on the output logit layer of a common neural network. In
particular, the methods infer the confidence of a particular prediction based
on the distribution of the logit values corresponding to this prediction. The
proposed methods constitute a tool that is recommended for filtering
predictions in the process of knowledge extraction, e.g. based on web
scrapping, where predictions subsets are identified that maximize the precision
on cost of the recall, which is less important due to the availability of data.
The method has been tested on different tasks including relation extraction,
named entity recognition and image classification to show the significant
increase of accuracy achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing. (arXiv:2210.07873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07873">
<div class="article-summary-box-inner">
<span><p>Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have
relied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.
2001). However, the data in HTB, a single-source newswire corpus, is now over
30 years old, and does not cover many aspects of contemporary Hebrew on the
web. This paper presents a new, freely available UD treebank of Hebrew
stratified from a range of topics selected from Hebrew Wikipedia. In addition
to introducing the corpus and evaluating the quality of its annotations, we
deploy automatic validation tools based on grew (Guillaume, 2021), and conduct
the first cross domain parsing experiments in Hebrew. We obtain new
state-of-the-art (SOTA) results on UD NLP tasks, using a combination of the
latest language modelling and some incremental improvements to existing
transformer based approaches. We also release a new version of the UD HTB
matching annotation scheme updates from our new corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective. (arXiv:2210.08590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08590">
<div class="article-summary-box-inner">
<span><p>We propose a new paradigm for zero-shot learners that is format agnostic,
i.e., it is compatible with any format and applicable to a list of language
tasks, such as text classification, commonsense reasoning, coreference
resolution, and sentiment analysis. Zero-shot learning aims to train a model on
a given task such that it can address new learning tasks without any additional
training. Our approach converts zero-shot learning into multiple-choice tasks,
avoiding problems in commonly used large-scale generative models such as FLAN.
It not only adds generalization ability to models but also significantly
reduces the number of parameters. Our method shares the merits of efficient
training and deployment. Our approach shows state-of-the-art performance on
several benchmarks and produces satisfactory results on tasks such as natural
language inference and text classification. Our model achieves this success
with only 235M parameters, which is substantially smaller than state-of-the-art
models with billions of parameters. The code and pre-trained models are
available at https://github.com/IDEA-CCNL/Fengshenbang-LM .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems. (arXiv:2210.08692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08692">
<div class="article-summary-box-inner">
<span><p>Building user simulators (USs) for reinforcement learning (RL) of
task-oriented dialog systems (DSs) has gained more and more attention, which,
however, still faces several fundamental challenges. First, it is unclear
whether we can leverage pretrained language models to design, for example,
GPT-2 based USs, to catch up and interact with the recently advanced GPT-2
based DSs. Second, an important ingredient in a US is that the user goal can be
effectively incorporated and tracked; but how to flexibly integrate goal state
tracking and develop an end-to-end trainable US for multi-domains has remained
to be a challenge. In this work, we propose a generative user simulator (GUS)
with GPT-2 based architecture and goal state tracking towards addressing the
above two challenges. Extensive experiments are conducted on MultiWOZ2.1.
Different DSs are trained via RL with GUS, the classic agenda-based user
simulator (ABUS) and other ablation simulators respectively, and are compared
for cross-model evaluation, corpus-based evaluation and human evaluation. The
GUS achieves superior results in all three evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Multilingual Knowledge Graph Completion and Alignment. (arXiv:2210.08922v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08922">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) alignment and completion are usually treated as two
independent tasks. While recent work has leveraged entity and relation
alignments from multiple KGs, such as alignments between multilingual KGs with
common entities and relations, a deeper understanding of the ways in which
multilingual KG completion (MKGC) can aid the creation of multilingual KG
alignments (MKGA) is still limited. Motivated by the observation that
structural inconsistencies -- the main challenge for MKGA models -- can be
mitigated through KG completion methods, we propose a novel model for jointly
completing and aligning knowledge graphs. The proposed model combines two
components that jointly accomplish KG completion and alignment. These two
components employ relation-aware graph neural networks that we propose to
encode multi-hop neighborhood structures into entity and relation
representations. Moreover, we also propose (i) a structural inconsistency
reduction mechanism to incorporate information from the completion into the
alignment component, and (ii) an alignment seed enlargement and triple
transferring mechanism to enlarge alignment seeds and transfer triples during
KGs alignment. Extensive experiments on a public multilingual benchmark show
that our proposed model outperforms existing competitive baselines, obtaining
new state-of-the-art results on both MKGC and MKGA tasks. We publicly release
the implementation of our model at https://github.com/vinhsuhi/JMAC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v1 [cs.AI] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09306">
<div class="article-summary-box-inner">
<span><p>An increasingly prevalent problem for intelligent technologies is text
safety, as uncontrolled systems may generate recommendations to their users
that lead to injury or life-threatening consequences. However, the degree of
explicitness of a generated statement that can cause physical harm varies. In
this paper, we distinguish types of text that can lead to physical harm and
establish one particularly underexplored category: covertly unsafe text. Then,
we further break down this category with respect to the system's information
and discuss solutions to mitigate the generation of text in each of these
subcategories. Ultimately, our work defines the problem of covertly unsafe
language that causes physical harm and argues that this subtle yet dangerous
issue needs to be prioritized by stakeholders and regulators. We highlight
mitigation strategies to inspire future researchers to tackle this challenging
problem and help improve safety within smart systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-19 23:21:09.744867338 UTC">2022-10-19 23:21:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>