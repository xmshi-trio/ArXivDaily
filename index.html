<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-13T01:30:00Z">06-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and Relation Names. (arXiv:2306.06141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06141">
<div class="article-summary-box-inner">
<span><p>Developing dialogue relation extraction (DRE) systems often requires a large
amount of labeled data, which can be costly and time-consuming to annotate. In
order to improve scalability and support diverse, unseen relation extraction,
this paper proposes a method for leveraging the ability to capture triggers and
relate them to previously unseen relation names. Specifically, we introduce a
model that enables zero-shot dialogue relation extraction by utilizing
trigger-capturing capabilities. Our experiments on a benchmark DialogRE dataset
demonstrate that the proposed model achieves significant improvements for both
seen and unseen relations. Notably, this is the first attempt at zero-shot
dialogue relation extraction using trigger-capturing capabilities, and our
results suggest that this approach is effective for inferring previously unseen
relation types. Overall, our findings highlight the potential for this method
to enhance the scalability and practicality of DRE systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation. (arXiv:2306.06147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06147">
<div class="article-summary-box-inner">
<span><p>This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis
dataset. Comprising 70,000 samples, it was created from diverse sources and
annotated by a gender-balanced team of linguists. SentiGOLD adheres to
established linguistic conventions agreed upon by the Government of Bangladesh
and a Bangla linguistics committee. Unlike English and other languages, Bangla
lacks standard sentiment analysis datasets due to the absence of a national
linguistics framework. The dataset incorporates data from online video
comments, social media posts, blogs, news, and other sources while maintaining
domain and class distribution rigorously. It spans 30 domains (e.g., politics,
entertainment, sports) and includes 5 sentiment classes (strongly negative,
weakly negative, neutral, and strongly positive). The annotation scheme,
approved by the national linguistics committee, ensures a robust Inter
Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and
cross-dataset evaluation protocols are applied to establish a standard
classification system. Cross-dataset evaluation on the noisy SentNoB dataset
presents a challenging test scenario. Additionally, zero-shot experiments
demonstrate the generalizability of SentiGOLD. The top model achieves a macro
f1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and
0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the
state-of-the-art. Fine-tuned sentiment analysis model can be accessed at
https://sentiment.bangla.gov.bd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06190">
<div class="article-summary-box-inner">
<span><p>Pre-training Transformers has shown promising results on open-domain and
domain-specific downstream tasks. However, state-of-the-art Transformers
require an unreasonably large amount of pre-training data and compute. In this
paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level
Metadata), a novel, compute-efficient framework that utilizes Document metadata
and Domain-Specific Taxonomy as supervision signals to pre-train transformer
encoder on a domain-specific corpus. The main innovation is that during
domain-specific pretraining, an open-domain encoder is continually pre-trained
using sentence-level embeddings as inputs (to accommodate long documents),
however, fine-tuning is done with token-level embeddings as inputs to this
encoder. We show that $FPDM$ outperforms several transformer-based baselines in
terms of character-level F1 scores and other automated metrics in the Customer
Support, Scientific, and Legal Domains, and shows a negligible drop in
performance on open-domain benchmarks. Importantly, the novel use of
document-level supervision along with sentence-level embedding input for
pre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$
times compared to MLM and/or NSP in Customer Support, Scientific, and Legal
Domains, respectively. Code and datasets are available at
https://bit.ly/FPDMCode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06199">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have become mainstream technology with their
versatile use cases and impressive performance. Despite the countless
out-of-the-box applications, LLMs are still not reliable. A lot of work is
being done to improve the factual accuracy, consistency, and ethical standards
of these models through fine-tuning, prompting, and Reinforcement Learning with
Human Feedback (RLHF), but no systematic analysis of the responses of these
models to different categories of statements, or on their potential
vulnerabilities to simple prompting changes is available. In this work, we
analyze what confuses GPT-3: how the model responds to certain sensitive topics
and what effects the prompt wording has on the model response. We find that
GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes
mistakes with common Misconceptions and Controversies. The model responses are
inconsistent across prompts and settings, highlighting GPT-3's unreliability.
Dataset and code of our analysis is available in
https://github.com/tanny411/GPT3-Reliability-Check.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphosyntactic probing of multilingual BERT models. (arXiv:2306.06205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06205">
<div class="article-summary-box-inner">
<span><p>We introduce an extensive dataset for multilingual probing of morphological
information in language models (247 tasks across 42 languages from 10
families), each consisting of a sentence with a target word and a morphological
tag as the desired label, derived from the Universal Dependencies treebanks. We
find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features
that attain strong performance across these tasks. We then apply two methods to
locate, for each probing task, where the disambiguating information resides in
the input. The first is a new perturbation method that masks various parts of
context; the second is the classical method of Shapley values. The most
intriguing finding that emerges is a strong tendency for the preceding context
to hold more information relevant to the prediction than the following context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conformalizing Machine Translation Evaluation. (arXiv:2306.06221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06221">
<div class="article-summary-box-inner">
<span><p>Several uncertainty estimation methods have been recently proposed for
machine translation evaluation. While these methods can provide a useful
indication of when not to trust model predictions, we show in this paper that
the majority of them tend to underestimate model uncertainty, and as a result
they often produce misleading confidence intervals that do not cover the ground
truth. We propose as an alternative the use of conformal prediction, a
distribution-free method to obtain confidence intervals with a theoretically
established guarantee on coverage. First, we demonstrate that split conformal
prediction can ``correct'' the confidence intervals of previous methods to
yield a desired coverage level. Then, we highlight biases in estimated
confidence intervals, both in terms of the translation language pairs and the
quality of translations. We apply conditional conformal prediction techniques
to obtain calibration subsets for each data subgroup, leading to equalized
coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration. (arXiv:2306.06232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06232">
<div class="article-summary-box-inner">
<span><p>Textless self-supervised speech models have grown in capabilities in recent
years, but the nature of the linguistic information they encode has not yet
been thoroughly examined. We evaluate the extent to which these models' learned
representations align with basic representational distinctions made by humans,
focusing on a set of phonetic (low-level) and phonemic (more abstract)
contrasts instantiated in word-initial stops. We find that robust
representations of both phonetic and phonemic distinctions emerge in early
layers of these models' architectures, and are preserved in the principal
components of deeper layer representations. Our analyses suggest two sources
for this success: some can only be explained by the optimization of the models
on speech data, while some can be attributed to these models' high-dimensional
architectures. Our findings show that speech-trained HuBERT derives a low-noise
and low-dimensional subspace corresponding to abstract phonological
distinctions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06234">
<div class="article-summary-box-inner">
<span><p>Foundation models, i.e. large neural networks pre-trained on large text
corpora, have revolutionized NLP. They can be instructed directly (e.g.
(<a href="/abs/2005.14165">arXiv:2005.14165</a>)) - this is called hard prompting - and they can be tuned
using very little data (e.g. (<a href="/abs/2104.08691">arXiv:2104.08691</a>)) - this technique is called
soft prompting. We seek to leverage their capabilities to detect policy
violations. Our contributions are: We identify a hard prompt that adapts
chain-of-thought prompting to policy violation tasks. This prompt produces
policy violation classifications, along with extractive explanations that
justify the classification. We compose the hard-prompts with soft prompt tuning
to produce a classifier that attains high accuracy with very little
supervision; the same classifier also produces explanations. Though the
supervision only acts on the classifications, we find that the modified
explanations remain consistent with the (tuned) model's response. Along the
way, we identify several unintuitive aspects of foundation models. For
instance, adding an example from a specific class can actually reduce
predictions of that class, and separately, the effects of tokenization on
scoring etc. Based on our technical results, we identify a simple workflow for
product teams to quickly develop effective policy violation detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Record Deduplication for Entity Distribution Modeling in ASR Transcripts. (arXiv:2306.06246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06246">
<div class="article-summary-box-inner">
<span><p>Voice digital assistants must keep up with trending search queries. We rely
on a speech recognition model using contextual biasing with a rapidly updated
set of entities, instead of frequent model retraining, to keep up with trends.
There are several challenges with this approach: (1) the entity set must be
frequently reconstructed, (2) the entity set is of limited size due to latency
and accuracy trade-offs, and (3) finding the true entity distribution for
biasing is complicated by ASR misrecognition. We address these challenges and
define an entity set by modeling customers true requested entity distribution
from ASR output in production using record deduplication, a technique from the
field of entity resolution. Record deduplication resolves or deduplicates
coreferences, including misrecognitions, of the same latent entity. Our method
successfully retrieves 95% of misrecognized entities and when used for
contextual biasing shows an estimated 5% relative word error rate reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Modifying Factual Knowledge in Large Language Models. (arXiv:2306.06264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06264">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) store an extensive amount of factual knowledge
obtained from vast collections of text. To effectively utilize these models for
downstream tasks, it is crucial to have reliable methods for measuring their
knowledge. However, existing approaches for knowledge measurement have certain
limitations, and despite recent efforts, they fail to provide accurate
measurements and the necessary insights for modifying the knowledge within
LLMs. In this work, we employ information theory-based measurements to provide
a framework estimating the factual knowledge contained within large language
models. More specifically, we measure knowledge by analyzing the LLM's
prediction probability distribution before and after instilling the target
knowledge, employing metrics such as entropy and KL-divergence. Introducing our
metrics, we first assess their accuracy in comparison to previous ranking-based
methods, surpassing them by over $35\%$ in a synthetic experiment. Then, we
explore two prominent methods of knowledge instillation, discovering that LLMs
exhibit limitations in capturing new knowledge under specific circumstances for
one of these methods. Lastly, we demonstrate the applicability of our methods
in extracting unlearned and mislearned facts in LLMs through their application
to in-context learning. We make code and data for all methods and experiments
in this paper publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protect Your Prompts: Protocols for IP Protection in LLM Applications. (arXiv:2306.06297v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06297">
<div class="article-summary-box-inner">
<span><p>With the rapid adoption of AI in the form of large language models (LLMs),
the potential value of carefully engineered prompts has become significant.
However, to realize this potential, prompts should be tradable on an open
market. Since prompts are, at present, generally economically non-excludable,
by virtue of their nature as text, no general competitive market has yet been
established. This note discusses two protocols intended to provide protection
of prompts, elevating their status as intellectual property, thus confirming
the intellectual property rights of prompt engineers, and potentially
supporting the flourishing of an open market for LLM prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Arabic Multimodal Dataset for Sentiment Analysis. (arXiv:2306.06322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06322">
<div class="article-summary-box-inner">
<span><p>Multimodal Sentiment Analysis (MSA) has recently become a centric research
direction for many real-world applications. This proliferation is due to the
fact that opinions are central to almost all human activities and are key
influencers of our behaviors. In addition, the recent deployment of Deep
Learning-based (DL) models has proven their high efficiency for a wide range of
Western languages. In contrast, Arabic DL-based multimodal sentiment analysis
(MSA) is still in its infantile stage due, mainly, to the lack of standard
datasets. In this paper, our investigation is twofold. First, we design a
pipeline that helps building our Arabic Multimodal dataset leveraging both
state-of-the-art transformers and feature extraction tools within word
alignment techniques. Thereafter, we validate our dataset using
state-of-the-art transformer-based model dealing with multimodality. Despite
the small size of the outcome dataset, experiments show that Arabic
multimodality is very promising
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. (arXiv:2306.06331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06331">
<div class="article-summary-box-inner">
<span><p>This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC. (arXiv:2306.06345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06345">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive approaches aim to improve the inference speed of
translation models, particularly those that generate output in a one-pass
forward manner. However, these approaches often suffer from a significant drop
in translation quality compared to autoregressive models. This paper introduces
a series of innovative techniques to enhance the translation quality of
Non-Autoregressive Translation (NAT) models while maintaining a substantial
acceleration in inference speed. We propose fine-tuning Pretrained Multilingual
Language Models (PMLMs) with the CTC loss to train NAT models effectively.
Furthermore, we adopt the MASK insertion scheme for up-sampling instead of
token duplication, and we present an embedding distillation method to further
enhance performance. In our experiments, our model outperforms the baseline
autoregressive model (Transformer \textit{base}) on multiple datasets,
including WMT'14 DE$\leftrightarrow$EN, WMT'16 RO$\leftrightarrow$EN, and
IWSLT'14 DE$\leftrightarrow$EN. Notably, our model achieves better performance
than the baseline autoregressive model on the IWSLT'14 En$\leftrightarrow$De
and WMT'16 En$\leftrightarrow$Ro datasets, even without using distillation data
during training. It is worth highlighting that on the IWSLT'14
DE$\rightarrow$EN dataset, our model achieves an impressive BLEU score of
39.59, setting a new state-of-the-art performance. Additionally, our model
exhibits a remarkable speed improvement of 16.35 times compared to the
autoregressive model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text. (arXiv:2306.06371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06371">
<div class="article-summary-box-inner">
<span><p>Java Code Generation consists in generating automatically Java code from a
Natural Language Text. This NLP task helps in increasing programmers'
productivity by providing them with immediate solutions to the simplest and
most repetitive tasks. Code generation is a challenging task because of the
hard syntactic rules and the necessity of a deep understanding of the semantic
aspect of the programming language. Many works tried to tackle this task using
either RNN-based, or Transformer-based models. The latter achieved remarkable
advancement in the domain and they can be divided into three groups: (1)
encoder-only models, (2) decoder-only models, and (3) encoder-decoder models.
In this paper, we provide a comprehensive review of the evolution and progress
of deep learning models in Java code generation task. We focus on the most
important methods and present their merits and limitations, as well as the
objective functions used by the community. In addition, we provide a detailed
description of datasets and evaluation metrics used in the literature. Finally,
we discuss results of different models on CONCODE dataset, then propose some
future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Row, Multi-Span Distant Supervision For Table+Text Question. (arXiv:2112.07337v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07337">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) over tables and linked text, also called TextTableQA,
has witnessed significant research in recent years, as tables are often found
embedded in documents along with related text. HybridQA and OTT-QA are the two
best-known TextTableQA datasets, with questions that are best answered by
combining information from both table cells and linked text passages. A common
challenge in both datasets, and TextTableQA in general, is that the training
instances include just the question and answer, where the gold answer may match
not only multiple table cells across table rows but also multiple text spans
within the scope of a table row and its associated text. This leads to a noisy
multi instance training regime. We present MITQA, a transformer-based
TextTableQA system that is explicitly designed to cope with distant supervision
along both these axes, through a multi-instance loss objective, together with
careful curriculum design. Our experiments show that the proposed
multi-instance distant supervision approach helps MITQA get state-of-the-art
results beating the existing baselines for both HybridQA and OTT-QA, putting
MITQA at the top of HybridQA leaderboard with best EM and F1 scores on a held
out test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00648">
<div class="article-summary-box-inner">
<span><p>Human speech data comprises a rich set of domain factors such as accent,
syntactic and semantic variety, or acoustic environment. Previous work explores
the effect of domain mismatch in automatic speech recognition between
pre-training and fine-tuning as a whole but does not dissect the contribution
of individual factors. In this paper, we present a controlled study to better
understand the effect of such factors on the performance of pre-trained
representations on automatic speech recognition. To do so, we pre-train models
either on modified natural speech or synthesized audio, with a single domain
factor modified, and then measure performance after fine-tuning. Results show
that phonetic domain factors play an important role during pre-training while
grammatical and syntactic factors are far less important. To our knowledge,
this is the first study to better understand the domain characteristics of
pre-trained sets in self-supervised pre-training for speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03047">
<div class="article-summary-box-inner">
<span><p>In recent years, considerable research has been dedicated to the application
of neural models in the field of natural language generation (NLG). The primary
objective is to generate text that is both linguistically natural and
human-like, while also exerting control over the generation process. This paper
offers a comprehensive and task-agnostic survey of the recent advancements in
neural text generation. These advancements have been facilitated through a
multitude of developments, which we categorize into four key areas: data
construction, neural frameworks, training and inference strategies, and
evaluation metrics. By examining these different aspects, we aim to provide a
holistic overview of the progress made in the field. Furthermore, we explore
the future directions for the advancement of neural text generation, which
encompass the utilization of neural pipelines and the incorporation of
background knowledge. These avenues present promising opportunities to further
enhance the capabilities of NLG systems. Overall, this survey serves to
consolidate the current state of the art in neural text generation and
highlights potential avenues for future research and development in this
dynamic field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClaimDiff: Comparing and Contrasting Claims on Contentious Issues. (arXiv:2205.12221v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12221">
<div class="article-summary-box-inner">
<span><p>With the growing importance of detecting misinformation, many studies have
focused on verifying factual claims by retrieving evidence. However, canonical
fact verification tasks do not apply to catching subtle differences in
factually consistent claims, which might still bias the readers, especially on
contentious political or economic issues. Our underlying assumption is that
among the trusted sources, one's argument is not necessarily more true than the
other, requiring comparison rather than verification. In this study, we propose
ClaimDiff, a novel dataset that primarily focuses on comparing the nuance
between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from
268 news articles. We observe that while humans are capable of detecting the
nuances between claims, strong baselines struggle to detect them, showing over
a 19% absolute gap with the humans. We hope this initial study could help
readers to gain an unbiased grasp of contentious issues through machine-aided
comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00621">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Cross-View Language Modeling, a simple and
effective pre-training framework that unifies cross-lingual and cross-modal
pre-training with shared architectures and objectives. Our approach is
motivated by a key observation that cross-lingual and cross-modal pre-training
share the same goal of aligning two different views of the same object into a
common semantic space. To this end, the cross-view language modeling framework
considers both multi-modal data (i.e., image-caption pairs) and multi-lingual
data (i.e., parallel sentence pairs) as two different views of the same object,
and trains the model to align the two views by maximizing the mutual
information between them with conditional masked language modeling and
contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language
Model, with the cross-view language modeling framework. Empirical results on
IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text
retrieval datasets show that while conceptually simpler, CCLM significantly
outperforms the prior state-of-the-art with an average absolute improvement of
over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained
model that surpasses the translate-test performance of representative English
vision-language models by zero-shot cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation. (arXiv:2206.02999v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02999">
<div class="article-summary-box-inner">
<span><p>The computational benefits of iterative non-autoregressive transformers
decrease as the number of decoding steps increases. As a remedy, we introduce
Distill Multiple Steps (DiMS), a simple yet effective distillation technique to
decrease the number of required steps to reach a certain translation quality.
The distilled model enjoys the computational benefits of early iterations while
preserving the enhancements from several iterative steps. DiMS relies on two
models namely student and teacher. The student is optimized to predict the
output of the teacher after multiple decoding steps while the teacher follows
the student via a slow-moving average. The moving average keeps the teacher's
knowledge updated and enhances the quality of the labels provided by the
teacher. During inference, the student is used for translation and no
additional computation is added. We verify the effectiveness of DiMS on various
models obtaining 7.8 and 12.9 BLEU points improvements in single-step
translation accuracy on distilled and raw versions of WMT'14 De-En.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04615">
<div class="article-summary-box-inner">
<span><p>Language models demonstrate both quantitative improvement and new qualitative
capabilities with increasing scale. Despite their potentially transformative
impact, these new capabilities are as yet poorly characterized. In order to
inform future research, prepare for disruptive new model capabilities, and
ameliorate socially harmful effects, it is vital that we understand the present
and near-future capabilities and limitations of language models. To address
this challenge, we introduce the Beyond the Imitation Game benchmark
(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450
authors across 132 institutions. Task topics are diverse, drawing problems from
linguistics, childhood development, math, common-sense reasoning, biology,
physics, social bias, software development, and beyond. BIG-bench focuses on
tasks that are believed to be beyond the capabilities of current language
models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense
transformer architectures, and Switch-style sparse transformers on BIG-bench,
across model sizes spanning millions to hundreds of billions of parameters. In
addition, a team of human expert raters performed all tasks in order to provide
a strong baseline. Findings include: model performance and calibration both
improve with scale, but are poor in absolute terms (and when compared with
rater performance); performance is remarkably similar across model classes,
though with benefits from sparsity; tasks that improve gradually and
predictably commonly involve a large knowledge or memorization component,
whereas tasks that exhibit "breakthrough" behavior at a critical scale often
involve multiple steps or components, or brittle metrics; social bias typically
increases with scale in settings with ambiguous context, but this can be
improved with prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15462">
<div class="article-summary-box-inner">
<span><p>We propose a margin-based loss for vision-language model pretraining that
encourages gradient-based explanations that are consistent with region-level
annotations. We refer to this objective as Attention Mask Consistency (AMC) and
demonstrate that it produces superior visual grounding performance compared to
models that rely instead on region-level annotations for explicitly training an
object detector such as Faster R-CNN. AMC works by encouraging gradient-based
explanation masks that focus their attention scores mostly within annotated
regions of interest for images that contain such annotations. Particularly, a
model trained with AMC on top of standard vision-language modeling objectives
obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding
benchmark, an absolute improvement of 5.48% when compared to the best previous
model. Our approach also performs exceedingly well on established benchmarks
for referring expression comprehension and offers the added benefit by design
of gradient-based explanations that better align with human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. (arXiv:2210.06313v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06313">
<div class="article-summary-box-inner">
<span><p>This paper studies the curious phenomenon for machine learning models with
Transformer architectures that their activation maps are sparse. By activation
map we refer to the intermediate output of the multi-layer perceptrons (MLPs)
after a ReLU activation function, and by sparse we mean that on average very
few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each
input to MLP. Moreover, larger Transformers with more layers and wider MLP
hidden dimensions are sparser as measured by the percentage of nonzero entries.
Through extensive experiments we demonstrate that the emergence of sparsity is
a prevalent phenomenon that occurs for both natural language processing and
vision tasks, on both training and evaluation data, for Transformers of various
configurations, at layers of all depth levels, as well as for other
architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also
emerges using training datasets with random labels, or with random inputs, or
with infinite amount of data, demonstrating that sparsity is not a result of a
specific family of datasets. We discuss how sparsity immediately implies a way
to significantly reduce the FLOP count and improve efficiency for Transformers.
Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser
activation via Top-k thresholding with a small value of k brings a collection
of desired but missing properties for Transformers, namely less sensitivity to
noisy training data, more robustness to input corruptions, and better
calibration for their prediction confidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis. (arXiv:2210.06629v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06629">
<div class="article-summary-box-inner">
<span><p>Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark datasets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
8.29 F1) in the few-shot learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00375">
<div class="article-summary-box-inner">
<span><p>The gender of any voice user interface is a key element of its perceived
identity. Recently, there has been increasing interest in interfaces where the
gender is ambiguous rather than clearly identifying as female or male. This
work addresses the task of generating novel gender-ambiguous TTS voices in a
multi-speaker, multilingual setting. This is accomplished by efficiently
sampling from a latent speaker embedding space using a proposed gender-aware
method. Extensive objective and subjective evaluations clearly indicate that
this method is able to efficiently generate a range of novel, diverse voices
that are consistent and perceived as more gender-ambiguous than a baseline
voice across all the languages examined. Interestingly, the gender perception
is found to be robust across two demographic factors of the listeners: native
language and gender. To our knowledge, this is the first systematic and
validated approach that can reliably generate a variety of gender-ambiguous
voices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Debiasing Inevitably Degrade the Model Performance. (arXiv:2211.07350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07350">
<div class="article-summary-box-inner">
<span><p>Gender bias in language models has attracted sufficient attention because it
threatens social justice. However, most of the current debiasing methods
degraded the model's performance on other tasks while the degradation mechanism
is still mysterious. We propose a theoretical framework explaining the three
candidate mechanisms of the language model's gender bias. We use our
theoretical framework to explain why the current debiasing methods cause
performance degradation. We also discover a pathway through which debiasing
will not degrade the model performance. We further develop a
causality-detection fine-tuning approach to correct gender bias. The numerical
experiment demonstrates that our method is able to lead to double dividends:
partially mitigating gender bias while avoiding performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07828">
<div class="article-summary-box-inner">
<span><p>Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced
impressive gains over purely parametric LMs, by leveraging large-scale
neighborhood retrieval over external memory datastores. However, there has been
little investigation into adapting such models for new domains. This work
attempts to fill that gap and suggests the following approaches for adapting
$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding
neighborhood retrieval over an additional adaptation datastore, and 3) adapting
the weights (scores) of retrieved neighbors using a learned Rescorer module. We
study each adaptation strategy separately, as well as the combined performance
improvement through ablation experiments and an extensive set of evaluations
run over seven adaptation domains. Our combined adaptation approach
consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)
baselines that construct datastores from the adaptation data. On average, we
see perplexity improvements of 17.1% and 16% for these respective baselines,
across domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Neural Topic Models. (arXiv:2212.02269v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02269">
<div class="article-summary-box-inner">
<span><p>Over the last years, topic modeling has emerged as a powerful technique for
organizing and summarizing big collections of documents or searching for
particular patterns in them. However, privacy concerns may arise when
cross-analyzing data from different sources. Federated topic modeling solves
this issue by allowing multiple parties to jointly train a topic model without
sharing their data. While several federated approximations of classical topic
models do exist, no research has been conducted on their application for neural
topic models. To fill this gap, we propose and analyze a federated
implementation based on state-of-the-art neural topic modeling implementations,
showing its benefits when there is a diversity of topics across the nodes'
documents and the need to build a joint model. In practice, our approach is
equivalent to a centralized model training, but preserves the privacy of the
nodes. Advantages of this federated scenario are illustrated by means of
experiments using both synthetic and real data scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Knowledge Distillation for Neural Machine Translation. (arXiv:2212.09097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09097">
<div class="article-summary-box-inner">
<span><p>While many parallel corpora are not publicly accessible for data copyright,
data privacy and competitive differentiation reasons, trained translation
models are increasingly available on open platforms. In this work, we propose a
method called continual knowledge distillation to take advantage of existing
translation models to improve one model of interest. The basic idea is to
sequentially transfer knowledge from each trained model to the distilled model.
Extensive experiments on Chinese-English and German-English datasets show that
our method achieves significant and consistent improvements over strong
baselines under both homogeneous and heterogeneous trained model settings and
is robust to malicious models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLIND: Bias Removal With No Demographics. (arXiv:2212.10563v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10563">
<div class="article-summary-box-inner">
<span><p>Models trained on real-world data tend to imitate and amplify social biases.
Common methods to mitigate biases require prior information on the types of
biases that should be mitigated (e.g., gender or racial bias) and the social
groups associated with each data sample. In this work, we introduce BLIND, a
method for bias removal with no prior knowledge of the demographics in the
dataset. While training a model on a downstream task, BLIND detects biased
samples using an auxiliary model that predicts the main model's success, and
down-weights those samples during the training process. Experiments with racial
and gender biases in sentiment classification and occupation classification
tasks demonstrate that BLIND mitigates social biases without relying on a
costly demographic annotation process. Our method is competitive with other
methods that require demographic information and sometimes even surpasses them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10773">
<div class="article-summary-box-inner">
<span><p>Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However, it
has yet to be explored for vision and multimodal tasks. In this work, we
introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark
dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq
format covering 10 broad categories. The tasks are derived from 21 existing
open-source datasets and each task is equipped with 5 expert-written
instructions. We take OFA as the base pre-trained model for multimodal
instruction tuning, and to further improve its zero-shot performance, we
explore multiple transfer learning strategies to leverage the large-scale
NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot
performance on various unseen multimodal tasks and the benefit of transfer
learning from a text-only instruction dataset. We also design a new evaluation
metric - Sensitivity, to evaluate how sensitive the model is to the variety of
instructions. Our results indicate that fine-tuning the model on a diverse set
of tasks and instructions leads to a reduced sensitivity to variations in
instructions for each task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10724">
<div class="article-summary-box-inner">
<span><p>OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and
revolutionized the approach in artificial intelligence to human-model
interaction. Several publications on ChatGPT evaluation test its effectiveness
on well-known natural language processing (NLP) tasks. However, the existing
studies are mostly non-automated and tested on a very limited scale. In this
work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,
most of them subjective even to humans, such as sentiment analysis, emotion
recognition, offensiveness, and stance detection. In contrast, the other tasks
require more objective reasoning like word sense disambiguation, linguistic
acceptability, and question answering. We also evaluated GPT-4 model on five
selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process
and analyzed more than 49k responses. Our comparison of its results with
available State-of-the-Art (SOTA) solutions showed that the average loss in
quality of the ChatGPT model was about 25% for zero-shot and few-shot
evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower
than for ChatGPT. We showed that the more difficult the task (lower SOTA
performance), the higher the ChatGPT loss. It especially refers to pragmatic
NLP problems like emotion recognition. We also tested the ability to
personalize ChatGPT responses for selected subjective tasks via Random
Contextual Few-Shot Personalization, and we obtained significantly better
user-based predictions. Additional qualitative analysis revealed a ChatGPT
bias, most likely due to the rules imposed on human trainers by OpenAI. Our
results provide the basis for a fundamental discussion of whether the high
quality of recent predictive NLP models can indicate a tool's usefulness to
society and how the learning and validation procedures for such systems should
be established.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of Text Vectorizers. (arXiv:2303.07203v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07203">
<div class="article-summary-box-inner">
<span><p>A fundamental issue in machine learning is the robustness of the model with
respect to changes in the input. In natural language processing, models
typically contain a first embedding layer, transforming a sequence of tokens
into vector representations. While the robustness with respect to changes of
continuous inputs is well-understood, the situation is less clear when
considering discrete changes, for instance replacing a word by another in an
input sentence. Our work formally proves that popular embedding schemes, such
as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit
robustness in the H\"older or Lipschitz sense with respect to the Hamming
distance. We provide quantitative bounds for these schemes and demonstrate how
the constants involved are affected by the length of the document. These
findings are exemplified through a series of numerical examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stop Words for Processing Software Engineering Documents: Do they Matter?. (arXiv:2303.10439v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10439">
<div class="article-summary-box-inner">
<span><p>Stop words, which are considered non-predictive, are often eliminated in
natural language processing tasks. However, the definition of uninformative
vocabulary is vague, so most algorithms use general knowledge-based stop lists
to remove stop words. There is an ongoing debate among academics about the
usefulness of stop word elimination, especially in domain-specific settings. In
this work, we investigate the usefulness of stop word removal in a software
engineering context. To do this, we replicate and experiment with three
software engineering research tools from related work. Additionally, we
construct a corpus of software engineering domain-related text from 10,000
Stack Overflow questions and identify 200 domain-specific stop words using
traditional information-theoretic methods. Our results show that the use of
domain-specific stop words significantly improved the performance of research
tools compared to the use of a general stop list and that 17 out of 19
evaluation measures showed better performance.
</p>
<p>Online appendix: https://zenodo.org/record/7865748
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflexion: Language Agents with Verbal Reinforcement Learning. (arXiv:2303.11366v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11366">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13310">
<div class="article-summary-box-inner">
<span><p>We present SwissBERT, a masked language model created specifically for
processing Switzerland-related text. SwissBERT is a pre-trained model that we
adapted to news articles written in the national languages of Switzerland --
German, French, Italian, and Romansh. We evaluate SwissBERT on natural language
understanding tasks related to Switzerland and find that it tends to outperform
previous models on these tasks, especially when processing contemporary news
and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be
extended to Swiss German dialects in future work. The model and our open-source
code are publicly released at https://github.com/ZurichNLP/swissbert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v3 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06858">
<div class="article-summary-box-inner">
<span><p>Vaccine hesitancy continues to be a main challenge for public health
officials during the COVID-19 pandemic. As this hesitancy undermines vaccine
campaigns, many researchers have sought to identify its root causes, finding
that the increasing volume of anti-vaccine misinformation on social media
platforms is a key element of this problem. We explored Twitter as a source of
misleading content with the goal of extracting overlapping cultural and
political beliefs that motivate the spread of vaccine misinformation. To do
this, we have collected a data set of vaccine-related Tweets and annotated them
with the help of a team of annotators with a background in communications and
journalism. Ultimately we hope this can lead to effective and targeted public
health communication strategies for reaching individuals with anti-vaccine
beliefs. Moreover, this information helps with developing Machine Learning
models to automatically detect vaccine misinformation posts and combat their
negative impacts. In this paper, we present Vax-Culture, a novel Twitter
COVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an
extensive set of human-provided annotations including vaccine-hesitancy stance,
indication of any misinformation in tweets, the entities criticized and
supported in each tweet and the communicated message of each tweet. Moreover,
we define five baseline tasks including four classification and one sequence
generation tasks, and report the results of a set of recent transformer-based
models for them. The dataset and code are publicly available at
https://github.com/mrzarei5/Vax-Culture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text. (arXiv:2304.06939v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06939">
<div class="article-summary-box-inner">
<span><p>In-context vision and language models like Flamingo support arbitrarily
interleaved sequences of images and text as input. This format not only enables
few-shot learning via interleaving independent supervised (image, text)
examples, but also, more complex prompts involving interaction between images,
e.g., "What do image A and image B have in common?" To support this interface,
pretraining occurs over web corpora that similarly contain interleaved
images+text. To date, however, large-scale data of this form have not been
publicly available.
</p>
<p>We release Multimodal C4, an augmentation of the popular text-only C4 corpus
with images interleaved. We use a linear assignment algorithm to place images
into longer bodies of text using CLIP features, a process that we show
outperforms alternatives. Multimodal C4 spans everyday topics like cooking,
travel, technology, etc. A manual inspection of a random sample of documents
shows that a vast majority (88%) of images are topically relevant, and that
linear assignment frequently selects individual sentences specifically
well-aligned with each image (80%). After filtering NSFW images, ads, etc., the
resulting corpus consists of 101.2M documents with 571M images interleaved in
43B English tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07297">
<div class="article-summary-box-inner">
<span><p>One of the fundamental quests of AI is to produce agents that coordinate well
with humans. This problem is challenging, especially in domains that lack high
quality human behavioral data, because multi-agent reinforcement learning (RL)
often converges to different equilibria from the ones that humans prefer. We
propose a novel framework, instructRL, that enables humans to specify what kind
of strategies they expect from their AI partners through natural language
instructions. We use pretrained large language models to generate a prior
policy conditioned on the human instruction and use the prior to regularize the
RL objective. This leads to the RL agent converging to equilibria that are
aligned with human preferences. We show that instructRL converges to human-like
policies that satisfy the given instructions in a proof-of-concept environment
as well as the challenging Hanabi benchmark. Finally, we show that knowing the
language instruction significantly boosts human-AI coordination performance in
human evaluations in Hanabi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09349">
<div class="article-summary-box-inner">
<span><p>Embodied AI focuses on the study and development of intelligent systems that
possess a physical or virtual embodiment (i.e. robots) and are able to
dynamically interact with their environment. Memory and control are the two
essential parts of an embodied system and usually require separate frameworks
to model each of them. In this paper, we propose a novel and generalizable
framework called LLM-Brain: using Large-scale Language Model as a robotic brain
to unify egocentric memory and control. The LLM-Brain framework integrates
multiple multimodal language models for robotic tasks, utilizing a zero-shot
learning approach. All components within LLM-Brain communicate using natural
language in closed-loop multi-round dialogues that encompass perception,
planning, control, and memory. The core of the system is an embodied LLM to
maintain egocentric memory and control the robot. We demonstrate LLM-Brain by
examining two downstream tasks: active exploration and embodied question
answering. The active exploration tasks require the robot to extensively
explore an unknown environment within a limited number of actions. Meanwhile,
the embodied question answering tasks necessitate that the robot answers
questions based on observations acquired during prior explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11473">
<div class="article-summary-box-inner">
<span><p>As ecommerce continues growing, huge investments in ML and NLP for
Information Retrieval are following. While the vector space model dominated
retrieval modelling in product search - even as vectorization itself greatly
changed with the advent of deep learning -, our position paper argues in a
contrarian fashion that program synthesis provides significant advantages for
many queries and a significant number of players in the market. We detail the
industry significance of the proposed approach, sketch implementation details,
and address common objections drawing from our experience building a similar
system at Tooso.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WizardLM: Empowering Large Language Models to Follow Complex Instructions. (arXiv:2304.12244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12244">
<div class="article-summary-box-inner">
<span><p>Training large language models (LLMs) with open-domain instruction following
data brings colossal success. However, manually creating such instruction data
is very time-consuming and labor-intensive. Moreover, humans may struggle to
produce high-complexity instructions. In this paper, we show an avenue for
creating large amounts of instruction data with varying levels of complexity
using LLM instead of humans. Starting with an initial set of instructions, we
use our proposed Evol-Instruct to rewrite them step by step into more complex
instructions. Then, we mix all generated instruction data to fine-tune LLaMA.
We call the resulting model WizardLM. Human evaluations on a
complexity-balanced test bed and Vicuna's testset show that instructions from
Evol-Instruct are superior to human-created ones. By analyzing the human
evaluation results of the high complexity part, we demonstrate that outputs
from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4
automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on
17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some
aspects, our findings suggest that fine-tuning with AI-evolved instructions is
a promising direction for enhancing LLMs. Our code and data are public at
https://github.com/nlpxucan/WizardLM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13620">
<div class="article-summary-box-inner">
<span><p>Automatic chart to text summarization is an effective tool for the visually
impaired people along with providing precise insights of tabular data in
natural language to the user. A large and well-structured dataset is always a
key part for data driven models. In this paper, we propose ChartSumm: a
large-scale benchmark dataset consisting of a total of 84,363 charts along with
their metadata and descriptions covering a wide range of topics and chart types
to generate short and long summaries. Extensive experiments with strong
baseline models show that even though these models generate fluent and
informative summaries by achieving decent scores in various automatic
evaluation metrics, they often face issues like suffering from hallucination,
missing out important data points, in addition to incorrect explanation of
complex trends in the charts. We also investigated the potential of expanding
ChartSumm to other languages using automated translation tools. These make our
dataset a challenging benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01210">
<div class="article-summary-box-inner">
<span><p>Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis benchmarking framework to rigorously evaluate the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HUMANEVAL
benchmark by 81x to build HUMANEVAL+. Our extensive evaluation across 19
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by 13.6-15.3% on average. Our work not only indicates
that prior popular code synthesis evaluation results do not accurately reflect
the true performance of LLMs for code synthesis, but also opens up a new
direction to improve such programming benchmarks through automated testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01876">
<div class="article-summary-box-inner">
<span><p>Concepts benefit natural language understanding but are far from complete in
existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs)
have been widely used in text-based concept extraction (CE). However, PLMs tend
to mine the co-occurrence associations from massive corpus as pre-trained
knowledge rather than the real causal effect between tokens. As a result, the
pre-trained knowledge confounds PLMs to extract biased concepts based on
spurious co-occurrence correlations, inevitably resulting in low precision. In
this paper, through the lens of a Structural Causal Model (SCM), we propose
equipping the PLM-based extractor with a knowledge-guided prompt as an
intervention to alleviate concept bias. The prompt adopts the topic of the
given entity from the existing knowledge in KGs to mitigate the spurious
co-occurrence correlations between entities and biased concepts. Our extensive
experiments on representative multilingual KG datasets justify that our
proposed prompt can effectively alleviate concept bias and improve the
performance of PLM-based CE models.The code has been released on
https://github.com/siyuyuan/KPCE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02170">
<div class="article-summary-box-inner">
<span><p>We present a pipeline for a statistical textual exploration, offering a
stylometry-based explanation and statistical validation of a hypothesized
partition of a text. Given a parameterization of the text, our pipeline: (1)
detects literary features yielding the optimal overlap between the hypothesized
and unsupervised partitions, (2) performs a hypothesis-testing analysis to
quantify the statistical significance of the optimal overlap, while conserving
implicit correlations between units of text that are more likely to be grouped,
and (3) extracts and quantifies the importance of features most responsible for
the classification, estimates their statistical stability and cluster-wise
abundance.
</p>
<p>We apply our pipeline to the first two books in the Bible, where one
stylistic component stands out in the eyes of biblical scholars, namely, the
Priestly component. We identify and explore statistically significant stylistic
differences between the Priestly and non-Priestly components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v2 [econ.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02531">
<div class="article-summary-box-inner">
<span><p>Language has a strong influence on our perceptions of time and rewards. This
raises the question of whether large language models, when asked in different
languages, show different preferences for rewards over time and if their
choices are similar to those of humans. In this study, we analyze the responses
of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages,
exploring preferences between smaller, sooner rewards and larger, later
rewards. Our results show that GPT displays greater patience when prompted in
languages with weak future tense references (FTR), such as German and Mandarin,
compared to languages with strong FTR, like English and French. These findings
are consistent with existing literature and suggest a correlation between GPT's
choices and the preferences of speakers of these languages. However, further
analysis reveals that the preference for earlier or later rewards does not
systematically change with reward gaps, indicating a lexicographic preference
for earlier payments. While GPT may capture intriguing variations across
languages, our findings indicate that the choices made by these models do not
correspond to those of human decision-makers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04492">
<div class="article-summary-box-inner">
<span><p>Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10276">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph. (arXiv:2305.12900v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12900">
<div class="article-summary-box-inner">
<span><p>There have been many recent investigations into prompt-based training of
transformer language models for new text genres in low-resource settings. The
prompt-based training approach has been found to be effective in generalizing
pre-trained or fine-tuned models for transfer to resource-scarce settings. This
work, for the first time, reports results on adopting prompt-based training of
transformers for \textit{scholarly knowledge graph object prediction}. The work
is unique in the following two main aspects. 1) It deviates from the other
works proposing entity and relation extraction pipelines for predicting objects
of a scholarly knowledge graph. 2) While other works have tested the method on
text genera relatively close to the general knowledge domain, we test the
method for a significantly different domain, i.e. scholarly knowledge, in turn
testing the linguistic, probabilistic, and factual generalizability of these
large-scale transformer models. We find that (i) per expectations, transformer
models when tested out-of-the-box underperform on a new domain of data, (ii)
prompt-based training of the models achieve performance boosts of up to 40\% in
a relaxed evaluation setting, and (iii) testing the models on a starkly
different domain even with a clever training objective in a low resource
setting makes evident the domain knowledge capture gap offering an
empirically-verified incentive for investing more attention and resources to
the scholarly domain in the context of transformer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation. (arXiv:2305.13119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13119">
<div class="article-summary-box-inner">
<span><p>Word sense disambiguation (WSD), which aims to determine an appropriate sense
for a target word given its context, is crucial for natural language
understanding. Existing supervised methods treat WSD as a classification task
and have achieved remarkable performance. However, they ignore uncertainty
estimation (UE) in the real-world setting, where the data is always noisy and
out of distribution. This paper extensively studies UE on the benchmark
designed for WSD. Specifically, we first compare four uncertainty scores for a
state-of-the-art WSD model and verify that the conventional predictive
probabilities obtained at the end of the model are inadequate to quantify
uncertainty. Then, we examine the capability of capturing data and model
uncertainties by the model with the selected UE score on well-designed test
scenarios and discover that the model reflects data uncertainty satisfactorily
but underestimates model uncertainty. Furthermore, we explore numerous lexical
properties that intrinsically affect data uncertainty and provide a detailed
analysis of four critical aspects: the syntactic category, morphology, sense
granularity, and semantic relations. The code is available at
https://github.com/RyanLiut/WSD-UE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13592">
<div class="article-summary-box-inner">
<span><p>Semantic understanding of programs has attracted great attention in the
community. Inspired by recent successes of large language models (LLMs) in
natural language understanding, tremendous progress has been made by treating
programming language as another sort of natural language and training LLMs on
corpora of program code. However, programs are essentially different from texts
after all, in a sense that they are normally heavily structured and
syntax-strict. In particular, programs and their basic units (i.e., functions
and subroutines) are designed to demonstrate a variety of behaviors and/or
provide possible outputs, given different inputs. The relationship between
inputs and possible outputs/behaviors represents the functions/subroutines and
profiles the program as a whole. Therefore, we propose to incorporate such a
relationship into learning, for achieving a deeper semantic understanding of
programs. To obtain inputs that are representative enough to trigger the
execution of most part of the code, we resort to fuzz testing and propose fuzz
tuning to boost the performance of program understanding and code
representation learning, given a pre-trained LLM. The effectiveness of the
proposed method is verified on two program understanding tasks including code
clone detection and code classification, and it outperforms current
state-of-the-arts by large margins. Code is available at
https://github.com/rabbitjy/FuzzTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Grammatical Error Correction Systems with Explanations. (arXiv:2305.15676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15676">
<div class="article-summary-box-inner">
<span><p>Grammatical error correction systems improve written communication by
detecting and correcting language mistakes. To help language learners better
understand why the GEC system makes a certain correction, the causes of errors
(evidence words) and the corresponding error types are two key factors. To
enhance GEC systems with explanations, we introduce EXPECT, a large dataset
annotated with evidence words and grammatical error types. We propose several
baselines and analysis to understand this task. Furthermore, human evaluation
verifies our explainable GEC system's explanations can assist second-language
learners in determining whether to accept a correction suggestion and in
understanding the associated grammar rule.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16243">
<div class="article-summary-box-inner">
<span><p>Augmenting language models with a retrieval mechanism has been shown to
significantly improve their performance while keeping the number of parameters
low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism
based on the similarity between dense representations of the query chunk and
potential neighbors. In this paper, we study the state-of-the-art Retro model
and observe that its performance gain is better explained by surface-level
similarities, such as token overlap. Inspired by this, we replace the semantic
retrieval in Retro with a surface-level method based on BM25, obtaining a
significant reduction in perplexity. As full BM25 retrieval can be
computationally costly for large datasets, we also apply it in a re-ranking
scenario, gaining part of the perplexity reduction with minimal computational
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information. (arXiv:2305.16967v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16967">
<div class="article-summary-box-inner">
<span><p>The long-standing one-to-many issue of the open-domain dialogues poses
significant challenges for automatic evaluation methods, i.e., there may be
multiple suitable responses which differ in semantics for a given
conversational context. To tackle this challenge, we propose a novel
learning-based automatic evaluation metric (CMN), which can robustly evaluate
open-domain dialogues by augmenting Conditional Variational Autoencoders
(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual
Information (MI) to model the semantic similarity of text in the latent space.
Experimental results on two open-domain dialogue datasets demonstrate the
superiority of our method compared with a wide range of baselines, especially
in handling responses which are distant to the golden reference responses in
semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17506">
<div class="article-summary-box-inner">
<span><p>Reusing off-the-shelf code snippets from online repositories is a common
practice, which significantly enhances the productivity of software developers.
To find desired code snippets, developers resort to code search engines through
natural language queries. Neural code search models are hence behind many such
engines. These models are based on deep learning and gain substantial attention
due to their impressive performance. However, the security aspect of these
models is rarely studied. Particularly, an adversary can inject a backdoor in
neural code search models, which return buggy or even vulnerable code with
security/privacy issues. This may impact the downstream software (e.g., stock
trading systems and autonomous driving) and cause financial loss and/or
life-threatening incidents. In this paper, we demonstrate such attacks are
feasible and can be quite stealthy. By simply modifying one variable/function
name, the attacker can make buggy/vulnerable code rank in the top 11%. Our
attack BADCODE features a special trigger generation and injection procedure,
making the attack more effective and stealthy. The evaluation is conducted on
two neural code search models and the results show our attack outperforms
baselines by 60%. Our user study demonstrates that our attack is more stealthy
than the baseline by two times based on the F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17682">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models for multiple tasks tends to be
expensive in terms of storage. To mitigate this, parameter-efficient transfer
learning (PETL) methods have been proposed to address this issue, but they
still require a significant number of parameters and storage when being applied
to broader ranges of tasks. To achieve even greater storage reduction, we
propose PROPETL, a novel method that enables efficient sharing of a single PETL
module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)
across layers and tasks. We then learn binary masks to select different
sub-networks from the shared prototype network and apply them as PETL modules
into different layers. We find that the binary masks can determine crucial
information from the network, which is often ignored in previous studies. Our
work can also be seen as a type of pruning method, where we find that
overparameterization also exists in the seemingly small PETL modules. We
evaluate PROPETL on various downstream tasks and show that it can outperform
other PETL methods with approximately 10% of the parameter storage required by
the latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18624">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19148">
<div class="article-summary-box-inner">
<span><p>Various design settings for in-context learning (ICL), such as the choice and
order of the in-context examples, can bias a model toward a particular
prediction without being reflective of an understanding of the task. While many
studies discuss these design choices, there have been few systematic
investigations into categorizing them and mitigating their impact. In this
work, we define a typology for three types of label biases in ICL for text
classification: vanilla-label bias, context-label bias, and domain-label bias
(which we conceptualize and detect for the first time).
</p>
<p>Our analysis demonstrates that prior label bias calibration methods fall
short of addressing all three types of biases. Specifically, domain-label bias
restricts LLMs to random-level performance on many tasks regardless of the
choice of in-context examples. To mitigate the effect of these biases, we
propose a simple bias calibration method that estimates a language model's
label bias using random in-domain words from the task corpus. After controlling
for this estimated bias when making predictions, our novel domain-context
calibration significantly improves the ICL performance of GPT-J and GPT-3 on a
wide range of tasks. The gain is substantial on tasks with large domain-label
bias (up to 37% in Macro-F1). Furthermore, our results generalize to models
with different scales, pretraining methods, and manually-designed task
instructions, showing the prevalence of label biases in ICL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information. (arXiv:2305.19344v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19344">
<div class="article-summary-box-inner">
<span><p>The success of NLP systems often relies on the availability of large,
high-quality datasets. However, not all samples in these datasets are equally
valuable for learning, as some may be redundant or noisy. Several methods for
characterizing datasets based on model-driven meta-information (e.g., model's
confidence) have been developed, but the relationship and complementary effects
of these methods have received less attention. In this paper, we introduce
infoVerse, a universal framework for dataset characterization, which provides a
new feature space that effectively captures multidimensional characteristics of
datasets by incorporating various model-driven meta-information. infoVerse
reveals distinctive regions of the dataset that are not apparent in the
original semantic space, hence guiding users (or models) in identifying which
samples to focus on for exploration, assessment, or annotation. Additionally,
we propose a novel sampling method on infoVerse to select a set of data points
that maximizes informativeness. In three real-world applications (data pruning,
active learning, and data annotation), the samples chosen on infoVerse space
consistently outperform strong baselines in all applications. Our code and demo
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19512">
<div class="article-summary-box-inner">
<span><p>Diffusion probabilistic models have shown great success in generating
high-quality images controllably, and researchers have tried to utilize this
controllability into text generation domain. Previous works on diffusion-based
language models have shown that they can be trained without external knowledge
(such as pre-trained weights) and still achieve stable performance and
controllability. In this paper, we trained a diffusion-based model on StylePTB
dataset, the standard benchmark for fine-grained text style transfers. The
tasks in StylePTB requires much more refined control over the output text
compared to tasks evaluated in previous works, and our model was able to
achieve state-of-the-art performance on StylePTB on both individual and
compositional transfers. Moreover, our model, trained on limited data from
StylePTB without external knowledge, outperforms previous works that utilized
pretrained weights, embeddings, and external grammar parsers, and this may
indicate that diffusion-based language models have great potential under
low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19567">
<div class="article-summary-box-inner">
<span><p>Despite the huge successes made in neutral TTS, content-leakage remains a
challenge. In this paper, we propose a new input representation and simple
architecture to achieve improved prosody modeling. Inspired by the recent
success in the use of discrete code in TTS, we introduce discrete code to the
input of the reference encoder. Specifically, we leverage the vector quantizer
from the audio compression model to exploit the diverse acoustic information it
has already been trained on. In addition, we apply the modified MLP-Mixer to
the reference encoder, making the architecture lighter. As a result, we train
the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of
our method through both subjective and objective evaluations. We demonstrate
that the reference encoder learns better speaker-independent prosody when
discrete code is utilized as input in the experiments. In addition, we obtain
comparable results even when fewer parameters are inputted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19915">
<div class="article-summary-box-inner">
<span><p>The increasingly popular adoption of source code in many critical tasks
motivates the development of data augmentation (DA) techniques to enhance
training data and improve various capabilities (e.g., robustness and
generalizability) of these models. Although a series of DA methods have been
proposed and tailored for source code models, there lacks a comprehensive
survey and examination to understand their effectiveness and implications. This
paper fills this gap by conducting a comprehensive and integrative survey of
data augmentation for source code, wherein we systematically compile and
encapsulate existing literature to provide a comprehensive overview of the
field. We start by constructing a taxonomy of DA for source code models model
approaches, followed by a discussion on prominent, methodologically
illustrative approaches. Next, we highlight the general strategies and
techniques to optimize the DA quality. Subsequently, we underscore techniques
that find utility in widely-accepted source code scenarios and downstream
tasks. Finally, we outline the prevailing challenges and potential
opportunities for future research. In essence, this paper endeavors to
demystify the corpus of existing literature on DA for source code models, and
foster further exploration in this sphere. Complementing this, we present a
continually updated GitHub repository that hosts a list of update-to-date
papers on DA for source code models, accessible at
\url{https://github.com/terryyz/DataAug4Code}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focused Prefix Tuning for Controllable Text Generation. (arXiv:2306.00369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00369">
<div class="article-summary-box-inner">
<span><p>In a controllable text generation dataset, there exist unannotated attributes
that could provide irrelevant learning signals to models that use it for
training and thus degrade their performance. We propose focused prefix
tuning(FPT) to mitigate the problem and to enable the control to focus on the
desired attribute. Experimental results show that FPT can achieve better
control accuracy and text fluency than baseline models in single-attribute
control tasks. In multi-attribute control tasks, FPT achieves comparable
control accuracy with the state-of-the-art approach while keeping the
flexibility to control new attributes without retraining existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00477">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)
has emerged as a highly successful approach, with training only a small number
of parameters without sacrificing performance and becoming the de-facto
learning paradigm with the increasing size of PLMs. However, existing PEFT
methods are not memory-efficient, because they still require caching most of
the intermediate activations for the gradient calculation, akin to fine-tuning.
One effective way to reduce the activation memory is to apply a reversible
model, so the intermediate activations are not necessary to be cached and can
be recomputed. Nevertheless, modifying a PLM to its reversible variant with
PEFT is not straightforward, since the reversible model has a distinct
architecture from the currently released PLMs. In this paper, we first
investigate what is a key factor for the success of existing PEFT methods, and
realize that it's essential to preserve the PLM's starting point when
initializing a PEFT method. With this finding, we propose memory-efficient
fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's
starting point and making it reversible without additional pre-training. We
evaluate MEFT on the GLUE benchmark and five question-answering tasks with
various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the
activation memory up to 84% of full fine-tuning with a negligible amount of
trainable parameters. Moreover, MEFT achieves the same score on GLUE and a
comparable score on the question-answering tasks as full fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02080">
<div class="article-summary-box-inner">
<span><p>Various adaptation methods, such as LoRA, prompts, and adapters, have been
proposed to enhance the performance of pre-trained vision-language models in
specific domains. The robustness of these adaptation methods against
distribution shifts have not been studied. In this study, we assess the
robustness of 11 widely-used adaptation methods across 4 vision-language
datasets under multimodal corruptions. Concretely, we introduce 7 benchmark
datasets, including 96 visual and 87 textual corruptions, to investigate the
robustness of different adaptation methods, the impact of available adaptation
examples, and the influence of trainable parameter size during adaptation. Our
analysis reveals that: 1) Adaptation methods are more sensitive to text
corruptions than visual corruptions. 2) Full fine-tuning does not consistently
provide the highest robustness; instead, adapters can achieve better robustness
with comparable clean performance. 3) Contrary to expectations, our findings
indicate that increasing the number of adaptation data and parameters does not
guarantee enhanced robustness; instead it results in even lower robustness. We
hope this study could benefit future research in the development of robust
multimodal adaptation methods. The benchmark, code, and dataset used in this
study can be accessed at https://adarobustness.github.io .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolution of Efficient Symbolic Communication Codes. (arXiv:2306.02383v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02383">
<div class="article-summary-box-inner">
<span><p>The paper explores how the human natural language structure can be seen as a
product of evolution of inter-personal communication code, targeting
maximisation of such culture-agnostic and cross-lingual metrics such as
anti-entropy, compression factor and cross-split F1 score. The exploration is
done as part of a larger unsupervised language learning effort, the attempt is
made to perform meta-learning in a space of hyper-parameters maximising F1
score based on the "ground truth" language structure, by means of maximising
the metrics mentioned above. The paper presents preliminary results of
cross-lingual word-level segmentation tokenisation study for Russian, Chinese
and English as well as subword segmentation or morphological parsing study for
English. It is found that language structure form the word-level segmentation
or tokenisation can be found as driven by all of these metrics, anti-entropy
being more relevant to English and Russian while compression factor more
specific for Chinese. The study for subword segmentation or morphological
parsing on English lexicon has revealed straight connection between the
compression been found to be associated with compression factor, while,
surprising, the same connection with anti-entropy has turned to be the inverse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02561">
<div class="article-summary-box-inner">
<span><p>We present LLM-Blender, an ensembling framework designed to attain
consistently superior performance by leveraging the diverse strengths of
multiple open-source large language models (LLMs). Our framework consists of
two modules: PairRanker and GenFuser, addressing the observation that optimal
LLMs for different examples can significantly vary. PairRanker employs a
specialized pairwise comparison method to distinguish subtle differences
between candidate outputs. It jointly encodes the input text and a pair of
candidates, using cross-attention encoders to determine the superior one. Our
results demonstrate that PairRanker exhibits the highest correlation with
ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,
generating an improved output by capitalizing on their strengths and mitigating
their weaknesses. To facilitate large-scale evaluation, we introduce a
benchmark dataset, MixInstruct, which is a mixture of multiple instruction
datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly
outperform individual LLMs and baseline methods across various metrics,
establishing a substantial performance gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. (arXiv:2306.02858v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02858">
<div class="article-summary-box-inner">
<span><p>We present Video-LLaMA, a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous
vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and
LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble the pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities as the pre-trained audio encoder, and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual &amp; audio encoders with LLM's
embedding space, we train Video-LLaMA on massive video/image-caption pairs as
well as visual-instruction-tuning datasets of moderate amount but higher
quality. We found Video-LLaMA showcases the ability to perceive and comprehend
video content, generating meaningful responses that are grounded in the visual
and auditory information presented in the videos. This highlights the potential
of Video-LLaMA as a promising prototype for audio-visual AI assistants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models. (arXiv:2306.03503v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03503">
<div class="article-summary-box-inner">
<span><p>This paper explores how AI-owners can develop safeguards for AI-generated
content by drawing from established codes of conduct and ethical standards in
other content-creation industries. It delves into the current state of ethical
awareness on Large Language Models (LLMs). By dissecting the mechanism of
content generation by LLMs, four key areas (upstream/downstream and at user
prompt/answer), where safeguards could be effectively applied, are identified.
A comparative analysis of these four areas follows and includes an evaluation
of the existing ethical safeguards in terms of cost, effectiveness, and
alignment with established industry practices. The paper's key argument is that
existing IT-related ethical codes, while adequate for traditional IT
engineering, are inadequate for the challenges posed by LLM-based content
generation. Drawing from established practices within journalism, we propose
potential standards for businesses involved in distributing and selling
LLM-generated content. Finally, potential conflicts of interest between dataset
curation at upstream and ethical benchmarking downstream are highlighted to
underscore the need for a broader evaluation beyond mere output. This study
prompts a nuanced conversation around ethical implications in this rapidly
evolving field of content generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data. (arXiv:2306.03722v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03722">
<div class="article-summary-box-inner">
<span><p>Most research on hate speech detection has focused on English where a
sizeable amount of labeled training data is available. However, to expand hate
speech detection into more languages, approaches that require minimal training
data are needed. In this paper, we test whether natural language inference
(NLI) models which perform well in zero- and few-shot settings can benefit hate
speech detection performance in scenarios where only a limited amount of
labeled data is available in the target language. Our evaluation on five
languages demonstrates large performance improvements of NLI fine-tuning over
direct fine-tuning in the target language. However, the effectiveness of
previous work that proposed intermediate fine-tuning on English data is hard to
match. Only in settings where the English training data does not match the test
domain, can our customised NLI-formulation outperform intermediate fine-tuning
on English. Based on our extensive experiments, we propose a set of
recommendations for hate speech detection in languages where minimal labeled
training data is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECQED: Emotion-Cause Quadruple Extraction in Dialogs. (arXiv:2306.03969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03969">
<div class="article-summary-box-inner">
<span><p>The existing emotion-cause pair extraction (ECPE) task, unfortunately,
ignores extracting the emotion type and cause type, while these fine-grained
meta-information can be practically useful in real-world applications, i.e.,
chat robots and empathic dialog generation. Also the current ECPE is limited to
the scenario of single text piece, while neglecting the studies at dialog level
that should have more realistic values. In this paper, we extend the ECPE task
with a broader definition and scenario, presenting a new task, Emotion-Cause
Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause
utterance pairs and emotion and cause types. We present an ECQED model based on
a structural and semantic heterogeneous graph as well as a parallel grid
tagging scheme, which advances in effectively incorporating the dialog context
structure, meanwhile solving the challenging overlapped quadruple issue. Via
experiments we show that introducing the fine-grained emotion and cause
features evidently helps better dialog generation. Also our proposed ECQED
system shows exceptional superiority over baselines on both the emotion-cause
quadruple or pair extraction tasks, meanwhile being highly efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named Entity Recognition. (arXiv:2306.03974v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03974">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) exploits limited annotated instances
to identify named mentions. Effectively transferring the internal or external
resources thus becomes the key to few-shot NER. While the existing prompt
tuning methods have shown remarkable few-shot performances, they still fail to
make full use of knowledge. In this work, we investigate the integration of
rich knowledge to prompt tuning for stronger few-shot NER. We propose
incorporating the deep prompt tuning framework with threefold knowledge (namely
TKDP), including the internal 1) context knowledge and the external 2) label
knowledge &amp; 3) sememe knowledge. TKDP encodes the three feature sources and
incorporates them into the soft prompt embeddings, which are further injected
into an existing pre-trained language model to facilitate predictions. On five
benchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1
over the raw deep prompt method, and significantly outperforms 8
strong-performing baseline systems in 5-/10-/20-shot settings, showing great
potential in few-shot NER. Our TKDP can be broadly adapted to other few-shot
tasks without effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Conversation Discourse for Dialogue Disentanglement. (arXiv:2306.03975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03975">
<div class="article-summary-box-inner">
<span><p>Dialogue disentanglement aims to detach the chronologically ordered
utterances into several independent sessions. Conversation utterances are
essentially organized and described by the underlying discourse, and thus
dialogue disentanglement requires the full understanding and harnessing of the
intrinsic discourse attribute. In this paper, we propose enhancing dialogue
disentanglement by taking full advantage of the dialogue discourse
characteristics. First of all, in feature encoding stage, we construct the
heterogeneous graph representations to model the various dialogue-specific
discourse structural features, including the static speaker-role structures
(i.e., speaker-utterance and speaker-mentioning structure) and the dynamic
contextual structures (i.e., the utterance-distance and partial-replying
structure). We then develop a structure-aware framework to integrate the rich
structural features for better modeling the conversational semantic context.
Second, in model learning stage, we perform optimization with a hierarchical
ranking loss mechanism, which groups dialogue utterances into different
discourse levels and carries training covering pair-wise and session-wise
levels hierarchically. Third, in inference stage, we devise an easy-first
decoding algorithm, which performs utterance pairing under the easy-to-hard
manner with a global context, breaking the constraint of traditional sequential
decoding order. On two benchmark datasets, our overall system achieves new
state-of-the-art performances on all evaluations. In-depth analyses further
demonstrate the efficacy of each proposed idea and also reveal how our methods
help advance the task. Our work has great potential to facilitate broader
multi-party multi-thread dialogue applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04357">
<div class="article-summary-box-inner">
<span><p>Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Recent
studies have been improving the accuracy of dialogue response selection through
post-training, mostly relying on naive masked language modeling methods.
However, the recently developed generative methods have shown promising text
representation capabilities in IR community, which could potentially lead to
better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE
(Dialogue Contextual Masking Auto-encoder), a straightforward yet effective
post-training technique tailored for dialogue response selection. Dial-MAE uses
an asymmetric encoder-decoder architecture that learns to better compress the
semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE
involves a deep encoder creating a dialogue embedding with the masked dialogue
context, followed by a shallow decoder that uses this embedding along with the
highly masked response to restore the original response. Our experiments have
demonstrated that Dial-MAE is highly effective, achieving state-of-the-art
performance on two commonly evaluated benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04757">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned large language models have revolutionized natural language
processing and have shown great potential in applications such as
conversational agents. These models, such as GPT-4, can not only master
language but also solve complex tasks in areas like mathematics, coding,
medicine, and law. Despite their impressive capabilities, there is still a lack
of comprehensive understanding regarding their full potential, primarily due to
the black-box nature of many models and the absence of holistic evaluation
studies. To address these challenges, we present INSTRUCTEVAL, a more
comprehensive evaluation suite designed specifically for instruction-tuned
large language models. Unlike previous works, our evaluation involves a
rigorous assessment of models based on problem-solving, writing ability, and
alignment to human values. We take a holistic approach to analyze various
factors affecting model performance, including the pretraining foundation,
instruction-tuning data, and training methods. Our findings reveal that the
quality of instruction data is the most crucial factor in scaling model
performance. While open-source models demonstrate impressive writing abilities,
there is substantial room for improvement in problem-solving and alignment. We
are encouraged by the rapid development of models by the open-source community,
but we also highlight the need for rigorous evaluation to support claims made
about these models. Through INSTRUCTEVAL, we aim to foster a deeper
understanding of instruction-tuned models and advancements in their
capabilities. INSTRUCTEVAL is publicly available at
https://github.com/declare-lab/instruct-eval.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-13 23:09:58.530180588 UTC">2023-06-13 23:09:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>