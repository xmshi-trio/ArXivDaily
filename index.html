<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-15T01:30:00Z">03-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters. (arXiv:2303.07354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07354">
<div class="article-summary-box-inner">
<span><p>State-sponsored trolls are the main actors of influence campaigns on social
media and automatic troll detection is important to combat misinformation at
scale. Existing troll detection models are developed based on training data for
known campaigns (e.g.\ the influence campaign by Russia's Internet Research
Agency on the 2016 US Election), and they fall short when dealing with {\em
novel} campaigns with new targets. We propose MetaTroll, a text-based troll
detection model based on the meta-learning framework that enables high
portability and parameter-efficient adaptation to new campaigns using only a
handful of labelled samples for few-shot transfer. We introduce
\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''
campaign-specific knowledge so as to tackle catastrophic forgetting, where a
model ``forgets'' how to detect trolls from older campaigns due to continual
adaptation. Our experiments demonstrate that MetaTroll substantially
outperforms baselines and state-of-the-art few-shot text classification models.
Lastly, we explore simple approaches to extend MetaTroll to multilingual and
multimodal detection. Source code for MetaTroll is available at:
https://github.com/ltian678/metatroll-code.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMOM: Adaptive Masking over Masking for Conditional Masked Language Model. (arXiv:2303.07457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07457">
<div class="article-summary-box-inner">
<span><p>Transformer-based autoregressive (AR) methods have achieved appealing
performance for varied sequence-to-sequence generation tasks, e.g., neural
machine translation, summarization, and code generation, but suffer from low
inference efficiency. To speed up the inference stage, many non-autoregressive
(NAR) strategies have been proposed in the past few years. Among them, the
conditional masked language model (CMLM) is one of the most versatile
frameworks, as it can support many different sequence generation scenarios and
achieve very competitive performance on these tasks. In this paper, we further
introduce a simple yet effective adaptive masking over masking strategy to
enhance the refinement capability of the decoder and make the encoder
optimization easier. Experiments on \textbf{3} different tasks (neural machine
translation, summarization, and code generation) with \textbf{15} datasets in
total confirm that our proposed simple method achieves significant performance
improvement over the strong CMLM model. Surprisingly, our proposed model yields
state-of-the-art performance on neural machine translation (\textbf{34.62} BLEU
on WMT16 EN$\to$RO, \textbf{34.82} BLEU on WMT16 RO$\to$EN, and \textbf{34.84}
BLEU on IWSLT De$\to$En) and even better performance than the \textbf{AR}
Transformer on \textbf{7} benchmark datasets with at least \textbf{2.2$\times$}
speedup. Our code is available at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Architext: Language-Driven Generative Architecture Design. (arXiv:2303.07519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07519">
<div class="article-summary-box-inner">
<span><p>Architectural design is a highly complex practice that involves a wide
diversity of disciplines, technologies, proprietary design software, expertise,
and an almost infinite number of constraints, across a vast array of design
tasks. Enabling intuitive, accessible, and scalable design processes is an
important step towards performance-driven and sustainable design for all. To
that end, we introduce Architext, a novel semantic generation assistive tool.
Architext enables design generation with only natural language prompts, given
to large-scale Language Models, as input. We conduct a thorough quantitative
evaluation of Architext's downstream task performance, focusing on semantic
accuracy and diversity for a number of pre-trained language models ranging from
120 million to 6 billion parameters. Architext models are able to learn the
specific design task, generating valid residential layouts at a near 100\%
rate. Accuracy shows great improvement when scaling the models, with the
largest model (GPT-J) yielding impressive accuracy ranging between 25% to over
80% for different prompt categories. We open source the finetuned Architext
models and our synthetic dataset, hoping to inspire experimentation in this
exciting area of design research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07522">
<div class="article-summary-box-inner">
<span><p>While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models in NLP: A Survey. (arXiv:2303.07576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07576">
<div class="article-summary-box-inner">
<span><p>Diffusion models have become a powerful family of deep generative models,
with record-breaking performance in many applications. This paper first gives
an overview and derivation of the basic theory of diffusion models, then
reviews the research results of diffusion models in the field of natural
language processing, from text generation, text-driven image generation and
other four aspects, and analyzes and summarizes the relevant literature
materials sorted out, and finally records the experience and feelings of this
topic literature review research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input-length-shortening and text generation via attention values. (arXiv:2303.07585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07585">
<div class="article-summary-box-inner">
<span><p>Identifying words that impact a task's performance more than others is a
challenge in natural language processing. Transformers models have recently
addressed this issue by incorporating an attention mechanism that assigns
greater attention (i.e., relevance) scores to some words than others. Because
of the attention mechanism's high computational cost, transformer models
usually have an input-length limitation caused by hardware constraints. This
limitation applies to many transformers, including the well-known bidirectional
encoder representations of the transformer (BERT) model. In this paper, we
examined BERT's attention assignment mechanism, focusing on two questions: (1)
How can attention be employed to reduce input length? (2) How can attention be
used as a control mechanism for conditional text generation? We investigated
these questions in the context of a text classification task. We discovered
that BERT's early layers assign more critical attention scores for text
classification tasks compared to later layers. We demonstrated that the first
layer's attention sums could be used to filter tokens in a given sequence,
considerably decreasing the input length while maintaining good test accuracy.
We also applied filtering, which uses a compute-efficient semantic similarities
algorithm, and discovered that retaining approximately 6\% of the original
sequence is sufficient to obtain 86.5\% accuracy. Finally, we showed that we
could generate data in a stable manner and indistinguishable from the original
one by only using a small percentage (10\%) of the tokens with high attention
scores according to BERT's first layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences. (arXiv:2303.07610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07610">
<div class="article-summary-box-inner">
<span><p>As a natural language assistant, ChatGPT is capable of performing various
tasks, including but not limited to article generation, code completion, and
data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable
level of accuracy and reliability in terms of content evaluation, exhibiting
the capability of mimicking human preferences. To further explore ChatGPT's
potential in this regard, a study is conducted to assess its ability to rank
content. In order to do so, a test set consisting of prompts is created,
covering a wide range of use cases, and five models are utilized to generate
corresponding responses. ChatGPT is then instructed to rank the responses
generated by these models. The results on the test set show that ChatGPT's
ranking preferences are consistent with human to a certain extent. This
preliminary experimental finding implies that ChatGPT's zero-shot ranking
capability could be used to reduce annotation pressure in a number of ranking
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Life Cycle of Knowledge in Big Language Models: A Survey. (arXiv:2303.07616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07616">
<div class="article-summary-box-inner">
<span><p>Knowledge plays a critical role in artificial intelligence. Recently, the
extensive success of pre-trained language models (PLMs) has raised significant
attention about how knowledge can be acquired, maintained, updated and used by
language models. Despite the enormous amount of related studies, there still
lacks a unified view of how knowledge circulates within language models
throughout the learning, tuning, and application processes, which may prevent
us from further understanding the connections between current progress or
realizing existing limitations. In this survey, we revisit PLMs as
knowledge-based systems by dividing the life circle of knowledge in PLMs into
five critical periods, and investigating how knowledge circulates when it is
built, maintained and used. To this end, we systematically review existing
studies of each period of the knowledge life cycle, summarize the main
challenges and current limitations, and discuss future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I3D: Transformer architectures with input-dependent dynamic depth for speech recognition. (arXiv:2303.07624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07624">
<div class="article-summary-box-inner">
<span><p>Transformer-based end-to-end speech recognition has achieved great success.
However, the large footprint and computational overhead make it difficult to
deploy these models in some real-world applications. Model compression
techniques can reduce the model size and speed up inference, but the compressed
model has a fixed architecture which might be suboptimal. We propose a novel
Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong
performance-efficiency trade-offs. With a similar number of layers at inference
time, I3D-based models outperform the vanilla Transformer and the static pruned
model via iterative layer pruning. We also present interesting analysis on the
gate probabilities and the input-dependency, which helps us better understand
deep encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Alzheimer's Disease detection based on paralinguistic and pre-trained features. (arXiv:2303.07650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07650">
<div class="article-summary-box-inner">
<span><p>We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,
which aims to investigate which acoustic features can be generalized and
transferred across languages for Alzheimer's Disease (AD) prediction. The
challenge consists of two tasks: one is to classify the speech of AD patients
and healthy individuals, and the other is to infer Mini Mental State
Examination (MMSE) score based on speech only. The difficulty is mainly
embodied in the mismatch of the dataset, in which the training set is in
English while the test set is in Greek. We extract paralinguistic features
using openSmile toolkit and acoustic features using XLSR-53. In addition, we
extract linguistic features after transcribing the speech into text. These
features are used as indicators for AD detection in our method. Our method
achieves an accuracy of 69.6% on the classification task and a root mean
squared error (RMSE) of 4.788 on the regression task. The results show that our
proposed method is expected to achieve automatic multilingual Alzheimer's
Disease detection through spontaneous speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer. (arXiv:2303.07665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07665">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07678">
<div class="article-summary-box-inner">
<span><p>This paper introduces a simple yet effective query expansion approach,
denoted as query2doc, to improve both sparse and dense retrieval systems. The
proposed method first generates pseudo-documents by few-shot prompting large
language models (LLMs), and then expands the query with generated
pseudo-documents. LLMs are trained on web-scale text corpora and are adept at
knowledge memorization. The pseudo-documents from LLMs often contain highly
relevant information that can aid in query disambiguation and guide the
retrievers. Experimental results demonstrate that query2doc boosts the
performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and
TREC DL, without any model fine-tuning. Furthermore, our method also benefits
state-of-the-art dense retrievers in terms of both in-domain and out-of-domain
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis. (arXiv:2303.07682v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07682">
<div class="article-summary-box-inner">
<span><p>Recent expressive text to speech (TTS) models focus on synthesizing emotional
speech, but some fine-grained styles such as intonation are neglected. In this
paper, we propose QI-TTS which aims to better transfer and control intonation
to further deliver the speaker's questioning intention while transferring
emotion from reference speech. We propose a multi-style extractor to extract
style embedding from two different levels. While the sentence level represents
emotion, the final syllable level represents intonation. For fine-grained
intonation control, we use relative attributes to represent intonation
intensity at the syllable level.Experiments have validated the effectiveness of
QI-TTS for improving intonation expressiveness in emotional speech synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy. (arXiv:2303.07687v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07687">
<div class="article-summary-box-inner">
<span><p>Because of predicting all the target tokens in parallel, the
non-autoregressive models greatly improve the decoding efficiency of speech
recognition compared with traditional autoregressive models. In this work, we
present dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross
Entropy (AXE), finding the monotonic alignment that minimizes the cross-entropy
loss through dynamic programming, (2) Dynamic Rectification, creating new
training samples by replacing some masks with model predicted tokens. The AXE
ignores the absolute position alignment between prediction and ground truth
sentence and focuses on tokens matching in relative order. The dynamic
rectification method makes the model capable of simulating the non-mask but
possible wrong tokens, even if they have high confidence. Our experiments on
WSJ dataset demonstrated that not only AXE loss but also the rectification
method could improve the WER performance of Mask CTC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Attention Model for Aspect-Level Sentiment Classification. (arXiv:2303.07689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07689">
<div class="article-summary-box-inner">
<span><p>I propose a novel dual-attention model(DAM) for aspect-level sentiment
classification. Many methods have been proposed, such as support vector
machines for artificial design features, long short-term memory networks based
on attention mechanisms, and graph neural networks based on dependency parsing.
While these methods all have decent performance, I think they all miss one
important piece of syntactic information: dependency labels. Based on this
idea, this paper proposes a model using dependency labels for the attention
mechanism to do this task. We evaluate the proposed approach on three datasets:
laptop and restaurant are from SemEval 2014, and the last one is a twitter
dataset. Experimental results show that the dual attention model has good
performance on all three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis. (arXiv:2303.07711v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07711">
<div class="article-summary-box-inner">
<span><p>Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesized speech of a target speaker's timbre. In most
previous methods, the synthesized fine-grained prosody features often represent
the source speaker's average style, similar to the one-to-many problem(i.e.,
multiple prosody variations correspond to the same text). In response to this
problem, a strength-controlled semi-supervised style extractor is proposed to
disentangle the style from content and timbre, improving the representation and
interpretability of the global style embedding, which can alleviate the
one-to-many mapping and data imbalance problems in prosody prediction. A
hierarchical prosody predictor is proposed to improve prosody modeling. We find
that better style transfer can be achieved by using the source speaker's
prosody features that are easily predicted. Additionally, a
speaker-transfer-wise cycle consistency loss is proposed to assist the model in
learning unseen style-timbre combinations during the training phase.
Experimental results show that the method outperforms the baseline. We provide
a website with audio samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion. (arXiv:2303.07726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07726">
<div class="article-summary-box-inner">
<span><p>Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework
that first transforms input sequences into character embeddings, obtains
linguistic information using language models, and then predicts the phonemes
based on global context about the entire input sequence. However, linguistic
knowledge alone is often inadequate. Language models frequently encode overly
general structures of a sentence and fail to cover specific cases needed to use
phonetic knowledge. Also, a handcrafted post-processing system is needed to
address the problems relevant to the tone of the characters. However, the
system exhibits inconsistency in the segmentation of word boundaries which
consequently degrades the performance of the G2P system. To address these
issues, we propose the Reinforcer that provides strong inductive bias for
language models by emphasizing the phonological information between neighboring
characters to help disambiguate pronunciations. Experimental results show that
the Reinforcer boosts the cutting-edge architectures by a large margin. We also
combine the Reinforcer with a large-scale pre-trained model and demonstrate the
validity of using neighboring context in knowledge transfer scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening. (arXiv:2303.07740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07740">
<div class="article-summary-box-inner">
<span><p>Under the flourishing development in performance, current image-text
retrieval methods suffer from $N$-related time complexity, which hinders their
application in practice. Targeting at efficiency improvement, this paper
presents a simple and effective keyword-guided pre-screening framework for the
image-text retrieval. Specifically, we convert the image and text data into the
keywords and perform the keyword matching across modalities to exclude a large
number of irrelevant gallery samples prior to the retrieval network. For the
keyword prediction, we transfer it into a multi-label classification problem
and propose a multi-task learning scheme by appending the multi-label
classifiers to the image-text retrieval network to achieve a lightweight and
high-performance keyword prediction. For the keyword matching, we introduce the
inverted index in the search engine and create a win-win situation on both time
and space complexities for the pre-screening. Extensive experiments on two
widely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of
the proposed framework. The proposed framework equipped with only two embedding
layers achieves $O(1)$ querying time complexity, while improving the retrieval
efficiency and keeping its performance, when applied prior to the common
image-text retrieval methods. Our code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation. (arXiv:2303.07833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07833">
<div class="article-summary-box-inner">
<span><p>In multi-turn dialogue generation, responses are not only related to the
topic and background of the context but also related to words and phrases in
the sentences of the context. However, currently widely used hierarchical
dialog models solely rely on context representations from the utterance-level
encoder, ignoring the sentence representations output by the word-level
encoder. This inevitably results in a loss of information while decoding and
generating. In this paper, we propose a new dialog model X-ReCoSa to tackle
this problem which aggregates multi-scale context information for hierarchical
dialog models. Specifically, we divide the generation decoder into upper and
lower parts, namely the intention part and the generation part. Firstly, the
intention part takes context representations as input to generate the intention
of the response. Then the generation part generates words depending on sentence
representations. Therefore, the hierarchical information has been fused into
response generation. we conduct experiments on the English dataset DailyDialog.
Experimental results exhibit that our method outperforms baseline models on
both automatic metric-based and human-based evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07865">
<div class="article-summary-box-inner">
<span><p>This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Learnability of In-Context Learning. (arXiv:2303.07895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07895">
<div class="article-summary-box-inner">
<span><p>In-context learning is a surprising and important phenomenon that emerged
when modern language models were scaled to billions of learned parameters.
Without modifying a large language model's weights, it can be tuned to perform
various downstream natural language tasks simply by including concatenated
training examples of these tasks in its input. Though disruptive for many
practical applications of large language models, this emergent learning
paradigm is not well understood from a theoretical perspective. In this paper,
we propose a first-of-its-kind PAC based framework for in-context learnability,
and use it to provide the first finite sample complexity results for the
in-context learning setup. Our framework includes an initial pretraining phase,
which fits a function to the pretraining distribution, and then a second
in-context learning phase, which keeps this function constant and concatenates
training examples of the downstream task in its input. We use our framework in
order to prove that, under mild assumptions, when the pretraining distribution
is a mixture of latent tasks (a model often considered for natural language
pretraining), these tasks can be efficiently learned via in-context learning,
even though the model's weights are unchanged and the input significantly
diverges from the pretraining distribution. Our theoretical analysis reveals
that in this setting, in-context learning is more about identifying the task
than about learning it, a result which is in line with a series of recent
empirical findings. We hope that the in-context learnability framework
presented in this paper will facilitate future progress towards a deeper
understanding of this important new learning paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference. (arXiv:2303.07914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07914">
<div class="article-summary-box-inner">
<span><p>A popular approach to streaming speech translation is to employ a single
offline model with a \textit{wait-$k$} policy to support different latency
requirements, which is simpler than training multiple online models with
different latency constraints. However, there is a mismatch problem in using a
model trained with complete utterances for streaming inference with partial
input. We demonstrate that speech representations extracted at the end of a
streaming input are significantly different from those extracted from a
complete utterance. To address this issue, we propose a new approach called
Future-Aware Streaming Translation (FAST) that adapts an offline ST model for
streaming input. FAST includes a Future-Aware Inference (FAI) strategy that
incorporates future context through a trainable masked embedding, and a
Future-Aware Distillation (FAD) framework that transfers future context from an
approximation of full speech to streaming input. Our experiments on the MuST-C
EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs
between translation quality and latency than strong baselines. Extensive
analyses suggest that our methods effectively alleviate the aforementioned
mismatch problem between offline training and online inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07924">
<div class="article-summary-box-inner">
<span><p>Thanks to the rise of self-supervised learning, automatic speech recognition
(ASR) systems now achieve near-human performance on a wide variety of datasets.
However, they still lack generalization capability and are not robust to domain
shifts like accent variations. In this work, we use speech audio representing
four different French accents to create fine-tuning datasets that improve the
robustness of pre-trained ASR models. By incorporating various accents in the
training set, we obtain both in-domain and out-of-domain improvements. Our
numerical experiments show that we can reduce error rates by up to 25%
(relative) on African and Belgian accents compared to single-domain training
while keeping a good performance on standard French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theory of Emergent In-Context Learning as Implicit Structure Induction. (arXiv:2303.07971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07971">
<div class="article-summary-box-inner">
<span><p>Scaling large language models (LLMs) leads to an emergent capacity to learn
in-context from example demonstrations. Despite progress, theoretical
understanding of this phenomenon remains limited. We argue that in-context
learning relies on recombination of compositional operations found in natural
language data. We derive an information-theoretic bound showing how in-context
learning abilities arise from generic next-token prediction when the
pretraining distribution has sufficient amounts of compositional structure,
under linguistically motivated assumptions. A second bound provides a
theoretical justification for the empirical success of prompting LLMs to output
intermediate steps towards an answer. To validate theoretical predictions, we
introduce a controlled setup for inducing in-context learning; unlike previous
approaches, it accounts for the compositional nature of language. Trained
transformers can perform in-context learning for a range of tasks, in a manner
consistent with the theoretical results. Mirroring real-world LLMs in a
miniature setup, in-context learning emerges when scaling parameters and data,
and models perform better when prompted to output intermediate steps. Probing
shows that in-context learning is supported by a representation of the input's
compositional structure. Taken together, these results provide a step towards
theoretical understanding of emergent behavior in large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07991">
<div class="article-summary-box-inner">
<span><p>Long-sequence transformers are designed to improve the representation of
longer texts by language models and their performance on downstream
document-level tasks. However, not much is understood about the quality of
token-level predictions in long-form models. We investigate the performance of
such architectures in the context of document classification with unsupervised
rationale extraction. We find standard soft attention methods to perform
significantly worse when combined with the Longformer language model. We
propose a compositional soft attention architecture that applies RoBERTa
sentence-wise to extract plausible rationales at the token-level. We find this
method to significantly outperform Longformer-driven baselines on sentiment
classification datasets, while also exhibiting significantly lower runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07992">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a powerful large language model (LLM) that has made remarkable
progress in natural language understanding. Nevertheless, the performance and
limitations of the model still need to be extensively evaluated. As ChatGPT
covers resources such as Wikipedia and supports natural language question
answering, it has garnered attention as a potential replacement for traditional
knowledge based question answering (KBQA) models. Complex question answering is
a challenge task of KBQA, which comprehensively tests the ability of models in
semantic parsing and reasoning. To assess the performance of ChatGPT as a
question answering system (QAS) using its own knowledge, we present a framework
that evaluates its ability to answer complex questions. Our approach involves
categorizing the potential features of complex questions and describing each
test question with multiple labels to identify combinatorial reasoning.
Following the black-box testing specifications of CheckList proposed by Ribeiro
et.al, we develop an evaluation method to measure the functionality and
reliability of ChatGPT in reasoning for answering complex questions. We use the
proposed framework to evaluate the performance of ChatGPT in question answering
on 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual
datasets, with a total of approximately 190,000 test cases. We compare the
evaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common
long-term problems in LLMs. The dataset and code are available at
https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification. (arXiv:2303.08006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08006">
<div class="article-summary-box-inner">
<span><p>To make robots accessible to a broad audience, it is critical to endow them
with the ability to take universal modes of communication, like commands given
in natural language, and extract a concrete desired task specification, defined
using a formal language like linear temporal logic (LTL). In this paper, we
present a learning-based approach for translating from natural language
commands to LTL specifications with very limited human-labeled training data.
This is in stark contrast to existing natural-language to LTL translators,
which require large human-labeled datasets, often in the form of labeled pairs
of LTL formulas and natural language commands, to train the translator. To
reduce reliance on human data, our approach generates a large synthetic
training dataset through algorithmic generation of LTL formulas, conversion to
structured English, and then exploiting the paraphrasing capabilities of modern
large language models (LLMs) to synthesize a diverse corpus of natural language
commands corresponding to the LTL formulas. We use this generated data to
finetune an LLM and apply a constrained decoding procedure at inference time to
ensure the returned LTL formula is syntactically correct. We evaluate our
approach on three existing LTL/natural language datasets and show that we can
translate natural language commands at 75\% accuracy with far less human data
($\le$12 annotations). Moreover, when training on large human-annotated
datasets, our method achieves higher test accuracy (95\% on average) than prior
work. Finally, we show the translated formulas can be used to plan
long-horizon, multi-stage tasks on a 12D quadrotor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does ChatGPT resemble humans in language use?. (arXiv:2303.08014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08014">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have
shown remarkable capacities in comprehending and producing language. However,
their internal workings remain a black box in cognitive terms, and it is
unclear whether LLMs and chatbots can develop humanlike characteristics in
language use. Cognitive scientists have devised many experiments that probe,
and have made great progress in explaining, how people process language. We
subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000
runs per experiment. In 10 of them, ChatGPT replicated the human pattern of
language use. It associated unfamiliar words with different meanings depending
on their forms, continued to access recently encountered meanings of ambiguous
words, reused recent sentence structures, reinterpreted implausible sentences
that were likely to have been corrupted by noise, glossed over errors, drew
reasonable inferences, associated causality with different discourse entities
according to verb semantics, and accessed different meanings and retrieved
different words depending on the identity of its interlocutor. However, unlike
humans, it did not prefer using shorter words to convey less informative
content and it did not use context to disambiguate syntactic ambiguities. We
discuss how these convergences and divergences may occur in the transformer
architecture. Overall, these experiments demonstrate that LLM-driven chatbots
like ChatGPT are capable of mimicking human language processing to a great
extent, and that they have the potential to provide insights into how people
learn and use language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Abuse in Financial Transaction Descriptions Using Machine Learning. (arXiv:2303.08016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08016">
<div class="article-summary-box-inner">
<span><p>Since introducing changes to the New Payments Platform (NPP) to include
longer messages as payment descriptions, it has been identified that people are
now using it for communication, and in some cases, the system was being used as
a targeted form of domestic and family violence. This type of tech-assisted
abuse poses new challenges in terms of identification, actions and approaches
to rectify this behaviour. Commonwealth Bank of Australia's Artificial
Intelligence Labs team (CBA AI Labs) has developed a new system using advances
in deep learning models for natural language processing (NLP) to create a
powerful abuse detector that periodically scores all the transactions, and
identifies cases of high-risk abuse in millions of records. In this paper, we
describe the problem of tech-assisted abuse in the context of banking services,
outline the developed model and its performance, and the operating framework
more broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08021">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel mechanism to obtain the optimal parameters of a
deep learning model using the Bees Algorithm, which is a recent promising swarm
intelligence algorithm. The optimization problem is to maximize the accuracy of
classifying ailments based on medical text given the initial hyper-parameters
to be adjusted throughout a definite number of iterations. Experiments included
two different datasets: English and Arabic. The highest accuracy achieved is
99.63% on the English dataset using Long Short-Term Memory (LSTM) along with
the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08032">
<div class="article-summary-box-inner">
<span><p>Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we introduce BODEGA: a benchmark for testing both victim models
and attack methods on four misinformation detection tasks in an evaluation
framework designed to simulate real use-cases of content moderation. We also
systematically test the robustness of popular text classifiers against
available attacking techniques and discover that, indeed, in some cases barely
significant changes in input text can mislead the models. We openly share the
BODEGA code and data in hope of enhancing the comparability and replicability
of further research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code. (arXiv:2303.08033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08033">
<div class="article-summary-box-inner">
<span><p>We analyzed effectiveness of three generative pre-trained transformer (GPT)
models in answering multiple-choice question (MCQ) assessments, often involving
short snippets of code, from introductory and intermediate programming courses
at the postsecondary level. This emerging technology stirs countless
discussions of its potential uses (e.g., exercise generation, code explanation)
as well as misuses in programming education (e.g., cheating). However, the
capabilities of GPT models and their limitations to reason about and/or analyze
code in educational settings have been under-explored. We evaluated several
OpenAI's GPT models on formative and summative MCQ assessments from three
Python courses (530 questions). We found that MCQs containing code snippets are
not answered as successfully as those that only contain natural language. While
questions requiring to fill-in a blank in the code or completing a natural
language statement about the snippet are handled rather successfully, MCQs that
require analysis and/or reasoning about the code (e.g., what is true/false
about the snippet, or what is its output) appear to be the most challenging.
These findings can be leveraged by educators to adapt their instructional
practices and assessments in programming courses, so that GPT becomes a
valuable assistant for a learner as opposed to a source of confusion and/or
potential hindrance in the learning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progress Note Understanding -- Assessment and Plan Reasoning: Overview of the 2022 N2C2 Track 3 Shared Task. (arXiv:2303.08038v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08038">
<div class="article-summary-box-inner">
<span><p>Daily progress notes are common types in the electronic health record (EHR)
where healthcare providers document the patient's daily progress and treatment
plans. The EHR is designed to document all the care provided to patients, but
it also enables note bloat with extraneous information that distracts from the
diagnoses and treatment plans. Applications of natural language processing
(NLP) in the EHR is a growing field with the majority of methods in information
extraction. Few tasks use NLP methods for downstream diagnostic decision
support. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:
Progress Note Understanding - Assessment and Plan Reasoning as one step towards
a new suite of tasks. The Assessment and Plan Reasoning task focuses on the
most critical components of progress notes, Assessment and Plan subsections
where health problems and diagnoses are contained. The goal of the task was to
develop and evaluate NLP systems that automatically predict causal relations
between the overall status of the patient contained in the Assessment section
and its relation to each component of the Plan section which contains the
diagnoses and treatment plans. The goal of the task was to identify and
prioritize diagnoses as the first steps in diagnostic decision support to find
the most relevant information in long documents like daily progress notes. We
present the results of 2022 n2c2 Track 3 and provide a description of the data,
evaluation, participation and system performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test Questions. (arXiv:2303.08039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08039">
<div class="article-summary-box-inner">
<span><p>Recently, more and more people study online for the convenience of access to
massive learning materials (e.g. test questions/notes), thus accurately
understanding learning materials became a crucial issue, which is essential for
many educational applications. Previous studies focus on using language models
to represent the question data. However, test questions (TQ) are usually
heterogeneous and multi-modal, e.g., some of them may only contain text, while
others half contain images with information beyond their literal description.
In this context, both supervised and unsupervised methods are difficult to
learn a fused representation of questions. Meanwhile, this problem cannot be
solved by conventional methods such as image caption, as the images may contain
information complementary rather than duplicate to the text. In this paper, we
first improve previous text-only representation with a two-stage unsupervised
instance level contrastive based pre-training method (MCL: Mixture Unsupervised
Contrastive Learning). Then, TQ-Net was proposed to fuse the content of images
to the representation of heterogeneous data. Finally, supervised contrastive
learning was conducted on relevance prediction-related downstream tasks, which
helped the model to learn the representation of questions effectively. We
conducted extensive experiments on question-based tasks on large-scale,
real-world datasets, which demonstrated the effectiveness of TQ-Net and improve
the precision of downstream applications (e.g. similar questions +2.02% and
knowledge point prediction +7.20%). Our code will be available, and we will
open-source a subset of our data to promote the development of relative
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Happy-GLL: modular, reusable and complete top-down parsers for parameterized nonterminals. (arXiv:2303.08044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08044">
<div class="article-summary-box-inner">
<span><p>Parser generators and parser combinator libraries are the most popular tools
for producing parsers. Parser combinators use the host language to provide
reusable components in the form of higher-order functions with parsers as
parameters. Very few parser generators support this kind of reuse through
abstraction and even fewer generate parsers that are as modular and reusable as
the parts of the grammar for which they are produced. This paper presents a
strategy for generating modular, reusable and complete top-down parsers from
syntax descriptions with parameterized nonterminals, based on the FUN-GLL
variant of the GLL algorithm.
</p>
<p>The strategy is discussed and demonstrated as a novel back-end for the Happy
parser generator. Happy grammars can contain `parameterized nonterminals' in
which parameters abstract over grammar symbols, granting an abstraction
mechanism to define reusable grammar operators. However, the existing Happy
back-ends do not deliver on the full potential of parameterized nonterminals as
parameterized nonterminals cannot be reused across grammars. Moreover, the
parser generation process may fail to terminate or may result in exponentially
large parsers generated in an exponential amount of time.
</p>
<p>The GLL back-end presented in this paper implements parameterized
nonterminals successfully by generating higher-order functions that resemble
parser combinators, inheriting all the advantages of top-down parsing. The
back-end is capable of generating parsers for the full class of context-free
grammars, generates parsers in linear time and generates parsers that find all
derivations of the input string. To our knowledge, the presented GLL back-end
makes Happy the first parser generator that combines all these features.
</p>
<p>This paper describes the translation procedure of the GLL back-end and
compares it to the LALR and GLR back-ends of Happy in several experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verbal behavior without syntactic structures: beyond Skinner and Chomsky. (arXiv:2303.08080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08080">
<div class="article-summary-box-inner">
<span><p>What does it mean to know language? Since the Chomskian revolution, one
popular answer to this question has been: to possess a generative grammar that
exclusively licenses certain syntactic structures. Decades later, not even an
approximation to such a grammar, for any language, has been formulated; the
idea that grammar is universal and innately specified has proved barren; and
attempts to show how it could be learned from experience invariably come up
short. To move on from this impasse, we must rediscover the extent to which
language is like any other human behavior: dynamic, social, multimodal,
patterned, and purposive, its purpose being to promote desirable actions (or
thoughts) in others and self. Recent psychological, computational,
neurobiological, and evolutionary insights into the shaping and structure of
behavior may then point us toward a new, viable account of language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. (arXiv:2303.08114v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08114">
<div class="article-summary-box-inner">
<span><p>Training data attribution (TDA) methods offer to trace a model's prediction
on any given example back to specific influential training examples. Existing
approaches do so by assigning a scalar influence score to each training
example, under a simplifying assumption that influence is additive. But in
reality, we observe that training examples interact in highly non-additive ways
due to factors such as inter-example redundancy, training order, and curriculum
learning effects.
</p>
<p>To study such interactions, we propose Simfluence, a new paradigm for TDA
where the goal is not to produce a single influence score per example, but
instead a training run simulator: the user asks, ``If my model had trained on
example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on
$z_{test}$?''; the simulator should then output a simulated training run, which
is a time series predicting the loss on $z_{test}$ at every step of the
simulated run. This enables users to answer counterfactual questions about what
their model would have learned under different training curricula, and to
directly see where in training that learning would occur.
</p>
<p>We present a simulator, Simfluence-Linear, that captures non-additive
interactions and is often able to predict the spiky trajectory of individual
example losses with surprising fidelity. Furthermore, we show that existing TDA
methods such as TracIn and influence functions can be viewed as special cases
of Simfluence-Linear. This enables us to directly compare methods in terms of
their simulation accuracy, subsuming several prior TDA approaches to
evaluation. In experiments on large language model (LLM) fine-tuning, we show
that our method predicts loss trajectories with much higher accuracy than
existing TDA methods (doubling Spearman's correlation and reducing mean-squared
error by 75%) across several tasks, models, and training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08117">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been shown to encode linguistic structures,
e.g. dependency and constituency parse trees, in their embeddings while being
trained on unsupervised loss functions like masked language modeling. Some
doubts have been raised whether the models actually are doing parsing or only
some computation weakly correlated with it. We study questions: (a) Is it
possible to explicitly describe transformers with realistic embedding
dimension, number of heads, etc. that are capable of doing parsing -- or even
approximate parsing? (b) Why do pre-trained models capture parsing structure?
This paper takes a step toward answering these questions in the context of
generative modeling with PCFGs. We show that masked language models like BERT
or RoBERTa of moderate sizes can approximately execute the Inside-Outside
algorithm for the English PCFG [Marcus et al, 1993]. We also show that the
Inside-Outside algorithm is optimal for masked language modeling loss on the
PCFG-generated data. We also give a construction of transformers with $50$
layers, $15$ attention heads, and $1275$ dimensional embeddings in average such
that using its embeddings it is possible to do constituency parsing with
$&gt;70\%$ F1 score on PTB dataset. We conduct probing experiments on models
pre-trained on PCFG-generated data to show that this not only allows recovery
of approximate parse tree, but also recovers marginal span probabilities
computed by the Inside-Outside algorithm, which suggests an implicit bias of
masked language modeling towards this algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08127">
<div class="article-summary-box-inner">
<span><p>CB2 is a multi-agent platform to study collaborative natural language
interaction in a grounded task-oriented scenario. It includes a 3D game
environment, a backend server designed to serve trained models to human agents,
and various tools and processes to enable scalable studies. We deploy CB2 at
https://cb2.ai as a system demonstration with a learned instruction following
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph. (arXiv:2101.06397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06397">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an explanation of representation for self-attention
network (SAN) based neural sequence encoders, which regards the information
captured by the model and the encoding of the model as graph structure and the
generation of these graph structures respectively. The proposed explanation
applies to existing works on SAN-based models and can explain the relationship
among the ability to capture the structural or linguistic information, depth of
model, and length of sentence, and can also be extended to other models such as
recurrent neural network based models. We also propose a revisited multigraph
called Multi-order-Graph (MoG) based on our explanation to model the graph
structures in the SAN-based model as subgraphs in MoG and convert the encoding
of SAN-based model to the generation of MoG. Based on our explanation, we
further introduce a Graph-Transformer by enhancing the ability to capture
multiple subgraphs of different orders and focusing on subgraphs of high
orders. Experimental results on multiple neural machine translation tasks show
that the Graph-Transformer can yield effective performance improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12220">
<div class="article-summary-box-inner">
<span><p>When humans solve complex problems, they typically create a sequence of ideas
(involving an intuitive decision, reflection, error correction, etc.) in order
to reach a conclusive decision. Contrary to this, today's models are mostly
trained to map an input to one single and fixed output. In this paper, we
investigate how we can give models the opportunity of a second, third and
$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the
concept of a thought flow which creates a sequence of predictions. We present a
self-correction mechanism that is trained to estimate the model's correctness
and performs iterative prediction updates based on the correctness prediction's
gradient. We introduce our method at the example of question answering and
conduct extensive experiments that demonstrate (i) our method's ability to
correct its own predictions and (ii) its potential to notably improve model
performances. In addition, we conduct a qualitative analysis of thought flow
correction patterns and explore how thought flow predictions affect human users
within a crowdsourcing study. We find that (iii) thought flows enable improved
user performance and are perceived as more natural, correct, and intelligent as
single and/or top-3 predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03501">
<div class="article-summary-box-inner">
<span><p>Solving symbolic mathematics has always been of in the arena of human
ingenuity that needs compositional reasoning and recurrence. However, recent
studies have shown that large-scale language models such as transformers are
universal and surprisingly can be trained as a sequence-to-sequence task to
solve complex mathematical equations. These large transformer models need
humongous amounts of training data to generalize to unseen symbolic mathematics
problems. In this paper, we present a sample efficient way of solving the
symbolic tasks by first pretraining the transformer model with language
translation and then fine-tuning the pretrained transformer model to solve the
downstream task of symbolic mathematics. We achieve comparable accuracy on the
integration task with our pretrained model while using around $1.5$ orders of
magnitude less number of training samples with respect to the state-of-the-art
deep learning for symbolic mathematics. The test accuracy on differential
equation tasks is considerably lower comparing with integration as they need
higher order recursions that are not present in language translations. We
propose the generalizability of our pretrained language model from Anna
Karenina Principle (AKP). We pretrain our model with different pairs of
language translations. Our results show language bias in solving symbolic
mathematics tasks. Finally, we study the robustness of the fine-tuned model on
symbolic math tasks against distribution shift, and our approach generalizes
better in distribution shift scenarios for the function integration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DECAR: Deep Clustering for learning general-purpose Audio Representations. (arXiv:2110.08895v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08895">
<div class="article-summary-box-inner">
<span><p>We introduce DECAR, a self-supervised pre-training approach for learning
general-purpose audio representations. Our system is based on clustering: it
utilizes an offline clustering step to provide target labels that act as
pseudo-labels for solving a prediction task. We develop on top of recent
advances in self-supervised learning for computer vision and design a
lightweight, easy-to-use self-supervised pre-training scheme. We pre-train
DECAR embeddings on a balanced subset of the large-scale Audioset dataset and
transfer those representations to 9 downstream classification tasks, including
speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct
ablation studies identifying key design choices and also make all our code and
pre-trained models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis. (arXiv:2201.04831v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04831">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment
analysis. To better comprehend long complicated sentences and obtain accurate
aspect-specific information, linguistic and commonsense knowledge are generally
required in this task. However, most current methods employ complicated and
inefficient approaches to incorporate external knowledge, e.g., directly
searching the graph nodes. Additionally, the complementarity between external
knowledge and linguistic information has not been thoroughly studied. To this
end, we propose a knowledge graph augmented network KGAN, which aims to
effectively incorporate external knowledge with explicitly syntactic and
contextual information. In particular, KGAN captures the sentiment feature
representations from multiple different perspectives, i.e., context-, syntax-
and knowledge-based. First, KGAN learns the contextual and syntactic
representations in parallel to fully extract the semantic features. Then, KGAN
integrates the knowledge graphs into the embedding space, based on which the
aspect-specific knowledge representations are further obtained via an attention
mechanism. Last, we propose a hierarchical fusion module to complement these
multi-view representations in a local-to-global manner. Extensive experiments
on five popular ABSA benchmarks demonstrate the effectiveness and robustness of
our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN
achieves a new record of state-of-the-art performance among all datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10852">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving CTC-based ASR Models with Gated Interlayer Collaboration. (arXiv:2205.12462v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12462">
<div class="article-summary-box-inner">
<span><p>The CTC-based automatic speech recognition (ASR) models without the external
language model usually lack the capacity to model conditional dependencies and
textual interactions. In this paper, we present a Gated Interlayer
Collaboration (GIC) mechanism to improve the performance of CTC-based models,
which introduces textual information into the model and thus relaxes the
conditional independence assumption of CTC-based models. Specifically, we
consider the weighted sum of token embeddings as the textual representation for
each position, where the position-specific weights are the softmax probability
distribution constructed via inter-layer auxiliary CTC losses. The textual
representations are then fused with acoustic features by developing a gate
unit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the
proposed method outperforms several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human heuristics for AI-generated language are flawed. (arXiv:2206.07271v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07271">
<div class="article-summary-box-inner">
<span><p>Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems suggest words, complete
sentences, or produce entire conversations. AI-generated language is often not
identified as such but presented as language written by humans, raising
concerns about novel forms of deception and manipulation. Here, we study how
humans discern whether verbal self-presentations, one of the most personal and
consequential forms of language, were generated by AI. In six experiments,
participants (N = 4,600) were unable to detect self-presentations generated by
state-of-the-art AI language models in professional, hospitality, and dating
contexts. A computational analysis of language features shows that human
judgments of AI-generated language are hindered by intuitive but flawed
heuristics such as associating first-person pronouns, use of contractions, or
family topics with human-written language. We experimentally demonstrate that
these heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce text perceived as "more human than
human." We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ordinal analysis of lexical patterns. (arXiv:2208.11175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.11175">
<div class="article-summary-box-inner">
<span><p>Words are fundamental linguistic units that connect thoughts and things
through meaning. However, words do not appear independently in a text sequence.
The existence of syntactic rules induces correlations among neighboring words.
Using an ordinal pattern approach, we present an analysis of lexical
statistical connections for 11 major languages. We find that the diverse
manners that languages utilize to express word relations give rise to unique
pattern structural distributions. Furthermore, fluctuations of these pattern
distributions for a given language can allow us to determine both the
historical period when the text was written and its author. Taken together, our
results emphasize the relevance of ordinal time series analysis in linguistic
typology, historical linguistics and stylometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural inhibition during speech planning contributes to contrastive hyperarticulation. (arXiv:2209.12278v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12278">
<div class="article-summary-box-inner">
<span><p>Previous work has demonstrated that words are hyperarticulated on dimensions
of speech that differentiate them from a minimal pair competitor. This
phenomenon has been termed contrastive hyperarticulation (CH). We present a
dynamic neural field (DNF) model of voice onset time (VOT) planning that
derives CH from an inhibitory influence of the minimal pair competitor during
planning. We test some predictions of the model with a novel experiment
investigating CH of voiceless stop consonant VOT in pseudowords. The results
demonstrate a CH effect in pseudowords, consistent with a basis for the effect
in the real-time planning and production of speech. The scope and magnitude of
CH in pseudowords was reduced compared to CH in real words, consistent with a
role for interactive activation between lexical and phonological levels of
planning. We discuss the potential of our model to unify an apparently
disparate set of phenomena, from CH to phonological neighborhood effects to
phonetic trace effects in speech errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16264">
<div class="article-summary-box-inner">
<span><p>Transformers have been the dominant architecture for Speech Translation in
recent years, achieving significant improvements in translation quality. Since
speech signals are longer than their textual counterparts, and due to the
quadratic complexity of the Transformer, a down-sampling step is essential for
its adoption in Speech Translation. Instead, in this research, we propose to
ease the complexity by using a Perceiver encoder to map the speech inputs to a
fixed-length latent representation. Furthermore, we introduce a novel way of
training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent
spaces without any additional computational overhead. Speech-to-Text Perceivers
with DLA can match the performance of Transformer baselines across three
language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to
DLA at inference, and can be flexibly deployed with various computational
budgets, without significant drops in translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03053">
<div class="article-summary-box-inner">
<span><p>Causal language modeling (LM) uses word history to predict the next word.
BERT, on the other hand, makes use of bi-directional word information in a
sentence to predict words at masked positions. While BERT is effective in
sequence encoding, it is non-causal by nature and is not designed for sequence
generation. In this paper, we propose a novel language model, SUffix
REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual
effect in an autoregressive manner. SUREALM employs an embedding retriever to
search for training sentences in a data store that share similar word history
during sequence generation. In particular, the suffix portions of the retrieved
sentences mimick the "future" context. We evaluated our proposed model on the
DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on
the validation and test set compared to competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04215">
<div class="article-summary-box-inner">
<span><p>Open Relation Extraction (OpenRE) aims to discover novel relations from open
domains. Previous OpenRE methods mainly suffer from two problems: (1)
Insufficient capacity to discriminate between known and novel relations. When
extending conventional test settings to a more general setting where test data
might also come from seen classes, existing approaches have a significant
performance decline. (2) Secondary labeling must be performed before practical
application. Existing methods cannot label human-readable and meaningful types
for novel relations, which is urgently required by the downstream tasks. To
address these issues, we propose the Active Relation Discovery (ARD) framework,
which utilizes relational outlier detection for discriminating known and novel
relations and involves active learning for labeling novel relations. Extensive
experiments on three real-world datasets show that ARD significantly
outperforms previous state-of-the-art methods on both conventional and our
proposed general OpenRE settings. The source code and datasets will be
available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14769">
<div class="article-summary-box-inner">
<span><p>Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16198">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00503">
<div class="article-summary-box-inner">
<span><p>This paper illustrates the technologies of user next intent prediction with a
concept knowledge graph. The system has been deployed on the Web at Alipay,
serving more than 100 million daily active users. To explicitly characterize
user intent, we propose AlipayKG, which is an offline concept knowledge graph
in the Life-Service domain modeling the historical behaviors of users, the rich
content interacted by users and the relations between them. We further
introduce a Transformer-based model which integrates expert rules from the
knowledge graph to infer the online user's next intent. Experimental results
demonstrate that the proposed system can effectively enhance the performance of
the downstream tasks while retaining explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR. (arXiv:2301.00656v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00656">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) models confront challenges of abrupt
informational collapse or slow dimensional collapse. We propose TriNet, which
introduces a novel triple-branch architecture for preventing collapse and
stabilizing the pre-training. TriNet learns the SSL latent embedding space and
incorporates it to a higher level space for predicting pseudo target vectors
generated by a frozen teacher. Our experimental results show that the proposed
method notably stabilizes and accelerates pre-training and achieves a relative
word error rate reduction (WERR) of 6.06% compared to the state-of-the-art
(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code
at https://github.com/tencent-ailab/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLADIS: A General and Large Acronym Disambiguation Benchmark. (arXiv:2302.01860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01860">
<div class="article-summary-box-inner">
<span><p>Acronym Disambiguation (AD) is crucial for natural language understanding on
various sources, including biomedical reports, scientific papers, and search
engine queries. However, existing acronym disambiguation benchmarks and tools
are limited to specific domains, and the size of prior benchmarks is rather
small. To accelerate the research on acronym disambiguation, we construct a new
benchmark named GLADIS with three components: (1) a much larger acronym
dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus
with 160 million sentences; (3) three datasets that cover the general,
scientific, and biomedical domains. We then pre-train a language model,
\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,
and show the challenges and values of our new benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08399">
<div class="article-summary-box-inner">
<span><p>Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of-Mind tasks. These tasks have shown both
successes and failures. We consider in particular a recent purported success
case, and show that small variations that maintain the principles of ToM turn
the results on their head. We argue that in general, the zero-hypothesis for
model evaluation in intuitive psychology should be skeptical, and that outlying
failure cases should outweigh average success rates. We also consider what
possible future successes on Theory-of-Mind tasks by more powerful LLMs would
mean for ToM tasks with people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering. (arXiv:2302.12529v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12529">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) have received increasing attention due to its wide
applications on natural language processing. However, its use case on temporal
question answering (QA) has not been well-explored. Most of existing methods
are developed based on pre-trained language models, which might not be capable
to learn \emph{temporal-specific} presentations of entities in terms of
temporal KGQA task. To alleviate this problem, we propose a novel
\textbf{T}ime-aware \textbf{M}ultiway \textbf{A}daptive (\textbf{TMA}) fusion
network. Inspired by the step-by-step reasoning behavior of humans. For each
given question, TMA first extracts the relevant concepts from the KG, and then
feeds them into a multiway adaptive module to produce a
\emph{temporal-specific} representation of the question. This representation
can be incorporated with the pre-trained KG embedding to generate the final
prediction. Empirical results verify that the proposed model achieves better
performance than the state-of-the-art models in the benchmark dataset. Notably,
the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex
questions are absolutely improved by 24\% and 10\% compared to the
best-performing baseline. Furthermore, we also show that TMA employing an
adaptive fusion mechanism can provide interpretability by analyzing the
proportion of information in question representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts. (arXiv:2302.12530v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12530">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07142">
<div class="article-summary-box-inner">
<span><p>This case study investigates the task of job classification in a real-world
setting, where the goal is to determine whether an English-language job posting
is appropriate for a graduate or entry-level position. We explore multiple
approaches to text classification, including supervised approaches such as
traditional models like Support Vector Machines (SVMs) and state-of-the-art
deep learning methods such as DeBERTa. We compare them with Large Language
Models (LLMs) used in both few-shot and zero-shot classification settings. To
accomplish this task, we employ prompt engineering, a technique that involves
designing prompts to guide the LLMs towards the desired output. Specifically,
we evaluate the performance of two commercially available state-of-the-art
GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also
conduct a detailed analysis of the impact of different aspects of prompt
engineering on the model's performance. Our results show that, with a
well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all
other models, achieving a 6% increase in Precision@95% Recall compared to the
best supervised approach. Furthermore, we observe that the wording of the
prompt is a critical factor in eliciting the appropriate "reasoning" in the
model, and that seemingly minor aspects of the prompt significantly affect the
model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Models Trained on Indian Legal Data Fair?. (arXiv:2303.07247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07247">
<div class="article-summary-box-inner">
<span><p>Recent advances and applications of language technology and artificial
intelligence have enabled much success across multiple domains like law,
medical and mental health. AI-based Language Models, like Judgement Prediction,
have recently been proposed for the legal sector. However, these models are
strife with encoded social biases picked up from the training data. While bias
and fairness have been studied across NLP, most studies primarily locate
themselves within a Western context. In this work, we present an initial
investigation of fairness from the Indian perspective in the legal domain. We
highlight the propagation of learnt algorithmic biases in the bail prediction
task for models trained on Hindi legal documents. We evaluate the fairness gap
using demographic parity and show that a decision tree model trained for the
bail prediction task has an overall fairness disparity of 0.237 between input
features associated with Hindus and Muslims. Additionally, we highlight the
need for further research and studies in the avenues of fairness/bias in
applying AI in the legal sector with a specific focus on the Indian context.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-15 23:10:24.662202926 UTC">2023-03-15 23:10:24 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>