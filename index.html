<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-18T01:30:00Z">09-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. (arXiv:2309.07927v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07927">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Automatic Speech Recognition (ASR) systems,
exemplified by Whisper, have demonstrated the potential of these systems to
approach human-level performance given sufficient data. However, this progress
doesn't readily extend to ASR for children due to the limited availability of
suitable child-specific databases and the distinct characteristics of
children's speech. A recent study investigated leveraging the My Science Tutor
(MyST) children's speech corpus to enhance Whisper's performance in recognizing
children's speech. This paper builds on these findings by enhancing the utility
of the MyST dataset through more efficient data preprocessing. We also
highlight important challenges towards improving children's ASR performance.
The results showcase the viable and efficient integration of Whisper for
effective children's speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Contextual Information for Effective Entity Salience Detection. (arXiv:2309.07990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07990">
<div class="article-summary-box-inner">
<span><p>In text documents such as news articles, the content and key events usually
revolve around a subset of all the entities mentioned in a document. These
entities, often deemed as salient entities, provide useful cues of the
aboutness of a document to a reader. Identifying the salience of entities was
found helpful in several downstream applications such as search, ranking, and
entity-centric summarization, among others. Prior work on salient entity
detection mainly focused on machine learning models that require heavy feature
engineering. We show that fine-tuning medium-sized language models with a
cross-encoder style architecture yields substantial performance gains over
feature engineering approaches. To this end, we conduct a comprehensive
benchmarking of four publicly available datasets using models representative of
the medium-sized pre-trained language model family. Additionally, we show that
zero-shot prompting of instruction-tuned language models yields inferior
results, indicating the task's uniqueness and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation. (arXiv:2309.07998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07998">
<div class="article-summary-box-inner">
<span><p>Human evaluation has been widely accepted as the standard for evaluating
chat-oriented dialogue systems. However, there is a significant variation in
previous work regarding who gets recruited as evaluators. Evaluator groups such
as domain experts, university students, and professional annotators have been
used to assess and compare dialogue systems, although it is unclear to what
extent the choice of an evaluator group can affect results. This paper analyzes
the evaluator group impact on dialogue system evaluation by testing 4
state-of-the-art dialogue systems using 4 distinct evaluator groups. Our
analysis reveals a robustness towards evaluator groups for Likert evaluations
that is not seen for Pairwise, with only minor differences observed when
changing evaluator groups. Furthermore, two notable limitations to this
robustness are observed, which reveal discrepancies between evaluators with
different levels of chatbot expertise and indicate that evaluator objectivity
is beneficial for certain dialogue metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiariST: Streaming Speech Translation with Speaker Diarization. (arXiv:2309.08007v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08007">
<div class="article-summary-box-inner">
<span><p>End-to-end speech translation (ST) for conversation recordings involves
several under-explored challenges such as speaker diarization (SD) without
accurate word time stamps and handling of overlapping speech in a streaming
fashion. In this work, we propose DiariST, the first streaming ST and SD
solution. It is built upon a neural transducer-based streaming ST system and
integrates token-level serialized output training and t-vector, which were
originally developed for multi-talker speech recognition. Due to the absence of
evaluation benchmarks in this area, we develop a new evaluation dataset,
DiariST-AliMeeting, by translating the reference Chinese transcriptions of the
AliMeeting corpus into English. We also propose new metrics, called
speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality
while taking SD accuracy into account. Our system achieves a strong ST and SD
capability compared to offline systems based on Whisper, while performing
streaming inference for overlapping speech. To facilitate the research in this
new direction, we release the evaluation data, the offline baseline systems,
and the evaluation code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08008">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable capabilities in Natural
Language Processing (NLP), especially in domains where labeled data is scarce
or expensive, such as clinical domain. However, to unlock the clinical
knowledge hidden in these LLMs, we need to design effective prompts that can
guide them to perform specific clinical NLP tasks without any task-specific
training data. This is known as in-context learning, which is an art and
science that requires understanding the strengths and weaknesses of different
LLMs and prompt engineering approaches. In this paper, we present a
comprehensive and systematic experimental study on prompt engineering for five
clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence
Extraction, Coreference Resolution, Medication Status Extraction, and
Medication Attribute Extraction. We assessed the prompts proposed in recent
literature, including simple prefix, simple cloze, chain of thought, and
anticipatory prompts, and introduced two new types of prompts, namely heuristic
prompting and ensemble prompting. We evaluated the performance of these prompts
on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted
zero-shot prompting with few-shot prompting, and provide novel insights and
guidelines for prompt engineering for LLMs in clinical NLP. To the best of our
knowledge, this is one of the first works on the empirical evaluation of
different prompt engineering approaches for clinical NLP in this era of
generative AI, and we hope that it will inspire and inform future research in
this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08030">
<div class="article-summary-box-inner">
<span><p>Speech enhancement systems are typically trained using pairs of clean and
noisy speech. In audio-visual speech enhancement (AVSE), there is not as much
ground-truth clean data available; most audio-visual datasets are collected in
real-world environments with background noise and reverberation, hampering the
development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based
audio-visual speech enhancement approach that can generate clean speech despite
the challenges of real-world training data. We obtain a subset of nearly clean
speech from an audio-visual corpus using a neural quality estimator, and then
train a diffusion model on this subset to generate waveforms conditioned on
continuous speech representations from AV-HuBERT with noise-robust training. We
use continuous rather than discrete representations to retain prosody and
speaker information. With this vocoding task alone, the model can perform
speech enhancement better than a masking-based baseline. We further fine-tune
the diffusion model on clean/noisy utterance pairs to improve the performance.
Our approach outperforms a masking-based baseline in terms of both automatic
metrics and a human listening test and is close in quality to the target speech
in the listening test. Audio samples can be found at
https://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08047">
<div class="article-summary-box-inner">
<span><p>Summarization is an important application of large language models (LLMs).
Most previous evaluation of summarization models has focused on their
performance in content selection, grammaticality and coherence. However, it is
well known that LLMs reproduce and reinforce harmful social biases. This raises
the question: Do these biases affect model outputs in a relatively constrained
setting like summarization?
</p>
<p>To help answer this question, we first motivate and introduce a number of
definitions for biased behaviours in summarization models, along with practical
measures to quantify them. Since we find biases inherent to the input document
can confound our analysis, we additionally propose a method to generate input
documents with carefully controlled demographic attributes. This allows us to
sidestep this issue, while still working with somewhat realistic input
documents.
</p>
<p>Finally, we apply our measures to summaries generated by both purpose-built
summarization models and general purpose chat models. We find that content
selection in single document summarization seems to be largely unaffected by
bias, while hallucinations exhibit evidence of biases propagating to generated
summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting the Dots in News Analysis: A Cross-Disciplinary Survey of Media Bias and Framing. (arXiv:2309.08069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08069">
<div class="article-summary-box-inner">
<span><p>The manifestation and effect of bias in news reporting have been central
topics in the social sciences for decades, and have received increasing
attention in the NLP community recently. While NLP can help to scale up
analyses or contribute automatic procedures to investigate the impact of biased
news in society, we argue that methodologies that are currently dominant fall
short of addressing the complex questions and effects addressed in theoretical
media studies. In this survey paper, we review social science approaches and
draw a comparison with typical task formulations, methods, and evaluation
metrics used in the analysis of media bias in NLP. We discuss open questions
and suggest possible directions to close identified gaps between theory and
predictive models, and their evaluation. These include model transparency,
considering document-external information, and cross-document reasoning rather
than single-label assignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection. (arXiv:2309.08099v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08099">
<div class="article-summary-box-inner">
<span><p>Existing deepfake speech detection systems lack generalizability to unseen
attacks (i.e., samples generated by generative algorithms not seen during
training). Recent studies have explored the use of universal speech
representations to tackle this issue and have obtained inspiring results. These
works, however, have focused on innovating downstream classifiers while leaving
the representation itself untouched. In this study, we argue that
characterizing the long-term temporal dynamics of these representations is
crucial for generalizability and propose a new method to assess representation
dynamics. Indeed, we show that different generative models generate similar
representation dynamics patterns with our proposed method. Experiments on the
ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to
detect deepfakes from methods unseen during training, significantly improving
on several benchmark methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on Joint Representation Learning Methods for Entity Neighborhood Information and Description Information. (arXiv:2309.08100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08100">
<div class="article-summary-box-inner">
<span><p>To address the issue of poor embedding performance in the knowledge graph of
a programming design course, a joint represen-tation learning model that
combines entity neighborhood infor-mation and description information is
proposed. Firstly, a graph at-tention network is employed to obtain the
features of entity neigh-boring nodes, incorporating relationship features to
enrich the structural information. Next, the BERT-WWM model is utilized in
conjunction with attention mechanisms to obtain the representation of entity
description information. Finally, the final entity vector representation is
obtained by combining the vector representations of entity neighborhood
information and description information. Experimental results demonstrate that
the proposed model achieves favorable performance on the knowledge graph
dataset of the pro-gramming design course, outperforming other baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08140">
<div class="article-summary-box-inner">
<span><p>We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system
that allows control over speaker identity using natural language descriptions.
To control speaker identity within the prompt-based TTS framework, we introduce
the concept of speaker prompt, which describes voice characteristics (e.g.,
gender-neutral, young, old, and muffled) designed to be approximately
independent of speaking style. Since there is no large-scale dataset containing
speaker prompts, we first construct a dataset based on the LibriTTS-R corpus
with manually annotated speaker prompts. We then employ a diffusion-based
acoustic model with mixture density networks to model diverse speaker factors
in the training data. Unlike previous studies that rely on style prompts
describing only a limited aspect of speaker individuality, such as pitch,
speaking speed, and energy, our method utilizes an additional speaker prompt to
effectively learn the mapping from natural language descriptions to the
acoustic features of diverse speakers. Our subjective evaluation results show
that the proposed method can better control speaker characteristics than the
methods without the speaker prompt. Audio samples are available at
https://reppy4620.github.io/demo.promptttspp/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Difference Learning for Audio Captioning. (arXiv:2309.08141v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08141">
<div class="article-summary-box-inner">
<span><p>This study introduces a novel training paradigm, audio difference learning,
for improving audio captioning. The fundamental concept of the proposed
learning method is to create a feature representation space that preserves the
relationship between audio, enabling the generation of captions that detail
intricate audio information. This method employs a reference audio along with
the input audio, both of which are transformed into feature representations via
a shared encoder. Captions are then generated from these differential features
to describe their differences. Furthermore, a unique technique is proposed that
involves mixing the input audio with additional audio, and using the additional
audio as a reference. This results in the difference between the mixed audio
and the reference audio reverting back to the original input audio. This allows
the original input's caption to be used as the caption for their difference,
eliminating the need for additional annotations for the differences. In the
experiments using the Clotho and ESC50 datasets, the proposed method
demonstrated an improvement in the SPIDEr score by 7% compared to conventional
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unimodal Aggregation for CTC-based Speech Recognition. (arXiv:2309.08150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08150">
<div class="article-summary-box-inner">
<span><p>This paper works on non-autoregressive automatic speech recognition. A
unimodal aggregation (UMA) is proposed to segment and integrate the feature
frames that belong to the same text token, and thus to learn better feature
representations for text tokens. The frame-wise features and weights are both
derived from an encoder. Then, the feature frames with unimodal weights are
integrated and further processed by a decoder. Connectionist temporal
classification (CTC) loss is applied for training. Compared to the regular CTC,
the proposed method learns better feature representations and shortens the
sequence length, resulting in lower recognition error and computational
complexity. Experiments on three Mandarin datasets show that UMA demonstrates
superior or comparable performance to other advanced non-autoregressive
methods, such as self-conditioned CTC. Moreover, by integrating
self-conditioned CTC into the proposed framework, the performance can be
further noticeably improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08156">
<div class="article-summary-box-inner">
<span><p>Evaluating open-domain dialogue systems is challenging for reasons such as
the one-to-many problem, i.e., many appropriate responses other than just the
golden response. As of now, automatic evaluation methods need better
consistency with humans, while reliable human evaluation can be time- and
cost-intensive. To this end, we propose the Reference-Assisted Dialogue
Evaluation (RADE) approach under the multi-task learning framework, which
leverages the pre-created utterance as reference other than the gold response
to relief the one-to-many problem. Specifically, RADE explicitly compares
reference and the candidate response to predict their overall scores. Moreover,
an auxiliary response generation task enhances prediction via a shared encoder.
To support RADE, we extend three datasets with additional rated responses other
than just a golden response by human annotation. Experiments on our three
datasets and two existing benchmarks demonstrate the effectiveness of our
method, where Pearson, Spearman, and Kendall correlations with human evaluation
outperform state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08163">
<div class="article-summary-box-inner">
<span><p>As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of "personality"
of LLMs using personality self-assessment tests. In this paper, we take three
such studies on personality measurement of LLMs that use personality
self-assessment tests created to study human behavior. We use the prompts used
in these three different papers to measure the personality of the same LLM. We
find that all three prompts lead very different personality scores. This simple
test reveals that personality self-assessment scores in LLMs depend on the
subjective choice of the prompter. Since we don't know the ground truth value
of personality scores for LLMs as there is no correct answer to such questions,
there's no way of claiming if one prompt is more or less correct than the
other. We then introduce the property of option order symmetry for personality
measurement of LLMs. Since most of the self-assessment tests exist in the form
of multiple choice question (MCQ) questions, we argue that the scores should
also be robust to not just the prompt template but also the order in which the
options are presented. This test unsurprisingly reveals that the answers to the
self-assessment tests are not robust to the order of the options. These simple
tests, done on ChatGPT and Llama2 models show that self-assessment personality
tests created for humans are not appropriate for measuring personality in LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. (arXiv:2309.08168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08168">
<div class="article-summary-box-inner">
<span><p>We present a novel inference scheme, self-speculative decoding, for
accelerating Large Language Models (LLMs) without the need for an auxiliary
model. This approach is characterized by a two-stage process: drafting and
verification. The drafting stage generates draft tokens at a slightly lower
quality but more quickly, which is achieved by selectively skipping certain
intermediate layers during drafting Subsequently, the verification stage
employs the original LLM to validate those draft output tokens in one forward
pass. This process ensures the final output remains identical to that produced
by the unaltered LLM, thereby maintaining output quality. The proposed method
requires no additional neural network training and no extra memory footprint,
making it a plug-and-play and cost-effective solution for inference
acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a
speedup up to 1.73$\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LASER: LLM Agent with State-Space Exploration for Web Navigation. (arXiv:2309.08172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08172">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been successfully adapted for interactive
decision-making tasks like web navigation. While achieving decent performance,
previous methods implicitly assume a forward-only execution mode for the model,
where they only provide oracle trajectories as in-context examples to teach the
model how to reason in the interactive environment. Consequently, the model
could not handle more challenging scenarios not covered in the in-context
examples, e.g., mistakes, leading to sub-optimal performance. To address this
issue, we propose to model the interactive task as state space exploration,
where the LLM agent transitions among a pre-defined set of states by performing
actions to complete the task. This formulation enables flexible back-tracking,
allowing the model to easily recover from errors. We evaluate our proposed LLM
Agent with State-Space ExploRation (LASER) on the WebShop task. Experimental
results show that our LASER agent significantly outperforms previous methods
and closes the gap with human performance on the web navigation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedJudge: Federated Legal Large Language Model. (arXiv:2309.08173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08173">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have gained prominence in the field of Legal
Intelligence, offering potential applications in assisting legal professionals
and laymen. However, the centralized training of these Legal LLMs raises data
privacy concerns, as legal data is distributed among various institutions
containing sensitive individual information. This paper addresses this
challenge by exploring the integration of Legal LLMs with Federated Learning
(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on
devices or clients, and their parameters are aggregated and distributed on a
central server, ensuring data privacy without directly sharing raw data.
However, computation and communication overheads hinder the full fine-tuning of
LLMs under the FL setting. Moreover, the distribution shift of legal data
reduces the effectiveness of FL methods. To this end, in this paper, we propose
the first Federated Legal Large Language Model (FedJudge) framework, which
fine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge
utilizes parameter-efficient fine-tuning methods to update only a few
additional parameters during the FL training. Besides, we explore the continual
learning methods to preserve the global model's important parameters when
training local clients to mitigate the problem of data shifts. Extensive
experimental results on three real-world datasets clearly validate the
effectiveness of FedJudge. Code is released at
https://github.com/yuelinan/FedJudge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Failure Mode Classification: An Investigation. (arXiv:2309.08181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08181">
<div class="article-summary-box-inner">
<span><p>In this paper we present the first investigation into the effectiveness of
Large Language Models (LLMs) for Failure Mode Classification (FMC). FMC, the
task of automatically labelling an observation with a corresponding failure
mode code, is a critical task in the maintenance domain as it reduces the need
for reliability engineers to spend their time manually analysing work orders.
We detail our approach to prompt engineering to enable an LLM to predict the
failure mode of a given observation using a restricted code list. We
demonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned on
annotated data is a significant improvement over a currently available text
classification model (F1=0.60) trained on the same annotated data set. The
fine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). This
investigation reinforces the need for high quality fine-tuning data sets for
domain-specific tasks using LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08182">
<div class="article-summary-box-inner">
<span><p>Our work demonstrates that large language model (LLM) pre-trained on texts
can not only solve pure math word problems, but also physics word
problems-problems to be solved by calculation and inference based on some prior
physical knowledge. We collect and annotate the first physics word problem
dataset-PhysQA, which contains over 1000 junior high school physics word
problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we
use OpenAI' s GPT3.5 to generate the answer of these problems and found that
GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning
and 73.2% on few-shot learning. This result show that by using similar problem
and its answer as prompt, LLM could solve elementary physics word problems
approaching human level. Besides automatically solving problems, GPT3.5 could
also summarize the knowledge or topic examined by the problem, generate the
relevant explanation, and synthesis new physics word problems according tothe
input problems.Our work is the first research on automatically solving,
explaining and generating physics word problems of multiple types and scenes,
and we gain an acceptable and state-of-art accuracy, which demonstrates the
potential of LLM's further application in the field of secondary education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning. (arXiv:2309.08185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08185">
<div class="article-summary-box-inner">
<span><p>Multilingual semantic search is the task of retrieving relevant contents to a
query expressed in different language combinations. This requires a better
semantic understanding of the user's intent and its contextual meaning.
Multilingual semantic search is less explored and more challenging than its
monolingual or bilingual counterparts, due to the lack of multilingual parallel
resources for this task and the need to circumvent "language bias". In this
work, we propose an alignment approach: MAML-Align, specifically for
low-resource scenarios. Our approach leverages meta-distillation learning based
on MAML, an optimization-based Model-Agnostic Meta-Learner. MAML-Align distills
knowledge from a Teacher meta-transfer model T-MAML, specialized in
transferring from monolingual to bilingual semantic search, to a Student model
S-MAML, which meta-transfers from bilingual to multilingual semantic search. To
the best of our knowledge, we are the first to extend meta-distillation to a
multilingual search application. Our empirical results show that on top of a
strong baseline based on sentence transformers, our meta-distillation approach
boosts the gains provided by MAML and significantly outperforms naive
fine-tuning methods. Furthermore, multilingual meta-distillation learning
improves generalization even to unseen languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoded Summarization: Summarizing Documents into Continuous Vector Space for Legal Case Retrieval. (arXiv:2309.08187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08187">
<div class="article-summary-box-inner">
<span><p>We present our method for tackling a legal case retrieval task by introducing
our method of encoding documents by summarizing them into continuous vector
space via our phrase scoring framework utilizing deep neural networks. On the
other hand, we explore the benefits from combining lexical features and latent
features generated with neural networks. Our experiments show that lexical
features and latent features generated with neural networks complement each
other to improve the retrieval system performance. Furthermore, our
experimental results suggest the importance of case summarization in different
aspects: using provided summaries and performing encoded summarization. Our
approach achieved F1 of 65.6% and 57.6% on the experimental datasets of legal
case retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Answerability of LLMs for Long-Form Question Answering. (arXiv:2309.08210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08210">
<div class="article-summary-box-inner">
<span><p>As we embark on a new era of LLMs, it becomes increasingly crucial to
understand their capabilities, limitations, and differences. Toward making
further progress in this direction, we strive to build a deeper understanding
of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective
open-source LLMs and their distilled counterparts. To this end, we specifically
focus on long-form question answering (LFQA) because it has several practical
and impactful applications (e.g., troubleshooting, customer service, etc.) yet
is still understudied and challenging for LLMs. We propose a
question-generation method from abstractive summaries and show that generating
follow-up questions from summaries of long documents can create a challenging
setting for LLMs to reason and infer from long contexts. Our experimental
results confirm that: (1) our proposed method of generating questions from
abstractive summaries pose a challenging setup for LLMs and shows performance
gaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)
open-source LLMs exhibit decreased reliance on context for generated questions
from the original document, but their generation capabilities drop
significantly on generated questions from summaries -- especially for longer
contexts (&gt;1024 tokens)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for Robust Polyglot Text-To-Speech. (arXiv:2309.08255v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08255">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a framework for cross-lingual speech synthesis,
which involves an upstream Voice Conversion (VC) model and a downstream
Text-To-Speech (TTS) model. The proposed framework consists of 4 stages. In the
first two stages, we use a VC model to convert utterances in the target locale
to the voice of the target speaker. In the third stage, the converted data is
combined with the linguistic features and durations from recordings in the
target language, which are then used to train a single-speaker acoustic model.
Finally, the last stage entails the training of a locale-independent vocoder.
Our evaluations show that the proposed paradigm outperforms state-of-the-art
approaches which are based on training a large multilingual TTS model. In
addition, our experiments demonstrate the robustness of our approach with
different model architectures, languages, speakers and amounts of data.
Moreover, our solution is especially beneficial in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Self-Supervised Objectives for Transformers. (arXiv:2309.08272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08272">
<div class="article-summary-box-inner">
<span><p>This thesis focuses on improving the pre-training of natural language models
using unsupervised raw data to make them more efficient and aligned with
downstream applications.
</p>
<p>In the first part, we introduce three alternative pre-training objectives to
BERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS),
Cluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling
(SLM). These objectives involve token swapping instead of masking, with RTS and
C-RTS aiming to predict token originality and SLM predicting the original token
values. Results show that RTS and C-RTS require less pre-training time while
maintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on
certain tasks despite using the same computational budget.
</p>
<p>In the second part, we proposes self-supervised pre-training tasks that align
structurally with downstream applications, reducing the need for labeled data.
We use large corpora like Wikipedia and CC-News to train models to recognize if
text spans originate from the same paragraph or document in several ways. By
doing continuous pre-training, starting from existing models like RoBERTa,
ELECTRA, DeBERTa, BART, and T5, we demonstrate significant performance
improvements in tasks like Fact Verification, Answer Sentence Selection, and
Summarization. These improvements are especially pronounced when limited
annotation data is available. The proposed objectives also achieve
state-of-the-art results on various benchmark datasets, including FEVER (dev
set), ASNQ, WikiQA, and TREC-QA, as well as enhancing the quality of summaries.
Importantly, these techniques can be easily integrated with other methods
without altering the internal structure of Transformer models, making them
versatile for various NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Consistent Narrative Prompts on Abductive Natural Language Inference. (arXiv:2309.08303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08303">
<div class="article-summary-box-inner">
<span><p>Abduction has long been seen as crucial for narrative comprehension and
reasoning about everyday situations. The abductive natural language inference
($\alpha$NLI) task has been proposed, and this narrative text-based task aims
to infer the most plausible hypothesis from the candidates given two
observations. However, the inter-sentential coherence and the model consistency
have not been well exploited in the previous works on this task. In this work,
we propose a prompt tuning model $\alpha$-PACE, which takes self-consistency
and inter-sentential coherence into consideration. Besides, we propose a
general self-consistent framework that considers various narrative sequences
(e.g., linear narrative and reverse chronology) for guiding the pre-trained
language model in understanding the narrative context of input. We conduct
extensive experiments and thorough ablation studies to illustrate the necessity
and effectiveness of $\alpha$-PACE. The performance of our method shows
significant improvement against extensive competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios. (arXiv:2309.08316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08316">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) excel in in-distribution (ID) scenarios where train and
test data are independent and identically distributed. However, their
performance often degrades in real-world applications like argument mining.
Such degradation happens when new topics emerge, or other text domains and
languages become relevant. To assess LMs' generalization abilities in such
out-of-distribution (OOD) scenarios, we simulate such distribution shifts by
deliberately withholding specific instances for testing, as from the social
media domain or the topic Solar Energy.
</p>
<p>Unlike prior studies focusing on specific shifts and metrics in isolation, we
comprehensively analyze OOD generalization. We define three metrics to pinpoint
generalization flaws and propose eleven classification tasks covering topic,
domain, and language shifts. Overall, we find superior performance of
prompt-based fine-tuning, notably when train and test splits primarily differ
semantically. Simultaneously, in-context learning is more effective than
prompt-based or vanilla fine-tuning for tasks when training data embodies heavy
discrepancies in label distribution compared to testing data. This reveals a
crucial drawback of gradient-based learning: it biases LMs regarding such
structural obstacles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributional Inclusion Hypothesis and Quantifications: Probing Hypernymy in Functional Distributional Semantics. (arXiv:2309.08325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08325">
<div class="article-summary-box-inner">
<span><p>Functional Distributional Semantics (FDS) models the meaning of words by
truth-conditional functions. This provides a natural representation for
hypernymy, but no guarantee that it is learnt when FDS models are trained on a
corpus. We demonstrate that FDS models learn hypernymy when a corpus strictly
follows the Distributional Inclusion Hypothesis. We further introduce a
training objective that allows FDS to handle simple universal quantifications,
thus enabling hypernymy learning under the reverse of DIH. Experimental results
on both synthetic and real data sets confirm our hypotheses and the
effectiveness of our proposed objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08345">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have already demonstrated remarkable abilities in
understanding and generating both natural and formal language. Despite these
advances, their integration with real-world environments such as large-scale
knowledge bases (KBs) remains an underdeveloped area, affecting applications
such as semantic parsing and indulging in "hallucinated" information. This
paper is an experimental investigation aimed at uncovering the robustness
challenges that LMs encounter when tasked with knowledge base question
answering (KBQA). The investigation covers scenarios with inconsistent data
distribution between training and inference, such as generalization to unseen
domains, adaptation to various language variations, and transferability across
different datasets. Our comprehensive experiments reveal that even when
employed with our proposed data augmentation techniques, advanced small and
large language models exhibit poor performance in various dimensions. While the
LM is a promising technology, the robustness of the current form in dealing
with complex environments is fragile and of limited practicality because of the
data distribution issue. This calls for future research on data collection and
LM learning paradims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08347">
<div class="article-summary-box-inner">
<span><p>Semi-structured explanation depicts the implicit process of a reasoner with
an explicit representation. This explanation highlights how available
information in a specific query is supplemented with information a reasoner
produces from its internal weights towards generating an answer. Despite the
recent improvements in generative capabilities of language models, producing
structured explanations to verify model's true reasoning capabilities remains a
challenge. This issue is particularly pronounced for not-so-large LMs, as the
reasoner is expected to couple a sequential answer with a structured
explanation which embodies both the correct presentation and the correct
reasoning process. In this work, we first underscore the limitations of
supervised fine-tuning (SFT) in tackling this challenge, and then introduce a
carefully crafted reward engineering method in reinforcement learning (RL) to
better address this problem. We investigate multiple reward aggregation methods
and provide a detailed discussion which sheds light on the promising potential
of RL for future research. Our proposed reward on two semi-structured
explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Headless Language Models: Learning without Predicting with Contrastive Weight Tying. (arXiv:2309.08351v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08351">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training of language models usually consists in
predicting probability distributions over extensive token vocabularies. In this
study, we propose an innovative method that shifts away from probability
prediction and instead focuses on reconstructing input embeddings in a
contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach
to pretrain Headless Language Models in both monolingual and multilingual
contexts. Our method offers practical advantages, substantially reducing
training computational requirements by up to 20 times, while simultaneously
enhancing downstream performance and data efficiency. We observe a significant
+1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement
compared to classical LMs within similar compute budgets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaCorrect: Error Correction Back-end For Speaker Diarization. (arXiv:2309.08377v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08377">
<div class="article-summary-box-inner">
<span><p>In this work, we propose an error correction framework, named DiaCorrect, to
refine the output of a diarization system in a simple yet effective way. This
method is inspired by error correction techniques in automatic speech
recognition. Our model consists of two parallel convolutional encoders and a
transform-based decoder. By exploiting the interactions between the input
recording and the initial system's outputs, DiaCorrect can automatically
correct the initial speaker activities to minimize the diarization errors.
Experiments on 2-speaker telephony data show that the proposed DiaCorrect can
effectively improve the initial model's results. Our source code is publicly
available at https://github.com/BUTSpeechFIT/diacorrect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatFig: Generating Short and Long Captions for Patent Figures. (arXiv:2309.08379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08379">
<div class="article-summary-box-inner">
<span><p>This paper introduces Qatent PatFig, a novel large-scale patent figure
dataset comprising 30,000+ patent figures from over 11,000 European patent
applications. For each figure, this dataset provides short and long captions,
reference numerals, their corresponding terms, and the minimal claim set that
describes the interactions between the components of the image. To assess the
usability of the dataset, we finetune an LVLM model on Qatent PatFig to
generate short and long descriptions, and we investigate the effects of
incorporating various text-based cues at the prediction stage of the patent
figure captioning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation. (arXiv:2309.08380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08380">
<div class="article-summary-box-inner">
<span><p>Incorporating external knowledge into dialogue generation (KIDG) is crucial
for improving the correctness of response, where evidence fragments serve as
knowledgeable snippets supporting the factual dialogue replies. However,
introducing irrelevant content often adversely impacts reply quality and easily
leads to hallucinated responses. Prior work on evidence retrieval and
integration in dialogue systems falls short of fully leveraging existing
evidence since the model fails to locate useful fragments accurately and
overlooks hidden evidence labels within the KIDG dataset. To fully Unleash the
potential of evidence, we propose a framework to effectively incorporate
Evidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, we
introduce an automatic evidence generation framework that harnesses the power
of Large Language Models (LLMs) to mine reliable evidence veracity labels from
unlabeled data. By utilizing these evidence labels, we train a reliable
evidence indicator to effectively identify relevant evidence from retrieved
passages. Furthermore, we propose an evidence-augmented generator with an
evidence-focused attention mechanism, which allows the model to concentrate on
evidenced segments. Experimental results on MultiDoc2Dial demonstrate the
efficacy of evidential label augmentation and refined attention mechanisms in
improving model performance. Further analysis confirms that the proposed method
outperforms other baselines (+3~+5 points) regarding coherence and factual
consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08448">
<div class="article-summary-box-inner">
<span><p>The evaluation of large language models is an essential task in the field of
language understanding and generation. As language models continue to advance,
the need for effective benchmarks to assess their performance has become
imperative. In the context of Traditional Chinese, there is a scarcity of
comprehensive and diverse benchmarks to evaluate the capabilities of language
models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,
and FGC dataset. To address this gap, we propose a novel set of benchmarks that
leverage existing English datasets and are tailored to evaluate language models
in Traditional Chinese. These benchmarks encompass a wide range of tasks,
including contextual question-answering, summarization, classification, and
table understanding. The proposed benchmarks offer a comprehensive evaluation
framework, enabling the assessment of language models' capabilities across
different tasks. In this paper, we evaluate the performance of GPT-3.5,
Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.
The evaluation results highlight that our model, Model 7-C, achieves
performance comparable to GPT-3.5 with respect to a part of the evaluated
capabilities. In an effort to advance the evaluation of language models in
Traditional Chinese and stimulate further research in this field, we have
open-sourced our benchmark and opened the model for trial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08454">
<div class="article-summary-box-inner">
<span><p>Many real-life applications of automatic speech recognition (ASR) require
processing of overlapped speech. A commonmethod involves first separating the
speech into overlap-free streams and then performing ASR on the resulting
signals. Recently, the inclusion of a mixture encoder in the ASR model has been
proposed. This mixture encoder leverages the original overlapped speech to
mitigate the effect of artifacts introduced by the speech separation.
Previously, however, the method only addressed two-speaker scenarios. In this
work, we extend this approach to more natural meeting contexts featuring an
arbitrary number of speakers and dynamic overlaps. We evaluate the performance
using different speech separators, including the powerful TF-GridNet model. Our
experiments show state-of-the-art performance on the LibriCSS dataset and
highlight the advantages of the mixture encoder. Furthermore, they demonstrate
the strong separation of TF-GridNet which largely closes the gap between
previous methods and oracle separation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08469">
<div class="article-summary-box-inner">
<span><p>Modern open-domain question answering systems often rely on accurate and
efficient retrieval components to find passages containing the facts necessary
to answer the question. Recently, neural retrievers have gained popularity over
lexical alternatives due to their superior performance. However, most of the
work concerns popular languages such as English or Chinese. For others, such as
Polish, few models are available. In this work, we present SilverRetriever, a
neural retriever for Polish trained on a diverse collection of manually or
weakly labeled datasets. SilverRetriever achieves much better results than
other Polish models and is competitive with larger multilingual models.
Together with the model, we open-source five new passage retrieval datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata. (arXiv:2309.08491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08491">
<div class="article-summary-box-inner">
<span><p>In this work, we explore the use of Large Language Models (LLMs) for
knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.
For this task, given subject and relation pairs sourced from Wikidata, we
utilize pre-trained LLMs to produce the relevant objects in string format and
link them to their respective Wikidata QIDs. We developed a pipeline using LLMs
for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata
entity mapping. The method achieved a macro-averaged F1-score of 0.701 across
the properties, with the scores varying from 1.00 to 0.328. These results
demonstrate that the knowledge of LLMs varies significantly depending on the
domain and that further experimentation is required to determine the
circumstances under which LLMs can be used for automatic Knowledge Base (e.g.,
Wikidata) completion and correction. The investigation of the results also
suggests the promising contribution of LLMs in collaborative knowledge
engineering. LLMKE won Track 2 of the challenge. The implementation is
available at https://github.com/bohuizhang/LLMKE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking. (arXiv:2309.08503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08503">
<div class="article-summary-box-inner">
<span><p>Seeking health-related advice on the internet has become a common practice in
the digital era. Determining the trustworthiness of medical claims found online
and finding appropriate evidence for this information is increasingly
challenging. Fact-checking has emerged as an approach to assess the veracity of
factual claims using evidence from credible knowledge sources. To help advance
the automation of this task, in this paper, we introduce a novel dataset of 750
health-related claims, labeled for veracity by medical experts and backed with
evidence from appropriate clinical studies. We provide an analysis of the
dataset, highlighting its characteristics and challenges. The dataset can be
used for Machine Learning tasks related to automated fact-checking such as
evidence retrieval, veracity prediction, and explanation generation. For this
purpose, we provide baseline models based on different approaches, examine
their performance, and discuss the findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens. (arXiv:2309.08531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08531">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose methods to build a powerful and efficient
Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing
the rich knowledge related to image comprehension and language modeling from a
large-scale pre-trained vision-language model into Im2Sp. We set the output of
the proposed Im2Sp as discretized speech units, i.e., the quantized speech
features of a self-supervised speech model. The speech units mainly contain
linguistic information while suppressing other characteristics of speech. This
allows us to incorporate the language modeling capability of the pre-trained
vision-language model into the spoken language modeling of Im2Sp. With the
vision-language pre-training strategy, we set new state-of-the-art Im2Sp
performances on two widely used benchmark databases, COCO and Flickr8k. Then,
we further improve the efficiency of the Im2Sp model. Similar to the speech
unit case, we convert the original image into image units, which are derived
through vector quantization of the raw image. With these image units, we can
drastically reduce the required data storage for saving image data to just 0.8%
when compared to the original image data in terms of bits. Demo page:
https://ms-dot-k.github.io/Image-to-Speech-Captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08532">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) excel in various tasks, but they rely on
carefully crafted prompts that often demand substantial human effort. To
automate this process, in this paper, we propose a novel framework for discrete
prompt optimization, called EvoPrompt, which borrows the idea of evolutionary
algorithms (EAs) as they exhibit good performance and fast convergence. To
enable EAs to work on discrete prompts, which are natural language expressions
that need to be coherent and human-readable, we connect LLMs with EAs. This
approach allows us to simultaneously leverage the powerful language processing
capabilities of LLMs and the efficient optimization performance of EAs.
Specifically, abstaining from any gradients or parameters, EvoPrompt starts
from a population of prompts and iteratively generates new prompts with LLMs
based on the evolutionary operators, improving the population based on the
development set. We optimize prompts for both closed- and open-source LLMs
including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and
generation tasks. EvoPrompt significantly outperforms human-engineered prompts
and existing methods for automatic prompt generation by up to 25% and 14%
respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs
creates synergies, which could inspire further research on the combination of
LLMs and conventional algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08541">
<div class="article-summary-box-inner">
<span><p>Using large language models (LMs) for query or document expansion can improve
generalization in information retrieval. However, it is unknown whether these
techniques are universally beneficial or only effective in specific settings,
such as for particular retrieval models, dataset domains, or query types. To
answer this, we conduct the first comprehensive analysis of LM-based expansion.
We find that there exists a strong negative correlation between retriever
performance and gains from expansion: expansion improves scores for weaker
models, but generally harms stronger models. We show this trend holds across a
set of eleven expansion techniques, twelve datasets with diverse distribution
shifts, and twenty-four retrieval models. Through qualitative error analysis,
we hypothesize that although expansions provide extra information (potentially
improving recall), they add additional noise that makes it difficult to discern
between the top relevant documents (thus introducing false positives). Our
results suggest the following recipe: use expansions for weaker models or when
the target dataset significantly differs from training corpus in format;
otherwise, avoid expansions to keep the relevance signal clear.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting conformers with structured state space models for online speech recognition. (arXiv:2309.08551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08551">
<div class="article-summary-box-inner">
<span><p>Online speech recognition, where the model only accesses context to the left,
is an important and challenging use case for ASR systems. In this work, we
investigate augmenting neural encoders for online ASR by incorporating
structured state-space sequence models (S4), which are a family of models that
provide a parameter-efficient way of accessing arbitrarily long left context.
We perform systematic ablation studies to compare variants of S4 models and
propose two novel approaches that combine them with convolutions. We find that
the most effective design is to stack a small S4 using real-valued recurrent
weights with a local convolution, allowing them to work complementarily. Our
best model achieves WERs of 4.01%/8.53% on test sets from Librispeech,
outperforming Conformers with extensively tuned convolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08565">
<div class="article-summary-box-inner">
<span><p>Customizing machine translation models to comply with fine-grained attributes
such as formality has seen tremendous progress recently. However, current
approaches mostly rely on at least some supervised data with attribute
annotation. Data scarcity therefore remains a bottleneck to democratizing such
customization possibilities to a wider range of languages, lower-resource ones
in particular. Given recent progress in pretrained massively multilingual
translation models, we use them as a foundation to transfer the attribute
controlling capabilities to languages without supervised data. In this work, we
present a comprehensive analysis of transferring attribute controllers based on
a pretrained NLLB-200 model. We investigate both training- and inference-time
control techniques under various data scenarios, and uncover their relative
strengths and weaknesses in zero-shot performance and domain robustness. We
show that both paradigms are complementary, as shown by consistent improvements
on 5 zero-shot directions. Moreover, a human evaluation on a real low-resource
language, Bengali, confirms our findings on zero-shot transfer to new target
languages. The code is
$\href{https://github.com/dannigt/attribute-controller-transfer}{\text{here}}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West. (arXiv:2309.08573v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08573">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer. (arXiv:2309.08583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08583">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art language models excel at the style transfer task,
current work does not address explainability of style transfer systems.
Explanations could be generated using large language models such as GPT-3.5 and
GPT-4, but the use of such complex systems is inefficient when smaller, widely
distributed, and transparent alternatives are available. We propose a framework
to augment and improve a formality style transfer dataset with explanations via
model distillation from ChatGPT. To further refine the generated explanations,
we propose a novel way to incorporate scarce expert human feedback using
in-context learning (ICLEF: In-Context Learning from Expert Feedback) by
prompting ChatGPT to act as a critic to its own outputs. We use the resulting
dataset of 9,960 explainable formality style transfer instances (e-GYAFC) to
show that current openly distributed instruction-tuned models (and, in some
settings, ChatGPT) perform poorly on the task, and that fine-tuning on our
high-quality dataset leads to significant improvements as shown by automatic
evaluation. In human evaluation, we show that models much smaller than ChatGPT
fine-tuned on our data align better with expert preferences. Finally, we
discuss two potential applications of models fine-tuned on the explainable
style transfer task: interpretable authorship verification and interpretable
adversarial attacks on AI-generated text detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08589">
<div class="article-summary-box-inner">
<span><p>Large language models have astounded the world with fascinating new
capabilities. However, they currently lack the ability to teach themselves new
skills, relying instead on being trained on large amounts of human-generated
data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a
proof-of-concept demonstration that language models can successfully teach
themselves new skills using chain-of-thought reasoning. Inspired by previous
work in both reinforcement learning (Silver et al., 2017) and human cognition
(Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think
its way through problems. SECToR then fine-tunes the model to generate those
same answers, this time without using chain-of-thought reasoning. Language
models trained via SECToR autonomously learn to add up to 29-digit numbers
without any access to any ground truth examples beyond an initial supervised
fine-tuning phase consisting only of numbers with 6 or fewer digits. Our
central hypothesis is that chain-of-thought reasoning can act as a policy
improvement operator, analogously to how Monte-Carlo Tree Search is used in
AlphaZero. We hope that this research can lead to new directions in which
language models can learn to teach themselves without the need for human
demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation Models Can Learn to be Few-shot Learners. (arXiv:2309.08590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08590">
<div class="article-summary-box-inner">
<span><p>The emergent ability of Large Language Models to use a small number of
examples to learn to perform in novel domains and tasks, also called in-context
learning (ICL). In this work, we show that a much smaller model can be trained
to perform ICL by fine-tuning towards a specialized training objective,
exemplified on the task of domain adaptation for neural machine translation.
With this capacity for ICL, the model can take advantage of relevant few-shot
examples to adapt its output towards the domain. We compare the quality of this
domain adaptation to traditional supervised techniques and ICL with a
40B-parameter Large Language Model. Our approach allows efficient batch
inference on a mix of domains and outperforms state-of-the-art baselines in
terms of both translation quality and immediate adaptation rate, i.e. the
ability to reproduce a specific term after being shown a single example.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings. (arXiv:2309.08591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08591">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are highly adept at question answering and
reasoning tasks, but when reasoning in situational context, human expectations
vary depending on the relevant cultural common ground. As human languages are
associated with diverse cultures, LLMs should also be culturally-diverse
reasoners. In this paper, we study the ability of a wide range of
state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings
in a conversational context. Our experiments reveal that: (1) mLLMs 'knows'
limited proverbs and memorizing proverbs does not mean understanding them
within a conversational context; (2) mLLMs struggle to reason with figurative
proverbs and sayings, and when asked to select the wrong answer (instead of
asking it to select the correct answer); and (3) there is a "culture gap" in
mLLMs when reasoning about proverbs and sayings translated from other
languages. We construct and release our evaluation dataset MAPS (MulticultrAl
Proverbs and Sayings) for proverb understanding with conversational context for
six different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs. (arXiv:2309.08594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08594">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) acquire extensive knowledge during pre-training,
known as their parametric knowledge. However, in order to remain up-to-date and
align with human instructions, LLMs inevitably require external knowledge
during their interactions with users. This raises a crucial question: How will
LLMs respond when external knowledge interferes with their parametric
knowledge? To investigate this question, we propose a framework that
systematically elicits LLM parametric knowledge and introduces external
knowledge. Specifically, we uncover the impacts by constructing a parametric
knowledge graph to reveal the different knowledge structures of LLMs, and
introduce external knowledge through distractors of varying degrees, methods,
positions, and formats. Our experiments on both black-box and open-source
models demonstrate that LLMs tend to produce responses that deviate from their
parametric knowledge, particularly when they encounter direct conflicts or
confounding changes of information within detailed contexts. We also find that
while LLMs are sensitive to the veracity of external knowledge, they can still
be distracted by unrelated information. These findings highlight the risk of
hallucination when integrating external knowledge, even indirectly, during
interactions with current LLMs. All the data and results are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08600">
<div class="article-summary-box-inner">
<span><p>One of the roadblocks to a better understanding of neural networks' internals
is \textit{polysemanticity}, where neurons appear to activate in multiple,
semantically distinct contexts. Polysemanticity prevents us from identifying
concise, human-understandable explanations for what neural networks are doing
internally. One hypothesised cause of polysemanticity is
\textit{superposition}, where neural networks represent more features than they
have neurons by assigning features to an overcomplete set of directions in
activation space, rather than to individual neurons. Here, we attempt to
identify those directions, using sparse autoencoders to reconstruct the
internal activations of a language model. These autoencoders learn sets of
sparsely activating features that are more interpretable and monosemantic than
directions identified by alternative approaches, where interpretability is
measured by automated methods. Ablating these features enables precise model
editing, for example, by removing capabilities such as pronoun prediction,
while disrupting model behaviour less than prior techniques. This work
indicates that it is possible to resolve superposition in language models using
a scalable, unsupervised method. Our method may serve as a foundation for
future mechanistic interpretability work, which we hope will enable greater
model transparency and steerability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00640">
<div class="article-summary-box-inner">
<span><p>Automated fact-checking systems verify claims against evidence to predict
their veracity. In real-world scenarios, the retrieved evidence may not
unambiguously support or refute the claim and yield conflicting but valid
interpretations. Existing fact-checking datasets assume that the models
developed with them predict a single veracity label for each claim, thus
discouraging the handling of such ambiguity. To address this issue we present
AmbiFC, a fact-checking dataset with 10k claims derived from real-world
information needs. It contains fine-grained evidence annotations of 50k
passages from 5k Wikipedia pages. We analyze the disagreements arising from
ambiguity when comparing claims against evidence in AmbiFC, observing a strong
correlation of annotator disagreement with linguistic phenomena such as
underspecification and probabilistic reasoning. We develop models for
predicting veracity handling this ambiguity via soft labels and find that a
pipeline that learns the label distribution for sentence-level evidence
selection and veracity prediction yields the best performance. We compare
models trained on different subsets of AmbiFC and show that models trained on
the ambiguous instances perform better when faced with the identified
linguistic phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11155">
<div class="article-summary-box-inner">
<span><p>Quantum density matrix represents all the information of the entire quantum
system, and novel models of meaning employing density matrices naturally model
linguistic phenomena such as hyponymy and linguistic ambiguity, among others in
quantum question answering tasks. Naturally, we argue that applying the quantum
density matrix into classical Question Answering (QA) tasks can show more
effective performance. Specifically, we (i) design a new mechanism based on
Long Short-Term Memory (LSTM) to accommodate the case when the inputs are
matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural
Network (CNN) and gain the LSTM-based QA model with the quantum density matrix.
Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging
results. Similarly, we argue that the quantum density matrix can also enhance
the image feature information and the relationship between the features for the
classical image classification. Thus, we (i) combine density matrices and CNN
to design a new mechanism; (ii) apply the new mechanism to some representative
classical image classification tasks. A series of experiments show that the
application of quantum density matrix in image classification has the
generalization and high efficiency on different datasets. The application of
quantum density matrix both in classical question answering tasks and classical
image classification tasks show more effective performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering. (arXiv:2205.11501v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11501">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) requires systems to perform concept-level
reasoning by unifying unstructured (e.g., the context in question and answer;
"QA context") and structured (e.g., knowledge graph for the QA context and
scene; "concept graph") multimodal knowledge. Existing works typically combine
a scene graph and a concept graph of the scene by connecting corresponding
visual nodes and concept nodes, then incorporate the QA context representation
to perform question answering. However, these methods only perform a
unidirectional fusion from unstructured knowledge to structured knowledge,
limiting their potential to capture joint reasoning over the heterogeneous
modalities of knowledge. To perform more expressive reasoning, we propose
VQA-GNN, a new VQA model that performs bidirectional fusion between
unstructured and structured multimodal knowledge to obtain unified knowledge
representations. Specifically, we inter-connect the scene graph and the concept
graph through a super node that represents the QA context, and introduce a new
multimodal GNN technique to perform inter-modal message passing for reasoning
that mitigates representational gaps between modalities. On two challenging VQA
tasks (VCR and GQA), our method outperforms strong baseline VQA methods by 3.2%
on VCR (Q-AR) and 4.6% on GQA, suggesting its strength in performing
concept-level reasoning. Ablation studies further demonstrate the efficacy of
the bidirectional fusion and multimodal GNN method in unifying unstructured and
structured multimodal knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diachronic Data Analysis Supports and Refines Conceptual Metaphor Theory. (arXiv:2209.12234v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12234">
<div class="article-summary-box-inner">
<span><p>As a contribution to metaphor analysis, we introduce a statistical,
data-based investigation with empirical analysis of long-standing conjectures
and a first-ever empirical exploration of the systematic features of metaphors.
Conversely, this also makes metaphor theory available as a basis of meaning
emergence that can be quantitatively explored and integrated into the framework
of NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06692">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06623">
<div class="article-summary-box-inner">
<span><p>Answering questions related to the legal domain is a complex task, primarily
due to the intricate nature and diverse range of legal document systems.
Providing an accurate answer to a legal query typically necessitates
specialized knowledge in the relevant domain, which makes this task all the
more challenging, even for human experts. Question answering (QA) systems are
designed to generate answers to questions asked in human languages. QA uses
natural language processing to understand questions and search through
information to find relevant answers. QA has various practical applications,
including customer service, education, research, and cross-lingual
communication. However, QA faces challenges such as improving natural language
understanding and handling complex and ambiguous questions. Answering questions
related to the legal domain is a complex task, primarily due to the intricate
nature and diverse range of legal document systems. Providing an accurate
answer to a legal query typically necessitates specialized knowledge in the
relevant domain, which makes this task all the more challenging, even for human
experts. At this time, there is a lack of surveys that discuss legal question
answering. To address this problem, we provide a comprehensive survey that
reviews 14 benchmark datasets for question-answering in the legal field as well
as presents a comprehensive review of the state-of-the-art Legal Question
Answering deep learning models. We cover the different architectures and
techniques used in these studies and the performance and limitations of these
models. Moreover, we have established a public GitHub repository where we
regularly upload the most recent articles, open data, and source code. The
repository is available at:
\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the potential of AI-assisted pragmatic annotation: The case of apologies. (arXiv:2305.08339v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08339">
<div class="article-summary-box-inner">
<span><p>Certain forms of linguistic annotation, like part of speech and semantic
tagging, can be automated with high accuracy. However, manual annotation is
still necessary for complex pragmatic and discursive features that lack a
direct mapping to lexical forms. This manual process is time-consuming and
error-prone, limiting the scalability of function-to-form approaches in corpus
linguistics. To address this, our study explores automating pragma-discursive
corpus annotation using large language models (LLMs). We compare ChatGPT, the
Bing chatbot, and a human coder in annotating apology components in English
based on the local grammar framework. We find that the Bing chatbot
outperformed ChatGPT, with accuracy approaching that of a human coder. These
results suggest that AI can be successfully deployed to aid pragma-discursive
corpus annotation, making the process more efficient and scalable. Keywords:
linguistic annotation, function-to-form approaches, large language models,
local grammar analysis, Bing chatbot, ChatGPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15096">
<div class="article-summary-box-inner">
<span><p>Most works on transformers trained with the Masked Language Modeling (MLM)
objective use the original BERT model's fixed masking rate of 15%. We propose
to instead dynamically schedule the masking rate throughout training. We find
that linearly decreasing the masking rate over the course of pretraining
improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and
BERT-large, respectively, compared to fixed rate baselines. These gains come
from exposure to both high and low masking rate regimes, providing benefits
from both settings. Our results demonstrate that masking rate scheduling is a
simple way to improve the quality of masked language models, achieving up to a
1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for
BERT-large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14096">
<div class="article-summary-box-inner">
<span><p>Entity-level fine-grained sentiment analysis in the financial domain is a
crucial subtask of sentiment analysis and currently faces numerous challenges.
The primary challenge stems from the lack of high-quality and large-scale
annotated corpora specifically designed for financial text sentiment analysis,
which in turn limits the availability of data necessary for developing
effective text processing techniques. Recent advancements in large language
models (LLMs) have yielded remarkable performance in natural language
processing tasks, primarily centered around language pattern matching. In this
paper, we propose a novel and extensive Chinese fine-grained financial
sentiment analysis dataset, FinChina SA, for enterprise early warning. We
thoroughly evaluate and experiment with well-known existing open-source LLMs
using our dataset. We firmly believe that our dataset will serve as a valuable
resource to advance the exploration of real-world financial sentiment analysis
tasks, which should be the focus of future research. The FinChina SA dataset is
publicly available at https://github.com/YerayL/FinChina-SA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09308">
<div class="article-summary-box-inner">
<span><p>Retrieval augmentation, which enhances downstream models by a knowledge
retriever and an external corpus instead of by merely increasing the number of
model parameters, has been successfully applied to many natural language
processing (NLP) tasks such as text classification, question answering and so
on. However, existing methods that separately or asynchronously train the
retriever and downstream model mainly due to the non-differentiability between
the two parts, usually lead to degraded performance compared to end-to-end
joint training. In this paper, we propose Differentiable Retrieval Augmentation
via Generative lANguage modeling(Dragan), to address this problem by a novel
differentiable reformulation. We demonstrate the effectiveness of our proposed
method on a challenging NLP task in e-commerce search, namely query intent
classification. Both the experimental results and ablation study show that the
proposed method significantly and reasonably improves the state-of-the-art
baselines on both offline evaluation and online A/B test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09729">
<div class="article-summary-box-inner">
<span><p>LLMs usually exhibit limitations in their ability to incorporate new
knowledge, the generation of hallucinations, and the transparency of their
decision-making process. In this paper, we explore how to prompt LLMs with
knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date
knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a
prompting pipeline that endows LLMs with the capability of comprehending KG
inputs and inferring with a combined implicit knowledge and the retrieved
external knowledge. In addition, we investigate eliciting the mind map on which
LLMs perform the reasoning and generate the answers. It is identified that the
produced mind map exhibits the reasoning pathways of LLMs grounded on the
ontology of knowledge, hence bringing the prospects of probing and gauging LLM
inference in production. The experiments on three question &amp; answering datasets
also show that MindMap prompting leads to a striking empirical gain. For
instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance
over GPT-4 consistently. We also demonstrate that with structured facts
retrieved from KG, MindMap can outperform a series of
prompting-with-document-retrieval methods, benefiting from more accurate,
concise, and comprehensive knowledge from KGs. To reproduce our results and
extend the framework further, we make our codebase available at
https://github.com/wyl.willing/MindMap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10755">
<div class="article-summary-box-inner">
<span><p>The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the
development of large models, leading to the creation of numerous impressive
large language models(LLMs) and multimodal large language models (MLLMs). These
cutting-edge models owe their remarkable performance to high-quality data.
However, the details of the training data used in leading paradigms are often
kept confidential. This lack of transparency, coupled with the scarcity of
open-source data, impedes further developments within the community. As a
response, this paper presents "Wan Juan", a large-scale multimodal dataset
composed of both Chinese and English data, collected from a wide range of web
sources. The dataset incorporates text, image-text, and video modalities, with
a total volume exceeding 2TB. It was utilized in the training of InternLM, a
model that demonstrated significant advantages in multi-dimensional evaluations
when compared to models of a similar scale. All data can be accessed at
https://opendatalab.org.cn/WanJuan1.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02706">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable
capabilities across a wide range of tasks, however, the attention given to
non-English languages has been limited in this field of research. To address
this gap and assess the proficiency of language models in the Korean language
and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary,
history, and general knowledge. Our evaluation of language models on this
benchmark highlights the potential advantages of employing Large
Language-Specific Models(LLSMs) over a comprehensive, universal model like
GPT-3.5. Remarkably, our study reveals that models approximately 13 times
smaller than GPT-3.5 can exhibit similar performance levels in terms of
language-specific knowledge retrieval. This observation underscores the
importance of homogeneous corpora for training professional-level
language-specific models. On the contrary, we also observe a perplexing
performance dip in these smaller LMs when they are tasked to generate
structured answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Everyone Deserves A Reward: Learning Customized Human Preferences. (arXiv:2309.03126v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03126">
<div class="article-summary-box-inner">
<span><p>Reward models (RMs) are essential for aligning large language models (LLMs)
with human preferences to improve interaction quality. However, the real world
is pluralistic, which leads to diversified human preferences with respect to
different religions, politics, cultures, etc. Moreover, each individual can
have their unique preferences on various topics. Neglecting the diversity of
human preferences, current human feedback aligning methods only consider a
general reward model, which is below satisfaction for customized or
personalized application scenarios. To explore customized preference learning,
we collect a domain-specific preference (DSP) dataset, which includes preferred
responses for each given query from four practical domains. Besides, from the
perspective of data efficiency, we propose a three-stage customized RM learning
scheme, then empirically verify its effectiveness on both general preference
datasets and our DSP set. Furthermore, we test multiple training and data
strategies on the three learning stages. We find several ways to better
preserve the general preferring ability while training the customized RMs,
especially general preference enrichment, and customized preference imitation
learning. The DSP dataset and code are available at
https://github.com/Linear95/DSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games: A Usability Assessment. (arXiv:2309.07773v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07773">
<div class="article-summary-box-inner">
<span><p>This paper presents an empirical investigation of the extent to which spoken
Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile
serious game (MSG) applications. The aim of the research is to assess the
impact of multiple agents and illusion of humanness on the quality of the
interaction. The experiment investigates two styles of agent presentation: an
agent of high human-likeness (HECA) and an agent of low human-likeness (text).
The purpose of the experiment is to assess whether and how agents of high
humanlikeness can evoke the illusion of humanness and affect usability. Agents
of high human-likeness were designed by following the ECA design model that is
a proposed guide for ECA development. The results of the experiment with 90
participants show that users prefer to interact with the HECAs. The difference
between the two versions is statistically significant with a large effect size
(d=1.01), with many of the participants justifying their choice by saying that
the human-like characteristics of the HECA made the version more appealing.
This research provides key information on the potential effect of HECAs on
serious games, which can provide insight into the design of future mobile
serious games.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration. (arXiv:2309.07822v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07822">
<div class="article-summary-box-inner">
<span><p>In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07864">
<div class="article-summary-box-inner">
<span><p>For a long time, humanity has pursued artificial intelligence (AI) equivalent
to or surpassing the human level, with AI agents considered a promising vehicle
for this pursuit. AI agents are artificial entities that sense their
environment, make decisions, and take actions. Many efforts have been made to
develop intelligent AI agents since the mid-20th century. However, these
efforts have mainly focused on advancement in algorithms or training strategies
to enhance specific capabilities or performance on particular tasks. Actually,
what the community lacks is a sufficiently general and powerful model to serve
as a starting point for designing AI agents that can adapt to diverse
scenarios. Due to the versatile and remarkable capabilities they demonstrate,
large language models (LLMs) are regarded as potential sparks for Artificial
General Intelligence (AGI), offering hope for building general AI agents. Many
research efforts have leveraged LLMs as the foundation to build AI agents and
have achieved significant progress. We start by tracing the concept of agents
from its philosophical origins to its development in AI, and explain why LLMs
are suitable foundations for AI agents. Building upon this, we present a
conceptual framework for LLM-based agents, comprising three main components:
brain, perception, and action, and the framework can be tailored to suit
different applications. Subsequently, we explore the extensive applications of
LLM-based agents in three aspects: single-agent scenarios, multi-agent
scenarios, and human-agent cooperation. Following this, we delve into agent
societies, exploring the behavior and personality of LLM-based agents, the
social phenomena that emerge when they form societies, and the insights they
offer for human society. Finally, we discuss a range of key topics and open
problems within the field.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-18 23:10:58.997310531 UTC">2023-09-18 23:10:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>