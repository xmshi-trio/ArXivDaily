<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-07-04T01:30:00Z">07-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00008">
<div class="article-summary-box-inner">
<span><p>The current era of natural language processing (NLP) has been defined by the
prominence of pre-trained language models since the advent of BERT. A feature
of BERT and models with similar architecture is the objective of masked
language modeling, in which part of the input is intentionally masked and the
model is trained to predict this piece of masked information. Data augmentation
is a data-driven technique widely used in machine learning, including research
areas like computer vision and natural language processing, to improve model
performance by artificially augmenting the training data set by designated
techniques. Masked language models (MLM), an essential training feature of
BERT, have introduced a novel approach to perform effective pre-training on
Transformer based models in natural language processing tasks. Recent studies
have utilized masked language model to generate artificially augmented data for
NLP downstream tasks. The experimental results show that Mask based data
augmentation method provides a simple but efficient approach to improve the
model performance. In this paper, we explore and discuss the broader
utilization of these data augmentation methods based on MLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00009">
<div class="article-summary-box-inner">
<span><p>Software issues contain units of work to fix, improve or create new threads
during the development and facilitate communication among the team members.
Assigning an issue to the most relevant team member and determining a category
of an issue is a tedious and challenging task. Wrong classifications cause
delays and rework in the project and trouble among the team members. This
thesis proposes a set of carefully curated linguistic features for shallow
machine learning methods and compares the performance of shallow and ensemble
methods with deep language models. Unlike the state-of-the-art, we assign
issues to four roles (designer, developer, tester, and leader) rather than to
specific individuals or teams to contribute to the generality of our solution.
We also consider the level of experience of the developers to reflect the
industrial practices in our solution formulation. We employ a classification
approach to categorize issues into distinct classes, namely bug, new feature,
improvement, and other. Additionally, we endeavor to further classify bugs
based on the specific type of modification required. We collect and annotate
five industrial data sets from one of the top three global television producers
to evaluate our proposal and compare it with deep language models. Our data
sets contain 5324 issues in total. We show that an ensemble classifier of
shallow techniques achieves 0.92 for issue assignment and 0.90 for issue
classification in accuracy which is statistically comparable to the
state-of-the-art deep language models. The contributions include the public
sharing of five annotated industrial issue data sets, the development of a
clear and comprehensive feature set, the introduction of a novel label set and
the validation of the efficacy of an ensemble classifier of shallow machine
learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAHAAYAK 2023 -- the Multi Domain Bilingual Parallel Corpus of Sanskrit to Hindi for Machine Translation. (arXiv:2307.00021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00021">
<div class="article-summary-box-inner">
<span><p>The data article presents the large bilingual parallel corpus of
low-resourced language pair Sanskrit-Hindi, named SAHAAYAK 2023. The corpus
contains total of 1.5M sentence pairs between Sanskrit and Hindi. To make the
universal usability of the corpus and to make it balanced, data from multiple
domain has been incorporated into the corpus that includes, News, Daily
conversations, Politics, History, Sport, and Ancient Indian Literature. The
multifaceted approach has been adapted to make a sizable multi-domain corpus of
low-resourced languages like Sanskrit. Our development approach is spanned from
creating a small hand-crafted dataset to applying a wide range of mining,
cleaning, and verification. We have used the three-fold process of mining:
mining from machine-readable sources, mining from non-machine readable sources,
and collation from existing corpora sources. Post mining, the dedicated
pipeline for normalization, alignment, and corpus cleaning is developed and
applied to the corpus to make it ready to use on machine translation
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00028">
<div class="article-summary-box-inner">
<span><p>Neural networks for computer vision extract uninterpretable features despite
achieving high accuracy on benchmarks. In contrast, humans can explain their
predictions using succinct and intuitive descriptions. To incorporate
explainability into neural networks, we train a vision model whose feature
representations are text. We show that such a model can effectively classify
ImageNet images, and we discuss the challenges we encountered when training it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00101">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are trained primarily on minimally processed web
text, which exhibits the same wide range of social biases held by the humans
who created that content. Consequently, text generated by LLMs can
inadvertently perpetuate stereotypes towards marginalized groups, like the
LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs
generate text describing people with different sexual identities. Analyzing
bias in the text generated by an LLM using regard score shows measurable bias
against queer people. We then show that a post-hoc method based on
chain-of-thought prompting using SHAP analysis can increase the regard of the
sentence, representing a promising approach towards debiasing the output of
LLMs in this setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00108">
<div class="article-summary-box-inner">
<span><p>An essential aspect of prioritizing incident tickets for resolution is
efficiently labeling tickets with fine-grained categories. However, ticket data
is often complex and poses several unique challenges for modern machine
learning methods: (1) tickets are created and updated either by machines with
pre-defined algorithms or by engineers with domain expertise that share
different protocols, (2) tickets receive frequent revisions that update ticket
status by modifying all or parts of ticket descriptions, and (3) ticket
labeling is time-sensitive and requires knowledge updates and new labels per
the rapid software and hardware improvement lifecycle. To handle these issues,
we introduce Ticket- BERT which trains a simple yet robust language model for
labeling tickets using our proposed ticket datasets. Experiments demonstrate
the superiority of Ticket-BERT over baselines and state-of-the-art text
classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT
with an active learning cycle and deploy it on the Microsoft IcM system, which
enables the model to quickly finetune on newly-collected tickets with a few
annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-training with Demonstration Retrieval for Efficient Few-shot Learning. (arXiv:2307.00119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00119">
<div class="article-summary-box-inner">
<span><p>Large language models show impressive results on few-shot NLP tasks. However,
these models are memory and computation-intensive. Meta-training allows one to
leverage smaller models for few-shot generalization in a domain-general and
task-agnostic manner; however, these methods alone results in models that may
not have sufficient parameterization or knowledge to adapt quickly to a large
variety of tasks. To overcome this issue, we propose meta-training with
demonstration retrieval, where we use a dense passage retriever to retrieve
semantically similar labeled demonstrations to each example for more varied
supervision. By separating external knowledge from model parameters, we can use
meta-training to train parameter-efficient models that generalize well on a
larger variety of tasks. We construct a meta-training set from UnifiedQA and
CrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our
knowledge, our work is the first to combine retrieval with meta-training, to
use DPR models to retrieve demonstrations, and to leverage demonstrations from
many tasks simultaneously, rather than randomly sampling demonstrations from
the training set of the target task. Our approach outperforms a variety of
targeted parameter-efficient and retrieval-augmented few-shot methods on QA,
NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our
approach can be meta-trained and fine-tuned quickly on a single GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Extraction in Domain and Generic Documents: Findings from Heuristic-based and Data-driven Approaches. (arXiv:2307.00130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00130">
<div class="article-summary-box-inner">
<span><p>Information extraction (IE) plays very important role in natural language
processing (NLP) and is fundamental to many NLP applications that used to
extract structured information from unstructured text data. Heuristic-based
searching and data-driven learning are two main stream implementation
approaches. However, no much attention has been paid to document genre and
length influence on IE tasks. To fill the gap, in this study, we investigated
the accuracy and generalization abilities of heuristic-based searching and
data-driven to perform two IE tasks: named entity recognition (NER) and
semantic role labeling (SRL) on domain-specific and generic documents with
different length. We posited two hypotheses: first, short documents may yield
better accuracy results compared to long documents; second, generic documents
may exhibit superior extraction outcomes relative to domain-dependent documents
due to training document genre limitations. Our findings reveals that no single
method demonstrated overwhelming performance in both tasks. For named entity
extraction, data-driven approaches outperformed symbolic methods in terms of
accuracy, particularly in short texts. In the case of semantic roles
extraction, we observed that heuristic-based searching method and data-driven
based model with syntax representation surpassed the performance of pure
data-driven approach which only consider semantic information. Additionally, we
discovered that different semantic roles exhibited varying accuracy levels with
the same method. This study offers valuable insights for downstream text mining
tasks, such as NER and SRL, when addressing various document features and
genres.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iMETRE: Incorporating Markers of Entity Types for Relation Extraction. (arXiv:2307.00132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00132">
<div class="article-summary-box-inner">
<span><p>Sentence-level relation extraction (RE) aims to identify the relationship
between 2 entities given a contextual sentence. While there have been many
attempts to solve this problem, the current solutions have a lot of room to
improve. In this paper, we approach the task of relationship extraction in the
financial dataset REFinD. Our approach incorporates typed entity markers
representations and various models finetuned on the dataset, which has allowed
us to achieve an F1 score of 69.65% on the validation set. Through this paper,
we discuss various approaches and possible limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding. (arXiv:2307.00135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00135">
<div class="article-summary-box-inner">
<span><p>We study the ability of transformer-based language models (LMs) to understand
social media language. Social media (SM) language is distinct from standard
written language, yet existing benchmarks fall short of capturing LM
performance in this socially, economically, and politically important domain.
We quantify the degree to which social media language differs from conventional
language and conclude that the difference is significant both in terms of token
distribution and rate of linguistic shift. Next, we introduce a new benchmark
for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and
eleven tasks. Finally, we show that learning a tokenizer and pretraining on a
mix of social media and conventional language yields an LM that outperforms the
best similar-sized alternative by 4.2 points on the overall SMILE score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00162">
<div class="article-summary-box-inner">
<span><p>Many self-supervised speech models (S3Ms) have been introduced over the last
few years, producing performance and data efficiency improvements for a variety
of speech tasks. Evidence is emerging that different S3Ms encode linguistic
information in different layers, and also that some S3Ms appear to learn
phone-like sub-word units. However, the extent to which these models capture
larger linguistic units, such as words, and where word-related information is
encoded, remains unclear. In this study, we conduct several analyses of word
segment representations extracted from different layers of three S3Ms:
wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a
lightweight analysis tool, to measure the similarity between these
representations and word-level linguistic properties. We find that the maximal
word-level linguistic content tends to be found in intermediate model layers,
while some lower-level information like pronunciation is also retained in
higher layers of HuBERT and WavLM. Syntactic and semantic word attributes have
similar layer-wise behavior. We also find that, for all of the models tested,
word identity information is concentrated near the center of each word segment.
We then test the layer-wise performance of the same models, when used directly
with no additional learned parameters, on several tasks: acoustic word
discrimination, word segmentation, and semantic sentence similarity. We find
similar layer-wise trends in performance, and furthermore, find that when using
the best-performing layer of HuBERT or WavLM, it is possible to achieve
performance on word segmentation and sentence similarity that rivals more
complex existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Integer Linear Programming Inference Cookbook. (arXiv:2307.00171v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00171">
<div class="article-summary-box-inner">
<span><p>Over the years, integer linear programs have been employed to model inference
in many natural language processing problems. This survey is meant to guide the
reader through the process of framing a new inference problem as an instance of
an integer linear program and is structured as a collection of recipes. At the
end, we will see two worked examples to illustrate the use of these recipes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00175">
<div class="article-summary-box-inner">
<span><p>We consider the questions of whether or not large language models (LLMs) have
beliefs, and, if they do, how we might measure them. First, we evaluate two
existing approaches, one due to Azaria and Mitchell (2023) and the other to
Burns et al. (2022). We provide empirical results that show that these methods
fail to generalize in very basic ways. We then argue that, even if LLMs have
beliefs, these methods are unlikely to be successful for conceptual reasons.
Thus, there is still no lie-detector for LLMs. After describing our empirical
results we take a step back and consider whether or not we should expect LLMs
to have something like beliefs in the first place. We consider some recent
arguments aiming to show that LLMs cannot have beliefs. We show that these
arguments are misguided. We provide a more productive framing of questions
surrounding the status of beliefs in LLMs, and highlight the empirical nature
of the problem. We conclude by suggesting some concrete paths for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00184">
<div class="article-summary-box-inner">
<span><p>The advent of large language models (LLMs) has revolutionized natural
language processing, enabling the generation of coherent and contextually
relevant text. As LLMs increasingly power conversational agents, the
synthesized personality embedded in these models by virtue of their training on
large amounts of human-generated data draws attention. Since personality is an
important factor determining the effectiveness of communication, we present a
comprehensive method for administering validated psychometric tests and
quantifying, analyzing, and shaping personality traits exhibited in text
generated from widely-used LLMs. We find that: 1) personality simulated in the
outputs of some LLMs (under specific prompting configurations) is reliable and
valid; 2) evidence of reliability and validity of LLM-simulated personality is
stronger for larger and instruction fine-tuned models; and 3) personality in
LLM outputs can be shaped along desired dimensions to mimic specific
personality profiles. We also discuss potential applications and ethical
implications of our measurement and shaping framework, especially regarding
responsible use of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain. (arXiv:2307.00186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00186">
<div class="article-summary-box-inner">
<span><p>Recent advancements in language models (LMs) have led to the emergence of
powerful models such as Small LMs (e.g., T5) and Large LMs (e.g., GPT-4). These
models have demonstrated exceptional capabilities across a wide range of tasks,
such as name entity recognition (NER) in the general domain. (We define SLMs as
pre-trained models with fewer parameters compared to models like GPT-3/3.5/4,
such as T5, BERT, and others.) Nevertheless, their efficacy in the medical
section remains uncertain and the performance of medical NER always needs high
accuracy because of the particularity of the field. This paper aims to provide
a thorough investigation to compare the performance of LMs in medical few-shot
NER and answer How far is LMs from 100\% Few-shot NER in Medical Domain, and
moreover to explore an effective entity recognizer to help improve the NER
performance. Based on our extensive experiments conducted on 16 NER models
spanning from 2018 to 2023, our findings clearly indicate that LLMs outperform
SLMs in few-shot medical NER tasks, given the presence of suitable examples and
appropriate logical frameworks. Despite the overall superiority of LLMs in
few-shot medical NER tasks, it is important to note that they still encounter
some challenges, such as misidentification, wrong template prediction, etc.
Building on previous findings, we introduce a simple and effective method
called \textsc{RT} (Retrieving and Thinking), which serves as retrievers,
finding relevant examples, and as thinkers, employing a step-by-step reasoning
process. Experimental results show that our proposed \textsc{RT} framework
significantly outperforms the strong open baselines on the two open medical
benchmark datasets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00209">
<div class="article-summary-box-inner">
<span><p>Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00259">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) performs tasks by prompting a large language model
(LLM) using an instruction and a small set of annotated examples called
demonstrations. Recent work has shown that the precise details of the inputs
used in the prompt significantly impacts ICL, which has incentivized
instruction selection algorithms. The effect of instruction-choice however is
severely underexplored, with existing analyses being restricted to shallow
subsets of models and tasks, which limits the generalizability of their
insights. We develop an ICL evaluation suite to conduct a thorough assessment
of these techniques. The suite includes 13 open-sourced LLMs of varying scales
from 4 distinct model families and covers 9 different tasks, representing a
range of task types across 3 categories. In this work, we evaluate the relative
performance of 7 popular instruction selection methods using our benchmark over
five desiderata relevant to ICL. We discover that using curated
manually-written instructions and simple instructions without any task-specific
descriptions often elicits superior ICL performance than that of automatic
instruction-induction methods, pointing to a lack of generalizability among the
latter. We release our evaluation suite for benchmarking instruction selection
approaches, and call for more rigorous and generalizable methods in this space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Pretraining for Biomedical Term Embeddings. (arXiv:2307.00266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00266">
<div class="article-summary-box-inner">
<span><p>Electronic health records (EHR) contain narrative notes that provide
extensive details on the medical condition and management of patients. Natural
language processing (NLP) of clinical notes can use observed frequencies of
clinical terms as predictive features for downstream applications such as
clinical decision making and patient trajectory prediction. However, due to the
vast number of highly similar and related clinical concepts, a more effective
modeling strategy is to represent clinical terms as semantic embeddings via
representation learning and use the low dimensional embeddings as feature
vectors for predictive modeling. To achieve efficient representation,
fine-tuning pretrained language models with biomedical knowledge graphs may
generate better embeddings for biomedical terms than those from standard
language models alone. These embeddings can effectively discriminate synonymous
pairs of from those that are unrelated. However, they often fail to capture
different degrees of similarity or relatedness for concepts that are
hierarchical in nature. To overcome this limitation, we propose HiPrBERT, a
novel biomedical term representation model trained on additionally complied
data that contains hierarchical structures for various biomedical terms. We
modify an existing contrastive loss function to extract information from these
hierarchies. Our numerical experiments demonstrate that HiPrBERT effectively
learns the pair-wise distance from hierarchical information, resulting in a
substantially more informative embeddings for further biomedical applications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let Me Teach You: Pedagogical Foundations of Feedback for Language Models. (arXiv:2307.00279v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00279">
<div class="article-summary-box-inner">
<span><p>Natural Language Feedback (NLF) is an increasingly popular avenue to align
Large Language Models (LLMs) to human preferences. Despite the richness and
diversity of the information it can convey, NLF is often hand-designed and
arbitrary. In a different world, research in pedagogy has long established
several effective feedback models. In this opinion piece, we compile ideas from
pedagogy to introduce FELT, a feedback framework for LLMs that outlines the
various characteristics of the feedback space, and a feedback content taxonomy
based on these variables. Our taxonomy offers both a general mapping of the
feedback space, as well as pedagogy-established discrete categories, allowing
us to empirically demonstrate the impact of different feedback types on revised
generations. In addition to streamlining existing NLF designs, FELT also brings
out new, unexplored directions for research in NLF. We make our taxonomy
available to the community, providing guides and examples for mapping our
categorizations to future resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A model of interaction semantics. (arXiv:2007.06258v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06258">
<div class="article-summary-box-inner">
<span><p>Purpose: The purpose of this article is to propose, based on a model of an
interaction semantics, a certain understanding of the ''meaning'' of the
exchanged characters within an interaction.
</p>
<p>Methodology: Based on a model of system interaction, I structure the model of
interaction semantics similar to the semantics of a formal language: first, I
identify adequate variables in my interaction model to assign values to, and
second, I identify the interpretation function to provide meaning. Thereby I
arrive at a model of interaction semantics which, in the sense of the late
Ludwig Wittgenstein, can do without a 'mental' mapping from characters to
concepts.
</p>
<p>Findings: The key findings are a better understanding of the tight relation
between the informatical approach to model interactions and game theory; of the
central 'chicken and egg' problem, any natural language has to solve, namely
that to interact sensibly, we have to understand each other and to acquire a
common understanding, we have to interact with each other, which I call the
'simultaneous interaction and understanding (SIAU)' problem; why ontologies are
less 'semantic' then their proponents suggest; and how 'semantic'
interoperability is to be achieved.
</p>
<p>Value: The main value of the proposed model of interaction semantics is that
it could be applied in many different disciplines and therefore could serve as
a basis for scientists of natural sciences and humanities as well as engineers
to understand each other more easily talking about semantics, especially with
the advent of cyber-physical systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPTAM: Constituency Parse Tree Aggregation Method. (arXiv:2201.07905v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07905">
<div class="article-summary-box-inner">
<span><p>Diverse Natural Language Processing tasks employ constituency parsing to
understand the syntactic structure of a sentence according to a phrase
structure grammar. Many state-of-the-art constituency parsers are proposed, but
they may provide different results for the same sentences, especially for
corpora outside their training domains. This paper adopts the truth discovery
idea to aggregate constituency parse trees from different parsers by estimating
their reliability in the absence of ground truth. Our goal is to consistently
obtain high-quality aggregated constituency parse trees. We formulate the
constituency parse tree aggregation problem in two steps, structure aggregation
and constituent label aggregation. Specifically, we propose the first truth
discovery solution for tree structures by minimizing the weighted sum of
Robinson-Foulds (RF) distances, a classic symmetric distance metric between two
trees. Extensive experiments are conducted on benchmark datasets in different
languages and domains. The experimental results show that our method, CPTAM,
outperforms the state-of-the-art aggregation baselines. We also demonstrate
that the weights estimated by CPTAM can adequately evaluate constituency
parsers in the absence of ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELQA: A Corpus of Metalinguistic Questions and Answers about English. (arXiv:2205.00395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00395">
<div class="article-summary-box-inner">
<span><p>We present ELQA, a corpus of questions and answers in and about the English
language. Collected from two online forums, the &gt;70k questions (from English
learners and others) cover wide-ranging topics including grammar, meaning,
fluency, and etymology. The answers include descriptions of general properties
of English vocabulary and grammar as well as explanations about specific
(correct and incorrect) usage examples. Unlike most NLP datasets, this corpus
is metalinguistic -- it consists of language about language. As such, it can
facilitate investigations of the metalinguistic capabilities of NLU models, as
well as educational applications in the language learning domain. To study
this, we define a free-form question answering task on our dataset and conduct
evaluations on multiple LLMs (Large Language Models) to analyze their capacity
to generate metalinguistic answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Reranking for Multi-hop QA via Language Model Prompting. (arXiv:2205.12650v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12650">
<div class="article-summary-box-inner">
<span><p>We study few-shot reranking for multi-hop QA with open-domain questions. To
alleviate the need for a large number of labeled question-document pairs for
retriever training, we propose PromptRank, which relies on large language
models prompting for multi-hop path reranking. PromptRank first constructs an
instruction-based prompt that includes a candidate document path and then
computes the relevance score between a given question and the path based on the
conditional likelihood of the question given the path prompt according to a
language model. PromptRank yields strong retrieval performance on HotpotQA with
only 128 training examples compared to state-of-the-art methods trained on
thousands of examples -- 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever
and 77.5 by multi-hop dense retrieval. Code available at
https://github.com/mukhal/PromptRank
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASTA: A Dataset for Modeling Participant States in Narratives. (arXiv:2208.00329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00329">
<div class="article-summary-box-inner">
<span><p>The events in a narrative are understood as a coherent whole via the
underlying states of their participants. Often, these participant states are
not explicitly mentioned, instead left to be inferred by the reader. A model
that understands narratives should likewise infer these implicit states, and
even reason about the impact of changes to these states on the narrative. To
facilitate this goal, we introduce a new crowdsourced English-language,
Participant States dataset, PASTA. This dataset contains inferable participant
states; a counterfactual perturbation to each state; and the changes to the
story that would be necessary if the counterfactual were true. We introduce
three state-based reasoning tasks that test for the ability to infer when a
state is entailed by a story, to revise a story conditioned on a counterfactual
state, and to explain the most likely state change given a revised story.
Experiments show that today's LLMs can reason about states to some degree, but
there is large room for improvement, especially in problems requiring access
and ability to reason with diverse types of knowledge (e.g. physical,
numerical, factual).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-agnostic Code-Switching in Sequence-To-Sequence Speech Recognition. (arXiv:2210.08992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08992">
<div class="article-summary-box-inner">
<span><p>Code-Switching (CS) is referred to the phenomenon of alternately using words
and phrases from different languages. While today's neural end-to-end (E2E)
models deliver state-of-the-art performances on the task of automatic speech
recognition (ASR) it is commonly known that these systems are very
data-intensive. However, there is only a few transcribed and aligned CS speech
available. To overcome this problem and train multilingual systems which can
transcribe CS speech, we propose a simple yet effective data augmentation in
which audio and corresponding labels of different source languages are
concatenated. By using this training data, our E2E model improves on
transcribing CS speech. It also surpasses monolingual models on monolingual
tests. The results show that this augmentation technique can even improve the
model's performance on inter-sentential language switches not seen during
training by 5,03% WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cracking Double-Blind Review: Authorship Attribution with Deep Learning. (arXiv:2211.07467v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07467">
<div class="article-summary-box-inner">
<span><p>Double-blind peer review is considered a pillar of academic research because
it is perceived to ensure a fair, unbiased, and fact-centered scientific
discussion. Yet, experienced researchers can often correctly guess from which
research group an anonymous submission originates, biasing the peer-review
process. In this work, we present a transformer-based, neural-network
architecture that only uses the text content and the author names in the
bibliography to attribute an anonymous manuscript to an author. To train and
evaluate our method, we created the largest authorship identification dataset
to date. It leverages all research papers publicly available on arXiv amounting
to over 2 million manuscripts. In arXiv-subsets with up to 2,000 different
authors, our method achieves an unprecedented authorship attribution accuracy,
where up to 73% of papers are attributed correctly. We present a scaling
analysis to highlight the applicability of the proposed method to even larger
datasets when sufficient compute capabilities are more widely available to the
academic community. Furthermore, we analyze the attribution accuracy in
settings where the goal is to identify all authors of an anonymous manuscript.
Thanks to our method, we are not only able to predict the author of an
anonymous work, but we also provide empirical evidence of the key aspects that
make a paper attributable. We have open-sourced the necessary tools to
reproduce our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBoosting: Black-Box Text Classification with Ten Forward Passes. (arXiv:2212.09257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09257">
<div class="article-summary-box-inner">
<span><p>We describe PromptBoosting, a query-efficient procedure for building a text
classifier from a neural language model (LM) without access to the LM's
parameters, gradients, or hidden representations. This form of "black-box"
classifier training has become increasingly important as the cost of training
and inference in large-scale LMs grows. But existing black-box LM classifier
learning approaches are themselves computationally inefficient, typically
specializing LMs to the target task by searching in a large space of (discrete
or continuous) prompts using zeroth-order optimization methods. Instead of
directly optimizing in prompt space, PromptBoosting obtains a small pool of
prompts via a gradient-free approach and then constructs a large pool of weak
learners by pairing these prompts with different elements of the LM's output
distribution. These weak learners are then ensembled using the AdaBoost
algorithm. The entire learning process requires only a small number of forward
passes and no backward pass. Experiments show that PromptBoosting achieves
state-of-the-art performance in multiple black-box few-shot classification
tasks, and matches or outperforms full fine-tuning in both few-shot and
standard learning paradigms, while training 10x faster than existing black-box
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09683">
<div class="article-summary-box-inner">
<span><p>We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. We make our data and
detailed annotation guidelines available to support the evaluation of
human-in-the-loop systems that identify novel misinformation directly from raw
user-generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages. (arXiv:2212.10180v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10180">
<div class="article-summary-box-inner">
<span><p>The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. (arXiv:2212.10511v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10511">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs' strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the long tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models' performance while reducing the inference
costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets. (arXiv:2301.12139v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12139">
<div class="article-summary-box-inner">
<span><p>We investigate five English NLP benchmark datasets (on the superGLUE
leaderboard) and two Swedish datasets for bias, along multiple axes. The
datasets are the following: Boolean Question (Boolq), CommitmentBank (CB),
Winograd Schema Challenge (WSC), Wino-gender diagnostic (AXg), Recognising
Textual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is
known to be common in data, which ML models learn from. In order to mitigate
bias in data, it is crucial to be able to estimate it objectively. We use
bipol, a novel multi-axes bias metric with explainability, to estimate and
explain how much bias exists in these datasets. Multilingual, multi-axes bias
evaluation is not very common. Hence, we also contribute a new, large Swedish
bias-labelled dataset (of 2 million samples), translated from the English
version and train the SotA mT5 model on it. In addition, we contribute new
multi-axes lexica for bias detection in Swedish. We make the codes, model, and
new dataset publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Specific Skill Localization in Fine-tuned Language Models. (arXiv:2302.06600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06600">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models can be fine-tuned to solve diverse NLP tasks,
including in few-shot settings. Thus fine-tuning allows the model to quickly
pick up task-specific ``skills,'' but there has been limited study of where
these newly-learnt skills reside inside the massive model. This paper
introduces the term skill localization for this problem and proposes a
solution. Given the downstream task and a model fine-tuned on that task, a
simple optimization is used to identify a very small subset of parameters
($\sim0.01$% of model parameters) responsible for ($&gt;95$%) of the model's
performance, in the sense that grafting the fine-tuned values for just this
tiny subset onto the pre-trained model gives performance almost as well as the
fine-tuned model. While reminiscent of recent works on parameter-efficient
fine-tuning, the novel aspects here are that: (i) No further re-training is
needed on the subset (unlike, say, with lottery tickets). (ii) Notable
improvements are seen over vanilla fine-tuning with respect to calibration of
predictions in-distribution ($40$-$90$% error reduction) as well as the quality
of predictions out-of-distribution (OOD). In models trained on multiple tasks,
a stronger notion of skill localization is observed, where the sparse regions
corresponding to different tasks are almost disjoint, and their overlap (when
it happens) is a proxy for task similarity. Experiments suggest that
localization via grafting can assist certain forms of continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14229">
<div class="article-summary-box-inner">
<span><p>Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has
attracted wide attention from the computational linguistics community. However,
it is not yet known the performance of LLMs on CLS. In this report, we
empirically use various prompts to guide LLMs to perform zero-shot CLS from
different paradigms (i.e., end-to-end and pipeline), and provide a preliminary
evaluation on the generated summaries. We find that ChatGPT and GPT-4
originally prefer to produce lengthy summaries with detailed information. These
two LLMs can further balance informativeness and conciseness with the help of
an interactive prompt, significantly improving their CLS performance.
Experimental results on three widely-used CLS datasets show that GPT-4 achieves
state-of-the-art zero-shot CLS performance, and performs competitively compared
with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and
bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited
zero-shot CLS ability. Due to the composite nature of CLS, which requires
models to perform summarization and translation simultaneously, accomplishing
this task in a zero-shot manner is even a challenge for LLMs. Therefore, we
sincerely hope and recommend future LLM research could use CLS as a testbed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview on Language Models: Recent Developments and Outlook. (arXiv:2303.05759v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05759">
<div class="article-summary-box-inner">
<span><p>Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12135">
<div class="article-summary-box-inner">
<span><p>The growth of pending legal cases in populous countries, such as India, has
become a major issue. Developing effective techniques to process and understand
legal documents is extremely useful in resolving this problem. In this paper,
we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi
et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that
considers the comprehensive context information in both intra- and
inter-sentence levels to predict rhetorical roles (subtask A) and then train a
Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize
legal entities (subtask B). Our evaluations demonstrate that our designed
models are more accurate than baselines, e.g., with an up to 15.0% better F1
score in subtask B. We achieved notable performance in the task leaderboard,
e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v3 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07619">
<div class="article-summary-box-inner">
<span><p>We examine the potential of ChatGPT, and other large language models, in
predicting stock market returns using sentiment analysis of news headlines. We
use ChatGPT to indicate whether a given headline is good, bad, or irrelevant
news for firms' stock prices. We then compute a numerical score and document a
positive correlation between these ``ChatGPT scores'' and subsequent daily
stock market returns. Further, ChatGPT outperforms traditional sentiment
analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT
cannot accurately forecast returns, indicating return predictability is an
emerging capacity of complex models. ChatGPT-4's implied Sharpe ratios are
larger than ChatGPT-3's; however, the latter model has larger total returns.
Our results suggest that incorporating advanced language models into the
investment decision-making process can yield more accurate predictions and
enhance the performance of quantitative trading strategies. Predictability is
concentrated on smaller stocks and more prominent on firms with bad news,
consistent with limits-to-arbitrage arguments rather than market
inefficiencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06174">
<div class="article-summary-box-inner">
<span><p>Climate change is the defining issue of our time, and we are at a defining
moment. Various interest groups, social movement organizations, and individuals
engage in collective action on this issue on social media. In addition, issue
advocacy campaigns on social media often arise in response to ongoing societal
concerns, especially those faced by energy industries. Our goal in this paper
is to analyze how those industries, their advocacy group, and climate advocacy
group use social media to influence the narrative on climate change. In this
work, we propose a minimally supervised model soup [57] approach combined with
messaging themes to identify the stances of climate ads on Facebook. Finally,
we release our stance dataset, model, and set of themes related to climate
campaigns for future work on opinion mining and the automatic detection of
climate change stances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06569">
<div class="article-summary-box-inner">
<span><p>Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07609">
<div class="article-summary-box-inner">
<span><p>The remarkable achievements of Large Language Models (LLMs) have led to the
emergence of a novel recommendation paradigm -- Recommendation via LLM
(RecLLM). Nevertheless, it is important to note that LLMs may contain social
prejudices, and therefore, the fairness of recommendations made by RecLLM
requires further investigation. To avoid the potential risks of RecLLM, it is
imperative to evaluate the fairness of RecLLM with respect to various sensitive
attributes on the user side. Due to the differences between the RecLLM paradigm
and the traditional recommendation paradigm, it is problematic to directly use
the fairness benchmark of traditional recommendation. To address the dilemma,
we propose a novel benchmark called Fairness of Recommendation via LLM
(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset
that accounts for eight sensitive attributes1 in two recommendation scenarios:
music and movies. By utilizing our FaiRLLM benchmark, we conducted an
evaluation of ChatGPT and discovered that it still exhibits unfairness to some
sensitive attributes when generating recommendations. Our code and dataset can
be found at https://github.com/jizhi-zhang/FaiRLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15689">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that natural-language prompts can help to
leverage the knowledge learned by pre-trained language models for the binary
sentence-level sentiment classification task. Specifically, these methods
utilize few-shot learning settings to fine-tune the sentiment classification
model using manual or automatically generated prompts. However, the performance
of these methods is sensitive to the perturbations of the utilized prompts.
Furthermore, these methods depend on a few labeled instances for automatic
prompt generation and prompt ranking. This study aims to find high-quality
prompts for the given task in a zero-shot setting. Given a base prompt, our
proposed approach automatically generates multiple prompts similar to the base
prompt employing positional, reasoning, and paraphrasing techniques and then
ranks the prompts using a novel metric. We empirically demonstrate that the
top-ranked prompts are high-quality and significantly outperform the base
prompt and the prompts generated using few-shot learning for the binary
sentence-level sentiment classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERGE: Fast Private Text Generation. (arXiv:2305.15769v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15769">
<div class="article-summary-box-inner">
<span><p>Recent years have seen increasing concerns about the private inference of NLP
services and Transformer models. However, existing two-party privacy-preserving
methods solely consider NLU scenarios, while the private inference of text
generation such as translation, dialogue, and code completion remains unsolved.
Besides, while migrated to NLG models, existing privacy-preserving methods
perform poorly in terms of inference speed, and suffer from the convergence
problem during the training stage. To address these issues, we propose MERGE, a
fast private text generation framework for Transformer-based language models.
Specifically, MERGE reuse the output hidden state as the word embedding to
bypass the embedding computation, and reorganize the linear operations in the
Transformer module to accelerate the forward procedure. Based on these two
optimizations, extensive experiments show that MERGE can achieve a 26.5x
speedup under the sequence length 512, and reduce 80\% communication bytes,
with an up to 10x speedup to existing state-of-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16380">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical PCG Through Large Language Models. (arXiv:2305.18243v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18243">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have proven to be useful tools in various
domains outside of the field of their inception, which was natural language
processing. In this study, we provide practical directions on how to use LLMs
to generate 2D-game rooms for an under-development game, named Metavoidal. Our
technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which
allows our method to create 37% Playable-Novel levels from as scarce data as
only 60 hand-designed rooms under a scenario of the non-trivial game, with
respect to (Procedural Content Generation) PCG, that has a good amount of local
and global constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18624">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00168">
<div class="article-summary-box-inner">
<span><p>Existing research on Domain Robustness (DR) suffers from disparate setups,
lack of evaluation task variety, and reliance on challenge sets. In this paper,
we pose a fundamental question: What is the state of affairs of the DR
challenge in the era of Large Language Models (LLMs)? To this end, we construct
a DR benchmark comprising diverse NLP tasks, including sentence and token-level
classification, QA, and generation, each task consists of several domains. We
explore the DR challenge of fine-tuned and few-shot learning models in natural
domain shift settings and devise two diagnostic metrics of Out-of-Distribution
(OOD) performance degradation: The commonly used Source Drop (SD) and the
overlooked Target Drop (TD). Our findings reveal important insights: First,
despite their capabilities, zero-to-few shot LLMs and fine-tuning approaches
still fail to meet satisfactory performance in the OOD context; Second, TD
approximates better than SD the average OOD degradation; Third, in a
significant proportion of domain shifts, either SD or TD is positive, but not
both, and therefore disregarding one can lead to incorrect DR conclusions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02561">
<div class="article-summary-box-inner">
<span><p>We present LLM-Blender, an ensembling framework designed to attain
consistently superior performance by leveraging the diverse strengths of
multiple open-source large language models (LLMs). Our framework consists of
two modules: PairRanker and GenFuser, addressing the observation that optimal
LLMs for different examples can significantly vary. PairRanker employs a
specialized pairwise comparison method to distinguish subtle differences
between candidate outputs. It jointly encodes the input text and a pair of
candidates, using cross-attention encoders to determine the superior one. Our
results demonstrate that PairRanker exhibits the highest correlation with
ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,
generating an improved output by capitalizing on their strengths and mitigating
their weaknesses. To facilitate large-scale evaluation, we introduce a
benchmark dataset, MixInstruct, which is a mixture of multiple instruction
datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly
outperform individual LLMs and baseline methods across various metrics,
establishing a substantial performance gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04634">
<div class="article-summary-box-inner">
<span><p>As LLMs become commonplace, machine-generated text has the potential to flood
the internet with spam, social media bots, and valueless content. Watermarking
is a simple and effective strategy for mitigating such harms by enabling the
detection and documentation of LLM-generated text. Yet a crucial question
remains: How reliable is watermarking in realistic settings in the wild? There,
watermarked text may be modified to suit a user's needs, or entirely rewritten
to avoid detection.
</p>
<p>We study the robustness of watermarked text after it is re-written by humans,
paraphrased by a non-watermarked LLM, or mixed into a longer hand-written
document. We find that watermarks remain detectable even after human and
machine paraphrasing. While these attacks dilute the strength of the watermark,
paraphrases are statistically likely to leak n-grams or even longer fragments
of the original text, resulting in high-confidence detections when enough
tokens are observed. For example, after strong human paraphrasing the watermark
is detectable after observing 800 tokens on average, when setting a 1e-5 false
positive rate. We also consider a range of new detection schemes that are
sensitive to short spans of watermarked text embedded inside a large document,
and we compare the robustness of watermarking to other kinds of detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15253">
<div class="article-summary-box-inner">
<span><p>Humans talk in free-form while negotiating the expressed meanings or common
ground. Despite the impressive conversational abilities of the large generative
language models, they do not consider the individual differences in contextual
understanding in a shared situated environment. In this work, we propose
MindDial, a novel conversational framework that can generate situated free-form
responses to negotiate common ground. We design an explicit mind module that
can track three-level beliefs -- the speaker's belief, the speaker's prediction
of the listener's belief, and the common belief based on the gap between the
first two. Then the speaking act classification head will decide to continue to
talk, end this turn, or take task-related action. We augment a common ground
alignment dataset MutualFriend with belief dynamics annotation, of which the
goal is to find a single mutual friend based on the free chat between two
agents. Experiments show that our model with mental state modeling can resemble
human responses when aligning common ground meanwhile mimic the natural human
conversation flow. The ablation study further validates the third-level common
belief can aggregate information of the first and second-order beliefs and
align common ground more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design. (arXiv:2306.15656v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15656">
<div class="article-summary-box-inner">
<span><p>This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be publicly
available upon paper acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Linguistic Knowledge and Token-level Text Augmentation. (arXiv:2306.16644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16644">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of token-level text augmentation
and the role of probabilistic linguistic knowledge within a
linguistically-motivated evaluation context. Two text augmentation programs,
REDA and REDA$_{NG}$, were developed, both implementing five token-level text
editing operations: Synonym Replacement (SR), Random Swap (RS), Random
Insertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$
leverages pretrained $n$-gram language models to select the most likely
augmented texts from REDA's output. Comprehensive and fine-grained experiments
were conducted on a binary question matching classification task in both
Chinese and English. The results strongly refute the general effectiveness of
the five token-level text augmentation techniques under investigation, whether
applied together or separately, and irrespective of various common
classification model types used, including transformers. Furthermore, the role
of probabilistic linguistic knowledge is found to be minimal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">weighted CapsuleNet networks for Persian multi-domain sentiment analysis. (arXiv:2306.17068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17068">
<div class="article-summary-box-inner">
<span><p>Sentiment classification is a fundamental task in natural language
processing, assigning one of the three classes, positive, negative, or neutral,
to free texts. However, sentiment classification models are highly domain
dependent; the classifier may perform classification with reasonable accuracy
in one domain but not in another due to the Semantic multiplicity of words
getting poor accuracy. This article presents a new Persian/Arabic multi-domain
sentiment analysis method using the cumulative weighted capsule networks
approach. Weighted capsule ensemble consists of training separate capsule
networks for each domain and a weighting measure called domain belonging degree
(DBD). This criterion consists of TF and IDF, which calculates the dependency
of each document for each domain separately; this value is multiplied by the
possible output that each capsule creates. In the end, the sum of these
multiplications is the title of the final output, and is used to determine the
polarity. And the most dependent domain is considered the final output for each
domain. The proposed method was evaluated using the Digikala dataset and
obtained acceptable accuracy compared to the existing approaches. It achieved
an accuracy of 0.89 on detecting the domain of belonging and 0.99 on detecting
the polarity. Also, for the problem of dealing with unbalanced classes, a
cost-sensitive function was used. This function was able to achieve 0.0162
improvements in accuracy for sentiment classification. This approach on Amazon
Arabic data can achieve 0.9695 accuracies in domain classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17408">
<div class="article-summary-box-inner">
<span><p>As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Multi-task Learning Framework for Chinese Text Error Correction. (arXiv:2306.17447v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17447">
<div class="article-summary-box-inner">
<span><p>Chinese Text Error Correction (CTEC) aims to detect and correct errors in the
input text, which benefits human's daily life and various downstream tasks.
Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve
CTEC task and achieve tremendous success. However, previous approaches suffer
from issues of over-correction and under-correction, and the former is
especially conspicuous in the precision-critical CTEC task. To mitigate the
issue of overcorrection, we propose a novel model-agnostic progressive
multitask learning framework for CTEC, named ProTEC, which guides a CTEC model
to learn the task from easy to difficult. We divide CTEC task into three
sub-tasks from easy to difficult: Error Detection, Error Type Identification,
and Correction Result Generation. During the training process, ProTEC guides
the model to learn text error correction progressively by incorporating these
sub-tasks into a multi-task training objective. During the inference process,
the model completes these sub-tasks in turn to generate the correction results.
Extensive experiments and detailed analyses fully demonstrate the effectiveness
and efficiency of our proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17840">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) provide a promising tool that enable robots to
perform complex robot reasoning tasks. However, the limited context window of
contemporary LLMs makes reasoning over long time horizons difficult. Embodied
tasks such as those that one might expect a household robot to perform
typically require that the planner consider information acquired a long time
ago (e.g., properties of the many objects that the robot previously encountered
in the environment). Attempts to capture the world state using an LLM's
implicit internal representation is complicated by the paucity of task- and
environment-relevant information available in a robot's action history, while
methods that rely on the ability to convey information via the prompt to the
LLM are subject to its limited context window. In this paper, we propose
Statler, a framework that endows LLMs with an explicit representation of the
world state as a form of ``memory'' that is maintained over time. Integral to
Statler is its use of two instances of general LLMs -- a world-model reader and
a world-model writer -- that interface with and maintain the world state. By
providing access to this world state ``memory'', Statler improves the ability
of existing LLMs to reason over longer time horizons without the constraint of
context length. We evaluate the effectiveness of our approach on three
simulated table-top manipulation domains and a real robot domain, and show that
it improves the state-of-the-art in LLM-based robot reasoning. Project website:
https://statler-lm.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17842">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling
frozen LLMs to perform both understanding and generation tasks involving
non-linguistic modalities such as images or videos. SPAE converts between raw
pixels and interpretable lexical tokens (or words) extracted from the LLM's
vocabulary. The resulting tokens capture both the semantic meaning and the
fine-grained details needed for visual reconstruction, effectively translating
the visual content into a language comprehensible to the LLM, and empowering it
to perform a wide array of multimodal tasks. Our approach is validated through
in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set
of image understanding and generation tasks. Our method marks the first
successful attempt to enable a frozen LLM to generate image content while
surpassing state-of-the-art performance in image understanding tasks, under the
same setting, by over 25%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-07-04 23:13:11.091546751 UTC">2023-07-04 23:13:11 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>