<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-24T01:30:00Z">10-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11498">
<div class="article-summary-box-inner">
<span><p>Traditional (fickle) adversarial examples involve finding a small
perturbation that does not change an input's true label but confuses the
classifier into outputting a different prediction. Conversely, obstinate
adversarial examples occur when an adversary finds a small perturbation that
preserves the classifier's prediction but changes the true label of an input.
Adversarial training and certified robust training have shown some
effectiveness in improving the robustness of machine learnt models to fickle
adversarial examples. We show that standard adversarial training methods
focused on reducing vulnerability to fickle adversarial examples may make a
model more vulnerable to obstinate adversarial examples, with experiments for
both natural language inference and paraphrase identification tasks. To counter
this phenomenon, we introduce Balanced Adversarial Training, which incorporates
contrastive learning to increase robustness against both fickle and obstinate
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11512">
<div class="article-summary-box-inner">
<span><p>We compare the 0-shot performance of a neural caption-based image retriever
when given as input either human-produced captions or captions generated by a
neural captioner. We conduct this comparison on the recently introduced
ImageCoDe data-set \citep{Krojer:etal:2022}, which contains hard distractors
nearly identical to the images to be retrieved. We find that the neural
retriever has much higher performance when fed neural rather than human
captions, despite the fact that the former, unlike the latter, were generated
without awareness of the distractors that make the task hard. Even more
remarkably, when the same neural captions are given to human subjects, their
retrieval performance is almost at chance level. Our results thus add to the
growing body of evidence that, even when the ``language'' of neural models
resembles English, this superficial resemblance might be deeply misleading.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Text Deidentification. (arXiv:2210.11528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11528">
<div class="article-summary-box-inner">
<span><p>Deidentification seeks to anonymize textual data prior to distribution.
Automatic deidentification primarily uses supervised named entity recognition
from human-labeled data points. We propose an unsupervised deidentification
method that masks words that leak personally-identifying information. The
approach utilizes a specially trained reidentification model to identify
individuals from redacted personal documents. Motivated by K-anonymity based
privacy, we generate redactions that ensure a minimum reidentification rank for
the correct profile of the document. To evaluate this approach, we consider the
task of deidentifying Wikipedia Biographies, and evaluate using an adversarial
reidentification metric. Compared to a set of unsupervised baselines, our
approach deidentifies documents more completely while removing fewer words.
Qualitatively, we see that the approach eliminates many identifying aspects
that would fall outside of the common named entity based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONSISTENT: Open-Ended Question Generation From News Articles. (arXiv:2210.11536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11536">
<div class="article-summary-box-inner">
<span><p>Recent work on question generation has largely focused on factoid questions
such as who, what, where, when about basic facts. Generating open-ended why,
how, what, etc. questions that require long-form answers have proven more
difficult. To facilitate the generation of open-ended questions, we propose
CONSISTENT, a new end-to-end system for generating open-ended questions that
are answerable from and faithful to the input text. Using news articles as a
trustworthy foundation for experimentation, we demonstrate our model's strength
over several baselines using both automatic and human=based evaluations. We
contribute an evaluation dataset of expert-generated open-ended questions.We
discuss potential downstream applications for news media organizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Dataset Shortcuts with Grammar Induction. (arXiv:2210.11560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11560">
<div class="article-summary-box-inner">
<span><p>Many NLP datasets have been found to contain shortcuts: simple decision rules
that achieve surprisingly high accuracy. However, it is difficult to discover
shortcuts automatically. Prior work on automatic shortcut detection has focused
on enumerating features like unigrams or bigrams, which can find only low-level
shortcuts, or relied on post-hoc model interpretability methods like saliency
maps, which reveal qualitative patterns without a clear statistical
interpretation. In this work, we propose to use probabilistic grammars to
characterize and discover shortcuts in NLP datasets. Specifically, we use a
context-free grammar to model patterns in sentence classification datasets and
use a synchronous context-free grammar to model datasets involving sentence
pairs. The resulting grammars reveal interesting shortcut features in a number
of datasets, including both simple and high-level features, and automatically
identify groups of test examples on which conventional classifiers fail.
Finally, we show that the features we discover can be used to generate
diagnostic contrast examples and incorporated into standard robust optimization
methods to improve worst-group accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Paraphrasing for Textual Enrichment. (arXiv:2210.11563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11563">
<div class="article-summary-box-inner">
<span><p>Understanding inferences and answering questions from text requires more than
merely recovering surface arguments, adjuncts, or strings associated with the
query terms. As humans, we interpret sentences as contextualized components of
a narrative or discourse, by both filling in missing information, and reasoning
about event consequences. In this paper, we define the process of rewriting a
textual expression (lexeme or phrase) such that it reduces ambiguity while also
making explicit the underlying semantics that is not (necessarily) expressed in
the economy of sentence structure as Dense Paraphrasing (DP). We build the
first complete DP dataset, provide the scope and design of the annotation task,
and present results demonstrating how this DP process can enrich a source text
to improve inferencing and QA task performance. The data and the source code
will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Human Strategies for Generating Word-Level Adversarial Examples. (arXiv:2210.11598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11598">
<div class="article-summary-box-inner">
<span><p>Adversarial examples in NLP are receiving increasing research attention. One
line of investigation is the generation of word-level adversarial examples
against fine-tuned Transformer models that preserve naturalness and
grammaticality. Previous work found that human- and machine-generated
adversarial examples are comparable in their naturalness and grammatical
correctness. Most notably, humans were able to generate adversarial examples
much more effortlessly than automated attacks. In this paper, we provide a
detailed analysis of exactly how humans create these adversarial examples. By
exploring the behavioural patterns of human workers during the generation
process, we identify statistically significant tendencies based on which words
humans prefer to select for adversarial replacement (e.g., word frequencies,
word saliencies, sentiment) as well as where and when words are replaced in an
input sequence. With our findings, we seek to inspire efforts that harness
human strategies for more robust NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VolcTrans System for WMT22 Multilingual Machine Translation Task. (arXiv:2210.11599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11599">
<div class="article-summary-box-inner">
<span><p>This report describes our VolcTrans system for the WMT22 shared task on
large-scale multilingual machine translation. We participated in the
unconstrained track which allows the use of external resources. Our system is a
transformerbased multilingual model trained on data from multiple sources
including the public training set from the data track, NLLB data provided by
Meta AI, self-collected parallel corpora, and pseudo bitext from
back-translation. A series of heuristic rules clean both bilingual and
monolingual texts. On the official test set, our system achieves 17.3 BLEU,
21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. The average
inference speed is 11.5 sentences per second using a single Nvidia Tesla V100
GPU. Our code and trained models are available at
https://github.com/xian8/wmt22
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tag-Set-Sequence Learning for Generating Question-Answer Pairs. (arXiv:2210.11608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11608">
<div class="article-summary-box-inner">
<span><p>Transformer-based QG models can generate question-answer pairs (QAPs) with
high qualities, but may also generate silly questions for certain texts. We
present a new method called tag-set sequence learning to tackle this problem,
where a tag-set sequence is a sequence of tag sets to capture the syntactic and
semantic information of the underlying sentence, and a tag set consists of one
or more language feature tags, including, for example, semantic-role-labeling,
part-of-speech, named-entity-recognition, and sentiment-indication tags. We
construct a system called TSS-Learner to learn tag-set sequences from given
declarative sentences and the corresponding interrogative sentences, and derive
answers to the latter. We train a TSS-Learner model for the English language
using a small training dataset and show that it can indeed generate adequate
QAPs for certain texts that transformer-based models do poorly. Human
evaluation on the QAPs generated by TSS-Learner over SAT practice reading tests
is encouraging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Self-Improve. (arXiv:2210.11610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11610">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate "high-confidence" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and
63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for a higher power in the human evaluation of MT. (arXiv:2210.11612v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11612">
<div class="article-summary-box-inner">
<span><p>In MT evaluation, pairwise comparisons are conducted to identify the better
system. In conducting the comparison, the experimenter must allocate a budget
to collect Direct Assessment (DA) judgments. We provide a cost effective way to
spend the budget, but show that typical budget sizes often do not allow for
solid comparison. Taking the perspective that the basis of solid comparison is
in achieving statistical significance, we study the power (rate of achieving
significance) on a large collection of pairwise DA comparisons. Due to the
nature of statistical estimation, power is low for differentiating less than
1-2 DA points, and to achieve a notable increase in power requires at least
2-3x more samples. Applying variance reduction alone will not yield these
gains, so we must face the reality of undetectable differences and spending
increases. In this context, we propose interim testing, an "early stopping"
collection procedure that yields more power per judgment collected, which
adaptively focuses the budget on pairs that are borderline significant. Interim
testing can achieve up to a 27% efficiency gain when spending 3x the current
budget, or 18% savings at the current evaluation power.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Natural Language Generation from Instructions with Meta-Learning. (arXiv:2210.11617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11617">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that language models (LMs) trained with multi-task
\textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and
few-shot settings with improved performance compared to prompt tuning. MTIL
illustrates that LMs can extract and use information about the task from
instructions beyond the surface patterns of the inputs and outputs. This
suggests that meta-learning may further enhance the utilization of instructions
for effective task transfer. In this paper we investigate whether meta-learning
applied to MTIL can further improve generalization to unseen tasks in a
zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in
three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network
(HNet) based adaptation to generate task specific parameters conditioned on
instructions, and 3) an approach combining HNet and MAML. Through extensive
experiments on the large scale Natural Instructions V2 dataset, we show that
our proposed approaches significantly improve over strong baselines in
zero-shot settings. In particular, meta-learning improves the effectiveness of
instructions and is most impactful when the test tasks are strictly zero-shot
(i.e. no similar tasks in the training set) and are "hard" for LMs,
illustrating the potential of meta-learning for MTIL for out-of-distribution
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve. (arXiv:2210.11618v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11618">
<div class="article-summary-box-inner">
<span><p>We find a surprising connection between multitask learning and robustness to
neuron failures. Our experiments show that bilingual language models retain
higher performance under various neuron perturbations, such as random
deletions, magnitude pruning and weight noise compared to equivalent
monolingual ones. We provide a theoretical justification for this robustness by
mathematically analyzing linear representation learning and showing that
multitasking creates more robust representations. Our analysis connects
robustness to spectral properties of the learned representation and proves that
multitasking leads to higher robustness for diverse task vectors. We
open-source our code and models:
https://github.com/giannisdaras/multilingual_robustness
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages. (arXiv:2210.11621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11621">
<div class="article-summary-box-inner">
<span><p>In recent years, multilingual machine translation models have achieved
promising performance on low-resource language pairs by sharing information
between similar languages, thus enabling zero-shot translation. To overcome the
"curse of multilinguality", these models often opt for scaling up the number of
parameters, which makes their use in resource-constrained environments
challenging. We introduce SMaLL-100, a distilled version of the M2M-100 (12B)
model, a massively multilingual machine translation model covering 100
languages. We train SMaLL-100 with uniform sampling across all language pairs
and therefore focus on preserving the performance of low-resource languages. We
evaluate SMaLL-100 on different low-resource benchmarks: FLORES-101, Tatoeba,
and TICO-19 and demonstrate that it outperforms previous massively multilingual
models of comparable sizes (200-600M) while improving inference latency and
memory usage. Additionally, our model achieves comparable results to M2M-100
(1.2B), while being 3.6x smaller and 4.3x faster at inference. Code and
pre-trained models: https://github.com/alirezamshi/small100
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Domains Be Transferred Across Languages in Multi-Domain Multilingual Neural Machine Translation?. (arXiv:2210.11628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11628">
<div class="article-summary-box-inner">
<span><p>Previous works mostly focus on either multilingual or multi-domain aspects of
neural machine translation (NMT). This paper investigates whether the domain
information can be transferred across languages on the composition of
multi-domain and multilingual NMT, particularly for the incomplete data
condition where in-domain bitext is missing for some language pairs. Our
results in the curated leave-one-domain-out experiments show that multi-domain
multilingual (MDML) NMT can boost zero-shot translation performance up to +10
gains on BLEU, as well as aid the generalisation of multi-domain NMT to the
missing domain. We also explore strategies for effective integration of
multilingual and multi-domain NMT, including language and domain tag
combination and auxiliary task training. We find that learning domain-aware
representations and adding target-language tags to the encoder leads to
effective MDML-NMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Models to Enhance Programming Error Messages. (arXiv:2210.11630v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11630">
<div class="article-summary-box-inner">
<span><p>A key part of learning to program is learning to understand programming error
messages. They can be hard to interpret and identifying the cause of errors can
be time-consuming. One factor in this challenge is that the messages are
typically intended for an audience that already knows how to program, or even
for programming environments that then use the information to highlight areas
in code. Researchers have been working on making these errors more novice
friendly since the 1960s, however progress has been slow. The present work
contributes to this stream of research by using large language models to
enhance programming error messages with explanations of the errors and
suggestions on how to fix the error. Large language models can be used to
create useful and novice-friendly enhancements to programming error messages
that sometimes surpass the original programming error messages in
interpretability and actionability. These results provide further evidence of
the benefits of large language models for computing educators, highlighting
their use in areas known to be challenging for students. We further discuss the
benefits and downsides of large language models and highlight future streams of
research for enhancing programming error messages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Semi-supervised End-to-end Automatic Speech Recognition using CycleGAN and Inter-domain Losses. (arXiv:2210.11642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11642">
<div class="article-summary-box-inner">
<span><p>We propose a novel method that combines CycleGAN and inter-domain losses for
semi-supervised end-to-end automatic speech recognition. Inter-domain loss
targets the extraction of an intermediate shared representation of speech and
text inputs using a shared network. CycleGAN uses cycle-consistent loss and the
identity mapping loss to preserve relevant characteristics of the input feature
after converting from one domain to another. As such, both approaches are
suitable to train end-to-end models on unpaired speech-text inputs. In this
paper, we exploit the advantages from both inter-domain loss and CycleGAN to
achieve better shared representation of unpaired speech and text inputs and
thus improve the speech-to-text mapping. Our experimental results on the WSJ
eval92 and Voxforge (non English) show 8~8.5% character error rate reduction
over the baseline, and the results on LibriSpeech test_clean also show
noticeable improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models. (arXiv:2210.11670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11670">
<div class="article-summary-box-inner">
<span><p>This paper describes the Stevens Institute of Technology's submission for the
WMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task
consisted of two subtasks, subtask $1$ Hindi/English to Hinglish and subtask
$2$ Hinglish to English translation. Our findings lie in the improvements made
through the use of large pre-trained multilingual NMT models and in-domain
datasets, as well as back-translation and ensemble techniques. The translation
output is automatically evaluated against the reference translations using
ROUGE-L and WER. Our system achieves the $1^{st}$ position on subtask $2$
according to ROUGE-L, WER, and human evaluation, $1^{st}$ position on subtask
$1$ according to WER and human evaluation, and $3^{rd}$ position on subtask $1$
with respect to ROUGE-L metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLING: Sino Linguistic Evaluation of Large Language Models. (arXiv:2210.11689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11689">
<div class="article-summary-box-inner">
<span><p>To understand what kinds of linguistic knowledge are encoded by pretrained
Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics
(SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese
grouped into 9 high-level linguistic phenomena. Each pair demonstrates the
acceptability contrast of a specific syntactic or semantic phenomenon (e.g.,
The keys are lost vs. The keys is lost), and an LM should assign lower
perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang
et al., 2021), which also contains Chinese minimal pairs and was created by
translating the vocabulary of the English BLiMP dataset, the minimal pairs in
SLING are derived primarily by applying syntactic and lexical transformations
to naturally-occurring, linguist-annotated sentences from the Chinese Treebank
9.0, thus addressing severe issues in CLiMP's data generation process. We test
18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and
multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show
that the average accuracy for LMs is far below human performance (69.7% vs.
97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested
LMs, even much larger ones. Additionally, we find that most LMs have a strong
gender and number (singular/plural) bias, and they perform better on local
phenomena than hierarchical ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem. (arXiv:2210.11694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11694">
<div class="article-summary-box-inner">
<span><p>Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Tuned Parameters are Task Embeddings. (arXiv:2210.11705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11705">
<div class="article-summary-box-inner">
<span><p>Intermediate-task transfer can benefit a wide range of NLP tasks with
properly selected source datasets. However, it is computationally infeasible to
experiment with all intermediate transfer combinations, making choosing a
useful source task a challenging problem. In this paper, we anticipate that
task-specific parameters updated in parameter-efficient tuning methods are
likely to encode task-specific information. Therefore, such parameters can be
predictive for inter-task transferability. Thus, we propose to exploit these
efficiently tuned parameters as off-the-shelf task embeddings for the efficient
selection of source datasets for intermediate-task transfer. We experiment with
11 text classification tasks and 11 question answering tasks. Experimental
results show that our approach can consistently outperform existing inter-task
transferability prediction methods while being conceptually simple and
computationally efficient. Our analysis also reveals that the ability of
efficiently tuned parameters on transferability prediction is disentangled with
their in-task performance. This allows us to use parameters from early
checkpoints as task embeddings to further improve efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning. (arXiv:2210.11708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11708">
<div class="article-summary-box-inner">
<span><p>Commonsense generation aims to generate a realistic sentence describing a
daily scene under the given concepts, which is very challenging, since it
requires models to have relational reasoning and compositional generalization
capabilities. Previous work focuses on retrieving prototype sentences for the
provided concepts to assist generation. They first use a sparse retriever to
retrieve candidate sentences, then re-rank the candidates with a ranker.
However, the candidates returned by their ranker may not be the most relevant
sentences, since the ranker treats all candidates equally without considering
their relevance to the reference sentences of the given concepts. Another
problem is that re-ranking is very expensive, but only using retrievers will
seriously degrade the performance of their generation models. To solve these
problems, we propose the metric distillation rule to distill knowledge from the
metric (e.g., BLEU) to the ranker. We further transfer the critical knowledge
summarized by the distilled ranker to the retriever. In this way, the relevance
scores of candidate sentences predicted by the ranker and retriever will be
more consistent with their quality measured by the metric. Experimental results
on the CommonGen benchmark verify the effectiveness of our proposed method: (1)
Our generation model with the distilled ranker achieves a new state-of-the-art
result. (2) Our generation model with the distilled retriever even surpasses
the previous SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling Multi-relations for Convolutional-based Knowledge Graph Embedding. (arXiv:2210.11711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11711">
<div class="article-summary-box-inner">
<span><p>Representation learning of knowledge graphs aims to embed entities and
relations into low-dimensional vectors. Most existing works only consider the
direct relations or paths between an entity pair. It is considered that such
approaches disconnect the semantic connection of multi-relations between an
entity pair, and we propose a convolutional and multi-relational representation
learning model, ConvMR. The proposed ConvMR model addresses the multi-relation
issue in two aspects: (1) Encoding the multi-relations between an entity pair
into a unified vector that maintains the semantic connection. (2) Since not all
relations are necessary while joining multi-relations, we propose an
attention-based relation encoder to automatically assign weights to different
relations based on semantic hierarchy. Experimental results on two popular
datasets, FB15k-237 and WN18RR, achieved consistent improvements on the mean
rank. We also found that ConvMR is efficient to deal with less frequent
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design a Sustainable Micro-mobility Future: Trends and Challenges in the United States and European Union Using Natural Language Processing Techniques. (arXiv:2210.11714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11714">
<div class="article-summary-box-inner">
<span><p>Micro-mobility devices are rapidly gaining popularity since people could
benefit from their efficiency, low cost and sustainability. However, people
still face challenges that detain the development and full integration of these
devices. In the present study, we examined people's opinions and experiences
about micro-mobility in the US and the EU using social media data on Twitter.
We made use of topic modeling based on advanced natural language processing
techniques and categorized the data into seven topics: promotion and service,
mobility, technical features, acceptance, recreation, infrastructure and
regulations. Furthermore, using sentiment analysis, we investigated people's
positive and negative attitudes towards specific aspects of these topics and
compared the patterns of the trends and challenges in the US and the EU. We
found that 1) promotion and service included the majority of Twitter
discussions in the both regions, 2) the EU had more positive opinions than the
US, 3) micro-mobility devices were more widely used for utilitarian mobility
and recreational purposes in the EU than in the US, and 4) compared to the EU,
people in the US had many more concerns related to infrastructure and
regulation issues. These findings help us understand the trends and challenges
and prioritize different aspects in micro-mobility to improve their safety and
experience across the two areas for designing a more sustainable micro-mobility
future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11715">
<div class="article-summary-box-inner">
<span><p>Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCSCSet: A Specialist-annotated Dataset for Medical-domain Chinese Spelling Correction. (arXiv:2210.11720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11720">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Correction (CSC) is gaining increasing attention due to its
promise of automatically detecting and correcting spelling errors in Chinese
texts. Despite its extensive use in many applications, like search engines and
optical character recognition systems, little has been explored in medical
scenarios in which complex and uncommon medical entities are easily misspelled.
Correcting the misspellings of medical entities is arguably more difficult than
those in the open domain due to its requirements of specificdomain knowledge.
In this work, we define the task of Medical-domain Chinese Spelling Correction
and propose MCSCSet, a large scale specialist-annotated dataset that contains
about 200k samples. In contrast to the existing open-domain CSC datasets,
MCSCSet involves: i) extensive real-world medical queries collected from
Tencent Yidian, ii) corresponding misspelled sentences manually annotated by
medical specialists. To ensure automated dataset curation, MCSCSet further
offers a medical confusion set consisting of the commonly misspelled characters
of given Chinese medical terms. This enables one to create the medical
misspelling dataset automatically. Extensive empirical studies have shown
significant performance gaps between the open-domain and medical-domain
spelling correction, highlighting the need to develop high-quality datasets
that allow for Chinese spelling correction in specific domains. Moreover, our
work benchmarks several representative Chinese spelling correction models,
establishing baselines for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Data Efficiency in Intra-Dataset Task Transfer for Dialog Understanding. (arXiv:2210.11729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11729">
<div class="article-summary-box-inner">
<span><p>Transfer learning is an exciting area of Natural Language Processing that has
the potential to both improve model performance and increase data efficiency.
This study explores the effects of varying quantities of target task training
data on sequential transfer learning in the dialog domain. We hypothesize that
a model can utilize the information learned from a source task to better learn
a target task, thereby reducing the number of target task training samples
required. Unintuitively, our data shows that often target task training data
size has minimal effect on how sequential transfer learning performs compared
to the same model without transfer learning. Our results lead us to believe
that this unexpected result could be due to the effects of catastrophic
forgetting, motivating further work into methods that prevent such forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroLID: A Neural Language Identification Tool for African Languages. (arXiv:2210.11744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11744">
<div class="article-summary-box-inner">
<span><p>Language identification (LID) is a crucial precursor for NLP, especially for
mining web data. Problematically, most of the world's $7000$+ languages today
are not covered by LID technologies. We address this pressing issue for Africa
by introducing~\ourLID, a neural LID toolkit for $517$ African languages and
varieties.~\ourLID~exploits a multi-domain web dataset manually curated from
across $14$ language families utilizing five orthographic systems. When
evaluated on our blind Test set,~\ourLID~achieves $95.89$ $F_1$-score. We also
compare~\ourLID~to five existing LID tools that each cover a small number of
African languages, finding it to outperform them on most languages. We further
show the utility of~\ourLID~in the wild by testing it on the acutely
under-served Twitter domain. Finally, we offer a number of controlled case
studies and perform a linguistically-motivated error analysis that allow us to
both showcase~\ourLID's powerful capabilities and limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer. (arXiv:2210.11753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11753">
<div class="article-summary-box-inner">
<span><p>Sanskrit Word Segmentation (SWS) is essential in making digitized texts
available and in deploying downstream tasks. It is, however, non-trivial
because of the sandhi phenomenon that modifies the characters at the word
boundaries, and needs special treatment. Existing lexicon driven approaches for
SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to
generate the complete candidate solution space, over which various methods are
applied to produce the most valid solution. However, these approaches fail
while encountering out-of-vocabulary tokens. On the other hand, purely
engineering methods for SWS have made use of recent advances in deep learning,
but cannot make use of the latent word information on availability.
</p>
<p>To mitigate the shortcomings of both families of approaches, we propose
Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST)
consisting of (1) a module that encodes the character input along with
latent-word information, which takes into account the sandhi phenomenon
specific to SWS and is apt to work with partial or no candidate solutions, (2)
a novel soft-masked attention to prioritize potential candidate words and (3) a
novel path ranking algorithm to rectify the corrupted predictions. Experiments
on the benchmark datasets for SWS show that TransLIST outperforms the current
state-of-the-art system by an average 7.2 points absolute gain in terms of
perfect match (PM) metric. The codebase and datasets are publicly available at
https://github.com/rsingha108/TransLIST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">University of Cape Town's WMT22 System: Multilingual Machine Translation for Southern African Languages. (arXiv:2210.11757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11757">
<div class="article-summary-box-inner">
<span><p>The paper describes the University of Cape Town's submission to the
constrained track of the WMT22 Shared Task: Large-Scale Machine Translation
Evaluation for African Languages. Our system is a single multilingual
translation model that translates between English and 8 South / South East
African Languages, as well as between specific pairs of the African languages.
We used several techniques suited for low-resource machine translation (MT),
including overlap BPE, back-translation, synthetic training data generation,
and adding more translation directions during training. Our results show the
value of these techniques, especially for directions where very little or no
bilingual training data is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-guided Localized Self-attention by Constituency Syntactic Distance. (arXiv:2210.11759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11759">
<div class="article-summary-box-inner">
<span><p>Recent works have revealed that Transformers are implicitly learning the
syntactic information in its lower layers from data, albeit is highly dependent
on the quality and scale of the training data. However, learning syntactic
information from data is not necessary if we can leverage an external syntactic
parser, which provides better parsing quality with well-defined syntactic
structures. This could potentially improve Transformer's performance and sample
efficiency. In this work, we propose a syntax-guided localized self-attention
for Transformer that allows directly incorporating grammar structures from an
external constituency parser. It prohibits the attention mechanism to
overweight the grammatically distant tokens over close ones. Experimental
results show that our model could consistently improve translation performance
on a variety of machine translation datasets, ranging from small to large
dataset sizes, and with different source languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Unintended Social Bias in Toxic Language Datasets. (arXiv:2210.11762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11762">
<div class="article-summary-box-inner">
<span><p>With the rise of online hate speech, automatic detection of Hate Speech,
Offensive texts as a natural language processing task is getting popular.
However, very little research has been done to detect unintended social bias
from these toxic language datasets. This paper introduces a new dataset
ToxicBias curated from the existing dataset of Kaggle competition named "Jigsaw
Unintended Bias in Toxicity Classification". We aim to detect social biases,
their categories, and targeted groups. The dataset contains instances annotated
for five different bias categories, viz., gender, race/ethnicity, religion,
political, and LGBTQ. We train transformer-based models using our curated
datasets and report baseline performance for bias identification, target
generation, and bias implications. Model biases and their mitigation are also
discussed in detail. Our study motivates a systematic extraction of social bias
data from toxic language datasets. All the codes and dataset used for
experiments in this work are publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEFR-Based Sentence Difficulty Annotation and Assessment. (arXiv:2210.11766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11766">
<div class="article-summary-box-inner">
<span><p>Controllable text simplification is a crucial assistive technique for
language learning and teaching. One of the primary factors hindering its
advancement is the lack of a corpus annotated with sentence difficulty levels
based on language ability descriptions. To address this problem, we created the
CEFR-based Sentence Profile (CEFR-SP) corpus, containing 17k English sentences
annotated with the levels based on the Common European Framework of Reference
for Languages assigned by English-education professionals. In addition, we
propose a sentence-level assessment model to handle unbalanced level
distribution because the most basic and highly proficient sentences are
naturally scarce. In the experiments in this study, our method achieved a
macro-F1 score of 84.5% in the level assessment, thus outperforming strong
baselines employed in readability assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11768">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is one of the primary methods of transferring
knowledge from large to small models. However, it requires massive
task-specific data, which may not be plausible in many real-world applications.
Data augmentation methods such as representation interpolation, token
replacement, or augmentation with models are applied to tackle this problem.
However, these data augmentation methods either potentially cause shifts in
decision boundaries (representation interpolation), are not expressive enough
(token replacement), or introduce too much computational overhead (augmentation
with models). To this end, we propose AugPro (Augmentation with Projection), an
effective and efficient data augmentation method for distillation. Our method
builds on top of representation interpolation augmentation methods to maintain
the diversity of expressions and converts the augmented data to tokens to avoid
shifting decision boundaries. It uses simple operations that come with little
computational overhead. The results on multiple GLUE tasks show that our
methods can improve distillation performance by a large margin at a low time
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InforMask: Unsupervised Informative Masking for Language Model Pretraining. (arXiv:2210.11771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11771">
<div class="article-summary-box-inner">
<span><p>Masked language modeling is widely used for pretraining large language models
for natural language understanding (NLU). However, random masking is
suboptimal, allocating an equal masking rate for all tokens. In this paper, we
propose InforMask, a new unsupervised masking strategy for training masked
language models. InforMask exploits Pointwise Mutual Information (PMI) to
select the most informative tokens to mask. We further propose two
optimizations for InforMask to improve its efficiency. With a one-off
preprocessing step, InforMask outperforms random masking and previously
proposed masking strategies on the factual recall benchmark LAMA and the
question answering benchmark SQuAD v1 and v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval. (arXiv:2210.11773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11773">
<div class="article-summary-box-inner">
<span><p>Sampling proper negatives from a large document pool is vital to effectively
train a dense retrieval model. However, existing negative sampling strategies
suffer from the uninformative or false negative problem. In this work, we
empirically show that according to the measured relevance scores, the negatives
ranked around the positives are generally more informative and less likely to
be false negatives. Intuitively, these negatives are not too hard (\emph{may be
false negatives}) or too easy (\emph{uninformative}). They are the ambiguous
negatives and need more attention during training. Thus, we propose a simple
ambiguous negatives sampling method, SimANS, which incorporates a new sampling
probability distribution to focusing on sampling more ambiguous negatives.
Extensive experiments on four public and one industry datasets show the
effectiveness of our approach. Our code and data are publicly available at the
link: \url{https://github.com/microsoft/SimXNS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Evaluating Faithfulness in Dialogue Summarization. (arXiv:2210.11777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11777">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization is abstractive in nature, making it suffer from
factual errors. The factual correctness of summaries has the highest priority
before practical applications. Many efforts have been made to improve
faithfulness in text summarization. However, there is a lack of systematic
study on dialogue summarization systems. In this work, we first perform the
fine-grained human analysis on the faithfulness of dialogue summaries and
observe that over 35% of generated summaries are faithfully inconsistent
respective the source dialogues. Furthermore, we present a new model-level
faithfulness evaluation method. It examines generation models with multi-choice
questions created by rule-based transformations. Experimental results show that
our evaluation schema is a strong proxy for the factual correctness of
summarization models. The human-annotated faithfulness samples and the
evaluation toolkit are released to facilitate future research toward faithful
dialogue summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs. (arXiv:2210.11787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11787">
<div class="article-summary-box-inner">
<span><p>We propose to leverage news discourse profiling to model document-level
temporal structures for building temporal dependency graphs. Our key
observation is that the functional roles of sentences used for profiling news
discourse signify different time frames relevant to a news story and can,
therefore, help to recover the global temporal structure of a document. Our
analyses and experiments with the widely used knowledge distillation technique
show that discourse profiling effectively identifies distant inter-sentence
event and (or) time expression pairs that are temporally related and otherwise
difficult to locate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences. (arXiv:2210.11794v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11794">
<div class="article-summary-box-inner">
<span><p>Efficient Transformers have been developed for long sequence modeling, due to
their subquadratic memory and time complexity. Sparse Transformer is a popular
approach to improving the efficiency of Transformers by restricting
self-attention to locations specified by the predefined sparse patterns.
However, leveraging sparsity may sacrifice expressiveness compared to
full-attention, when important token correlations are multiple hops away. To
combine advantages of both the efficiency of sparse transformer and the
expressiveness of full-attention Transformer, we propose \textit{Diffuser}, a
new state-of-the-art efficient Transformer. Diffuser incorporates all token
interactions within one attention layer while maintaining low computation and
memory costs. The key idea is to expand the receptive field of sparse attention
using Attention Diffusion, which computes multi-hop token correlations based on
all paths between corresponding disconnected tokens, besides attention among
neighboring tokens. Theoretically, we show the expressiveness of Diffuser as a
universal sequence approximator for sequence-to-sequence modeling, and
investigate its ability to approximate full-attention by analyzing the graph
expander property from the spectral perspective. Experimentally, we investigate
the effectiveness of Diffuser with extensive evaluations, including language
modeling, image modeling, and Long Range Arena (LRA). Evaluation results show
that Diffuser achieves improvements by an average of 0.94% on text
classification tasks and 2.30% on LRA, with 1.67$\times$ memory savings
compared to state-of-the-art benchmarks, which demonstrates superior
performance of Diffuser in both expressiveness and efficiency aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction. (arXiv:2210.11800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11800">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) has achieved remarkable progress with the help of
pre-trained language models. However, existing RE models are usually incapable
of handling two situations: implicit expressions and long-tail relation types,
caused by language complexity and data sparsity. In this paper, we introduce a
simple enhancement of RE using $k$ nearest neighbors ($k$NN-RE). $k$NN-RE
allows the model to consult training relations at test time through a
nearest-neighbor search and provides a simple yet effective means to tackle the
two issues above. Additionally, we observe that $k$NN-RE serves as an effective
way to leverage distant supervision (DS) data for RE. Experimental results show
that the proposed $k$NN-RE achieves state-of-the-art performances on a variety
of supervised RE datasets, i.e., ACE05, SciERC, and Wiki80, along with
outperforming the best model to date on the i2b2 and Wiki80 datasets in the
setting of allowing using DS. Our code and models are available at:
https://github.com/YukinoWan/kNN-RE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Checkpoint Averaging for Neural Machine Translation. (arXiv:2210.11803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11803">
<div class="article-summary-box-inner">
<span><p>Checkpoint averaging is a simple and effective method to boost the
performance of converged neural machine translation models. The calculation is
cheap to perform and the fact that the translation improvement almost comes for
free, makes it widely adopted in neural machine translation research. Despite
the popularity, the method itself simply takes the mean of the model parameters
from several checkpoints, the selection of which is mostly based on empirical
recipes without many justifications. In this work, we revisit the concept of
checkpoint averaging and consider several extensions. Specifically, we
experiment with ideas such as using different checkpoint selection strategies,
calculating weighted average instead of simple mean, making use of gradient
information and fine-tuning the interpolation weights on development data. Our
results confirm the necessity of applying checkpoint averaging for optimal
performance, but also suggest that the landscape between the converged
checkpoints is rather flat and not much further improvement compared to simple
averaging is to be obtained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering New Intents Using Latent Variables. (arXiv:2210.11804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11804">
<div class="article-summary-box-inner">
<span><p>Discovering new intents is of great significance to establishing Bootstrapped
Task-Oriented Dialogue System. Most existing methods either lack the ability to
transfer prior knowledge in the known intent data or fall into the dilemma of
forgetting prior knowledge in the follow-up. More importantly, these methods do
not deeply explore the intrinsic structure of unlabeled data, so they can not
seek out the characteristics that make an intent in general. In this paper,
starting from the intuition that discovering intents could be beneficial to the
identification of the known intents, we propose a probabilistic framework for
discovering intents where intent assignments are treated as latent variables.
We adopt Expectation Maximization framework for optimization. Specifically, In
E-step, we conduct discovering intents and explore the intrinsic structure of
unlabeled data by the posterior of intent assignments. In M-step, we alleviate
the forgetting of prior knowledge transferred from known intents by optimizing
the discrimination of labeled data. Extensive experiments conducted in three
challenging real-world datasets demonstrate our method can achieve substantial
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals. (arXiv:2210.11805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11805">
<div class="article-summary-box-inner">
<span><p>For text classification tasks, finetuned language models perform remarkably
well. Yet, they tend to rely on spurious patterns in training data, thus
limiting their performance on out-of-distribution (OOD) test data. Among recent
models aiming to avoid this spurious pattern problem, adding extra
counterfactual samples to the training data has proven to be very effective.
Yet, counterfactual data generation is costly since it relies on human
annotation. Thus, we propose a novel solution that only requires annotation of
a small fraction (e.g., 1%) of the original training data, and uses automatic
generation of extra counterfactuals in an encoding vector space. We demonstrate
the effectiveness of our approach in sentiment classification, using IMDb data
for training and other sets for OOD tests (i.e., Amazon, SemEval and Yelp). We
achieve noticeable accuracy improvements by adding only 1% manual
counterfactuals: +3% compared to adding +100% in-distribution training samples,
+1.3% compared to alternate counterfactual approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Encoder-Decoder Redundant for Neural Machine Translation?. (arXiv:2210.11807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11807">
<div class="article-summary-box-inner">
<span><p>Encoder-decoder architecture is widely adopted for sequence-to-sequence
modeling tasks. For machine translation, despite the evolution from long
short-term memory networks to Transformer networks, plus the introduction and
development of attention mechanism, encoder-decoder is still the de facto
neural network architecture for state-of-the-art models. While the motivation
for decoding information from some hidden space is straightforward, the strict
separation of the encoding and decoding steps into an encoder and a decoder in
the model architecture is not necessarily a must. Compared to the task of
autoregressive language modeling in the target language, machine translation
simply has an additional source sentence as context. Given the fact that neural
language models nowadays can already handle rather long contexts in the target
language, it is natural to ask whether simply concatenating the source and
target sentences and training a language model to do translation would work. In
this work, we investigate the aforementioned concept for machine translation.
Specifically, we experiment with bilingual translation, translation with
additional target monolingual data, and multilingual translation. In all cases,
this alternative approach performs on par with the baseline encoder-decoder
Transformer, suggesting that an encoder-decoder architecture might be redundant
for neural machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Textless Metric for Speech-to-Speech Comparison. (arXiv:2210.11835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11835">
<div class="article-summary-box-inner">
<span><p>This paper proposes a textless speech-to-speech comparison metric that allows
comparing a speech hypothesis with a speech reference without falling-back to
their text transcripts. We leverage recently proposed speech2unit encoders
(such as HuBERT) to pseudo-transcribe the speech utterances into discrete
acoustic units and propose a simple neural architecture that learns a
speech-based metric which correlates well with its text-based counterpart. Such
a textless metric could ultimately be interesting for speech-to-speech
translation evaluation (for oral languages or languages with no reliable ASR
system available).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The use of the word "\{gamma}\u{psion}{\nu}{\alpha}{\iota}\k{appa}{\omicron}\k{appa}{\tau}{\omicron}{\nu}{\iota}{\alpha}" (femicide) in Greek-speaking Twitter. (arXiv:2210.11837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11837">
<div class="article-summary-box-inner">
<span><p>Between 2019 and 2022, Greek media attention has been attracted by a rather
unusually high number of femicide cases which have been trending for several
weeks up to months in the public debate and one of the contributing factors is
the feedback loop between traditional media and social media. In this paper we
are investigating the use of the term
"\{gamma}\u{psion}{\nu}{\alpha}{\iota}\k{appa}{\omicron}\k{appa}{\tau}{\omicron}{\nu}{\iota}{\alpha}"
(femicide) in Greek speaking twitter. More specifically, we approach the
problem from a stance detection perspective, aiming to automatically identify
user position with regards to the feministic semantics of the word. We also
discuss findings from an identity analysis perspective and intercorrelations
with hate speech that have been identified in the collected corpus of tweets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Probing. (arXiv:2210.11860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11860">
<div class="article-summary-box-inner">
<span><p>Linguistic information is encoded at varying timescales (subwords, phrases,
etc.) and communicative levels, such as syntax and semantics. Contextualized
embeddings have analogously been found to capture these phenomena at
distinctive layers and frequencies. Leveraging these findings, we develop a
fully learnable frequency filter to identify spectral profiles for any given
task. It enables vastly more granular analyses than prior handcrafted filters,
and improves on efficiency. After demonstrating the informativeness of spectral
probing over manual filters in a monolingual setting, we investigate its
multilingual characteristics across seven diverse NLP tasks in six languages.
Our analyses identify distinctive spectral profiles which quantify cross-task
similarity in a linguistically intuitive manner, while remaining consistent
across languages-highlighting their potential as robust, lightweight task
descriptors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LittleBird: Efficient Faster & Longer Transformer for Question Answering. (arXiv:2210.11870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11870">
<div class="article-summary-box-inner">
<span><p>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a
limitation dealing with long inputs due to its attention mechanism. Longformer,
ETC and BigBird addressed this issue and effectively solved the quadratic
dependency problem. However we find that these models are not sufficient, and
propose LittleBird, a novel model based on BigBird with improved speed and
memory footprint while maintaining accuracy. In particular, we devise a more
flexible and efficient position representation method based on Attention with
Linear Biases (ALiBi). We also show that replacing the method of global
information represented in the BigBird with pack and unpack attention is more
effective. The proposed model can work on long inputs even after being
pre-trained on short inputs, and can be trained efficiently reusing existing
pre-trained language model for short inputs. This is a significant benefit for
low-resource languages where large amounts of long text data are difficult to
obtain. As a result, our experiments show that LittleBird works very well in a
variety of languages, achieving high performance in question answering tasks,
particularly in KorQuAD2.0, Korean Question Answering Dataset for long
paragraphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer. (arXiv:2210.11885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11885">
<div class="article-summary-box-inner">
<span><p>In recent years, the standard hybrid DNN-HMM speech recognizers are
outperformed by the end-to-end speech recognition systems. One of the very
promising approaches is the grapheme Wav2Vec 2.0 model, which uses the
self-supervised pretraining approach combined with transfer learning of the
fine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and
language model, the approach is suitable for tasks where obtaining such models
is not easy or almost impossible.
</p>
<p>In this paper, we use the Wav2Vec speech recognizer in the task of spoken
term detection over a large set of spoken documents. The method employs a deep
LSTM network which maps the recognized hypothesis and the searched term into a
shared pronunciation embedding space in which the term occurrences and the
assigned scores are easily computed.
</p>
<p>The paper describes a bootstrapping approach that allows the transfer of the
knowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid
ASR into the context of grapheme-based Wav2Vec. The proposed method outperforms
the previously published system based on the combination of the DNN-HMM hybrid
ASR and phoneme recognizer by a large margin on the MALACH data in both English
and Czech languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing. (arXiv:2210.11888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11888">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel SQL guided pre-training framework STAR for
context-dependent text-to-SQL parsing, which leverages contextual information
to enrich natural language (NL) utterance and table schema representations for
text-to-SQL conversations. Concretely, we propose two novel pre-training
objectives which respectively explore the context-dependent interactions of NL
utterances and SQL queries within each text-to-SQL conversation: (i) schema
state tracking (SST) objective that tracks and explores the schema states of
context-dependent SQL queries in the form of schema-states by predicting and
updating the value of each schema slot during interaction; (ii) utterance
dependency tracking (UDT) objective that employs weighted contrastive learning
to pull together two semantically similar NL utterances and push away the
representations of semantically dissimilar NL utterances within each
conversation. In addition, we construct a high-quality large-scale
context-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive
experiments show that STAR achieves new state-of-the-art performance on two
downstream benchmarks (SParC and CoSQL), significantly outperforming previous
pre-training methods and ranking first on the leaderboard. We believe the
release of the constructed corpus, codebase and pre-trained STAR checkpoints
would push forward the research in this area. For reproducibility, we release
our code and data at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/star.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioLORD: Learning Ontological Representations from Definitions (for Biomedical Concepts and their Textual Descriptions). (arXiv:2210.11892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11892">
<div class="article-summary-box-inner">
<span><p>This work introduces BioLORD, a new pre-training strategy for producing
meaningful representations for clinical sentences and biomedical concepts.
State-of-the-art methodologies operate by maximizing the similarity in
representation of names referring to the same concept, and preventing collapse
through contrastive learning. However, because biomedical names are not always
self-explanatory, it sometimes results in non-semantic representations. BioLORD
overcomes this issue by grounding its concept representations using
definitions, as well as short descriptions derived from a multi-relational
knowledge graph consisting of biomedical ontologies. Thanks to this grounding,
our model produces more semantic concept representations that match more
closely the hierarchical structure of ontologies. BioLORD establishes a new
state of the art for text similarity on both clinical sentences (MedSTS) and
biomedical concepts (MayoSRS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken Term Detection and Relevance Score Estimation using Dot-Product of Pronunciation Embeddings. (arXiv:2210.11895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11895">
<div class="article-summary-box-inner">
<span><p>The paper describes a novel approach to Spoken Term Detection (STD) in large
spoken archives using deep LSTM networks. The work is based on the previous
approach of using Siamese neural networks for STD and naturally extends it to
directly localize a spoken term and estimate its relevance score. The phoneme
confusion network generated by a phoneme recognizer is processed by the deep
LSTM network which projects each segment of the confusion network into an
embedding space. The searched term is projected into the same embedding space
using another deep LSTM network. The relevance score is then computed using a
simple dot-product in the embedding space and calibrated using a sigmoid
function to predict the probability of occurrence. The location of the searched
term is then estimated from the sequence of output probabilities. The deep LSTM
networks are trained in a self-supervised manner from paired recognition
hypotheses on word and phoneme levels. The method is experimentally evaluated
on MALACH data in English and Czech languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT. (arXiv:2210.11899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11899">
<div class="article-summary-box-inner">
<span><p>In the online world, Machine Translation (MT) systems are extensively used to
translate User-Generated Text (UGT) such as reviews, tweets, and social media
posts, where the main message is often the author's positive or negative
attitude towards the topic of the text. However, MT systems still lack accuracy
in some low-resource languages and sometimes make critical translation errors
that completely flip the sentiment polarity of the target word or phrase and
hence delivers a wrong affect message. This is particularly noticeable in texts
that do not follow common lexico-grammatical standards such as the dialectical
Arabic (DA) used on online platforms. In this research, we aim to improve the
translation of sentiment in UGT written in the dialectical versions of the
Arabic language to English. Given the scarcity of gold-standard parallel data
for DA-EN in the UGT domain, we introduce a semi-supervised approach that
exploits both monolingual and parallel data for training an NMT system
initialised by a cross-lingual language model trained with supervised and
unsupervised modeling objectives. We assess the accuracy of sentiment
translation by our proposed system through a numerical 'sentiment-closeness'
measure as well as human evaluation. We will show that our semi-supervised MT
system can significantly help with correcting sentiment errors detected in the
online translation of dialectical Arabic UGT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation. (arXiv:2210.11900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11900">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) starts its translation before reading
the whole source sentence and employs either fixed or adaptive policy to
generate the target sentence. Compared to the fixed policy, the adaptive policy
achieves better latency-quality tradeoffs by adopting a flexible translation
policy. If the policy can evaluate rationality before taking action, the
probability of incorrect actions will also decrease. However, previous methods
lack evaluation of actions before taking them. In this paper, we propose a
method of performing the adaptive policy via integrating post-evaluation into
the fixed policy. Specifically, whenever a candidate token is generated, our
model will evaluate the rationality of the next action by measuring the change
in the source content. Our model will then take different actions based on the
evaluation results. Experiments on three translation tasks show that our method
can exceed strong baselines under all latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms. (arXiv:2210.11905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11905">
<div class="article-summary-box-inner">
<span><p>Prominent questions about the role of sensory vs. linguistic input in the way
we acquire and use language have been extensively studied in the
psycholinguistic literature. However, the relative effect of various factors in
a person's overall experience on their linguistic system remains unclear. We
study this question by making a step forward towards a better understanding of
the conceptual perception of colors by color-blind individuals, as reflected in
their spontaneous linguistic productions. Using a novel and carefully curated
dataset, we show that red-green color-blind speakers use the "red" and "green"
color terms in less predictable contexts, and in linguistic environments
evoking mental image to a lower extent, when compared to their normal-sighted
counterparts. These findings shed some new and interesting light on the role of
sensory experience on our linguistic system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$m^4Adapter$: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter. (arXiv:2210.11912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11912">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation models (MNMT) yield state-of-the-art
performance when evaluated on data from a domain and language pair seen at
training time. However, when a MNMT model is used to translate under domain
shift or to a new language pair, performance drops dramatically. We consider a
very challenging scenario: adapting the MNMT model both to a new domain and to
a new language pair at the same time. In this paper, we propose $m^4Adapter$
(Multilingual Multi-Domain Adaptation for Machine Translation with a
Meta-Adapter), which combines domain and language knowledge using meta-learning
with adapters. We present results showing that our approach is a
parameter-efficient solution which effectively adapts a model to both a new
language pair and a new domain, while outperforming other adapter methods. An
ablation study also shows that our approach more effectively transfers domain
knowledge across different languages and language information across different
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEREL-BIO: A Dataset of Biomedical Abstracts Annotated with Nested Named Entities. (arXiv:2210.11913v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11913">
<div class="article-summary-box-inner">
<span><p>This paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed
abstracts in Russian and smaller number of abstracts in English. NEREL-BIO
extends the general domain dataset NEREL by introducing domain-specific entity
types. NEREL-BIO annotation scheme covers both general and biomedical domains
making it suitable for domain transfer experiments. NEREL-BIO provides
annotation for nested named entities as an extension of the scheme employed for
NEREL. Nested named entities may cross entity boundaries to connect to shorter
entities nested within longer entities, making them harder to detect.
</p>
<p>NEREL-BIO contains annotations for 700+ Russian and 100+ English abstracts.
All English PubMed annotations have corresponding Russian counterparts. Thus,
NEREL-BIO comprises the following specific features: annotation of nested named
entities, it can be used as a benchmark for cross-domain (NEREL -&gt; NEREL-BIO)
and cross-language (English -&gt; Russian) transfer. We experiment with both
transformer-based sequence models and machine reading comprehension (MRC)
models and report their results.
</p>
<p>The dataset is freely available at https://github.com/nerel-ds/NEREL-BIO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling. (arXiv:2210.11929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11929">
<div class="article-summary-box-inner">
<span><p>Recent large-scale video-language pre-trained models have shown appealing
performance on various downstream tasks. However, the pre-training process is
computationally expensive due to the requirement of millions of video-text
pairs and the redundant data structure of each video. To mitigate these
problems, we propose LiteVL, which adapts a pre-trained image-language model
BLIP into a video-text model directly on downstream tasks, without heavy
pre-training. To enhance the temporal modeling lacking in the image-language
model, we propose to add temporal attention modules in the image encoder of
BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also
propose a non-parametric pooling mechanism to adaptively reweight the
fine-grained video embedding conditioned on the text. Experimental results on
text-video retrieval and video question answering show that the proposed LiteVL
even outperforms previous video-language pre-trained models by a clear margin,
though without any video-language pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing over Long Tail Concepts for Medical Term Normalization. (arXiv:2210.11947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11947">
<div class="article-summary-box-inner">
<span><p>Medical term normalization consists in mapping a piece of text to a large
number of output classes. Given the small size of the annotated datasets and
the extremely long tail distribution of the concepts, it is of utmost
importance to develop models that are capable to generalize to scarce or unseen
concepts. An important attribute of most target ontologies is their
hierarchical structure. In this paper we introduce a simple and effective
learning strategy that leverages such information to enhance the
generalizability of both discriminative and generative models. The evaluation
shows that the proposed strategy produces state-of-the-art performance on seen
concepts and consistent improvements on unseen ones, allowing also for
efficient zero-shot knowledge transfer across text typologies and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Detection and Injection for Direct Speech Translation. (arXiv:2210.11981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11981">
<div class="article-summary-box-inner">
<span><p>In a sentence, certain words are critical for its semantic. Among them, named
entities (NEs) are notoriously challenging for neural models. Despite their
importance, their accurate handling has been neglected in speech-to-text (S2T)
translation research, and recent work has shown that S2T models perform poorly
for locations and notably person names, whose spelling is challenging unless
known in advance. In this work, we explore how to leverage dictionaries of NEs
known to likely appear in a given context to improve S2T model outputs. Our
experiments show that we can reliably detect NEs likely present in an utterance
starting from S2T encoder outputs. Indeed, we demonstrate that the current
detection quality is sufficient to improve NE accuracy in the translation with
a 31% reduction in person name errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shift-Reduce Task-Oriented Semantic Parsing with Stack-Transformers. (arXiv:2210.11984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11984">
<div class="article-summary-box-inner">
<span><p>Intelligent voice assistants, such as Apple Siri and Amazon Alexa, are widely
used nowadays. These task-oriented dialog systems require a semantic parsing
module in order to process user utterances and understand the action to be
performed. This semantic parsing component was initially implemented by
rule-based or statistical slot-filling approaches for processing simple
queries; however, the appearance of more complex utterances demanded the
application of shift-reduce parsers or sequence-to-sequence models. While
shift-reduce approaches initially demonstrated to be the best option, recent
efforts on sequence-to-sequence systems pushed them to become the
highest-performing method for that task. In this article, we advance the
research on shift-reduce semantic parsing for task-oriented dialog. In
particular, we implement novel shift-reduce parsers that rely on
Stack-Transformers. These allow to adequately model transition systems on the
cutting-edge Transformer architecture, notably boosting shift-reduce parsing
performance. Additionally, we adapt alternative transition systems from
constituency parsing to task-oriented parsing, and empirically prove that the
in-order algorithm substantially outperforms the commonly-used top-down
strategy. Finally, we extensively test our approach on multiple domains from
the Facebook TOP benchmark, improving over existing shift-reduce parsers and
state-of-the-art sequence-to-sequence models in both high-resource and
low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Speech Translation and Named Entity Recognition. (arXiv:2210.11987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11987">
<div class="article-summary-box-inner">
<span><p>Modern automatic translation systems aim at place the human at the center by
providing contextual support and knowledge. In this context, a critical task is
enriching the output with information regarding the mentioned entities, which
is currently achieved processing the generated translation with named entity
recognition (NER) and entity linking systems. In light of the recent promising
results shown by direct speech translation (ST) models and the known weaknesses
of cascades (error propagation and additional latency), in this paper we
propose multitask models that jointly perform ST and NER, and compare them with
a cascade baseline. The experimental results show that our models significantly
outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in
terms of translation quality, and with the same computational efficiency of a
plain direct ST model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing text representations to capture (dis)similarity between political parties. (arXiv:2210.11989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11989">
<div class="article-summary-box-inner">
<span><p>Even though fine-tuned neural language models have been pivotal in enabling
"deep" automatic text analysis, optimizing text representations for specific
applications remains a crucial bottleneck. In this study, we look at this
problem in the context of a task from computational social science, namely
modeling pairwise similarities between political parties. Our research question
is what level of structural information is necessary to create robust text
representation, contrasting a strongly informed approach (which uses both claim
span and claim category annotations) with approaches that forgo one or both
types of annotation with document structure-based heuristics. Evaluating our
models on the manifestos of German parties for the 2021 federal election. We
find that heuristics that maximize within-party over between-party similarity
along with a normalization step lead to reliable party similarity prediction,
without the need for manual annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance-Efficiency Trade-Offs in Adapting Language Models to Text Classification Tasks. (arXiv:2210.12022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12022">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) obtain state-of-the-art performance when
adapted to text classification tasks. However, when using such models in
real-world applications, efficiency considerations are paramount. In this
paper, we study how different training procedures that adapt LMs to text
classification perform, as we vary model and train set size. More specifically,
we compare standard fine-tuning, prompting, and knowledge distillation (KD)
when the teacher was trained with either fine-tuning or prompting. Our findings
suggest that even though fine-tuning and prompting work well to train large LMs
on large train sets, there are more efficient alternatives that can reduce
compute or data cost. Interestingly, we find that prompting combined with KD
can reduce compute and data cost at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12023">
<div class="article-summary-box-inner">
<span><p>We have recently witnessed a number of impressive results on hard
mathematical reasoning problems with language models. At the same time, the
robustness of these models has also been called into question; recent works
have shown that models can rely on shallow patterns in the problem description
when predicting a solution. Building on the idea of behavioral testing, we
propose a novel framework, which pins down the causal effect of various factors
in the input, e.g., the surface form of the problem text, the operands and math
operators on the output solution. By grounding the behavioral analysis in a
causal graph describing an intuitive reasoning process, we study the behavior
of language models in terms of robustness and sensitivity to direct
interventions in the input space. We apply our framework on a test bed of
bivariate math word problems. Our analysis shows that robustness does not
appear to continuously improve as a function of scale, but that the recent LLM,
GPT-3-Instruct (175B), achieves a dramatic improvement in both robustness and
sensitivity, compared to all other GPT variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping NLP tools across low-resourced African languages: an overview and prospects. (arXiv:2210.12027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12027">
<div class="article-summary-box-inner">
<span><p>Computing and Internet access are substantially growing markets in Southern
Africa, which brings with it increasing demands for local content and tools in
indigenous African languages. Since most of those languages are low-resourced,
efforts have gone into the notion of bootstrapping tools for one African
language from another. This paper provides an overview of these efforts for
Niger-Congo B (`Bantu') languages. Bootstrapping grammars for geographically
distant languages has been shown to still have positive outcomes for morphology
and rules or grammar-based natural language generation. Bootstrapping with
data-driven approaches to NLP tasks is difficult to use meaningfully regardless
geographic proximity, which is largely due to lexical diversity due to both
orthography and vocabulary. Cladistic approaches in comparative linguistics may
inform bootstrapping strategies and similarity measures might serve as proxy
for bootstrapping potential as well, with both fertile ground for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12040">
<div class="article-summary-box-inner">
<span><p>Semantic communication (SC) aims to communicate reliably with minimal data
transfer while simultaneously providing seamless connectivity to heterogeneous
services and users. In this paper, a novel emergent SC (ESC) system framework
is proposed and is composed of a signaling game for emergent language design
and a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal
reasoning. In order to design the language, the signaling game is solved using
an alternating maximization between the communicating node's utilities. The
emergent language helps create a context-aware transmit vocabulary (minimal
semantic representation) and aids the reasoning process (enabling
generalization to unseen scenarios) by splitting complex messages into simpler
reasoning tasks for the receiver. The causal description at the transmitter is
then modeled (a neural component) as a posterior distribution of the relevant
attributes present in the data. Using the reconstructed causal state, the
receiver evaluates a set of logical formulas (symbolic part) to execute its
task. The nodes NeSy reasoning components are implemented by the recently
proposed AI tool called Generative Flow Networks, and they are optimized for
higher semantic reliability. The ESC system is designed to enhance the novel
metrics of semantic information, reliability, distortion and similarity that
are designed using rigorous algebraic properties from category theory thereby
generalizing the metrics beyond Shannon's notion of uncertainty. Simulation
results validate the ability of ESC to communicate efficiently (with reduced
bits) and achieve better semantic reliability than conventional wireless and
state-of-the-art systems that do not exploit causal reasoning capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards. (arXiv:2210.12050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12050">
<div class="article-summary-box-inner">
<span><p>Derivative-free prompt learning has emerged as a lightweight alternative to
prompt tuning, which only requires model inference to optimize the prompts.
However, existing work did not take full advantage of the over-parameterized
characteristics of large pre-trained language models (PLMs). In this paper, we
propose Clip-Tuning, a simple yet effective method that adopts diverse frozen
"thinned" networks of PLMs to obtain a mixture of rewards and thus advance the
derivative-free prompt learning. The thinned networks consist of all the hidden
units that survive a stationary dropout strategy, whose inference predictions
reflect an ensemble of partial views over prompted training samples. Our method
outperforms previous gradient-free prompt learning methods and achieves parity
with gradient-based counterparts on seven language understanding benchmarks
under few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiencer-Specific Emotion and Appraisal Prediction. (arXiv:2210.12078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12078">
<div class="article-summary-box-inner">
<span><p>Emotion classification in NLP assigns emotions to texts, such as sentences or
paragraphs. With texts like "I felt guilty when he cried", focusing on the
sentence level disregards the standpoint of each participant in the situation:
the writer ("I") and the other entity ("he") could in fact have different
affective states. The emotions of different entities have been considered only
partially in emotion semantic role labeling, a task that relates semantic roles
to emotion cue words. Proposing a related task, we narrow the focus on the
experiencers of events, and assign an emotion (if any holds) to each of them.
To this end, we represent each emotion both categorically and with appraisal
variables, as a psychological access to explaining why a person develops a
particular emotion. On an event description corpus, our experiencer-aware
models of emotions and appraisals outperform the experiencer-agnostic
baselines, showing that disregarding event participants is an
oversimplification for the emotion detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?. (arXiv:2210.12079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12079">
<div class="article-summary-box-inner">
<span><p>Recent advances in vision-and-language modeling have seen the development of
Transformer architectures that achieve remarkable performance on multimodal
reasoning tasks. Yet, the exact capabilities of these black-box models are
still poorly understood. While much of previous work has focused on studying
their ability to learn meaning at the word-level, their ability to track
syntactic dependencies between words has received less attention. We take a
first step in closing this gap by creating a new multimodal task targeted at
evaluating understanding of predicate-noun dependencies in a controlled setup.
We evaluate a range of state-of-the-art models and find that their performance
on the task varies considerably, with some models performing relatively well
and others at chance level. In an effort to explain this variability, our
analyses indicate that the quality (and not only sheer quantity) of pretraining
data is essential. Additionally, the best performing models leverage
fine-grained multimodal pretraining objectives in addition to the standard
image-text matching objectives. This study highlights that targeted and
controlled evaluations are a crucial step for a precise and rigorous test of
the multimodal knowledge of vision-and-language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoding a Neural Retriever's Latent Space for Query Suggestion. (arXiv:2210.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12084">
<div class="article-summary-box-inner">
<span><p>Neural retrieval models have superseded classic bag-of-words methods such as
BM25 as the retrieval framework of choice. However, neural systems lack the
interpretability of bag-of-words models; it is not trivial to connect a query
change to a change in the latent space that ultimately determines the retrieval
results. To shed light on this embedding space, we learn a "query decoder"
that, given a latent representation of a neural search engine, generates the
corresponding query. We show that it is possible to decode a meaningful query
from its latent representation and, when moving in the right direction in
latent space, to decode a query that retrieves the relevant paragraph. In
particular, the query decoder can be useful to understand "what should have
been asked" to retrieve a particular paragraph from the collection. We employ
the query decoder to generate a large synthetic dataset of query reformulations
for MSMarco, leading to improved retrieval performance. On this data, we train
a pseudo-relevance feedback (PRF) T5 model for the application of query
suggestion that outperforms both query reformulation and PRF information
retrieval baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play. (arXiv:2210.12096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12096">
<div class="article-summary-box-inner">
<span><p>The task of context-dependent text-to-SQL aims to convert multi-turn user
utterances to formal SQL queries. This is a challenging task due to both the
scarcity of training data from which to learn complex contextual dependencies
and to generalize to unseen databases. In this paper we explore augmenting the
training datasets using self-play, which leverages contextual information to
synthesize new interactions to adapt the model to new databases. We first
design a SQL-to-text model conditioned on a sampled goal query, which
represents a user's intent, that then converses with a text-to-SQL semantic
parser to generate new interactions. We then filter the synthesized
interactions and retrain the models with the augmented data. We find that
self-play improves the accuracy of a strong baseline on SParC and CoSQL, two
widely used cross-domain text-to-SQL datasets. Our analysis shows that
self-play simulates various conversational thematic relations, enhances
cross-domain generalization and improves beam-search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Describing Sets of Images with Textual-PCA. (arXiv:2210.12112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12112">
<div class="article-summary-box-inner">
<span><p>We seek to semantically describe a set of images, capturing both the
attributes of single images and the variations within the set. Our procedure is
analogous to Principle Component Analysis, in which the role of projection
vectors is replaced with generated phrases. First, a centroid phrase that has
the largest average semantic similarity to the images in the set is generated,
where both the computation of the similarity and the generation are based on
pretrained vision-language models. Then, the phrase that generates the highest
variation among the similarity scores is generated, using the same models. The
next phrase maximizes the variance subject to being orthogonal, in the latent
space, to the highest-variance phrase, and the process continues. Our
experiments show that our method is able to convincingly capture the essence of
image sets and describe the individual elements in a semantically meaningful
way within the context of the entire set. Our code is available at:
https://github.com/OdedH/textual-pca.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-to-Intent Using Acoustic-Textual Subword Representations from End-to-End ASR. (arXiv:2210.12134v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12134">
<div class="article-summary-box-inner">
<span><p>Accurate prediction of the user intent to interact with a voice assistant
(VA) on a device (e.g. on the phone) is critical for achieving naturalistic,
engaging, and privacy-centric interactions with the VA. To this end, we present
a novel approach to predict the user's intent (the user speaking to the device
or not) directly from acoustic and textual information encoded at subword
tokens which are obtained via an end-to-end ASR model. Modeling directly the
subword tokens, compared to modeling of the phonemes and/or full words, has at
least two advantages: (i) it provides a unique vocabulary representation, where
each token has a semantic meaning, in contrast to the phoneme-level
representations, (ii) each subword token has a reusable "sub"-word acoustic
pattern (that can be used to construct multiple full words), resulting in a
largely reduced vocabulary space than of the full words. To learn the subword
representations for the audio-to-intent classification, we extract: (i)
acoustic information from an E2E-ASR model, which provides frame-level CTC
posterior probabilities for the subword tokens, and (ii) textual information
from a pre-trained continuous bag-of-words model capturing the semantic meaning
of the subword tokens. The key to our approach is the way it combines acoustic
subword-level posteriors with text information using the notion of
positional-encoding in order to account for multiple ASR hypotheses
simultaneously. We show that our approach provides more robust and richer
representations for audio-to-intent classification, and is highly accurate with
correctly mitigating 93.3% of unintended user audio from invoking the smart
assistant at 99% true positive rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiWhy: Answering and Explaining Cause-and-Effect Questions. (arXiv:2210.12152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12152">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) grow larger and more sophisticated, assessing
their "reasoning" capabilities in natural language grows more challenging.
Recent question answering (QA) benchmarks that attempt to assess reasoning are
often limited by a narrow scope of covered situations and subject matters. We
introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining
why an answer is true in natural language. WikiWhy contains over 9,000 "why"
question-answer-rationale triples, grounded on Wikipedia facts across a diverse
set of topics. Each rationale is a set of supporting statements connecting the
question to the answer. WikiWhy serves as a benchmark for the reasoning
capabilities of LLMs because it demands rigorous explicit rationales for each
answer to demonstrate the acquisition of implicit commonsense knowledge, which
is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%
human-evaluated correctness in the end-to-end answer &amp; explain condition,
leaving significant room for future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward. (arXiv:2109.06717v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06717">
<div class="article-summary-box-inner">
<span><p>Controllable text generation is an appealing but challenging task, which
allows users to specify particular attributes of the generated outputs. In this
paper, we propose a controllable dialogue generation model to steer response
generation under multi-attribute constraints. Specifically, we define and
categorize the commonly used control attributes into global and local ones,
which possess different granularities of effects on response generation. Then,
we significantly extend the conventional seq2seq framework by introducing a
novel two-stage decoder, which first uses a multi-grained style specification
layer to impose the stylistic constraints and determine word-level control
states of responses based on the attributes, and then employs a response
generation layer to generate final responses maintaining both semantic
relevancy to the contexts and fidelity to the attributes. Furthermore, we train
our model with an attribute consistency reward to promote response control with
explicit supervision signals. Extensive experiments and in-depth analyses on
two datasets indicate that our model can significantly outperform competitive
baselines in terms of response quality, content diversity and controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HydraSum: Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04400">
<div class="article-summary-box-inner">
<span><p>Summarization systems make numerous "decisions" about summary properties
during inference, e.g. degree of copying, specificity and length of outputs,
etc. However, these are implicitly encoded within model parameters and specific
styles cannot be enforced. To address this, we introduce HydraSum, a new
summarization architecture that extends the single decoder framework of current
models to a mixture-of-experts version with multiple decoders. We show that
HydraSum's multiple decoders automatically learn contrasting summary styles
when trained under the standard training objective without any extra
supervision. Through experiments on three summarization datasets (CNN, Newsroom
and XSum), we show that HydraSum provides a simple mechanism to obtain
stylistically-diverse summaries by sampling from either individual decoders or
their mixtures, outperforming baseline models. Finally, we demonstrate that a
small modification to the gating strategy during training can enforce an even
stricter style partitioning, e.g. high- vs low-abstractiveness or high- vs
low-specificity, allowing users to sample from a larger area in the generation
space and vary summary styles along multiple dimensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyphrase Generation Beyond the Boundaries of Title and Abstract. (arXiv:2112.06776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06776">
<div class="article-summary-box-inner">
<span><p>Keyphrase generation aims at generating important phrases (keyphrases) that
best describe a given document. In scholarly domains, current approaches have
largely used only the title and abstract of the articles to generate
keyphrases. In this paper, we comprehensively explore whether the integration
of additional information from the full text of a given article or from
semantically similar articles can be helpful for a neural keyphrase generation
model or not. We discover that adding sentences from the full text,
particularly in the form of the extractive summary of the article can
significantly improve the generation of both types of keyphrases that are
either present or absent from the text. Experimental results with three widely
used models for keyphrase generation along with one of the latest transformer
models suitable for longer documents, Longformer Encoder-Decoder (LED) validate
the observation. We also present a new large-scale scholarly dataset FullTextKP
for keyphrase generation. Unlike prior large-scale datasets, FullTextKP
includes the full text of the articles along with the title and abstract. We
release the source code at https://github.com/kgarg8/FullTextKP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to General American English: A Case Study of a Chinese Learner with Advanced English. (arXiv:2112.13571v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13571">
<div class="article-summary-box-inner">
<span><p>The current paper concerns language transfer at the phonetic level and
concentrates on the transfer phenomenon in an advanced English language
learner's acquisition of the English vowels /i/ and its lax counterpart. By
determining whether the Chinese English-language learner (ELL), named Vanya,
can accurately distinguish between /i/ and its lax counterpart, and pronounce
them precisely in General American English (GAE), this paper serves as a
reference for further studying language transfer among Chinese ELLs. There were
two objectives: first, the learner's perceptual ability to distinguish between
vowels /i/ and its lax counterpart was examined; second, the effect of the
phonetic transfer was determined. Two perception tests and a production test
were used to attain these two objectives. The results of two perception tests
demonstrated Vanya's perceptual competence in distinguishing between /i/ and
its lax counterpart and laid a solid foundation for the validity of the
subsequent production test. Given that Vanya's production of F1 and F2 values
of /i/ were highly similar across his first language (Mandarin Chinese) and
second language (GAE) and that both values were lower than the typical values
for common /i/ in GAE, with an especially prominent disparity between the F2
values, it is reasonable to conclude that a phonetic transfer occurred. The
participant's high perceptual competence as an advanced-level ELL did not
noticeably moderate the effect of phonetic transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06991">
<div class="article-summary-box-inner">
<span><p>In this paper, we demonstrate that information retrieval can be accomplished
with a single Transformer, in which all information about the corpus is encoded
in the parameters of the model. To this end, we introduce the Differentiable
Search Index (DSI), a new paradigm that learns a text-to-text model that maps
string queries directly to relevant docids; in other words, a DSI model answers
queries directly using only its parameters, dramatically simplifying the whole
retrieval process. We study variations in how documents and their identifiers
are represented, variations in training procedures, and the interplay between
models and corpus sizes. Experiments demonstrate that given appropriate design
choices, DSI significantly outperforms strong baselines such as dual encoder
models. Moreover, DSI demonstrates strong generalization capabilities,
outperforming a BM25 baseline in a zero-shot setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural-Symbolic Approach to Natural Language Understanding. (arXiv:2203.10557v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10557">
<div class="article-summary-box-inner">
<span><p>Deep neural networks, empowered by pre-trained language models, have achieved
remarkable results in natural language understanding (NLU) tasks. However,
their performances can drastically deteriorate when logical reasoning is
needed. This is because NLU in principle depends on not only analogical
reasoning, which deep neural networks are good at, but also logical reasoning.
According to the dual-process theory, analogical reasoning and logical
reasoning are respectively carried out by System 1 and System 2 in the human
brain. Inspired by the theory, we present a novel framework for NLU called
Neural-Symbolic Processor (NSP), which performs analogical reasoning based on
neural processing and logical reasoning based on both neural and symbolic
processing. As a case study, we conduct experiments on two NLU tasks, question
answering (QA) and natural language inference (NLI), when numerical reasoning
(a type of logical reasoning) is necessary. The experimental results show that
our method significantly outperforms state-of-the-art methods in both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SecureBERT: A Domain-Specific Language Model for Cybersecurity. (arXiv:2204.02685v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02685">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) has recently gained wide attention in
cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber
automation. Increased connection and automation have revolutionized the world's
economic and cultural infrastructures, while they have introduced risks in
terms of cyber attacks. CTI is information that helps cybersecurity analysts
make intelligent security decisions, that is often delivered in the form of
natural language text, which must be transformed to machine readable format
through an automated procedure before it can be used for automated security
measures.
</p>
<p>This paper proposes SecureBERT, a cybersecurity language model capable of
capturing text connotations in cybersecurity text (e.g., CTI) and therefore
successful in automation for many critical cybersecurity tasks that would
otherwise rely on human expertise and time-consuming manual efforts. SecureBERT
has been trained using a large corpus of cybersecurity text.To make SecureBERT
effective not just in retaining general English understanding, but also when
applied to text with cybersecurity implications, we developed a customized
tokenizer as well as a method to alter pre-trained weights. The SecureBERT is
evaluated using the standard Masked Language Model (MLM) test as well as two
additional standard NLP tasks. Our evaluation studies show that
SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}}
outperforms existing similar models, confirming its capability for solving
crucial NLP tasks in cybersecurity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative or Contrastive? Phrase Reconstruction for Better Sentence Representation Learning. (arXiv:2204.09358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09358">
<div class="article-summary-box-inner">
<span><p>Though offering amazing contextualized token-level representations, current
pre-trained language models actually take less attention on acquiring
sentence-level representation during its self-supervised pre-training. If
self-supervised learning can be distinguished into two subcategories,
generative and contrastive, then most existing studies show that sentence
representation learning may more benefit from the contrastive methods but not
the generative methods. However, contrastive learning cannot be well compatible
with the common token-level generative self-supervised learning, and does not
guarantee good performance on downstream semantic retrieval tasks. Thus, to
alleviate such obvious inconveniences, we instead propose a novel generative
self-supervised learning objective based on phrase reconstruction. Empirical
studies show that our generative learning may yield powerful enough sentence
representation and achieve performance in Sentence Textual Similarity (STS)
tasks on par with contrastive learning. Further, in terms of unsupervised
setting, our generative method outperforms previous state-of-the-art SimCSE on
the benchmark of downstream semantic retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08232">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization as Indirect Supervision for Relation Extraction. (arXiv:2205.09837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09837">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) models have been challenged by their reliance on
training data with expensive annotations. Considering that summarization tasks
aim at acquiring concise expressions of synoptical information from the longer
context, these tasks naturally align with the objective of RE, i.e., extracting
a kind of synoptical information that describes the relation of entity
mentions. We present SuRE, which converts RE into a summarization formulation.
SuRE leads to more precise and resource-efficient RE based on indirect
supervision from summarization tasks. To achieve this goal, we develop sentence
and relation conversion techniques that essentially bridge the formulation of
summarization and RE tasks. We also incorporate constraint decoding techniques
with Trie scoring to further enhance summarization-based RE with robust
inference. Experiments on three RE datasets demonstrate the effectiveness of
SuRE in both full-dataset and low-resource settings, showing that summarization
is a promising source of indirect supervision to improve RE models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Neural Machine Translation: A Call for Clarity. (arXiv:2205.10577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10577">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive approaches aim to improve the inference speed of
translation models by only requiring a single forward pass to generate the
output sequence instead of iteratively producing each predicted token.
Consequently, their translation quality still tends to be inferior to their
autoregressive counterparts due to several issues involving output token
interdependence. In this work, we take a step back and revisit several
techniques that have been proposed for improving non-autoregressive translation
models and compare their combined translation quality and speed implications
under third-party testing environments. We provide novel insights for
establishing strong baselines using length prediction or CTC-based architecture
variants and contribute standardized BLEU, chrF++, and TER scores using
sacreBLEU on four translation tasks, which crucially have been missing as
inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7
BLEU points. Our open-sourced code is integrated into fairseq for
reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10828">
<div class="article-summary-box-inner">
<span><p>Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Template-based Method for Constrained Neural Machine Translation. (arXiv:2205.11255v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11255">
<div class="article-summary-box-inner">
<span><p>Machine translation systems are expected to cope with various types of
constraints in many practical scenarios. While neural machine translation (NMT)
has achieved strong performance in unconstrained cases, it is non-trivial to
impose pre-specified constraints into the translation process of NMT models.
Although many approaches have been proposed to address this issue, most
existing methods can not satisfy the following three desiderata at the same
time: (1) high translation quality, (2) high match accuracy, and (3) low
latency. In this work, we propose a template-based method that can yield
results with high translation quality and match accuracy and the inference
speed of our method is comparable with unconstrained NMT models. Our basic idea
is to rearrange the generation of constrained and unconstrained tokens through
a template. Our method does not require any changes in the model architecture
and the decoding algorithm. Experimental results show that the proposed
template-based approach can outperform several representative baselines in both
lexically and structurally constrained translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models. (arXiv:2205.11432v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11432">
<div class="article-summary-box-inner">
<span><p>Current Natural Language Inference (NLI) models achieve impressive results,
sometimes outperforming humans when evaluating on in-distribution test sets.
However, as these models are known to learn from annotation artefacts and
dataset biases, it is unclear to what extent the models are learning the task
of NLI instead of learning from shallow heuristics in their training data. We
address this issue by introducing a logical reasoning framework for NLI,
creating highly transparent model decisions that are based on logical rules.
Unlike prior work, we show that improved interpretability can be achieved
without decreasing the predictive accuracy. We almost fully retain performance
on SNLI, while also identifying the exact hypothesis spans that are responsible
for each model prediction. Using the e-SNLI human explanations, we verify that
our model makes sensible decisions at a span level, despite not using any span
labels during training. We can further improve model performance and span-level
decisions by using the e-SNLI explanations during training. Finally, our model
is more robust in a reduced data setting. When training with only 1,000
examples, out-of-distribution performance improves on the MNLI matched and
mismatched validation sets by 13% and 16% relative to the baseline. Training
with fewer observations yields further improvements, both in-distribution and
out-of-distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Contrastive Learning for Relation Extraction. (arXiv:2205.12491v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12491">
<div class="article-summary-box-inner">
<span><p>Recent relation extraction (RE) works have shown encouraging improvements by
conducting contrastive learning on silver labels generated by distant
supervision before fine-tuning on gold labels. Existing methods typically
assume all these silver labels are accurate and treat them equally; however,
distant supervision is inevitably noisy -- some silver labels are more reliable
than others. In this paper, we propose fine-grained contrastive learning
(FineCL) for RE, which leverages fine-grained information about which silver
labels are and are not noisy to improve the quality of learned relationship
representations for RE. We first assess the quality of silver labels via a
simple and automatic approach we call "learning order denoising," where we
train a language model to learn these relations and record the order of learned
training instances. We show that learning order largely corresponds to label
accuracy -- early-learned silver labels have, on average, more accurate labels
than later-learned silver labels. Then, during pre-training, we increase the
weights of accurate labels within a novel contrastive learning objective.
Experiments on several RE benchmarks show that FineCL makes consistent and
significant performance gains over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Text Generation with Neurally-Decomposed Oracle. (arXiv:2205.14219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14219">
<div class="article-summary-box-inner">
<span><p>We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. Based on posterior regularization, we
present the closed-form optimal solution to incorporate the token-level
guidance into the base model for controllable generation. We further provide a
theoretical analysis of how the approximation quality of NADO affects the
controllable generation results. Experiments conducted on two applications: (1)
text generation with lexical constraints and (2) machine translation with
formality control demonstrate that our framework efficiently guides the base
model towards the given oracle while maintaining high generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04153">
<div class="article-summary-box-inner">
<span><p>Neural network models trained on text data have been found to encode
undesirable linguistic or sensitive concepts in their representation. Removing
such concepts is non-trivial because of a complex relationship between the
concept, text input, and the learnt representation. Recent work has proposed
post-hoc and adversarial methods to remove such unwanted concepts from a
model's representation. Through an extensive theoretical and empirical
analysis, we show that these methods can be counter-productive: they are unable
to remove the concepts entirely, and in the worst case may end up destroying
all task-relevant features. The reason is the methods' reliance on a probing
classifier as a proxy for the concept. Even under the most favorable conditions
for learning a probing classifier when a concept's relevant features in
representation space alone can provide 100% accuracy, we prove that a probing
classifier is likely to use non-concept features and thus post-hoc or
adversarial methods will fail to remove the concept correctly. These
theoretical implications are confirmed by experiments on models trained on
synthetic, Multi-NLI, and Twitter datasets. For sensitive applications of
concept removal such as fairness, we recommend caution against using these
methods and propose a spuriousness metric to gauge the quality of the final
classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress. (arXiv:2207.05691v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05691">
<div class="article-summary-box-inner">
<span><p>The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to
multimodal sentiment and emotion recognition. For this year's challenge, we
feature three datasets: (i) the Passau Spontaneous Football Coach Humor
(Passau-SFCH) dataset that contains audio-visual recordings of German football
coaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in
which reactions of individuals to emotional stimuli have been annotated with
respect to seven emotional expression intensities, and (iii) the Ulm-Trier
Social Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled
with continuous emotion values (arousal and valence) of people in stressful
dispositions. Using the introduced datasets, MuSe 2022 2022 addresses three
contemporary affective computing problems: in the Humor Detection Sub-Challenge
(MuSe-Humor), spontaneous humour has to be recognised; in the Emotional
Reactions Sub-Challenge (MuSe-Reaction), seven fine-grained `in-the-wild'
emotions have to be predicted; and in the Emotional Stress Sub-Challenge
(MuSe-Stress), a continuous prediction of stressed emotion values is featured.
The challenge is designed to attract different research communities,
encouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the
communities of audio-visual emotion recognition, health informatics, and
symbolic sentiment analysis. This baseline paper describes the datasets as well
as the feature sets extracted from them. A recurrent neural network with LSTM
cells is used to set competitive baseline results on the test partitions for
each sub-challenge. We report an Area Under the Curve (AUC) of .8480 for
MuSe-Humor; .2801 mean (from 7-classes) Pearson's Correlations Coefficient for
MuSe-Reaction, as well as .4931 Concordance Correlation Coefficient (CCC) and
.4761 for valence and arousal in MuSe-Stress, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation. (arXiv:2207.06130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06130">
<div class="article-summary-box-inner">
<span><p>The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03755">
<div class="article-summary-box-inner">
<span><p>Mis- and disinformation are now a substantial global threat to our security
and safety. To cope with the scale of online misinformation, one viable
solution is to automate the fact-checking of claims by retrieving and verifying
against relevant evidence. While major recent advances have been achieved in
pushing forward the automatic fact-verification, a comprehensive evaluation of
the possible attack vectors against such systems is still lacking.
Particularly, the automated fact-verification process might be vulnerable to
the exact disinformation campaigns it is trying to combat. In this work, we
assume an adversary that automatically tampers with the online evidence in
order to disrupt the fact-checking model via camouflaging the relevant
evidence, or planting a misleading one. We first propose an exploratory
taxonomy that spans these two targets and the different threat model
dimensions. Guided by this, we design and propose several potential attack
methods. We show that it is possible to subtly modify claim-salient snippets in
the evidence, in addition to generating diverse and claim-aligned evidence. As
a result, we highly degrade the fact-checking performance under many different
permutations of the taxonomy's dimensions. The attacks are also robust against
post-hoc modifications of the claim. Our analysis further hints at potential
limitations in models' inference when faced with contradicting evidence. We
emphasize that these attacks can have harmful implications on the inspectable
and human-in-the-loop usage scenarios of such models, and we conclude by
discussing challenges and directions for future defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vega-MT: The JD Explore Academy Translation System for WMT22. (arXiv:2209.09444v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09444">
<div class="article-summary-box-inner">
<span><p>We describe the JD Explore Academy's submission of the WMT 2022 shared
general translation task. We participated in all high-resource tracks and one
medium-resource track, including Chinese-English, German-English,
Czech-English, Russian-English, and Japanese-English. We push the limit of our
previous work -- bidirectional training for translation by scaling up two main
factors, i.e. language pairs and model sizes, namely the \textbf{Vega-MT}
system. As for language pairs, we scale the "bidirectional" up to the
"multidirectional" settings, covering all participating languages, to exploit
the common knowledge across languages, and transfer them to the downstream
bilingual tasks. As for model sizes, we scale the Transformer-Big up to the
extremely large model that owns nearly 4.7 Billion parameters, to fully enhance
the model capacity for our Vega-MT. Also, we adopt the data augmentation
strategies, e.g. cycle translation for monolingual data, and bidirectional
self-training for bilingual and monolingual data, to comprehensively exploit
the bilingual and monolingual data. To adapt our Vega-MT to the general domain
test set, generalization tuning is designed. Based on the official automatic
scores of constrained systems, in terms of the sacreBLEU shown in Figure-1, we
got the 1st place on {Zh-En (33.5), En-Zh (49.7), De-En (33.7), En-De (37.8),
Cs-En (54.9), En-Cs (41.4) and En-Ru (32.7)}, 2nd place on {Ru-En (45.1) and
Ja-En (25.6)}, and 3rd place on {En-Ja(41.5)}, respectively; W.R.T the COMET,
we got the 1st place on {Zh-En (45.1), En-Zh (61.7), De-En (58.0), En-De
(63.2), Cs-En (74.7), Ru-En (64.9), En-Ru (69.6) and En-Ja (65.1)}, 2nd place
on {En-Cs (95.3) and Ja-En (40.6)}, respectively. Models will be released to
facilitate the MT community through GitHub and OmniForce Platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation. (arXiv:2210.02952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02952">
<div class="article-summary-box-inner">
<span><p>Prompt tuning, or the conditioning of a frozen pretrained language model
(PLM) with soft prompts learned from data, has demonstrated impressive
performance on a wide range of NLP tasks. However, prompt tuning requires a
large training dataset to be effective and is outperformed by finetuning the
entire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al.,
2022) proposed to transfer soft prompts pretrained on the source domain to the
target domain. In this paper, we explore domain adaptation for prompt tuning, a
problem setting where unlabeled data from the target domain are available
during pretraining. We propose bOosting Prompt TunIng with doMain Adaptation
(OPTIMA), which regularizes the decision boundary to be smooth around regions
where source and target data distributions are similar. Extensive experiments
demonstrate that OPTIMA significantly enhances the transferability and
sample-efficiency of prompt tuning compared to strong baselines. Moreover, in
few-shot settings, OPTIMA exceeds full-model tuning by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Language Models for Paragraph-Level Question Generation. (arXiv:2210.03992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03992">
<div class="article-summary-box-inner">
<span><p>Powerful generative models have led to recent progress in question generation
(QG). However, it is difficult to measure advances in QG research since there
are no standardized resources that allow a uniform comparison among approaches.
In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark
for QG that unifies existing question answering datasets by converting them to
a standard QG setting. It includes general-purpose datasets such as SQuAD for
English, datasets from ten domains and two styles, as well as datasets in eight
different languages. Using QG-Bench as a reference, we perform an extensive
analysis of the capabilities of language models for the task. First, we propose
robust QG baselines based on fine-tuning generative language models. Then, we
complement automatic evaluation based on standard metrics with an extensive
manual evaluation, which in turn sheds light on the difficulty of evaluating QG
models. Finally, we analyse both the domain adaptability of these models as
well as the effectiveness of multilingual models in languages other than
English. QG-Bench is released along with the fine-tuned models presented in the
paper https://github.com/asahi417/lm-question-generation, which are also
available as a demo https://autoqg.net/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04284">
<div class="article-summary-box-inner">
<span><p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only
fine-tunes a few extra modules, becomes an appealing efficient alternative to
the full model fine-tuning. Although computationally efficient, the recent
Adapters often increase parameters (e.g. bottleneck dimension) for matching the
performance of full model fine-tuning, which we argue goes against their
original intention. In this work, we re-examine the parameter-efficiency of
Adapters through the lens of network pruning (we name such plug-in concept as
\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or
better performance than standard Adapters when the sparse ratio reaches up to
80\%. Based on our findings, we introduce an easy but effective setting
``\textit{Large-Sparse}'' to improve the model capacity of Adapters under the
same parameter budget. Experiments on five competitive Adapters upon three
advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.
40\%) SparseAdapter can consistently outperform their corresponding
counterpart. Encouragingly, with the \textit{Large-Sparse} setting, we can
obtain further appealing gains, even outperforming the full fine-tuning by a
large margin. Our code will be released at:
https://github.com/Shwai-He/SparseAdapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response. (arXiv:2210.04573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04573">
<div class="article-summary-box-inner">
<span><p>Timely and effective response to humanitarian crises requires quick and
accurate analysis of large amounts of text data - a process that can highly
benefit from expert-assisted NLP systems trained on validated and annotated
data in the humanitarian response domain. To enable creation of such NLP
systems, we introduce and release HumSet, a novel and rich multilingual dataset
of humanitarian response documents annotated by experts in the humanitarian
response community. The dataset provides documents in three languages (English,
French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021
across the globe. For each document, HUMSET provides selected snippets
(entries) as well as assigned classes to each entry annotated using common
humanitarian information analysis frameworks. HUMSET also provides novel and
challenging entry extraction and multi-label entry classification tasks. In
this paper, we take a first step towards approaching these tasks and conduct a
set of experiments on Pre-trained Language Models (PLM) to establish strong
baselines for future research in this domain. The dataset is available at
https://blog.thedeep.io/humset/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPerform: An Efficient Approach for Performance Testing of Resource-Constrained Neural Networks. (arXiv:2210.05370v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05370">
<div class="article-summary-box-inner">
<span><p>Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are
being used on resource-constrained embedded devices. We observe that, similar
to traditional software, redundant computation exists in AdNNs, resulting in
considerable performance degradation. The performance degradation is dependent
on the input and is referred to as input-dependent performance bottlenecks
(IDPBs). To ensure an AdNN satisfies the performance requirements of
resource-constrained applications, it is essential to conduct performance
testing to detect IDPBs in the AdNN. Existing neural network testing methods
are primarily concerned with correctness testing, which does not involve
performance testing. To fill this gap, we propose DeepPerform, a scalable
approach to generate test samples to detect the IDPBs in AdNNs. We first
demonstrate how the problem of generating performance test samples detecting
IDPBs can be formulated as an optimization problem. Following that, we
demonstrate how DeepPerform efficiently handles the optimization problem by
learning and estimating the distribution of AdNNs' computational consumption.
We evaluate DeepPerform on three widely used datasets against five popular AdNN
models. The results show that DeepPerform generates test samples that cause
more severe performance degradation (FLOPs: increase up to 552\%). Furthermore,
DeepPerform is substantially more efficient than the baseline methods in
generating test inputs(runtime overhead: only 6-10 milliseconds).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06346">
<div class="article-summary-box-inner">
<span><p>The number of clinical citations received from clinical guidelines or
clinical trials has been considered as one of the most appropriate indicators
for quantifying the clinical impact of biomedical papers. Therefore, the early
prediction of the clinical citation count of biomedical papers is critical to
scientific activities in biomedicine, such as research evaluation, resource
allocation, and clinical translation. In this study, we designed a four-layer
multilayer perceptron neural network (MPNN) model to predict the clinical
citation count of biomedical papers in the future by using 9,822,620 biomedical
papers published from 1985 to 2005. We extracted ninety-one paper features from
three dimensions as the input of the model, including twenty-one features in
the paper dimension, thirty-five in the reference dimension, and thirty-five in
the citing paper dimension. In each dimension, the features can be classified
into three categories, i.e., the citation-related features, the clinical
translation-related features, and the topic-related features. Besides, in the
paper dimension, we also considered the features that have previously been
demonstrated to be related to the citation counts of research papers. The
results showed that the proposed MPNN model outperformed the other five
baseline models, and the features in the reference dimension were the most
important.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning. (arXiv:2210.08459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08459">
<div class="article-summary-box-inner">
<span><p>Existing automatic story evaluation methods place a premium on story lexical
level coherence, deviating from human preference. We go beyond this limitation
by considering a novel \textbf{Story} \textbf{E}valuation method that mimics
human preference when judging a story, namely \textbf{StoryER}, which consists
of three sub-tasks: \textbf{R}anking, \textbf{R}ating and \textbf{R}easoning.
Given either a machine-generated or a human-written story, StoryER requires the
machine to output 1) a preference score that corresponds to human preference,
2) specific ratings and their corresponding confidences and 3) comments for
various aspects (e.g., opening, character-shaping). To support these tasks, we
introduce a well-annotated dataset comprising (i) 100k ranked story pairs; and
(ii) a set of 46k ratings and comments on various aspects of the story. We
finetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the
encoder responsible for preference score and aspect prediction and the decoder
for comment generation. Our comprehensive experiments result in a competitive
benchmark for each task, showing the high correlation to human preference. In
addition, we have witnessed the joint learning of the preference scores, the
aspect ratings, and the comments brings gain in each single task. Our dataset
and benchmarks are publicly available to advance the research of story
evaluation tasks.\footnote{Dataset and pre-trained model demo are available at
anonymous website \url{<a href="http://storytelling-lab.com/eval">this http URL</a>} and
\url{https://github.com/sairin1202/StoryER}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Representation Learning with Generative Objective rather than Contrastive Objective. (arXiv:2210.08474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08474">
<div class="article-summary-box-inner">
<span><p>Though offering amazing contextualized token-level representations, current
pre-trained language models take less attention on accurately acquiring
sentence-level representation during their self-supervised pre-training.
However, contrastive objectives which dominate the current sentence
representation learning bring little linguistic interpretability and no
performance guarantee on downstream semantic tasks. We instead propose a novel
generative self-supervised learning objective based on phrase reconstruction.
To overcome the drawbacks of previous generative methods, we carefully model
intra-sentence structure by breaking down one sentence into pieces of important
phrases. Empirical studies show that our generative learning achieves powerful
enough performance improvement and outperforms the current state-of-the-art
contrastive methods not only on the STS benchmarks, but also on downstream
semantic retrieval and reranking tasks. Our code is available at
https://github.com/chengzhipanpan/PaSeR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold. (arXiv:2210.08806v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08806">
<div class="article-summary-box-inner">
<span><p>Conventional event detection models under supervised learning settings suffer
from the inability of transfer to newly-emerged event types owing to lack of
sufficient annotations. A commonly-adapted solution is to follow a
identify-then-classify manner, which first identifies the triggers and then
converts the classification task via a few-shot learning paradigm. However,
these methods still fall far short of expectations due to: (i) insufficient
learning of discriminative representations in low-resource scenarios, and (ii)
trigger misidentification caused by the overlap of the learned representations
of triggers and non-triggers. To address the problems, in this paper, we
propose a novel Hybrid Contrastive Learning method with a Task-Adaptive
Threshold (abbreviated as HCLTAT), which enables discriminative representation
learning with a two-view contrastive loss (support-support and
prototype-query), and devises a easily-adapted threshold to alleviate
misidentification of triggers. Extensive experiments on the benchmark dataset
FewEvent demonstrate the superiority of our method to achieve better results
compared to the state-of-the-arts. All the code and data of this paper will be
available for online public access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADI 2022: The Third Nuanced Arabic Dialect Identification Shared Task. (arXiv:2210.09582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09582">
<div class="article-summary-box-inner">
<span><p>We describe findings of the third Nuanced Arabic Dialect Identification
Shared Task (NADI 2022). NADI aims at advancing state of the art Arabic NLP,
including on Arabic dialects. It does so by affording diverse datasets and
modeling opportunities in a standardized context where meaningful comparisons
between models and approaches are possible. NADI 2022 targeted both dialect
identification (Subtask 1) and dialectal sentiment analysis (Subtask 2) at the
country level. A total of 41 unique teams registered for the shared task, of
whom 21 teams have actually participated (with 105 valid submissions). Among
these, 19 teams participated in Subtask 1 and 10 participated in Subtask 2. The
winning team achieved 27.06 F1 on Subtask 1 and F1=75.16 on Subtask 2,
reflecting that the two subtasks remain challenging and motivating future work
in this area. We describe methods employed by participating teams and offer an
outlook for NADI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR. (arXiv:2210.10027v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10027">
<div class="article-summary-box-inner">
<span><p>Training state-of-the-art Automated Speech Recognition (ASR) models typically
requires a substantial amount of transcribed speech. In this work, we
demonstrate that a modality-matched joint speech and text model can be
leveraged to train a massively multilingual ASR model without any supervised
(manually transcribed) speech for some languages. This paper explores the use
of jointly learnt speech and text representations in a massively multilingual,
zero supervised speech, real-world setting to expand the set of languages
covered by ASR with only unlabeled speech and text in the target languages.
Using the FLEURS dataset, we define the task to cover $102$ languages, where
transcribed speech is available in $52$ of these languages and can be used to
improve end-to-end ASR quality on the remaining $50$. First, we show that by
combining speech representations with byte-level text representations and use
of language embeddings, we can dramatically reduce the Character Error Rate
(CER) on languages with no supervised speech from 64.8\% to 30.8\%, a relative
reduction of 53\%. Second, using a subset of South Asian languages we show that
Maestro-U can promote knowledge transfer from languages with supervised speech
even when there is limited to no graphemic overlap. Overall, Maestro-U closes
the gap to oracle performance by 68.5\% relative and reduces the CER of 19
languages below 15\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering. (arXiv:2210.10176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10176">
<div class="article-summary-box-inner">
<span><p>Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Type-supervised sequence labeling based on the heterogeneous star graph for named entity recognition. (arXiv:2210.10240v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10240">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a fundamental task in natural language
processing, identifying the span and category of entities in unstructured
texts. The traditional sequence labeling methodology ignores the nested
entities, i.e. entities included in other entity mentions. Many approaches
attempt to address this scenario, most of which rely on complex structures or
have high computation complexity. The representation learning of the
heterogeneous star graph containing text nodes and type nodes is investigated
in this paper. In addition, we revise the graph attention mechanism into a
hybrid form to address its unreasonableness in specific topologies. The model
performs the type-supervised sequence labeling after updating nodes in the
graph. The annotation scheme is an extension of the single-layer sequence
labeling and is able to cope with the vast majority of nested entities.
Extensive experiments on public NER datasets reveal the effectiveness of our
model in extracting both flat and nested entities. The method achieved
state-of-the-art performance on both flat and nested datasets. The significant
improvement in accuracy reflects the superiority of the multi-layer labeling
strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continued Pretraining for Better Zero- and Few-Shot Promptability. (arXiv:2210.10258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10258">
<div class="article-summary-box-inner">
<span><p>Recently introduced language model prompting methods can achieve high
accuracy in zero- and few-shot settings while requiring few to no learned
task-specific parameters. Nevertheless, these methods still often trail behind
full model finetuning. In this work, we investigate if a dedicated continued
pretraining stage could improve "promptability", i.e., zero-shot performance
with natural language prompts or few-shot performance with prompt tuning. We
reveal settings where existing continued pretraining methods lack
promptability. We also identify current methodological gaps, which we fill with
thorough large-scale experiments. We demonstrate that a simple recipe,
continued pretraining that incorporates a trainable prompt during multi-task
learning, leads to improved promptability in both zero- and few-shot settings
compared to existing methods, up to 31% relative. On the other hand, we find
that continued pretraining using MAML-style meta-learning, a method that
directly optimizes few-shot promptability, yields subpar performance. We
validate our findings with two prompt tuning methods, and, based on our
results, we provide concrete recommendations to optimize promptability for
different use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models. (arXiv:2210.10289v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10289">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its
variants, have led to significant improvements on various NLP tasks in past
years. However, a theoretical framework for studying their relationships is
still missing. In this paper, we fill this gap by investigating the linear
dependency between pre-trained LMs. The linear dependency of LMs is defined
analogously to the linear dependency of vectors. We propose Language Model
Decomposition (LMD) to represent a LM using a linear combination of other LMs
as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD
similar to the coefficient of determination is defined and used to measure the
linear dependency of a set of LMs. In experiments, we find that BERT and eleven
(11) BERT-like LMs are 91% linearly dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly "correlated". To further advance
SOTA we need more diverse and novel LMs that are less dependent on existing
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revision Transformers: Getting RiT of No-Nos. (arXiv:2210.10332v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10332">
<div class="article-summary-box-inner">
<span><p>Current transformer language models (LM) are large-scale models with billions
of parameters. They have been shown to provide high performances on a variety
of tasks but are also prone to shortcut learning and bias. Addressing such
incorrect model behavior via parameter adjustments is very costly. This is
particularly problematic for updating dynamic concepts, such as moral values,
which vary culturally or interpersonally. In this work, we question the current
common practice of storing all information in the model parameters and propose
the Revision Transformer (RiT) employing information retrieval to facilitate
easy model updating. The specific combination of a large-scale pre-trained LM
that inherently but also diffusely encodes world knowledge with a
clear-structured revision engine makes it possible to update the model's
knowledge with little effort and the help of user interaction. We exemplify RiT
on a moral dataset and simulate user feedback demonstrating strong performance
in model revision even with small data. This way, users can easily design a
model regarding their preferences, paving the way for more transparent and
personalized AI models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A baseline revisited: Pushing the limits of multi-segment models for context-aware translation. (arXiv:2210.10906v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10906">
<div class="article-summary-box-inner">
<span><p>This paper addresses the task of contextual translation using multi-segment
models. Specifically we show that increasing model capacity further pushes the
limits of this approach and that deeper models are more suited to capture
context dependencies. Furthermore, improvements observed with larger models can
be transferred to smaller models using knowledge distillation. Our experiments
show that this approach achieves competitive performance across several
languages and benchmarks, without additional language-specific tuning and task
specific architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts. (arXiv:2210.11292v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11292">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing
pre-trained models (PTMs) that simply prepends a soft prompt to the input and
only optimizes the prompt to adapt PTMs to downstream tasks. Although it is
parameter- and deployment-efficient, its performance still lags behind other
state-of-the-art PETuning methods. Besides, the training cost of prompt tuning
is not significantly reduced due to the back-propagation through the entire
model. Through empirical analyses, we shed some light on the lagging
performance of prompt tuning and recognize a trade-off between the propagation
distance from label signals to the inserted prompt and the influence of the
prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that
inserts a late prompt into an intermediate layer of the PTM instead of the
input layer or all layers. The late prompt is obtained by a neural prompt
generator conditioned on the hidden states before the prompt insertion layer
and therefore is instance-dependent. Through extensive experimental results
across various tasks and PTMs, we show that LPT can achieve competitive
performance to full model tuning and other PETuning methods under both
full-data and few-shot scenarios while possessing faster training speed and
lower memory cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11416">
<div class="article-summary-box-inner">
<span><p>Finetuning language models on a collection of datasets phrased as
instructions has been shown to improve model performance and generalization to
unseen tasks. In this paper we explore instruction finetuning with a particular
focus on (1) scaling the number of tasks, (2) scaling the model size, and (3)
finetuning on chain-of-thought data. We find that instruction finetuning with
the above aspects dramatically improves performance on a variety of model
classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and
evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For
instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM
540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
state-of-the-art performance on several benchmarks, such as 75.2% on five-shot
MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong
few-shot performance even compared to much larger models, such as PaLM 62B.
Overall, instruction finetuning is a general method for improving the
performance and usability of pretrained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are E2E ASR models ready for an industrial usage?. (arXiv:2112.12572v2 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12572">
<div class="article-summary-box-inner">
<span><p>The Automated Speech Recognition (ASR) community experiences a major turning
point with the rise of the fully-neural (End-to-End, E2E) approaches. At the
same time, the conventional hybrid model remains the standard choice for the
practical usage of ASR. According to previous studies, the adoption of E2E ASR
in real-world applications was hindered by two main limitations: their ability
to generalize on unseen domains and their high operational cost. In this paper,
we investigate both above-mentioned drawbacks by performing a comprehensive
multi-domain benchmark of several contemporary E2E models and a hybrid
baseline. Our experiments demonstrate that E2E models are viable alternatives
for the hybrid approach, and even outperform the baseline both in accuracy and
in operational efficiency. As a result, our study shows that the generalization
and complexity issues are no longer the major obstacle for industrial
integration, and draws the community's attention to other potential limitations
of the E2E approaches in some specific use-cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions. (arXiv:2201.00768v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00768">
<div class="article-summary-box-inner">
<span><p>Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-24 23:20:33.523984184 UTC">2022-10-24 23:20:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>