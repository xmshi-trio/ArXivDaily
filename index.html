<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-22T01:30:00Z">05-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11186">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), armed with billions of parameters, exhibit
exceptional performance across a wide range of Natural Language Processing
(NLP) tasks. However, they present a significant computational challenge during
inference, especially when deploying on common hardware such as single GPUs. As
such, minimizing the latency of LLM inference by curtailing computational and
memory requirements, though achieved through compression, becomes critically
important. However, this process inevitably instigates a trade-off between
efficiency and accuracy, as compressed LLMs typically experience a reduction in
predictive precision. In this research, we introduce an innovative perspective:
to optimize this trade-off, compressed LLMs require a unique input format that
varies from that of the original models. Our findings indicate that the
generation quality in a compressed LLM can be markedly improved for specific
queries by selecting prompts with precision. Capitalizing on this insight, we
introduce a prompt learning paradigm that cultivates an additive prompt over a
compressed LLM to bolster their accuracy. Our empirical results imply that
through our strategic prompt utilization, compressed LLMs can match, and
occasionally even exceed, the accuracy of the original models. Moreover, we
demonstrated that these learned prompts have a certain degree of
transferability across various datasets, tasks, and compression levels. These
insights shine a light on new possibilities for enhancing the balance between
accuracy and efficiency in LLM inference. Specifically, they underscore the
importance of judicious input editing to a compressed large model, hinting at
potential advancements in scaling LLMs on common hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11206">
<div class="article-summary-box-inner">
<span><p>Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Trends in Unsupervised Summarization. (arXiv:2305.11231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11231">
<div class="article-summary-box-inner">
<span><p>Unsupervised summarization is a powerful technique that enables training
summarizing models without requiring labeled datasets. This survey covers
different recent techniques and models used for unsupervised summarization. We
cover extractive, abstractive, and hybrid models and strategies used to achieve
unsupervised summarization. While the main focus of this survey is on recent
research, we also cover some of the important previous research. We
additionally introduce a taxonomy, classifying different research based on
their approach to unsupervised training. Finally, we discuss the current
approaches and mention some datasets and evaluation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Biases and the Impact of Multilingual Training across Multiple Languages. (arXiv:2305.11242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11242">
<div class="article-summary-box-inner">
<span><p>Studies in bias and fairness in natural language processing have primarily
examined social biases within a single language and/or across few attributes
(e.g. gender, race). However, biases can manifest differently across various
languages for individual attributes. As a result, it is critical to examine
biases within each language and attribute. Of equal importance is to study how
these biases compare across languages and how the biases are affected when
training a model on multilingual data versus monolingual data. We present a
bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the
downstream sentiment analysis task to observe whether specific demographics are
viewed more positively. We study bias similarities and differences across these
languages and investigate the impact of multilingual vs. monolingual training
data. We adapt existing sentiment bias templates in English to Italian,
Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality,
and gender. Our results reveal similarities in bias expression such as
favoritism of groups that are dominant in each language's culture (e.g.
majority religions and nationalities). Additionally, we find an increased
variation in predictions across protected groups, indicating bias
amplification, after multilingual finetuning in comparison to multilingual
pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11243">
<div class="article-summary-box-inner">
<span><p>Developmental psychologists have spent decades devising experiments to test
the intelligence and knowledge of infants and children, tracing the origin of
crucial concepts and capacities. Moreover, experimental techniques in
developmental psychology have been carefully designed to discriminate the
cognitive capacities that underlie particular behaviors. We propose that using
classical experiments from child development is a particularly effective way to
probe the computational abilities of AI models, in general, and LLMs in
particular. First, the methodological techniques of developmental psychology,
such as the use of novel stimuli to control for past experience or control
conditions to determine whether children are using simple associations, can be
equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs
in this way can tell us whether the information that is encoded in text is
sufficient to enable particular responses, or whether those responses depend on
other kinds of information, such as information from exploration of the
physical world. In this work we adapt classical developmental experiments to
evaluate the capabilities of LaMDA, a large language model from Google. We
propose a novel LLM Response Score (LRS) metric which can be used to evaluate
other language models, such as GPT. We find that LaMDA generates appropriate
responses that are similar to those of children in experiments involving social
understanding, perhaps providing evidence that knowledge of these domains is
discovered through language. On the other hand, LaMDA's responses in early
object and action understanding, theory of mind, and especially causal
reasoning tasks are very different from those of young children, perhaps
showing that these domains require more real-world, self-initiated exploration
and cannot simply be learned from patterns in language input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11244">
<div class="article-summary-box-inner">
<span><p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational thematics: Comparing algorithms for clustering the genres of literary fiction. (arXiv:2305.11251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11251">
<div class="article-summary-box-inner">
<span><p>What are the best methods of capturing thematic similarity between literary
texts? Knowing the answer to this question would be useful for automatic
clustering of book genres, or any other thematic grouping. This paper compares
a variety of algorithms for unsupervised learning of thematic similarities
between texts, which we call "computational thematics". These algorithms belong
to three steps of analysis: text preprocessing, extraction of text features,
and measuring distances between the lists of features. Each of these steps
includes a variety of options. We test all the possible combinations of these
options: every combination of algorithms is given a task to cluster a corpus of
books belonging to four pre-tagged genres of fiction. This clustering is then
validated against the "ground truth" genre labels. Such comparison of
algorithms allows us to learn the best and the worst combinations for
computational thematic analysis. To illustrate the sharp difference between the
best and the worst methods, we then cluster 5000 random novels from the
HathiTrust corpus of fiction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11255">
<div class="article-summary-box-inner">
<span><p>While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is at
https://github.com/scofield7419/THOR-ISA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models. (arXiv:2305.11262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11262">
<div class="article-summary-box-inner">
<span><p>\textit{\textbf{\textcolor{red}{Warning}:} This paper contains content that
may be offensive or upsetting.} Pretrained conversational agents have been
exposed to safety issues, exhibiting a range of stereotypical human biases such
as gender bias. However, there are still limited bias categories in current
research, and most of them only focus on English. In this paper, we introduce a
new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese
conversational language models. Apart from those previous well-explored bias
categories, CHBias includes under-explored bias categories, such as ageism and
appearance biases, which received less attention. We evaluate two popular
pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.
Furthermore, to mitigate different biases, we apply several debiasing methods
to the Chinese pretrained models. Experimental results show that these Chinese
pretrained models are potentially risky for generating texts that contain
social biases, and debiasing methods using the proposed dataset can make
response generation less biased while preserving the models' conversational
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11271">
<div class="article-summary-box-inner">
<span><p>Collaborative tasks often begin with partial task knowledge and incomplete
initial plans from each partner. To complete these tasks, agents need to engage
in situated communication with their partners and coordinate their partial
plans towards a complete plan to achieve a joint task goal. While such
collaboration seems effortless in a human-human team, it is highly challenging
for human-AI collaboration. To address this limitation, this paper takes a step
towards collaborative plan acquisition, where humans and agents strive to learn
and communicate with each other to acquire a complete plan for joint tasks.
Specifically, we formulate a novel problem for agents to predict the missing
task knowledge for themselves and for their partners based on rich perceptual
and dialogue history. We extend a situated dialogue benchmark for symmetric
collaborative tasks in a 3D blocks world and investigate computational
strategies for plan acquisition. Our empirical results suggest that predicting
the partner's missing knowledge is a more viable approach than predicting one's
own. We show that explicit modeling of the partner's dialogue moves and mental
states produces improved and more stable results than without. These results
provide insight for future AI agents that can predict what knowledge their
partner is missing and, therefore, can proactively communicate such information
to help their partner acquire such missing knowledge toward a common
understanding of joint tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution. (arXiv:2305.11315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11315">
<div class="article-summary-box-inner">
<span><p>Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{https://github.com/clulab/geonorm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11317">
<div class="article-summary-box-inner">
<span><p>The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11326">
<div class="article-summary-box-inner">
<span><p>Tabular data is the most common format to publish and exchange structured
data online. A clear example is the growing number of open data portals
published by all types of public administrations. However, exploitation of
these data sources is currently limited to technical people able to
programmatically manipulate and digest such data. As an alternative, we propose
the use of chatbots to offer a conversational interface to facilitate the
exploration of tabular data sources. With our approach, any regular citizen can
benefit and leverage them. Moreover, our chatbots are not manually created:
instead, they are automatically generated from the data source itself thanks to
the instantiation of a configurable collection of conversation patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11334">
<div class="article-summary-box-inner">
<span><p>We introduce two novel methods, Tree-Search and Self-contextualizing QA,
designed to enhance the performance of large language models (LLMs) in
question-answering tasks. Tree-Search is a sampling technique specifically
created to extract diverse information from an LLM for a given prompt.
Self-contextualizing QA leverages Tree-Search to enable the model to create its
own context using a wide range of information relevant to the prompt, evaluate
it explicitly and return a open book answer to the initial prompt . We
demonstrate that the quality of generated answers improves according to various
metrics, including accuracy, informativeness, coherence, and consistency, as
evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods
result in increased robustness and that performance is positively correlated
with tree size, benefiting both answer quality and robustness. Finally, we
discuss other promising applications of Tree-Search, highlighting its potential
to enhance a broad range of tasks beyond question-answering.
</p>
<p>\noindent We also discuss several areas for future work, including refining
the Tree-Search and Self-Contextualizing QA methods, improving the coherence of
the generated context, and investigating the impact of bootstrapping on model
robustness
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11348">
<div class="article-summary-box-inner">
<span><p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11349">
<div class="article-summary-box-inner">
<span><p>The emergence of social media as one of the main platforms for people to
access news has enabled the wide dissemination of fake news. This has motivated
numerous studies on automating fake news detection. Although there have been
limited attempts at unsupervised fake news detection, their performance suffers
due to not exploiting the knowledge from various modalities related to news
records and due to the presence of various latent biases in the existing news
datasets. To address these limitations, this work proposes an effective
framework for unsupervised fake news detection, which first embeds the
knowledge available in four modalities in news records and then proposes a
novel noise-robust self-supervised learning technique to identify the veracity
of news records from the multi-modal embeddings. Also, we propose a novel
technique to construct news datasets minimizing the latent biases in existing
news datasets. Following the proposed approach for dataset construction, we
produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles
related to COVID-19, acronymed as LUND-COVID. We trained the proposed
unsupervised framework using LUND-COVID to exploit the potential of large
datasets, and evaluate it using a set of existing labelled datasets. Our
results show that the proposed unsupervised framework largely outperforms
existing unsupervised baselines for different tasks such as multi-modal fake
news detection, fake news early detection and few-shot fake news detection,
while yielding notable improvements for unseen domains during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11351">
<div class="article-summary-box-inner">
<span><p>Deep generative models are known to produce undesirable samples such as
harmful content. Traditional mitigation methods include re-training from
scratch, filtering, or editing; however, these are either computationally
expensive or can be circumvented by third parties. In this paper, we take a
different approach and study how to post-edit an already-trained conditional
generative model so that it redacts certain conditionals that will, with high
probability, lead to undesirable content. This is done by distilling the
conditioning network in the models, giving a solution that is effective,
efficient, controllable, and universal for a class of deep generative models.
We conduct experiments on redacting prompts in text-to-image models and
redacting voices in text-to-speech models. Our method is computationally light,
leads to better redaction quality and robustness than baseline methods while
still retaining high generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MD3: The Multi-Dialect Dataset of Dialogues. (arXiv:2305.11355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11355">
<div class="article-summary-box-inner">
<span><p>We introduce a new dataset of conversational speech representing English from
India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues
(MD3) strikes a new balance between open-ended conversational speech and
task-oriented dialogue by prompting participants to perform a series of short
information-sharing tasks. This facilitates quantitative cross-dialectal
comparison, while avoiding the imposition of a restrictive task structure that
might inhibit the expression of dialect features. Preliminary analysis of the
dataset reveals significant differences in syntax and in the use of discourse
markers. The dataset, which will be made publicly available with the
publication of this paper, includes more than 20 hours of audio and more than
200,000 orthographically-transcribed tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11364">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
shorturl.at/zHOUV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11366">
<div class="article-summary-box-inner">
<span><p>Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60\% winning rate against the GPT-3.5
baselines via human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing tradeoffs between teaching via language and demonstrations in multi-agent systems. (arXiv:2305.11374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11374">
<div class="article-summary-box-inner">
<span><p>Humans teach others about the world through language and demonstration. When
might one of these modalities be more effective than the other? In this work,
we study the factors that modulate the effectiveness of language vs.
demonstration using multi-agent systems to model human communication.
Specifically, we train neural network agents to teach via language or
demonstration in a grounded communication task, manipulating 1) the inherent
difficulty of the task and 2) the competence of the teacher. We find that
teaching by demonstration is more effective in the simplest settings, but
language is more effective as task difficulty increases, due to its ability to
generalize more effectively to unseen scenarios. Overall, these results provide
converging evidence for a tradeoff between language and demonstration as
teaching modalities in humans, and make the novel predictions that
demonstration may be optimal for easy tasks, while language enables
generalization in more challenging settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding. (arXiv:2305.11392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11392">
<div class="article-summary-box-inner">
<span><p>Transformers achieve promising performance in document understanding because
of their high effectiveness and still suffer from quadratic computational
complexity dependency on the sequence length. General efficient transformers
are challenging to be directly adapted to model document. They are unable to
handle the layout representation in documents, e.g. word, line and paragraph,
on different granularity levels and seem hard to achieve a good trade-off
between efficiency and performance. To tackle the concerns, we propose
Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT
algorithm with an hourglass transformer architecture, for visual document
understanding. Specifically, we design a modality-guided dynamic token merging
block to make the model learn multi-granularity representation and prunes
redundant tokens. Additionally, we present a multi-modal interaction module
called Symmetry Cross Attention (SCA) to consider multi-modal fusion and
efficiently guide the token mergence. The SCA allows one modality input as
query to calculate cross attention with another modality in a dual phase.
Extensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our
model achieves the state-of-the-art performance and almost 1.9X faster
inference time than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comfort Foods and Community Connectedness: Investigating Diet Change during COVID-19 Using YouTube Videos on Twitter. (arXiv:2305.11398v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11398">
<div class="article-summary-box-inner">
<span><p>Unprecedented lockdowns at the start of the COVID-19 pandemic have
drastically changed the routines of millions of people, potentially impacting
important health-related behaviors. In this study, we use YouTube videos
embedded in tweets about diet, exercise and fitness posted before and during
COVID-19 to investigate the influence of the pandemic lockdowns on diet and
nutrition. In particular, we examine the nutritional profile of the foods
mentioned in the transcript, description and title of each video in terms of
six macronutrients (protein, energy, fat, sodium, sugar, and saturated fat).
These macronutrient values were further linked to demographics to assess if
there are specific effects on those potentially having insufficient access to
healthy sources of food. Interrupted time series analysis revealed a
considerable shift in the aggregated macronutrient scores before and during
COVID-19. In particular, whereas areas with lower incomes showed decrease in
energy, fat, and saturated fat, those with higher percentage of African
Americans showed an elevation in sodium. Word2Vec word similarities and odds
ratio analysis suggested a shift from popular diets and lifestyle bloggers
before the lockdowns to the interest in a variety of healthy foods, communal
sharing of quick and easy recipes, as well as a new emphasis on comfort foods.
To the best of our knowledge, this work is novel in terms of linking attention
signals in tweets, content of videos, their nutrients profile, and aggregate
demographics of the users. The insights made possible by this combination of
resources are important for monitoring the secondary health effects of social
distancing, and informing social programs designed to alleviate these effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11408">
<div class="article-summary-box-inner">
<span><p>Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUB: Discrete Unit Back-translation for Speech Translation. (arXiv:2305.11411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11411">
<div class="article-summary-box-inner">
<span><p>How can speech-to-text translation (ST) perform as well as machine
translation (MT)? The key point is to bridge the modality gap between speech
and text so that useful MT techniques can be applied to ST. Recently, the
approach of representing speech with unsupervised discrete units yields a new
way to ease the modality problem. This motivates us to propose Discrete Unit
Back-translation (DUB) to answer two questions: (1) Is it better to represent
speech with discrete units than with continuous features in direct ST? (2) How
much benefit can useful MT techniques bring to ST? With DUB, the
back-translation technique can successfully be applied on direct ST and obtains
an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource
language scenario, our method achieves comparable performance to existing
methods that rely on large-scale external data. Code and models are available
at https://github.com/0nutation/DUB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11426">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex tasks. Moreover, recent research has shown that
incorporating human-annotated rationales (e.g., Chain-of- Thought prompting)
during in-context learning can significantly enhance the performance of these
models, particularly on tasks that require reasoning capabilities. However,
incorporating such rationales poses challenges in terms of scalability as this
requires a high degree of human involvement. In this work, we present a novel
framework, Amplifying Model Performance by Leveraging In-Context Learning with
Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges
by automating the process of rationale generation. To this end, we leverage
post hoc explanation methods which output attribution scores (explanations)
capturing the influence of each of the input features on model predictions.
More specifically, we construct automated natural language rationales that
embed insights from post hoc explanations to provide corrective signals to
LLMs. Extensive experimentation with real-world datasets demonstrates that our
framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%
over a wide range of tasks, including those where prior approaches which rely
on human-annotated rationales such as Chain-of-Thought prompting fall short.
Our work makes one of the first attempts at highlighting the potential of post
hoc explanations as valuable tools for enhancing the effectiveness of LLMs.
Furthermore, we conduct additional empirical analyses and ablation studies to
demonstrate the impact of each of the components of AMPLIFY, which, in turn,
lead to critical insights for refining in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11430">
<div class="article-summary-box-inner">
<span><p>While LLMs have shown great success in understanding and generating text in
traditional conversational settings, their potential for performing ill-defined
complex tasks is largely under-studied. Indeed, we are yet to conduct
comprehensive benchmarking studies with multiple LLMs that are exclusively
focused on a complex task. However, conducting such benchmarking studies is
challenging because of the large variations in LLMs' performance when different
prompt types/styles are used and different degrees of detail are provided in
the prompts. To address this issue, the paper proposes a general taxonomy that
can be used to design prompts with specific properties in order to perform a
wide range of complex tasks. This taxonomy will allow future benchmarking
studies to report the specific categories of prompts used as part of the study,
enabling meaningful comparisons across different studies. Also, by establishing
a common standard through this taxonomy, researchers will be able to draw more
accurate conclusions about LLMs' performance on a specific complex task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11435">
<div class="article-summary-box-inner">
<span><p>In this paper, we show that representations capturing syllabic units emerge
when training a self-supervised speech model with a visually-grounded training
objective. We demonstrate that a nearly identical model architecture (HuBERT)
trained with a masked language modeling loss does not exhibit this same
ability, suggesting that the visual grounding objective is responsible for the
emergence of this phenomenon. We propose the use of a minimum cut algorithm to
automatically predict syllable boundaries in speech, followed by a 2-stage
clustering method to group identical syllables together. We show that our model
not only outperforms a state-of-the-art syllabic segmentation method on the
language it was trained on (English), but also generalizes in a zero-shot
fashion to Estonian. Finally, we show that the same model is capable of
zero-shot generalization for a word segmentation task on 4 other languages from
the Zerospeech Challenge, in some cases beating the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring. (arXiv:2305.11438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11438">
<div class="article-summary-box-inner">
<span><p>Speech fluency/disfluency can be evaluated by analyzing a range of phonetic
and prosodic features. Deep neural networks are commonly trained to map
fluency-related features into the human scores. However, the effectiveness of
deep learning-based models is constrained by the limited amount of labeled
training samples. To address this, we introduce a self-supervised learning
(SSL) approach that takes into account phonetic and prosody awareness for
fluency scoring. Specifically, we first pre-train the model using a
reconstruction loss function, by masking phones and their durations jointly on
a large amount of unlabeled speech and text prompts. We then fine-tune the
pre-trained model using human-annotated scoring data. Our experimental results,
conducted on datasets such as Speechocean762 and our non-native datasets, show
that our proposed method outperforms the baseline systems in terms of Pearson
correlation coefficients (PCC). Moreover, we also conduct an ablation study to
better understand the contribution of phonetic and prosody factors during the
pre-training stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11442">
<div class="article-summary-box-inner">
<span><p>Existing solutions to zero-shot text classification either conduct prompting
with pre-trained language models, which is sensitive to the choices of
templates, or rely on large-scale annotated data of relevant tasks for
meta-tuning. In this work, we propose a new paradigm based on self-supervised
learning to solve zero-shot text classification tasks by tuning the language
models with unlabeled data, called self-supervised tuning. By exploring the
inherent structure of free texts, we propose a new learning objective called
first sentence prediction to bridge the gap between unlabeled data and text
classification tasks. After tuning the model to learn to predict the first
sentence in a paragraph based on the rest, the model is able to conduct
zero-shot inference on unseen tasks such as topic classification and sentiment
analysis. Experimental results show that our model outperforms the
state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals
that our model is less sensitive to the prompt design. Our code and pre-trained
models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arukikata Travelogue Dataset. (arXiv:2305.11444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11444">
<div class="article-summary-box-inner">
<span><p>We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast. (arXiv:2305.11449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11449">
<div class="article-summary-box-inner">
<span><p>Existing research has shown that a multilingual pre-trained language model
fine-tuned with one (source) language also performs well on downstream tasks
for non-source languages, even though no fine-tuning is done on these
languages. However, there is a clear gap between the performance of the source
language and that of the non-source languages. This paper analyzes the
fine-tuning process, discovers when the performance gap changes and identifies
which network weights affect the overall performance most. Additionally, the
paper seeks to answer to what extent the gap can be reduced by reducing
forgetting. Based on the analysis results, a method named Fine-tuning slow and
fast with four training policies is proposed to address these issues.
Experimental results show the proposed method outperforms baselines by a clear
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11455">
<div class="article-summary-box-inner">
<span><p>A centerpiece of the ever-popular reinforcement learning from human feedback
(RLHF) approach to fine-tuning autoregressive language models is the explicit
training of a reward model to emulate human feedback, distinct from the
language model itself. This reward model is then coupled with policy-gradient
methods to dramatically improve the alignment between language model outputs
and desired responses. In this work, we adopt a novel perspective wherein a
pre-trained language model is itself simultaneously a policy, reward function,
and transition function. An immediate consequence of this is that reward
learning and language model fine-tuning can be performed jointly and directly,
without requiring any further downstream policy optimization. While this
perspective does indeed break the traditional agent-environment interface, we
nevertheless maintain that there can be enormous statistical benefits afforded
by bringing to bear traditional algorithmic concepts from reinforcement
learning. Our experiments demonstrate one concrete instance of this through
efficient exploration based on the representation and resolution of epistemic
uncertainty. In order to illustrate these ideas in a transparent manner, we
restrict attention to a simple didactic data generating process and leave for
future work extension to systems of practical scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions. (arXiv:2305.11460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11460">
<div class="article-summary-box-inner">
<span><p>Finding an agreement among diverse opinions is a challenging topic in
multiagent systems. Recently, large language models (LLMs) have shown great
potential in addressing this challenge due to their remarkable capabilities in
comprehending human opinions and generating human-like text. However, they
typically rely on extensive human-annotated data. In this paper, we propose
Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find
agreement using data generated by LLM itself. Specifically, our approach
employs the generative pre-trained transformer-3 (GPT-3) to generate multiple
opinions for each question in a question dataset and create several agreement
candidates among these opinions. Then, a bidirectional encoder representations
from transformers (BERT)-based model evaluates the agreement score of each
agreement candidate and selects the one with the highest agreement score. This
process yields a dataset of question-opinion-agreements, which we use to
fine-tune a pre-trained LLM for discovering agreements among diverse opinions.
Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework
achieves comparable performance to GPT-3 with only 1/25 of its parameters,
showcasing its ability to identify agreement among various opinions without the
need for human-annotated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Memory for Language Modelling. (arXiv:2305.11462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11462">
<div class="article-summary-box-inner">
<span><p>Breakthroughs in deep learning and memory networks have made major advances
in natural language understanding. Language is sequential and information
carried through the sequence can be captured through memory networks. Learning
the sequence is one of the key aspects in learning the language. However,
memory networks are not capable of holding infinitely long sequences in their
memories and are limited by various constraints such as the vanishing or
exploding gradient problem. Therefore, natural language understanding models
are affected when presented with long sequential text. We introduce Long Term
Memory network (LTM) to learn from infinitely long sequences. LTM gives
priority to the current inputs to allow it to have a high impact. Language
modeling is an important factor in natural language understanding. LTM was
tested in language modeling, which requires long term memory. LTM is tested on
Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We
compare LTM with other language models which require long term memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11473">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently soared in popularity due to their
ease of access and the unprecedented intelligence exhibited on diverse
applications. However, LLMs like ChatGPT present significant limitations in
supporting complex information tasks due to the insufficient affordances of the
text-based medium and linear conversational structure. Through a formative
study with ten participants, we found that LLM interfaces often present
long-winded responses, making it difficult for people to quickly comprehend and
interact flexibly with various pieces of information, particularly during more
complex tasks. We present Graphologue, an interactive system that converts
text-based responses from LLMs into graphical diagrams to facilitate
information-seeking and question-answering tasks. Graphologue employs novel
prompting strategies and interface designs to extract entities and
relationships from LLM responses and constructs node-link diagrams in
real-time. Further, users can interact with the diagrams to flexibly adjust the
graphical presentation and to submit context-specific prompts to obtain more
information. Utilizing diagrams, Graphologue enables graphical, non-linear
dialogues between humans and LLMs, facilitating information exploration,
organization, and comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCGen: Explainable Complementary Concept Generation in E-Commerce. (arXiv:2305.11480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11480">
<div class="article-summary-box-inner">
<span><p>We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., "Digital Cameras", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona. (arXiv:2305.11482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11482">
<div class="article-summary-box-inner">
<span><p>The personalized dialogue explores the consistent relationship between
dialogue generation and personality. Existing personalized dialogue agents
model persona profiles from three resources: sparse or dense persona
descriptions and dialogue histories. However, sparse structured persona
attributes are explicit but uninformative, dense persona texts contain rich
persona descriptions with much noise, and dialogue history query is both noisy
and uninformative for persona modeling. In this work, we combine the advantages
of the three resources to obtain a richer and more accurate persona. We design
a Contrastive Latent Variable-based model (CLV) that clusters the dense persona
descriptions into sparse categories, which are combined with the history query
to generate personalized responses. Experimental results on Chinese and English
datasets demonstrate our model's superiority in personalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11490">
<div class="article-summary-box-inner">
<span><p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding. (arXiv:2305.11497v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11497">
<div class="article-summary-box-inner">
<span><p>Prompt tuning has achieved great success in transferring the knowledge from
large pretrained vision-language models into downstream tasks, and has
dominated the performance on visual grounding (VG). However, almost all
existing prompt tuning paradigms suffer from poor interpretability. In this
paper, we argue that their poor interpretability is attributed to the holistic
prompt generation and inference process. By "holistic", we mean that they
usually directly learn a set of vectors as the prompt (i.e., prompt
generation), and use the learned global prompt to augment the textual input for
the VG model (i.e., prompt inference). To this end, we propose a new prompt
construction paradigm with explicit explainable ability, named TreePrompt.
Specifically, we first deconstruct a complex sentence into a tree, that is
consistent with human reasoning. Then, following the syntax tree, we compose a
structured prompt in a bottom-up manner. Thanks to this step-by-step prompt
construction process, each intermediate prompt (i.e., tree node) permits us to
understand the reasoning process. Extensive ablations on various backbones and
benchmarks consistently demonstrate the effectiveness and interpretability of
our TreePrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11498">
<div class="article-summary-box-inner">
<span><p>Event Extraction (EE), aiming to identify and classify event triggers and
arguments from event mentions, has benefited from pre-trained language models
(PLMs). However, existing PLM-based methods ignore the information of
trigger/argument fields, which is crucial for understanding event schemas. To
this end, we propose a Probabilistic reCoupling model enhanced Event extraction
framework (ProCE). Specifically, we first model the syntactic-related event
fields as probabilistic biases, to clarify the event fields from ambiguous
entanglement. Furthermore, considering multiple occurrences of the same
triggers/arguments in EE, we explore probabilistic interaction strategies among
multiple fields of the same triggers/arguments, to recouple the corresponding
clarified distributions and capture more latent information fields. Experiments
on EE datasets demonstrate the effectiveness and generalization of our proposed
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11499">
<div class="article-summary-box-inner">
<span><p>Large language Models (LLMs) have achieved promising performance on
arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting. However, LLMs face challenges in maintaining factual consistency
during reasoning, exhibiting tendencies to condition overlooking, question
misinterpretation, and condition hallucination over given problems. Existing
methods use coarse-grained feedback (e.g., whether the answer is correct) to
improve factual consistency. In this work, we propose RCoT (Reversing
Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by
automatically detecting and rectifying factual inconsistency in LLMs' generated
solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct
the problem based on generated solutions. Then fine-grained comparisons between
the original problem and the reconstructed problem expose the factual
inconsistency in the original solutions. To rectify the solution, RCoT
formulates detected factual inconsistency into fine-grained feedback to guide
LLMs in revising solutions. Experimental results demonstrate consistent
improvements of RCoT over standard CoT across seven arithmetic datasets.
Moreover, we find that manually written fine-grained feedback can dramatically
improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on
GSM8K), encouraging the community to further explore the fine-grained feedback
generation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment. (arXiv:2305.11501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11501">
<div class="article-summary-box-inner">
<span><p>Entity Alignment (EA) aims to find the equivalent entities between two
Knowledge Graphs (KGs). Existing methods usually encode the triples of entities
as embeddings and learn to align the embeddings, which prevents the direct
interaction between the original information of the cross-KG entities.
Moreover, they encode the relational triples and attribute triples of an entity
in heterogeneous embedding spaces, which prevents them from helping each other.
In this paper, we transform both triples into unified textual sequences, and
model the EA task as a bi-directional textual entailment task between the
sequences of cross-KG entities. Specifically, we feed the sequences of two
entities simultaneously into a pre-trained language model (PLM) and propose two
kinds of PLM-based entity aligners that model the entailment probability
between sequences as the similarity between entities. Our approach captures the
unified correlation pattern of two kinds of information between entities, and
explicitly models the fine-grained interaction between original entity
information. The experiments on five cross-lingual EA datasets show that our
approach outperforms the state-of-the-art EA methods and enables the mutual
enhancement of the heterogeneous information. Codes are available at
https://github.com/OreOZhao/TEA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Topic-aware Summarization Framework with Different Modal Side Information. (arXiv:2305.11503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11503">
<div class="article-summary-box-inner">
<span><p>Automatic summarization plays an important role in the exponential document
growth on the Web. On content websites such as CNN.com and WikiHow.com, there
often exist various kinds of side information along with the main document for
attention attraction and easier understanding, such as videos, images, and
queries. Such information can be used for better summarization, as they often
explicitly or implicitly mention the essence of the article. However, most of
the existing side-aware summarization methods are designed to incorporate
either single-modal or multi-modal side information, and cannot effectively
adapt to each other. In this paper, we propose a general summarization
framework, which can flexibly incorporate various modalities of side
information. The main challenges in designing a flexible summarization model
with side information include: (1) the side information can be in textual or
visual format, and the model needs to align and unify it with the document into
the same semantic space, (2) the side inputs can contain information from
various aspects, and the model should recognize the aspects useful for
summarization. To address these two challenges, we first propose a unified
topic encoder, which jointly discovers latent topics from the document and
various kinds of side information. The learned topics flexibly bridge and guide
the information flow between multiple inputs in a graph encoder through a
topic-aware interaction. We secondly propose a triplet contrastive learning
mechanism to align the single-modal or multi-modal information into a unified
semantic space, where the summary quality is enhanced by better understanding
the document and side information. Results show that our model significantly
surpasses strong baselines on three public single-modal or multi-modal
benchmark summarization datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11508">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems aim to provide accurate answers to patients,
necessitating specific domain knowledge. Recent advancements in Large Language
Models (LLMs) have demonstrated their exceptional capabilities in the medical
Q&amp;A domain, indicating a rich understanding of common sense. However, LLMs are
insufficient for direct diagnosis due to the absence of diagnostic strategies.
The conventional approach to address this challenge involves expensive
fine-tuning of LLMs. Alternatively, a more appealing solution is the
development of a plugin that empowers LLMs to perform medical conversation
tasks. Drawing inspiration from in-context learning, we propose PlugMed, a
Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue
actions by LLMs through two modules: the prompt generation (PG) module and the
response ranking (RR) module. The PG module is designed to capture dialogue
information from both global and local perspectives. It selects suitable
prompts by assessing their similarity to the entire dialogue history and recent
utterances grouped by patient symptoms, respectively. Additionally, the RR
module incorporates fine-tuned SLMs as response filters and selects appropriate
responses generated by LLMs. Moreover, we devise a novel evaluation method
based on intent and medical entities matching to assess the efficacy of
dialogue strategies in medical conversations more effectively. Experimental
evaluations conducted on three unlabeled medical dialogue datasets, including
both automatic and manual assessments, demonstrate that our model surpasses the
strong fine-tuning baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Word Vector-based Methods for Discovering Semantic Differences with No Training nor Word Alignment. (arXiv:2305.11516v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11516">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose methods for discovering semantic differences in
words appearing in two corpora based on the norms of contextualized word
vectors. The key idea is that the coverage of meanings is reflected in the norm
of its mean word vector. The proposed methods do not require the assumptions
concerning words and corpora for comparison that the previous methods do. All
they require are to compute the mean vector of contextualized word vectors and
its norm for each word type. Nevertheless, they are (i) robust for the skew in
corpus size; (ii) capable of detecting semantic differences in infrequent
words; and (iii) effective in pinpointing word instances that have a meaning
missing in one of the two corpora for comparison. We show these advantages for
native and non-native English corpora and also for historical corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion. (arXiv:2305.11517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11517">
<div class="article-summary-box-inner">
<span><p>Diffusion models have emerged as the new state-of-the-art family of deep
generative models, and their promising potentials for text generation have
recently attracted increasing attention. Existing studies mostly adopt a single
encoder architecture with partially noising processes for conditional text
generation, but its degree of flexibility for conditional modeling is limited.
In fact, the encoder-decoder architecture is naturally more flexible for its
detachable encoder and decoder modules, which is extensible to multilingual and
multimodal generation tasks for conditions and target texts. However, the
encoding process of conditional texts lacks the understanding of target texts.
To this end, a spiral interaction architecture for encoder-decoder text
diffusion (DiffuSIA) is proposed. Concretely, the conditional information from
encoder is designed to be captured by the diffusion decoder, while the target
information from decoder is designed to be captured by the conditional encoder.
These two types of information flow run through multilayer interaction spirally
for deep fusion and understanding. DiffuSIA is evaluated on four text
generation tasks, including paraphrase, text simplification, question
generation, and open-domain dialogue generation. Experimental results show that
DiffuSIA achieves competitive performance among previous methods on all four
tasks, demonstrating the effectiveness and generalization ability of the
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11527">
<div class="article-summary-box-inner">
<span><p>We introduce a new Information Extraction (IE) task dubbed Instruction-based
IE, which aims to ask the system to follow specific instructions or guidelines
to extract information. To facilitate research in this area, we construct a
dataset called InstructIE, consisting of 270,000 weakly supervised data from
Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We
further evaluate the performance of various baseline models on the InstructIE
dataset. The results reveal that although current models exhibit promising
performance, there is still room for improvement. Furthermore, we conduct a
comprehensive case study analysis, underlining the challenges inherent in the
Instruction-based IE task. Code and dataset are available at
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sequence-to-Sequence Approach for Arabic Pronoun Resolution. (arXiv:2305.11529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11529">
<div class="article-summary-box-inner">
<span><p>This paper proposes a sequence-to-sequence learning approach for Arabic
pronoun resolution, which explores the effectiveness of using advanced natural
language processing (NLP) techniques, specifically Bi-LSTM and the BERT
pre-trained Language Model, in solving the pronoun resolution problem in
Arabic. The proposed approach is evaluated on the AnATAr dataset, and its
performance is compared to several baseline models, including traditional
machine learning models and handcrafted feature-based models. Our results
demonstrate that the proposed model outperforms the baseline models, which
include KNN, logistic regression, and SVM, across all metrics. In addition, we
explore the effectiveness of various modifications to the model, including
concatenating the anaphor text beside the paragraph text as input, adding a
mask to focus on candidate scores, and filtering candidates based on gender and
number agreement with the anaphor. Our results show that these modifications
significantly improve the model's performance, achieving up to 81% on MRR and
71% for F1 score while also demonstrating higher precision, recall, and
accuracy. These findings suggest that the proposed model is an effective
approach to Arabic pronoun resolution and highlights the potential benefits of
leveraging advanced NLP neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PORTRAIT: a hybrid aPproach tO cReate extractive ground-TRuth summAry for dIsaster evenT. (arXiv:2305.11536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11536">
<div class="article-summary-box-inner">
<span><p>Disaster summarization approaches provide an overview of the important
information posted during disaster events on social media platforms, such as,
Twitter. However, the type of information posted significantly varies across
disasters depending on several factors like the location, type, severity, etc.
Verification of the effectiveness of disaster summarization approaches still
suffer due to the lack of availability of good spectrum of datasets along with
the ground-truth summary. Existing approaches for ground-truth summary
generation (ground-truth for extractive summarization) relies on the wisdom and
intuition of the annotators. Annotators are provided with a complete set of
input tweets from which a subset of tweets is selected by the annotators for
the summary. This process requires immense human effort and significant time.
Additionally, this intuition-based selection of the tweets might lead to a high
variance in summaries generated across annotators. Therefore, to handle these
challenges, we propose a hybrid (semi-automated) approach (PORTRAIT) where we
partly automate the ground-truth summary generation procedure. This approach
reduces the effort and time of the annotators while ensuring the quality of the
created ground-truth summary. We validate the effectiveness of PORTRAIT on 5
disaster events through quantitative and qualitative comparisons of
ground-truth summaries generated by existing intuitive approaches, a
semi-automated approach, and PORTRAIT. We prepare and release the ground-truth
summaries for 5 disaster events which consist of both natural and man-made
disaster events belonging to 4 different countries. Finally, we provide a study
about the performance of various state-of-the-art summarization approaches on
the ground-truth summaries generated by PORTRAIT using ROUGE-N F1-scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots. (arXiv:2305.11540v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11540">
<div class="article-summary-box-inner">
<span><p>Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11541">
<div class="article-summary-box-inner">
<span><p>Large Language Model (LLM) has gained popularity and achieved remarkable
results in open-domain tasks, but its performance in real industrial
domain-specific scenarios is average since there is no specific knowledge in
it. This issue has attracted widespread attention, but there are few relevant
benchmarks available. In this paper, we provide a benchmark Question Answering
(QA) dataset named MSQA, which is about Microsoft products and IT technical
problems encountered by customers. This dataset contains industry
cloud-specific QA knowledge, which is not available for general LLM, so it is
well suited for evaluating methods aimed at improving domain-specific
capabilities of LLM. In addition, we propose a new model interaction paradigm
that can empower LLM to achieve better performance on domain-specific tasks
where it is not proficient. Extensive experiments demonstrate that the approach
following our model fusion framework outperforms the commonly used LLM with
retrieval methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11543">
<div class="article-summary-box-inner">
<span><p>As the foundation of current natural language processing methods, pre-trained
language model has achieved excellent performance. However, the black-box
structure of the deep neural network in pre-trained language models seriously
limits the interpretability of the language modeling process. After revisiting
the coupled requirement of deep neural representation and semantics logic of
language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by
introducing the alignment processing between uninterpretable neural
representation and interpretable statistical logic. Moreover, a clustering
process is also designed to connect the word- and context-level semantics.
Specifically, an associative knowledge network (AKN), considered interpretable
statistical logic, is introduced in the alignment process for word-level
semantics. Furthermore, the context-relative distance is employed as the
semantic feature for the downstream classifier, which is greatly different from
the current uninterpretable semantic representations of pre-trained models. Our
experiments for performance evaluation and interpretable analysis are executed
on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a
novel evaluation strategy for the interpretability of machine learning models
is first proposed. According to the experimental results, our language model
can achieve better performance and highly credible interpretable ability
compared to related state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11550">
<div class="article-summary-box-inner">
<span><p>We argue that translation quality alone is not a sufficient metric for
measuring knowledge transfer in multilingual neural machine translation. To
support this claim, we introduce Representational Transfer Potential (RTP),
which measures representational similarities between languages. We show that
RTP can measure both positive and negative transfer (interference), and find
that RTP is strongly correlated with changes in translation quality, indicating
that transfer does occur. Furthermore, we investigate data and language
characteristics that are relevant for transfer, and find that multi-parallel
overlap is an important yet under-explored feature. Based on this, we develop a
novel training scheme, which uses an auxiliary similarity loss that encourages
representations to be more invariant across languages by taking advantage of
multi-parallel data. We show that our method yields increased translation
quality for low- and mid-resource languages across multiple data and model
setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information. (arXiv:2305.11553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11553">
<div class="article-summary-box-inner">
<span><p>The abstracts of scientific papers consist of premises and conclusions.
Structured abstracts explicitly highlight the conclusion sentences, whereas
non-structured abstracts may have conclusion sentences at uncertain positions.
This implicit nature of conclusion positions makes the automatic segmentation
of scientific abstracts into premises and conclusions a challenging task. In
this work, we empirically explore using Normalized Mutual Information (NMI) for
abstract segmentation. We consider each abstract as a recurrent cycle of
sentences and place segmentation boundaries by greedily optimizing the NMI
score between premises and conclusions. On non-structured abstracts, our
proposed unsupervised approach GreedyCAS achieves the best performance across
all evaluation metrics; on structured abstracts, GreedyCAS outperforms all
baseline methods measured by $P_k$. The strong correlation of NMI to our
evaluation metrics reveals the effectiveness of NMI for abstract segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11554">
<div class="article-summary-box-inner">
<span><p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blank-regularized CTC for Frame Skipping in Neural Transducer. (arXiv:2305.11558v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11558">
<div class="article-summary-box-inner">
<span><p>Neural Transducer and connectionist temporal classification (CTC) are popular
end-to-end automatic speech recognition systems. Due to their frame-synchronous
design, blank symbols are introduced to address the length mismatch between
acoustic frames and output tokens, which might bring redundant computation.
Previous studies managed to accelerate the training and inference of neural
Transducers by discarding frames based on the blank symbols predicted by a
co-trained CTC. However, there is no guarantee that the co-trained CTC can
maximize the ratio of blank symbols. This paper proposes two novel
regularization methods to explicitly encourage more blanks by constraining the
self-loop of non-blank symbols in the CTC. It is interesting to find that the
frame reduction ratio of the neural Transducer can approach the theoretical
boundary. Experiments on LibriSpeech corpus show that our proposed method
accelerates the inference of neural Transducer by 4 times without sacrificing
performance. Our work is open-sourced and publicly available
https://github.com/k2-fsa/icefall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11564">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition. (arXiv:2305.11569v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11569">
<div class="article-summary-box-inner">
<span><p>We improve low-resource ASR by integrating the ideas of multilingual training
and self-supervised learning. Concretely, we leverage an International Phonetic
Alphabet (IPA) multilingual model to create frame-level pseudo labels for
unlabeled speech, and use these pseudo labels to guide hidden-unit BERT
(HuBERT) based speech pretraining in a phonetically-informed manner. The
experiments on the Multilingual Speech (MLS) Corpus show that the proposed
approach consistently outperforms the standard HuBERT on all the target
languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT,
the approach performs better, meanwhile is able to save supervised training
data by 1.5k hours (75%) at most. Our approach outperforms most of the state of
the arts, with much less pretraining data in terms of hours and language
diversity. Compared to XLSR-53 and a retraining based multilingual method, our
approach performs better with full and limited finetuning data scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-universal phonetic encoder for low-resource speech recognition. (arXiv:2305.11576v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11576">
<div class="article-summary-box-inner">
<span><p>Multilingual training is effective in improving low-resource ASR, which may
partially be explained by phonetic representation sharing between languages. In
end-to-end (E2E) ASR systems, graphemes are often used as basic modeling units,
however graphemes may not be ideal for multilingual phonetic sharing. In this
paper, we leverage International Phonetic Alphabet (IPA) based
language-universal phonetic model to improve low-resource ASR performances, for
the first time within the attention encoder-decoder architecture. We propose an
adaptation method on the phonetic IPA model to further improve the proposed
approach on extreme low-resource languages. Experiments carried out on the
open-source MLS corpus and our internal databases show our approach outperforms
baseline monolingual models and most state-of-the-art works. Our main approach
and adaptation are effective on extremely low-resource languages, even within
domain- and language-mismatched scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11579">
<div class="article-summary-box-inner">
<span><p>Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster Tweet Summarization. (arXiv:2305.11592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11592">
<div class="article-summary-box-inner">
<span><p>Online social media platforms, such as Twitter, are one of the most valuable
sources of information during disaster events. Therefore, humanitarian
organizations, government agencies, and volunteers rely on a summary of this
information, i.e., tweets, for effective disaster management. Although there
are several existing supervised and unsupervised approaches for automated tweet
summary approaches, these approaches either require extensive labeled
information or do not incorporate specific domain knowledge of disasters.
Additionally, the most recent approaches to disaster summarization have
proposed BERT-based models to enhance the summary quality. However, for further
improved performance, we introduce the utilization of domain-specific knowledge
without any human efforts to understand the importance (salience) of a tweet
which further aids in summary creation and improves summary quality. In this
paper, we propose a disaster-specific tweet summarization framework, IKDSumm,
which initially identifies the crucial and important information from each
tweet related to a disaster through key-phrases of that tweet. We identify
these key-phrases by utilizing the domain knowledge (using existing ontology)
of disasters without any human intervention. Further, we utilize these
key-phrases to automatically generate a summary of the tweets. Therefore, given
tweets related to a disaster, IKDSumm ensures fulfillment of the summarization
key objectives, such as information coverage, relevance, and diversity in
summary without any human intervention. We evaluate the performance of IKDSumm
with 8 state-of-the-art techniques on 12 disaster datasets. The evaluation
results show that IKDSumm outperforms existing techniques by approximately
2-79% in terms of ROUGE-N F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11595">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive zero-shot or
few-shot commonsense reasoning performance on various natural language
processing (NLP) tasks. However, despite their strong commonsense reasoning
abilities, LLMs still exhibit various kinds of inconsistency problems. While
previous researches mainly focused on the self-consistency within a single LLM,
we propose to explore the inter-consistency issue between two or more LLMs,
which is critical for diverse and precise decision-making processes. Since the
LLMs possess human-like intelligence after instruction tuning and reinforcement
learning with human feedback (RLHF), we design a formal debate framework to
delve into the inter-consistency problem among LLMs with three-stage debate:
fair debate, mismatched debate, and roundtable debate. Through extensive
experiments on 7 commonsense reasoning datasets, LLMs not only become more
inter-consistent by compromising and refuting but also achieve higher
performance and stronger interpretability. Furthermore, we find a much stronger
LLM would be dominant in mismatched debates, while it will be easily misled by
relatively weaker LLMs in a more complex debate scenario such as roundtable
debate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11596">
<div class="article-summary-box-inner">
<span><p>Modern NLP models are often trained over large untrusted datasets, raising
the potential for a malicious adversary to compromise model behaviour. For
instance, backdoors can be implanted through crafting training instances with a
specific textual trigger and a target label. This paper posits that backdoor
poisoning attacks exhibit spurious correlation between simple text features and
classification labels, and accordingly, proposes methods for mitigating
spurious correlation as means of defence. Our empirical study reveals that the
malicious triggers are highly correlated to their target labels; therefore such
correlations are extremely distinguishable compared to those scores of benign
features, and can be used to filter out potentially problematic instances.
Compared with several existing defences, our defence method significantly
reduces attack success rates across backdoor attacks, and in the case of
insertion based attacks, our method provides a near-perfect defence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introspective Tips: Large Language Model for In-Context Decision Making. (arXiv:2305.11598v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11598">
<div class="article-summary-box-inner">
<span><p>The emergence of large language models (LLMs) has substantially influenced
natural language processing, demonstrating exceptional results across various
tasks. In this study, we employ ``Introspective Tips" to facilitate LLMs in
self-optimizing their decision-making. By introspectively examining
trajectories, LLM refines its policy by generating succinct and valuable tips.
Our method enhances the agent's performance in both few-shot and zero-shot
learning situations by considering three essential scenarios: learning from the
agent's past experiences, integrating expert demonstrations, and generalizing
across diverse games. Importantly, we accomplish these improvements without
fine-tuning the LLM parameters; rather, we adjust the prompt to generalize
insights from the three aforementioned situations. Our framework not only
supports but also emphasizes the advantage of employing LLM in in-contxt
decision-making. Experiments involving over 100 games in TextWorld illustrate
the superior performance of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attributable and Scalable Opinion Summarization. (arXiv:2305.11603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11603">
<div class="article-summary-box-inner">
<span><p>We propose a method for unsupervised opinion summarization that encodes
sentences from customer reviews into a hierarchical discrete latent space, then
identifies common opinions based on the frequency of their encodings. We are
able to generate both abstractive summaries by decoding these frequent
encodings, and extractive summaries by selecting the sentences assigned to the
same frequent encodings. Our method is attributable, because the model
identifies sentences used to generate the summary as part of the summarization
process. It scales easily to many hundreds of input reviews, because
aggregation is performed in the latent space rather than over long sequences of
tokens. We also demonstrate that our appraoch enables a degree of control,
generating aspect-specific summaries by restricting the model to parts of the
encoding space that correspond to desired aspects (e.g., location or food).
Automatic and human evaluation on two datasets from different domains
demonstrates that our method generates summaries that are more informative than
prior work and better grounded in the input reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets. (arXiv:2305.11625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11625">
<div class="article-summary-box-inner">
<span><p>Code search is an important task that has seen many developments in recent
years. However, previous attempts have mostly considered the problem of
searching for code by a text query. We argue that using a code snippet (and
possibly an associated traceback) as a query and looking for answers with
bugfixing instructions and code samples is a natural use case that is not
covered by existing approaches. Moreover, existing datasets use comments
extracted from code rather than full-text descriptions as text, making them
unsuitable for this use case. We present a new SearchBySnippet dataset
implementing the search-by-code use case based on StackOverflow data; it turns
out that in this setting, existing architectures fall short of the simplest
BM25 baseline even after fine-tuning. We present a new single encoder model
SnippeR that outperforms several strong baselines on the SearchBySnippet
dataset with a result of 0.451 Recall@10; we propose the SearchBySnippet
dataset and SnippeR as a new important benchmark for code search evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11626">
<div class="article-summary-box-inner">
<span><p>We consider the clone detection and information retrieval problems for source
code, well-known tasks important for any programming language. Although it is
also an important and interesting problem to find code snippets that operate
identically but are written in different programming languages, to the best of
our knowledge multilingual clone detection has not been studied in literature.
In this work, we formulate the multilingual clone detection problem and present
XCD, a new benchmark dataset produced from the CodeForces submissions dataset.
Moreover, we present a novel training procedure, called cross-consistency
training (CCT), that we apply to train language models on source code in
different programming languages. The resulting CCT-LM model, initialized with
GraphCodeBERT and fine-tuned with CCT, achieves new state of the art,
outperforming existing approaches on the POJ-104 clone detection benchmark with
95.67\% MAP and AdvTest code search benchmark with 47.18\% MRR; it also shows
the best results on the newly created multilingual clone detection benchmark
XCD across all programming languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11627">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
both the deployment, inference, and training stages. With LLM being a
general-purpose task solver, we explore its compression in a task-agnostic
manner, which aims to preserve the multi-task solving and language generation
ability of the original LLM. One challenge to achieving this is the enormous
size of the training corpus of LLM, which makes both data transfer and model
post-training over-burdensome. Thus, we tackle the compression of LLMs within
the bound of two constraints: being task-agnostic and minimizing the reliance
on the original training dataset. Our method, named LLM-Pruner, adopts
structural pruning that selectively removes non-critical coupled structures
based on gradient information, maximally preserving the majority of the LLM's
functionality. To this end, the performance of pruned models can be efficiently
recovered through tuning techniques, LoRA, in merely 3 hours, requiring only
50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,
and ChatGLM, and demonstrate that the compressed models still exhibit
satisfactory capabilities in zero-shot classification and generation. The code
is available at: https://github.com/horseee/LLM-Pruner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11662">
<div class="article-summary-box-inner">
<span><p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithmic failure as a humanities methodology: machine learning's mispredictions identify rich cases for qualitative analysis. (arXiv:2305.11663v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11663">
<div class="article-summary-box-inner">
<span><p>This commentary tests a methodology proposed by Munk et al. (2022) for using
failed predictions in machine learning as a method to identify ambiguous and
rich cases for qualitative analysis. Using a dataset describing actions
performed by fictional characters interacting with machine vision technologies
in 500 artworks, movies, novels and videogames, I trained a simple machine
learning algorithm (using the kNN algorithm in R) to predict whether or not an
action was active or passive using only information about the fictional
characters. Predictable actions were generally unemotional and unambiguous
activities where machine vision technologies were treated as simple tools.
Unpredictable actions, that is, actions that the algorithm could not correctly
predict, were more ambivalent and emotionally loaded, with more complex power
relationships between characters and technologies. The results thus support
Munk et al.'s theory that failed predictions can be productively used to
identify rich cases for qualitative analysis. This test goes beyond simply
replicating Munk et al.'s results by demonstrating that the method can be
applied to a broader humanities domain, and that it does not require complex
neural networks but can also work with a simpler machine learning algorithm.
Further research is needed to develop an understanding of what kinds of data
the method is useful for and which kinds of machine learning are most
generative. To support this, the R code required to produce the results is
included so the test can be replicated. The code can also be reused or adapted
to test the method on other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages. (arXiv:2305.11673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11673">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis (SA) systems are used in many products and hundreds of
languages. Gender and racial biases are well-studied in English SA systems, but
understudied in other languages, with few resources for such studies. To remedy
this, we build a counterfactual evaluation corpus for gender and racial/migrant
bias in four languages. We demonstrate its usefulness by answering a simple but
important question that an engineer might need to answer when deploying a
system: What biases do systems import from pre-trained models when compared to
a baseline with no pre-training? Our evaluation corpus, by virtue of being
counterfactual, not only reveals which models have less bias, but also
pinpoints changes in model bias behaviour, which enables more targeted
mitigation strategies. We release our code and evaluation corpora to facilitate
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensing of inspiration events from speech: comparison of deep learning and linguistic methods. (arXiv:2305.11683v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11683">
<div class="article-summary-box-inner">
<span><p>Respiratory chest belt sensor can be used to measure the respiratory rate and
other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms
estimate the belt sensor waveform from speech audio. In this paper we compare
the detection of inspiration events (IE) from respiratory belt sensor data
using a novel neural VRB algorithm and the detections based on time-aligned
linguistic content. The results show the superiority of the VRB method over
word pause detection or grammatical content segmentation. The comparison of the
methods show that both read and spontaneous speech content has a significant
amount of ungrammatical breathing, that is, breathing events that are not
aligned with grammatically appropriate places in language. This study gives new
insights into the development of VRB methods and adds to the general
understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,
for the reconstruction of the continuous breathing waveform is demonstrated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11685">
<div class="article-summary-box-inner">
<span><p>Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11692">
<div class="article-summary-box-inner">
<span><p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. (arXiv:2305.11694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11694">
<div class="article-summary-box-inner">
<span><p>Formulating selective information needs results in queries that implicitly
specify set operations, such as intersection, union, and difference. For
instance, one might search for "shorebirds that are not sandpipers" or
"science-fiction films shot in England". To study the ability of retrieval
systems to meet such information needs, we construct QUEST, a dataset of 3357
natural language queries with implicit set operations, that map to a set of
entities corresponding to Wikipedia documents. The dataset challenges models to
match multiple constraints mentioned in queries with corresponding evidence in
documents and correctly perform various set operations. The dataset is
constructed semi-automatically using Wikipedia category names. Queries are
automatically composed from individual categories, then paraphrased and further
validated for naturalness and fluency by crowdworkers. Crowdworkers also assess
the relevance of entities based on their documents and highlight attribution of
query constraints to spans of document text. We analyze several modern
retrieval systems, finding that they often struggle on such queries. Queries
involving negation and conjunction are particularly challenging and systems are
further challenged with combinations of these operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11707">
<div class="article-summary-box-inner">
<span><p>In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11719">
<div class="article-summary-box-inner">
<span><p>Existing research on multimodal relation extraction (MRE) faces two
co-existing challenges, internal-information over-utilization and
external-information under-exploitation. To combat that, we propose a novel
framework that simultaneously implements the idea of internal-information
screening and external-information exploiting. First, we represent the
fine-grained semantic structures of the input image and text with the visual
and textual scene graphs, which are further fused into a unified cross-modal
graph (CMG). Based on CMG, we perform structure refinement with the guidance of
the graph information bottleneck principle, actively denoising the
less-informative features. Next, we perform topic modeling over the input image
and text, incorporating latent multimodal topic features to enrich the
contexts. On the benchmark MRE dataset, our system outperforms the current best
model significantly. With further in-depth analyses, we reveal the great
potential of our method for the MRE task. Our codes are open at
https://github.com/ChocoWu/MRE-ISE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering. (arXiv:2305.11725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11725">
<div class="article-summary-box-inner">
<span><p>Answering multi-hop questions over hybrid factual knowledge from the given
text and table (TextTableQA) is a challenging task. Existing models mainly
adopt a retriever-reader framework, which have several deficiencies, such as
noisy labeling in training retriever, insufficient utilization of heterogeneous
information over text and table, and deficient ability for different reasoning
operations. In this paper, we propose a three-stage TextTableQA framework
S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever
with refinement training to solve the noisy labeling problem. Then, a hybrid
selector considers the linked relationships between heterogeneous data to
select the most relevant factual knowledge. For the final stage, instead of
adapting a reading comprehension module like in previous methods, we employ a
generation-based reasoner to obtain answers. This includes two approaches: a
row-wise generator and an LLM prompting generator~(first time used in this
task). The experimental results demonstrate that our method achieves
competitive results in the few-shot setting. When trained on the full dataset,
our approach outperforms all baseline methods, ranking first on the HybridQA
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persian Typographical Error Type Detection using Many-to-Many Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11731">
<div class="article-summary-box-inner">
<span><p>Digital technologies have led to an influx of text created daily in a variety
of languages, styles, and formats. A great deal of the popularity of
spell-checking systems can be attributed to this phenomenon since they are
crucial to polishing the digitally conceived text. In this study, we tackle
Typographical Error Type Detection in Persian, which has been relatively
understudied. In this paper, we present a public dataset named FarsTypo,
containing 3.4 million chronologically ordered and part-of-speech tagged words
of diverse topics and linguistic styles. An algorithm for applying
Persian-specific errors is developed and applied to a scalable size of these
words, forming a parallel dataset of correct and incorrect words. Using
FarsTypo, we establish a firm baseline and compare different methodologies
using various architectures. In addition, we present a novel Many-to-Many Deep
Sequential Neural Network to perform token classification using both word and
character embeddings in combination with bidirectional LSTM layers to detect
typographical errors across 51 classes. We compare our approach with
highly-advanced industrial systems that, unlike this study, have been developed
utilizing a variety of resources. The results of our final method were
competitive in that we achieved an accuracy of 97.62%, a precision of 98.83%, a
recall of 98.61%, and outperformed the rest in terms of speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11738">
<div class="article-summary-box-inner">
<span><p>Recent developments in large language models (LLMs) have been impressive.
However, these models sometimes show inconsistencies and problematic behavior,
such as hallucinating facts, generating flawed code, or creating offensive and
toxic content. Unlike these models, humans typically utilize external tools to
cross-check and refine their initial content, like using a search engine for
fact-checking, or a code interpreter for debugging. Inspired by this
observation, we introduce a framework called CRITIC that allows LLMs, which are
essentially "black boxes" to validate and progressively amend their own outputs
in a manner similar to human interaction with tools. More specifically,
starting with an initial output, CRITIC interacts with appropriate tools to
evaluate certain aspects of the text, and then revises the output based on the
feedback obtained during this validation process. Comprehensive evaluations
involving free-form question answering, mathematical program synthesis, and
toxicity reduction demonstrate that CRITIC consistently enhances the
performance of LLMs. Meanwhile, our research highlights the crucial importance
of external feedback in promoting the ongoing self-improvement of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11744">
<div class="article-summary-box-inner">
<span><p>Neural information retrieval often adopts a retrieve-and-rerank framework: a
bi-encoder network first retrieves K (e.g., 100) candidates that are then
re-ranked using a more powerful cross-encoder model to rank the better
candidates higher. The re-ranker generally produces better candidate scores
than the retriever, but is limited to seeing only the top K retrieved
candidates, thus providing no improvements in retrieval performance as measured
by Recall@K. In this work, we leverage the re-ranker to also improve retrieval
by providing inference-time relevance feedback to the retriever. Concretely, we
update the retriever's query representation for a test instance using a
lightweight inference-time distillation of the re-ranker's prediction for that
instance. The distillation loss is designed to bring the retriever's candidate
scores closer to those of the re-ranker. A second retrieval step is then
performed with the updated query vector. We empirically show that our approach,
which can serve arbitrary retrieve-and-rerank pipelines, significantly improves
retrieval recall in multiple domains, languages, and modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation. (arXiv:2305.11746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11746">
<div class="article-summary-box-inner">
<span><p>Hallucinations in machine translation are translations that contain
information completely unrelated to the input. Omissions are translations that
do not include some of the input information. While both cases tend to be
catastrophic errors undermining user trust, annotated data with these types of
pathologies is extremely scarce and is limited to a few high-resource
languages. In this work, we release an annotated dataset for the hallucination
and omission phenomena covering 18 translation directions with varying resource
levels and scripts. Our annotation covers different levels of partial and full
hallucinations as well as omissions both at the sentence and at the word level.
Additionally, we revisit previous methods for hallucination and omission
detection, show that conclusions made based on a single language pair largely
do not hold for a large-scale evaluation, and establish new solid baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11747">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, \ie content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation for Large Language Models (HELMA) benchmark, a large collection of
generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing and alleviating hallucination. To generate
these samples, we propose a ChatGPT-based two-step framework, \ie
sampling-then-filtering. Specifically, we first adopt two different sampling
methods to generate hallucinated samples based on instructions, and then use an
example-enhanced filtering method to select the best one. Furthermore, we also
hire some human labelers to annotate the hallucinations in ChatGPT responses.
The empirical results suggest that ChatGPT has some probabilities to generate
hallucinations and existing LLMs face great challenges in recognizing the
hallucinations in text. In addition, the performance can be improved by
providing external knowledge or adding reasoning steps. Our benchmark can be
accessed at https://github.com/RUCAIBox/HELMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11759">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are known to memorize significant portions of
their training data. Parts of this memorized content have been shown to be
extractable by simply querying the model, which poses a privacy risk. We
present a novel approach which uses prompt-tuning to control the extraction
rates of memorized content in LLMs. We present two prompt training strategies
to increase and decrease extraction rates, which correspond to an attack and a
defense, respectively. We demonstrate the effectiveness of our techniques by
using models from the GPT-Neo family on a public benchmark. For the 1.3B
parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in
extraction rate compared to our baseline. Our defense can be tuned to achieve
different privacy-utility trade-offs by a user-specified hyperparameter. We
achieve an extraction rate reduction of up to 97.7% relative to our baseline,
with a perplexity increase of 16.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation. (arXiv:2305.11761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11761">
<div class="article-summary-box-inner">
<span><p>Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue of
Neural Machine Translation (NMT) generating translation outputs that contain
toxic words not present in the input. The objective is to mitigate the
introduction of toxic language without the need for re-training. In the case of
identified added toxicity during the inference process, ReSeTOX dynamically
adjusts the key-value self-attention weights and re-evaluates the beam search
hypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable
57% reduction in added toxicity while maintaining an average translation
quality of 99.5% across 164 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11768">
<div class="article-summary-box-inner">
<span><p>Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
https://github.com/zhaoyucs/VSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11769">
<div class="article-summary-box-inner">
<span><p>Large pre-trained multimodal models have demonstrated significant success in
a range of downstream tasks, including image captioning, image-text retrieval,
visual question answering (VQA), etc. However, many of these methods rely on
image-text pairs collected from the web as pre-training data and unfortunately
overlook the need for fine-grained feature alignment between vision and
language modalities, which requires detailed understanding of images and
language expressions. While integrating VQA and dense captioning (DC) into
pre-training can address this issue, acquiring image-question-answer as well as
image-location-caption triplets is challenging and time-consuming.
Additionally, publicly available datasets for VQA and dense captioning are
typically limited in scale due to manual data collection and labeling efforts.
In this paper, we propose a novel method called Joint QA and DC GEneration
(JADE), which utilizes a pre-trained multimodal model and easily-crawled
image-text pairs to automatically generate and filter large-scale VQA and dense
captioning datasets. We apply this method to the Conceptual Caption (CC3M)
dataset to generate a new dataset called CC3M-QA-DC. Experiments show that when
used for pre-training in a multi-task manner, CC3M-QA-DC can improve the
performance with various backbones on various downstream tasks. Furthermore,
our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,
CC15M) and achieve competitive results compared with models using much more
data. Code and dataset will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11778">
<div class="article-summary-box-inner">
<span><p>The recent rapid progress in pre-training Large Language Models has relied on
using self-supervised language modeling objectives like next token prediction
or span corruption. On the other hand, Machine Translation Systems are mostly
trained using cross-lingual supervision that requires aligned data between
source and target languages. We demonstrate that pre-training Large Language
Models on a mixture of a self-supervised Language Modeling objective and the
supervised Machine Translation objective, therefore including cross-lingual
parallel data during pre-training, yields models with better in-context
learning abilities. As pre-training is a very resource-intensive process and a
grid search on the best mixing ratio between the two objectives is
prohibitively expensive, we propose a simple yet effective strategy to learn it
during pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMDD: A Large-Scale Dataset for Dataset Mentions Detection. (arXiv:2305.11779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11779">
<div class="article-summary-box-inner">
<span><p>The recognition of dataset names is a critical task for automatic information
extraction in scientific literature, enabling researchers to understand and
identify research opportunities. However, existing corpora for dataset mention
detection are limited in size and naming diversity. In this paper, we introduce
the Dataset Mentions Detection Dataset (DMDD), the largest publicly available
corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219
scientific articles with over 449,000 dataset mentions weakly annotated in the
format of in-text spans, and an evaluation set, which comprises of 450
scientific articles manually annotated for evaluation purposes. We use DMDD to
establish baseline performance for dataset mention detection and linking. By
analyzing the performance of various models on DMDD, we are able to identify
open problems in dataset mention detection. We invite the community to use our
dataset as a challenge to develop novel dataset mention detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11789">
<div class="article-summary-box-inner">
<span><p>Humans work together to solve common problems by having discussions,
explaining, and agreeing or disagreeing with each other. Similarly, if a system
can have discussions with humans when solving tasks, it can improve the
system's performance and reliability. In previous research on explainability,
it has only been possible for the system to make predictions and for humans to
ask questions about them rather than having a mutual exchange of opinions. This
research aims to create a dataset and computational framework for systems that
discuss and refine their predictions through dialogue. Through experiments, we
show that the proposed system can have beneficial discussions with humans
improving the accuracy by up to 25 points in the natural language inference
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11790">
<div class="article-summary-box-inner">
<span><p>Prompting with natural language instructions has recently emerged as a
popular method of harnessing the capabilities of large language models. Given
the inherent ambiguity present in natural language, it is intuitive to consider
the possible advantages of prompting with less ambiguous prompt styles, such as
the use of pseudo-code.
</p>
<p>In this paper we explore if prompting via pseudo-code instructions helps
improve the performance of pre-trained language models. We manually create a
dataset of pseudo-code prompts for 132 different tasks spanning classification,
QA and generative language tasks, sourced from the Super-NaturalInstructions
dataset. Using these prompts along with their counterparts in natural language,
we study their performance on two LLM families - BLOOM and CodeGen. Our
experiments show that using pseudo-code instructions leads to better results,
with an average increase (absolute) of 7-16 points in F1 scores for
classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE
scores across all tasks. We include detailed ablation studies which indicate
that code comments, docstrings, and the structural clues encoded in pseudo-code
all contribute towards the improvement in performance.
</p>
<p>To the best of our knowledge our work is the first to demonstrate how
pseudo-code prompts can be helpful in improving the performance of pre-trained
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Few-shot NER with Prompt Ordering based Data Augmentation. (arXiv:2305.11791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11791">
<div class="article-summary-box-inner">
<span><p>Recently, data augmentation (DA) methods have been proven to be effective for
pre-trained language models (PLMs) in low-resource settings, including few-shot
named entity recognition (NER). However, conventional NER DA methods are mostly
aimed at sequence labeling models, i.e., token-level classification, and few
are compatible with unified autoregressive generation frameworks, which can
handle a wider range of NER tasks, such as nested NER. Furthermore, these
generation frameworks have a strong assumption that the entities will appear in
the target sequence with the same left-to-right order as the source sequence.
In this paper, we claim that there is no need to keep this strict order, and
more diversified but reasonable target entity sequences can be provided during
the training stage as a novel DA method. Nevertheless, a naive mixture of
augmented data can confuse the model since one source sequence will then be
paired with different target sequences. Therefore, we propose a simple but
effective Prompt Ordering based Data Augmentation (PODA) method to improve the
training of unified autoregressive generation frameworks under few-shot NER
scenarios. Experimental results on three public NER datasets and further
analyses demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11792">
<div class="article-summary-box-inner">
<span><p>The way and content in which users ask questions can provide insight into
their current status, including their personality, emotions, and psychology.
Instead of directly prompting the large language models (LLMs), we explore how
chain-of-thought prompting helps in this scenario to perform reasoning and
planning according to user status, aiming to provide a more personalized and
engaging experience for the user query. To this end, we first construct a
benchmark of 6 dialogue or question-answering datasets in both English and
Chinese, covering 3 different aspects of user status (\textit{including}
\textit{personality}, \textit{emotion}, and \textit{psychology}). Then we
prompt the LLMs to generate the response regarding the user status as
intermediate reasoning processing. We propose a novel demonstration selection
strategy using the semantic similarity of intermediate reasoning instead of
test queries. To evaluate the effectiveness and robustness of our approach, we
conduct extensive experiments with 7 LLMs under zero-shot and one-shot
settings. The experimental results show that our approach consistently
outperforms standard prompting in terms of both \textit{helpfulness} and
\textit{acceptness} across all datasets, regardless of the LLMs used. The code
and dataset can be found at
\url{https://github.com/ruleGreen/Dialogue\_CoT.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11806">
<div class="article-summary-box-inner">
<span><p>Neural metrics for machine translation evaluation, such as COMET, exhibit
significant improvements in their correlation with human judgments, as compared
to traditional metrics based on lexical overlap, such as BLEU. Yet, neural
metrics are, to a great extent, "black boxes" returning a single sentence-level
score without transparency about the decision-making process. In this work, we
develop and compare several neural explainability methods and demonstrate their
effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our
study reveals that these metrics leverage token-level information that can be
directly attributed to translation errors, as assessed through comparison of
token-level neural saliency maps with Multidimensional Quality Metrics (MQM)
annotations and with synthetically-generated critical translation errors. To
ease future research, we release our code at:
https://github.com/Unbabel/COMET/tree/explainable-metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Label Training and Model Inertia in Neural Machine Translation. (arXiv:2305.11808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11808">
<div class="article-summary-box-inner">
<span><p>Like many other machine learning applications, neural machine translation
(NMT) benefits from over-parameterized deep neural models. However, these
models have been observed to be brittle: NMT model predictions are sensitive to
small input changes and can show significant variation across re-training or
incremental model updates. This work studies a frequently used method in NMT,
pseudo-label training (PLT), which is common to the related techniques of
forward-translation (or self-training) and sequence-level knowledge
distillation. While the effect of PLT on quality is well-documented, we
highlight a lesser-known effect: PLT can enhance a model's stability to model
updates and input perturbations, a set of properties we call model inertia. We
study inertia effects under different training settings and we identify
distribution simplification as a mechanism behind the observed results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11826">
<div class="article-summary-box-inner">
<span><p>Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11828">
<div class="article-summary-box-inner">
<span><p>Medical systematic reviews are crucial for informing clinical decision making
and healthcare policy. But producing such reviews is onerous and
time-consuming. Thus, high-quality evidence synopses are not available for many
questions and may be outdated even when they are available. Large language
models (LLMs) are now capable of generating long-form texts, suggesting the
tantalizing possibility of automatically generating literature reviews on
demand. However, LLMs sometimes generate inaccurate (and potentially
misleading) texts by hallucinating or omitting important information. In the
healthcare context, this may render LLMs unusable at best and dangerous at
worst. Most discussion surrounding the benefits and risks of LLMs have been
divorced from specific applications. In this work, we seek to qualitatively
characterize the potential utility and risks of LLMs for assisting in
production of medical evidence reviews. We conducted 16 semi-structured
interviews with international experts in systematic reviews, grounding
discussion in the context of generating evidence reviews. Domain experts
indicated that LLMs could aid writing reviews, as a tool for drafting or
creating plain language summaries, generating templates or suggestions,
distilling information, crosschecking, and synthesizing or interpreting text
inputs. But they also identified issues with model outputs and expressed
concerns about potential downstream harms of confidently composed but
inaccurate LLM outputs which might mislead. Other anticipated potential
downstream harms included lessened accountability and proliferation of
automatically generated reviews that might be of low quality. Informed by this
qualitative analysis, we identify criteria for rigorous evaluation of
biomedical LLMs aligned with domain expert views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models. (arXiv:2305.11840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11840">
<div class="article-summary-box-inner">
<span><p>Stereotype benchmark datasets are crucial to detect and mitigate social
stereotypes about groups of people in NLP models. However, existing datasets
are limited in size and coverage, and are largely restricted to stereotypes
prevalent in the Western society. This is especially problematic as language
technologies gain hold across the globe. To address this gap, we present
SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative
capabilities of large language models such as PaLM, and GPT-3, and leveraging a
globally diverse rater pool to validate the prevalence of those stereotypes in
society. SeeGULL is in English, and contains stereotypes about identity groups
spanning 178 countries across 8 different geo-political regions across 6
continents, as well as state-level identities within the US and India. We also
include fine-grained offensiveness scores for different stereotypes and
demonstrate their global disparities. Furthermore, we include comparative
annotations about the same groups by annotators living in the region vs. those
that are based in North America, and demonstrate that within-region stereotypes
about groups differ from those prevalent in North America. CONTENT WARNING:
This paper contains stereotype examples that may be offensive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Generative Retrieval Scale to Millions of Passages?. (arXiv:2305.11841v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11841">
<div class="article-summary-box-inner">
<span><p>Popularized by the Differentiable Search Index, the emerging paradigm of
generative retrieval re-frames the classic information retrieval problem into a
sequence-to-sequence modeling task, forgoing external indices and encoding an
entire document corpus within a single Transformer. Although many different
approaches have been proposed to improve the effectiveness of generative
retrieval, they have only been evaluated on document corpora on the order of
100k in size. We conduct the first empirical study of generative retrieval
techniques across various corpus scales, ultimately scaling up to the entire MS
MARCO passage ranking task with a corpus of 8.8M passages and evaluating model
sizes up to 11B parameters. We uncover several findings about scaling
generative retrieval to millions of passages; notably, the central importance
of using synthetic queries as document representations during indexing, the
ineffectiveness of existing proposed architecture modifications when accounting
for compute cost, and the limits of naively scaling model parameters with
respect to retrieval performance. While we find that generative retrieval is
competitive with state-of-the-art dual encoders on small corpora, scaling to
millions of passages remains an important and unsolved challenge. We believe
these findings will be valuable for the community to clarify the current state
of generative retrieval, highlight the unique challenges, and inspire new
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. (arXiv:2305.11845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11845">
<div class="article-summary-box-inner">
<span><p>Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11846">
<div class="article-summary-box-inner">
<span><p>We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at https://codi-gen.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11853">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with in-context learning have demonstrated
remarkable capability in the text-to-SQL task. Previous research has prompted
LLMs with various demonstration-retrieval strategies and intermediate reasoning
steps to enhance the performance of LLMs. However, those works often employ
varied strategies when constructing the prompt text for text-to-SQL inputs,
such as databases and demonstration examples. This leads to a lack of
comparability in both the prompt constructions and their primary contributions.
Furthermore, selecting an effective prompt construction has emerged as a
persistent problem for future research. To address this limitation, we
comprehensively investigate the impact of prompt constructions across various
settings and provide insights for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Claim Verification with Evidence Retrieved in the Wild. (arXiv:2305.11859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11859">
<div class="article-summary-box-inner">
<span><p>Evidence retrieval is a core part of automatic fact-checking. Prior work
makes simplifying assumptions in retrieval that depart from real-world use
cases: either no access to evidence, access to evidence curated by a human
fact-checker, or access to evidence available long after the claim has been
made. In this work, we present the first fully automated pipeline to check
real-world claims by retrieving raw evidence from the web. We restrict our
retriever to only search documents available prior to the claim's making,
modeling the realistic scenario where an emerging claim needs to be checked.
Our pipeline includes five components: claim decomposition, raw document
retrieval, fine-grained evidence retrieval, claim-focused summarization, and
veracity judgment. We conduct experiments on complex political claims in the
ClaimDecomp dataset and show that the aggregated evidence produced by our
pipeline improves veracity judgments. Human evaluation finds the evidence
summary produced by our system is reliable (it does not hallucinate
information) and relevant to answering key questions about a claim, suggesting
that it can assist fact-checkers even when it cannot surface a complete
evidence set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs. (arXiv:2305.11860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11860">
<div class="article-summary-box-inner">
<span><p>A popular approach for improving the correctness of output from large
language models (LLMs) is Self-Consistency - poll the LLM multiple times and
output the most frequent solution. Existing Self-Consistency techniques always
draw a constant number of samples per question, where a better approach will be
to non-uniformly distribute the available budget based on the amount of
agreement in the samples drawn so far. In response, we introduce
Adaptive-Consistency, a cost-efficient, model-agnostic technique that
dynamically adjusts the number of samples per question using a lightweight
stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate
that Adaptive-Consistency reduces sample budget by up to 6.0 times with an
average accuracy drop of less than 0.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11862">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable performance in
various tasks and gained significant attention. LLMs are also used for local
sequence transduction tasks, including grammatical error correction (GEC) and
formality style transfer, where most tokens in a source text are kept
unchanged. However, it is inefficient to generate all target tokens because a
prediction error of a target token may cause a catastrophe in predicting
subsequent tokens and because the computational cost grows quadratically with
the target sequence length. This paper proposes to predict a set of edit
operations for the source text for local sequence transduction tasks.
Representing an edit operation with a span of the source text and changed
tokens, we can reduce the length of the target sequence and thus the
computational cost for inference. We apply instruction tuning for LLMs on the
supervision data of edit operations. Experiments show that the proposed method
achieves comparable performance to the baseline in four tasks, paraphrasing,
formality style transfer, GEC, and text simplification, despite reducing the
length of the target text by as small as 21\%. Furthermore, we report that the
instruction tuning with the proposed method achieved the state-of-the-art
performance in the four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11863">
<div class="article-summary-box-inner">
<span><p>Representations from transformer-based unidirectional language models are
known to be effective at predicting brain responses to natural language.
However, most studies comparing language models to brains have used GPT-2 or
similarly sized language models. Here we tested whether larger open-source
models such as those from the OPT and LLaMA families are better at predicting
brain responses recorded using fMRI. Mirroring scaling results from other
contexts, we found that brain prediction performance scales log-linearly with
model size from 125M to 30B parameter models, with ~15% increased encoding
performance as measured by correlation with a held-out test set across 3
subjects. Similar log-linear behavior was observed when scaling the size of the
fMRI training set. We also characterized scaling for acoustic encoding models
that use HuBERT, WavLM, and Whisper, and we found comparable improvements with
model size. A noise ceiling analysis of these large, high-performance encoding
models showed that performance is nearing the theoretical maximum for brain
areas such as the precuneus and higher auditory cortex. These results suggest
that increasing scale in both models and data will yield incredibly effective
models of language processing in the brain, enabling better scientific
understanding as well as applications such as decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">North S\'{a}mi Dialect Identification with Self-supervised Speech Models. (arXiv:2305.11864v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11864">
<div class="article-summary-box-inner">
<span><p>The North S\'{a}mi (NS) language encapsulates four primary dialectal variants
that are related but that also have differences in their phonology, morphology,
and vocabulary. The unique geopolitical location of NS speakers means that in
many cases they are bilingual in S\'{a}mi as well as in the dominant state
language: Norwegian, Swedish, or Finnish. This enables us to study the NS
variants both with respect to the spoken state language and their acoustic
characteristics. In this paper, we investigate an extensive set of acoustic
features, including MFCCs and prosodic features, as well as state-of-the-art
self-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the
automatic detection of the four NS variants. In addition, we examine how the
majority state language is reflected in the dialects. Our results show that NS
dialects are influenced by the state language and that the four dialects are
separable, reaching high classification accuracy, especially with the XLS-R
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secondary Use of Clinical Problem List Entries for Neural Network-Based Disease Code Assignment. (arXiv:2112.13756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13756">
<div class="article-summary-box-inner">
<span><p>Clinical information systems have become large repositories for
semi-structured and partly annotated electronic health record data, which have
reached a critical mass that makes them interesting for supervised data-driven
neural network approaches. We explored automated coding of 50 character long
clinical problem list entries using the International Classification of
Diseases (ICD-10) and evaluated three different types of network architectures
on the top 100 ICD-10 three-digit codes. A fastText baseline reached a
macro-averaged F1-score of 0.83, followed by a character-level LSTM with a
macro-averaged F1-score of 0.84. The top performing approach used a
downstreamed RoBERTa model with a custom language model, yielding a
macro-averaged F1-score of 0.88. A neural network activation analysis together
with an investigation of the false positives and false negatives unveiled
inconsistent manual coding as a main limiting factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?. (arXiv:2207.12101v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12101">
<div class="article-summary-box-inner">
<span><p>The use of Deep Learning and Computer Vision in the Cultural Heritage domain
is becoming highly relevant in the last few years with lots of applications
about audio smart guides, interactive museums and augmented reality. All these
technologies require lots of data to work effectively and be useful for the
user. In the context of artworks, such data is annotated by experts in an
expensive and time consuming process. In particular, for each artwork, an image
of the artwork and a description sheet have to be collected in order to perform
common tasks like Visual Question Answering. In this paper we propose a method
for Visual Question Answering that allows to generate at runtime a description
sheet that can be used for answering both visual and contextual questions about
the artwork, avoiding completely the image and the annotation process. For this
purpose, we investigate on the use of GPT-3 for generating descriptions for
artworks analyzing the quality of generated descriptions through captioning
metrics. Finally we evaluate the performance for Visual Question Answering and
captioning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient NLP Model Finetuning via Multistage Data Filtering. (arXiv:2207.14386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14386">
<div class="article-summary-box-inner">
<span><p>As model finetuning is central to the modern NLP, we set to maximize its
efficiency. Motivated by redundancy in training examples and the sheer sizes of
pretrained models, we exploit a key opportunity: training only on important
data. To this end, we set to filter training examples in a streaming fashion,
in tandem with training the target model. Our key techniques are two: (1)
automatically determine a training loss threshold for skipping backward
training passes; (2) run a meta predictor for further skipping forward training
passes. We integrate the above techniques in a holistic, three-stage training
process. On a diverse set of benchmarks, our method reduces the required
training examples by up to 5.3$\times$ and training time by up to 6.8$\times$,
while only seeing minor accuracy degradation. Our method is effective even when
training one epoch, where each training example is encountered only once. It is
simple to implement and is compatible with the existing finetuning techniques.
Code is available at: https://github.com/xo28/efficient-
NLP-multistage-training
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Environmental Claim Detection. (arXiv:2209.00507v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00507">
<div class="article-summary-box-inner">
<span><p>To transition to a green economy, environmental claims made by companies must
be reliable, comparable, and verifiable. To analyze such claims at scale,
automated methods are needed to detect them in the first place. However, there
exist no datasets or models for this. Thus, this paper introduces the task of
environmental claim detection. To accompany the task, we release an
expert-annotated dataset and models trained on this dataset. We preview one
potential application of such models: We detect environmental claims made in
quarterly earning calls and find that the number of environmental claims has
steadily increased since the Paris Agreement in 2015.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16611">
<div class="article-summary-box-inner">
<span><p>Model architectures such as wav2vec 2.0 and HuBERT have been proposed to
learn speech representations from audio waveforms in a self-supervised manner.
When they are combined with downstream tasks such as keyword spotting and
speaker verification, they provide state-of-the-art performance. However, these
models use a large number of parameters, the smallest version of which has 95
million parameters. This constitutes a challenge for edge AI device
deployments. In this paper, we investigate the application of knowledge
distillation to speech representation learning (SRL) models followed by joint
fine-tuning with multiple downstream voice-activated tasks. In our experiments
on two such tasks, our approach results in nearly 75% reduction in model size
while suffering only 0.1% accuracy and 0.9% equal error rate degradation
compared to the full-size model. In addition, we show that fine-tuning the SRL
models results in a significant performance boost compared to using frozen SRL
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLPeer: A Unified Resource for the Computational Study of Peer Review. (arXiv:2211.06651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06651">
<div class="article-summary-box-inner">
<span><p>Peer review constitutes a core component of scholarly publishing; yet it
demands substantial expertise and training, and is susceptible to errors and
biases. Various applications of NLP for peer reviewing assistance aim to
support reviewers in this complex process, but the lack of clearly licensed
datasets and multi-domain corpora prevent the systematic study of NLP for peer
review. To remedy this, we introduce NLPeer -- the first ethically sourced
multidomain corpus of more than 5k papers and 11k review reports from five
different venues. In addition to the new datasets of paper drafts, camera-ready
versions and peer reviews from the NLP community, we establish a unified data
representation and augment previous peer review datasets to include parsed and
structured paper representations, rich metadata and versioning information. We
complement our resource with implementations and analysis of three reviewing
assistance tasks, including a novel guided skimming task. Our work paves the
path towards systematic, multi-faceted, evidence-based study of peer review in
NLP and beyond. The data and code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11275">
<div class="article-summary-box-inner">
<span><p>Although speech is a simple and effective way for humans to communicate with
the outside world, a more realistic speech interaction contains multimodal
information, e.g., vision, text. How to design a unified framework to integrate
different modal information and leverage different resources (e.g.,
visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to
facilitate speech representation learning was not well explored. In this paper,
we propose a unified cross-modal representation learning framework VATLM
(Visual-Audio-Text Language Model). The proposed VATLM employs a unified
backbone network to model the modality-independent information and utilizes
three simple modality-dependent modules to preprocess visual, speech, and text
inputs. In order to integrate these three modalities into one shared semantic
space, VATLM is optimized with a masked prediction task of unified tokens,
given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on
audio-visual related downstream tasks, including audio-visual speech
recognition (AVSR), visual speech recognition (VSR) tasks. Results show that
the proposed VATLM outperforms previous the state-of-the-art models, such as
audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that
VATLM is capable of aligning different modalities into the same space. To
facilitate future research, we release the code and pre-trained models at
https://aka.ms/vatlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.17192">
<div class="article-summary-box-inner">
<span><p>Inference from large autoregressive models like Transformers is slow -
decoding K tokens takes K serial runs of the model. In this work we introduce
speculative decoding - an algorithm to sample from autoregressive models faster
without any changes to the outputs, by computing several tokens in parallel. At
the heart of our approach lie the observations that (1) hard language-modeling
tasks often include easier subtasks that can be approximated well by more
efficient models, and (2) using speculative execution and a novel sampling
method, we can make exact decoding from the large models faster, by running
them in parallel on the outputs of the approximation models, potentially
generating several tokens concurrently, and without changing the distribution.
Our method can accelerate existing off-the-shelf models without retraining or
architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration
compared to the standard T5X implementation, with identical outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02021">
<div class="article-summary-box-inner">
<span><p>The focus of this work is to investigate unsupervised approaches to overcome
quintessential challenges in designing task-oriented dialog schema: assigning
intent labels to each dialog turn (intent clustering) and generating a set of
intents based on the intent clustering methods (intent induction). We postulate
there are two salient factors for automatic induction of intents: (1)
clustering algorithm for intent labeling and (2) user utterance embedding
space. We compare existing off-the-shelf clustering models and embeddings based
on DSTC11 evaluation. Our extensive experiments demonstrate that the combined
selection of utterance embedding and clustering method in the intent induction
task should be carefully considered. We also present that pretrained MiniLM
with Agglomerative clustering shows significant improvement in NMI, ARI, F1,
accuracy and example coverage in intent induction tasks. The source codes are
available at https://github.com/Jeiyoon/dstc11-track2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03000">
<div class="article-summary-box-inner">
<span><p>Objective: We aim to develop an open-source natural language processing (NLP)
package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models
to extract social determinants of health (SDoH) for cancer patients, examine
the generalizability of SODA to a new disease domain (i.e., opioid use), and
evaluate the extraction rate of SDoH using cancer populations.
</p>
<p>Methods: We identified SDoH categories and attributes and developed an SDoH
corpus using clinical notes from a general cancer cohort. We compared four
transformer-based NLP models to extract SDoH, examined the generalizability of
NLP models to a cohort of patients prescribed with opioids, and explored
customization strategies to improve performance. We applied the best NLP model
to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804),
and colorectal cancer (n=6,240) cohorts.
</p>
<p>Results and Conclusion: We developed a corpus of 629 cancer patients notes
with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH.
The Bidirectional Encoder Representations from Transformers (BERT) model
achieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH
concept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts.
Fine-tuning the NLP models using new annotations from opioid use patients
improved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The
extraction rates among 19 categories of SDoH varied greatly, where 10 SDoH
could be extracted from &gt;70% of cancer patients, but 9 SDoH had a low
extraction rate (&lt;70% of cancer patients). The SODA package with pre-trained
transformer models is publicly available at
https://github.com/uf-hobiinformatics-lab/SDoH_SODA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06711">
<div class="article-summary-box-inner">
<span><p>Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions. We hope to inspire
more valid and reliable TPC research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06742">
<div class="article-summary-box-inner">
<span><p>Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
release our code and pre-trained checkpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07530">
<div class="article-summary-box-inner">
<span><p>Multilingual machine translation models can benefit from synergy between
different language pairs, but also suffer from interference. While there is a
growing number of sophisticated methods that aim to eliminate interference, our
understanding of interference as a phenomenon is still limited. This work
identifies the main factors that contribute to interference in multilingual
machine translation. Through systematic experimentation, we find that
interference (or synergy) are primarily determined by model size, data size,
and the proportion of each language pair within the total dataset. We observe
that substantial interference occurs mainly when the model is very small with
respect to the available training data, and that using standard transformer
configurations with less than one billion parameters largely alleviates
interference and promotes synergy. Moreover, we show that tuning the sampling
temperature to control the proportion of each language pair in the data is key
to balancing the amount of interference between low and high resource language
pairs effectively, and can lead to superior performance overall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL. (arXiv:2212.08902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08902">
<div class="article-summary-box-inner">
<span><p>The task of text-to-SQL aims to convert a natural language question into its
corresponding SQL query within the context of relational tables. Existing
text-to-SQL parsers generate a "plausible" SQL query for an arbitrary user
question, thereby failing to correctly handle problematic user questions. To
formalize this problem, we conduct a preliminary study on the observed
ambiguous and unanswerable cases in text-to-SQL and summarize them into 6
feature categories. Correspondingly, we identify the causes behind each
category and propose requirements for handling ambiguous and unanswerable
questions. Following this study, we propose a simple yet effective
counterfactual example generation approach that automatically produces
ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a
weakly supervised DTE (Detecting-Then-Explaining) model for error detection,
localization, and explanation. Experimental results show that our model
achieves the best result on both real-world examples and generated examples
compared with various baselines. We release our data and code at:
\href{https://github.com/wbbeyourself/DTE}{https://github.com/wbbeyourself/DTE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation. (arXiv:2212.09631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09631">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) has become the de-facto standard in
real-world machine translation applications. However, NMT models can
unpredictably produce severely pathological translations, known as
hallucinations, that seriously undermine user trust. It becomes thus crucial to
implement effective preventive strategies to guarantee their proper
functioning. In this paper, we address the problem of hallucination detection
in NMT by following a simple intuition: as hallucinations are detached from the
source content, they exhibit encoder-decoder attention patterns that are
statistically different from those of good quality translations. We frame this
problem with an optimal transport formulation and propose a fully unsupervised,
plug-in detector that can be used with any attention-based NMT model.
Experimental results show that our detector not only outperforms all previous
model-based detectors, but is also competitive with detectors that employ large
models trained on millions of samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09667">
<div class="article-summary-box-inner">
<span><p>Users' physical safety is an increasing concern as the market for intelligent
systems continues to grow, where unconstrained systems may recommend users
dangerous actions that can lead to serious injury. Covertly unsafe text is an
area of particular interest, as such text may arise from everyday scenarios and
are challenging to detect as harmful. We propose FARM, a novel framework
leveraging external knowledge for trustworthy rationale generation in the
context of safety. In particular, FARM foveates on missing knowledge to qualify
the information required to reason in specific scenarios and retrieves this
information with attribution to trustworthy sources. This knowledge is used to
both classify the safety of the original text and generate human-interpretable
rationales, shedding light on the risk of systems to specific user groups and
helping both stakeholders manage the risks of their systems and policymakers to
provide concrete safeguards for consumer safety. Our experiments show that FARM
obtains state-of-the-art results on the SafeText dataset, showing absolute
improvement in safety classification accuracy by 5.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10020">
<div class="article-summary-box-inner">
<span><p>In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore is confused by truncation errors in summarization, and
MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or
middle of generations. Further, we investigate the reasons behind these blind
spots and suggest practical workarounds for a more reliable evaluation of text
generation. We have released our code and data at
https://github.com/cloudygoose/blindspot_nlg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Sentence Encoders with Prompting and Label Retrieval for Zero-shot Text Classification. (arXiv:2212.10391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10391">
<div class="article-summary-box-inner">
<span><p>With contrastive pre-training, sentence encoders are generally optimized to
locate semantically similar samples closer to each other in their embedding
spaces. In this work, we focus on the potential of their embedding spaces to be
readily adapted to zero-shot text classification, as semantically distinct
samples are already well-separated. Our framework, RaLP (Retrieval augmented
Label Prompts for sentence encoder), encodes prompted label candidates with a
sentence encoder, then assigns the label whose prompt embedding has the highest
similarity with the input text embedding. In order to compensate for the
potentially poorly descriptive labels in their original format, RaLP retrieves
sentences that are semantically similar to the original label prompt from
external corpora and use them as additional pseudo-label prompts. RaLP achieves
competitive or stronger performance than much larger baselines on various
closed-set classification and multiple-choice QA datasets under zero-shot
settings. We show that the retrieval component plays a pivotal role in RaLP's
success, and its results are robustly attained regardless of verbalizer
variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Execution-Based Evaluation for Open-Domain Code Generation. (arXiv:2212.10481v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10481">
<div class="article-summary-box-inner">
<span><p>To extend the scope of coding queries to more realistic settings, we propose
ODEX, the first Open-Domain EXecution-based natural language (NL) to Python
code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse
libraries, along with 1,707 human-written test cases for execution. Our NL-Code
pairs are harvested from StackOverflow forums to encourage natural and
practical coding queries. Moreover, ODEX supports four natural languages as
intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing
behavioral differences among top-performing code language models (LM). While
CODEX achieves better overall results, CODEGEN improves effectively via scaling
-- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show
substantial gaps between open and closed domains, but CODEGEN gaps tend to
decrease with model size while CODEX gaps increase. We release ODEX to
facilitate research into open-domain problems for the code generation
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10410">
<div class="article-summary-box-inner">
<span><p>Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes are available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis. (arXiv:2302.02813v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02813">
<div class="article-summary-box-inner">
<span><p>The war in Ukraine seems to have positively changed the attitude toward the
critical societal topic of migration in Europe -- at least towards refugees
from Ukraine. We investigate whether this impression is substantiated by how
the topic is reflected in online news and social media, thus linking the
representation of the issue on the Web to its perception in society. For this
purpose, we combine and adapt leading-edge automatic text processing for a
novel multilingual stance detection approach. Starting from 5.5M Twitter posts
published by 565 European news outlets in one year, beginning September 2021,
plus replies, we perform a multilingual analysis of migration-related media
coverage and associated social media interaction for Europe and selected
European countries.
</p>
<p>The results of our analysis show that there is actually a reframing of the
discussion illustrated by the terminology change, e.g., from "migrant" to
"refugee", often even accentuated with phrases such as "real refugees".
However, concerning a stance shift in public perception, the picture is more
diverse than expected. All analyzed cases show a noticeable temporal stance
shift around the start of the war in Ukraine. Still, there are apparent
national differences in the size and stability of this shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07863">
<div class="article-summary-box-inner">
<span><p>The recent emergence of Large Language Models based on the Transformer
architecture has enabled dramatic advancements in the field of Natural Language
Processing. However, these models have long inference latency, which limits
their deployment, and which makes them prohibitively expensive for various
real-time applications. The inference latency is further exacerbated by
autoregressive generative tasks, as models need to run iteratively to generate
tokens sequentially without leveraging token-level parallelization. To address
this, we propose Big Little Decoder (BiLD), a framework that can improve
inference efficiency and latency for a wide range of text generation
applications. The BiLD framework contains two models with different sizes that
collaboratively generate text. The small model runs autoregressively to
generate text with a low inference cost, and the large model is only invoked
occasionally to refine the small model's inaccurate predictions in a
non-autoregressive manner. To coordinate the small and large models, BiLD
introduces two simple yet effective policies: (1) the fallback policy that
determines when to hand control over to the large model; and (2) the rollback
policy that determines when the large model needs to correct the small model's
inaccurate predictions. To evaluate our framework across different tasks and
models, we apply BiLD to various text generation scenarios encompassing machine
translation on IWSLT 2017 De-En and WMT 2014 De-En, and summarization on XSUM
and CNN/DailyMail. On an NVIDIA T4 GPU, our framework achieves a speedup of up
to 2.12x speedup with minimal generation quality degradation. Furthermore, our
framework is fully plug-and-play and can be applied without any modifications
in the training process or model architecture. Our code is open-sourced
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind Arguments by Leveraging Their Definitions. (arXiv:2302.13925v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13925">
<div class="article-summary-box-inner">
<span><p>We describe our experiments for SemEval-2023 Task 4 on the identification of
human values behind arguments (ValueEval). Because human values are subjective
concepts which require precise definitions, we hypothesize that incorporating
the definitions of human values (in the form of annotation instructions and
validated survey items) during model training can yield better prediction
performance. We explore this idea and show that our proposed models perform
better than the challenge organizers' baselines, with improvements in macro F1
scores of up to 18%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniFLG: Unified Facial Landmark Generator from Text or Speech. (arXiv:2302.14337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14337">
<div class="article-summary-box-inner">
<span><p>Talking face generation has been extensively investigated owing to its wide
applicability. The two primary frameworks used for talking face generation
comprise a text-driven framework, which generates synchronized speech and
talking faces from text, and a speech-driven framework, which generates talking
faces from speech. To integrate these frameworks, this paper proposes a unified
facial landmark generator (UniFLG). The proposed system exploits end-to-end
text-to-speech not only for synthesizing speech but also for extracting a
series of latent representations that are common to text and speech, and feeds
it to a landmark decoder to generate facial landmarks. We demonstrate that our
system achieves higher naturalness in both speech synthesis and facial landmark
generation compared to the state-of-the-art text-driven method. We further
demonstrate that our system can generate facial landmarks from speech of
speakers without facial video data or even speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15714">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose a novel system
that uses language models to perform multi-step logical reasoning. Our system
incorporates explicit planning into its inference procedure, thus able to make
more informed reasoning decisions at each step by looking ahead into their
future effects. Moreover, we propose a training strategy that safeguards the
planning process from being led astray by spurious features. Our full system
significantly outperforms other competing methods on multiple standard
datasets. When using a T5 model as its core component, our system performs
competitively compared to GPT-3 despite having only about 1B parameters (i.e.,
175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms
chain-of-thought prompting on the challenging PrOntoQA dataset. We have
conducted extensive empirical studies to demonstrate that explicit planning
plays a crucial role in the system's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09797">
<div class="article-summary-box-inner">
<span><p>The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted extensive and comprehensive experiments on seven benchmarks. The
results show that PHP significantly improves accuracy while remaining highly
efficient. For instance, with text-davinci-003, we observed a 4.2% improvement
on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction
in sample paths with self-consistency. With GPT-4 and PHP, we achieve
state-of-the-art performances on SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%),
AQuA (76.4% -&gt; 79.9%) and MATH (50.3% -&gt; 53.9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02547">
<div class="article-summary-box-inner">
<span><p>Despite the many use cases for large language models (LLMs) in the design of
chatbots in various industries and the research showing the importance of
personalizing chatbots to cater to different personality traits, little work
has been done to evaluate whether the behaviors of personalized LLMs can
reflect certain personality traits accurately and consistently. We consider
studying the behavior of LLM-based simulated agents which refer to as LLM
personas and present a case study with GPT-3.5 (text-davinci-003) to
investigate whether LLMs can generate content with consistent, personalized
traits when assigned Big Five personality types and gender roles. We created
320 LLM personas (5 females and 5 males for each of the 32 Big Five personality
types) and prompted them to complete the classic 44-item Big Five Inventory
(BFI) and then write an 800-word story about their childhood. Results showed
that LLM personas' self-reported BFI scores are consistent with their assigned
personality types, with large effect sizes found on all five traits. Moreover,
significant correlations were found between assigned personality types and some
Linguistic Inquiry and Word Count (LIWC) psycholinguistic features of their
writings. For instance, extroversion is associated with pro-social and active
words, and neuroticism is associated with words related to negative emotions
and mental health. Besides, we only found significant differences in using
technological and cultural words in writing between LLM-generated female and
male personas. This work provides a first step for further research on
personalized LLMs and their applications in Human-AI conversation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06590">
<div class="article-summary-box-inner">
<span><p>In real world applications, knowledge graphs (KG) are widely used in various
domains (e.g. medical applications and dialogue agents). However, for fact
verification, KGs have not been adequately utilized as a knowledge source. KGs
can be a valuable knowledge source in fact verification due to their
reliability and broad applicability. A KG consists of nodes and edges which
makes it clear how concepts are linked together, allowing machines to reason
over chains of topics. However, there are many challenges in understanding how
these machine-readable concepts map to information in text. To enable the
community to better use KGs, we introduce a new dataset, FactKG: Fact
Verification via Reasoning on Knowledge Graphs. It consists of 108k natural
language claims with five types of reasoning: One-hop, Conjunction, Existence,
Multi-hop, and Negation. Furthermore, FactKG contains various linguistic
patterns, including colloquial style claims as well as written style claims to
increase practicality. Lastly, we develop a baseline approach and analyze
FactKG over these reasoning types. We believe FactKG can advance both
reliability and practicality in KG-based fact verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence Modeling. (arXiv:2305.08285v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08285">
<div class="article-summary-box-inner">
<span><p>The increasing size of language models raises great research interests in
parameter-efficient fine-tuning such as LoRA that freezes the pre-trained
model, and injects small-scale trainable parameters for multiple downstream
tasks (e.g., summarization, question answering and translation). To further
enhance the efficiency of fine-tuning, we propose a framework that integrates
LoRA and structured layer pruning. The integrated framework is validated on two
created deidentified medical report summarization datasets based on
MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6%
parameters of the original model and pruning over 30% Transformer-layers, our
framework can reduce 50% of GPU memory usage and speed up 100% of the training
phase, while preserving over 92% generation qualities on free-text
sequence-to-sequence tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08703">
<div class="article-summary-box-inner">
<span><p>Conventional Knowledge Graph Construction (KGC) approaches typically follow
the static information extraction paradigm with a closed set of pre-defined
schema. As a result, such approaches fall short when applied to dynamic
scenarios or domains, whereas a new type of knowledge emerges. This
necessitates a system that can handle evolving schema automatically to extract
information for KGC. To address this need, we propose a new task called
schema-adaptable KGC, which aims to continually extract entity, relation, and
event based on a dynamically changing schema graph without re-training. We
first split and convert existing datasets based on three principles to build a
benchmark, i.e., horizontal schema expansion, vertical schema expansion, and
hybrid schema expansion; then investigate the schema-adaptable performance of
several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We
further propose a simple yet effective baseline dubbed AdaKGC, which contains
schema-enriched prefix instructor and schema-conditioned dynamic decoding to
better handle evolving schema. Comprehensive experimental results illustrate
that AdaKGC can outperform baselines but still have room for improvement. We
hope the proposed work can deliver benefits to the community. Code and datasets
will be available in https://github.com/zjunlp/AdaKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09193">
<div class="article-summary-box-inner">
<span><p>Information extraction (IE) systems aim to automatically extract structured
information, such as named entities, relations between entities, and events,
from unstructured texts. While most existing work addresses a particular IE
task, universally modeling various IE tasks with one model has achieved great
success recently. Despite their success, they employ a one-stage learning
strategy, i.e., directly learning to extract the target structure given the
input text, which contradicts the human learning process. In this paper, we
propose a unified easy-to-hard learning framework consisting of three stages,
i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking
the human learning process. By breaking down the learning process into multiple
stages, our framework facilitates the model to acquire general IE task
knowledge and improve its generalization ability. Extensive experiments across
four IE tasks demonstrate the effectiveness of our framework. We achieve new
state-of-the-art results on 13 out of 17 datasets. Our code is available at
\url{https://github.com/DAMO-NLP-SG/IE-E2H}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09515">
<div class="article-summary-box-inner">
<span><p>Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained with a left-to-right auto-regressive approach. To
account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks, including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated its superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code is available at
https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09846">
<div class="article-summary-box-inner">
<span><p>Detecting norm violations in online communities is critical to maintaining
healthy and safe spaces for online discussions. Existing machine learning
approaches often struggle to adapt to the diverse rules and interpretations
across different communities due to the inherent challenges of fine-tuning
models for such context-specific tasks. In this paper, we introduce
Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a
novel method that employs prompt-based learning to detect norm violations
across various types of rules. CPL-NoViD outperforms the baseline by
incorporating context through natural language prompts and demonstrates
improved performance across different rule types. Significantly, it not only
excels in cross-rule-type and cross-community norm violation detection but also
exhibits adaptability in few-shot learning scenarios. Most notably, it
establishes a new state-of-the-art in norm violation detection, surpassing
existing benchmarks. Our work highlights the potential of prompt-based learning
for context-sensitive norm violation detection and paves the way for future
research on more adaptable, context-aware models to better support online
community moderators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10306">
<div class="article-summary-box-inner">
<span><p>We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10645">
<div class="article-summary-box-inner">
<span><p>This paper looks at the ability of large language models to participate in
educational guided reading. We specifically, evaluate their ability to generate
meaningful questions from the input text, generate diverse questions both in
terms of content coverage and difficulty of the questions and evaluate their
ability to recommend part of the text that a student should re-read based on
the student's responses to the questions. Based on our evaluation of ChatGPT
and Bard, we report that,
</p>
<p>1) Large language models are able to generate high quality meaningful
questions that have high correlation with the input text, 2) They generate
diverse question that cover most topics in the input text even though this
ability is significantly degraded as the input text increases, 3)The large
language models are able to generate both low and high cognitive questions even
though they are significantly biased toward low cognitive question, 4) They are
able to effectively summarize responses and extract a portion of text that
should be re-read.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10847">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10927">
<div class="article-summary-box-inner">
<span><p>The goal of document-grounded dialogue (DocGD) is to generate a response by
grounding the evidence in a supporting document in accordance with the dialogue
context. This process involves four variables that are causally connected.
Recently, task-specific pre-training has greatly boosted performances on many
downstream tasks. Existing DocGD methods, however, continue to rely on general
pre-trained language models without a specifically tailored pre-training
approach that explicitly captures the causal relationships. To tackle this
issue, we are the first to present a causally-complete dataset construction
strategy for building million-level DocGD pre-training corpora. To better
capture causality, we further propose a causally-perturbed pre-training
strategy, which introduces causal perturbations on the variables and optimizes
the overall causal effect. Experiments on three benchmark datasets demonstrate
that our causal pre-training achieves considerable and consistent improvements
under fully-supervised, low-resource, few-shot, and zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10951">
<div class="article-summary-box-inner">
<span><p>The performance of automatic speech recognition (ASR) systems has advanced
substantially in recent years, particularly for languages for which a large
amount of transcribed speech is available. Unfortunately, for low-resource
languages, such as minority languages, regional languages or dialects, ASR
performance generally remains much lower. In this study, we investigate whether
data augmentation techniques could help improve low-resource ASR performance,
focusing on four typologically diverse minority languages or language variants
(West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For
all four languages, we examine the use of self-training, where an ASR system
trained with the available human-transcribed data is used to generate
transcriptions, which are then combined with the original data to train a new
ASR system. For Gronings, for which there was a pre-existing text-to-speech
(TTS) system available, we also examined the use of TTS to generate ASR
training data from text-only sources. We find that using a self-training
approach consistently yields improved performance (a relative WER reduction up
to 20.5% compared to using an ASR system trained on 24 minutes of manually
transcribed speech). The performance gain from TTS augmentation for Gronings
was even stronger (up to 25.5% relative reduction in WER compared to a system
based on 24 minutes of manually transcribed speech). In sum, our results show
the benefit of using self-training or (if possible) TTS-generated data as an
efficient solution to overcome the limitations of data availability for
resource-scarce languages in order to improve ASR performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11000">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models are regarded as a crucial step towards
Artificial General Intelligence (AGI) and have garnered significant interest
with the emergence of ChatGPT. However, current speech-language models
typically adopt the cascade paradigm, preventing inter-modal knowledge
transfer. In this paper, we propose SpeechGPT, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and
generating multi-model content. With discrete speech representations, we first
construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes
modality-adaptation pre-training, cross-modal instruction fine-tuning, and
chain-of-modality instruction fine-tuning. The experimental results demonstrate
that SpeechGPT has an impressive capacity to follow multi-modal human
instructions and highlight the potential of handling multiple modalities with
one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11004">
<div class="article-summary-box-inner">
<span><p>Taxonomy completion, a task aimed at automatically enriching an existing
taxonomy with new concepts, has gained significant interest in recent years.
Previous works have introduced complex modules, external information, and
pseudo-leaves to enrich the representation and unify the matching process of
attachment and insertion. While they have achieved good performance, these
introductions may have brought noise and unfairness during training and
scoring. In this paper, we present TaxBox, a novel framework for taxonomy
completion that maps taxonomy concepts to box embeddings and employs two
probabilistic scorers for concept attachment and insertion, avoiding the need
for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a
graph aggregation module to leverage the structural information of the taxonomy
and two lightweight decoders that map features to box embedding and capture
complex relationships between concepts; (2) two probabilistic scorers that
correspond to attachment and insertion operations and ensure the avoidance of
pseudo-leaves; and (3) three learning objectives that assist the model in
mapping concepts more granularly onto the box embedding space. Experimental
results on four real-world datasets suggest that TaxBox outperforms baseline
methods by a considerable margin and surpasses previous state-of-art methods to
a certain extent.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-22 23:11:37.445775377 UTC">2023-05-22 23:11:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>