<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-02T01:30:00Z">12-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Operationalizing Specifications, In Addition to Test Sets for Evaluating Constrained Generative Models. (arXiv:2212.00006v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00006">
<div class="article-summary-box-inner">
<span><p>In this work, we present some recommendations on the evaluation of
state-of-the-art generative models for constrained generation tasks. The
progress on generative models has been rapid in recent years. These large-scale
models have had three impacts: firstly, the fluency of generation in both
language and vision modalities has rendered common average-case evaluation
metrics much less useful in diagnosing system errors. Secondly, the same
substrate models now form the basis of a number of applications, driven both by
the utility of their representations as well as phenomena such as in-context
learning, which raise the abstraction level of interacting with such models.
Thirdly, the user expectations around these models and their feted public
releases have made the technical challenge of out of domain generalization much
less excusable in practice. Subsequently, our evaluation methodologies haven't
adapted to these changes. More concretely, while the associated utility and
methods of interacting with generative models have expanded, a similar
expansion has not been observed in their evaluation practices. In this paper,
we argue that the scale of generative models could be exploited to raise the
abstraction level at which evaluation itself is conducted and provide
recommendations for the same. Our recommendations are based on leveraging
specifications as a powerful instrument to evaluate generation quality and are
readily applicable to a variety of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Specific Embeddings for Ante-Hoc Explainable Text Classification. (arXiv:2212.00086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00086">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art approaches to text classification typically leverage
BERT-style Transformer models with a softmax classifier, jointly fine-tuned to
predict class labels of a target task. In this paper, we instead propose an
alternative training objective in which we learn task-specific embeddings of
text: our proposed objective learns embeddings such that all texts that share
the same target class label should be close together in the embedding space,
while all others should be far apart. This allows us to replace the softmax
classifier with a more interpretable k-nearest-neighbor classification
approach. In a series of experiments, we show that this yields a number of
interesting benefits: (1) The resulting order induced by distances in the
embedding space can be used to directly explain classification decisions. (2)
This facilitates qualitative inspection of the training data, helping us to
better understand the problem space and identify labelling quality issues. (3)
The learned distances to some degree generalize to unseen classes, allowing us
to incrementally add new classes without retraining the model. We present
extensive experiments which show that the benefits of ante-hoc explainability
and incremental learning come at no cost in overall classification accuracy,
thus pointing to practical applicability of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Alignment in the Era of Deep Learning: A Tutorial. (arXiv:2212.00138v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00138">
<div class="article-summary-box-inner">
<span><p>The word alignment task, despite its prominence in the era of statistical
machine translation (SMT), is niche and under-explored today. In this two-part
tutorial, we argue for the continued relevance for word alignment. The first
part provides a historical background to word alignment as a core component of
the traditional SMT pipeline. We zero-in on GIZA++, an unsupervised,
statistical word aligner with surprising longevity. Jumping forward to the era
of neural machine translation (NMT), we show how insights from word alignment
inspired the attention mechanism fundamental to present-day NMT. The second
part shifts to a survey approach. We cover neural word aligners, showing the
slow but steady progress towards surpassing GIZA++ performance. Finally, we
cover the present-day applications of word alignment, from cross-lingual
annotation projection, to improving translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Relation and Event Type Discovery with Type Abstraction. (arXiv:2212.00178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00178">
<div class="article-summary-box-inner">
<span><p>Conventional closed-world information extraction (IE) approaches rely on
human ontologies to define the scope for extraction. As a result, such
approaches fall short when applied to new domains. This calls for systems that
can automatically infer new types from given corpora, a task which we refer to
as type discovery. To tackle this problem, we introduce the idea of type
abstraction, where the model is prompted to generalize and name the type. Then
we use the similarity between inferred names to induce clusters. Observing that
this abstraction-based representation is often complementary to the
entity/trigger token representation, we set up these two representations as two
views and design our model as a co-training framework. Our experiments on
multiple relation extraction and event extraction datasets consistently show
the advantage of our type abstraction approach. Code available at
https://github.com/raspberryice/type-discovery-abs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUG-FedPrompt: Practical Few-shot Federated NLP with Data-augmented Prompts. (arXiv:2212.00192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00192">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models have become the de-facto solution for
NLP tasks. Fine-tuning such pre-trained models for downstream tasks often
requires tremendous amount of data that is both private and labeled. However,
in reality: 1) such private data cannot be collected and is distributed across
mobile devices, and 2) well-curated labeled data is scarce. To tackle those
issues, we first define a data generator for federated few-shot learning tasks,
which encompasses the quantity and distribution of scarce labeled data in a
realistic setting. Then we propose AUG-FedPrompt, a prompt-based federated
learning algorithm that carefully annotates abundant unlabeled data for data
augmentation. AUG-FedPrompt can perform on par with full-set fine-tuning with
very few initial labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions. (arXiv:2212.00193v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00193">
<div class="article-summary-box-inner">
<span><p>Step-by-step reasoning approaches like chain-of-thought (CoT) have proved to
be a very effective technique to induce reasoning capabilities in large
language models. However, the success of the CoT approach depends primarily on
model size, and often billion parameter-scale models are needed to get CoT to
work. In this paper, we propose a knowledge distillation approach, that
leverages the step-by-step CoT reasoning capabilities of larger models and
distils these reasoning abilities into smaller models. Our approach
Decompositional Distillation learns a semantic decomposition of the original
problem into a sequence of subproblems and uses it to train two models: a) a
problem decomposer that learns to decompose the complex reasoning problem into
a sequence of simpler sub-problems and b) a problem solver that uses the
intermediate subproblems to solve the overall problem. On a multi-step math
word problem dataset (GSM8K), we boost the performance of GPT-2 variants up to
35% when distilled with our approach compared to CoT. We show that using our
approach, it is possible to train a GPT-2-large model (775M) that can
outperform a 10X larger GPT-3 (6B) model trained using CoT reasoning. Finally,
we also demonstrate that our approach of problem decomposition can also be used
as an alternative to CoT prompting, which boosts the GPT-3 performance by 40%
compared to CoT prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00196">
<div class="article-summary-box-inner">
<span><p>Language models trained on massive prompted multitask datasets like T0 (Sanh
et al., 2021) or FLAN (Wei et al., 2021a) can generalize to tasks unseen during
training. We show that training on a carefully chosen subset of instances can
outperform training on all available data on a variety of datasets. We assume
access to a small number (250--1000) of unlabeled target task instances, select
their nearest neighbors from a pool of multitask data, and use the retrieved
data to train target task-specific models. Our method is more data-efficient
than training a single multitask model, while still outperforming it by large
margins. We evaluate across a diverse set of tasks not in the multitask pool we
retrieve from, including those used to evaluate T0 and additional complex tasks
including legal and scientific document QA. We retrieve small subsets of P3
(the collection of prompted datasets from which T0's training data was sampled)
and finetune T5 models that outperform the 3-billion parameter variant of T0
(T0-3B) by 3--30% on 12 out of 14 evaluation datasets while using at most 2% of
the data used to train T0-3B. These models also provide a better initialization
than T0-3B for few-shot finetuning on target-task data, as shown by a 2--23%
relative improvement over few-shot finetuned T0-3B models on 8 datasets. Our
code is available at https://github.com/allenai/data-efficient-finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework. (arXiv:2212.00223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00223">
<div class="article-summary-box-inner">
<span><p>In order to assist the drug discovery/development process, pharmaceutical
companies often apply biomedical NER and linking techniques over internal and
public corpora. Decades of study of the field of BioNLP has produced a plethora
of algorithms, systems and datasets. However, our experience has been that no
single open source system meets all the requirements of a modern pharmaceutical
company. In this work, we describe these requirements according to our
experience of the industry, and present Kazu, a highly extensible, scalable
open source framework designed to support BioNLP for the pharmaceutical sector.
Kazu is a built around a computationally efficient version of the BERN2 NER
model (TinyBERN2), and subsequently wraps several other BioNLP technologies
into one coherent system. KAZU framework is open-sourced:
https://github.com/AstraZeneca/KAZU
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder. (arXiv:2212.00231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00231">
<div class="article-summary-box-inner">
<span><p>Complex dialogue mappings (CDM), including one-to-many and many-to-one
mappings, tend to make dialogue models generate incoherent or dull responses,
and modeling these mappings remains a huge challenge for neural dialogue
systems. To alleviate these problems, methods like introducing external
information, reconstructing the optimization function, and manipulating data
samples are proposed, while they primarily focus on avoiding training with CDM,
inevitably weakening the model's ability of understanding CDM in human
conversations and limiting further improvements in model performance. This
paper proposes a Sentence Semantic \textbf{Seg}mentation guided
\textbf{C}onditional \textbf{V}ariational \textbf{A}uto-\textbf{E}ncoder
(SegCVAE) method which can model and take advantages of the CDM data.
Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE
uses response-related prominent semantics to constrained the latent variable.
To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments
multiple prominent semantics to enrich the latent variables. Three novel
components, Internal Separation, External Guidance, and Semantic Norms, are
proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic
and human evaluation results show that SegCVAE achieves new state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference of Media Bias and Content Quality Using Natural-Language Processing. (arXiv:2212.00237v1 [physics.soc-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00237">
<div class="article-summary-box-inner">
<span><p>Media bias can significantly impact the formation and development of opinions
and sentiments in a population. It is thus important to study the emergence and
development of partisan media and political polarization. However, it is
challenging to quantitatively infer the ideological positions of media outlets.
In this paper, we present a quantitative framework to infer both political bias
and content quality of media outlets from text, and we illustrate this
framework with empirical experiments with real-world data. We apply a
bidirectional long short-term memory (LSTM) neural network to a data set of
more than 1 million tweets to generate a two-dimensional ideological-bias and
content-quality measurement for each tweet. We then infer a ``media-bias
chart'' of (bias, quality) coordinates for the media outlets by integrating the
(bias, quality) measurements of the tweets of the media outlets. We also apply
a variety of baseline machine-learning methods, such as a naive-Bayes method
and a support-vector machine (SVM), to infer the bias and quality values for
each tweet. All of these baseline approaches are based on a bag-of-words
approach. We find that the LSTM-network approach has the best performance of
the examined methods. Our results illustrate the importance of leveraging word
order into machine-learning methods in text analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Label Detection for Speaker Recognition. (arXiv:2212.00239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00239">
<div class="article-summary-box-inner">
<span><p>The success of deep neural networks requires both high annotation quality and
massive data. However, the size and the quality of a dataset are usually a
trade-off in practice, as data collection and cleaning are expensive and
time-consuming. Therefore, automatic noisy label detection (NLD) techniques are
critical to real-world applications, especially those using crowdsourcing
datasets. As this is an under-explored topic in automatic speaker verification
(ASV), we present a simple but effective solution to the task. First, we
compare the effectiveness of various commonly used metric learning loss
functions under different noise settings. Then, we propose two ranking-based
NLD methods, inter-class inconsistency and intra-class inconsistency ranking.
They leverage the inconsistent nature of noisy labels and show high detection
precision even under a high level of noise. Our solution gives rise to both
efficient and effective cleaning of large-scale speaker recognition datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. (arXiv:2212.00259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00259">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) models often perform poorly on
out-of-distribution data and struggle on domain generalization. Due to the
multi-modal nature of this task, multiple factors of variation are intertwined,
making generalization difficult to analyze. This motivates us to introduce a
virtual benchmark, Super-CLEVR, where different factors in VQA domain shifts
can be isolated in order that their effects can be studied independently. Four
factors are considered: visual complexity, question redundancy, concept
distribution and concept compositionality. With controllably generated data,
Super-CLEVR enables us to test VQA methods in situations where the test data
differs from the training data along each of these axes. We study four existing
methods, including two neural symbolic methods NSCL and NSVQA, and two
non-symbolic methods FiLM and mDETR; and our proposed method, probabilistic
NSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA
outperforms other methods on three of the four domain shift factors. Our
results suggest that disentangling reasoning and perception, combined with
probabilistic uncertainty, form a strong VQA model that is more robust to
domain shifts. The dataset and code are released at
https://github.com/Lizw14/Super-CLEVR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIZZA: A new benchmark for complex end-to-end task-oriented parsing. (arXiv:2212.00265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00265">
<div class="article-summary-box-inner">
<span><p>Much recent work in task-oriented parsing has focused on finding a middle
ground between flat slots and intents, which are inexpressive but easy to
annotate, and powerful representations such as the lambda calculus, which are
expressive but costly to annotate. This paper continues the exploration of
task-oriented parsing by introducing a new dataset for parsing pizza and drink
orders, whose semantics cannot be captured by flat slots and intents. We
perform an extensive evaluation of deep-learning techniques for task-oriented
parsing on this dataset, including different flavors of seq2seq systems and
RNNGs. The dataset comes in two main versions, one in a recently introduced
utterance-level hierarchical notation that we call TOP, and one whose targets
are executable representations (EXR). We demonstrate empirically that training
the parser to directly generate EXR notation not only solves the problem of
entity resolution in one fell swoop and overcomes a number of expressive
limitations of TOP notation, but also results in significantly greater parsing
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization vs. Semantics: How Can Language Benefit Visual Representation Learning?. (arXiv:2212.00281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00281">
<div class="article-summary-box-inner">
<span><p>Despite the superior performance brought by vision-and-language pretraining,
it remains unclear whether learning with multi-modal data can help understand
each individual modality. In this work, we investigate how language can help
with visual representation learning from a probing perspective. Specifically,
we compare vision-and-language and vision-only models by probing their visual
representations on a broad range of tasks, in order to assess the quality of
the learned representations in a fine-grained manner. Interestingly, our
probing results suggest that vision-and-language models are better at label
prediction tasks like object and attribute prediction, while vision-only models
are stronger at dense prediction tasks that require more localized information.
With further analysis using detailed metrics, our study suggests that language
helps vision models learn better semantics, but not localization. Code is
released at https://github.com/Lizw14/visual_probing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Commonsense-Infused Language-Agnostic Learning Framework for Enhancing Prediction of Political Polarity in Multilingual News Headlines. (arXiv:2212.00298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00298">
<div class="article-summary-box-inner">
<span><p>Predicting the political polarity of news headlines is a challenging task
that becomes even more challenging in a multilingual setting with low-resource
languages. To deal with this, we propose to utilise the Inferential Commonsense
Knowledge via a Translate-Retrieve-Translate strategy to introduce a learning
framework. To begin with, we use the method of translation and retrieval to
acquire the inferential knowledge in the target language. We then employ an
attention mechanism to emphasise important inferences. We finally integrate the
attended inferences into a multilingual pre-trained language model for the task
of bias prediction. To evaluate the effectiveness of our framework, we present
a dataset of over 62.6K multilingual news headlines in five European languages
annotated with their respective political polarities. We evaluate several
state-of-the-art multilingual pre-trained language models since their
performance tends to vary across languages (low/high resource). Evaluation
results demonstrate that our proposed framework is effective regardless of the
models employed. Overall, the best performing model trained with only headlines
show 0.90 accuracy and F1, and 0.83 jaccard score. With attended knowledge in
our framework, the same model show an increase in 2.2% accuracy and F1, and
3.6% jaccard score. Extending our experiments to individual languages reveals
that the models we analyze for Slovenian perform significantly worse than other
languages in our dataset. To investigate this, we assess the effect of
translation quality on prediction performance. It indicates that the disparity
in performance is most likely due to poor translation quality. We release our
dataset and scripts at: https://github.com/Swati17293/KG-Multi-Bias for future
research. Our framework has the potential to benefit journalists, social
scientists, news producers, and consumers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Select from Multiple Options. (arXiv:2212.00301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00301">
<div class="article-summary-box-inner">
<span><p>Many NLP tasks can be regarded as a selection problem from a set of options,
such as classification tasks, multi-choice question answering, etc. Textual
entailment (TE) has been shown as the state-of-the-art (SOTA) approach to
dealing with those selection problems. TE treats input texts as premises (P),
options as hypotheses (H), then handles the selection problem by modeling (P,
H) pairwise. Two limitations: first, the pairwise modeling is unaware of other
options, which is less intuitive since humans often determine the best options
by comparing competing candidates; second, the inference process of pairwise TE
is time-consuming, especially when the option space is large. To deal with the
two issues, this work first proposes a contextualized TE model (Context-TE) by
appending other k options as the context of the current (P, H) modeling.
Context-TE is able to learn more reliable decision for the H since it considers
various context. Second, we speed up Context-TE by coming up with Parallel-TE,
which learns the decisions of multiple options simultaneously. Parallel-TE
significantly improves the inference speed while keeping comparable performance
with Context-TE. Our methods are evaluated on three tasks (ultra-fine entity
typing, intent detection and multi-choice QA) that are typical selection
problems with different sizes of options. Experiments show our models set new
SOTA performance; particularly, Parallel-TE is faster than the pairwise TE by k
times in inference. Our code is publicly available at
https://github.com/jiangshdd/LearningToSelect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anger Breeds Controversy: Analyzing Controversy and Emotions on Reddit. (arXiv:2212.00339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00339">
<div class="article-summary-box-inner">
<span><p>Emotions play an important role in interpersonal interactions and social
conflict, yet their function in the development of controversy and disagreement
in online conversations has not been explored. To address this gap, we study
controversy on Reddit, a popular network of online discussion forums. We
collect discussions from a wide variety of topical forums and use emotion
detection to recognize a range of emotions from text, including anger, fear,
joy, admiration, etc. Our study has three main findings. First, controversial
comments express more anger and less admiration, joy and optimism than
non-controversial comments. Second, controversial comments affect emotions of
downstream comments in a discussion, usually resulting in long-term increase in
anger and a decrease in positive emotions, although the magnitude and direction
of emotional change depends on the forum. Finally, we show that emotions help
better predict which comments will become controversial. Understanding
emotional dynamics of online discussions can help communities to better manage
conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Pre-training on True Negatives. (arXiv:2212.00460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00460">
<div class="article-summary-box-inner">
<span><p>Discriminative pre-trained language models (PLMs) learn to predict original
texts from intentionally corrupted ones. Taking the former text as positive and
the latter as negative samples, the PLM can be trained effectively for
contextualized representation. However, the training of such a type of PLMs
highly relies on the quality of the automatically constructed samples. Existing
PLMs simply treat all corrupted texts as equal negative without any
examination, which actually lets the resulting model inevitably suffer from the
false negative issue where training is carried out on pseudo-negative data and
leads to less efficiency and less robustness in the resulting PLMs. In this
work, on the basis of defining the false negative issue in discriminative PLMs
that has been ignored for a long time, we design enhanced pre-training methods
to counteract false negative predictions and encourage pre-training language
models on true negatives by correcting the harmful gradient updates subject to
false negative predictions. Experimental results on GLUE and SQuAD benchmarks
show that our counter-false-negative pre-training methods indeed bring about
better performance together with stronger robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUNI Non-Autoregressive System for the WMT 22 Efficient Translation Shared Task. (arXiv:2212.00477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00477">
<div class="article-summary-box-inner">
<span><p>We present a non-autoregressive system submission to the WMT 22 Efficient
Translation Shared Task. Our system was used by Helcl et al. (2022) in an
attempt to provide fair comparison between non-autoregressive and
autoregressive models. This submission is an effort to establish solid
baselines along with sound evaluation methodology, particularly in terms of
measuring the decoding speed. The model itself is a 12-layer Transformer model
trained with connectionist temporal classification on knowledge-distilled
dataset by a strong autoregressive teacher model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection. (arXiv:2212.00482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00482">
<div class="article-summary-box-inner">
<span><p>The task of response selection in multi-turn dialogue is to find the best
option from all candidates. In order to improve the reasoning ability of the
model, previous studies pay more attention to using explicit algorithms to
model the dependencies between utterances, which are deterministic, limited and
inflexible. In addition, few studies consider differences between the options
before and after reasoning. In this paper, we propose an Implicit Relational
Reasoning Graph Network to address these issues, which consists of the
Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR
aims to implicitly extract dependencies between utterances, as well as
utterances and options, and make reasoning with relational graph convolutional
networks. ODC focuses on perceiving the difference between the options through
dual comparison, which can eliminate the interference of the noise options.
Experimental results on two multi-turn dialogue reasoning benchmark datasets
MuTual and MuTual+ show that our method significantly improves the baseline of
four pretrained language models and achieves state-of-the-art performance. The
model surpasses human performance for the first time on the MuTual dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUNI Systems for the WMT22 Czech-Ukrainian Translation Task. (arXiv:2212.00486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00486">
<div class="article-summary-box-inner">
<span><p>We present Charles University submissions to the WMT22 General Translation
Shared Task on Czech-Ukrainian and Ukrainian-Czech machine translation. We
present two constrained submissions based on block back-translation and tagged
back-translation and experiment with rule-based romanization of Ukrainian. Our
results show that the romanization only has a minor effect on the translation
quality. Further, we describe Charles Translator, a system that was developed
in March 2022 as a response to the migration from Ukraine to the Czech
Republic. Compared to our constrained systems, it did not use the romanization
and used some proprietary data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition. (arXiv:2212.00500v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00500">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel multi-modal multi-task encoder-decoder
pre-training framework (MMSpeech) for Mandarin automatic speech recognition
(ASR), which employs both unlabeled speech and text data. The main difficulty
in speech-text joint pre-training comes from the significant difference between
speech and text modalities, especially for Mandarin speech and text. Unlike
English and other languages with an alphabetic writing system, Mandarin uses an
ideographic writing system where character and sound are not tightly mapped to
one another. Therefore, we propose to introduce the phoneme modality into
pre-training, which can help capture modality-invariant information between
Mandarin speech and text. Specifically, we employ a multi-task learning
framework including five self-supervised and supervised tasks with speech and
text data. For end-to-end pre-training, we introduce self-supervised
speech-to-pseudo-codes (S2C) and phoneme-to-text (P2T) tasks utilizing
unlabeled speech and text data, where speech-pseudo-codes pairs and
phoneme-text pairs are a supplement to the supervised speech-text pairs. To
train the encoder to learn better speech representation, we introduce
self-supervised masked speech prediction (MSP) and supervised phoneme
prediction (PP) tasks to learn to map speech into phonemes. Besides, we
directly add the downstream supervised speech-to-text (S2T) task into the
pre-training process, which can further improve the pre-training performance
and achieve better recognition results even without fine-tuning. Experiments on
AISHELL-1 show that our proposed method achieves state-of-the-art performance,
with a more than 40% relative improvement compared with other pre-training
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CultureBERT: Fine-Tuning Transformer-Based Language Models for Corporate Culture. (arXiv:2212.00509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00509">
<div class="article-summary-box-inner">
<span><p>This paper introduces supervised machine learning to the literature measuring
corporate culture from text documents. We compile a unique data set of employee
reviews that were labeled by human evaluators with respect to the information
the reviews reveal about the firms' corporate culture. Using this data set, we
fine-tune state-of-the-art transformer-based language models to perform the
same classification task. In out-of-sample predictions, our language models
classify 16 to 28 percent points more of employee reviews in line with human
evaluators than traditional approaches of text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on the application of contrastive learning in multi-label text classification. (arXiv:2212.00552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00552">
<div class="article-summary-box-inner">
<span><p>The effective application of contrastive learning technology in natural
language processing tasks shows the superiority of contrastive learning in text
analysis tasks. How to construct positive and negative samples correctly and
reasonably is the core challenge of contrastive learning. Since it is difficult
to construct contrastive objects in multi-label multi-classification tasks,
there are few contrastive losses for multi-label multi-classification text
classification. In this paper, we propose five contrastive losses for
multi-label multi-classification tasks. They are Strict Contrastive Loss (SCL),
Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL),
and Jaccard Similarity Probability Contrastive Loss (JSPCL) and Stepwise Label
Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning
for multi-label multi-classification tasks under different strategies, and
provide a set of baseline methods for contrastive learning techniques on
multi-label classification tasks. We also perform an interpretability analysis
of our approach to show how different contrastive learning methods play their
roles. The experimental results in this paper demonstrate that our proposed
contrastive losses can bring some improvement for multi-label
multi-classification tasks. Our work reveal how to "appropriately" change the
contrastive way of contrastive learning is the key idea to improve the
adaptability of contrastive learning in multi-label multi-classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Document Cross-Lingual Summarization. (arXiv:2212.00586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00586">
<div class="article-summary-box-inner">
<span><p>Cross-Lingual Summarization (CLS) aims at generating summaries in one
language for the given documents in another language. CLS has attracted wide
research attention due to its practical significance in the multi-lingual
world. Though great contributions have been made, existing CLS works typically
focus on short documents, such as news articles, short dialogues and guides.
Different from these short texts, long documents such as academic articles and
business reports usually discuss complicated subjects and consist of thousands
of words, making them non-trivial to process and summarize. To promote CLS
research on long documents, we construct Perseus, the first long-document CLS
dataset which collects about 94K Chinese scientific documents paired with
English summaries. The average length of documents in Perseus is more than two
thousand tokens. As a preliminary study on long-document CLS, we build and
evaluate various CLS baselines, including pipeline and end-to-end methods.
Experimental results on Perseus show the superiority of the end-to-end
baseline, outperforming the strong pipeline models equipped with sophisticated
machine translation systems. Furthermore, to provide a deeper understanding, we
manually analyze the model outputs and discuss specific challenges faced by
current approaches. We hope that our work could benchmark long-document CLS and
benefit future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding generation for text classification of Brazilian Portuguese user reviews: from bag-of-words to transformers. (arXiv:2212.00587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00587">
<div class="article-summary-box-inner">
<span><p>Text classification is a natural language processing (NLP) task relevant to
many commercial applications, like e-commerce and customer service. Naturally,
classifying such excerpts accurately often represents a challenge, due to
intrinsic language aspects, like irony and nuance. To accomplish this task, one
must provide a robust numerical representation for documents, a process known
as embedding. Embedding represents a key NLP field nowadays, having faced a
significant advance in the last decade, especially after the introduction of
the word-to-vector concept and the popularization of Deep Learning models for
solving NLP tasks, including Convolutional Neural Networks (CNNs), Recurrent
Neural Networks (RNNs), and Transformer-based Language Models (TLMs). Despite
the impressive achievements in this field, the literature coverage regarding
generating embeddings for Brazilian Portuguese texts is scarce, especially when
considering commercial user reviews. Therefore, this work aims to provide a
comprehensive experimental study of embedding approaches targeting a binary
sentiment classification of user reviews in Brazilian Portuguese. This study
includes from classical (Bag-of-Words) to state-of-the-art (Transformer-based)
NLP models. The methods are evaluated with five open-source databases with
pre-defined data partitions made available in an open digital repository to
encourage reproducibility. The Fine-tuned TLMs achieved the best results for
all cases, being followed by the Feature-based TLM, LSTM, and CNN, with
alternate ranks, depending on the database under analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language models and brain alignment: beyond word-level semantics and prediction. (arXiv:2212.00596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00596">
<div class="article-summary-box-inner">
<span><p>Pretrained language models that have been trained to predict the next word
over billions of text documents have been shown to also significantly predict
brain recordings of people comprehending language. Understanding the reasons
behind the observed similarities between language in machines and language in
the brain can lead to more insight into both systems. Recent works suggest that
the prediction of the next word is a key mechanism that contributes to the
alignment between the two. What is not yet understood is whether prediction of
the next word is necessary for this observed alignment or simply sufficient,
and whether there are other shared mechanisms or information that is similarly
important. In this work, we take a first step towards a better understanding
via two simple perturbations in a popular pretrained language model. The first
perturbation is to improve the model's ability to predict the next word in the
specific naturalistic stimulus text that the brain recordings correspond to. We
show that this indeed improves the alignment with the brain recordings.
However, this improved alignment may also be due to any improved word-level or
multi-word level semantics for the specific world that is described by the
stimulus narrative. We aim to disentangle the contribution of next word
prediction and semantic knowledge via our second perturbation: scrambling the
word order at inference time, which reduces the ability to predict the next
word, but maintains any newly learned word-level semantics. By comparing the
alignment with brain recordings of these differently perturbed models, we show
that improvements in alignment with brain recordings are due to more than
improvements in next word prediction and word-level semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extensible Prompts for Language Models. (arXiv:2212.00616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00616">
<div class="article-summary-box-inner">
<span><p>We propose eXtensible Prompt (X-Prompt) for prompting a large language model
(LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL
but also an extensible vocabulary of imaginary words that are introduced to
help represent what NL words hardly describe, allowing a prompt to be more
descriptive. Like NL prompts, X-Prompt is out-of-distribution (OOD) robust, for
which we propose context-guided learning with prompt augmentation to learn its
imaginary words for general usability, enabling them to use in different prompt
contexts for fine-grain specifications. The promising results of X-Prompt
demonstrate its potential of approaching advanced interaction between humans
and LLMs to bridge their communication gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the State of Computer Science Research with the DBLP Discovery Dataset. (arXiv:2212.00629v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00629">
<div class="article-summary-box-inner">
<span><p>The number of scientific publications continues to rise exponentially,
especially in Computer Science (CS). However, current solutions to analyze
those publications restrict access behind a paywall, offer no features for
visual analysis, limit access to their data, only focus on niches or
sub-fields, and/or are not flexible and modular enough to be transferred to
other datasets. In this thesis, we conduct a scientometric analysis to uncover
the implicit patterns hidden in CS metadata and to determine the state of CS
research. Specifically, we investigate trends of the quantity, impact, and
topics for authors, venues, document types (conferences vs. journals), and
fields of study (compared to, e.g., medicine). To achieve this we introduce the
CS-Insights system, an interactive web application to analyze CS publications
with various dashboards, filters, and visualizations. The data underlying this
system is the DBLP Discovery Dataset (D3), which contains metadata from 5
million CS publications. Both D3 and CS-Insights are open-access, and
CS-Insights can be easily adapted to other datasets in the future. The most
interesting findings of our scientometric analysis include that i) there has
been a stark increase in publications, authors, and venues in the last two
decades, ii) many authors only recently joined the field, iii) the most cited
authors and venues focus on computer vision and pattern recognition, while the
most productive prefer engineering-related topics, iv) the preference of
researchers to publish in conferences over journals dwindles, v) on average,
journal articles receive twice as many citations compared to conference papers,
but the contrast is much smaller for the most cited conferences and journals,
and vi) journals also get more citations in all other investigated fields of
study, while only CS and engineering publish more in conferences than journals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis. (arXiv:2212.00678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00678">
<div class="article-summary-box-inner">
<span><p>Multimodal learning pipelines have benefited from the success of pretrained
language models. However, this comes at the cost of increased model parameters.
In this work, we propose Adapted Multimodal BERT (AMB), a BERT-based
architecture for multimodal tasks that uses a combination of adapter modules
and intermediate fusion layers. The adapter adjusts the pretrained language
model for the task at hand, while the fusion layers perform task-specific,
layer-wise fusion of audio-visual information with textual BERT
representations. During the adaptation process the pre-trained language model
parameters remain frozen, allowing for fast, parameter-efficient training. In
our ablations we see that this approach leads to efficient models, that can
outperform their fine-tuned counterparts and are robust to input noise. Our
experiments on sentiment analysis with CMU-MOSEI show that AMB outperforms the
current state-of-the-art across metrics, with 3.4% relative reduction in the
resulting error and 2.1% relative improvement in 7-class classification
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text. (arXiv:2212.00689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00689">
<div class="article-summary-box-inner">
<span><p>Climate change is threatening human health in unprecedented orders and many
ways. These threats are expected to grow unless effective and evidence-based
policies are developed and acted upon to minimize or eliminate them. Attaining
such a task requires the highest degree of the flow of knowledge from science
into policy. The multidisciplinary, location-specific, and vastness of
published science makes it challenging to keep track of novel work in this
area, as well as making the traditional knowledge synthesis methods inefficient
in infusing science into policy. To this end, we consider developing multiple
domain-specific language models (LMs) with different variations from Climate-
and Health-related information, which can serve as a foundational step toward
capturing available knowledge to enable solving different tasks, such as
detecting similarities between climate- and health-related concepts,
fact-checking, relation extraction, evidence of health effects to policy text
generation, and more. To our knowledge, this is the first work that proposes
developing multiple domain-specific language models for the considered domains.
We will make the developed models, resources, and codebase available for the
researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do you MEME? Generating Explanations for Visual Semantic Role Labelling in Memes. (arXiv:2212.00715v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00715">
<div class="article-summary-box-inner">
<span><p>Memes are powerful means for effective communication on social media. Their
effortless amalgamation of viral visuals and compelling messages can have
far-reaching implications with proper marketing. Previous research on memes has
primarily focused on characterizing their affective spectrum and detecting
whether the meme's message insinuates any intended harm, such as hate, offense,
racism, etc. However, memes often use abstraction, which can be elusive. Here,
we introduce a novel task - EXCLAIM, generating explanations for visual
semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset
that offers natural language explanations of connotative roles for three types
of entities - heroes, villains, and victims, encompassing 4,680 entities
present in 3K memes. We also benchmark ExHVV with several strong unimodal and
multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task
learning framework that endeavors to address EXCLAIM optimally by jointly
learning to predict the correct semantic roles and correspondingly to generate
suitable natural language explanations. LUMEN distinctly outperforms the best
baseline across 18 standard natural language generation evaluation metrics. Our
systematic evaluation and analyses demonstrate that characteristic multimodal
cues required for adjudicating semantic roles are also helpful for generating
suitable explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving astroBERT using Semantic Textual Similarity. (arXiv:2212.00744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00744">
<div class="article-summary-box-inner">
<span><p>The NASA Astrophysics Data System (ADS) is an essential tool for researchers
that allows them to explore the astronomy and astrophysics scientific
literature, but it has yet to exploit recent advances in natural language
processing. At ADASS 2021, we introduced astroBERT, a machine learning language
model tailored to the text used in astronomy papers in ADS. In this work we:
</p>
<p>- announce the first public release of the astroBERT language model;
</p>
<p>- show how astroBERT improves over existing public language models on
astrophysics specific tasks;
</p>
<p>- and detail how ADS plans to harness the unique structure of scientific
papers, the citation graph and citation context, to further improve astroBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00768">
<div class="article-summary-box-inner">
<span><p>Sequence models based on linear state spaces (SSMs) have recently emerged as
a promising choice of architecture for modeling long range dependencies across
various modalities. However, they invariably rely on discretization of a
continuous state space, which complicates their presentation and understanding.
In this work, we dispose of the discretization step, and propose a model based
on vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that
$\mathrm{DLR}$ is as performant as previously-proposed SSMs in the presence of
strong supervision, despite being conceptually much simpler. Moreover, we
characterize the expressivity of SSMs (including $\mathrm{DLR}$) and
attention-based models via a suite of $13$ synthetic sequence-to-sequence tasks
involving interactions over tens of thousands of tokens, ranging from simple
operations, such as shifting an input sequence, to detecting co-dependent
visual features over long spatial ranges in flattened images. We find that
while SSMs report near-perfect performance on tasks that can be modeled via
$\textit{few}$ convolutional kernels, they struggle on tasks requiring
$\textit{many}$ such kernels and especially when the desired sequence
manipulation is $\textit{context-dependent}$. For example, $\mathrm{DLR}$
learns to perfectly shift a $0.5M$-long input by an arbitrary number of
positions but fails when the shift size depends on context. Despite these
limitations, $\mathrm{DLR}$ reaches high performance on two higher-order
reasoning tasks $\mathrm{ListOpsSubTrees}$ and
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths $8K$
and $65K$ respectively, and gives encouraging performance on
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$
for which attention is not a viable choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColBERT: Using BERT Sentence Embedding in Parallel Neural Networks for Computational Humor. (arXiv:2004.12765v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12765">
<div class="article-summary-box-inner">
<span><p>Automation of humor detection and rating has interesting use cases in modern
technologies, such as humanoid robots, chatbots, and virtual assistants. In
this paper, we propose a novel approach for detecting and rating humor in short
texts based on a popular linguistic theory of humor. The proposed technical
method initiates by separating sentences of the given text and utilizing the
BERT model to generate embeddings for each one. The embeddings are fed to
separate lines of hidden layers in a neural network (one line for each
sentence) to extract latent features. At last, the parallel lines are
concatenated to determine the congruity and other relationships between the
sentences and predict the target value. We accompany the paper with a novel
dataset for humor detection consisting of 200,000 formal short texts. In
addition to evaluating our work on the novel dataset, we participated in a live
machine learning competition focused on rating humor in Spanish tweets. The
proposed model obtained F1 scores of 0.982 and 0.869 in the humor detection
experiments which outperform general and state-of-the-art models. The
evaluation performed on two contrasting settings confirm the strength and
robustness of the model and suggests two important factors in achieving high
accuracy in the current task: 1) usage of sentence embeddings and 2) utilizing
the linguistic structure of humor in designing the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts. (arXiv:2205.11961v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11961">
<div class="article-summary-box-inner">
<span><p>This work introduces a new multi-task, parameter-efficient language model
(LM) tuning method that learns to transfer knowledge across different tasks via
a mixture of soft prompts-small prefix embedding vectors pre-trained for
different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt
Tuning), obtains source prompts as encodings of large-scale source tasks into a
small number of parameters and trains an attention module to interpolate the
source prompts and a newly initialized target prompt for every instance in the
target task. During training, only the target task prompt and the attention
weights, which are shared between tasks in multi-task training, are updated,
while the original LM and source prompts are intact. ATTEMPT is highly
parameter-efficient (e.g., updates 2,300 times fewer parameters than full
fine-tuning) while achieving high task performance using knowledge from
high-resource tasks. Moreover, it is modular using pre-trained soft prompts,
and can flexibly add or remove source prompts for effective knowledge transfer.
Our experimental results across 21 diverse NLP datasets show that ATTEMPT
significantly outperforms prompt tuning and outperforms or matches fully
fine-tuned or other parameter-efficient tuning approaches that use over ten
times more parameters. Finally, ATTEMPT outperforms previous work in few-shot
learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Conditional Coverage & Calibration via Neural Model Approximations. (arXiv:2205.14310v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14310">
<div class="article-summary-box-inner">
<span><p>A typical desideratum for quantifying the uncertainty from a classification
model as a prediction set is class-conditional singleton set calibration. That
is, such sets should map to the output of well-calibrated selective
classifiers, matching the observed frequencies of similar instances. Recent
works proposing adaptive and localized conformal p-values for deep networks do
not guarantee this behavior, nor do they achieve it empirically. Instead, we
use the strong signals for prediction reliability from KNN-based approximations
of Transformer networks to construct data-driven partitions for Mondrian
Conformal Predictors, which are treated as weak selective classifiers that are
then calibrated via a new Inductive Venn Predictor, the Venn-ADMIT Predictor.
The resulting selective classifiers are well-calibrated, in a conservative but
practically useful sense for a given threshold. They are inherently robust to
changes in the proportions of the data partitions, and straightforward
conservative heuristics provide additional robustness to covariate shifts. We
compare and contrast to the quantities produced by recent Conformal Predictors
on several representative and challenging natural language processing
classification tasks, including class-imbalanced and distribution-shifted
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11697">
<div class="article-summary-box-inner">
<span><p>Recently Convolution-augmented Transformer (Conformer) has shown promising
results in Automatic Speech Recognition (ASR), outperforming the previous best
published Transformer Transducer. In this work, we believe that the output
information of each block in the encoder and decoder is not completely
inclusive, in other words, their output information may be complementary. We
study how to take advantage of the complementary information of each block in a
parameter-efficient way, and it is expected that this may lead to more robust
performance. Therefore we propose the Block-augmented Transformer for speech
recognition, named Blockformer. We have implemented two block ensemble methods:
the base Weighted Sum of the Blocks Output (Base-WSBO), and the
Squeeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).
Experiments have proved that the Blockformer significantly outperforms the
state-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER
of 4.29\% without using a language model and 4.05\% with an external language
model on the testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03273">
<div class="article-summary-box-inner">
<span><p>Entities, as important carriers of real-world knowledge, play a key role in
many NLP tasks. We focus on incorporating entity knowledge into an
encoder-decoder framework for informative text generation. Existing approaches
tried to index, retrieve, and read external documents as evidence, but they
suffered from a large computational overhead. In this work, we propose an
encoder-decoder framework with an entity memory, namely EDMem. The entity
knowledge is stored in the memory as latent representations, and the memory is
pre-trained on Wikipedia along with encoder-decoder parameters. To precisely
generate entity names, we design three decoding methods to constrain entity
generation by linking entities in the memory. EDMem is a unified framework that
can be used on various entity-intensive question answering and generation
tasks. Extensive experimental results show that EDMem outperforms both
memory-based auto-encoder models and non-memory encoder-decoder models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09723">
<div class="article-summary-box-inner">
<span><p>Textual entailment recognition is one of the basic natural language
understanding(NLU) tasks. Understanding the meaning of sentences is a
prerequisite before applying any natural language processing(NLP) techniques to
automatically recognize the textual entailment. A text entails a hypothesis if
and only if the true value of the hypothesis follows the text. Classical
approaches generally utilize the feature value of each word from word embedding
to represent the sentences. In this paper, we propose a novel approach to
identifying the textual entailment relationship between text and hypothesis,
thereby introducing a new semantic feature focusing on empirical
threshold-based semantic text representation. We employ an element-wise
Manhattan distance vector-based feature that can identify the semantic
entailment relationship between the text-hypothesis pair. We carried out
several experiments on a benchmark entailment classification(SICK-RTE) dataset.
We train several machine learning(ML) algorithms applying both semantic and
lexical features to classify the text-hypothesis pair as entailment, neutral,
or contradiction. Our empirical sentence representation technique enriches the
semantic information of the texts and hypotheses found to be more efficient
than the classical ones. In the end, our approach significantly outperforms
known methods in understanding the meaning of the sentences for the textual
entailment classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12714">
<div class="article-summary-box-inner">
<span><p>Generative Knowledge Graph Construction (KGC) refers to those methods that
leverage the sequence-to-sequence framework for building knowledge graphs,
which is flexible and can be adapted to widespread tasks. In this study, we
summarize the recent compelling progress in generative knowledge graph
construction. We present the advantages and weaknesses of each paradigm in
terms of different generation targets and provide theoretical insight and
empirical analysis. Based on the review, we suggest promising research
directions for the future. Our contributions are threefold: (1) We present a
detailed, complete taxonomy for the generative KGC methods; (2) We provide a
theoretical and empirical analysis of the generative KGC methods; (3) We
propose several research directions that can be developed in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tail Batch Sampling: Approximating Global Contrastive Losses as Optimization over Batch Assignments. (arXiv:2210.12874v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12874">
<div class="article-summary-box-inner">
<span><p>Contrastive Learning has recently achieved state-of-the-art performance in a
wide range of tasks. Many contrastive learning approaches use mined hard
negatives to make batches more informative during training but these approaches
are inefficient as they increase epoch length proportional to the number of
mined negatives and require frequent updates of nearest neighbor indices or
mining from recent batches. In this work, we provide an alternative to hard
negative mining in supervised contrastive learning, Tail Batch Sampling (TBS),
an efficient approximation to the batch assignment problem that upper bounds
the gap between the global and training losses, $\mathcal{L}^{Global} -
\mathcal{L}^{Train}$. TBS \textbf{improves state-of-the-art performance} in
sentence embedding (+0.37 Spearman) and code-search tasks (+2.2\% MRR), is easy
to implement - requiring only a few additional lines of code, does not maintain
external data structures such as nearest neighbor indices, is more
computationally efficient when compared to the most minimal hard negative
mining approaches, and makes no changes to the model being trained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maknuune: A Large Open Palestinian Arabic Lexicon. (arXiv:2210.12985v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12985">
<div class="article-summary-box-inner">
<span><p>We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.
Maknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries
include diacritized Arabic orthography, phonological transcription and English
glosses. Some entries are enriched with additional information such as broken
plurals and templatic feminine forms, associated phrases and collocations,
Standard Arabic glosses, and examples or notes on grammar, usage, or location
of collected entry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality Detection using Multiple Annotation Decisions. (arXiv:2210.14852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14852">
<div class="article-summary-box-inner">
<span><p>The paper describes the work that has been submitted to the 5th workshop on
Challenges and Applications of Automated Extraction of socio-political events
from text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3
that aims to detect causality in protest news corpus. The authors used
different large language models with customized cross-entropy loss functions
that exploit annotation information. The experiments showed that
bert-based-uncased with refined cross-entropy outperformed the others,
achieving a F1 score of 0.8501 on the Causal News Corpus dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09778">
<div class="article-summary-box-inner">
<span><p>Many high-level skills that are required for computer vision tasks, such as
parsing questions, comparing and contrasting semantics, and writing
descriptions, are also required in other domains such as natural language
processing. In this paper, we ask whether this makes it possible to learn those
skills from text data and then use them to complete vision tasks without ever
training on visual training data. Key to our approach is exploiting the joint
embedding space of contrastively trained vision and language encoders. In
practice, there can be systematic differences between embedding spaces for
different modalities in contrastive models, and we analyze how these
differences affect our approach and study a variety of strategies to mitigate
this concern. We produce models using only text training data on three tasks:
image captioning, visual entailment and visual question answering, and evaluate
them on standard benchmarks using images. We find that this kind of transfer is
possible and results in only a small drop in performance relative to models
trained on images. We also showcase a variety of stylistic image captioning
models that were trained using no image data and no human-curated language
data, but instead text data from books, the web, or language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models. (arXiv:2211.14777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14777">
<div class="article-summary-box-inner">
<span><p>Alignment between image and text has shown promising improvements on
patch-level pre-trained document image models. However, investigating more
effective or finer-grained alignment techniques during pre-training requires a
large amount of computation cost and time. Thus, a question naturally arises:
Could we fine-tune the pre-trained models adaptive to downstream tasks with
alignment objectives and achieve comparable or better performance? In this
paper, we propose a new model architecture with alignment-enriched tuning
(dubbed AETNet) upon pre-trained document image models, to adapt downstream
tasks with the joint task-specific supervised and alignment-aware contrastive
objective. Specifically, we introduce an extra visual transformer as the
alignment-ware image encoder and an extra text transformer as the
alignment-ware text encoder before multimodal fusion. We consider alignment in
the following three aspects: 1) document-level alignment by leveraging the
cross-modal and intra-modal contrastive loss; 2) global-local alignment for
modeling localized and structural information in document images; and 3)
local-level alignment for more accurate patch-level information. Experiments on
various downstream tasks show that AETNet can achieve state-of-the-art
performance on various downstream tasks. Notably, AETNet consistently
outperforms state-of-the-art pre-trained models, such as LayoutLMv3 with
fine-tuning techniques, on three different downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5. (arXiv:2211.14875v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14875">
<div class="article-summary-box-inner">
<span><p>Automated software debugging is a crucial task for improving the productivity
of software developers. Many neural-based techniques have been proven effective
for debugging-related tasks such as bug localization and program repair (or bug
fixing). However, these techniques often focus only on either one of them or
approach them in a stage-wise manner, ignoring the mutual benefits between
them. In this work, we propose a novel unified \emph{Detect-Localize-Repair}
framework based on a pretrained programming language model CodeT5 to seamlessly
address these tasks, named CodeT5-DLR. Specifically, we propose three
objectives to adapt the generic CodeT5 for debugging: a bug detection objective
to determine whether a given code snippet is buggy or not, a bug localization
objective to identify the buggy lines, and a program repair objective to
translate the buggy code to its fixed version. We evaluate it on each of these
tasks and their combined setting on two newly collected line-level debugging
datasets in Java and Python. Extensive results show that our model
significantly outperforms existing baselines from both NLP and software
engineering domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EURO: ESPnet Unsupervised ASR Open-source Toolkit. (arXiv:2211.17196v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.17196">
<div class="article-summary-box-inner">
<span><p>This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),
an end-to-end open-source toolkit for unsupervised automatic speech recognition
(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the
Wav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised
speech representations and adversarial training. In addition to wav2vec2, EURO
extends the functionality and promotes reproducibility for UASR tasks by
integrating S3PRL and k2, resulting in flexible frontends from 27
self-supervised models and various graph-based decoding strategies. EURO is
implemented in ESPnet and follows its unified pipeline to provide UASR recipes
with a complete setup. This improves the pipeline's efficiency and allows EURO
to be easily applied to existing datasets in ESPnet. Extensive experiments on
three mainstream self-supervised models demonstrate the toolkit's effectiveness
and achieve state-of-the-art UASR performance on TIMIT and LibriSpeech
datasets. EURO will be publicly available at https://github.com/espnet/espnet,
aiming to promote this exciting and emerging research area based on UASR
through open-source activity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models. (arXiv:2211.15029v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15029">
<div class="article-summary-box-inner">
<span><p>We present DiffusionBERT, a new generative masked language model based on
discrete diffusion models. Diffusion models and many pre-trained language
models have a shared training objective, i.e., denoising, making it possible to
combine the two powerful models and enjoy the best of both worlds. On the one
hand, diffusion models offer a promising training strategy that helps improve
the generation quality. On the other hand, pre-trained denoising language
models (e.g., BERT) can be used as a good initialization that accelerates
convergence. We explore training BERT to learn the reverse process of a
discrete diffusion process with an absorbing state and elucidate several
designs to improve it. First, we propose a new noise schedule for the forward
diffusion process that controls the degree of noise added at each step based on
the information of each token. Second, we investigate several designs of
incorporating the time step into BERT. Experiments on unconditional text
generation demonstrate that DiffusionBERT achieves significant improvement over
existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous
generative masked language models in terms of perplexity and BLEU score.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-02 23:12:47.917512446 UTC">2022-12-02 23:12:47 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>