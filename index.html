<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-29T01:30:00Z">06-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementing contextual biasing in GPU decoder for online ASR. (arXiv:2306.15685v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15685">
<div class="article-summary-box-inner">
<span><p>GPU decoding significantly accelerates the output of ASR predictions. While
GPUs are already being used for online ASR decoding, post-processing and
rescoring on GPUs have not been properly investigated yet. Rescoring with
available contextual information can considerably improve ASR predictions.
Previous studies have proven the viability of lattice rescoring in decoding and
biasing language model (LM) weights in offline and online CPU scenarios. In
real-time GPU decoding, partial recognition hypotheses are produced without
lattice generation, which makes the implementation of biasing more complex. The
paper proposes and describes an approach to integrate contextual biasing in
real-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides
the biasing of partial ASR predictions, our approach also permits dynamic
context switching allowing a flexible rescoring per each speech segment
directly on GPU. The code is publicly released and tested with open-sourced
test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning. (arXiv:2306.15686v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15686">
<div class="article-summary-box-inner">
<span><p>Despite the impressive performance recently achieved by automatic speech
recognition (ASR), we observe two primary challenges that hinder its broader
applications: (1) The difficulty of introducing scalability into the model to
support more languages with limited training, inference, and storage overhead;
(2) The low-resource adaptation ability that enables effective low-resource
adaptation while avoiding over-fitting and catastrophic forgetting issues.
Inspired by recent findings, we hypothesize that we can address the above
challenges with modules widely shared across languages. To this end, we propose
an ASR framework, dubbed \METHODNS, that, \textit{for the first time},
simultaneously achieves strong multilingual scalability and low-resource
adaptation ability thanks to its modularize-then-assemble strategy.
Specifically, \METHOD learns a small set of generalizable sub-modules and
adaptively assembles them for different languages to reduce the multilingual
overhead and enable effective knowledge transfer for low-resource adaptation.
Extensive experiments and visualizations demonstrate that \METHOD can
effectively discover language similarity and improve multilingual and
low-resource ASR performance over state-of-the-art (SOTA) methods, e.g., under
multilingual-ASR, our framework achieves a 0.13$\sim$2.41 lower character error
rate (CER) with 30\% smaller inference overhead over SOTA solutions on
multilingual ASR and a comparable CER, with nearly 50 times fewer trainable
parameters over SOTA solutions on low-resource tuning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15687">
<div class="article-summary-box-inner">
<span><p>Large-scale generative models such as GPT and DALL-E have revolutionized
natural language processing and computer vision research. These models not only
generate high fidelity text or image outputs, but are also generalists which
can solve tasks not explicitly taught. In contrast, speech generative models
are still primitive in terms of scale and task generalization. In this paper,
we present Voicebox, the most versatile text-guided generative model for speech
at scale. Voicebox is a non-autoregressive flow-matching model trained to
infill speech, given audio context and text, trained on over 50K hours of
speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can
perform many different tasks through in-context learning, but is more flexible
as it can also condition on future context. Voicebox can be used for mono or
cross-lingual zero-shot text-to-speech synthesis, noise removal, content
editing, style conversion, and diverse sample generation. In particular,
Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both
intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs
0.681) while being up to 20 times faster. See voicebox.metademolab.com for a
demo of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15705">
<div class="article-summary-box-inner">
<span><p>Detecting adversarial samples that are carefully crafted to fool the model is
a critical step to socially-secure applications. However, existing adversarial
detection methods require access to sufficient training data, which brings
noteworthy concerns regarding privacy leakage and generalizability. In this
work, we validate that the adversarial sample generated by attack algorithms is
strongly related to a specific vector in the high-dimensional inputs. Such
vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated
without original training data. Based on this discovery, we propose a
data-agnostic adversarial detection framework, which induces different
responses between normal and adversarial samples to UAPs. Experimental results
show that our method achieves competitive detection performance on various text
classification tasks, and maintains an equivalent time consumption to normal
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly Supervised Classifier and Dataset of White Supremacist Language. (arXiv:2306.15732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15732">
<div class="article-summary-box-inner">
<span><p>We present a dataset and classifier for detecting the language of white
supremacist extremism, a growing issue in online hate speech. Our weakly
supervised classifier is trained on large datasets of text from explicitly
white supremacist domains paired with neutral and anti-racist data from similar
domains. We demonstrate that this approach improves generalization performance
to new domains. Incorporating anti-racist texts as counterexamples to white
supremacist language mitigates bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15736">
<div class="article-summary-box-inner">
<span><p>Biomedical named entity recognition (BNER) serves as the foundation for
numerous biomedical text mining tasks. Unlike general NER, BNER require a
comprehensive grasp of the domain, and incorporating external knowledge beyond
training data poses a significant challenge. In this study, we propose a novel
BNER framework called DMNER. By leveraging existing entity representation
models SAPBERT, we tackle BNER as a two-step process: entity boundary detection
and biomedical entity matching. DMNER exhibits applicability across multiple
NER scenarios: 1) In supervised NER, we observe that DMNER effectively
rectifies the output of baseline NER models, thereby further enhancing
performance. 2) In distantly supervised NER, combining MRC and AutoNER as span
boundary detectors enables DMNER to achieve satisfactory results. 3) For
training NER by merging multiple datasets, we adopt a framework similar to
DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the
training. Through extensive experiments conducted on 10 benchmark datasets, we
demonstrate the versatility and effectiveness of DMNER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15745">
<div class="article-summary-box-inner">
<span><p>Online communities of involuntary celibates (incels) are a prominent source
of misogynist hate speech. In this paper, we use quantitative text and network
analysis approaches to examine how identity groups are discussed on incels.is,
the largest black-pilled incels forum. We find that this community produces a
wide range of novel identity terms and, while terms for women are most common,
mentions of other minoritized identities are increasing. An analysis of the
associations made with identity groups suggests an essentialist ideology where
physical appearance, as well as gender and racial hierarchies, determine human
value. We discuss implications for research into automated misogynist hate
speech detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15766">
<div class="article-summary-box-inner">
<span><p>State-of-the-art supervised NLP models achieve high accuracy but are also
susceptible to failures on inputs from low-data regimes, such as domains that
are not represented in training data. As an approximation to collecting
ground-truth labels for the specific domain, we study the use of large language
models (LLMs) for annotating inputs and improving the generalization of NLP
models. Specifically, given a budget for LLM annotations, we present an
algorithm for sampling the most informative inputs to annotate and retrain the
NLP model. We find that popular active learning strategies such as
uncertainty-based sampling do not work well. Instead, we propose a sampling
strategy based on the difference in prediction scores between the base model
and the finetuned NLP model, utilizing the fact that most NLP models are
finetuned from a base model. Experiments with classification (semantic
similarity) and ranking (semantic search) tasks show that our sampling strategy
leads to significant gains in accuracy for both the training and target
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15774">
<div class="article-summary-box-inner">
<span><p>Through iterative, cross-disciplinary discussions, we define and propose
next-steps for Human-centered Generative AI (HGAI) from a technical
perspective. We contribute a roadmap that lays out future directions of
Generative AI spanning three levels: Aligning with human values; Accommodating
humans' expression of intents; and Augmenting humans' abilities in a
collaborative workflow. This roadmap intends to draw interdisciplinary research
teams to a comprehensive list of emergent ideas in HGAI, identifying their
interested topics while maintaining a coherent big picture of the future work
landscape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15788">
<div class="article-summary-box-inner">
<span><p>We investigate the effectiveness of GPT-3.5 and GPT-4, two large language
models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese
and compare their performance against Microsoft Word and Google Docs. We
introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,
Spelling, Internet, and Fast typing. Our results show that while GPT-4 has
higher recall than other methods, LLMs tend to have lower precision, leading to
overcorrection. This study demonstrates the potential of LLMs as practical GEC
tools for Brazilian Portuguese and encourages further exploration of LLMs for
non-English languages and other educational settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15799">
<div class="article-summary-box-inner">
<span><p>Many efficient approximate self-attention techniques have become prevalent
since the inception of the transformer architecture. Two popular classes of
these techniques are low-rank and kernel methods. Each of these methods has its
own strengths. We observe these strengths synergistically complement each other
and exploit these synergies to fuse low-rank and kernel methods, producing a
new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA
provide sizable performance gains over these approximate techniques and are of
high quality. We theoretically and empirically evaluate both the runtime
performance and quality of FLuRKA. Our runtime analysis posits a variety of
parameter configurations where FLuRKA exhibit speedups and our accuracy
analysis bounds the error of FLuRKA with respect to full-attention. We
instantiate three FLuRKA variants which experience empirical speedups of up to
3.3x and 1.7x over low-rank and kernel methods respectively. This translates to
speedups of up to 30x over models with full-attention. With respect to model
quality, FLuRKA can match the accuracy of low-rank and kernel methods on GLUE
after pre-training on wiki-text 103. When pre-training on a fixed time budget,
FLuRKA yield better perplexity scores than models with full-attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15824">
<div class="article-summary-box-inner">
<span><p>The number of end-to-end speech recognition models grows every year. These
models are often adapted to new domains or languages resulting in a
proliferation of expert systems that achieve great results on target data,
while generally showing inferior performance outside of their domain of
expertise. We explore combination of such experts via confidence-based
ensembles: ensembles of models where only the output of the most-confident
model is used. We assume that models' target data is not available except for a
small validation set. We demonstrate effectiveness of our approach with two
applications. First, we show that a confidence-based ensemble of 5 monolingual
models outperforms a system where model selection is performed via a dedicated
language identification block. Second, we demonstrate that it is possible to
combine base and adapted models to achieve strong results on both original and
target data. We validate all our results on multiple datasets and model
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15826">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large-scale pre-trained language models has been demonstrated
effective for various natural language processing (NLP) tasks. Previous studies
have established that incorporating adversarial training during the fine-tuning
stage can significantly enhance model generalization and robustness. However,
from the perspective of game theory, such utilizations of adversarial training
correspond to pure-strategy games, which are inherently limited in terms of the
scope of their strategies, thereby still having room for improvement. In order
to push the performance boundaries, we propose a novel Mixed-strategy
Adversarial Training algorithm (MAT). Methodologically, we derive the Nash
equilibrium of a mixed-strategy game for adversarial training using Entropy
Mirror Descent to establish MAT by sampling method. To verify the effectiveness
of MAT, we conducted extensive benchmark experiments on large-scale pre-trained
models, such as BERT and RoBERTa. MAT significantly outperforms the
state-of-the-art methods on both the GLUE and ANLI benchmarks in terms of
generalization and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality. (arXiv:2306.15837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15837">
<div class="article-summary-box-inner">
<span><p>We present a computational model for a symbol emergence system that enables
the emergence of lexical knowledge with combinatoriality among agents through a
Metropolis-Hastings naming game and cross-situational learning. Many
computational models have been proposed to investigate combinatoriality in
emergent communication and symbol emergence in cognitive and developmental
robotics. However, existing models do not sufficiently address category
formation based on sensory-motor information and semiotic communication through
the exchange of word sequences within a single integrated model. Our proposed
model facilitates the emergence of lexical knowledge with combinatoriality by
performing category formation using multimodal sensory-motor information and
enabling semiotic communication through the exchange of word sequences among
agents in a unified model. Furthermore, the model enables an agent to predict
sensory-motor information for unobserved situations by combining words
associated with categories in each modality. We conducted two experiments with
two humanoid robots in a simulated environment to evaluate our proposed model.
The results demonstrated that the agents can acquire lexical knowledge with
combinatoriality through interpersonal cross-situational learning based on the
Metropolis-Hastings naming game and cross-situational learning. Furthermore,
our results indicate that the lexical knowledge developed using our proposed
model exhibits generalization performance for novel situations through
interpersonal cross-modal inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15895">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been recently leveraged as training data
generators for various natural language processing (NLP) tasks. While previous
research has explored different approaches to training models using generated
data, they generally rely on simple class-conditional prompts, which may limit
the diversity of the generated data and inherit systematic biases of LLM. Thus,
we investigate training data generation with diversely attributed prompts
(e.g., specifying attributes like length and style), which have the potential
to yield diverse and attributed generated data. Our investigation focuses on
datasets with high cardinality and diverse domains, wherein we demonstrate that
attributed prompts outperform simple class-conditional prompts in terms of the
resulting model's performance. Additionally, we present a comprehensive
empirical study on data generation encompassing vital aspects like bias,
diversity, and efficiency, and highlight three key observations: firstly,
synthetic datasets generated by simple prompts exhibit significant biases, such
as regional bias; secondly, attribute diversity plays a pivotal role in
enhancing model performance; lastly, attributed prompts achieve the performance
of simple class-conditional prompts while utilizing only 5\% of the querying
cost of ChatGPT associated with the latter. We release the generated dataset
and used prompts to facilitate future research. The data and code will be
available on \url{https://github.com/yueyu1030/AttrPrompt}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence-Calibrated Ensemble Dense Phrase Retrieval. (arXiv:2306.15917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15917">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the extent to which the transformer-based Dense
Passage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can
be optimized without further pre-training. Our method involves two particular
insights: we apply the DPR context encoder at various phrase lengths (e.g.
one-sentence versus five-sentence segments), and we take a
confidence-calibrated ensemble prediction over all of these different
segmentations. This somewhat exhaustive approach achieves start-of-the-art
results on benchmark datasets such as Google NQ and SQuAD. We also apply our
method to domain-specific datasets, and the results suggest how different
granularities are optimal for different domains
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15926">
<div class="article-summary-box-inner">
<span><p>Despite rapid advancement in the field of Constrained Natural Language
Generation, little time has been spent on exploring the potential of language
models which have had their vocabularies lexically, semantically, and/or
phonetically constrained. We find that most language models generate compelling
text even under significant constraints. We present a simple and universally
applicable technique for modifying the output of a language model by
compositionally applying filter functions to the language models vocabulary
before a unit of text is generated. This approach is plug-and-play and requires
no modification to the model. To showcase the value of this technique, we
present an easy to use AI writing assistant called Constrained Text Generation
Studio (CTGS). CTGS allows users to generate or choose from text with any
combination of a wide variety of constraints, such as banning a particular
letter, forcing the generated words to have a certain number of syllables,
and/or forcing the words to be partial anagrams of another word. We introduce a
novel dataset of prose that omits the letter e. We show that our method results
in strictly superior performance compared to fine-tuning alone on this dataset.
We also present a Huggingface space web-app presenting this technique called
Gadsby. The code is available to the public here:
https://github.com/Hellisotherpeople/Constrained-Text-Generation-Studio
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15933">
<div class="article-summary-box-inner">
<span><p>Despite significant advancements in existing models, generating text
descriptions from structured data input, known as data-to-text generation,
remains a challenging task. In this paper, we propose a novel approach that
goes beyond traditional one-shot generation methods by introducing a multi-step
process consisting of generation, verification, and correction stages. Our
approach, VCP(Verification and Correction Prompting), begins with the model
generating an initial output. We then proceed to verify the correctness of
different aspects of the generated text. The observations from the verification
step are converted into a specialized error-indication prompt, which instructs
the model to regenerate the output while considering the identified errors. To
enhance the model's correction ability, we have developed a carefully designed
training procedure. This procedure enables the model to incorporate feedback
from the error-indication prompt, resulting in improved output generation.
Through experimental results, we demonstrate that our approach effectively
reduces slot error rates while maintaining the overall quality of the generated
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-to-Label Generation Framework for Multi-task Learning of Japanese Sentence Classification and Named Entity Recognition. (arXiv:2306.15978v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15978">
<div class="article-summary-box-inner">
<span><p>Information extraction(IE) is a crucial subfield within natural language
processing. In this study, we introduce a Sentence Classification and Named
Entity Recognition Multi-task (SCNM) approach that combines Sentence
Classification (SC) and Named Entity Recognition (NER). We develop a
Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia
dataset containing both SC and NER. Using a format converter, we unify input
formats and employ a generative model to generate SC-labels, NER-labels, and
associated text segments. We propose a Constraint Mechanism (CM) to improve
generated format accuracy. Our results show SC accuracy increased by 1.13
points and NER by 1.06 points in SCNM compared to standalone tasks, with CM
raising format accuracy from 63.61 to 100. The findings indicate mutual
reinforcement effects between SC and NER, and integration enhances both tasks'
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16001">
<div class="article-summary-box-inner">
<span><p>The utilization of social media in epidemic surveillance has been well
established. Nonetheless, bias is often introduced when pre-defined lexicons
are used to retrieve relevant corpus. This study introduces a framework aimed
at curating extensive dictionaries of medical colloquialisms and Unified
Medical Language System (UMLS) concepts. The framework comprises three modules:
a BERT-based Named Entity Recognition (NER) model that identifies medical
entities from social media content, a deep-learning powered normalization
module that standardizes the extracted entities, and a semi-supervised
clustering module that assigns the most probable UMLS concept to each
standardized entity. We applied this framework to COVID-19-related tweets from
February 1, 2020, to April 30, 2022, generating a symptom dictionary (available
at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249
standardized entities mapped to 876 UMLS concepts and 38,175 colloquial
expressions. This framework demonstrates encouraging potential in addressing
the constraints of keyword matching information retrieval in social media-based
public health research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16007">
<div class="article-summary-box-inner">
<span><p>The integration of Language Models (LMs) has proven to be an effective way to
address domain shifts in speech recognition. However, these approaches usually
require a significant amount of target domain text data for the training of
LMs. Different from these methods, in this work, with only a domain-specific
text prompt, we propose two zero-shot ASR domain adaptation methods using
LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two
ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR
system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an
encoder-decoder based ASR system. Experiments show that, with only one domain
prompt, both methods can effectively reduce word error rates (WER) on
out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep
LLM-fusion has the advantage of better recall of entity and out-of-vocabulary
words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Transducers through Adjacent Token Merging. (arXiv:2306.16009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16009">
<div class="article-summary-box-inner">
<span><p>Recent end-to-end automatic speech recognition (ASR) systems often utilize a
Transformer-based acoustic encoder that generates embedding at a high frame
rate. However, this design is inefficient, particularly for long speech signals
due to the quadratic computation of self-attention. To address this, we propose
a new method, Adjacent Token Merging (A-ToMe), which gradually combines
adjacent tokens with high similarity scores between their key values. In this
way, the total time step could be reduced, and the inference of both the
encoder and joint network is accelerated. Experiments on LibriSpeech show that
our method can reduce 57% of tokens and improve the inference speed on GPU by
70% without any notable loss of accuracy. Additionally, we demonstrate that
A-ToMe is also an effective solution to reduce tokens in long-form ASR, where
the input speech consists of multiple utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Spatial-Temporal Variations of Public Discourse on Social Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy. (arXiv:2306.16031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16031">
<div class="article-summary-box-inner">
<span><p>This paper proposes a methodology for exploring how linguistic behaviour on
social media can be used to explore societal reactions to important events such
as those that transpired during the SARS CoV2 pandemic. In particular, where
spatial and temporal aspects of events are important features. Our methodology
consists of grounding spatial-temporal categories in tweet usage trends using
time-series analysis and clustering. Salient terms in each category were then
identified through qualitative comparative analysis based on scaled f-scores
aggregated into hand-coded categories. To exemplify this approach, we conducted
a case study on the first wave of the coronavirus in Italy. We used our
proposed methodology to explore existing psychological observations which
claimed that physical distance from events affects what is communicated about
them. We confirmed these findings by showing that the epicentre of the disease
and peripheral regions correspond to clear time-series clusters and that those
living in the epicentre of the SARS CoV2 outbreak were more focused on
solidarity and policy than those from more peripheral regions. Furthermore, we
also found that temporal categories corresponded closely to policy changes
during the handling of the pandemic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022 Using Twitter and AI. (arXiv:2306.16049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16049">
<div class="article-summary-box-inner">
<span><p>Twitter is a social media platform bridging most countries and allows
real-time news discovery. Since the tweets on Twitter are usually short and
express public feelings, thus provide a source for opinion mining and sentiment
analysis for global events. This paper proposed an effective solution, in
providing a sentiment on tweets related to the FIFA World Cup. At least 130k
tweets, as the first in the community, are collected and implemented as a
dataset to evaluate the performance of the proposed machine learning solution.
These tweets are collected with the related hashtags and keywords of the Qatar
World Cup 2022. The Vader algorithm is used in this paper for sentiment
analysis. Through the machine learning method and collected Twitter tweets, we
discovered the sentiments and fun facts of several aspects important to the
period before the World Cup. The result shows people are positive to the
opening of the World Cup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Conversation Analysis: Exploring Utility and Privacy. (arXiv:2306.16071v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16071">
<div class="article-summary-box-inner">
<span><p>The analysis of conversations recorded in everyday life requires privacy
protection. In this contribution, we explore a privacy-preserving feature
extraction method based on input feature dimension reduction, spectral
smoothing and the low-cost speaker anonymization technique based on McAdams
coefficient. We assess the utility of the feature extraction methods with a
voice activity detection and a speaker diarization system, while privacy
protection is determined with a speech recognition and a speaker verification
model. We show that the combination of McAdams coefficient and spectral
smoothing maintains the utility while improving privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16092">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown the potential to revolutionize
natural language processing tasks in various domains, sparking great interest
in vertical-specific large models. However, unlike proprietary models such as
BloombergGPT and FinGPT, which have leveraged their unique data accumulations
to make strides in the finance domain, there hasn't not many similar large
language models in the Chinese legal domain to facilitate its digital
transformation.
</p>
<p>In this paper, we propose an open-source legal large language model named
ChatLaw. Due to the importance of data quality, we carefully designed a legal
domain fine-tuning dataset. Additionally, to overcome the problem of model
hallucinations in legal data screening during reference data retrieval, we
introduce a method that combines vector database retrieval with keyword
retrieval to effectively reduce the inaccuracy of relying solely on vector
database retrieval. Furthermore, we propose a self-attention method to enhance
the ability of large models to overcome errors present in reference data,
further optimizing the issue of model hallucinations at the model level and
improving the problem-solving capabilities of large models. We also
open-sourced our model and part of the data at
https://github.com/PKU-YuanGroup/ChatLaw.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16108">
<div class="article-summary-box-inner">
<span><p>We assessed the performance of commercial Large Language Models (LLMs)
GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b
Phase B, which is focused on answer generation, both models demonstrated
competitive abilities with leading systems. Remarkably, they achieved this with
simple zero-shot learning, grounded with relevant snippets. Even without
relevant snippets, their performance was decent, though not on par with the
best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was
able to compete with GPT-4 in the grounded Q&amp;A setting on factoid and list
answers. In Task 11b Phase A, focusing on retrieval, query expansion through
zero-shot learning improved performance, but the models fell short compared to
other systems. The code needed to rerun these experiments is available through
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16125">
<div class="article-summary-box-inner">
<span><p>This paper describes our participation in the MentalRiskES task at IberLEF
2023. The task involved predicting the likelihood of an individual experiencing
depression based on their social media activity. The dataset consisted of
conversations from 175 Telegram users, each labeled according to their evidence
of suffering from the disorder. We used a combination of traditional machine
learning and deep learning techniques to solve four predictive subtasks: binary
classification, simple regression, multiclass classification, and multiclass
regression. We approached this by training a model to solve the multiclass
regression case and then transforming the predictions to work for the other
three subtasks. We compare the performance of two different modeling
approaches: fine-tuning a BERT-based model and using sentence embeddings as
inputs to a linear regressor, with the latter yielding better results. The code
to reproduce our results can be found at:
https://github.com/simonsanvil/EarlyDepression-MentalRiskES.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16143">
<div class="article-summary-box-inner">
<span><p>User experience (UX) is a part of human-computer interaction (HCI) research
and focuses on increasing intuitiveness, transparency, simplicity, and trust
for system users. Most of the UX research for machine learning (ML) or natural
language processing (NLP) focuses on a data-driven methodology, i.e., it fails
to focus on users' requirements, and engages domain users mainly for usability
evaluation. Moreover, more typical UX methods tailor the systems towards user
usability, unlike learning about the user needs first. The paper proposes a
methodology for integrating generative UX research into developing domain NLP
applications. Generative UX research employs domain users at the initial stages
of prototype development, i.e., ideation and concept evaluation, and the last
stage for evaluating the change in user value. In the case study, we report the
full-cycle prototype development of a domain-specific semantic search for daily
operations in the process industry. Our case study shows that involving domain
experts increases their interest and trust in the final NLP application.
Moreover, we show that synergetic UX+NLP research efficiently considers data-
and user-driven opportunities and constraints, which can be crucial for NLP
applications in narrow domains
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillNet-X: A Multilingual Multitask Model with Sparsely Activated Skills. (arXiv:2306.16176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16176">
<div class="article-summary-box-inner">
<span><p>Traditional multitask learning methods basically can only exploit common
knowledge in task- or language-wise, which lose either cross-language or
cross-task knowledge. This paper proposes a general multilingual multitask
model, named SkillNet-X, which enables a single model to tackle many different
tasks from different languages. To this end, we define several
language-specific skills and task-specific skills, each of which corresponds to
a skill module. SkillNet-X sparsely activates parts of the skill modules which
are relevant either to the target task or the target language. Acting as
knowledge transit hubs, skill modules are capable of absorbing task-related
knowledge and language-related knowledge consecutively. Based on Transformer,
we modify the multi-head attention layer and the feed forward network layer to
accommodate skill modules. We evaluate SkillNet-X on eleven natural language
understanding datasets in four languages. Results show that SkillNet-X performs
better than task-specific baselines and two multitask learning baselines (i.e.,
dense joint model and Mixture-of-Experts model). Furthermore, skill
pre-training further improves the performance of SkillNet-X on almost all
datasets. To investigate the generalization of our model, we conduct
experiments on two new tasks and find that SkillNet-X significantly outperforms
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. (arXiv:2306.16195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16195">
<div class="article-summary-box-inner">
<span><p>Incorporating external graph knowledge into neural chatbot models has been
proven effective for enhancing dialogue generation. However, in conventional
graph neural networks (GNNs), message passing on a graph is independent from
text, resulting in the graph representation hidden space differing from that of
the text. This training regime of existing models therefore leads to a semantic
gap between graph knowledge and text. In this study, we propose a novel
framework for knowledge graph enhanced dialogue generation. We dynamically
construct a multi-hop knowledge graph with pseudo nodes to involve the language
model in feature aggregation within the graph at all steps. To avoid the
semantic biases caused by learning on vanilla subgraphs, the proposed framework
applies hierarchical graph attention to aggregate graph features on pseudo
nodes and then attains a global feature. Therefore, the framework can better
utilise the heterogeneous features from both the post and external graph
knowledge. Extensive experiments demonstrate that our framework outperforms
state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also
shows that our representation learning framework can fill the semantic gap by
coagulating representations of both text and graph knowledge. Moreover, the
language model also learns how to better select knowledge triples for a more
informative response via exploiting subgraph patterns within our feature
aggregation process. Our code and resources are available at
https://github.com/tangg555/SaBART.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16207">
<div class="article-summary-box-inner">
<span><p>When humans cooperate, they frequently coordinate their activity through both
verbal communication and non-verbal actions, using this information to infer a
shared goal and plan. How can we model this inferential ability? In this paper,
we introduce a model of a cooperative team where one agent, the principal, may
communicate natural language instructions about their shared plan to another
agent, the assistant, using GPT-3 as a likelihood function for instruction
utterances. We then show how a third person observer can infer the team's goal
via multi-modal Bayesian inverse planning from actions and instructions,
computing the posterior distribution over goals under the assumption that
agents will act and communicate rationally to achieve them. We evaluate this
approach by comparing it with human goal inferences in a multi-agent gridworld,
finding that our model's inferences closely correlate with human judgments (R =
0.96). When compared to inference from actions alone, we also find that
instructions lead to more rapid and less uncertain goal inference, highlighting
the importance of verbal communication for cooperative agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16244">
<div class="article-summary-box-inner">
<span><p>Holistically measuring societal biases of large language models is crucial
for detecting and reducing ethical risks in highly capable AI models. In this
work, we present a Chinese Bias Benchmark dataset that consists of over 100K
questions jointly constructed by human experts and generative language models,
covering stereotypes and societal biases in 14 social dimensions related to
Chinese culture and values. The curation process contains 4 essential steps:
bias identification via extensive literature review, ambiguous context
generation, AI-assisted disambiguous context generation, snd manual review \&amp;
recomposition. The testing instances in the dataset are automatically derived
from 3K+ high-quality templates manually authored with stringent quality
control. The dataset exhibits wide coverage and high diversity. Extensive
experiments demonstrate the effectiveness of the dataset in detecting model
bias, with all 10 publicly available Chinese large language models exhibiting
strong bias in certain categories. Additionally, we observe from our
experiments that fine-tuned models could, to a certain extent, heed
instructions and avoid generating outputs that are morally harmful in some
types, in the way of "moral self-correction". Our dataset and results are
publicly available at
\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},
offering debiasing research opportunities to a widened community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Analysis of Tweets Banning Education in Afghanistan. (arXiv:2306.16268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16268">
<div class="article-summary-box-inner">
<span><p>This paper introduces the first emotion annotated dataset for the Dari
variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains
7,600 tweets posted in reaction to the Taliban ban of women rights to education
in 2022 and has been manually annotated according to Ekman emotion categories.
We here detail the data collection and annotation process, present relevant
dataset statistics as well as initial experiments on the resulting dataset,
benchmarking a number of different neural architectures for the task of Dari
emotion classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16275">
<div class="article-summary-box-inner">
<span><p>Food effect summarization from New Drug Application (NDA) is an essential
component of product-specific guidance (PSG) development and assessment.
However, manual summarization of food effect from extensive drug application
review documents is time-consuming, which arouses a need to develop automated
methods. Recent advances in large language models (LLMs) such as ChatGPT and
GPT-4, have demonstrated great potential in improving the effectiveness of
automated text summarization, but its ability regarding the accuracy in
summarizing food effect for PSG assessment remains unclear. In this study, we
introduce a simple yet effective approach, iterative prompting, which allows
one to interact with ChatGPT or GPT-4 more effectively and efficiently through
multi-turn interaction. Specifically, we propose a three-turn iterative
prompting approach to food effect summarization in which the keyword-focused
and length-controlled prompts are respectively provided in consecutive turns to
refine the quality of the generated summary. We conduct a series of extensive
evaluations, ranging from automated metrics to FDA professionals and even
evaluation by GPT-4, on 100 NDA review documents selected over the past five
years. We observe that the summary quality is progressively improved throughout
the process. Moreover, we find that GPT-4 performs better than ChatGPT, as
evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).
Importantly, all the FDA professionals unanimously rated that 85% of the
summaries generated by GPT-4 are factually consistent with the golden reference
summary, a finding further supported by GPT-4 rating of 72% consistency. These
results strongly suggest a great potential for GPT-4 to draft food effect
summaries that could be reviewed by FDA professionals, thereby improving the
efficiency of PSG assessment cycle and promoting the generic drug product
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra. (arXiv:2306.16282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16282">
<div class="article-summary-box-inner">
<span><p>The emergence of ChatGPT has been rapid, and although it has demonstrated
positive impacts in certain domains, its influence is not universally
advantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics
Education, particularly in teaching basic Linear Algebra. While there are
instances where ChatGPT delivers accurate and well-motivated answers, it is
crucial to recognize numerous cases where it makes significant mathematical
errors and fails in logical inference. These occurrences raise concerns
regarding the system's genuine understanding of mathematics, as it appears to
rely more on visual patterns rather than true comprehension. Additionally, the
suitability of ChatGPT as a teacher for students also warrants consideration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection. (arXiv:2306.16313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16313">
<div class="article-summary-box-inner">
<span><p>Text correction, especially the semantic correction of more widely used
scenes, is strongly required to improve, for the fluency and writing efficiency
of the text. An adversarial multi-task learning method is proposed to enhance
the modeling and detection ability of character polysemy in Chinese sentence
context. Wherein, two models, the masked language model and scoring language
model, are introduced as a pair of not only coupled but also adversarial
learning tasks. Moreover, the Monte Carlo tree search strategy and a policy
network are introduced to accomplish the efficient Chinese text correction task
with semantic detection. The experiments are executed on three datasets and
five comparable methods, and the experimental results show that our method can
obtain good performance in Chinese text correction task for better semantic
rationality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models. (arXiv:2306.16322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16322">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive performance on
various downstream tasks without requiring fine-tuning, including ChatGPT, a
chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having
a lower training proportion compared to English, these models also exhibit
remarkable capabilities in other languages. In this study, we assess the
performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:
sentiment analysis, translation, transliteration, paraphrasing, part of speech
tagging, summarization, and diacritization. Our findings reveal that GPT-4
outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an
extensive analysis of the sentiment analysis task, providing insights into how
LLMs achieve exceptional results on a challenging dialectal dataset.
Additionally, we introduce a new Python interface
https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks
effortlessly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16326">
<div class="article-summary-box-inner">
<span><p>We present Variational Bayesian Network (VBN) - a novel Bayesian entity
representation learning model that utilizes hierarchical and relational side
information and is particularly useful for modeling entities in the
``long-tail'', where the data is scarce. VBN provides better modeling for
long-tail entities via two complementary mechanisms: First, VBN employs
informative hierarchical priors that enable information propagation between
entities sharing common ancestors. Additionally, VBN models explicit relations
between entities that enforce complementary structure and consistency, guiding
the learned representations towards a more meaningful arrangement in space.
Second, VBN represents entities by densities (rather than vectors), hence
modeling uncertainty that plays a complementary role in coping with data
scarcity. Finally, we propose a scalable Variational Bayes optimization
algorithm that enables fast approximate Bayesian inference. We evaluate the
effectiveness of VBN on linguistic, recommendations, and medical inference
tasks. Our findings show that VBN outperforms other existing methods across
multiple datasets, and especially in the long-tail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16367">
<div class="article-summary-box-inner">
<span><p>The prodigious growth of digital health data has precipitated a mounting
interest in harnessing machine learning methodologies, such as natural language
processing (NLP), to scrutinize medical records, clinical notes, and other
text-based health information. Although NLP techniques have exhibited
substantial potential in augmenting patient care and informing clinical
decision-making, data privacy and adherence to regulations persist as critical
concerns. Federated learning (FL) emerges as a viable solution, empowering
multiple organizations to train machine learning models collaboratively without
disseminating raw data. This paper proffers a pragmatic approach to medical NLP
by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA.
We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based
model and Bidirectional Encoder Representations from Transformers (BERT), which
have demonstrated exceptional performance in comprehending context and
semantics within medical data. This paper encompasses the development of an
integrated framework that addresses data privacy and regulatory compliance
challenges while maintaining elevated accuracy and performance, incorporating
BERT pretraining, and comprehensively substantiating the efficacy of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16388">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) may not equitably represent diverse global
perspectives on societal issues. In this paper, we develop a quantitative
framework to evaluate whose opinions model-generated responses are more similar
to. We first build a dataset, GlobalOpinionQA, comprised of questions and
answers from cross-national surveys designed to capture diverse opinions on
global issues across different countries. Next, we define a metric that
quantifies the similarity between LLM-generated survey responses and human
responses, conditioned on country. With our framework, we run three experiments
on an LLM trained to be helpful, honest, and harmless with Constitutional AI.
By default, LLM responses tend to be more similar to the opinions of certain
populations, such as those from the USA, and some European and South American
countries, highlighting the potential for biases. When we prompt the model to
consider a particular country's perspective, responses shift to be more similar
to the opinions of the prompted populations, but can reflect harmful cultural
stereotypes. When we translate GlobalOpinionQA questions to a target language,
the model's responses do not necessarily become the most similar to the
opinions of speakers of those languages. We release our dataset for others to
use and build on. Our data is at
https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide
an interactive visualization at https://llmglobalvalues.anthropic.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language. (arXiv:2306.16410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16410">
<div class="article-summary-box-inner">
<span><p>We propose LENS, a modular approach for tackling computer vision problems by
leveraging the power of large language models (LLMs). Our system uses a
language model to reason over outputs from a set of independent and highly
descriptive vision modules that provide exhaustive information about an image.
We evaluate the approach on pure computer vision settings such as zero- and
few-shot object recognition, as well as on vision and language problems. LENS
can be applied to any off-the-shelf LLM and we find that the LLMs with LENS
perform highly competitively with much bigger and much more sophisticated
systems, without any multimodal training whatsoever. We open-source our code at
https://github.com/ContextualAI/lens and provide an interactive demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16413">
<div class="article-summary-box-inner">
<span><p>Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. In order to accelerate progress towards
understudied modalities and tasks while ensuring real-world robustness, we
release MultiZoo, a public toolkit consisting of standardized implementations
of &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark
spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.
Together, these provide an automated end-to-end machine learning pipeline that
simplifies and standardizes data loading, experimental setup, and model
evaluation. To enable holistic evaluation, we offer a comprehensive methodology
to assess (1) generalization, (2) time and space complexity, and (3) modality
robustness. MultiBench paves the way towards a better understanding of the
capabilities and limitations of multimodal models, while ensuring ease of use,
accessibility, and reproducibility. Our toolkits are publicly available, will
be regularly updated, and welcome inputs from the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Many real-world problems are inherently multimodal, from spoken language,
gestures, and paralinguistics humans use to communicate, to force,
proprioception, and visual sensors on robots. While there has been an explosion
of interest in multimodal learning, these methods are focused on a small set of
modalities primarily in language, vision, and audio. In order to accelerate
generalization towards diverse and understudied modalities, this paper studies
efficient representation learning for high-modality scenarios involving a large
set of diverse modalities. Since adding new models for every new modality
becomes prohibitively expensive, a critical technical challenge is
heterogeneity quantification: how can we measure which modalities encode
similar information and interactions in order to permit parameter sharing with
previous modalities? This paper proposes two new information theoretic metrics
for heterogeneity quantification: (1) modality heterogeneity studies how
similar 2 modalities {X1,X2} are by measuring how much information can be
transferred from X1 to X2, while (2) interaction heterogeneity studies how
similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much
information can be transferred from fusing {X1,X2} to {X3,X4}. We show the
importance of these 2 proposed metrics as a way to automatically prioritize the
fusion of modalities that contain unique information or interactions. The
result is a single model, HighMMT, that scales up to 10 modalities (text,
image, audio, video, sensors, proprioception, speech, time-series, sets, and
tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform
prior methods on the tradeoff between performance and efficiency, it also
demonstrates a crucial scaling behavior: performance continues to improve with
each modality added, and it transfers to entirely new modalities and tasks
during fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06604">
<div class="article-summary-box-inner">
<span><p>The Electronic Health Record (EHR) is an essential part of the modern medical
system and impacts healthcare delivery, operations, and research. Unstructured
text is attracting much attention despite structured information in the EHRs
and has become an exciting research field. The success of the recent neural
Natural Language Processing (NLP) method has led to a new direction for
processing unstructured clinical notes. In this work, we create a python
library for clinical texts, EHRKit. This library contains two main parts:
MIMIC-III-specific functions and tasks specific functions. The first part
introduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,
including basic search, information retrieval, and information extraction. The
second part integrates many third-party libraries for up to 12 off-shelf NLP
tasks such as named entity recognition, summarization, machine translation,
etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11490">
<div class="article-summary-box-inner">
<span><p>Subword tokenization schemes are the dominant technique used in current NLP
models. However, such schemes can be rigid and tokenizers built on one corpus
do not adapt well to other parallel corpora. It has also been observed that in
multilingual corpora, subword tokenization schemes over-segment low-resource
languages leading to a drop in translation performance. A simple alternative to
subword tokenizers is byte-based methods i.e. tokenization into byte sequences
using encoding schemes such as UTF-8. Byte tokens often represent inputs at a
sub-character granularity i.e. one character can be represented by a sequence
of multiple byte tokens. This results in byte sequences that are significantly
longer than character sequences. Enforcing aggregation of local information in
the lower layers can guide the model to build higher-level semantic
information. We propose a Local Byte Fusion (LOBEF) method for byte-based
machine translation -- utilizing byte $n$-gram and word boundaries -- to
aggregate local semantic information. Extensive experiments on multilingual
translation, zero-shot cross-lingual transfer, and domain adaptation reveal a
consistent improvement over traditional byte-based models and even over subword
techniques. Further analysis also indicates that our byte-based models are
parameter-efficient and can be trained faster than subword models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13912">
<div class="article-summary-box-inner">
<span><p>Funding agencies are largely relied on a topic matching between domain
experts and research proposals to assign proposal reviewers. As proposals are
increasingly interdisciplinary, it is challenging to profile the
interdisciplinary nature of a proposal, and, thereafter, find expert reviewers
with an appropriate set of expertise. An essential step in solving this
challenge is to accurately model and classify the interdisciplinary labels of a
proposal. Existing methodological and application-related literature, such as
textual classification and proposal classification, are insufficient in jointly
addressing the three key unique issues introduced by interdisciplinary proposal
data: 1) the hierarchical structure of discipline labels of a proposal from
coarse-grain to fine-grain, e.g., from information science to AI to
fundamentals of AI. 2) the heterogeneous semantics of various main textual
parts that play different roles in a proposal; 3) the number of proposals is
imbalanced between non-interdisciplinary and interdisciplinary research. Can we
simultaneously address the three issues in understanding the proposal's
interdisciplinary nature? In response to this question, we propose a
hierarchical mixup multiple-label classification framework, which we called
H-MixUp. H-MixUp leverages a transformer-based semantic information extractor
and a GCN-based interdisciplinary knowledge extractor for the first and second
issues. H-MixUp develops a fused training method of Wold-level MixUp,
Word-level CutMix, Manifold MixUp, and Document-level MixUp to address the
third issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07730">
<div class="article-summary-box-inner">
<span><p>Zero-shot transfer learning for document understanding is a crucial yet
under-investigated scenario to help reduce the high cost involved in annotating
document entities. We present a novel query-based framework, QueryForm, that
extracts entity values from form-like documents in a zero-shot fashion.
QueryForm contains a dual prompting mechanism that composes both the document
schema and a specific entity type into a query, which is used to prompt a
Transformer model to perform a single entity extraction task. Furthermore, we
propose to leverage large-scale query-entity pairs generated from form-like
webpages with weak HTML annotations to pre-train QueryForm. By unifying
pre-training and fine-tuning into the same query-based framework, QueryForm
enables models to learn from structured documents containing various entities
and layouts, leading to better generalization to target document types without
the need for target-specific training data. QueryForm sets new state-of-the-art
average F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%)
zero-shot benchmark, with a smaller model size and no additional image input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization. (arXiv:2212.01476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01476">
<div class="article-summary-box-inner">
<span><p>Narrative summarization aims to produce a distilled version of a narrative to
describe its most salient events and characters. Summarizing a narrative is
challenging as it requires an understanding of event causality and character
behaviors. To encourage research in this direction, we propose NarraSum, a
large-scale narrative summarization dataset. It contains 122K narrative
documents, which are collected from plot descriptions of movies and TV episodes
with diverse genres, and their corresponding abstractive summaries. Experiments
show that there is a large performance gap between humans and the
state-of-the-art summarization models on NarraSum. We hope that this dataset
will promote future research in summarization, as well as broader studies of
natural language understanding and generation. The dataset is available at
https://github.com/zhaochaocs/narrasum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10422">
<div class="article-summary-box-inner">
<span><p>In the era of digital healthcare, the huge volumes of textual information
generated every day in hospitals constitute an essential but underused asset
that could be exploited with task-specific, fine-tuned biomedical language
representation models, improving patient care and management. For such
specialized domains, previous research has shown that fine-tuning models
stemming from broad-coverage checkpoints can largely benefit additional
training rounds over large-scale in-domain resources. However, these resources
are often unreachable for less-resourced languages like Italian, preventing
local medical institutions to employ in-domain adaptation. In order to reduce
this gap, our work investigates two accessible approaches to derive biomedical
language models in languages other than English, taking Italian as a concrete
use-case: one based on neural machine translation of English resources,
favoring quantity over quality; the other based on a high-grade, narrow-scoped
corpus natively written in Italian, thus preferring quality over quantity. Our
study shows that data quantity is a harder constraint than data quality for
biomedical adaptation, but the concatenation of high-quality data can improve
model performance even when dealing with relatively size-limited corpora. The
models published from our investigations have the potential to unlock important
research opportunities for Italian hospitals and academia. Finally, the set of
lessons learned from the study constitutes valuable insights towards a solution
to build biomedical language models that are generalizable to other
less-resourced languages and different domain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.07695">
<div class="article-summary-box-inner">
<span><p>We present a new text-to-SQL dataset for electronic health records (EHRs).
The utterances were collected from 222 hospital staff members, including
physicians, nurses, and insurance review and health records teams. To construct
the QA dataset on structured EHR data, we conducted a poll at a university
hospital and used the responses to create seed questions. We then manually
linked these questions to two open-source EHR databases, MIMIC-III and eICU,
and included various time expressions and held-out unanswerable questions in
the dataset, which were also collected from the poll. Our dataset poses a
unique set of challenges: the model needs to 1) generate SQL queries that
reflect a wide range of needs in the hospital, including simple retrieval and
complex operations such as calculating survival rate, 2) understand various
time expressions to answer time-sensitive questions in healthcare, and 3)
distinguish whether a given question is answerable or unanswerable. We believe
our dataset, EHRSQL, can serve as a practical benchmark for developing and
assessing QA models on structured EHR data and take a step further towards
bridging the gap between text-to-SQL research and its real-life deployment in
healthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Manifold Learning for Reading Comprehension and Logical Reasoning Tasks with Polytuplet Loss. (arXiv:2304.01046v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01046">
<div class="article-summary-box-inner">
<span><p>The current trend in developing machine learning models for reading
comprehension and logical reasoning tasks is focused on improving the models'
abilities to understand and utilize logical rules. This work focuses on
providing a novel loss function and accompanying model architecture that has
more interpretable components than some other models by representing a common
strategy employed by humans when given reading comprehension and logical
reasoning tasks. This strategy involves emphasizing relative accuracy over
absolute accuracy and can theoretically produce the correct answer without full
knowledge of the information required to solve the question. We examine the
effectiveness of applying such a strategy to train transfer learning models to
solve reading comprehension and logical reasoning questions. The models were
evaluated on the ReClor dataset, a challenging reading comprehension and
logical reasoning benchmark. We propose the polytuplet loss function, an
extension of the triplet loss function, to ensure prioritization of learning
the relative correctness of answer choices over learning the true accuracy of
each choice. Our results indicate that models employing polytuplet loss
outperform existing baseline models. Although polytuplet loss is a promising
alternative to other contrastive loss functions, further research is required
to quantify the benefits it may present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective. (arXiv:2305.15408v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15408">
<div class="article-summary-box-inner">
<span><p>Recent studies have discovered that Chain-of-Thought prompting (CoT) can
dramatically improve the performance of Large Language Models (LLMs),
particularly when dealing with complex tasks involving mathematics or
reasoning. Despite the enormous empirical success, the underlying mechanisms
behind CoT and how it unlocks the potential of LLMs remain elusive. In this
paper, we take a first step towards theoretically answering these questions.
Specifically, we examine the expressivity of LLMs with CoT in solving
fundamental mathematical and decision-making problems. We start by giving an
impossibility result showing that bounded-depth Transformers are unable to
directly produce correct answers for basic arithmetic/equation tasks unless the
model size grows super-polynomially with respect to the input length. In
contrast, we then prove by construction that autoregressive Transformers of
constant size suffice to solve both tasks by generating CoT derivations using a
commonly-used math language format. Moreover, we show LLMs with CoT are capable
of solving a general class of decision-making problems known as Dynamic
Programming, thus justifying its power in tackling complex real-world tasks.
Finally, extensive experiments on four tasks show that, while Transformers
always fail to predict the answers directly, they can consistently learn to
generate correct solutions step-by-step given sufficient CoT demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19567">
<div class="article-summary-box-inner">
<span><p>Despite the huge successes made in neutral TTS, content-leakage remains a
challenge. In this paper, we propose a new input representation and simple
architecture to achieve improved prosody modeling. Inspired by the recent
success in the use of discrete code in TTS, we introduce discrete code to the
input of the reference encoder. Specifically, we leverage the vector quantizer
from the audio compression model to exploit the diverse acoustic information it
has already been trained on. In addition, we apply the modified MLP-Mixer to
the reference encoder, making the architecture lighter. As a result, we train
the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of
our method through both subjective and objective evaluations. We demonstrate
that the reference encoder learns better speaker-independent prosody when
discrete code is utilized as input in the experiments. In addition, we obtain
comparable results even when fewer parameters are inputted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stance Prediction and Analysis of Twitter data : A case study of Ghana 2020 Presidential Elections. (arXiv:2306.14203v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14203">
<div class="article-summary-box-inner">
<span><p>On December 7, 2020, Ghanaians participated in the polls to determine their
president for the next four years. To gain insights from this presidential
election, we conducted stance analysis (which is not always equivalent to
sentiment analysis) to understand how Twitter, a popular social media platform,
reflected the opinions of its users regarding the two main presidential
candidates. We collected a total of 99,356 tweets using the Twitter API
(Tweepy) and manually annotated 3,090 tweets into three classes: Against,
Neutral, and Support. We then performed preprocessing on the tweets. The
resulting dataset was evaluated using two lexicon-based approaches, VADER and
TextBlob, as well as five supervised machine learning-based approaches: Support
Vector Machine (SVM), Logistic Regression (LR), Multinomial Na\"ive Bayes
(MNB), Stochastic Gradient Descent (SGD), and Random Forest (RF), based on
metrics such as accuracy, precision, recall, and F1-score. The best performance
was achieved by Logistic Regression with an accuracy of 71.13%. We utilized
Logistic Regression to classify all the extracted tweets and subsequently
conducted an analysis and discussion of the results. For access to our data and
code, please visit:
https://github.com/ShesterG/Stance-Detection-Ghana-2020-Elections.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines. (arXiv:2306.14764v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14764">
<div class="article-summary-box-inner">
<span><p>The detection of hate speech in political discourse is a critical issue, and
this becomes even more challenging in low-resource languages. To address this
issue, we introduce a new dataset named IEHate, which contains 11,457 manually
annotated Hindi tweets related to the Indian Assembly Election Campaign from
November 1, 2021, to March 9, 2022. We performed a detailed analysis of the
dataset, focusing on the prevalence of hate speech in political communication
and the different forms of hateful language used. Additionally, we benchmark
the dataset using a range of machine learning, deep learning, and
transformer-based algorithms. Our experiments reveal that the performance of
these models can be further improved, highlighting the need for more advanced
techniques for hate speech detection in low-resource languages. In particular,
the relatively higher score of human evaluation over algorithms emphasizes the
importance of utilizing both human and automated approaches for effective hate
speech moderation. Our IEHate dataset can serve as a valuable resource for
researchers and practitioners working on developing and evaluating hate speech
detection techniques in low-resource languages. Overall, our work underscores
the importance of addressing the challenges of identifying and mitigating hate
speech in political discourse, particularly in the context of low-resource
languages. The dataset and resources for this work are made available at
https://github.com/Farhan-jafri/Indian-Election.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15123">
<div class="article-summary-box-inner">
<span><p>Review score prediction requires review text understanding, a critical
real-world application of natural language processing. Due to dissimilar text
domains in product reviews, a common practice is fine-tuning BERT models upon
reviews of differing domains. However, there has not yet been an empirical
study of cross-domain behaviors of BERT models in the various tasks of product
review understanding. In this project, we investigate text classification BERT
models fine-tuned on single-domain and multi-domain Amazon review data. In our
findings, though single-domain models achieved marginally improved performance
on their corresponding domain compared to multi-domain models, multi-domain
models outperformed single-domain models when evaluated on multi-domain data,
single-domain data the single-domain model was not fine-tuned on, and on
average when considering all tests. Though slight increases in accuracy can be
achieved through single-domain model fine-tuning, computational resources and
costs can be reduced by utilizing multi-domain models that perform well across
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15253">
<div class="article-summary-box-inner">
<span><p>Humans talk in free-form while negotiating the expressed meanings or common
ground. Despite the impressive conversational abilities of the large generative
language models, they do not consider the individual differences in contextual
understanding in a shared situated environment. In this work, we propose
MindDial, a novel conversational framework that can generate situated free-form
responses to negotiate common ground. We design an explicit mind module that
can track three-level beliefs -- the speaker's belief, the speaker's prediction
of the listener's belief, and the common belief based on the gap between the
first two. Then the speaking act classification head will decide to continue to
talk, end this turn, or take task-related action. We augment a common ground
alignment dataset MutualFriend with belief dynamics annotation, of which the
goal is to find a single mutual friend based on the free chat between two
agents. Experiments show that our model with mental state modeling can resemble
human responses when aligning common ground meanwhile mimic the natural human
conversation flow. The ablation study further validates the third-level common
belief can aggregate information of the first and second-order beliefs and
align common ground more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15354">
<div class="article-summary-box-inner">
<span><p>Disentangling uncorrelated information in speech utterances is a crucial
research topic within speech community. Different speech-related tasks focus on
extracting distinct speech representations while minimizing the affects of
other uncorrelated information. We present a large-scale speech corpus to
facilitate the research of speech representation disentanglement. 3D-Speaker
contains over 10,000 speakers, each of whom are simultaneously recorded by
multiple Devices, locating at different Distances, and some speakers are
speaking multiple Dialects. The controlled combinations of multi-dimensional
audio data yield a matrix of a diverse blend of speech representation
entanglement, thereby motivating intriguing methods to untangle them. The
multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate
large universal speech models and experiment methods of out-of-domain learning
and self-supervised learning. https://3dspeaker.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15595">
<div class="article-summary-box-inner">
<span><p>We present Position Interpolation (PI) that extends the context window sizes
of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal
fine-tuning (within 1000 steps), while demonstrating strong empirical results
on various tasks that require long context, including passkey retrieval,
language modeling, and long document summarization from LLaMA 7B to 65B.
Meanwhile, the extended model by Position Interpolation preserve quality
relatively well on tasks within its original context window. To achieve this
goal, Position Interpolation linearly down-scales the input position indices to
match the original context window size, rather than extrapolating beyond the
trained context length which may lead to catastrophically high attention scores
that completely ruin the self-attention mechanism. Our theoretical study shows
that the upper bound of interpolation is at least $\sim 600 \times$ smaller
than that of extrapolation, further demonstrating its stability. Models
extended via Position Interpolation retain its original architecture and can
reuse most pre-existing optimization and infrastructure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15634">
<div class="article-summary-box-inner">
<span><p>The automatic annotation of direct speech (AADS) in written text has been
often used in computational narrative understanding. Methods based on either
rules or deep neural networks have been explored, in particular for English or
German languages. Yet, for French, our target language, not many works exist.
Our goal is to create a unified framework to design and evaluate AADS models in
French. For this, we consolidated the largest-to-date French narrative dataset
annotated with DS per word; we adapted various baselines for sequence labelling
or from AADS in other languages; and we designed and conducted an extensive
evaluation focused on generalisation. Results show that the task still requires
substantial efforts and emphasise characteristics of each baseline. Although
this framework could be improved, it is a step further to encourage more
research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01537">
<div class="article-summary-box-inner">
<span><p>Dementia is a family of neurogenerative conditions affecting memory and
cognition in an increasing number of individuals in our globally aging
population. Automated analysis of language, speech and paralinguistic
indicators have been gaining popularity as potential indicators of cognitive
decline. Here we propose a novel longitudinal multi-modal dataset collected
from people with mild dementia and age matched controls over a period of
several months in a natural setting. The multi-modal data consists of spoken
conversations, a subset of which are transcribed, as well as typed and written
thoughts and associated extra-linguistic information such as pen strokes and
keystrokes. We describe the dataset in detail and proceed to focus on a task
using the speech modality. The latter involves distinguishing controls from
people with dementia by exploiting the longitudinal nature of the data. Our
experiments showed significant differences in how the speech varied from
session to session in the control and dementia groups.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-29 23:12:39.064571419 UTC">2023-06-29 23:12:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>