<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-30T01:30:00Z">06-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering. (arXiv:2306.16478v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16478">
<div class="article-summary-box-inner">
<span><p>This paper studies a category of visual question answering tasks, in which
accessing external knowledge is necessary for answering the questions. This
category is called outside-knowledge visual question answering (OK-VQA). A
major step in developing OK-VQA systems is to retrieve relevant documents for
the given multi-modal query. Current state-of-the-art asymmetric dense
retrieval model for this task uses an architecture with a multi-modal query
encoder and a uni-modal document encoder. Such an architecture requires a large
amount of training data for effective performance. We propose an automatic data
generation pipeline for pre-training passage retrieval models for OK-VQA tasks.
The proposed approach leads to 26.9% Precision@5 improvements compared to the
current state-of-the-art asymmetric architecture. Additionally, the proposed
pre-training approach exhibits a good ability in zero-shot retrieval scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16533">
<div class="article-summary-box-inner">
<span><p>Video retrieval (VR) involves retrieving the ground truth video from the
video database given a text caption or vice-versa. The two important components
of compositionality: objects \&amp; attributes and actions are joined using correct
semantics to form a proper text query. These components (objects \&amp; attributes,
actions and semantics) each play an important role to help distinguish among
videos and retrieve the correct ground truth video. However, it is unclear what
is the effect of these components on the video retrieval performance. We
therefore, conduct a systematic study to evaluate the compositional and
semantic understanding of video retrieval models on standard benchmarks such as
MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video
retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned
on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)
(ii) which adapt pre-trained image-text representations like CLIP for video
retrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that
actions and semantics play a minor role compared to objects \&amp; attributes in
video understanding. Moreover, video retrieval models that use pre-trained
image-text representations (CLIP) have better semantic and compositional
understanding as compared to models pre-trained on video-text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16564">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable capabilities out of
box for a wide range of applications, yet accuracy still remains a major growth
area, especially in mission-critical domains such as biomedicine. An effective
method to calibrate the confidence level on LLM responses is essential to
automatically detect errors and facilitate human-in-the-loop verification. An
important source of calibration signals stems from expert-stipulated
programmatic supervision, which is often available at low cost but has its own
limitations such as noise and coverage. In this paper, we introduce a Pareto
optimal self-supervision framework that can leverage available programmatic
supervision to systematically calibrate LLM responses by producing a risk score
for every response, without any additional manual efforts. This is accomplished
by learning a harmonizer model to align LLM output with other available
supervision sources, which would assign higher risk scores to more uncertain
LLM responses and facilitate error correction. Experiments on standard relation
extraction tasks in biomedical and general domains demonstrate the promise of
this approach, with our proposed risk scores highly correlated with the real
error rate of LLMs. For the most uncertain test instances, dynamic prompting
based on our proposed risk scores results in significant accuracy improvement
for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA)
weak supervision and GPT-4 results past SOTA supervised results on challenging
evaluation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16601">
<div class="article-summary-box-inner">
<span><p>In recent years, Transformer-based language models have become the standard
approach for natural language processing tasks. However, stringent throughput
and latency requirements in industrial applications are limiting their
adoption. To mitigate the gap, model compression techniques such as structured
pruning are being used to improve inference efficiency. However, most existing
neural network inference runtimes lack adequate support for structured
sparsity. In this paper, we propose an efficient sparse deep learning inference
software stack for Transformer-based language models where the weights are
pruned with constant block size. Our sparse software accelerator leverages
Intel Deep Learning Boost to maximize the performance of sparse matrix - dense
matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel
outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an
order of magnitude on a wide range of GEMM shapes under 5 representative
sparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up
to 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library
widely used in industry. We apply our sparse accelerator on widely-used
Transformer-based language models including Bert-Mini, DistilBERT, Bert-Base,
and BERT-Large. Our sparse inference software shows up to 1.5x speedup over
Neural Magic's Deepsparse under same configurations on Xeon on Amazon Web
Services under proxy production latency constraints. We also compare our
solution with two framework-based inference solutions, ONNX Runtime and
PyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over
PyTorch on Xeon under the latency constraints. All the source code is publicly
available on Github: https://github.com/intel/intel-extension-for-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16636">
<div class="article-summary-box-inner">
<span><p>We present the Chinese Elementary School Math Word Problems (CMATH) dataset,
comprising 1.7k elementary school-level math word problems with detailed
annotations, source from actual Chinese workbooks and exams. This dataset aims
to provide a benchmark tool for assessing the following question: to what grade
level of elementary school math do the abilities of popular large language
models (LLMs) correspond? We evaluate a variety of popular LLMs, including both
commercial and open-source options, and discover that only GPT-4 achieves
success (accuracy $\geq$ 60\%) across all six elementary school grades, while
other models falter at different grade levels. Furthermore, we assess the
robustness of several top-performing LLMs by augmenting the original problems
in the CMATH dataset with distracting information. Our findings reveal that
GPT-4 is able to maintains robustness, while other model fail. We anticipate
that our study will expose limitations in LLMs' arithmetic and reasoning
capabilities, and promote their ongoing development and advancement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A negation detection assessment of GPTs: analysis with the xNot360 dataset. (arXiv:2306.16638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16638">
<div class="article-summary-box-inner">
<span><p>Negation is a fundamental aspect of natural language, playing a critical role
in communication and comprehension. Our study assesses the negation detection
performance of Generative Pre-trained Transformer (GPT) models, specifically
GPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in
natural language using a zero-shot prediction approach applied to our custom
xNot360 dataset. Our approach examines sentence pairs labeled to indicate
whether the second sentence negates the first. Our findings expose a
considerable performance disparity among the GPT models, with GPT-4 surpassing
its counterparts and GPT-3.5 displaying a marked performance reduction. The
overall proficiency of the GPT models in negation detection remains relatively
modest, indicating that this task pushes the boundaries of their natural
language understanding capabilities. We not only highlight the constraints of
GPT models in handling negation but also emphasize the importance of logical
reliability in high-stakes domains such as healthcare, science, and law.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Linguistic Knowledge and Token-level Text Augmentation. (arXiv:2306.16644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16644">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of token-level text augmentation
and the role of probabilistic linguistic knowledge within a
linguistically-motivated evaluation context. Two text augmentation programs,
REDA and REDA$_{NG}$, were developed, both implementing five token-level text
editing operations: Synonym Replacement (SR), Random Swap (RS), Random
Insertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$
leverages pretrained $n$-gram language models to select the most likely
augmented texts from REDA's output. Comprehensive and fine-grained experiments
were conducted on a binary question matching classification task in both
Chinese and English. The results strongly refute the general effectiveness of
the five token-level text augmentation techniques under investigation, whether
applied together or separately, and irrespective of various common
classification model types used, including transformers. Furthermore, the role
of probabilistic linguistic knowledge is found to be minimal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple Oracles. (arXiv:2306.16649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16649">
<div class="article-summary-box-inner">
<span><p>Automatically generating textual content with desired attributes is an
ambitious task that people have pursued long. Existing works have made a series
of progress in incorporating unimodal controls into language models (LMs),
whereas how to generate controllable sentences with multimodal signals and high
efficiency remains an open question. To tackle the puzzle, we propose a new
paradigm of zero-shot controllable text generation with multimodal signals
(\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text
and image successively from token-level to sentence-level and maps them into a
unified probability space at decoding, which customizes the LM outputs by
weighted addition without extra training. To achieve better inter-modal
trade-offs, we further introduce an effective dynamic weighting mechanism to
regulate all control weights. Moreover, we conduct substantial experiments to
probe the relationship of being in-depth or in-width between signals from
distinct modalities. Encouraging empirical results on three downstream tasks
show that \textsc{ZeroGen} not only outperforms its counterparts on captioning
tasks by a large margin but also shows great potential in multimodal news
generation with a higher degree of control. Our code will be released at
https://github.com/ImKeTT/ZeroGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation. (arXiv:2306.16650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16650">
<div class="article-summary-box-inner">
<span><p>Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which
aims to generate a natural language sentence for a multimodal social post (an
image as well as its caption) to explain why it contains sarcasm. Although the
existing pioneer study has achieved great success with the BART backbone, it
overlooks the gap between the visual feature space and the decoder semantic
space, the object-level metadata of the image, as well as the potential
external knowledge. To solve these limitations, in this work, we propose a
novel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme,
named TEAM. In particular, TEAM extracts the object-level semantic meta-data
instead of the traditional global visual features from the input image.
Meanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge
concepts for the input text and the extracted object meta-data. Thereafter,
TEAM introduces a multi-source semantic graph that comprehensively characterize
the multi-source (i.e., caption, object meta-data, external knowledge) semantic
relations to facilitate the sarcasm reasoning. Extensive experiments on a
public released dataset MORE verify the superiority of our model over
cutting-edge methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition of Non-Native Child Speech for Language Learning Applications. (arXiv:2306.16710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16710">
<div class="article-summary-box-inner">
<span><p>Voicebots have provided a new avenue for supporting the development of
language skills, particularly within the context of second language learning.
Voicebots, though, have largely been geared towards native adult speakers. We
sought to assess the performance of two state-of-the-art ASR systems,
Wav2Vec2.0 and Whisper AI, with a view to developing a voicebot that can
support children acquiring a foreign language. We evaluated their performance
on read and extemporaneous speech of native and non-native Dutch children. We
also investigated the utility of using ASR technology to provide insight into
the children's pronunciation and fluency. The results show that recent,
pre-trained ASR transformer-based models achieve acceptable performance from
which detailed feedback on phoneme pronunciation quality can be extracted,
despite the challenging nature of child and non-native speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Paraphrastic Robustness in Textual Entailment Models. (arXiv:2306.16722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16722">
<div class="article-summary-box-inner">
<span><p>We present PaRTE, a collection of 1,126 pairs of Recognizing Textual
Entailment (RTE) examples to evaluate whether models are robust to
paraphrasing. We posit that if RTE models understand language, their
predictions should be consistent across inputs that share the same meaning. We
use the evaluation set to determine if RTE models' predictions change when
examples are paraphrased. In our experiments, contemporary models change their
predictions on 8-16\% of paraphrased examples, indicating that there is still
room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Language Representation for Question Answering over Text, Tables, and Images. (arXiv:2306.16762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16762">
<div class="article-summary-box-inner">
<span><p>When trying to answer complex questions, people often rely on multiple
sources of information, such as visual, textual, and tabular data. Previous
approaches to this problem have focused on designing input features or model
structure in the multi-modal space, which is inflexible for cross-modal
reasoning or data-efficient training. In this paper, we call for an alternative
paradigm, which transforms the images and tables into unified language
representations, so that we can simplify the task into a simpler textual QA
problem that can be solved using three steps: retrieval, ranking, and
generation, all within a language space. This idea takes advantage of the power
of pre-trained language models and is implemented in a framework called Solar.
Our experimental results show that Solar outperforms all existing methods by
10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different
metrics. Additionally, Solar achieves the best performance on the WebQA
leaderboard
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations. (arXiv:2306.16770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16770">
<div class="article-summary-box-inner">
<span><p>In open-domain dialogue generation tasks, contexts and responses in most
datasets are one-to-one mapped, violating an important many-to-many
characteristic: a context leads to various responses, and a response answers
multiple contexts. Without such patterns, models poorly generalize and prefer
responding safely. Many attempts have been made in either multi-turn settings
from a one-to-many perspective or in a many-to-many perspective but limited to
single-turn settings. The major challenge to many-to-many augment multi-turn
dialogues is that discretely replacing each turn with semantic similarity
breaks fragile context coherence. In this paper, we propose DialoGue Path
Sampling (DialoGPS) method in continuous semantic space, the first many-to-many
augmentation method for multi-turn dialogues. Specifically, we map a dialogue
to our extended Brownian Bridge, a special Gaussian process. We sample latent
variables to form coherent dialogue paths in the continuous space. A dialogue
path corresponds to a new multi-turn dialogue and is used as augmented training
data. We show the effect of DialoGPS with both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages. (arXiv:2306.16774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16774">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) has advanced the performance of many
vision-language tasks, such as image-text retrieval, visual entailment, and
visual reasoning. The pre-training mostly utilizes lexical databases and image
queries in English. Previous work has demonstrated that the pre-training in
English does not transfer well to other languages in a zero-shot setting.
However, multilingual pre-trained language models (MPLM) have excelled at a
variety of single-modal language tasks. In this paper, we propose a simple yet
efficient approach to adapt VLP to unseen languages using MPLM. We utilize a
cross-lingual contextualized token embeddings alignment approach to train text
encoders for non-English languages. Our approach does not require image input
and primarily uses machine translation, eliminating the need for target
language data. Our evaluation across three distinct tasks (image-text
retrieval, visual entailment, and natural language visual reasoning)
demonstrates that this approach outperforms the state-of-the-art multilingual
vision-language models without requiring large parallel corpora. Our code is
available at https://github.com/Yasminekaroui/CliCoTea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Large Language Model Capabilities for Conditional Generation. (arXiv:2306.16793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16793">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16805">
<div class="article-summary-box-inner">
<span><p>Perceptually Aligned Gradients (PAG) refer to an intriguing property observed
in robust image classification models, wherein their input gradients align with
human perception and pose semantic meanings. While this phenomenon has gained
significant research attention, it was solely studied in the context of
unimodal vision-only architectures. In this work, we extend the study of PAG to
Vision-Language architectures, which form the foundations for diverse
image-text tasks and applications. Through an adversarial robustification
finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit
PAG in contrast to their vanilla counterparts. This work reveals the merits of
CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we
show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to
substantial improvements in vision-language generative applications.
Furthermore, leveraging its PAG property, CLIPAG enables text-to-image
generation without any generative model, which typically requires huge
generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Formal Perspective on Byte-Pair Encoding. (arXiv:2306.16837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16837">
<div class="article-summary-box-inner">
<span><p>Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in
NLP, despite being devised initially as a compression method. BPE appears to be
a greedy algorithm at face value, but the underlying optimization problem that
BPE seeks to solve has not yet been laid down. We formalize BPE as a
combinatorial optimization problem. Via submodular functions, we prove that the
iterative greedy version is a
$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation
of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the
total backward curvature with respect to the optimal merge sequence
$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is
$\approx 0.37$.
</p>
<p>We provide a faster implementation of BPE which improves the runtime
complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log
M\right)$, where $N$ is the sequence length and $M$ is the merge count.
Finally, we optimize the brute-force algorithm for optimal BPE using
memoization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tokenization and the Noiseless Channel. (arXiv:2306.16842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16842">
<div class="article-summary-box-inner">
<span><p>Subword tokenization is a key part of many NLP pipelines. However, little is
known about why some tokenizer and hyperparameter combinations lead to better
downstream model performance than others. We propose that good tokenizers lead
to \emph{efficient} channel usage, where the channel is the means by which some
input is conveyed to the model and efficiency can be quantified in
information-theoretic terms as the ratio of the Shannon entropy to the maximum
possible entropy of the token distribution. Yet, an optimal encoding according
to Shannon entropy assigns extremely long codes to low-frequency tokens and
very short codes to high-frequency tokens. Defining efficiency in terms of
R\'enyi entropy, on the other hand, penalizes distributions with either very
high or very low-frequency tokens. In machine translation, we find that across
multiple tokenizers, the R\'enyi entropy with $\alpha = 2.5$ has a very strong
correlation with \textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for
compressed length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16900">
<div class="article-summary-box-inner">
<span><p>Many recent improvements in NLP stem from the development and use of large
pre-trained language models (PLMs) with billions of parameters. Large model
sizes makes computational cost one of the main limiting factors for training
and evaluating such models; and has raised severe concerns about the
sustainability, reproducibility, and inclusiveness for researching PLMs. These
concerns are often based on personal experiences and observations. However,
there had not been any large-scale surveys that investigate them. In this work,
we provide a first attempt to quantify these concerns regarding three topics,
namely, environmental impact, equity, and impact on peer reviewing. By
conducting a survey with 312 participants from the NLP community, we capture
existing (dis)parities between different and within groups with respect to
seniority, academia, and industry; and their impact on the peer reviewing
process. For each topic, we provide an analysis and devise recommendations to
mitigate found disparities, some of which already successfully implemented.
Finally, we discuss additional concerns raised by many participants in
free-text responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Cross-Utterance Context For ASR Decoding. (arXiv:2306.16903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16903">
<div class="article-summary-box-inner">
<span><p>While external language models (LMs) are often incorporated into the decoding
stage of automated speech recognition systems, these models usually operate
with limited context. Cross utterance information has been shown to be
beneficial during second pass re-scoring, however this limits the hypothesis
space based on the local information available to the first pass LM. In this
work, we investigate the incorporation of long-context transformer LMs for
cross-utterance decoding of acoustic models via beam search, and compare
against results from n-best rescoring. Results demonstrate that beam search
allows for an improved use of cross-utterance context. When evaluating on the
long-format dataset AMI, results show a 0.7\% and 0.3\% absolute reduction on
dev and test sets compared to the single-utterance setting, with improvements
when including up to 500 tokens of prior context. Evaluations are also provided
for Tedlium-1 with less significant improvements of around 0.1\% absolute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?. (arXiv:2306.16931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16931">
<div class="article-summary-box-inner">
<span><p>This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023
shared task for Task-A and Task-C. We focus especially on Task-C and propose a
novel LLMs cooperation system named a doctor-patient loop to generate
high-quality conversation data sets. The experiment results demonstrate that
our approaches yield reasonable performance as evaluated by automatic metrics
such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we
conducted a comparative analysis between our proposed method and ChatGPT and
GPT-4. This analysis also investigates the potential of utilizing cooperation
LLMs to generate high-quality datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Music Hierarchies with a Graph-Based Neural Decoder. (arXiv:2306.16955v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16955">
<div class="article-summary-box-inner">
<span><p>This paper describes a data-driven framework to parse musical sequences into
dependency trees, which are hierarchical structures used in music cognition
research and music analysis. The parsing involves two steps. First, the input
sequence is passed through a transformer encoder to enrich it with contextual
information. Then, a classifier filters the graph of all possible dependency
arcs to produce the dependency tree. One major benefit of this system is that
it can be easily integrated into modern deep-learning pipelines. Moreover,
since it does not rely on any particular symbolic grammar, it can consider
multiple musical features simultaneously, make use of sequential context
information, and produce partial results for noisy inputs. We test our approach
on two datasets of musical trees -- time-span trees of monophonic note
sequences and harmonic trees of jazz chord sequences -- and show that our
approach outperforms previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis. (arXiv:2306.16956v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16956">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis is a long-standing research interest in the
field of opinion mining, and in recent years, researchers have gradually
shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA
tasks. However, the datasets currently used in the research are limited to
individual elements of specific tasks, usually focusing on in-domain settings,
ignoring implicit aspects and opinions, and with a small data scale. To address
these issues, we propose a large-scale Multi-Element Multi-Domain dataset
(MEMD) that covers the four elements across five domains, including nearly
20,000 review sentences and 30,000 quadruples annotated with explicit and
implicit aspects and opinions for ABSA research. Meanwhile, we evaluate
generative and non-generative baselines on multiple ABSA subtasks under the
open domain setting, and the results show that open domain ABSA as well as
mining implicit aspects and opinions remain ongoing challenges to be addressed.
The datasets are publicly released at \url{https://github.com/NUSTM/MEMD-ABSA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units. (arXiv:2306.17005v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17005">
<div class="article-summary-box-inner">
<span><p>The goal of Automatic Voice Over (AVO) is to generate speech in sync with a
silent video given its text script. Recent AVO frameworks built upon
text-to-speech synthesis (TTS) have shown impressive results. However, the
current AVO learning objective of acoustic feature reconstruction brings in
indirect supervision for inter-modal alignment learning, thus limiting the
synchronization performance and synthetic speech quality. To this end, we
propose a novel AVO method leveraging the learning objective of self-supervised
discrete speech unit prediction, which not only provides more direct
supervision for the alignment learning, but also alleviates the mismatch
between the text-video context and acoustic features. Experimental results show
that our proposed method achieves remarkable lip-speech synchronization and
high speech quality by outperforming baselines in both objective and subjective
evaluations. Code and speech samples are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17020">
<div class="article-summary-box-inner">
<span><p>The task of determining crime types based on criminal behavior facts has
become a very important and meaningful task in social science. But the problem
facing the field now is that the data samples themselves are unevenly
distributed, due to the nature of the crime itself. At the same time, data sets
in the judicial field are less publicly available, and it is not practical to
produce large data sets for direct training. This article proposes a new
training model to solve this problem through NLP processing methods. We first
propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the
defects of uneven data set distribution by generating new samples. Then we use
a large open source dataset (CAIL-big) as our pretraining dataset and a small
dataset collected by ourselves for Fine-tuning, giving it good generalization
ability to unfamiliar small datasets. At the same time, we use the improved
Bert model with dynamic masking to improve the model. Experiments show that the
proposed method achieves state-of-the-art results on the present dataset. At
the same time, the effectiveness of module CFDPM is proved by experiments. This
article provides a valuable methodology contribution for classifying social
science texts such as criminal behaviors. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring & Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion. (arXiv:2306.17034v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17034">
<div class="article-summary-box-inner">
<span><p>Sparse knowledge graph (KG) scenarios pose a challenge for previous Knowledge
Graph Completion (KGC) methods, that is, the completion performance decreases
rapidly with the increase of graph sparsity. This problem is also exacerbated
because of the widespread existence of sparse KGs in practical applications. To
alleviate this challenge, we present a novel framework, LR-GCN, that is able to
automatically capture valuable long-range dependency among entities to
supplement insufficient structure features and distill logical reasoning
knowledge for sparse KGC. The proposed approach comprises two main components:
a GNN-based predictor and a reasoning path distiller. The reasoning path
distiller explores high-order graph structures such as reasoning paths and
encodes them as rich-semantic edges, explicitly compositing long-range
dependencies into the predictor. This step also plays an essential role in
densifying KGs, effectively alleviating the sparse issue. Furthermore, the path
distiller further distills logical reasoning knowledge from these mined
reasoning paths into the predictor. These two components are jointly optimized
using a well-designed variational EM algorithm. Extensive experiments and
analyses on four sparse benchmarks demonstrate the effectiveness of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Grammatical Tagging for the Legal Language of Cybersecurity. (arXiv:2306.17042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17042">
<div class="article-summary-box-inner">
<span><p>Legal language can be understood as the language typically used by those
engaged in the legal profession and, as such, it may come both in spoken or
written form. Recent legislation on cybersecurity obviously uses legal language
in writing, thus inheriting all its interpretative complications due to the
typical abundance of cases and sub-cases as well as to the general richness in
detail. This paper faces the challenge of the essential interpretation of the
legal language of cybersecurity, namely of the extraction of the essential
Parts of Speech (POS) from the legal documents concerning cybersecurity. The
challenge is overcome by our methodology for POS tagging of legal language. It
leverages state-of-the-art open-source tools for Natural Language Processing
(NLP) as well as manual analysis to validate the outcomes of the tools. As a
result, the methodology is automated and, arguably, general for any legal
language following minor tailoring of the preprocessing step. It is
demonstrated over the most relevant EU legislation on cybersecurity, namely on
the NIS 2 directive, producing the first, albeit essential, structured
interpretation of such a relevant document. Moreover, our findings indicate
that tools such as SpaCy and ClausIE reach their limits over the legal language
of the NIS 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17059">
<div class="article-summary-box-inner">
<span><p>Documents hold spatial focus and valuable locality characteristics. For
example, descriptions of listings in real estate or travel blogs contain
information about specific local neighborhoods. This information is valuable to
characterize how humans perceive their environment. However, the first step to
making use of this information is to identify the spatial focus (e.g., a city)
of a document. Traditional approaches for identifying the spatial focus of a
document rely on detecting and disambiguating toponyms from the document. This
approach requires a vocabulary set of location phrases and ad-hoc rules, which
ignore important words related to location. Recent topic modeling approaches
using large language models often consider a few topics, each with broad
coverage. In contrast, the spatial focus of a document can be a country, a
city, or even a neighborhood, which together, is much larger than the number of
topics considered in these approaches. Additionally, topic modeling methods are
often applied to broad topics of news articles where context is easily
distinguishable. To identify the geographic focus of a document effectively, we
present a simple but effective Joint Embedding of multi-LocaLitY (JELLY), which
jointly learns representations with separate encoders of document and location.
JELLY significantly outperforms state-of-the-art methods for identifying
spatial focus from documents from a number of sources. We also demonstrate case
studies on the arithmetic of the learned representations, including identifying
cities with similar locality characteristics and zero-shot learning to identify
document spatial focus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Presenting an approach based on weighted CapsuleNet networks for Arabic and Persian multi-domain sentiment analysis. (arXiv:2306.17068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17068">
<div class="article-summary-box-inner">
<span><p>Sentiment classification is a fundamental task in natural language
processing, assigning one of the three classes, positive, negative, or neutral,
to free texts. However, sentiment classification models are highly domain
dependent; the classifier may perform classification with reasonable accuracy
in one domain but not in another due to the Semantic multiplicity of words
getting poor accuracy. This article presents a new Persian/Arabic multi-domain
sentiment analysis method using the cumulative weighted capsule networks
approach. Weighted capsule ensemble consists of training separate capsule
networks for each domain and a weighting measure called domain belonging degree
(DBD). This criterion consists of TF and IDF, which calculates the dependency
of each document for each domain separately; this value is multiplied by the
possible output that each capsule creates. In the end, the sum of these
multiplications is the title of the final output, and is used to determine the
polarity. And the most dependent domain is considered the final output for each
domain. The proposed method was evaluated using the Digikala dataset and
obtained acceptable accuracy compared to the existing approaches. It achieved
an accuracy of 0.89 on detecting the domain of belonging and 0.99 on detecting
the polarity. Also, for the problem of dealing with unbalanced classes, a
cost-sensitive function was used. This function was able to achieve 0.0162
improvements in accuracy for sentiment classification. This approach on Amazon
Arabic data can achieve 0.9695 accuracies in domain classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17089">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been successfully used in many
natural-language tasks and applications including text generation and AI
chatbots. They also are a promising new technology for concept-oriented deep
learning (CODL). However, the prerequisite is that LLMs understand concepts and
ensure conceptual consistency. We discuss these in this paper, as well as major
uses of LLMs for CODL including concept extraction from text, concept graph
extraction from text, and concept learning. Human knowledge consists of both
symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only
LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal
LLMs, on the other hand, are capable of representing the full range (conceptual
and sensory) of human knowledge. We discuss conceptual understanding in
visual-language LLMs, the most important multimodal LLMs, and major uses of
them for CODL including concept extraction from image, concept graph extraction
from image, and concept learning. While uses of LLMs for CODL are valuable
standalone, they are particularly valuable as part of LLM applications such as
AI chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17103">
<div class="article-summary-box-inner">
<span><p>We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic
lyrics transcription method achieving state-of-the-art performance on various
lyrics transcription datasets, even in challenging genres such as rock and
metal. Our novel, training-free approach utilizes Whisper, a weakly supervised
robust speech recognition model, and GPT-4, today's most performant chat-based
large language model. In the proposed method, Whisper functions as the "ear" by
transcribing the audio, while GPT-4 serves as the "brain," acting as an
annotator with a strong performance for contextualized output selection and
correction. Our experiments show that LyricWhiz significantly reduces Word
Error Rate compared to existing methods in English and can effectively
transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to
create the first publicly available, large-scale, multilingual lyrics
transcription dataset with a CC-BY-NC-SA copyright license, based on
MTG-Jamendo, and offer a human-annotated subset for noise level estimation and
evaluation. We anticipate that our proposed method and dataset will advance the
development of multilingual lyrics transcription, a challenging and emerging
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17107">
<div class="article-summary-box-inner">
<span><p>Instruction tuning unlocks the superior capability of Large Language Models
(LLM) to interact with humans. Furthermore, recent instruction-following
datasets include images as visual inputs, collecting responses for image-based
instructions. However, visual instruction-tuned models cannot comprehend
textual details within images well. This work enhances the current visual
instruction tuning pipeline with text-rich images (e.g., movie posters, book
covers, etc.). Specifically, we first use publicly available OCR tools to
collect results on 422K text-rich images from the LAION dataset. Moreover, we
prompt text-only GPT-4 with recognized texts and image captions to generate 16K
conversations, each containing question-answer pairs for text-rich images. By
combining our collected data with previous multi-modal instruction-following
data, our model, LLaVAR, substantially improves the LLaVA model's capability on
text-based VQA datasets (up to 20% accuracy improvement) while achieving an
accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following
evaluation also demonstrates the improvement of our model on both natural
images and text-rich images. Through qualitative analysis, LLaVAR shows
promising interaction (e.g., reasoning, writing, and elaboration) skills with
humans based on the latest real-world online content that combines text and
images. We make our code/data/models publicly available at
https://llavar.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17156">
<div class="article-summary-box-inner">
<span><p>Generative AI and large language models hold great promise in enhancing
computing education by powering next-generation educational technologies for
introductory programming. Recent works have studied these models for different
scenarios relevant to programming education; however, these works are limited
for several reasons, as they typically consider already outdated models or only
specific scenario(s). Consequently, there is a lack of a systematic study that
benchmarks state-of-the-art models for a comprehensive set of programming
education scenarios. In our work, we systematically evaluate two models,
ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human
tutors for a variety of scenarios. We evaluate using five introductory Python
programming problems and real-world buggy programs from an online platform, and
assess performance using expert-based annotations. Our results show that GPT-4
drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human
tutors' performance for several scenarios. These results also highlight
settings where GPT-4 still struggles, providing exciting future directions on
developing techniques to improve the performance of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Did AI get more negative recently?. (arXiv:2202.13610v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13610">
<div class="article-summary-box-inner">
<span><p>In this paper, we classify scientific articles in the domain of natural
language processing (NLP) and machine learning (ML), as core subfields of
artificial intelligence (AI), into whether (i) they extend the current
state-of-the-art by the introduction of novel techniques which beat existing
models or whether (ii) they mainly criticize the existing state-of-the-art,
i.e. that it is deficient with respect to some property (e.g. wrong evaluation,
wrong datasets, misleading task specification). We refer to contributions under
(i) as having a 'positive stance' and contributions under (ii) as having a
'negative stance' (to related work). We annotate over 1.5 k papers from NLP and
ML to train a SciBERT-based model to automatically predict the stance of a
paper based on its title and abstract. We then analyse large-scale trends on
over 41 k papers from the last approximately 35 years in NLP and ML, finding
that papers have become substantially more positive over time, but negative
papers also got more negative and we observe considerably more negative papers
in recent years. Negative papers are also more influential in terms of
citations they receive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04636">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks are a major challenge faced by current machine learning
research. These purposely crafted inputs fool even the most advanced models,
precluding their deployment in safety-critical applications. Extensive research
in computer vision has been carried to develop reliable defense strategies.
However, the same issue remains less explored in natural language processing.
Our work presents a model-agnostic detector of adversarial text examples. The
approach identifies patterns in the logits of the target classifier when
perturbing the input text. The proposed detector improves the current
state-of-the-art performance in recognizing adversarial inputs and exhibits
strong generalization capabilities across different NLP models, datasets, and
word-level attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16001">
<div class="article-summary-box-inner">
<span><p>A good automatic evaluation metric for language generation ideally correlates
highly with human judgements of text quality. Yet, there is a dearth of such
metrics, which inhibits the rapid and efficient progress of language
generators. One exception is the recently proposed Mauve. In theory, Mauve
measures an information-theoretic divergence between two probability
distributions over strings: one representing the language generator under
evaluation; the other representing the true natural language distribution.
Mauve's authors argue that its success comes from the qualitative properties of
their proposed divergence. Yet in practice, as this divergence is uncomputable,
Mauve approximates it by measuring the divergence between multinomial
distributions over clusters instead, where cluster assignments are attained by
grouping strings based on a pre-trained language model's embeddings. As we
show, however, this is not a tight approximation -- in either theory or
practice. This begs the question: why does Mauve work so well? In this work, we
show that Mauve was right for the wrong reasons, and that its newly proposed
divergence is not necessary for its high performance. In fact, classical
divergences paired with its proposed cluster-based approximation may actually
serve as better evaluation metrics. We finish the paper with a probing
analysis; this analysis leads us to conclude that -- by encoding syntactic- and
coherence-level features of text, while ignoring surface-level features -- such
cluster-based substitutes to string distributions may simply be better for
evaluating state-of-the-art language generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Knowledge Embeddings. (arXiv:2206.12617v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12617">
<div class="article-summary-box-inner">
<span><p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding
entities and relations into continuous vector spaces. Existing methods are
mainly structure-based or description-based. Structure-based methods learn
representations that preserve the inherent structure of KGs. They cannot well
represent abundant long-tail entities in real-world KGs with limited structural
information. Description-based methods leverage textual information and
language models. Prior approaches in this direction barely outperform
structure-based ones, and suffer from problems like expensive negative sampling
and restrictive description demand. In this paper, we propose LMKE, which
adopts Language Models to derive Knowledge Embeddings, aiming at both enriching
representations of long-tail entities and solving problems of prior
description-based methods. We formulate description-based KE learning with a
contrastive learning framework to improve efficiency in training and
evaluation. Experimental results show that LMKE achieves state-of-the-art
performance on KE benchmarks of link prediction and triple classification,
especially for long-tail entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The expected sum of edge lengths in planar linearizations of trees. Theory and applications. (arXiv:2207.05564v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05564">
<div class="article-summary-box-inner">
<span><p>Dependency trees have proven to be a very successful model to represent the
syntactic structure of sentences of human languages. In these structures,
vertices are words and edges connect syntactically-dependent words. The
tendency of these dependencies to be short has been demonstrated using random
baselines for the sum of the lengths of the edges or its variants. A ubiquitous
baseline is the expected sum in projective orderings (wherein edges do not
cross and the root word of the sentence is not covered by any edge), that can
be computed in time $O(n)$. Here we focus on a weaker formal constraint, namely
planarity. In the theoretical domain, we present a characterization of
planarity that, given a sentence, yields either the number of planar
permutations or an efficient algorithm to generate uniformly random planar
permutations of the words. We also show the relationship between the expected
sum in planar arrangements and the expected sum in projective arrangements. In
the domain of applications, we derive a $O(n)$-time algorithm to calculate the
expected value of the sum of edge lengths. We also apply this research to a
parallel corpus and find that the gap between actual dependency distance and
the random baseline reduces as the strength of the formal constraint on
dependency structures increases, suggesting that formal constraints absorb part
of the dependency distance minimization effect. Our research paves the way for
replicating past research on dependency distance minimization using random
planar linearizations as random baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10012">
<div class="article-summary-box-inner">
<span><p>Methods for erasing human-interpretable concepts from neural representations
that assume linearity have been found to be tractable and useful. However, the
impact of this removal on the behavior of downstream classifiers trained on the
modified representations is not fully understood. In this work, we formally
define the notion of log-linear guardedness as the inability of an adversary to
predict the concept directly from the representation, and study its
implications. We show that, in the binary case, under certain assumptions, a
downstream log-linear model cannot recover the erased concept. However, we
demonstrate that a multiclass log-linear model \emph{can} be constructed that
indirectly recovers the concept in some cases, pointing to the inherent
limitations of log-linear guardedness as a downstream bias mitigation
technique. These findings shed light on the theoretical limitations of linear
erasure methods and highlight the need for further research on the connections
between intrinsic and extrinsic bias in neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MooseNet: A Trainable Metric for Synthesized Speech with a PLDA Module. (arXiv:2301.07087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.07087">
<div class="article-summary-box-inner">
<span><p>We present MooseNet, a trainable speech metric that predicts the listeners'
Mean Opinion Score (MOS). We propose a novel approach where the Probabilistic
Linear Discriminative Analysis (PLDA) generative model is used on top of an
embedding obtained from a self-supervised learning (SSL) neural network (NN)
model. We show that PLDA works well with a non-finetuned SSL model when trained
only on 136 utterances (ca. one minute training time) and that PLDA
consistently improves various neural MOS prediction models, even
state-of-the-art models with task-specific fine-tuning. Our ablation study
shows PLDA training superiority over SSL model fine-tuning in a low-resource
scenario. We also improve SSL model fine-tuning using a convenient optimizer
choice and additional contrastive and multi-task training objectives. The
fine-tuned MooseNet NN with the PLDA module achieves the best results,
surpassing the SSL baseline on the VoiceMOS Challenge data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11850">
<div class="article-summary-box-inner">
<span><p>Automated news credibility and fact-checking at scale require accurately
predicting news factuality and media bias. This paper introduces a large
sentence-level dataset, titled "FactNews", composed of 6,191 sentences expertly
annotated according to factuality and media bias definitions proposed by
AllSides. We use FactNews to assess the overall reliability of news sources, by
formulating two text classification problems for predicting sentence-level
factuality of news reporting and bias of media outlets. Our experiments
demonstrate that biased sentences present a higher number of words compared to
factual sentences, besides having a predominance of emotions. Hence, the
fine-grained analysis of subjectivity and impartiality of news articles
provided promising results for predicting the reliability of media outlets.
Finally, due to the severity of fake news and political polarization in Brazil,
and the lack of research for Portuguese, both dataset and baseline were
proposed for Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data. (arXiv:2303.02577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02577">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated that using parameter efficient tuning techniques
such as prefix tuning (or P-tuning) on pretrained language models can yield
performance that is comparable or superior to fine-tuning while dramatically
reducing trainable parameters. Nevertheless, the effectiveness of such methods
under the context of data augmentation, a common strategy to improve learning
under low data regimes, has not been fully explored. In this paper, we examine
the effectiveness of several popular task-agnostic data augmentation
techniques, i.e., EDA, Back Translation, and Mixup, when using two general
parameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity.
We show that data augmentation can be used to boost the performance of P-tuning
and LoRA models, but the effectiveness of each technique varies and certain
methods can lead to a notable degradation in performance, particularly when
using larger models and on harder tasks. We further analyze the sentence
representations of P-tuning compared to fine-tuning to help understand the
above behaviour, and reveal how P-tuning generally presents a more limited
ability to separate the sentence embeddings from different classes of augmented
data. In addition, it displays poorer performance on heavily altered data.
However, we demonstrate that by adding a simple contrastive loss function it
can help mitigate such issues for prefix tuning, resulting in sizable
improvements to augmented data performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11156">
<div class="article-summary-box-inner">
<span><p>In this paper, both empirically and theoretically, we show that several
AI-text detectors are not reliable in practical scenarios. Empirically, we show
that paraphrasing attacks, where a light paraphraser is applied on top of a
large language model (LLM), can break a whole range of detectors, including
ones using watermarking schemes as well as neural network-based detectors and
zero-shot classifiers. Our experiments demonstrate that retrieval-based
detectors, designed to evade paraphrasing attacks, are still vulnerable to
recursive paraphrasing. We then provide a theoretical impossibility result
indicating that as language models become more sophisticated and better at
emulating human text, the performance of even the best-possible detector
decreases. For a sufficiently advanced language model seeking to imitate human
text, even the best-possible detector may only perform marginally better than a
random classifier. Our result is general enough to capture specific scenarios
such as particular writing styles, clever prompt design, or text paraphrasing.
We also extend the impossibility result to include the case where pseudorandom
number generators are used for AI-text generation instead of true randomness.
We show that the same result holds with a negligible correction term for all
polynomial-time computable detectors. Finally, we show that even LLMs protected
by watermarking schemes can be vulnerable against spoofing attacks where
adversarial humans can infer hidden LLM text signatures and add them to
human-generated text to be detected as text generated by the LLMs, potentially
causing reputational damage to their developers. We believe these results can
open an honest conversation in the community regarding the ethical and reliable
use of AI-generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v11 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07396">
<div class="article-summary-box-inner">
<span><p>Physicians considering clinical trials for their patients are met with the
laborious process of checking many text based eligibility criteria. Large
Language Models (LLMs) have shown to perform well for clinical information
extraction and clinical reasoning, including medical tests, but not yet in
real-world scenarios. This paper investigates the use of InstructGPT to assist
physicians in determining eligibility for clinical trials based on a patient's
summarised medical profile. Using a prompting strategy combining one-shot,
selection-inference and chain-of-thought techniques, we investigate the
performance of LLMs on 10 synthetically created patient profiles. Performance
is evaluated at four levels: ability to identify screenable eligibility
criteria from a trial given a medical profile; ability to classify for each
individual criterion whether the patient qualifies; the overall classification
whether a patient is eligible for a clinical trial and the percentage of
criteria to be screened by physician. We evaluated against 146 clinical trials
and a total of 4,135 eligibility criteria. The LLM was able to correctly
identify the screenability of 72% (2,994/4,135) of the criteria. Additionally,
72% (341/471) of the screenable criteria were evaluated correctly. The
resulting trial level classification as eligible or ineligible resulted in a
recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0
and precision of 0.71 on clinical trial level can be achieved while reducing
the amount of criteria to be checked by an estimated 90%. LLMs can be used to
assist physicians with pre-screening of patients for clinical trials. By
forcing instruction-tuned LLMs to produce chain-of-thought responses, the
reasoning can be made transparent to and the decision process becomes amenable
by physicians, thereby making such a system feasible for use in real-world
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14177">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models, including ChatGPT, have demonstrated
exceptional performance in various natural language generation tasks. However,
there has been limited research evaluating ChatGPT's keyphrase generation
ability, which involves identifying informative phrases that accurately reflect
a document's content. This study seeks to address this gap by comparing
ChatGPT's keyphrase generation performance with state-of-the-art models, while
also testing its potential as a solution for two significant challenges in the
field: domain adaptation and keyphrase generation from long documents. We
conducted experiments on six publicly available datasets from scientific
articles and news domains, analyzing performance on both short and long
documents. Our results show that ChatGPT outperforms current state-of-the-art
models in all tested datasets and environments, generating high-quality
keyphrases that adapt well to diverse domains and document lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19915">
<div class="article-summary-box-inner">
<span><p>The increasingly popular adoption of source code in many critical tasks
motivates the development of data augmentation (DA) techniques to enhance
training data and improve various capabilities (e.g., robustness and
generalizability) of these models. Although a series of DA methods have been
proposed and tailored for source code models, there lacks a comprehensive
survey and examination to understand their effectiveness and implications. This
paper fills this gap by conducting a comprehensive and integrative survey of
data augmentation for source code, wherein we systematically compile and
encapsulate existing literature to provide a comprehensive overview of the
field. We start by constructing a taxonomy of DA for source code models model
approaches, followed by a discussion on prominent, methodologically
illustrative approaches. Next, we highlight the general strategies and
techniques to optimize the DA quality. Subsequently, we underscore techniques
that find utility in widely-accepted source code scenarios and downstream
tasks. Finally, we outline the prevailing challenges and potential
opportunities for future research. In essence, this paper endeavors to
demystify the corpus of existing literature on DA for source code models, and
foster further exploration in this sphere. Complementing this, we present a
continually updated GitHub repository that hosts a list of update-to-date
papers on DA for source code models, accessible at
\url{https://github.com/terryyz/DataAug4Code}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13651">
<div class="article-summary-box-inner">
<span><p>With the rise of Large Language Models (LLMs) and their ubiquitous deployment
in diverse domains, measuring language model behavior on realistic data is
imperative. For example, a company deploying a client-facing chatbot must
ensure that the model will not respond to client requests with profanity.
Current evaluations approach this problem using small, domain-specific datasets
with human-curated labels. These evaluation sets are often sampled from a
narrow and simplified distribution, and data sources can unknowingly be leaked
into the training set which can lead to misleading evaluations. To bypass these
drawbacks, we propose a framework for self-supervised evaluation of LLMs by
analyzing their sensitivity or invariance to transformations on the input text.
Self-supervised evaluation can directly monitor LLM behavior on datasets
collected in the wild or streamed during live model deployment. We demonstrate
self-supervised evaluation strategies for measuring closed-book knowledge,
toxicity, and long-range context dependence, in addition to sensitivity to
grammatical structure and tokenization errors. When comparisons to similar
human-labeled benchmarks are available, we find strong correlations between
self-supervised and human-supervised evaluations. The self-supervised paradigm
complements current evaluation strategies that rely on labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14122">
<div class="article-summary-box-inner">
<span><p>Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction
(MRE) necessitate the fundamental reasoning capacity for intricate linguistic
and multimodal comprehension. In this study, we explore distilling the
reasoning ability of large language models (LLMs) into a more compact student
model by generating a \textit{chain of thought} (CoT) -- a sequence of
intermediate reasoning steps. Specifically, we commence by exemplifying the
elicitation of such reasoning ability from LLMs through CoT prompts covering
multi-grain (noun, sentence, multimodality) and data-augmentation (style,
entity, image) dimensions. Subsequently, we present a novel conditional prompt
distillation method to assimilate the commonsense reasoning ability from LLMs,
thereby enhancing the utility of the student model in addressing text-only
inputs without the requisite addition of image and CoT knowledge. Extensive
experiments reveal that our approach attains state-of-the-art accuracy and
manifests a plethora of advantages concerning interpretability, data
efficiency, and cross-domain generalization on MNER and MRE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.16125">
<div class="article-summary-box-inner">
<span><p>This paper describes our participation in the MentalRiskES task at IberLEF
2023. The task involved predicting the likelihood of an individual experiencing
depression based on their social media activity. The dataset consisted of
conversations from 175 Telegram users, each labeled according to their evidence
of suffering from the disorder. We used a combination of traditional machine
learning and deep learning techniques to solve four predictive subtasks: binary
classification, simple regression, multiclass classification, and multi-output
regression.
</p>
<p>We approached this by training a model to solve the multi-output regression
case and then transforming the predictions to work for the other three
subtasks.
</p>
<p>We compare the performance of two modeling approaches: fine-tuning a
BERT-based model directly for the task or using its embeddings as inputs to a
linear regressor, with the latter yielding better results. The code to
reproduce our results can be found at:
https://github.com/simonsanvil/EarlyDepression-MentalRiskES
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-07-01 23:12:13.452562277 UTC">2023-07-01 23:12:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>