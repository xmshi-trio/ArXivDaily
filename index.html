<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-19T01:30:00Z">06-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09361">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition aims to identify and analyze emotional states in
target speech similar to humans. Perfect emotion recognition can greatly
benefit a wide range of human-machine interaction tasks. Inspired by the human
process of understanding emotions, we demonstrate that compared to quantized
modeling, understanding speech content from a continuous perspective, akin to
human-like comprehension, enables the model to capture more comprehensive
emotional information. Additionally, considering that humans adjust their
perception of emotional words in textual semantic based on certain cues present
in speech, we design a novel search space and search for the optimal fusion
strategy for the two types of information. Experimental results further
validate the significance of this perception adjustment. Building on these
observations, we propose a novel framework called Multiple perspectives Fusion
Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge
to capture speech semantic and quantization-based knowledge to learn textual
semantic. Then, we search for the optimal fusion strategy for them.
Experimental results demonstrate that MFAS surpasses existing models in
comprehensively capturing speech emotion information and can automatically
adjust fusion strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09390">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel framework for quantitatively evaluating the
interactive ChatGPT model in the context of suicidality assessment from social
media posts, utilizing the University of Maryland Reddit suicidality dataset.
We conduct a technical evaluation of ChatGPT's performance on this task using
Zero-Shot and Few-Shot experiments and compare its results with those of two
fine-tuned transformer-based models. Additionally, we investigate the impact of
different temperature parameters on ChatGPT's response generation and discuss
the optimal temperature based on the inconclusiveness rate of ChatGPT. Our
results indicate that while ChatGPT attains considerable accuracy in this task,
transformer-based models fine-tuned on human-annotated datasets exhibit
superior performance. Moreover, our analysis sheds light on how adjusting the
ChatGPT's hyperparameters can improve its ability to assist mental health
professionals in this critical task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09442">
<div class="article-summary-box-inner">
<span><p>Deploying Large language models (LLMs) can pose hazards from harmful outputs
such as toxic or dishonest speech. Prior work has introduced tools that elicit
harmful outputs in order to identify and mitigate these risks. While this is a
valuable step toward securing language models, these approaches typically rely
on a pre-existing classifier for undesired outputs. This limits their
application to situations where the type of harmful behavior is known with
precision beforehand. However, this skips a central challenge of red teaming:
developing a contextual understanding of the behaviors that a model can
exhibit. Furthermore, when such a classifier already exists, red teaming has
limited marginal value because the classifier could simply be used to filter
training data or model outputs. In this work, we consider red teaming under the
assumption that the adversary is working from a high-level, abstract
specification of undesired behavior. The red team is expected to refine/extend
this specification and identify methods to elicit this behavior from the model.
Our red teaming framework consists of three steps: 1) Exploring the model's
behavior in the desired context; 2) Establishing a measurement of undesired
behavior (e.g., a classifier trained to reflect human evaluations); and 3)
Exploiting the model's flaws using this measure and an established red teaming
methodology. We apply this approach to red team GPT-2 and GPT-3 models to
systematically discover classes of prompts that elicit toxic and dishonest
statements. In doing so, we also construct and release the CommonClaim dataset
of 20,000 statements that have been labeled by human subjects as
common-knowledge-true, common-knowledge-false, or neither. Code is available at
https://github.com/thestephencasper/explore_establish_exploit_llms. CommonClaim
is available at https://github.com/thestephencasper/common_claim.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09479">
<div class="article-summary-box-inner">
<span><p>Work on scaling laws has found that large language models (LMs) show
predictable improvements to overall loss with increased scale (model size,
training data, and compute). Here, we present evidence for the claim that LMs
may show inverse scaling, or worse task performance with increased scale, e.g.,
due to flaws in the training objective and data. We present empirical evidence
of inverse scaling on 11 datasets collected by running a public contest, the
Inverse Scaling Prize, with a substantial prize pool. Through analysis of the
datasets, along with other examples found in the literature, we identify four
potential causes of inverse scaling: (i) preference to repeat memorized
sequences over following in-context instructions, (ii) imitation of undesirable
patterns in the training data, (iii) tasks containing an easy distractor task
which LMs could focus on, rather than the harder real task, and (iv) correct
but misleading few-shot demonstrations of the task. We release the winning
datasets at https://inversescaling.com/data to allow for further investigation
of inverse scaling. Our tasks have helped drive the discovery of U-shaped and
inverted-U scaling trends, where an initial trend reverses, suggesting that
scaling trends are less reliable at predicting the behavior of larger-scale
models than previously understood. Overall, our results suggest that there are
tasks for which increased model scale alone may not lead to progress, and that
more careful thought needs to go into the data and objectives for training
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events. (arXiv:2306.09505v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09505">
<div class="article-summary-box-inner">
<span><p>Biographical event detection is a relevant task for the exploration and
comparison of the ways in which people's lives are told and represented. In
this sense, it may support several applications in digital humanities and in
works aimed at exploring bias about minoritized groups. Despite that, there are
no corpora and models specifically designed for this task. In this paper we
fill this gap by presenting a new corpus annotated for biographical event
detection. The corpus, which includes 20 Wikipedia biographies, was compared
with five existing corpora to train a model for the biographical event
detection task. The model was able to detect all mentions of the target-entity
in a biography with an F-score of 0.808 and the entity-related events with an
F-score of 0.859. Finally, the model was used for performing an analysis of
biases about women and non-Western people in Wikipedia biographies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09519">
<div class="article-summary-box-inner">
<span><p>Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts
of a relation with few-shot reference entity pairs. Current approaches randomly
select one negative sample for each reference entity pair to minimize a
margin-based ranking loss, which easily leads to a zero-loss problem if the
negative sample is far away from the positive sample and then out of the
margin. Moreover, the entity should have a different representation under a
different context. To tackle these issues, we propose a novel Relation-Aware
Network with Attention-Based Loss (RANA) framework. Specifically, to better
utilize the plentiful negative samples and alleviate the zero-loss issue, we
strategically select relevant negative samples and design an attention-based
loss function to further differentiate the importance of each negative sample.
The intuition is that negative samples more similar to positive samples will
contribute more to the model. Further, we design a dynamic relation-aware
entity encoder for learning a context-dependent entity representation.
Experiments demonstrate that RANA outperforms the state-of-the-art models on
two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09525">
<div class="article-summary-box-inner">
<span><p>Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09539">
<div class="article-summary-box-inner">
<span><p>State space models (SSMs) have shown impressive results on tasks that require
modeling long-range dependencies and efficiently scale to long sequences owing
to their subquadratic runtime complexity. Originally designed for continuous
signals, SSMs have shown superior performance on a plethora of tasks, in vision
and audio; however, SSMs still lag Transformer performance in Language Modeling
tasks. In this work, we propose a hybrid layer named Block-State Transformer
(BST), that internally combines an SSM sublayer for long-range
contextualization, and a Block Transformer sublayer for short-term
representation of sequences. We study three different, and completely
parallelizable, variants that integrate SSMs and block-wise attention. We show
that our model outperforms similar Transformer-based architectures on language
modeling perplexity and generalizes to longer sequences. In addition, the
Block-State Transformer demonstrates more than tenfold increase in speed at the
layer level compared to the Block-Recurrent Transformer when model
parallelization is employed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts. (arXiv:2306.09544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09544">
<div class="article-summary-box-inner">
<span><p>This paper explores methods for extracting information from radiology reports
that generalize across exam modalities to reduce requirements for annotated
data. We demonstrate that multi-pass T5-based text-to-text generative models
exhibit better generalization across exam modalities compared to approaches
that employ BERT-based task-specific classification layers. We then develop
methods that reduce the inference cost of the model, making large-scale corpus
processing more feasible for clinical applications. Specifically, we introduce
a generative technique that decomposes complex tasks into smaller subtask
blocks, which improves a single-pass model when combined with multitask
training. In addition, we leverage target-domain contexts during inference to
enhance domain adaptation, enabling use of smaller models. Analyses offer
insights into the benefits of different cost reduction strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09562">
<div class="article-summary-box-inner">
<span><p>Scientific progress in NLP rests on the reproducibility of researchers'
claims. The *CL conferences created the NLP Reproducibility Checklist in 2020
to be completed by authors at submission to remind them of key information to
include. We provide the first analysis of the Checklist by examining 10,405
anonymous responses to it. First, we find evidence of an increase in reporting
of information on efficiency, validation performance, summary statistics, and
hyperparameters after the Checklist's introduction. Further, we show acceptance
rate grows for submissions with more Yes responses. We find that the 44% of
submissions that gather new data are 5% less likely to be accepted than those
that did not; the average reviewer-rated reproducibility of these submissions
is also 2% lower relative to the rest. We find that only 46% of submissions
claim to open-source their code, though submissions that do have 8% higher
reproducibility score relative to those that do not, the most for any item. We
discuss what can be inferred about the state of reproducibility in NLP, and
provide a set of recommendations for future conferences, including: a) allowing
submitting code and appendices one week after the deadline, and b) measuring
dataset reproducibility by a checklist of data collection practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09572">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effect of tokenizers on the downstream
performance of pretrained language models (PLMs) in scriptio continua languages
where no explicit spaces exist between words, using Japanese as a case study.
The tokenizer for such languages often consists of a morphological analyzer and
a subword tokenizer, requiring us to conduct a comprehensive study of all
possible pairs. However, previous studies lack this comprehensiveness. We
therefore train extensive sets of tokenizers, build a PLM using each, and
measure the downstream performance on a wide range of tasks. Our results
demonstrate that each downstream task has a different optimal morphological
analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather
than WordPiece as a subword tokenizer, regardless of the type of task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09594">
<div class="article-summary-box-inner">
<span><p>Traditional comparative learning sentence embedding directly uses the encoder
to extract sentence features, and then passes in the comparative loss function
for learning. However, this method pays too much attention to the sentence body
and ignores the influence of some words in the sentence on the sentence
semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive
learning framework based on conditional MLM. On the basis of traditional
contrastive learning, an additional auxiliary network is added to integrate
sentence embedding to perform MLM tasks, forcing sentence embedding to learn
more masked word information. Finally, when Bertbase was used as the
pretraining language model, we exceeded SimCSE by 0.55 percentage points on
average in textual similarity tasks, and when Robertabase was used as the
pretraining language model, we exceeded SimCSE by 0.3 percentage points on
average in textual similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09597">
<div class="article-summary-box-inner">
<span><p>Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a serious of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot scenarios on
a number of English and Chinese benchmark datasets. Experimental results show
that LLMs cannot achieve the best results compared to the state-of-the-art deep
and fine-tuning PLMs methods. Different from the human intuition, the
experiments demonstrated that LLMs cannot make satisfied clickbait detection
just by the headlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain. (arXiv:2306.09607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09607">
<div class="article-summary-box-inner">
<span><p>PhotoBook is a collaborative dialogue game where two players receive private,
partially-overlapping sets of images and resolve which images they have in
common. It presents machines with a great challenge to learn how people build
common ground around multimodal context to communicate effectively. Methods
developed in the literature, however, cannot be deployed to real gameplay since
they only tackle some subtasks of the game, and they require additional
reference chains inputs, whose extraction process is imperfect. Therefore, we
propose a reference chain-free listener model that directly addresses the
game's predictive task, i.e., deciding whether an image is shared with partner.
Our DeBERTa-based listener model reads the full dialogue, and utilizes
CLIPScore features to assess utterance-image relevance. We achieve &gt;77%
accuracy on unseen sets of images/game themes, outperforming baseline by &gt;17
points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets. (arXiv:2306.09631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09631">
<div class="article-summary-box-inner">
<span><p>High-quality data is essential for conversational recommendation systems and
serves as the cornerstone of the network architecture development and training
strategy design. Existing works contribute heavy human efforts to manually
labeling or designing and extending recommender dialogue templates. However,
they suffer from (i) the limited number of human annotators results in that
datasets can hardly capture rich and large-scale cases in the real world, (ii)
the limited experience and knowledge of annotators account for the
uninformative corpus and inappropriate recommendations. In this paper, we
propose a novel automatic dataset synthesis approach that can generate both
large-scale and high-quality recommendation dialogues through a data2text
generation process, where unstructured recommendation conversations are
generated from structured graphs based on user-item information from the real
world. In doing so, we comprehensively exploit: (i) rich personalized user
profiles from traditional recommendation datasets, (ii) rich external knowledge
from knowledge graphs, and (iii) the conversation ability contained in
human-to-human conversational recommendation datasets. Extensive experiments
validate the benefit brought by the automatically synthesized data under
low-resource scenarios and demonstrate the promising potential to facilitate
the development of a more effective conversational recommendation system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09642">
<div class="article-summary-box-inner">
<span><p>Given the dynamic nature of toxic language use, automated methods for
detecting toxic spans are likely to encounter distributional shift. To explore
this phenomenon, we evaluate three approaches for detecting toxic spans under
cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned
language models. Our findings indicate that a simple method using off-the-shelf
lexicons performs best in the cross-domain setup. The cross-domain error
analysis suggests that (1) rationale extraction methods are prone to false
negatives, while (2) language models, despite performing best for the in-domain
case, recall fewer explicitly toxic words than lexicons and are prone to
certain types of false positives. Our code is publicly available at:
https://github.com/sfschouten/toxic-cross-domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09649">
<div class="article-summary-box-inner">
<span><p>Multimodal interactions have been shown to be more flexible, efficient, and
adaptable for diverse users and tasks than traditional graphical interfaces.
However, existing multimodal development frameworks either do not handle the
complexity and compositionality of multimodal commands well or require
developers to write a substantial amount of code to support these multimodal
interactions. In this paper, we present ReactGenie, a programming framework
that uses a shared object-oriented state abstraction to support building
complex multimodal mobile applications. Having different modalities share the
same state abstraction allows developers using ReactGenie to seamlessly
integrate and compose these modalities to deliver multimodal interaction.
</p>
<p>ReactGenie is a natural extension to the existing workflow of building a
graphical app, like the workflow with React-Redux. Developers only have to add
a few annotations and examples to indicate how natural language is mapped to
the user-accessible functions in the program. ReactGenie automatically handles
the complex problem of understanding natural language by generating a parser
that leverages large language models.
</p>
<p>We evaluated the ReactGenie framework by using it to build three demo apps.
We evaluated the accuracy of the language parser using elicited commands from
crowd workers and evaluated the usability of the generated multimodal app with
16 participants. Our results show that ReactGenie can be used to build
versatile multimodal applications with highly accurate language parsers, and
the multimodal app can lower users' cognitive load and task completion time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Distillation for Pseudo-Relevance Feedback. (arXiv:2306.09657v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09657">
<div class="article-summary-box-inner">
<span><p>Model distillation has emerged as a prominent technique to improve neural
search models. To date, distillation taken an offline approach, wherein a new
neural model is trained to predict relevance scores between arbitrary queries
and documents. In this paper, we explore a departure from this offline
distillation strategy by investigating whether a model for a specific query can
be effectively distilled from neural re-ranking results (i.e., distilling in an
online setting). Indeed, we find that a lexical model distilled online can
reasonably replicate the re-ranking of a neural model. More importantly, these
models can be used as queries that execute efficiently on indexes. This second
retrieval stage can enrich the pool of documents for re-ranking by identifying
documents that were missed in the first retrieval stage. Empirically, we show
that this approach performs favourably when compared with established pseudo
relevance feedback techniques, dense retrieval methods, and sparse-dense
ensemble "hybrid" approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data. (arXiv:2306.09697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09697">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) aims to extract relations from sentences and
documents. Existing relation extraction models typically rely on supervised
machine learning. However, recent studies showed that many RE datasets are
incompletely annotated. This is known as the false negative problem in which
valid relations are falsely annotated as 'no_relation'. Models trained with
such data inevitably make similar mistakes during the inference stage.
Self-training has been proven effective in alleviating the false negative
problem. However, traditional self-training is vulnerable to confirmation bias
and exhibits poor performance in minority classes. To overcome this limitation,
we proposed a novel class-adaptive re-sampling self-training framework.
Specifically, we re-sampled the pseudo-labels for each class by precision and
recall scores. Our re-sampling strategy favored the pseudo-labels of classes
with high precision and low recall, which improved the overall recall without
significantly compromising precision. We conducted experiments on
document-level and biomedical relation extraction datasets, and the results
showed that our proposed self-training framework consistently outperforms
existing competitive methods on the Re-DocRED and ChemDisgene datasets when the
training data are incompletely annotated. Our code is released at
https://github.com/DAMO-NLP-SG/CAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09704">
<div class="article-summary-box-inner">
<span><p>Text readability assessment has gained significant attention from researchers
in various domains. However, the lack of exploration into corpus compatibility
poses a challenge as different research groups utilize different corpora. In
this study, we propose a novel evaluation framework, Cross-corpus text
Readability Compatibility Assessment (CRCA), to address this issue. The
framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES,
OSP, and RACE. Linguistic features, GloVe word vector representations, and
their fusion features were extracted. (2) Classification models: Machine
learning methods (XGBoost, SVM) and deep learning methods (BiLSTM,
Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and
NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with
OSP standing out as significantly different from other datasets. (2) An
adaptation effect among corpora, feature representations, and classification
methods. (3) Consistent outcomes across the three metrics, validating the
robustness of the compatibility assessment framework. The outcomes of this
study offer valuable insights into corpus selection, feature representation,
and classification methods, and it can also serve as a beginning effort for
cross-corpus transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09705">
<div class="article-summary-box-inner">
<span><p>Anticipating audience reaction towards a certain text is integral to several
facets of society ranging from politics, research, and commercial industries.
Sentiment analysis (SA) is a useful natural language processing (NLP) technique
that utilizes lexical/statistical and deep learning methods to determine
whether different-sized texts exhibit positive, negative, or neutral emotions.
Recurrent networks are widely used in machine-learning communities for problems
with sequential data. However, a drawback of models based on Long-Short Term
Memory networks and Gated Recurrent Units is the significantly high number of
parameters, and thus, such models are computationally expensive. This drawback
is even more significant when the available data are limited. Also, such models
require significant over-parameterization and regularization to achieve optimal
performance. Tensorized models represent a potential solution. In this paper,
we classify the sentiment of some social media posts. We compare traditional
recurrent models with their tensorized version, and we show that with the
tensorized models, we reach comparable performances with respect to the
traditional models while using fewer resources for the training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09712">
<div class="article-summary-box-inner">
<span><p>In reinforcement learning (RL), there are two major settings for interacting
with the environment: online and offline. Online methods explore the
environment at significant time cost, and offline methods efficiently obtain
reward signals by sacrificing exploration capability. We propose semi-offline
RL, a novel paradigm that smoothly transits from offline to online settings,
balances exploration capability and training cost, and provides a theoretical
foundation for comparing different RL settings. Based on the semi-offline
formulation, we present the RL setting that is optimal in terms of optimization
cost, asymptotic error, and overfitting error bound. Extensive experiments show
that our semi-offline approach is efficient and yields comparable or often
better performance compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09719">
<div class="article-summary-box-inner">
<span><p>Despite the success of ChatGPT, its performances on most NLP tasks are still
well below the supervised baselines. In this work, we looked into the causes,
and discovered that its subpar performance was caused by the following factors:
(1) token limit in the prompt does not allow for the full utilization of the
supervised datasets; (2) mismatch between the generation nature of ChatGPT and
NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly
focus on certain keywords, etc.
</p>
<p>In this work, we propose a collection of general modules to address these
issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed
modules include (1) a one-input-multiple-prompts strategy that employs multiple
prompts for one input to accommodate more demonstrations; (2) using fine-tuned
models for better demonstration retrieval; (3) transforming tasks to formats
that are more tailored to the generation nature; (4) employing reasoning
strategies that are tailored to addressing the task-specific complexity; (5)
the self-verification strategy to address the hallucination issue of LLMs; (6)
the paraphrase strategy to improve the robustness of model predictions.
</p>
<p>We conduct experiments on 21 datasets of 10 representative NLP tasks,
including question answering, commonsense reasoning, natural language
inference, sentiment analysis, named entity recognition, entity-relation
extraction, event extraction, dependency parsing, semantic role labeling, and
part-of-speech tagging. Using the proposed assemble of techniques, we are able
to significantly boost the performance of ChatGPT on the selected NLP tasks,
achieving performances comparable to or better than supervised baselines, or
even existing SOTA performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Representation Structure Parsing for Chinese. (arXiv:2306.09725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09725">
<div class="article-summary-box-inner">
<span><p>Previous work has predominantly focused on monolingual English semantic
parsing. We, instead, explore the feasibility of Chinese semantic parsing in
the absence of labeled data for Chinese meaning representations. We describe
the pipeline of automatically collecting the linearized Chinese meaning
representation data for sequential-to sequential neural networks. We further
propose a test suite designed explicitly for Chinese semantic parsing, which
provides fine-grained evaluation for parsing performance, where we aim to study
Chinese parsing difficulties. Our experimental results show that the difficulty
of Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese
parsing through machine translation and an English parser yields slightly lower
performance than training a model directly on Chinese data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation. (arXiv:2306.09737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09737">
<div class="article-summary-box-inner">
<span><p>The fast-growing number of research articles makes it problematic for
scholars to keep track of the new findings related to their areas of expertise.
Furthermore, linking knowledge across disciplines in rapidly developing fields
becomes challenging for complex topics like climate change that demand
interdisciplinary solutions. At the same time, the rise of Black Box types of
text summarization makes it difficult to understand how text relationships are
built, let alone relate to existing theories conceptualizing cause-effect
relationships and permitting hypothesizing. This work aims to sensibly use
Natural Language Processing by extracting variables relations and synthesizing
their findings using networks while relating to key concepts dominant in
relevant disciplines. As an example, we apply our methodology to the analysis
of farmers' adaptation to climate change. For this, we perform a Natural
Language Processing analysis of publications returned by Scopus in August 2022.
Results show that the use of Natural Language Processing together with networks
in a descriptive manner offers a fast and interpretable way to synthesize
literature review findings as long as researchers back up results with theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09752">
<div class="article-summary-box-inner">
<span><p>In efforts to keep up with the rapid progress and use of large language
models, gender bias research is becoming more prevalent in NLP. Non-English
bias research, however, is still in its infancy with most work focusing on
English. In our work, we study how grammatical gender bias relating to
politeness levels manifests in Japanese and Korean language models. Linguistic
studies in these languages have identified a connection between gender bias and
politeness levels, however it is not yet known if language models reproduce
these biases. We analyze relative prediction probabilities of the male and
female grammatical genders using templates and find that informal polite speech
is most indicative of the female grammatical gender, while rude and formal
speech is most indicative of the male grammatical gender. Further, we find
politeness levels to be an attack vector for allocational gender bias in
cyberbullying detection models. Cyberbullies can evade detection through simple
techniques abusing politeness levels. We introduce an attack dataset to (i)
identify representational gender bias across politeness levels, (ii)
demonstrate how gender biases can be abused to bypass cyberbullying detection
models and (iii) show that allocational biases can be mitigated via training on
our proposed dataset. Through our findings we highlight the importance of bias
research moving beyond its current English-centrism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09782">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09802">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) is a task that identifies relationships between
entities in a text, enabling the acquisition of relational facts and bridging
the gap between natural language and structured knowledge. However, current RE
models often rely on small datasets with low coverage of relation types,
particularly when working with languages other than English. In this paper, we
address the above issue and provide two new resources that enable the training
and evaluation of multilingual RE systems. First, we present SRED$^{\rm FM}$,
an automatically annotated dataset covering 18 languages, 400 relation types,
13 entity types, totaling more than 40 million triplet instances. Second, we
propose RED$^{\rm FM}$, a smaller, human-revised dataset for seven languages
that allows for the evaluation of multilingual RE systems. To demonstrate the
utility of these novel datasets, we experiment with the first end-to-end
multilingual RE model, mREBEL, that extracts triplets, including entity types,
in multiple languages. We release our resources and model checkpoints at
https://www.github.com/babelscape/rebel
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody. (arXiv:2306.09814v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09814">
<div class="article-summary-box-inner">
<span><p>This paper investigates the use of word surprisal, a measure of the
predictability of a word in a given context, as a feature to aid speech
synthesis prosody. We explore how word surprisal extracted from large language
models (LLMs) correlates with word prominence, a signal-based measure of the
salience of a word in a given discourse. We also examine how context length and
LLM size affect the results, and how a speech synthesizer conditioned with
surprisal values compares with a baseline system. To evaluate these factors, we
conducted experiments using a large corpus of English text and LLMs of varying
sizes. Our results show that word surprisal and word prominence are moderately
correlated, suggesting that they capture related but distinct aspects of
language use. We find that length of context and size of the LLM impact the
correlations, but not in the direction anticipated, with longer contexts and
larger LLMs generally underpredicting prominent words in a nearly linear
manner. We demonstrate that, in line with these findings, a speech synthesizer
conditioned with surprisal values provides a minimal improvement over the
baseline with the results suggesting a limited effect of using surprisal values
for eliciting appropriate prominence patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09821">
<div class="article-summary-box-inner">
<span><p>Dialogue systems and large language models (LLMs) have gained considerable
attention. However, the direct utilization of LLMs as task-oriented dialogue
(TOD) models has been found to underperform compared to smaller task-specific
models. Nonetheless, it is crucial to acknowledge the significant potential of
LLMs and explore improved approaches for leveraging their impressive abilities.
Motivated by the goal of leveraging LLMs, we propose an alternative approach
called User-Guided Response Optimization (UGRO) to combine it with a smaller
TOD model. This approach uses LLM as annotation-free user simulator to assess
dialogue responses, combining them with smaller fine-tuned end-to-end TOD
models. By utilizing the satisfaction feedback generated by LLMs, UGRO further
optimizes the supervised fine-tuned TOD model. Specifically, the TOD model
takes the dialogue history as input and, with the assistance of the user
simulator's feedback, generates high-satisfaction responses that meet the
user's requirements. Through empirical experiments on two TOD benchmarks, we
validate the effectiveness of our method. The results demonstrate that our
approach outperforms previous state-of-the-art (SOTA) results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Process Knowledge-infused Learning for Clinician-friendly Explanations. (arXiv:2306.09824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09824">
<div class="article-summary-box-inner">
<span><p>Language models have the potential to assess mental health using social media
data. By analyzing online posts and conversations, these models can detect
patterns indicating mental health conditions like depression, anxiety, or
suicidal thoughts. They examine keywords, language markers, and sentiment to
gain insights into an individual's mental well-being. This information is
crucial for early detection, intervention, and support, improving mental health
care and prevention strategies. However, using language models for mental
health assessments from social media has two limitations: (1) They do not
compare posts against clinicians' diagnostic processes, and (2) It's
challenging to explain language model outputs using concepts that the clinician
can understand, i.e., clinician-friendly explanations. In this study, we
introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm
that layers clinical process knowledge structures on language model outputs,
enabling clinician-friendly explanations of the underlying language model
predictions. We rigorously test our methods on existing benchmark datasets,
augmented with such clinical process knowledge, and release a new dataset for
assessing suicidality. PK-iL performs competitively, achieving a 70% agreement
with users, while other XAI methods only achieve 47% agreement (average
inter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL
effectively explains model predictions to clinicians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages. (arXiv:2306.09830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09830">
<div class="article-summary-box-inner">
<span><p>In this paper we describe the University of Sheffield's submission to the
AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages
which comprises the translation from Spanish to eleven indigenous languages.
Our approach consists of extending, training, and ensembling different
variations of NLLB-200. We use data provided by the organizers and data from
various other sources such as constitutions, handbooks, news articles, and
backtranslations generated from monolingual data. On the dev set, our best
submission outperforms the baseline by 11% average chrF across all languages,
with substantial improvements particularly for Aymara, Guarani and Quechua. On
the test set, we achieve the highest average chrF of all the submissions, we
rank first in four of the eleven languages, and at least one of our submissions
ranks in the top 3 for all languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09841">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved great success in various natural
language tasks. It has aroused much interest in evaluating the specific
reasoning capability of LLMs, such as multilingual reasoning and mathematical
reasoning. However, as one of the key reasoning perspectives, logical reasoning
capability has not yet been thoroughly evaluated. In this work, we aim to
bridge those gaps and provide comprehensive evaluations. Firstly, to offer
systematic evaluations, this paper selects fifteen typical logical reasoning
datasets and organizes them into deductive, inductive, abductive and mixed-form
reasoning settings. Considering the comprehensiveness of evaluations, we
include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD)
and evaluate them on all selected datasets under zero-shot, one-shot and
three-shot settings. Secondly, different from previous evaluations relying only
on simple metrics (e.g., accuracy), we propose fine-level evaluations from
objective and subjective manners, covering both answers and explanations. Also,
to uncover the logical flaws of LLMs, bad cases will be attributed to five
error types from two dimensions. Thirdly, to avoid the influences of knowledge
bias and purely focus on benchmarking the logical reasoning capability of LLMs,
we propose a new dataset with neutral content. It contains 3K samples and
covers deductive, inductive and abductive reasoning settings. Based on the
in-depth evaluations, this paper finally concludes the ability maps of logical
reasoning capability from six dimensions (i.e., correct, rigorous, self-aware,
active, oriented and no hallucination). It reflects the pros and cons of LLMs
and gives guiding directions for future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09869">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes. (arXiv:2306.09877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09877">
<div class="article-summary-box-inner">
<span><p>We aimed to investigate the impact of social circumstances on cancer therapy
selection using natural language processing to derive insights from social
worker documentation. We developed and employed a Bidirectional Encoder
Representations from Transformers (BERT) based approach, using a hierarchical
multi-step BERT model (BERT-MS) to predict the prescription of targeted cancer
therapy to patients based solely on documentation by clinical social workers.
Our corpus included free-text clinical social work notes, combined with
medication prescription information, for all patients treated for breast
cancer. We conducted a feature importance analysis to pinpoint the specific
social circumstances that impact cancer therapy selection. Using only social
work notes, we consistently predicted the administration of targeted therapies,
suggesting systematic differences in treatment selection exist due to
non-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF,
outperformed other publicly available language models with an AUROC of 0.675
and a Macro F1 score of 0.599. The UCSF BERT-MS model, capable of leveraging
multiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and
Macro-F1. Our feature importance analysis identified several clinically
intuitive social determinants of health (SDOH) that potentially contribute to
disparities in treatment. Our findings indicate that significant disparities
exist among breast cancer patients receiving different types of therapies based
on social determinants of health. Social work reports play a crucial role in
understanding these disparities in clinical decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09896">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown remarkable aptitude in code
generation but still struggle on challenging programming tasks. Self-repair --
in which the model debugs and fixes mistakes in its own code -- has recently
become a popular way to boost performance in these settings. However, only very
limited studies on how and when self-repair works effectively exist in the
literature, and one might wonder to what extent a model is really capable of
providing accurate feedback on why the code is wrong when that code was
generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's
ability to perform self-repair on APPS, a challenging dataset consisting of
diverse coding challenges. To do so, we first establish a new evaluation
strategy dubbed pass@t that measures the pass rate of the tasks against the
total number of tokens sampled from the model, enabling a fair comparison to
purely sampling-based approaches. With this evaluation strategy, we find that
the effectiveness of self-repair is only seen in GPT-4. We also observe that
self-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback
on the programs generated by GPT-3.5 and using expert human programmers to give
feedback on the programs generated by GPT-4, we unlock significant performance
gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference. (arXiv:2306.09918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09918">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) has been a cornerstone task in evaluating
language models' inferential reasoning capabilities. However, the standard
three-way classification scheme used in NLI has well-known shortcomings in
evaluating models' ability to capture the nuances of natural human reasoning.
In this paper, we argue that the operationalization of the neutral label in
current NLI datasets has low validity, is interpreted inconsistently, and that
at least one important sense of neutrality is often ignored. We uncover the
detrimental impact of these shortcomings, which in some cases leads to
annotation datasets that actually decrease performance on downstream tasks. We
compare approaches of handling annotator disagreement and identify flaws in a
recent NLI dataset that designs an annotator study based on a problematic
operationalization. Our findings highlight the need for a more refined
evaluation framework for NLI, and we hope to spark further discussion and
action in the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09922">
<div class="article-summary-box-inner">
<span><p>When robots perform long action sequences, users will want to easily and
reliably find out what they have done. We therefore demonstrate the task of
learning to summarize and answer questions about a robot agent's past actions
using natural language alone. A single system with a large language model at
its core is trained to both summarize and answer questions about action
sequences given ego-centric video frames of a virtual robot and a question
prompt. To enable training of question answering, we develop a method to
automatically generate English-language questions and answers about objects,
actions, and the temporal order in which actions occurred during episodes of
robot action in the virtual environment. Training one model to both summarize
and answer questions enables zero-shot transfer of representations of objects
learned through question answering to improved action summarization. %
involving objects not seen in training to summarize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09927">
<div class="article-summary-box-inner">
<span><p>Attention-based neural networks such as transformers have demonstrated a
remarkable ability to exhibit in-context learning (ICL): Given a short prompt
sequence of tokens from an unseen task, they can formulate relevant per-token
and next-token predictions without any parameter updates. By embedding a
sequence of labeled training data and unlabeled test data as a prompt, this
allows for transformers to behave like supervised learning algorithms. Indeed,
recent work has shown that when training transformer architectures over random
instances of linear regression problems, these models' predictions mimic those
of ordinary least squares.
</p>
<p>Towards understanding the mechanisms underlying this phenomenon, we
investigate the dynamics of ICL in transformers with a single linear
self-attention layer trained by gradient flow on linear regression tasks. We
show that despite non-convexity, gradient flow with a suitable random
initialization finds a global minimum of the objective function. At this global
minimum, when given a test prompt of labeled examples from a new prediction
task, the transformer achieves prediction error competitive with the best
linear predictor over the test prompt distribution. We additionally
characterize the robustness of the trained transformer to a variety of
distribution shifts and show that although a number of shifts are tolerated,
shifts in the covariate distribution of the prompts are not. Motivated by this,
we consider a generalized ICL setting where the covariate distributions can
vary across prompts. We show that although gradient flow succeeds at finding a
global minimum in this setting, the trained transformer is still brittle under
mild covariate shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation. (arXiv:2306.09968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09968">
<div class="article-summary-box-inner">
<span><p>Large language models have exhibited exceptional performance on various
Natural Language Processing (NLP) tasks, leveraging techniques such as the
pre-training, and instruction fine-tuning. Despite these advances, their
effectiveness in medical applications is limited, due to challenges such as
factual inaccuracies, reasoning abilities, and lack grounding in real-world
experience. In this study, we present ClinicalGPT, a language model explicitly
designed and optimized for clinical scenarios. By incorporating extensive and
diverse real-world data, such as medical records, domain-specific knowledge,
and multi-round dialogue consultations in the training process, ClinicalGPT is
better prepared to handle multiple clinical task. Furthermore, we introduce a
comprehensive evaluation framework that includes medical knowledge
question-answering, medical exams, patient consultations, and diagnostic
analysis of medical records. Our results demonstrate that ClinicalGPT
significantly outperforms other models in these tasks, highlighting the
effectiveness of our approach in adapting large language models to the critical
domain of healthcare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09996">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) is a challenging task that requires the
ability to comprehend and reason with visual information. While recent
vision-language models have made strides, they continue to struggle with
zero-shot VQA, particularly in handling complex compositional questions and
adapting to new domains i.e. knowledge-based reasoning. This paper explores the
use of various prompting strategies, focusing on the BLIP2 model, to enhance
zero-shot VQA performance. We conduct a comprehensive investigation across
several VQA datasets, examining the effectiveness of different question
templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT)
reasoning, and the benefits of incorporating image captions as additional
visual cues. Despite the varied outcomes, our findings demonstrate that
carefully designed question templates and the integration of additional visual
cues, like image captions, can contribute to improved VQA performance,
especially when used in conjunction with few-shot examples. However, we also
identify a limitation in the use of chain-of-thought rationalization, which
negatively affects VQA accuracy. Our study thus provides critical insights into
the potential of prompting for improving zero-shot VQA performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10012">
<div class="article-summary-box-inner">
<span><p>Text-guided image editing is widely needed in daily life, ranging from
personal use to professional applications such as Photoshop. However, existing
methods are either zero-shot or trained on an automatically synthesized
dataset, which contains a high volume of noise. Thus, they still require lots
of manual tuning to produce desirable outcomes in practice. To address this
issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),
the first large-scale, manually annotated dataset for instruction-guided real
image editing that covers diverse scenarios: single-turn, multi-turn,
mask-provided, and mask-free editing. MagicBrush comprises over 10K manually
annotated triples (source image, instruction, target image), which supports
trainining large-scale text-guided image editing models. We fine-tune
InstructPix2Pix on MagicBrush and show that the new model can produce much
better images according to human evaluation. We further conduct extensive
experiments to evaluate current image editing baselines from multiple
dimensions including quantitative, qualitative, and human evaluations. The
results reveal the challenging nature of our dataset and the gap between
current baselines and real-world editing needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10015">
<div class="article-summary-box-inner">
<span><p>Language model training in distributed settings is limited by the
communication cost of gradient exchanges. In this short note, we extend recent
work from Malladi et al. (2023), using shared randomness to perform distributed
fine-tuning with low bandwidth. The method is a natural decentralized extension
of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).
Each iteration, each machine seeds a Random Number Generator (RNG) to perform
local reproducible perturbations on model weights and calculate and exchange
scalar projected gradients, which are then used to update each model. By using
a (machine, sample) identifier as the random seed, each model can regenerate
one another's perturbations. As machines only exchange single-byte projected
gradients, this is highly communication efficient. There are also potential
privacy benefits, as projected gradients may be calculated on different
training data, and models never access the other's data. Our approach not only
drastically reduces communication bandwidth requirements but also accommodates
dynamic addition or removal of machines during the training process and retains
the memory-efficient and inference-only advantages of recent work. We perform
proof-of-concept experiments to demonstrate the potential usefulness of this
method, building off of rich literature on distributed optimization and
memory-efficient training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12696">
<div class="article-summary-box-inner">
<span><p>The DocRED dataset is one of the most popular and widely used benchmarks for
document-level relation extraction (RE). It adopts a recommend-revise
annotation scheme so as to have a large-scale annotated dataset. However, we
find that the annotation of DocRED is incomplete, i.e., false negative samples
are prevalent. We analyze the causes and effects of the overwhelming false
negative problem in the DocRED dataset. To address the shortcoming, we
re-annotate 4,053 documents in the DocRED dataset by adding the missed relation
triples back to the original DocRED. We name our revised DocRED dataset
Re-DocRED. We conduct extensive experiments with state-of-the-art neural models
on both datasets, and the experimental results show that the models trained and
evaluated on our Re-DocRED achieve performance improvements of around 13 F1
points. Moreover, we conduct a comprehensive analysis to identify the potential
areas for further improvement. Our dataset is publicly available at
https://github.com/tonytan48/Re-DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04039">
<div class="article-summary-box-inner">
<span><p>In recent years, machine learning (ML) has relied heavily on crowdworkers
both for building datasets and for addressing research questions requiring
human interaction or judgment. The diverse tasks performed and uses of the data
produced render it difficult to determine when crowdworkers are best thought of
as workers (versus human subjects). These difficulties are compounded by
conflicting policies, with some institutions and researchers regarding all ML
crowdworkers as human subjects and others holding that they rarely constitute
human subjects. Notably few ML papers involving crowdwork mention IRB
oversight, raising the prospect of non-compliance with ethical and regulatory
requirements. We investigate the appropriate designation of ML crowdsourcing
studies, focusing our inquiry on natural language processing to expose unique
challenges for research oversight. Crucially, under the U.S. Common Rule, these
judgments hinge on determinations of aboutness, concerning both whom (or what)
the collected data is about and whom (or what) the analysis is about. We
highlight two challenges posed by ML: the same set of workers can serve
multiple roles and provide many sorts of information; and ML research tends to
embrace a dynamic workflow, where research questions are seldom stated ex ante
and data sharing opens the door for future studies to aim questions at
different targets. Our analysis exposes a potential loophole in the Common
Rule, where researchers can elude research ethics oversight by splitting data
collection and analysis into distinct studies. Finally, we offer several policy
recommendations to address these concerns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03755">
<div class="article-summary-box-inner">
<span><p>Mis- and disinformation are a substantial global threat to our security and
safety. To cope with the scale of online misinformation, researchers have been
working on automating fact-checking by retrieving and verifying against
relevant evidence. However, despite many advances, a comprehensive evaluation
of the possible attack vectors against such systems is still lacking.
Particularly, the automated fact-verification process might be vulnerable to
the exact disinformation campaigns it is trying to combat. In this work, we
assume an adversary that automatically tampers with the online evidence in
order to disrupt the fact-checking model via camouflaging the relevant evidence
or planting a misleading one. We first propose an exploratory taxonomy that
spans these two targets and the different threat model dimensions. Guided by
this, we design and propose several potential attack methods. We show that it
is possible to subtly modify claim-salient snippets in the evidence and
generate diverse and claim-aligned evidence. Thus, we highly degrade the
fact-checking performance under many different permutations of the taxonomy's
dimensions. The attacks are also robust against post-hoc modifications of the
claim. Our analysis further hints at potential limitations in models' inference
when faced with contradicting evidence. We emphasize that these attacks can
have harmful implications on the inspectable and human-in-the-loop usage
scenarios of such models, and we conclude by discussing challenges and
directions for future defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03347">
<div class="article-summary-box-inner">
<span><p>Visually-situated language is ubiquitous -- sources range from textbooks with
diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on
domain-specific recipes with limited sharing of the underlying data, model
architectures, and objectives. We present Pix2Struct, a pretrained
image-to-text model for purely visual language understanding, which can be
finetuned on tasks containing visually-situated language. Pix2Struct is
pretrained by learning to parse masked screenshots of web pages into simplified
HTML. The web, with its richness of visual elements cleanly reflected in the
HTML structure, provides a large source of pretraining data well suited to the
diversity of downstream tasks. Intuitively, this objective subsumes common
pretraining signals such as OCR, language modeling, image captioning. In
addition to the novel pretraining strategy, we introduce a variable-resolution
input representation and a more flexible integration of language and vision
inputs, where language prompts such as questions are rendered directly on top
of the input image. For the first time, we show that a single pretrained model
can achieve state-of-the-art results in six out of nine tasks across four
domains: documents, illustrations, user interfaces, and natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks. (arXiv:2212.10525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10525">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) tasks have been studied for many decades
in the speech research community, but have not received as much attention as
lower-level tasks like speech and speaker recognition. In particular, there are
not nearly as many SLU task benchmarks, and many of the existing ones use data
that is not freely available to all researchers. Recent work has begun to
introduce such benchmark datasets for several tasks. In this work, we introduce
several new annotated SLU benchmark tasks based on freely available speech
data, which complement existing benchmarks and address gaps in the SLU
evaluation landscape. We contribute four tasks: question answering and
summarization involve inference over longer speech sequences; named entity
localization addresses the speech-specific task of locating the targeted
content in the signal; dialog act classification identifies the function of a
given speech utterance. We follow the blueprint of the Spoken Language
Understanding Evaluation (SLUE) benchmark suite. In order to facilitate the
development of SLU models that leverage the success of pre-trained speech
representations, we will be publishing for each task (i) annotations for a
relatively small fine-tuning set, (ii) annotated development and test sets, and
(iii) baseline models for easy reproducibility and comparisons. In this work,
we present the details of data collection and annotation and the performance of
the baseline models. We also perform sensitivity analysis of pipeline models'
performance (speech recognizer + text model) to the speech recognition
accuracy, using more than 20 state-of-the-art speech recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00944">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been shown to be vulnerable to small perturbations
of their inputs, known as adversarial attacks. In this paper, we investigate
the vulnerability of Neural Machine Translation (NMT) models to adversarial
attacks and propose a new attack algorithm called TransFool. To fool NMT
models, TransFool builds on a multi-term optimization problem and a gradient
projection step. By integrating the embedding representation of a language
model, we generate fluent adversarial examples in the source language that
maintain a high level of semantic similarity with the clean samples.
Experimental results demonstrate that, for different translation tasks and NMT
architectures, our white-box attack can severely degrade the translation
quality while the semantic similarity between the original and the adversarial
sentences stays high. Moreover, we show that TransFool is transferable to
unknown target models. Finally, based on automatic and human evaluations,
TransFool leads to improvement in terms of success rate, semantic similarity,
and fluency compared to the existing attacks both in white-box and black-box
settings. Thus, TransFool permits us to better characterize the vulnerability
of NMT models and outlines the necessity to design strong defense mechanisms
and more robust NMT systems for real-life applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.03363">
<div class="article-summary-box-inner">
<span><p>Activity and property prediction models are the central workhorses in drug
discovery and materials sciences, but currently they have to be trained or
fine-tuned for new tasks. Without training or fine-tuning, scientific language
models could be used for such low-data tasks through their announced zero- and
few-shot capabilities. However, their predictive quality at activity prediction
is lacking. In this work, we envision a novel type of activity prediction model
that is able to adapt to new prediction tasks at inference time, via
understanding textual information describing the task. To this end, we propose
a new architecture with separate modules for chemical and natural language
inputs, and a contrastive pre-training objective on data from large biochemical
databases. In extensive experiments, we show that our method CLAMP yields
improved predictive performance on few-shot learning benchmarks and zero-shot
problems in drug discovery. We attribute the advances of our method to the
modularized architecture and to our pre-training objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03394">
<div class="article-summary-box-inner">
<span><p>Student opinions for a course are important to educators and administrators,
regardless of the type of the course or the institution. Reading and manually
analyzing open-ended feedback becomes infeasible for massive volumes of
comments at institution level or online forums. In this paper, we collected and
pre-processed a large number of course reviews publicly available online. We
applied machine learning techniques with the goal to gain insight into student
sentiments and topics. Specifically, we utilized current Natural Language
Processing (NLP) techniques, such as word embeddings and deep neural networks,
and state-of-the-art BERT (Bidirectional Encoder Representations from
Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet
(Generalized Auto-regression Pre-training). We performed extensive
experimentation to compare these techniques versus traditional approaches. This
comparative study demonstrates how to apply modern machine learning approaches
for sentiment polarity extraction and topic-based classification utilizing
course feedback. For sentiment polarity, the top model was RoBERTa with 95.5%
accuracy and 84.7% F1-macro, while for topic classification, an SVM (Support
Vector Machine) was the top classifier with 79.8% accuracy and 80.6% F1-macro.
We also provided an in-depth exploration of the effect of certain
hyperparameters on the model performance and discussed our observations. These
findings can be used by institutions and course providers as a guide for
analyzing their own course feedback using NLP models towards self-evaluation
and improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04187">
<div class="article-summary-box-inner">
<span><p>The standard paradigm for fake news detection mainly utilizes text
information to model the truthfulness of news. However, the discourse of online
fake news is typically subtle and it requires expert knowledge to use textual
information to debunk fake news. Recently, studies focusing on multimodal fake
news detection have outperformed text-only methods. Recent approaches utilizing
the pre-trained model to extract unimodal features, or fine-tuning the
pre-trained model directly, have become a new paradigm for detecting fake news.
Again, this paradigm either requires a large number of training instances, or
updates the entire set of pre-trained model parameters, making real-world fake
news detection impractical. Furthermore, traditional multimodal methods fuse
the cross-modal features directly without considering that the uncorrelated
semantic representation might inject noise into the multimodal features. This
paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)
framework. First, we incorporate prompt learning into multimodal fake news
detection. Prompt learning, which only tunes prompts with a frozen language
model, can reduce memory usage significantly and achieve comparable
performances, compared with fine-tuning. We analyse three prompt templates with
a soft verbalizer to detect fake news. In addition, we introduce the
similarity-aware fusing method to adaptively fuse the intensity of multimodal
representation and mitigate the noise injection via uncorrelated cross-modal
features. For evaluation, SAMPLE surpasses the F1 and the accuracies of
previous works on two benchmark multimodal datasets, demonstrating the
effectiveness of the proposed method in detecting fake news. In addition,
SAMPLE also is superior to other approaches regardless of few-shot and
data-rich settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"When Words Fail, Emojis Prevail": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity. (arXiv:2305.04105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04105">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a form of figurative language that serves as a humorous tool for
mockery and ridicule. We present a novel architecture for sarcasm generation
with emoji from a non-sarcastic input sentence in English. We divide the
generation task into two sub tasks: one for generating textual sarcasm and
another for collecting emojis associated with those sarcastic sentences. Two
key elements of sarcasm are incorporated into the textual sarcasm generation
task: valence reversal and semantic incongruity with context, where the context
may involve shared commonsense or general knowledge between the speaker and
their audience. The majority of existing sarcasm generation works have focused
on this textual form. However, in the real world, when written texts fall short
of effectively capturing the emotional cues of spoken and face-to-face
communication, people often opt for emojis to accurately express their
emotions. Due to the wide range of applications of emojis, incorporating
appropriate emojis to generate textual sarcastic sentences helps advance
sarcasm generation. We conclude our study by evaluating the generated sarcastic
sentences using human judgement. All the codes and data used in this study has
been made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17680">
<div class="article-summary-box-inner">
<span><p>Recent research has focused on using large language models (LLMs) to generate
explanations for hate speech through fine-tuning or prompting. Despite the
growing interest in this area, these generated explanations' effectiveness and
potential limitations remain poorly understood. A key concern is that these
explanations, generated by LLMs, may lead to erroneous judgments about the
nature of flagged content by both users and content moderators. For instance,
an LLM-generated explanation might inaccurately convince a content moderator
that a benign piece of content is hateful. In light of this, we propose an
analytical framework for examining hate speech explanations and conducted an
extensive survey on evaluating such explanations. Specifically, we prompted
GPT-3 to generate explanations for both hateful and non-hateful content, and a
survey was conducted with 2,400 unique respondents to evaluate the generated
explanations. Our findings reveal that (1) human evaluators rated the
GPT-generated explanations as high quality in terms of linguistic fluency,
informativeness, persuasiveness, and logical soundness, (2) the persuasive
nature of these explanations, however, varied depending on the prompting
strategy employed, and (3) this persuasiveness may result in incorrect
judgments about the hatefulness of the content. Our study underscores the need
for caution in applying LLM-generated explanations for content moderation. Code
and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07567">
<div class="article-summary-box-inner">
<span><p>When using adversarial training, it is common practice to train against the
most egregious failures. However, this might imply using examples with
sensitive information (such as leaked passwords or security vulnerabilities) as
training data. One might assume that language models trained with gradient
descent never generate text snippets which were only present in examples
associated with the lowest possible reward. In this paper, we show that this
assumption is wrong: in some situations, large language models do learn from
such negatively-reinforced examples. We present a specific training setup that
enables Pythia-160M to guess passwords 13% more often than it would by guessing
randomly, despite only showing it these passwords on examples where the model
is incentivized to not output these passwords. Our code is available at
www.github.com/FabienRoger/Learning-From-Negative-Examples
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08161">
<div class="article-summary-box-inner">
<span><p>Applications built on top of Large Language Models (LLMs) such as GPT-4
represent a revolution in AI due to their human-level capabilities in natural
language processing. However, they also pose many significant risks such as the
presence of biased, private, or harmful text, and the unauthorized inclusion of
copyrighted material.
</p>
<p>We introduce h2oGPT, a suite of open-source code repositories for the
creation and use of LLMs based on Generative Pretrained Transformers (GPTs).
The goal of this project is to create the world's best truly open-source
alternative to closed-source approaches. In collaboration with and as part of
the incredible and unstoppable open-source community, we open-source several
fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial
use under fully permissive Apache 2.0 licenses. Included in our release is
100\% private document search using natural language.
</p>
<p>Open-source language models help boost AI development and make it more
accessible and trustworthy. They lower entry hurdles, allowing people and
groups to tailor these models to their needs. This openness increases
innovation, transparency, and fairness. An open-source strategy is needed to
share AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-19 23:11:39.226384598 UTC">2023-06-19 23:11:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>