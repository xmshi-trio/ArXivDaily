<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-01T01:30:00Z">11-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Radically Lower Data-Labeling Costs for Visually Rich Document Extraction Models. (arXiv:2210.16391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16391">
<div class="article-summary-box-inner">
<span><p>A key bottleneck in building automatic extraction models for visually rich
documents like invoices is the cost of acquiring the several thousand
high-quality labeled documents that are needed to train a model with acceptable
accuracy. We propose Selective Labeling to simplify the labeling task to
provide "yes/no" labels for candidate extractions predicted by a model trained
on partially labeled documents. We combine this with a custom active learning
strategy to find the predictions that the model is most uncertain about. We
show through experiments on document types drawn from 3 different domains that
selective labeling can reduce the cost of acquiring labeled data by $10\times$
with a negligible loss in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">System Demo: Tool and Infrastructure for Offensive Language Error Analysis (OLEA) in English. (arXiv:2210.16398v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16398">
<div class="article-summary-box-inner">
<span><p>The automatic detection of offensive language is a pressing societal need.
Many systems perform well on explicit offensive language but struggle to detect
more complex, nuanced, or implicit cases of offensive and hateful language.
OLEA is an open-source Python library that provides easy-to-use tools for error
analysis in the context of detecting offensive language in English. OLEA also
provides an infrastructure for re-distribution of new datasets and analysis
methods requiring very little coding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE. (arXiv:2210.16407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16407">
<div class="article-summary-box-inner">
<span><p>Figurative language (e.g., "he flew like the wind") is challenging to
understand, as it is hard to tell what implicit information is being conveyed
from the surface form alone. We hypothesize that to perform this task well, the
reader needs to mentally elaborate the scene being described to identify a
sensible meaning of the language. We present DREAM-FLUTE, a figurative language
understanding system that does this, first forming a "mental model" of
situations described in a premise and hypothesis before making an
entailment/contradiction decision and generating an explanation. DREAM-FLUTE
uses an existing scene elaboration model, DREAM, for constructing its "mental
model." In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint)
first place (Acc@60=63.3%), and can perform even better with ensemble
techniques, demonstrating the effectiveness of this approach. More generally,
this work suggests that adding a reflective component to pretrained language
models can improve their performance beyond standard fine-tuning (3.3%
improvement in Acc@60).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Unifying Text Segmentation and Long Document Summarization. (arXiv:2210.16422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16422">
<div class="article-summary-box-inner">
<span><p>Text segmentation is important for signaling a document's structure. Without
segmenting a long document into topically coherent sections, it is difficult
for readers to comprehend the text, let alone find important information. The
problem is only exacerbated by a lack of segmentation in transcripts of
audio/video recordings. In this paper, we explore the role that section
segmentation plays in extractive summarization of written and spoken documents.
Our approach learns robust sentence representations by performing summarization
and segmentation simultaneously, which is further enhanced by an
optimization-based regularizer to promote selection of diverse summary
sentences. We conduct experiments on multiple datasets ranging from scientific
articles to spoken transcripts to evaluate the model's performance. Our
findings suggest that the model can not only achieve state-of-the-art
performance on publicly available benchmarks, but demonstrate better
cross-genre transferability when equipped with text segmentation. We perform a
series of analyses to quantify the impact of section segmentation on
summarizing written and spoken documents of substantial length and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention. (arXiv:2210.16431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16431">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (V-L) tasks require the system to understand both vision
content and natural language, thus learning fine-grained joint representations
of vision and language (a.k.a. V-L representations) is of paramount importance.
Recently, various pre-trained V-L models are proposed to learn V-L
representations and achieve improved results in many tasks. However, the
mainstream models process both vision and language inputs with the same set of
attention matrices. As a result, the generated V-L representations are
entangled in one common latent space. To tackle this problem, we propose
DiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel
framework that applies separated attention spaces for vision and language, and
the representations of multi-modalities can thus be disentangled explicitly. To
enhance the correlation between vision and language in disentangled spaces, we
introduce the visual concepts to DiMBERT which represent visual information in
textual format. In this manner, visual concepts help to bridge the gap between
the two modalities. We pre-train DiMBERT on a large amount of image-sentence
pairs on two tasks: bidirectional language modeling and sequence-to-sequence
language modeling. After pre-train, DiMBERT is further fine-tuned for the
downstream tasks. Experiments show that DiMBERT sets new state-of-the-art
performance on three tasks (over four datasets), including both generation
tasks (image captioning and visual storytelling) and classification tasks
(referring expressions). The proposed DiM (short for Disentangled
Multimodal-Attention) module can be easily incorporated into existing
pre-trained V-L models to boost their performance, up to a 5% increase on the
representative task. Finally, we conduct a systematic analysis and demonstrate
the effectiveness of our DiM and the introduced visual concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16433">
<div class="article-summary-box-inner">
<span><p>Fully-parametric language models generally require a huge number of model
parameters to store the necessary knowledge for solving multiple natural
language tasks in zero/few-shot settings. In addition, it is hard to adapt to
the evolving world knowledge without the costly model re-training. In this
paper, we develop a novel semi-parametric language model architecture,
Knowledge-in-Context (KiC), which empowers a parametric text-to-text language
model with a knowledge-rich external memory. Specifically, the external memory
contains six different types of knowledge: entity, dictionary, commonsense,
event, script, and causality knowledge. For each input instance, the KiC model
adaptively selects a knowledge type and retrieves the most helpful pieces of
knowledge. The input instance along with its knowledge augmentation is fed into
a text-to-text model (e.g., T5) to generate the output answer, where both the
input and the output are in natural language forms after prompting.
Interestingly, we find that KiC can be identified as a special
mixture-of-experts (MoE) model, where the knowledge selector plays the role of
a router that is used to determine the sequence-to-expert assignment in MoE.
This key observation inspires us to develop a novel algorithm for training KiC
with an instance-adaptive knowledge selector. As a knowledge-rich
semi-parametric language model, KiC only needs a much smaller parametric part
to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+
different tasks, we show that KiC_Large with 770M parameters easily outperforms
large language models (LMs) that are 4-39x larger by a large margin. We also
demonstrate that KiC exhibits emergent abilities at a much smaller model scale
compared to the fully-parametric models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Classification of Code-Switched Text using Pre-trained Multilingual Embeddings and Segmentation. (arXiv:2210.16461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16461">
<div class="article-summary-box-inner">
<span><p>With increasing globalization and immigration, various studies have estimated
that about half of the world population is bilingual. Consequently, individuals
concurrently use two or more languages or dialects in casual conversational
settings. However, most research is natural language processing is focused on
monolingual text. To further the work in code-switched sentiment analysis, we
propose a multi-step natural language processing algorithm utilizing points of
code-switching in mixed text and conduct sentiment analysis around those
identified points. The proposed sentiment analysis algorithm uses semantic
similarity derived from large pre-trained multilingual models with a
handcrafted set of positive and negative words to determine the polarity of
code-switched text. The proposed approach outperforms a comparable baseline
model by 11.2% for accuracy and 11.64% for F1-score on a Spanish-English
dataset. Theoretically, the proposed algorithm can be expanded for sentiment
analysis of multiple languages with limited human expertise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating RNN-T Training and Inference Using CTC guidance. (arXiv:2210.16481v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16481">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to accelerate training and inference process of
recurrent neural network transducer (RNN-T) based on the guidance from a
co-trained connectionist temporal classification (CTC) model. We made a key
assumption that if an encoder embedding frame is classified as a blank frame by
the CTC model, it is likely that this frame will be aligned to blank for all
the partial alignments or hypotheses in RNN-T and it can be discarded from the
decoder input. We also show that this frame reduction operation can be applied
in the middle of the encoder, which result in significant speed up for the
training and inference in RNN-T. We further show that the CTC alignment, a
by-product of the CTC decoder, can also be used to perform lattice reduction
for RNN-T during training. Our method is evaluated on the Librispeech and
SpeechStew tasks. We demonstrate that the proposed method is able to accelerate
the RNN-T inference by 2.2 times with similar or slightly better word error
rates (WER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot Classification. (arXiv:2210.16489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16489">
<div class="article-summary-box-inner">
<span><p>The effectiveness of prompt learning has been demonstrated in different
pre-trained language models. By formulating suitable template and choosing
representative label mapping, prompt learning can be used as an efficient
knowledge probe. However, finding suitable prompt in existing methods requires
multiple experimental attempts or appropriate vector initialization on
formulating suitable template and choosing representative label mapping, which
it is more common in few-shot learning tasks. Motivating by PLM working
process, we try to construct the prompt from task semantic perspective and thus
propose the STPrompt -Semantic-guided and Task-driven Prompt model.
Specifically, two novel prompts generated from the semantic dependency tree
(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstly
constructed in a prompt augmented pool, and the proposed model would
automatically select a suitable semantic prompt to motivating the prompt
learning process. Our results show that the proposed model achieves the
state-of-the-art performance in five different datasets of few-shot text
classification tasks, which prove that more semantic and significant prompts
could assume as a better knowledge proving tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering. (arXiv:2210.16495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16495">
<div class="article-summary-box-inner">
<span><p>We propose a simple refactoring of multi-choice question answering (MCQA)
tasks as a series of binary classifications. The MCQA task is generally
performed by scoring each (question, answer) pair normalized over all the
pairs, and then selecting the answer from the pair that yield the highest
score. For n answer choices, this is equivalent to an n-class classification
setup where only one class (true answer) is correct. We instead show that
classifying (question, true answer) as positive instances and (question, false
answer) as negative instances is significantly more effective across various
models and datasets. We show the efficacy of our proposed approach in different
tasks -- abductive reasoning, commonsense question answering, science question
answering, and sentence completion. Our DeBERTa binary classification model
reaches the top or close to the top performance on public leaderboards for
these tasks. The source code of the proposed approach is available at
https://github.com/declare-lab/TEAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Data Augmentation for Contrastive Sentence Representation Learning. (arXiv:2210.16536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16536">
<div class="article-summary-box-inner">
<span><p>Fine-tuning a pre-trained language model via the contrastive learning
framework with a large amount of unlabeled sentences or labeled sentence pairs
is a common way to obtain high-quality sentence representations. Although the
contrastive learning framework has shown its superiority on sentence
representation learning over previous methods, the potential of such a
framework is under-explored so far due to the simple method it used to
construct positive pairs. Motivated by this, we propose a method that makes
hard positives from the original training examples. A pivotal ingredient of our
approach is the use of prefix that is attached to a pre-trained language model,
which allows for differentiable data augmentation during contrastive learning.
Our method can be summarized in two steps: supervised prefix-tuning followed by
joint contrastive fine-tuning with unlabeled or labeled examples. Our
experiments confirm the effectiveness of our data augmentation approach. The
proposed method yields significant improvements over existing methods under
both semi-supervised and supervised settings. Our experiments under a low
labeled data setting also show that our method is more label-efficient than the
state-of-the-art contrastive learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phonemic Representation and Transcription for Speech to Text Applications for Under-resourced Indigenous African Languages: The Case of Kiswahili. (arXiv:2210.16537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16537">
<div class="article-summary-box-inner">
<span><p>Building automatic speech recognition (ASR) systems is a challenging task,
especially for under-resourced languages that need to construct corpora nearly
from scratch and lack sufficient training data. It has emerged that several
African indigenous languages, including Kiswahili, are technologically
under-resourced. ASR systems are crucial, particularly for the hearing-impaired
persons who can benefit from having transcripts in their native languages.
However, the absence of transcribed speech datasets has complicated efforts to
develop ASR models for these indigenous languages. This paper explores the
transcription process and the development of a Kiswahili speech corpus, which
includes both read-out texts and spontaneous speech data from native Kiswahili
speakers. The study also discusses the vowels and consonants in Kiswahili and
provides an updated Kiswahili phoneme dictionary for the ASR model that was
created using the CMU Sphinx speech recognition toolbox, an open-source speech
recognition toolkit. The ASR model was trained using an extended phonetic set
that yielded a WER and SER of 18.87% and 49.5%, respectively, an improved
performance than previous similar research for under-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16539">
<div class="article-summary-box-inner">
<span><p>Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating
preventive care and to delay further progression. Speech based automatic AD
screening systems provide a non-intrusive and more scalable alternative to
other clinical screening techniques. Textual embedding features produced by
pre-trained language models (PLMs) such as BERT are widely used in such
systems. However, PLM domain fine-tuning is commonly based on the masked word
or sentence prediction costs that are inconsistent with the back-end AD
detection task. To this end, this paper investigates the use of prompt-based
fine-tuning of PLMs that consistently uses AD classification errors as the
training objective function. Disfluency features based on hesitation or pause
filler token frequencies are further incorporated into prompt phrases during
PLM fine-tuning. The exploit of the complementarity between BERT or RoBERTa
based PLMs that are either prompt learning fine-tuned, or optimized using
conventional masked word or sentence prediction costs, decision voting based
system combination between them is further applied. Mean, standard deviation
and the maximum among accuracy scores over 15 experiment runs are adopted as
performance measurements for the AD detection system. Mean detection accuracy
of 84.20% (with std 2.09%, best 87.5%) and 82.64% (with std 4.0%, best 89.58%)
were obtained using manual and ASR speech transcripts respectively on the
ADReSS20 test set consisting of 48 elderly speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-centered Cross-document Relation Extraction. (arXiv:2210.16541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16541">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) is a fundamental task of information extraction,
which has attracted a large amount of research attention. Previous studies
focus on extracting the relations within a sentence or document, while
currently researchers begin to explore cross-document RE. However, current
cross-document RE methods directly utilize text snippets surrounding target
entities in multiple given documents, which brings considerable noisy and
non-relevant sentences. Moreover, they utilize all the text paths in a document
bag in a coarse-grained way, without considering the connections between these
text paths.In this paper, we aim to address both of these shortages and push
the state-of-the-art for cross-document RE. First, we focus on input
construction for our RE model and propose an entity-based document-context
filter to retain useful information in the given documents by using the bridge
entities in the text paths. Second, we propose a cross-document RE model based
on cross-path entity relation attention, which allow the entity relations
across text paths to interact with each other. We compare our cross-document RE
method with the state-of-the-art methods in the dataset CodRED. Our method
outperforms them by at least 10% in F1, thus demonstrating its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Spoken Language Understanding with Tree-constrained Pointer Generator. (arXiv:2210.16554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16554">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) suffers from the long-tail
word problem. This paper exploits contextual biasing, a technique to improve
the speech recognition of rare words, in end-to-end SLU systems. Specifically,
a tree-constrained pointer generator (TCPGen), a powerful and efficient biasing
model component, is studied, which leverages a slot shortlist with
corresponding entities to extract biasing lists. Meanwhile, to bias the SLU
model output slot distribution, a slot probability biasing (SPB) mechanism is
proposed to calculate a slot distribution from TCPGen. Experiments on the SLURP
dataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially
on unseen entities. On a new split by holding out 5 slot types for the test,
TCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%
compared to baselines which can not deal with it. In addition to slot filling,
the intent classification accuracy was also improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Attribute-Entangled Controllable Text Generation: A Pilot Study of Blessing Generation. (arXiv:2210.16557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16557">
<div class="article-summary-box-inner">
<span><p>Controllable Text Generation (CTG) has obtained great success due to its
fine-grained generation ability obtained by focusing on multiple attributes.
However, most existing CTG researches overlook how to utilize the attribute
entanglement to enhance the diversity of the controlled generated texts. Facing
this dilemma, we focus on a novel CTG scenario, i.e., blessing generation which
is challenging because high-quality blessing texts require CTG models to
comprehensively consider the entanglement between multiple attributes (e.g.,
objects and occasions). To promote the research on blessing generation, we
present EBleT, a large-scale Entangled Blessing Text dataset containing 293K
English sentences annotated with multiple attributes. Furthermore, we propose
novel evaluation metrics to measure the quality of the blessing texts generated
by the baseline models we designed. Our study opens a new research direction
for controllable text generation and enables the development of
attribute-entangled CTG models. Our dataset and source codes are available at
\url{https://github.com/huangshulin123/Blessing-Generation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTULM: Enriching Social Media Text Representations with Non-Textual Units. (arXiv:2210.16586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16586">
<div class="article-summary-box-inner">
<span><p>On social media, additional context is often present in the form of
annotations and meta-data such as the post's author, mentions, Hashtags, and
hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit
that NTUs provide social context beyond their textual semantics and leveraging
these units can enrich social media text representations. In this work we
construct an NTU-centric social heterogeneous network to co-embed NTUs. We then
principally integrate these NTU embeddings into a large pretrained language
model by fine-tuning with these additional units. This adds context to noisy
short-text social media. Experiments show that utilizing NTU-augmented text
representations significantly outperforms existing text-only baselines by 2-5\%
relative points on many downstream tasks highlighting the importance of context
to social media NLP. We also highlight that including NTU context into the
initial layers of language model alongside text is better than using it after
the text embedding is generated. Our work leads to the generation of holistic
general purpose social media content embedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing. (arXiv:2210.16604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16604">
<div class="article-summary-box-inner">
<span><p>We review the state of research on empathy in natural language processing and
identify the following issues: (1) empathy definitions are absent or abstract,
which (2) leads to low construct validity and reproducibility. Moreover, (3)
emotional empathy is overemphasized, skewing our focus to a narrow subset of
simplified tasks. We believe these issues hinder research progress and argue
that current directions will benefit from a clear conceptualization that
includes operationalizing cognitive empathy components. Our main objectives are
to provide insight and guidance on empathy conceptualization for NLP research
objectives and to encourage researchers to pursue the overlooked opportunities
in this area, highly relevant, e.g., for clinical and educational sectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16611">
<div class="article-summary-box-inner">
<span><p>Model architectures such as wav2vec 2.0 and HuBERT have been proposed to
learn speech representations from audio waveforms in a self-supervised manner.
When these models are combined with downstream tasks such as speech
recognition, they have been shown to provide state-of-the-art performance.
However, these models use a large number of parameters, the smallest version of
which has about 95 million parameters. This constitutes a challenge for edge AI
device deployments. In this paper, we use knowledge distillation to reduce the
original model size by about 75% while maintaining similar performance levels.
Moreover, we use wav2vec 2.0 and HuBERT models for distillation and present a
comprehensive performance analysis through our experiments where we fine-tune
the distilled models on single task and multi-task frameworks separately. In
particular, our experiments show that fine-tuning the distilled models on
keyword spotting and speaker verification tasks result in only 0.1% accuracy
and 0.9% equal error rate degradations, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers. (arXiv:2210.16613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16613">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Evaluation of Post-Training Quantization Methods for Language Tasks. (arXiv:2210.16621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16621">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures like BERT have achieved great success in a
wide range of Natural Language tasks. Despite their decent performance, the
models still have numerous parameters and high computational complexity,
impeding their deployment in resource-constrained environments. Post-Training
Quantization (PTQ), which enables low-bit computations without extra training,
could be a promising tool. In this work, we conduct an empirical evaluation of
three PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),
Analytical Clipping for Integer Quantization (ACIQ), and Outlier Channel
Splitting (OCS). OCS theoretically surpasses the others in minimizing the Mean
Square quantization Error and avoiding distorting the weights' outliers. That
is consistent with the evaluation results of most language tasks of GLUE
benchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized
BERT models could outperform the corresponding 32-bit baselines on several
small language tasks, which we attribute to the alleviation of
over-parameterization. We further explore the limit of quantization bit and
show that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%
and 96% of the performance on the GLUE benchmark accordingly. Moreover, we
conduct quantization on the whole BERT family, i.e., BERT models in different
configurations, and comprehensively evaluate their performance on the GLUE
benchmark and SQuAD, hoping to provide valuable guidelines for their deployment
in various computation environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations. (arXiv:2210.16637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16637">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition. (arXiv:2210.16642v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16642">
<div class="article-summary-box-inner">
<span><p>Traditionally, in paralinguistic analysis for emotion detection from speech,
emotions have been identified with discrete or dimensional (continuous-valued)
labels. Accordingly, models that have been proposed for emotion detection use
one or the other of these label types. However, psychologists like Russell and
Plutchik have proposed theories and models that unite these views, maintaining
that these representations have shared and complementary information. This
paper is an attempt to validate these viewpoints computationally. To this end,
we propose a model to jointly predict continuous and discrete emotional
attributes and show how the relationship between these can be utilized to
improve the robustness and performance of emotion recognition tasks. Our
approach comprises multi-task and hierarchical multi-task learning frameworks
that jointly model the relationships between continuous-valued and discrete
emotion labels. Experimental results on two widely used datasets (IEMOCAP and
MSPPodcast) for speech-based emotion recognition show that our model results in
statistically significant improvements in performance over strong baselines
with non-unified approaches. We also demonstrate that using one type of label
(discrete or continuous-valued) for training improves recognition performance
in tasks that use the other type of label. Experimental results and reasoning
for this approach (called the mismatched training approach) are also presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers. (arXiv:2210.16643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16643">
<div class="article-summary-box-inner">
<span><p>Transformers are among the state of the art for many tasks in speech, vision,
and natural language processing, among others. Self-attentions, which are
crucial contributors to this performance have quadratic computational
complexity, which makes training on longer input sequences challenging. Prior
work has produced state-of-the-art transformer variants with linear attention,
however, current models sacrifice performance to achieve efficient
implementations. In this work, we develop a novel linear transformer by
examining the properties of the key-query product within self-attentions. Our
model outperforms state of the art approaches on speech recognition and speech
summarization, resulting in 1 % absolute WER improvement on the Librispeech-100
speech recognition benchmark and a new INTERVIEW speech recognition benchmark,
and 5 points on ROUGE for summarization with How2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dependencies of Discrete Speech Representations with Neural Hidden Markov Models. (arXiv:2210.16659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16659">
<div class="article-summary-box-inner">
<span><p>While discrete latent variable models have had great success in
self-supervised learning, most models assume that frames are independent. Due
to the segmental nature of phonemes in speech perception, modeling dependencies
among latent variables at the frame level can potentially improve the learned
representations on phonetic-related tasks. In this work, we assume Markovian
dependencies among latent variables, and propose to learn speech
representations with neural hidden Markov models. Our general framework allows
us to compare to self-supervised models that assume independence, while keeping
the number of parameters fixed. The added dependencies improve the
accessibility of phonetic information, phonetic segmentation, and the cluster
purity of phones, showcasing the benefit of the assumed dependencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16663">
<div class="article-summary-box-inner">
<span><p>This paper presents BERT-CTC, a novel formulation of end-to-end speech
recognition that adapts BERT for connectionist temporal classification (CTC).
Our formulation relaxes the conditional independence assumptions used in
conventional CTC and incorporates linguistic knowledge through the explicit
output dependency obtained by BERT contextual embedding. BERT-CTC attends to
the full contexts of the input and hypothesized output sequences via the
self-attention mechanism. This mechanism encourages a model to learn
inner/inter-dependencies between the audio and token representations while
maintaining CTC's training efficiency. During inference, BERT-CTC combines a
mask-predict algorithm with CTC decoding, which iteratively refines an output
sequence. The experimental results reveal that BERT-CTC improves over
conventional approaches across variations in speaking styles and languages.
Finally, we show that the semantic representations in BERT-CTC are beneficial
towards downstream spoken language understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far are We from Robust Long Abstractive Summarization?. (arXiv:2210.16732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16732">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization has made tremendous progress in recent years. In
this work, we perform fine-grained human annotations to evaluate long document
abstractive summarization systems (i.e., models and metrics) with the aim of
implementing them to generate reliable summaries. For long document abstractive
models, we show that the constant strive for state-of-the-art ROUGE results can
lead us to generate more relevant summaries but not factual ones. For long
document evaluation metrics, human evaluation results show that ROUGE remains
the best at evaluating the relevancy of a summary. It also reveals important
limitations of factuality metrics in detecting different types of factual
errors and the reasons behind the effectiveness of BARTScore. We then suggest
promising directions in the endeavor of developing factual consistency metrics.
Finally, we release our annotated long document dataset with the hope that it
can contribute to the development of metrics across a broader range of
summarization settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set. (arXiv:2210.16739v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16739">
<div class="article-summary-box-inner">
<span><p>In a multilingual country like India, multilingual Automatic Speech
Recognition (ASR) systems have much scope. Multilingual ASR systems exhibit
many advantages like scalability, maintainability, and improved performance
over the monolingual ASR systems. However, building multilingual systems for
Indian languages is challenging since different languages use different scripts
for writing. On the other hand, Indian languages share a lot of common sounds.
Common Label Set (CLS) exploits this idea and maps graphemes of various
languages with similar sounds to common labels. Since Indian languages are
mostly phonetic, building a parser to convert from native script to CLS is
easy. In this paper, we explore various approaches to build multilingual ASR
models. We also propose a novel architecture called Encoder-Decoder-Decoder for
building multilingual systems that use both CLS and native script labels. We
also analyzed the effectiveness of CLS-based multilingual systems combined with
machine transliteration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text. (arXiv:2210.16755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16755">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training has been successful in both text and speech
processing. Speech and text offer different but complementary information. The
question is whether we are able to perform a speech-text joint pre-training on
unpaired speech and text. In this paper, we take the idea of self-supervised
pre-training one step further and propose token2vec, a novel joint pre-training
framework for unpaired speech and text based on discrete representations of
speech. Firstly, due to the distinct characteristics between speech and text
modalities, where speech is continuous while text is discrete, we first
discretize speech into a sequence of discrete speech tokens to solve the
modality mismatch problem. Secondly, to solve the length mismatch problem,
where the speech sequence is usually much longer than text sequence, we convert
the words of text into phoneme sequences and randomly repeat each phoneme in
the sequences. Finally, we feed the discrete speech and text tokens into a
modality-agnostic Transformer encoder and pre-train with token-level masking
language modeling (tMLM). Experiments show that token2vec is significantly
superior to various speech-only pre-training baselines, with up to 17.7%
relative WER reduction. Token2vec model is also validated on a non-ASR task,
i.e., spoken intent classification, and shows good transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16771">
<div class="article-summary-box-inner">
<span><p>In recent years, pretrained models revolutionized the paradigm of natural
language understanding (NLU), where we append a randomly initialized
classification head after the pretrained backbone, e.g. BERT, and finetune the
whole model. As the pretrained backbone makes a major contribution to the
improvement, we naturally expect a good pretrained classification head can also
benefit the training. However, the final-layer output of the backbone, i.e. the
input of the classification head, will change greatly during finetuning, making
the usual head-only pretraining (LP-FT) ineffective. In this paper, we find
that parameter-efficient tuning makes a good classification head, with which we
can simply replace the randomly initialized heads for a stable performance
gain. Our experiments demonstrate that the classification head jointly
pretrained with parameter-efficient tuning consistently improves the
performance on 9 tasks in GLUE and SuperGLUE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks. (arXiv:2210.16773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16773">
<div class="article-summary-box-inner">
<span><p>Access to external knowledge is essential for many natural language
processing tasks, such as question answering and dialogue. Existing methods
often rely on a parametric model that stores knowledge in its parameters, or
use a retrieval-augmented model that has access to an external knowledge
source. Parametric and retrieval-augmented models have complementary strengths
in terms of computational efficiency and predictive accuracy. To combine the
strength of both approaches, we propose the Efficient Memory-Augmented
Transformer (EMAT) -- it encodes external knowledge into a key-value memory and
exploits the fast maximum inner product search for memory querying. We also
introduce pre-training tasks that allow EMAT to encode informative key-value
representations, and to learn an implicit strategy to integrate multiple memory
slots into the transformer. Experiments on various knowledge-intensive tasks
such as question answering and dialogue datasets show that, simply augmenting
parametric models (T5-base) using our method produces more accurate results
(e.g., 25.8 -&gt; 44.3 EM on NQ) while retaining a high throughput (e.g., 1000
queries/s on NQ). Compared to retrieval-augmented models, EMAT runs
substantially faster across the board and produces more accurate results on WoW
and ELI5. Our code and datasets are available at https://github.
com/uclnlp/EMAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework. (arXiv:2210.16798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16798">
<div class="article-summary-box-inner">
<span><p>Most sentence embedding techniques heavily rely on expensive human-annotated
sentence pairs as the supervised signals. Despite the use of large-scale
unlabeled data, the performance of unsupervised methods typically lags far
behind that of the supervised counterparts in most downstream tasks. In this
work, we propose a semi-supervised sentence embedding framework, GenSE, that
effectively leverages large-scale unlabeled data. Our method include three
parts: 1) Generate: A generator/discriminator model is jointly trained to
synthesize sentence pairs from open-domain unlabeled corpus; 2) Discriminate:
Noisy sentence pairs are filtered out by the discriminator to acquire
high-quality positive and negative sentence pairs; 3) Contrast: A prompt-based
contrastive approach is presented for sentence representation learning with
both annotated and synthesized data. Comprehensive experiments show that GenSE
achieves an average correlation score of 85.19 on the STS datasets and
consistent performance improvement on four domain adaptation tasks,
significantly surpassing the state-of-the-art methods and convincingly
corroborating its effectiveness and generalization ability.Code, Synthetic data
and Models available at https://github.com/MatthewCYM/GenSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues. (arXiv:2210.16838v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16838">
<div class="article-summary-box-inner">
<span><p>The construction of open-domain dialogue systems requires high-quality
dialogue datasets. The dialogue data admits a wide variety of responses for a
given dialogue history, especially responses with different semantics. However,
collecting high-quality such a dataset in most scenarios is labor-intensive and
time-consuming. In this paper, we propose a data augmentation method to
automatically augment high-quality responses with different semantics by
counterfactual inference. Specifically, given an observed dialogue, our
counterfactual generation model first infers semantically different responses
by replacing the observed reply perspective with substituted ones. Furthermore,
our data selection method filters out detrimental augmented responses.
Experimental results show that our data augmentation method can augment
high-quality responses with different semantics for a given dialogue history,
and can outperform competitive baselines on multiple downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actionable Phrase Detection using NLP. (arXiv:2210.16841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16841">
<div class="article-summary-box-inner">
<span><p>Actionable sentences are terms that, in the most basic sense, imply the
necessity of taking a specific action. In Linguistic terms, they are steps to
achieve an operation, often through the usage of action verbs. For example, the
sentence, `Get your homework finished by tomorrow` qualifies as actionable
since it demands a specific action (In this case, finishing homework) to be
taken. In contrast, a simple sentence such as, `I like to play the guitar` does
not qualify as an actionable phrase since it simply states a personal choice of
the person instead of demanding a task to be finished.
</p>
<p>In this paper, the aim is to explore if Actionables can be extracted from raw
text using Linguistic filters designed from scratch. These filters are
specially catered to identifying actionable text using Transfer Learning as the
lead role. Actionable Detection can be used in detecting emergency tasks during
a crisis, Instruction accuracy for First aid and can also be used to make
productivity tools like automatic ToDo list generators from conferences. To
accomplish this, we use the Enron Email Dataset and apply our Linguistic
filters on the cleaned textual data. We then use Transfer Learning with the
Universal Sentence Encoder to train a model to classify whether a given string
of raw text is actionable or not.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16848">
<div class="article-summary-box-inner">
<span><p>Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Codes Prediction from Clinical Notes: From Human Coders to Machines. (arXiv:2210.16850v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16850">
<div class="article-summary-box-inner">
<span><p>Prediction of medical codes from clinical notes is a practical and essential
need for every healthcare delivery organization within current medical systems.
Automating annotation will save significant time and excessive effort that
human coders spend today. However, the biggest challenge is directly
identifying appropriate medical codes from several thousands of
high-dimensional codes from unstructured free-text clinical notes. This complex
medical codes prediction problem from clinical notes has received substantial
interest in the NLP community, and several recent studies have shown the
state-of-the-art code prediction results of full-fledged deep learning-based
methods. This progress raises the fundamental question of how far automated
machine learning systems are from human coders' working performance, as well as
the important question of how well current explainability methods apply to
advanced neural network models such as transformers. This is to predict correct
codes and present references in clinical notes that support code prediction, as
this level of explainability and accuracy of the prediction outcomes is
critical to gaining trust from professional medical coders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts. (arXiv:2210.16865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16865">
<div class="article-summary-box-inner">
<span><p>Explicit decomposition modeling, which involves breaking down complex tasks
into more straightforward and often more interpretable sub-tasks, has long been
a central theme in developing robust and interpretable NLU systems. However,
despite the many datasets and resources built as part of this effort, the
majority have small-scale annotations and limited scope, which is insufficient
to solve general decomposition tasks. In this paper, we look at large-scale
intermediate pre-training of decomposition-based transformers using distant
supervision from comparable texts, particularly large-scale parallel news. We
show that with such intermediate pre-training, developing robust
decomposition-based models for a diverse range of tasks becomes more feasible.
For example, on semantic parsing, our model, DecompT5, improves 20% to 30% on
two datasets, Overnight and TORQUE, over the baseline language model. We
further use DecompT5 to build a novel decomposition-based QA system named
DecompEntail, improving over state-of-the-art models, including GPT-3, on both
HotpotQA and StrategyQA by 8% and 4%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusER: Discrete Diffusion via Edit-based Reconstruction. (arXiv:2210.16886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16886">
<div class="article-summary-box-inner">
<span><p>In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Large-Scale Image Captioning from Alt-text Data using Content Selection Models. (arXiv:2009.05175v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05175">
<div class="article-summary-box-inner">
<span><p>Training large-scale image captioning (IC) models demands access to a rich
and diverse set of training examples, gathered from the wild, often from noisy
alt-text data. However, recent modeling approaches to IC often fall short in
terms of performance in this case, because they assume a clean annotated
dataset (as opposed to the noisier alt-text--based annotations), and employ an
end-to-end generation approach, which often lacks both controllability and
interpretability. We address these problems by breaking down the task into two
simpler, more controllable tasks -- skeleton prediction and skeleton-based
caption generation. Specifically, we show that selecting content words as
skeletons} helps in generating improved and denoised captions when leveraging
rich yet noisy alt-text--based uncurated datasets. We also show that the
predicted English skeletons can be further cross-lingually leveraged to
generate non-English captions, and present experimental results covering
caption generation in French, Italian, German, Spanish and Hindi. We also show
that skeleton-based prediction allows for better control of certain caption
properties, such as length, content, and gender expression, providing a handle
to perform human-in-the-loop semi-automatic corrections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07983">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations for understanding the behavior of black box models
has gained a lot of attention recently as they provide potential for recourse.
In this paper, we propose a method Contrastive Attributed explanations for Text
(CAT) which provides contrastive explanations for natural language text data
with a novel twist as we build and exploit attribute classifiers leading to
more semantically meaningful explanations. To ensure that our contrastive
generated text has the fewest possible edits with respect to the original text,
while also being fluent and close to a human generated contrastive, we resort
to a minimal perturbation approach regularized using a BERT language model and
attribute classifiers trained on available attributes. We show through
qualitative examples and a user study that our method not
onlyconveysmoreinsightbecauseoftheseattributes,butalsoleadstobetterquality(contrastive)text.
Quantitatively, we show that our method outperforms other state-of-the-art
methods across four data sets on four benchmark metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15254">
<div class="article-summary-box-inner">
<span><p>We introduce a new Slovak masked language model called SlovakBERT. This is to
our best knowledge the first paper discussing Slovak transformers-based
language models. We evaluate our model on several NLP tasks and achieve
state-of-the-art results. This evaluation is likewise the first attempt to
establish a benchmark for Slovak language models. We publish the masked
language model, as well as the fine-tuned models for part-of-speech tagging,
sentiment analysis and semantic textual similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01661">
<div class="article-summary-box-inner">
<span><p>Iterating with new and improved OCR solutions enforces decision making when
it comes to targeting the right candidates for reprocessing. This especially
applies when the underlying data collection is of considerable size and rather
diverse in terms of fonts, languages, periods of publication and consequently
OCR quality. This article captures the efforts of the National Library of
Luxembourg to support those targeting decisions. They are crucial in order to
guarantee low computational overhead and reduced quality degradation risks,
combined with a more quantifiable OCR improvement. In particular, this work
explains the methodology of the library with respect to text block level
quality assessment. Through extension of this technique, a regression model,
that is able to take into account the enhancement potential of a new OCR
engine, is also presented. They both mark promising approaches, especially for
cultural institutions dealing with historical data of lower quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03611">
<div class="article-summary-box-inner">
<span><p>Current dense text retrieval models face two typical challenges. First, they
adopt a siamese dual-encoder architecture to encode queries and documents
independently for fast indexing and searching, while neglecting the
finer-grained term-wise interactions. This results in a sub-optimal recall
performance. Second, their model training highly relies on a negative sampling
technique to build up the negative documents in their contrastive losses. To
address these challenges, we present Adversarial Retriever-Ranker (AR2), which
consists of a dual-encoder retriever plus a cross-encoder ranker. The two
models are jointly optimized according to a minimax adversarial objective: the
retriever learns to retrieve negative documents to cheat the ranker, while the
ranker learns to rank a collection of candidates including both the
ground-truth and the retrieved ones, as well as providing progressive direct
feedback to the dual-encoder retriever. Through this adversarial game, the
retriever gradually produces harder negative documents to train a better
ranker, whereas the cross-encoder ranker provides progressive feedback to
improve retriever. We evaluate AR2 on three benchmarks. Experimental results
show that AR2 consistently and significantly outperforms existing dense
retriever methods and achieves new state-of-the-art results on all of them.
This includes the improvements on Natural Questions R@5 to 77.9%(+2.1%),
TriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). Code and
models are available at https://github.com/microsoft/AR2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemPrompt: Memory-assisted Prompt Editing with User Feedback. (arXiv:2201.06009v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06009">
<div class="article-summary-box-inner">
<span><p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are
obvious to humans. For example, GPT-3 would mistakenly interpret "What word is
similar to good?" to mean a homophone, while the user intended a synonym. Our
goal is to effectively correct such errors via user interactions with the
system but without retraining, which will be prohibitively costly. We pair
GPT-3 with a growing memory of recorded cases where the model misunderstood the
user's intents, along with user feedback for clarification. Such a memory
allows our system to produce enhanced prompts for any new query based on the
user feedback for error correction on similar cases in the past. On four tasks
(two lexical tasks, two advanced ethical reasoning tasks), we show how a
(simulated) user can interactively teach a deployed GPT-3, substantially
increasing its accuracy over the queries with different kinds of
misunderstandings by the GPT-3. Our approach is a step towards the low-cost
utility enhancement for very large pre-trained LMs. Code, data, and
instructions to implement MEMPROMPT for a new task at
https://www.memprompt.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning. (arXiv:2201.06206v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06206">
<div class="article-summary-box-inner">
<span><p>Multi-hop knowledge graph (KG) reasoning has been widely studied in recent
years to provide interpretable predictions on missing links with evidential
paths. Most previous works use reinforcement learning (RL) based methods that
learn to navigate the path towards the target entity. However, these methods
suffer from slow and poor convergence, and they may fail to infer a certain
path when there is a missing edge along the path. Here we present SQUIRE, the
first Sequence-to-sequence based multi-hop reasoning framework, which utilizes
an encoder-decoder Transformer structure to translate the query to a path. Our
framework brings about two benefits: (1) It can learn and predict in an
end-to-end fashion, which gives better and faster convergence; (2) Our
Transformer model does not rely on existing edges to generate the path, and has
the flexibility to complete missing edges along the path, especially in sparse
KGs. Experiments on standard and sparse KGs show that our approach yields
significant improvement over prior methods, while converging 4x-7x faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization. (arXiv:2201.06910v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06910">
<div class="article-summary-box-inner">
<span><p>We propose a multitask pretraining approach ZeroPrompt for zero-shot
generalization, focusing on task scaling and zero-shot prompting. While
previous models are trained on only a few dozen tasks, we scale to 1,000 tasks
for the first time using real-world data. This leads to a crucial discovery
that task scaling can be an efficient alternative to model scaling; i.e., the
model size has little impact on performance with an extremely large number of
tasks. Our results show that task scaling can substantially improve training
efficiency by 30 times in FLOPs. Moreover, we present a prompting method that
incorporates a genetic algorithm to automatically search for the best prompt
for unseen tasks, along with a few other improvements. Empirically, ZeroPrompt
substantially improves both the efficiency and the performance of zero-shot
learning across a variety of academic and production datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>As two important textual modalities in electronic health records (EHR), both
structured data (clinical codes) and unstructured data (clinical narratives)
have recently been increasingly applied to the healthcare domain. Most existing
EHR-oriented studies, however, either focus on a particular modality or
integrate data from different modalities in a straightforward manner, which
usually treats structured and unstructured data as two independent sources of
information about patient admission and ignore the intrinsic interactions
between them. In fact, the two modalities are documented during the same
encounter where structured data inform the documentation of unstructured data
and vice versa. In this paper, we proposed a Medical Multimodal Pre-trained
Language Model, named MedM-PLM, to learn enhanced EHR representations over
structured and unstructured data and explore the interaction of two modalities.
In MedM-PLM, two Transformer-based neural network components are firstly
adopted to learn representative characteristics from each modality. A
cross-modal module is then introduced to model their interactions. We
pre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of
the model on three downstream clinical tasks, i.e., medication recommendation,
30-day readmission prediction and ICD coding. Extensive experiments demonstrate
the power of MedM-PLM compared with state-of-the-art methods. Further analyses
and visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00964">
<div class="article-summary-box-inner">
<span><p>We show that existing model interpretation methods such as linear probes and
prompts have some key limitations in answering these questions. We revisit KI
from an information-theoretic view and propose a new theoretically sound probe
called Graph Convolution Simulator (GCS) for KI interpretation. GCS uses graph
attention on the corresponding knowledge graph for interpretation. In our
experiments we verify that GCS can provide reasonable interpretation results
for two well-known knowledge-enhanced LMs: ERNIE and K-Adapter. We also find
that only a marginal amount of knowledge is successfully integrated in these
models, and simply increasing the size of the KI corpus may not lead to better
knowledge-enhanced LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01771">
<div class="article-summary-box-inner">
<span><p>Language model (LM) pre-training is useful in many language processing tasks.
But can pre-trained LMs be further leveraged for more general machine learning
problems? We propose an approach for using LMs to scaffold learning and
generalization in general sequential decision-making problems. In this
approach, goals and observations are represented as a sequence of embeddings,
and a policy network initialized with a pre-trained LM predicts the next
action. We demonstrate that this framework enables effective combinatorial
generalization across different environments and supervisory modalities. We
begin by assuming access to a set of expert demonstrations, and show that
initializing policies with LMs and fine-tuning them via behavior cloning
improves task completion rates by 43.6% in the VirtualHome environment. Next,
we integrate an active data gathering procedure in which agents iteratively
interact with the environment, relabel past "failed" experiences with new
goals, and update their policies in a self-supervised loop. Active data
gathering further improves combinatorial generalization, outperforming the best
baseline by 25.1%. Finally, we explain these results by investigating three
possible factors underlying the effectiveness of the LM-based policy. We find
that sequential input representations (vs. fixed-dimensional feature vectors)
and LM-based weight initialization are both important for generalization.
Surprisingly, however, the format of the policy inputs encoding (e.g. as a
natural language string vs. an arbitrary sequential encoding) has little
influence. Together, these results suggest that language modeling induces
representations that are useful for modeling not just language, but also goals
and plans; these representations can aid learning and generalization even
outside of language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13363">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a variational autoencoder with disentanglement
priors, VAE-DPRIOR, for task-specific natural language generation with none or
a handful of task-specific labeled examples. In order to tackle compositional
generalization across tasks, our model performs disentangled representation
learning by introducing a conditional prior for the latent content space and
another conditional prior for the latent label space. Both types of priors
satisfy a novel property called $\epsilon$-disentangled. We show both
empirically and theoretically that the novel priors can disentangle
representations even without specific regularizations as in the prior work. The
content prior enables directly sampling diverse content representations from
the content space learned from the seen tasks, and fuse them with the
representations of novel tasks for generating semantically diverse texts in the
low-resource settings. Our extensive experiments demonstrate the superior
performance of our model over competitive baselines in terms of i) data
augmentation in continuous zero/few-shot learning, and ii) text style transfer
in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation. (arXiv:2203.09553v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09553">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Co-Training for Learning Discrete Speech Representations. (arXiv:2203.15840v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15840">
<div class="article-summary-box-inner">
<span><p>While several self-supervised approaches for learning discrete speech
representation have been proposed, it is unclear how these seemingly similar
approaches relate to each other. In this paper, we consider a generative model
with discrete latent variables that learns a discrete representation for
speech. The objective of learning the generative model is formulated as
information-theoretic co-training. Besides the wide generality, the objective
can be optimized with several approaches, subsuming HuBERT-like training and
vector quantization for learning discrete representation. Empirically, we find
that the proposed approach learns discrete representation that is highly
correlated with phonetic units, more correlated than HuBERT-like training and
vector quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation. (arXiv:2204.13031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13031">
<div class="article-summary-box-inner">
<span><p>Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks adopted in language models (LMs) and variational
autoencoders (VAEs): 1) masked language model; 2) response generation; 3)
bag-of-words prediction; and 4) KL divergence reduction. We also add additional
parameters to model the turn structure in dialogs to improve the performance of
the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and
DSTC7-AVSD benchmarks for response generation. Experimental results show that
our model achieves the new state-of-the-art results on all these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02357">
<div class="article-summary-box-inner">
<span><p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual
knowledge, have recently been successfully applied to tasks such as information
retrieval, question answering, and recommendation system. Since most MKGs are
far from complete, extensive knowledge graph completion studies have been
proposed focusing on the multimodal entity, relation extraction and link
prediction. However, different tasks and modalities require changes to the
model architecture, and not all images/objects are relevant to text input,
which hinders the applicability to diverse real-world scenarios. In this paper,
we propose a hybrid transformer with multi-level fusion to address those
issues. Specifically, we leverage a hybrid transformer architecture with
unified input-output for diverse multimodal knowledge graph completion tasks.
Moreover, we propose multi-level fusion, which integrates visual and text
representation via coarse-grained prefix-guided interaction and fine-grained
correlation-aware fusion modules. We conduct extensive experiments to validate
that our MKGformer can obtain SOTA performance on four datasets of multimodal
link prediction, multimodal RE, and multimodal NER. Code is available in
https://github.com/zjunlp/MKGformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Literal and Implied Subquestions to Fact-check Complex Claims. (arXiv:2205.06938v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06938">
<div class="article-summary-box-inner">
<span><p>Verifying complex political claims is a challenging task, especially when
politicians use various tactics to subtly misrepresent the facts. Automatic
fact-checking systems fall short here, and their predictions like "half-true"
are not very useful in isolation, since we have no idea which parts of the
claim are true and which are not. In this work, we focus on decomposing a
complex claim into a comprehensive set of yes-no subquestions whose answers
influence the veracity of the claim. We present ClaimDecomp, a dataset of
decompositions for over 1000 claims. Given a claim and its verification
paragraph written by fact-checkers, our trained annotators write subquestions
covering both explicit propositions of the original claim and its implicit
facets, such as asking about additional political context that changes our view
of the claim's veracity. We study whether state-of-the-art models can generate
such subquestions, showing that these models generate reasonable questions to
ask, but predicting the comprehensive set of subquestions from the original
claim without evidence remains challenging. We further show that these
subquestions can help identify relevant evidence to fact-check the full claim
and derive the veracity through their answers, suggesting that they can be
useful pieces of a fact-checking pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Supervised Contrastive Learning for Fair Text Classification. (arXiv:2205.11485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11485">
<div class="article-summary-box-inner">
<span><p>Contrastive representation learning has gained much attention due to its
superior performance in learning representations from both image and sequential
data. However, the learned representations could potentially lead to
performance disparities in downstream tasks, such as increased silencing of
underrepresented groups in toxicity comment classification. In light of this
challenge, in this work, we study learning fair representations that satisfy a
notion of fairness known as equalized odds for text classification via
contrastive learning. Specifically, we first theoretically analyze the
connections between learning representations with a fairness constraint and
conditional supervised contrastive objectives, and then propose to use
conditional supervised contrastive objectives to learn fair representations for
text classification. We conduct experiments on two text datasets to demonstrate
the effectiveness of our approaches in balancing the trade-offs between task
performance and bias mitigation among existing baselines for text
classification. Furthermore, we also show that the proposed methods are stable
in different hyperparameter settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition. (arXiv:2205.11799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11799">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has recently become a common practice
in building NLP models for various tasks, especially few-shot tasks. We argue
that under the few-shot setting, formulating fine-tuning closer to the
pre-training objectives shall be able to unleash more benefits from the
pre-trained language models. In this work, we take few-shot named entity
recognition (NER) for a pilot study, where existing fine-tuning strategies are
much different from pre-training. We propose a novel few-shot fine-tuning
framework for NER, FFF-NER. Specifically, we introduce three new types of
tokens, "is-entity", "which-type" and bracket, so we can formulate the NER
fine-tuning as (masked) token prediction or generation, depending on the choice
of pre-trained language models. In our experiments, we apply FFF-NER to
fine-tune both BERT and BART for few-shot NER on several benchmark datasets and
observe significant improvements over existing fine-tuning strategies,
including sequence labeling, prototype meta-learning, and prompt-based
approaches. We further perform a series of ablation studies, showing few-shot
NER performance is strongly correlated with the similarity between fine-tuning
and pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeDef: Weakly Supervised Backdoor Defense for Text Classification. (arXiv:2205.11803v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11803">
<div class="article-summary-box-inner">
<span><p>Existing backdoor defense methods are only effective for limited trigger
types. To defend different trigger types at once, we start from the
class-irrelevant nature of the poisoning process and propose a novel weakly
supervised backdoor defense framework WeDef. Recent advances in weak
supervision make it possible to train a reasonably accurate text classifier
using only a small number of user-provided, class-indicative seed words. Such
seed words shall be considered independent of the triggers. Therefore, a weakly
supervised text classifier trained by only the poisoned documents without their
labels will likely have no backdoor. Inspired by this observation, in WeDef, we
define the reliability of samples based on whether the predictions of the weak
classifier agree with their labels in the poisoned training set. We further
improve the results through a two-phase sanitization: (1) iteratively refine
the weak classifier based on the reliable samples and (2) train a binary poison
classifier by distinguishing the most unreliable samples from the most reliable
samples. Finally, we train the sanitized model on the samples that the poison
classifier predicts as benign. Extensive experiments show that WeDefis
effective against popular trigger-based attacks (e.g., words, sentences, and
paraphrases), outperforming existing defense methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12393">
<div class="article-summary-box-inner">
<span><p>Recent work on large language models relies on the intuition that most
natural language processing tasks can be described via natural language
instructions. Language models trained on these instructions show strong
zero-shot performance on several standard datasets. However, these models even
though impressive still perform poorly on a wide range of tasks outside of
their respective training and evaluation sets. To address this limitation, we
argue that a model should be able to keep extending its knowledge and
abilities, without forgetting previous skills. In spite of the limited success
of Continual Learning we show that Language Models can be continual learners.
We empirically investigate the reason for this success and conclude that
Continual Learning emerges from self-supervision pre-training. Our resulting
model Continual-T0 (CT0) is able to learn diverse new tasks, while still
maintaining good performance on previous tasks, spanning remarkably through 70
datasets in total. Finally, we show that CT0 is able to combine instructions in
ways it was never trained for, demonstrating some compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Locality in Abstractive Text Summarization. (arXiv:2205.12476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12476">
<div class="article-summary-box-inner">
<span><p>Neural attention models have achieved significant improvements on many
natural language processing tasks. However, the quadratic memory complexity of
the self-attention module with respect to the input length hinders their
applications in long text summarization. Instead of designing more efficient
attention modules, we approach this problem by investigating if models with a
restricted context can have competitive performance compared with the
memory-efficient attention models that maintain a global context by treating
the input as a single sequence. Our model is applied to individual pages which
contain parts of inputs grouped by the principle of locality during both
encoding and decoding. We empirically investigated three kinds of locality in
text summarization at different levels of granularity, ranging from sentences
to documents. Our experimental results show that our model has a better
performance compared with strong baselines with efficient attention modules,
and our analysis provides further insights into our locality-aware modeling
strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Language-neutral Sub-networks in Multilingual Language Models. (arXiv:2205.12672v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12672">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained language models transfer remarkably well on
cross-lingual downstream tasks. However, the extent to which they learn
language-neutral representations (i.e., shared representations that encode
similar phenomena across languages), and the effect of such representations on
cross-lingual transfer performance, remain open questions. In this work, we
conceptualize language neutrality of multilingual models as a function of the
overlap between language-encoding sub-networks of these models. We employ the
lottery ticket hypothesis to discover sub-networks that are individually
optimized for various languages and tasks. Our evaluation across three distinct
tasks and eleven typologically-diverse languages demonstrates that sub-networks
for different languages are topologically similar (i.e., language-neutral),
making them effective initializations for cross-lingual transfer with limited
performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06522">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that can reduce training memory requirements by
more substantial amounts. Unlike existing parameter-efficient methods that
insert additional parameters inside backbone networks, we train a ladder side
network, a small and separate network that takes intermediate activations as
input via shortcut connections (called ladders) from backbone networks and
makes predictions. LST has significantly lower memory requirements than
previous methods, because it does not require backpropagation through the
backbone network, but instead only through the side network and ladder
connections. We evaluate our method with various models (T5 and CLIP-T5) on
both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST
saves 69% of the memory costs to fine-tune the whole network, while other
methods only save 26% of that in similar parameter usages (hence, 2.7x more
memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA
in a low-memory regime. To further show the advantage of this better memory
efficiency, we also apply LST to larger T5 models, attaining better GLUE
performance than full fine-tuning and other PETL methods. The
accuracy-efficiency trade-off also holds on VL tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06565">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained language models (LMs) without making any architectural
changes has become a norm for learning various language downstream tasks.
However, for non-language downstream tasks, a common practice is to employ
task-specific designs for input, output layers, and loss functions. For
instance, it is possible to fine-tune an LM into an MNIST classifier by
replacing the word embedding layer with an image patch embedding layer, the
word token output layer with a 10-way output layer, and the word prediction
loss with a 10-way classification loss, respectively. A natural question
arises: Can LM fine-tuning solve non-language downstream tasks without changing
the model architecture or loss function? To answer this, we propose
Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations
by conducting an extensive empirical study on a suite of non-language
classification and regression tasks. LIFT does not make any changes to the
model architecture or loss function, and it solely relies on the natural
language interface, enabling "no-code machine learning with LMs." We find that
LIFT performs comparably well across a wide range of low-dimensional
classification and regression tasks, matching the performances of the best
baselines in many cases, especially for the classification tasks. We also
report experimental results on the fundamental properties of LIFT, including
inductive bias, robustness, and sample complexity. We also analyze the effect
of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g.,
context-aware learning via appropriate prompting, calibrated predictions, data
generation, and two-stage fine-tuning. Our code is available at
https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10498">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs) have transformed the field of
natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art
performance on natural language tasks is being pushed forward with every new
large language model. Along with natural language abilities, there has been a
significant interest in understanding whether such models exhibit reasoning
capabilities with the use of reasoning benchmarks. However, even though results
are seemingly positive, these benchmarks prove to be simplistic in nature and
the performance of LLMs on these benchmarks cannot be used as evidence to
support, many a times outlandish, claims being made about LLMs' reasoning
capabilities. Further, these only represent a very limited set of simple
reasoning tasks and we need to look at more sophisticated reasoning problems if
we are to measure the true limits of such LLM-based systems. Motivated by this,
we propose an extensible assessment framework to test the capabilities of LLMs
on reasoning about actions and change, a central aspect of human intelligence.
We provide multiple test cases that are more involved than any of the
previously established benchmarks and each test case evaluates a different
aspect of reasoning about actions and change. Results on GPT-3 (davinci),
Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance
on such reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Principal Phrase Mining. (arXiv:2206.13748v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13748">
<div class="article-summary-box-inner">
<span><p>Extracting frequent words from a collection of texts is commonly performed in
many subjects. However, as useful as it is to obtain a collection of commonly
occurring words from texts, there is a need for more specific information to be
obtained from texts in the form of most commonly occurring phrases. Despite
this need, extracting frequent phrases is not commonly done due to inherent
complications, the most significant being double-counting. Double-counting
occurs when words or phrases are counted when they appear inside longer phrases
that themselves are also counted, resulting in a selection of mostly
meaningless phrases that are frequent only because they occur inside frequent
super phrases. Several papers have been written on phrase mining that describe
solutions to this issue; however, they either require a list of so-called
quality phrases to be available to the extracting process, or they require
human interaction to identify those quality phrases during the process. We
present here a method that eliminates double-counting via a unique
rectification process that does not require lists of quality phrases. In the
context of a set of texts, we define a principal phrase as a phrase that does
not cross punctuation marks, does not start with a stop word, with the
exception of the stop words "not" and "no", does not end with a stop word, is
frequent within those texts without being double counted, and is meaningful to
the user. Our method identifies such principal phrases independently without
human input, and enables their extraction from any texts within a reasonable
amount of time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14382">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that are self-supervised and can be adapted with fine
tuning to a wide range of natural language tasks, each of which previously
would have required a separate network model. This is one step closer to the
extraordinary versatility of human language. GPT-3 and more recently LaMDA can
carry on dialogs with humans on many topics after minimal priming with a few
examples. However, there has been a wide range of reactions and debate on
whether these LLMs understand what they are saying or exhibit signs of
intelligence. This high variance is exhibited in three interviews with LLMs
reaching wildly different conclusions. A new possibility was uncovered that
could explain this divergence. What appears to be intelligence in LLMs may in
fact be a mirror that reflects the intelligence of the interviewer, a
remarkable twist that could be considered a Reverse Turing Test. If so, then by
studying interviews we may be learning more about the intelligence and beliefs
of the interviewer than the intelligence of the LLMs. As LLMs become more
capable they may transform the way we interact with machines and how they
interact with each other. Increasingly, LLMs are being coupled with
sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A
road map for achieving artificial general autonomy is outlined with seven major
improvements inspired by brain systems. LLMs could be used to uncover new
insights into brain function by downloading brain data during natural
behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours. (arXiv:2208.01483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01483">
<div class="article-summary-box-inner">
<span><p>Text classification can be useful in many real-world scenarios, saving a lot
of time for end users. However, building a custom classifier typically requires
coding skills and ML knowledge, which poses a significant barrier for many
potential users. To lift this barrier, we introduce Label Sleuth, a free open
source system for labeling and creating text classifiers. This system is unique
for (a) being a no-code system, making NLP accessible to non-experts, (b)
guiding users through the entire labeling process until they obtain a custom
classifier, making the process efficient -- from cold start to classifier in a
few hours, and (c) being open for configuration and extension by developers. By
open sourcing Label Sleuth we hope to build a community of users and developers
that will broaden the utilization of NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning. (arXiv:2208.13077v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.13077">
<div class="article-summary-box-inner">
<span><p>We propose a recommendation system that suggests treatment strategies to a
therapist during the psychotherapy session in real-time. Our system uses a
turn-level rating mechanism that predicts the therapeutic outcome by computing
a similarity score between the deep embedding of a scoring inventory, and the
current sentence that the patient is speaking. The system automatically
transcribes a continuous audio stream and separates it into turns of the
patient and of the therapist and perform real-time inference of their
therapeutic working alliance. The dialogue pairs along with their computed
working alliance as ratings are then fed into a deep reinforcement learning
recommendation system where the sessions are treated as users and the topics
are treated as items. Other than evaluating the empirical advantages of the
core components on an existing dataset of psychotherapy sessions, we
demonstrate the effectiveness of this system in a web app.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual and Cross-Domain Crisis Classification for Low-Resource Scenarios. (arXiv:2209.02139v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.02139">
<div class="article-summary-box-inner">
<span><p>Social media data has emerged as a useful source of timely information about
real-world crisis events. One of the main tasks related to the use of social
media for disaster management is the automatic identification of crisis-related
messages. Most of the studies on this topic have focused on the analysis of
data for a particular type of event in a specific language. This limits the
possibility of generalizing existing approaches because models cannot be
directly applied to new types of events or other languages. In this work, we
study the task of automatically classifying messages that are related to crisis
events by leveraging cross-language and cross-domain labeled data. Our goal is
to make use of labeled data from high-resource languages to classify messages
from other (low-resource) languages and/or of new (previously unseen) types of
crisis situations. For our study we consolidated from the literature a large
unified dataset containing multiple crisis events and languages. Our empirical
findings show that it is indeed possible to leverage data from crisis events in
English to classify the same type of event in other languages, such as Spanish
and Italian (80.0% F1-score). Furthermore, we achieve good performance for the
cross-domain task (80.0% F1-score) in a cross-lingual setting. Overall, our
work contributes to improving the data scarcity problem that is so important
for multilingual crisis classification. In particular, mitigating cold-start
situations in emergency events, when time is of essence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Scene-based Topic Channel Construction System for E-Commerce. (arXiv:2210.02643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02643">
<div class="article-summary-box-inner">
<span><p>Scene marketing that well demonstrates user interests within a certain
scenario has proved effective for offline shopping. To conduct scene marketing
for e-commerce platforms, this work presents a novel product form, scene-based
topic channel which typically consists of a list of diverse products belonging
to the same usage scenario and a topic title that describes the scenario with
marketing words. As manual construction of channels is time-consuming due to
billions of products as well as dynamic and diverse customers' interests, it is
necessary to leverage AI techniques to automatically construct channels for
certain usage scenarios and even discover novel topics. To be specific, we
first frame the channel construction task as a two-step problem, i.e.,
scene-based topic generation and product clustering, and propose an E-commerce
Scene-based Topic Channel construction system (i.e., ESTC) to achieve automated
production, consisting of scene-based topic generation model for the e-commerce
domain, product clustering on the basis of topic similarity, as well as quality
control based on automatic model filtering and human screening. Extensive
offline experiments and online A/B test validates the effectiveness of such a
novel product form as well as the proposed system. In addition, we also
introduce the experience of deploying the proposed system on a real-world
e-commerce recommendation platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Visual Question Answering with Outside Knowledge. (arXiv:2210.03809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03809">
<div class="article-summary-box-inner">
<span><p>Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA
task that requires retrieval of external knowledge to answer questions about
images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve
documents from external knowledge bases, such as Wikipedia, but with DPR
trained separately from answer generation, introducing a potential limit on the
overall system performance. Instead, we propose a joint training scheme which
includes differentiable DPR integrated with answer generation so that the
system can be trained in an end-to-end fashion. Our experiments show that our
scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also
introduce new diagnostic metrics to analyze how retrieval and generation
interact. The strong retrieval ability of our model significantly reduces the
number of retrieved documents needed in training, yielding significant benefits
in answer quality and computation required for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Soft and Hard Target RNN-T Distillation for Large-scale ASR. (arXiv:2210.05793v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05793">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an effective machine learning technique to transfer
knowledge from a teacher model to a smaller student model, especially with
unlabeled data. In this paper, we focus on knowledge distillation for the RNN-T
model, which is widely used in state-of-the-art (SoTA) automatic speech
recognition (ASR). Specifically, we compared using soft and hard target
distillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight
public dataset (60k hours) and our in-house data (600k hours). We found that
hard tar-gets are more effective when the teacher and student have different
architecture, such as large teacher and small streaming student. On the other
hand, soft target distillation works better in self-training scenario like
iterative large teacher training. For a large model with0.6B weights, we
achieve a new SoTA word error rate (WER) on LibriSpeech (8% relative
improvement on dev-other) using Noisy Student Training with soft target
distillation. It also allows our production teacher to adapt new data domain
continuously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HashFormers: Towards Vocabulary-independent Pre-trained Transformers. (arXiv:2210.07904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07904">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models are vocabulary-dependent,
mapping by default each token to its corresponding embedding. This one-to-one
mapping results into embedding matrices that occupy a lot of memory (i.e.
millions of parameters) and grow linearly with the size of the vocabulary.
Previous work on on-device transformers dynamically generate token embeddings
on-the-fly without embedding matrices using locality-sensitive hashing over
morphological information. These embeddings are subsequently fed into
transformer layers for text classification. However, these methods are not
pre-trained. Inspired by this line of work, we propose HashFormers, a new
family of vocabulary-independent pre-trained transformers that support an
unlimited vocabulary (i.e. all possible tokens in a corpus) given a
substantially smaller fixed-sized embedding matrix. We achieve this by first
introducing computationally cheap hashing functions that bucket together
individual tokens to embeddings. We also propose three variants that do not
require an embedding matrix at all, further reducing the memory requirements.
We empirically demonstrate that HashFormers are more memory efficient compared
to standard pre-trained transformers while achieving comparable predictive
performance when fine-tuned on multiple text classification tasks. For example,
our most efficient HashFormer variant has a negligible performance degradation
(0.4\% on GLUE) using only 99.1K parameters for representing the embeddings
compared to 12.3-38M parameters of state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation. (arXiv:2210.08182v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08182">
<div class="article-summary-box-inner">
<span><p>Unsupervised representation learning for speech audios attained impressive
performances for speech recognition tasks, particularly when annotated speech
is limited. However, the unsupervised paradigm needs to be carefully designed
and little is known about what properties these representations acquire. There
is no guarantee that the model learns meaningful representations for valuable
information for recognition. Moreover, the adaptation ability of the learned
representations to other domains still needs to be estimated. In this work, we
explore learning domain-invariant representations via a direct mapping of
speech representations to their corresponding high-level linguistic
informations. Results prove that the learned latents not only capture the
articulatory feature of each phoneme but also enhance the adaptation ability,
outperforming the baseline largely on accented benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning. (arXiv:2210.08634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08634">
<div class="article-summary-box-inner">
<span><p>We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging. (arXiv:2210.09840v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09840">
<div class="article-summary-box-inner">
<span><p>Part-of-Speech (POS) tagging is an important component of the NLP pipeline,
but many low-resource languages lack labeled data for training. An established
method for training a POS tagger in such a scenario is to create a labeled
training set by transferring from high-resource languages. In this paper, we
propose a novel method for transferring labels from multiple high-resource
source to low-resource target languages. We formalize POS tag projection as
graph-based label propagation. Given translations of a sentence in multiple
languages, we create a graph with words as nodes and alignment links as edges
by aligning words for all language pairs. We then propagate node labels from
source to target using a Graph Neural Network augmented with transformer
layers. We show that our propagation creates training sets that allow us to
train POS taggers for a diverse set of languages. When combined with enhanced
contextualized embeddings, our method achieves a new state-of-the-art for
unsupervised POS tagging of low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation. (arXiv:2210.10349v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10349">
<div class="article-summary-box-inner">
<span><p>Symbolic music generation aims to generate music scores automatically. A
recent trend is to use Transformer or its variants in music generation, which
is, however, suboptimal, because the full attention cannot efficiently model
the typically long music sequences (e.g., over 10,000 tokens), and the existing
models have shortcomings in generating musical repetition structures. In this
paper, we propose Museformer, a Transformer with a novel fine- and
coarse-grained attention for music generation. Specifically, with the
fine-grained attention, a token of a specific bar directly attends to all the
tokens of the bars that are most relevant to music structures (e.g., the
previous 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with
the coarse-grained attention, a token only attends to the summarization of the
other bars rather than each token of them so as to reduce the computational
cost. The advantages are two-fold. First, it can capture both music
structure-related correlations via the fine-grained attention, and other
contextual information via the coarse-grained attention. Second, it is
efficient and can model over 3X longer music sequences compared to its
full-attention counterpart. Both objective and subjective experimental results
demonstrate its ability to generate long music sequences with high quality and
better structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11498">
<div class="article-summary-box-inner">
<span><p>Traditional (fickle) adversarial examples involve finding a small
perturbation that does not change an input's true label but confuses the
classifier into outputting a different prediction. Conversely, obstinate
adversarial examples occur when an adversary finds a small perturbation that
preserves the classifier's prediction but changes the true label of an input.
Adversarial training and certified robust training have shown some
effectiveness in improving the robustness of machine learnt models to fickle
adversarial examples. We show that standard adversarial training methods
focused on reducing vulnerability to fickle adversarial examples may make a
model more vulnerable to obstinate adversarial examples, with experiments for
both natural language inference and paraphrase identification tasks. To counter
this phenomenon, we introduce Balanced Adversarial Training, which incorporates
contrastive learning to increase robustness against both fickle and obstinate
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design a Sustainable Micro-mobility Future: Trends and Challenges in the United States and European Union Using Natural Language Processing Techniques. (arXiv:2210.11714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11714">
<div class="article-summary-box-inner">
<span><p>Micro-mobility is promising to contribute to sustainable cities in the future
with its efficiency and low cost. To better design such a sustainable future,
it is necessary to understand the trends and challenges. Thus, we examined
people's opinions on micro-mobility in the US and the EU using Tweets. We used
topic modeling based on advanced natural language processing techniques and
categorized the data into seven topics: promotion and service, mobility,
technical features, acceptance, recreation, infrastructure and regulations.
Furthermore, using sentiment analysis, we investigated people's positive and
negative attitudes towards specific aspects of these topics and compared the
patterns of the trends and challenges in the US and the EU. We found that 1)
promotion and service included the majority of Twitter discussions in the both
regions, 2) the EU had more positive opinions than the US, 3) micro-mobility
devices were more widely used for utilitarian mobility and recreational
purposes in the EU than in the US, and 4) compared to the EU, people in the US
had many more concerns related to infrastructure and regulation issues. These
findings help us understand the trends and challenges and prioritize different
aspects in micro-mobility to improve their safety and experience across the two
areas for designing a more sustainable micro-mobility future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms. (arXiv:2210.11905v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11905">
<div class="article-summary-box-inner">
<span><p>Prominent questions about the role of sensory vs. linguistic input in the way
we acquire and use language have been extensively studied in the
psycholinguistic literature. However, the relative effect of various factors in
a person's overall experience on their linguistic system remains unclear. We
study this question by making a step forward towards a better understanding of
the conceptual perception of colors by color-blind individuals, as reflected in
their spontaneous linguistic productions. Using a novel and carefully curated
dataset, we show that red-green color-blind speakers use the "red" and "green"
color terms in less predictable contexts, and in linguistic environments
evoking mental image to a lower extent, when compared to their normal-sighted
counterparts. These findings shed some new and interesting light on the role of
sensory experience on our linguistic system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12273">
<div class="article-summary-box-inner">
<span><p>Since its original appearance in 1991, the Perso-Arabic script representation
in Unicode has grown from 169 to over 440 atomic isolated characters spread
over several code pages representing standard letters, various diacritics and
punctuation for the original Arabic and numerous other regional orthographic
traditions. This paper documents the challenges that Perso-Arabic presents
beyond the best-documented languages, such as Arabic and Persian, building on
earlier work by the expert community. We particularly focus on the situation in
natural language processing (NLP), which is affected by multiple, often
neglected, issues such as the use of visually ambiguous yet canonically
nonequivalent letters and the mixing of letters from different orthographies.
Among the contributing conflating factors are the lack of input methods, the
instability of modern orthographies, insufficient literacy, and loss or lack of
orthographic tradition. We evaluate the effects of script normalization on
eight languages from diverse language families in the Perso-Arabic script
diaspora on machine translation and statistical language modeling tasks. Our
results indicate statistically significant improvements in performance in most
conditions for all the languages considered when normalization is applied. We
argue that better understanding and representation of Perso-Arabic script
variation within regional orthographic traditions, where those are present, is
crucial for further progress of modern computational NLP techniques especially
for languages with a paucity of resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling. (arXiv:2210.12378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12378">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization models often generate inconsistent summaries
containing factual errors or hallucinated content. Recent works focus on
correcting factual errors in generated summaries via post-editing. Such
correction models are trained using adversarial non-factual summaries
constructed using heuristic rules for injecting errors. However, generating
non-factual summaries using heuristics often does not generalize well to actual
model errors. In this work, we propose to generate hard, representative
synthetic examples of non-factual summaries through infilling language models.
With this data, we train a more robust fact-correction model to post-edit the
summaries to improve factual consistency. Through quantitative and qualitative
experiments on two popular summarization datasets -- CNN/DM and XSum -- we show
that our approach vastly outperforms prior methods in correcting erroneous
summaries. Our model -- FactEdit -- improves factuality scores by over ~11
points on CNN/DM and over ~31 points on XSum on average across multiple
summarization models, producing more factual summaries while maintaining
competitive summarization quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12770">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have been deployed in many natural
language processing (NLP) tasks and in various domains. Language model
pre-training from general or mixed domain rich data plus fine-tuning using
small amounts of available data in a low resource domain demonstrated
beneficial results by researchers. In this work, we question this statement and
verify if BERT-based PLMs from the biomedical domain can perform well in
clinical text mining tasks via fine-tuning. We test the state-of-the-art
models, i.e. Bioformer which is pre-trained on a large amount of biomedical
data from PubMed corpus. We use a historical n2c2 clinical NLP challenge
dataset for fine-tuning its task-adapted version (BioformerApt), and show that
their performances are actually very low. We also present our own end-to-end
model, TransformerCRF, which is developed using Transformer and conditional
random fields (CRFs) as encoder and decoder. We further create a new variation
model by adding a CRF layer on top of PLM Bioformer (BioformerCRF). We
investigate the performances of TransformerCRF on clinical text mining tasks by
training from scratch using a limited amount of data, as well as the model
BioformerCRF. Experimental evaluation shows that, in a \textit{constrained
setting}, all tested models are \textit{far from ideal} regarding extreme
low-frequency special token recognition, even though they can achieve
relatively higher accuracy on overall text tagging. Our models including source
codes will be hosted at \url{https://github.com/poethan/TransformerCRF}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks. (arXiv:2210.12786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12786">
<div class="article-summary-box-inner">
<span><p>Humans can reason compositionally whilst grounding language utterances to the
real world. Recent benchmarks like ReaSCAN use navigation tasks grounded in a
grid world to assess whether neural models exhibit similar capabilities. In
this work, we present a simple transformer-based model that outperforms
specialized architectures on ReaSCAN and a modified version of gSCAN. On
analyzing the task, we find that identifying the target location in the grid
world is the main challenge for the models. Furthermore, we show that a
particular split in ReaSCAN, which tests depth generalization, is unfair. On an
amended version of this split, we show that transformers can generalize to
deeper input structures. Finally, we design a simpler grounded compositional
generalization task, RefEx, to investigate how transformers reason
compositionally. We show that a single self-attention layer with a single head
generalizes to novel combinations of object attributes. Moreover, we derive a
precise mathematical construction of the transformer's computations from the
learned network. Overall, we provide valuable insights about the grounded
compositional generalization task and the behaviour of transformers on it,
which would be useful for researchers working in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Automated Essay Scoring using Transformer Models. (arXiv:2210.12809v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12809">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring is one of the most important problem in Natural
Language Processing. It has been explored for a number of years, and it remains
partially solved. In addition to its economic and educational usefulness, it
presents research problems. Transfer learning has proved to be beneficial in
NLP. Data augmentation techniques have also helped build state-of-the-art
models for automated essay scoring. Many works in the past have attempted to
solve this problem by using RNNs, LSTMs, etc. This work examines the
transformer models like BERT, RoBERTa, etc. We empirically demonstrate the
effectiveness of transformer models and data augmentation for automated essay
grading across many topics using a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Information Alleviates Hallucinations in Abstractive Summarization. (arXiv:2210.13210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13210">
<div class="article-summary-box-inner">
<span><p>Despite significant progress in the quality of language generated from
abstractive summarization models, these models still exhibit the tendency to
hallucinate, i.e., output content not supported by the source document. A
number of works have tried to fix--or at least uncover the source of--the
problem with limited success. In this paper, we identify a simple criterion
under which models are significantly more likely to assign more probability to
hallucinated content during generation: high model uncertainty. This finding
offers a potential explanation for hallucinations: models default to favoring
text with high marginal probability, i.e., high-frequency occurrences in the
training set, when uncertain about a continuation. It also motivates possible
routes for real-time intervention during decoding to prevent such
hallucinations. We propose a decoding strategy that switches to optimizing for
pointwise mutual information of the source and target token--rather than purely
the probability of the target token--when the model exhibits uncertainty.
Experiments on the XSum dataset show that our method decreases the probability
of hallucinated tokens while maintaining the Rouge and BertS scores of
top-performing decoding strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation. (arXiv:2210.13832v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13832">
<div class="article-summary-box-inner">
<span><p>Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14061">
<div class="article-summary-box-inner">
<span><p>This study is devoted to two of the oldest known manuscripts in which the
oeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,
KBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of
codicological and contextual arguments, it is assumed that the scribe who
produced B used A as an exemplar. While the similarities in both layout and
content between the two manuscripts are striking, the present article seeks to
identify the differences. After all, regardless of the intention to produce a
copy that closely follows the exemplar, subtle linguistic variation is
apparent. Divergences relate to spelling conventions, but also to the way in
which words are abbreviated (and the extent to which abbreviations occur). The
present study investigates the spelling profiles of the scribes who produced
mss. A and B in a computational way. In the first part of this study, we will
present both manuscripts in more detail, after which we will consider prior
research carried out on scribal profiling. The current study both builds and
expands on Kestemont (2015). Next, we outline the methodology used to analyse
and measure the degree of scribal appropriation that took place when ms. B was
copied off the exemplar ms. A. After this, we will discuss the results
obtained, focusing on the scribal variation that can be found both at the level
of individual words and n-grams. To this end, we use machine learning to
identify the most distinctive features that separate manuscript A from B.
Finally, we look at possible diachronic trends in the appropriation by B's
scribe of his exemplar. We argue that scribal takeovers in the exemplar impacts
the practice of the copying scribe, while transitions to a different content
matter cause little to no effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14140">
<div class="article-summary-box-inner">
<span><p>Generating text with autoregressive language models (LMs) is of great
importance to many natural language processing (NLP) applications. Previous
solutions for this task often produce text that contains degenerative
expressions or lacks semantic consistency. Recently, Su et al. introduced a new
decoding method, contrastive search, based on the isotropic representation
space of the language model and obtained new state of the art on various
benchmarks. Additionally, Su et al. argued that the representations of
autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also
shared by previous studies. Therefore, to ensure the language model follows an
isotropic distribution, Su et al. proposed a contrastive learning scheme,
SimCTG, which calibrates the language model's representations through
additional training.
</p>
<p>In this study, we first answer the question: "Are autoregressive LMs really
anisotropic?". To this end, we extensively evaluate the isotropy of LMs across
16 major languages. Surprisingly, we find that the anisotropic problem only
exists in the two specific English GPT-2-small/medium models. On the other
hand, all other evaluated LMs are naturally isotropic which is in contrast to
the conclusion drawn by previous studies. Based on our findings, we further
assess the contrastive search decoding method using off-the-shelf LMs on four
generation tasks across 16 languages. Our experimental results demonstrate that
contrastive search significantly outperforms previous decoding methods without
any additional training. More notably, on 12 out of the 16 evaluated languages,
contrastive search performs comparably with human-level performances as judged
by human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">arXivEdits: Understanding the Human Revision Process in Scientific Writing. (arXiv:2210.15067v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15067">
<div class="article-summary-box-inner">
<span><p>Scientific publications are the primary means to communicate research
discoveries, where the writing quality is of crucial importance. However, prior
work studying the human editing process in this domain mainly focused on the
abstract or introduction sections, resulting in an incomplete picture. In this
work, we provide a complete computational framework for studying text revision
in scientific writing. We first introduce arXivEdits, a new annotated corpus of
751 full papers from arXiv with gold sentence alignment across their multiple
versions of revision, as well as fine-grained span-level edits and their
underlying intentions for 1,000 sentence pairs. It supports our data-driven
analysis to unveil the common strategies practiced by researchers for revising
their papers. To scale up the analysis, we also develop automatic methods to
extract revision at document-, sentence-, and word-levels. A neural CRF
sentence alignment model trained on our corpus achieves 93.8 F1, enabling the
reliable matching of sentences between different versions. We formulate the
edit extraction task as a span alignment problem, and our proposed method
extracts more fine-grained and explainable edits, compared to the commonly used
diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1
on the fine-grained intent classification task. Our data and system are
released at tiny.one/arxivedits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Language-driven Scientific AI. (arXiv:2210.15327v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15327">
<div class="article-summary-box-inner">
<span><p>Inspired by recent and revolutionary developments in AI, particularly in
language understanding and generation, we set about designing AI systems that
are able to address complex scientific tasks that challenge human capabilities
to make new discoveries. Central to our approach is the notion of natural
language as core representation, reasoning, and exchange format between
scientific AI and human scientists. In this paper, we identify and discuss some
of the main research challenges to accomplish such vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16031">
<div class="article-summary-box-inner">
<span><p>Diffusion generative models have recently greatly improved the power of
text-conditioned image generation. Existing image generation models mainly
include text conditional diffusion model and cross-modal guided diffusion
model, which are good at small scene image generation and complex scene image
generation respectively. In this work, we propose a simple yet effective
approach, namely UPainting, to unify simple and complex scene image generation,
as shown in Figure 1. Based on architecture improvements and diverse guidance
schedules, UPainting effectively integrates cross-modal guidance from a
pretrained image-text matching model into a text conditional diffusion model
that utilizes a pretrained Transformer language model as the text encoder. Our
key findings is that combining the power of large-scale Transformer language
model in understanding language and image-text matching model in capturing
cross-modal semantics and style, is effective to improve sample fidelity and
image-text alignment of image generation. In this way, UPainting has a more
general image generation capability, which can generate images of both simple
and complex scenes more effectively. To comprehensively compare text-to-image
models, we further create a more general benchmark, UniBench, with well-written
Chinese and English prompts in both simple and complex scenes. We compare
UPainting with recent models and find that UPainting greatly outperforms other
models in terms of caption similarity and image fidelity in both simple and
complex scenes.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-01 23:18:09.631224835 UTC">2022-11-01 23:18:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>