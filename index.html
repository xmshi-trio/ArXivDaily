<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-19T01:30:00Z">05-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback. (arXiv:2305.10433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10433">
<div class="article-summary-box-inner">
<span><p>Toxic language is difficult to define, as it is not monolithic and has many
variations in perceptions of toxicity. This challenge of detecting toxic
language is increased by the highly contextual and subjectivity of its
interpretation, which can degrade the reliability of datasets and negatively
affect detection model performance. To fill this void, this paper introduces a
toxicity inspector framework that incorporates a human-in-the-loop pipeline
with the aim of enhancing the reliability of toxicity benchmark datasets by
centering the evaluator's values through an iterative feedback cycle. The
centerpiece of this framework is the iterative feedback process, which is
guided by two metric types (hard and soft) that provide evaluators and dataset
creators with insightful examination to balance the tradeoff between
performance gains and toxicity avoidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10434">
<div class="article-summary-box-inner">
<span><p>Visual text evokes an image in a person's mind, while non-visual text fails
to do so. A method to automatically detect visualness in text will unlock the
ability to augment text with relevant images, as neural text-to-image
generation and retrieval models operate on the implicit assumption that the
input text is visual in nature. We curate a dataset of 3,620 English sentences
and their visualness scores provided by multiple human annotators.
Additionally, we use documents that contain text and visual assets to create a
distantly supervised corpus of document text and associated images. We also
propose a fine-tuning strategy that adapts large vision-language models like
CLIP that assume a one-to-one correspondence between text and image to the task
of scoring text visualness from text input alone. Our strategy involves
modifying the model's contrastive learning objective to map text identified as
non-visual to a common NULL image while matching visual text to their
corresponding images in the document. We evaluate the proposed approach on its
ability to (i) classify visual and non-visual text accurately, and (ii) attend
over words that are identified as visual in psycholinguistic studies. Empirical
evaluation indicates that our approach performs better than several heuristics
and baseline models for the proposed task. Furthermore, to highlight the
importance of modeling the visualness of text, we conduct qualitative analyses
of text-to-image generation systems like DALL-E.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10435">
<div class="article-summary-box-inner">
<span><p>The Generative Pre-trained Transformer models represent a notable
breakthrough in the domain of natural language processing, which is propelling
us toward the development of machines that can understand and communicate using
language in a manner that closely resembles that of humans. Generative
Pre-trained Transformer models are based on the transformer architecture, a
deep neural network designed for natural language processing tasks. Due to
their impressive performance on natural language processing tasks and ability
to effectively converse, Generative Pre-trained Transformer models have gained
significant popularity among researchers and industrial communities, making
them one of the most widely used and effective models in natural language
processing and related fields, which motivated to conduct this review. This
review provides a detailed overview of the Generative Pre-trained Transformer,
including its architecture, working process, training procedures, enabling
technologies, and its impact on various applications. In this review, we also
explored the potential challenges and limitations of a Generative Pre-trained
Transformer. Furthermore, we discuss potential solutions and future directions.
Overall, this paper aims to provide a comprehensive understanding of Generative
Pre-trained Transformers, enabling technologies, their impact on various
applications, emerging challenges, and potential solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues. (arXiv:2305.10436v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10436">
<div class="article-summary-box-inner">
<span><p>In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10438">
<div class="article-summary-box-inner">
<span><p>Word embeddings, i.e., semantically meaningful vector representation of
words, are largely influenced by the distributional hypothesis "You shall know
a word by the company it keeps" (Harris, 1954), whereas modern prediction-based
neural network embeddings rely on design choices and hyperparameter
optimization. Word embeddings like Word2Vec, GloVe etc. well capture the
contextuality and real-world analogies but contemporary convolution-based image
embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge.
The popular king-queen analogy does not hold true for most commonly used vision
embeddings.
</p>
<p>In this paper, we introduce a pre-trained joint embedding (JE), named
IMAGINATOR, trained on 21K distinct image objects level from 1M image+text
pairs. JE is a way to encode multimodal data into a vector space where the text
modality serves as the ground-ing key, which the complementary modality (in
this case, the image) is anchored with. IMAGINATOR encapsulates three
individual representations: (i) object-object co-location, (ii) word-object
co-location, and (iii) word-object correlation. These three ways capture
complementary aspects of the two modalities which are further combined to
obtain the final JEs.
</p>
<p>Generated JEs are intrinsically evaluated to assess how well they capture the
contextuality and real-world analogies. We also evaluate pre-trained IMAGINATOR
JEs on three downstream tasks: (i) image captioning, (ii) Image2Tweet, and
(iii) text-based image retrieval. IMAGINATOR establishes a new standard on the
aforementioned down-stream tasks by outperforming the current SoTA on all the
selected tasks. IMAGINATOR will be made publicly available. The codes are
available at https://github.com/varunakk/IMAGINATOR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10445">
<div class="article-summary-box-inner">
<span><p>Over-parameterized neural language models (LMs) can memorize and recite long
sequences of training data. While such memorization is normally associated with
undesired properties such as overfitting and information leaking, our work
casts memorization as an unexplored capability of LMs. We propose the first
symmetric encryption algorithm with autoregressive language models (SELM). We
show that autoregressive LMs can encode arbitrary data into a compact
real-valued vector (i.e., encryption) and then losslessly decode the vector to
the original message (i.e., decryption) via random subspace optimization and
greedy decoding. While SELM is not amenable to conventional cryptanalysis, we
investigate its security through a novel empirical variant of the classic
IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and
datasets are available at https://github.com/OSU-NLP-Group/SELM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation. (arXiv:2305.10446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10446">
<div class="article-summary-box-inner">
<span><p>Emotion regulation is a crucial element in dealing with emotional events and
has positive effects on mental health. This paper aims to provide a more
comprehensive understanding of emotional events by introducing a new French
corpus of emotional narratives collected using a questionnaire for emotion
regulation. We follow the theoretical framework of the Component Process Model
which considers emotions as dynamic processes composed of four interrelated
components (behavior, feeling, thinking and territory). Each narrative is
related to a discrete emotion and is structured based on all emotion components
by the writers. We study the interaction of components and their impact on
emotion classification with machine learning methods and pre-trained language
models. Our results show that each component improves prediction performance,
and that the best results are achieved by jointly considering all components.
Our results also show the effectiveness of pre-trained language models in
predicting discrete emotion from certain components, which reveal differences
in how emotion components are expressed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring. (arXiv:2305.10447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10447">
<div class="article-summary-box-inner">
<span><p>Neural networks and in particular the attention mechanism have brought
significant advances to the field of Automated Essay Scoring. Many of these
systems use a regression-based model which may be prone to underfitting when
the model only predicts the mean of the training data. In this paper, we
present a dynamic loss function that creates an incentive for the model to
predict with the correct distribution, as well as predicting the correct
values. Our loss function achieves this goal without sacrificing any
performance achieving a Quadratic Weighted Kappa score of 0.752 on the
Automated Student Assessment Prize Automated Essay Scoring dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10448">
<div class="article-summary-box-inner">
<span><p>This paper presents GenDoc, a general sequence-to-sequence document
understanding model pre-trained with unified masking across three modalities:
text, image, and layout. The proposed model utilizes an encoder-decoder
architecture, which allows for increased adaptability to a wide range of
downstream tasks with diverse output formats, in contrast to the encoder-only
models commonly employed in document understanding. In addition to the
traditional text infilling task used in previous encoder-decoder models, our
pre-training extends to include tasks of masked image token prediction and
masked layout prediction. We also design modality-specific instruction and
adopt both disentangled attention and the mixture-of-modality-experts strategy
to effectively capture the information leveraged by each modality. Evaluation
of the proposed model through extensive experiments on several downstream tasks
in document understanding demonstrates its ability to achieve superior or
competitive performance compared to state-of-the-art approaches. Our analysis
further suggests that GenDoc is more robust than the encoder-only models in
scenarios where the OCR quality is imperfect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10450">
<div class="article-summary-box-inner">
<span><p>Cardiac diseases are one of the leading mortality factors in modern,
industrialized societies, which cause high expenses in public health systems.
Due to high costs, developing analytical methods to improve cardiac diagnostics
is essential. The heart's electric activity was first modeled using a set of
nonlinear differential equations. Following this, variations of cardiac spectra
originating from deterministic dynamics are investigated. Analyzing a normal
human heart's power spectra offers His-Purkinje network, which possesses a
fractal-like structure. Phase space trajectories are extracted from the time
series electrocardiogram (ECG) graph with third-order derivate Taylor Series.
Here in this study, phase space analysis and Convolutional Neural Networks
(CNNs) method are applied to 44 records via the MIT-BIH database recorded with
MLII. In order to increase accuracy, a straight line is drawn between the
highest Q-R distance in the phase space images of the records. Binary CNN
classification is used to determine healthy or unhealthy hearts. With a 90.90%
accuracy rate, this model could classify records according to their heart
status.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10496">
<div class="article-summary-box-inner">
<span><p>Feature attribution methods (FAs) are popular approaches for providing
insights into the model reasoning process of making predictions. The more
faithful a FA is, the more accurately it reflects which parts of the input are
more important for the prediction. Widely used faithfulness metrics, such as
sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely
removing or retaining the top most important tokens ranked by a given FA and
observing the changes in predictive likelihood. However, this hard criterion
ignores the importance of each individual token, treating them all equally for
computing sufficiency and comprehensiveness. In this paper, we propose a simple
yet effective soft erasure criterion. Instead of entirely removing or retaining
tokens from the input, we randomly mask parts of the token vector
representations proportionately to their FA importance. Extensive experiments
across various natural language processing tasks and different FAs show that
our soft-sufficiency and soft-comprehensiveness metrics consistently prefer
more faithful explanations compared to hard sufficiency and comprehensiveness.
Our code: https://github.com/casszhao/SoftFaith
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages. (arXiv:2305.10510v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10510">
<div class="article-summary-box-inner">
<span><p>In this multicultural age, language translation is one of the most performed
tasks, and it is becoming increasingly AI-moderated and automated. As a novel
AI system, ChatGPT claims to be proficient in such translation tasks and in
this paper, we put that claim to the test. Specifically, we examine ChatGPT's
accuracy in translating between English and languages that exclusively use
gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most
spoken language globally, but also generalize our findings across five other
languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT
perpetuates gender defaults and stereotypes assigned to certain occupations
(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to
work), as it converts gender-neutral pronouns in languages to `he' or `she'. We
also observe ChatGPT completely failing to translate the English gender-neutral
pronoun `they' into equivalent gender-neutral pronouns in other languages, as
it produces translations that are incoherent and incorrect. While it does
respect and provide appropriately gender-marked versions of Bengali words when
prompted with gender information in English, ChatGPT appears to confer a higher
respect to men than to women in the same occupation. We conclude that ChatGPT
exhibits the same gender biases which have been demonstrated for tools like
Google Translate or MS Translator, as we provide recommendations for a human
centered approach for future designers of AIs that perform language translation
to better accommodate such low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10512">
<div class="article-summary-box-inner">
<span><p>Currently, dialogue systems have achieved high performance in processing
text-based communication. However, they have not yet effectively incorporated
visual information, which poses a significant challenge. Furthermore, existing
models that incorporate images in dialogue generation focus on discussing the
image itself. Our proposed approach presents a novel perspective on multi-modal
dialogue systems, which interprets the image in the context of the dialogue. By
doing so, we aim to expand the capabilities of current dialogue systems and
transition them from single modality (text) to multi-modality. However, there
is a lack of validated English datasets that contain both images and dialogue
contexts for this task. Thus, we propose a two-stage approach to automatically
construct a multi-modal dialogue dataset. In the first stage, we utilize
text-to-image similarity and sentence similarity to identify which utterances
could be replaced with an image. In the second stage, we replace those
utterances by selecting a subset of relevant images and filtering them with a
visual question answering model. We used this approach, along with additional
labeling, to create the IMage Augmented multi-modal Dialogue dataset (IMAD),
which can serve as a validated dataset for this task. Furthermore, we propose a
baseline model trained on this dataset, which outperforms model trained on the
same data without images and BlenderBot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10519">
<div class="article-summary-box-inner">
<span><p>Generative Language Models (GLMs) have demonstrated capabilities to store
factual knowledge and answer queries efficiently. Given varying prompts, does a
GLM consistently generate factually correct answers? In this paper, we
introduce a statistical knowledge assessment framework guided by latent
variables and the KaRR metric, which quantifies a model's knowledge by
computing its continuous probability across diverse text forms. We conduct a
comprehensive comparison of knowledge across 14 GLMs using our framework,
including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment
encompasses 600 relation types and exhibits a strong correlation (0.43
Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge
in GLMs with the same backbone architecture adheres to the scaling law, and
that tuning on instruction-following data may compromise the model's ability to
generate factually correct text consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10528">
<div class="article-summary-box-inner">
<span><p>Off-Policy reinforcement learning has been a driving force for the
state-of-the-art conversational AIs leading to more natural humanagent
interactions and improving the user satisfaction for goal-oriented agents.
However, in large-scale commercial settings, it is often challenging to balance
between policy improvements and experience continuity on the broad spectrum of
applications handled by such system. In the literature, off-policy evaluation
and guard-railing on aggregate statistics has been commonly used to address
this problem. In this paper, we propose a method for curating and leveraging
high-precision samples sourced from historical regression incident reports to
validate, safe-guard, and improve policies prior to the online deployment. We
conducted extensive experiments using data from a real-world conversational
system and actual regression incidents. The proposed method is currently
deployed in our production system to protect customers against broken
experiences and enable long-term policy improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10557">
<div class="article-summary-box-inner">
<span><p>Automatic postediting (APE) is an automated process to refine a given machine
translation (MT). Recent findings present that existing APE systems are not
good at handling high-quality MTs even for a language pair with abundant data
resources, English$\unicode{x2013}$German: the better the given MT is, the
harder it is to decide what parts to edit and how to fix these errors. One
possible solution to this problem is to instill deeper knowledge about the
target language into the model. Thus, we propose a linguistically motivated
method of regularization that is expected to enhance APE models' understanding
of the target language: a loss function that encourages symmetric
self-attention on the given MT. Our analysis of experimental results
demonstrates that the proposed method helps improving the state-of-the-art
architecture's APE quality for high-quality MTs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search. (arXiv:2305.10561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10561">
<div class="article-summary-box-inner">
<span><p>In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,
zero-shot event extraction system and accompanying user interface for event
visualization &amp; search. Using only English training data, ISI-Clear makes
global events available on-demand, processing user-supplied text in 100
languages ranging from Afrikaans to Yiddish. We provide multiple event-centric
views of extracted events, including both a graphical representation and a
document-level summary. We also integrate existing cross-lingual search
algorithms with event extraction capabilities to provide cross-lingual
event-centric search, allowing English-speaking users to search over events
automatically extracted from a corpus of non-English documents, using either
English natural language queries (e.g. cholera outbreaks in Iran) or structured
queries (e.g. find all events of type Disease-Outbreak with agent cholera and
location Iran).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10563">
<div class="article-summary-box-inner">
<span><p>The success of the knowledge graph completion task heavily depends on the
quality of the knowledge graph embeddings (KGEs), which relies on
self-supervised learning and augmenting the dataset with negative triples.
There is a gap in literature between the theoretical analysis of negative
samples on contrastive loss and heuristic generation of quality (i.e., hard)
negative triples. In this paper, we modify the InfoNCE loss to explicitly
account for the negative sample distribution. We show minimizing InfoNCE loss
with hard negatives maximizes the KL-divergence between the given and negative
triple embedding. However, we also show that hard negatives can lead to false
negatives (i.e., accidentally factual triples) and reduce downstream task
performance. To address this issue, we propose a novel negative sample
distribution that uses the graph structure of the knowledge graph to remove the
false negative triples. We call our algorithm Hardness and Structure-aware
(\textbf{HaSa}) contrastive KGE. Experiments show that our method outperforms
state-of-the-art KGE methods in several metrics for WN18RR and FB15k-237
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10568">
<div class="article-summary-box-inner">
<span><p>Noun compound interpretation is the task of expressing a noun compound (e.g.
chocolate bunny) in a free-text paraphrase that makes the relationship between
the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose
modifications to the data and evaluation setup of the standard task (Hendrickx
et al., 2013), and show that GPT-3 solves it almost perfectly. We then
investigate the task of noun compound conceptualization, i.e. paraphrasing a
novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped
chocolate. This task requires creativity, commonsense, and the ability to
generalize knowledge about similar concepts. While GPT-3's performance is not
perfect, it is better than that of humans -- likely thanks to its access to
vast amounts of knowledge, and because conceptual processing is effortful for
people (Connell and Lynott, 2012). Finally, we estimate the extent to which
GPT-3 is reasoning about the world vs. parroting its training data. We find
that the outputs from GPT-3 often have significant overlap with a large web
corpus, but that the parroting strategy is less beneficial for novel noun
compounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10588">
<div class="article-summary-box-inner">
<span><p>Estimating the log-likelihood of a given sentence under an autoregressive
language model is straightforward: one can simply apply the chain rule and sum
the log-likelihood values for each successive token. However, for masked
language models, there is no direct way to estimate the log-likelihood of a
sentence. To address this issue, Salazar et al. (2020) propose to estimate
sentence pseudo-log-likelihood (PLL) scores, computed by successively masking
each sentence token, retrieving its score using the rest of the sentence as
context, and summing the resulting values. Here, we demonstrate that the
original PLL method yields inflated scores for out-of-vocabulary words and
propose an adapted metric, in which we mask not only the target token, but also
all within-word tokens to the right of the target. We show that our adapted
metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric
in which all within-word tokens are masked. In particular, it better satisfies
theoretical desiderata and better correlates with scores from autoregressive
models. Finally, we show that the choice of metric affects even tightly
controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring
the importance of selecting an appropriate scoring metric for evaluating MLM
properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10601">
<div class="article-summary-box-inner">
<span><p>Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/ysymyth/tree-of-thought-llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10610">
<div class="article-summary-box-inner">
<span><p>Cosine similarity between two words, computed using their contextualised
token embeddings obtained from masked language models (MLMs) such as BERT has
shown to underestimate the actual similarity between those words (Zhou et al.,
2022). This similarity underestimation problem is particularly severe for
highly frequent words. Although this problem has been noted in prior work, no
solution has been proposed thus far. We observe that the L2 norm of
contextualised embeddings of a word correlates with its log-frequency in the
pretraining corpus. Consequently, the larger L2 norms associated with the
highly frequent words reduce the cosine similarity values measured between
them, thus underestimating the similarity scores. To solve this issue, we
propose a method to discount the L2 norm of a contextualised word embedding by
the frequency of that word in a corpus when measuring the cosine similarities
between words. We show that the so called stop words behave differently from
the rest of the words, which require special consideration during their
discounting process. Experimental results on a contextualised word similarity
dataset show that our proposed discounting method accurately solves the
similarity underestimation problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10613">
<div class="article-summary-box-inner">
<span><p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to
predict future facts using knowledge of past facts. In this paper, we apply
large language models (LLMs) to these benchmarks using in-context learning
(ICL). We investigate whether and to what extent LLMs can be used for TKG
forecasting, especially without any fine-tuning or explicit modules for
capturing structural and temporal information. For our experiments, we present
a framework that converts relevant historical facts into prompts and generates
ranked predictions using token probabilities. Surprisingly, we observe that
LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully
designed and trained for TKG forecasting. Our extensive evaluation presents
performances across several models and datasets with different characteristics,
compares alternative heuristics for preparing contextual information, and
contrasts to prominent TKG methods and simple frequency and recency baselines.
We also discover that using numerical indices instead of entity/relation names,
i.e., hiding semantic information, does not significantly affect the
performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is
unnecessary; instead, LLMs can leverage the existing patterns in the context to
achieve such performance. Our analysis also reveals that ICL enables LLMs to
learn irregular patterns from the historical context, going beyond simple
predictions based on common or recent information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10614">
<div class="article-summary-box-inner">
<span><p>While there is much recent interest in studying why Transformer-based large
language models make predictions the way they do, the complex computations
performed within each layer have traditionally posed a strong bottleneck. To
mitigate this shortcoming, this work presents a linear decomposition of final
hidden states from autoregressive language models based on each initial input
token, which is exact for virtually all contemporary Transformer architectures.
This decomposition allows the definition of probability distributions that
ablate the contribution of specific input tokens, which can be used to analyze
their influence on model probabilities over a sequence of upcoming words with
only one forward pass from the model. Using the change in next-word probability
as a measure of importance, this work first examines which context words make
the biggest contribution to language model predictions. Regression experiments
suggest that Transformer-based language models rely primarily on collocational
associations, followed by linguistic factors such as syntactic dependencies and
coreference relationships in making next-word predictions. Additionally,
analyses using these measures to predict syntactic dependencies and coreferent
mention spans show that collocational association and repetitions of the same
token respectively, largely explain the language model's predictions on the
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10615">
<div class="article-summary-box-inner">
<span><p>Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard
to benchmark the performance of Self-Supervised Learning (SSL) models on
various speech processing tasks. However, SUPERB largely considers English
speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),
covering 143 languages (ranging from high-resource to endangered), and
considering both automatic speech recognition and language identification.
Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and
employs a simple framework for multilingual tasks by learning a shallow
downstream model. Similar to the SUPERB benchmark, we find speech SSL models
can significantly improve performance compared to FBANK features. Furthermore,
we find that multilingual models do not always perform better than their
monolingual counterparts. We will release ML-SUPERB as a challenge with
organized datasets and reproducible training scripts for future multilingual
representation research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10626">
<div class="article-summary-box-inner">
<span><p>While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical elastic
weight consolidation (EWC) for selective weight updates, combined with low-rank
adapters (LoRA) for training efficiency. Extensive experiments show our
approach substantially improves base LMs on 18 downstream tasks by 64.28% on
average. In particular, the small LMs (1.3B and 6B) enhanced by our approach
match or even outperform much larger LMs (e.g., ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10645">
<div class="article-summary-box-inner">
<span><p>This paper looks at the ability of large language models to participate in
educational guided reading. We specifically, evaluate their ability to generate
meaningful questions from the input text, generate diverse questions both in
terms of content coverage and difficulty of the questions and evaluate their
ability to recommend part of the text that a student should re-read based on
the student's responses to the questions. Based on our evaluation of ChatGPT
and Bard, we report that,
</p>
<p>1) Large language models are able to generate high quality meaningful
questions that have high correlation with the input text, 2) They generate
diverse question that cover most topics in the input text even though this
ability is significantly degraded as the input text increases, 3)The large
language models are able to generate both low and high cognitive questions even
though they are significantly biased toward low cognitive question, 4) They are
able to effectively summarize responses and extract a portion of text that
should be re-read.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10647">
<div class="article-summary-box-inner">
<span><p>Biomedical Named Entity Recognition (BioNER) is the fundamental task of
identifying named entities from biomedical text. However, BioNER suffers from
severe data scarcity and lacks high-quality labeled data due to the highly
specialized and expert knowledge required for annotation. Though data
augmentation has shown to be highly effective for low-resource NER in general,
existing data augmentation techniques fail to produce factual and diverse
augmentations for BioNER. In this paper, we present BioAug, a novel data
augmentation framework for low-resource BioNER. BioAug, built on BART, is
trained to solve a novel text reconstruction task based on selective masking
and knowledge augmentation. Post training, we perform conditional generation
and generate diverse augmentations conditioning BioAug on selectively corrupted
text similar to the training stage. We demonstrate the effectiveness of BioAug
on 5 benchmark BioNER datasets and show that BioAug outperforms all our
baselines by a significant margin (1.5%-21.5% absolute improvement) and is able
to generate augmentations that are both more factual and diverse. Code:
https://github.com/Sreyan88/BioAug.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10649">
<div class="article-summary-box-inner">
<span><p>In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding
Prompt-and-Refine strategy (Figure 3), two simple but effective
\textbf{training-free} methods to decrease the Token Display Time (TDT) of
streaming ASR models \textbf{without any accuracy loss}. The core idea of
ZeroPrompt is to append zeroed content to each chunk during inference, which
acts like a prompt to encourage the model to predict future tokens even before
they were spoken. We argue that streaming acoustic encoders naturally have the
modeling ability of Masked Language Models and our experiments demonstrate that
ZeroPrompt is engineering cheap and can be applied to streaming acoustic
encoders on any dataset without any accuracy loss. Specifically, compared with
our baseline models, we achieve 350 $\sim$ 700ms reduction on First Token
Display Time (TDT-F) and 100 $\sim$ 400ms reduction on Last Token Display Time
(TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and
Librispeech datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10652">
<div class="article-summary-box-inner">
<span><p>The current monaural state of the art tools for speech separation relies on
supervised learning. This means that they must deal with permutation problem,
they are impacted by the mismatch on the number of speakers used in training
and inference. Moreover, their performance heavily relies on the presence of
high-quality labelled data. These problems can be effectively addressed by
employing a fully unsupervised technique for speech separation. In this paper,
we use contrastive learning to establish the representations of frames then use
the learned representations in the downstream deep modularization task.
Concretely, we demonstrate experimentally that in speech separation, different
frames of a speaker can be viewed as augmentations of a given hidden standard
frame of that speaker. The frames of a speaker contain enough prosodic
information overlap which is key in speech separation. Based on this, we
implement a self-supervised learning to learn to minimize the distance between
frames belonging to a given speaker. The learned representations are used in a
downstream deep modularization task to cluster frames based on speaker
identity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix
shows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively
in WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7
respectively in WSJ0-2mix. Its greatest strength being that as the number of
speakers increase, its performance does not degrade significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10666">
<div class="article-summary-box-inner">
<span><p>The front-end is a critical component of English text-to-speech (TTS)
systems, responsible for extracting linguistic features that are essential for
a text-to-speech model to synthesize speech, such as prosodies and phonemes.
The English TTS front-end typically consists of a text normalization (TN)
module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme
(G2P) module. However, current research on the English TTS front-end focuses
solely on individual modules, neglecting the interdependence between them and
resulting in sub-optimal performance for each module. Therefore, this paper
proposes a unified front-end framework that captures the dependencies among the
English TTS front-end modules. Extensive experiments have demonstrated that the
proposed method achieves state-of-the-art (SOTA) performance in all modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation. (arXiv:2305.10679v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10679">
<div class="article-summary-box-inner">
<span><p>Code generation aims to automatically generate source code from high-level
task specifications, which can significantly increase productivity of software
engineering. Recently, approaches based on large language models (LLMs) have
shown remarkable code generation abilities on simple tasks. However, generate
code for more complex tasks, such as competition-level problems, remains
challenging. In this paper, we introduce Brainstorm framework for code
generation. It leverages a brainstorming step that generates and selects
diverse thoughts on the problem to facilitate algorithmic reasoning, where the
thoughts are possible blueprint of solving the problem. We demonstrate that
Brainstorm significantly enhances the ability of LLMs to solve
competition-level programming problems, resulting in a more than 50% increase
in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving
state-of-the-art performance. Furthermore, our experiments conducted on
LeetCode contests show that our framework boosts the ability of ChatGPT to a
level comparable to that of human programmers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10680">
<div class="article-summary-box-inner">
<span><p>Estimating confidence scores for recognition results is a classic task in ASR
field and of vital importance for kinds of downstream tasks and training
strategies. Previous end-to-end~(E2E) based confidence estimation models (CEM)
predict score sequences of equal length with input transcriptions, leading to
unreliable estimation when deletion and insertion errors occur. In this paper
we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve
accurate and reliable confidence estimation based on novel non-autoregressive
E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of
continuous integrate-and-fire (CIF) mechanism to generate token-synchronous
acoustic embedding, which solves the estimation failure issue above. We measure
the quality of estimation with AUC and RMSE in token level and ECE-U - a
proposed metrics in utterance level. CA-CEM gains 24% and 19% relative
reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore,
we conduct analysis to explore the potential of CEM for different ASR related
usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10683">
<div class="article-summary-box-inner">
<span><p>Action knowledge involves the understanding of textual, visual, and temporal
aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)
containing two carefully designed probing tasks: Action Antonym and Video
Reversal, which targets multimodal alignment capabilities and temporal
understanding skills of the model, respectively. Despite recent video-language
models' (VidLM) impressive performance on various benchmark tasks, our
diagnostic tasks reveal their surprising deficiency (near-random performance)
in action knowledge, suggesting that current models rely on object recognition
abilities as a shortcut for action understanding. To remedy this, we propose a
novel framework, Paxion, along with a new Discriminative Video Dynamics
Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher
network to encode new action knowledge and a Knowledge Fuser component to
integrate the Patcher into frozen VidLMs without compromising their existing
capabilities. Due to limitations of the widely-used Video-Text Contrastive
(VTC) loss for learning action knowledge, we introduce the DVDM objective to
train the Knowledge Patcher. DVDM forces the model to encode the correlation
between the action text and the correct ordering of video frames. Our extensive
analyses show that Paxion and DVDM together effectively fill the gap in action
knowledge understanding (~50% to 80%), while maintaining or improving
performance on a wide spectrum of both object- and action-centric downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RMSSinger: Realistic-Music-Score based Singing Voice Synthesis. (arXiv:2305.10686v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10686">
<div class="article-summary-box-inner">
<span><p>We are interested in a challenging task, Realistic-Music-Score based Singing
Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices
given realistic music scores with different note types (grace, slur, rest,
etc.). Though significant progress has been achieved, recent singing voice
synthesis (SVS) methods are limited to fine-grained music scores, which require
a complicated data collection pipeline with time-consuming manual annotation to
align music notes with phonemes. Furthermore, these manual annotation destroys
the regularity of note durations in music scores, making fine-grained music
scores inconvenient for composing. To tackle these challenges, we propose
RMSSinger, the first RMS-SVS method, which takes realistic music scores as
input, eliminating most of the tedious manual annotation and avoiding the
aforementioned inconvenience. Note that music scores are based on words rather
than phonemes, in RMSSinger, we introduce word-level modeling to avoid the
time-consuming phoneme duration annotation and the complicated phoneme-level
mel-note alignment. Furthermore, we propose the first diffusion-based pitch
modeling method, which ameliorates the naturalness of existing pitch-modeling
methods. To achieve these, we collect a new dataset containing realistic music
scores and singing voices according to these realistic music scores from
professional singers. Extensive experiments on the dataset demonstrate the
effectiveness of our methods. Audio samples are available at
https://rmssinger.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10688">
<div class="article-summary-box-inner">
<span><p>Generative pre-trained Transformer (GPT) has demonstrates its great success
in natural language processing and related techniques have been adapted into
molecular modeling. Considering that text is the most important record for
scientific discovery, in this paper, we propose MolXPT, a unified language
model of text and molecules pre-trained on SMILES (a sequence representation of
molecules) wrapped by text. Briefly, we detect the molecule names in each
sequence and replace them to the corresponding SMILES. In this way, the SMILES
could leverage the information from surrounding text, and vice versa. The above
wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem
are all fed into a language model for pre-training. Experimental results
demonstrate that MolXPT outperforms strong baselines of molecular property
prediction on MoleculeNet, performs comparably to the best model in
text-molecule translation while using less than half of its parameters, and
enables zero-shot molecular generation without finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10703">
<div class="article-summary-box-inner">
<span><p>With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing. (arXiv:2305.10709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10709">
<div class="article-summary-box-inner">
<span><p>Large-scale datasets in the real world inevitably involve label noise. Deep
models can gradually overfit noisy labels and thus degrade model
generalization. To mitigate the effects of label noise, learning with noisy
labels (LNL) methods are designed to achieve better generalization performance.
Due to the lack of suitable datasets, previous studies have frequently employed
synthetic label noise to mimic real-world label noise. However, synthetic noise
is not instance-dependent, making this approximation not always effective in
practice. Recent research has proposed benchmarks for learning with real-world
noisy labels. However, the noise sources within may be single or fuzzy, making
benchmarks different from data with heterogeneous label noises in the real
world. To tackle these issues, we contribute NoisywikiHow, the largest NLP
benchmark built with minimal supervision. Specifically, inspired by human
cognition, we explicitly construct multiple sources of label noise to imitate
human errors throughout the annotation, replicating real-world noise, whose
corruption is affected by both ground-truth labels and instances. Moreover, we
provide a variety of noise levels to support controlled experiments on noisy
data, enabling us to evaluate LNL methods systematically and comprehensively.
After that, we conduct extensive multi-dimensional experiments on a broad range
of LNL methods, obtaining new and intriguing findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10713">
<div class="article-summary-box-inner">
<span><p>With growing capabilities of large language models, prompting them has become
the dominant way to access them. This has motivated the development of
strategies for automatically selecting effective language prompts. In this
paper, we introduce prompt flatness, a new metric to quantify the expected
utility of a language prompt. This metric is inspired by flatness
regularization in statistical learning that quantifies the robustness of the
model towards its parameter perturbations. We provide theoretical foundations
for this metric and its relationship with other prompt selection metrics,
providing a comprehensive understanding of existing methods. Empirically, we
show that combining prompt flatness with existing metrics improves both
performance and sample efficiency. Our metric outperforms the previous prompt
selection metrics with an average increase of 5% in accuracy and 10% in Pearson
correlation across 6 classification benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10731">
<div class="article-summary-box-inner">
<span><p>Toxic language, such as hate speech, can deter users from participating in
online communities and enjoying popular platforms. Previous approaches to
detecting toxic language and norm violations have been primarily concerned with
conversations from online forums and social media, such as Reddit and Twitter.
These approaches are less effective when applied to conversations on
live-streaming platforms, such as Twitch and YouTube Live, as each comment is
only visible for a limited time and lacks a thread structure that establishes
its relationship with other comments. In this work, we share the first NLP
study dedicated to detecting norm violations in conversations on live-streaming
platforms. We define norm violation categories in live-stream chats and
annotate 4,583 moderated comments from Twitch. We articulate several facets of
live-stream data that differ from other forums, and demonstrate that existing
models perform poorly in this setting. By conducting a user study, we identify
the informational context humans use in live-stream moderation, and train
models leveraging context to identify norm violations. Our results show that
appropriate contextual information can boost moderation performance by 35\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10734">
<div class="article-summary-box-inner">
<span><p>Diffusion-based speech enhancement (SE) has been investigated recently, but
its decoding is very time-consuming. One solution is to initialize the decoding
process with the enhanced feature estimated by a predictive SE system. However,
this two-stage method ignores the complementarity between predictive and
diffusion SE. In this paper, we propose a unified system that integrates these
two SE modules. The system encodes both generative and predictive information,
and then applies both generative and predictive decoders, whose outputs are
fused. Specifically, the two SE modules are fused in the first and final
diffusion steps: the first step fusion initializes the diffusion process with
the predictive SE for improving the convergence, and the final step fusion
combines the two complementary SE outputs to improve the SE performance.
Experiments on the Voice-Bank dataset show that the diffusion score estimation
can benefit from the predictive information and speed up the decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10736">
<div class="article-summary-box-inner">
<span><p>Despite substantial progress in abstractive text summarization to generate
fluent and informative texts, the factual inconsistency in the generated
summaries remains an important yet challenging problem to be solved. In this
paper, we construct causal graphs for abstractive text summarization and
identify the intrinsic causes of the factual inconsistency, i.e., the language
bias and irrelevancy bias, and further propose a debiasing framework, named
CoFactSum, to alleviate the causal effects of these biases by counterfactual
estimation. Specifically, the proposed CoFactSum provides two counterfactual
estimation strategies, i.e., Explicit Counterfactual Masking with an explicit
dynamic masking strategy, and Implicit Counterfactual Training with an implicit
discriminative cross-attention mechanism. Meanwhile, we design a Debiasing
Degree Adjustment mechanism to dynamically adapt the debiasing degree at each
decoding step. Extensive experiments on two widely-used summarization datasets
demonstrate the effectiveness of CoFactSum in enhancing the factual consistency
of generated summaries compared with several baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10786">
<div class="article-summary-box-inner">
<span><p>Prior studies diagnose the anisotropy problem in sentence representations
from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis
reveals that the sentence embeddings from BERT suffer from a bias towards
uninformative words, limiting the performance in semantic textual similarity
(STS) tasks. To address this bias, we propose a simple and efficient
unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words
with model-based importance estimations and computes the weighted average of
word representations from pre-trained models as sentence embeddings. Ditto can
be easily applied to any pre-trained language model as a postprocessing
operation. Compared to prior sentence embedding approaches, Ditto does not add
parameters nor requires any learning. Empirical evaluations demonstrate that
our proposed Ditto can alleviate the anisotropy problem and improve various
pre-trained models on STS tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10788">
<div class="article-summary-box-inner">
<span><p>Due to the rapid development of computing hardware resources and the dramatic
growth of data, pre-trained models in speech recognition, such as Whisper, have
significantly improved the performance of speech recognition tasks. However,
these models usually have a high computational overhead, making it difficult to
execute effectively on resource-constrained devices. To speed up inference and
reduce model size while maintaining performance, we propose a novel guided
knowledge distillation and quantization for large pre-trained model Whisper.
The student model selects distillation and quantization layers based on
quantization loss and distillation loss, respectively. We compressed
$\text{Whisper}_\text{small}$ to $\text{Whisper}_\text{base}$ and
$\text{Whisper}_\text{tiny}$ levels, making $\text{Whisper}_\text{small}$
5.18x/10.48x smaller, respectively. Moreover, compared to the original
$\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$, there is also a
relative character error rate~(CER) reduction of 11.3% and 14.0% for the new
compressed model respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10818">
<div class="article-summary-box-inner">
<span><p>Despite the potential benefits of Diffusion Models for NLP applications,
publicly available implementations, trained models, or reproducible training
procedures currently need to be publicly available. We present the Democratized
Diffusion Language Model (DDLM), based on the Continuous Diffusion for
Categorical Data (CDCD) framework, to address these challenges. We propose a
simplified training procedure for DDLM using the C4 dataset and perform an
in-depth analysis of the trained model's behavior. Furthermore, we introduce a
novel early-exiting strategy for faster sampling with models trained with score
interpolation. Since no previous works aimed at solving downstream tasks with
pre-trained Diffusion LM (e.g., classification tasks), we experimented with
GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this
paper, we propose available training and evaluation pipelines to other
researchers and pre-trained DDLM models, which could be used in future research
with Diffusion LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10819">
<div class="article-summary-box-inner">
<span><p>It is intractable to evaluate the performance of Grammatical Error Correction
(GEC) systems since GEC is a highly subjective task. Designing an evaluation
metric that is as objective as possible is crucial to the development of GEC
task. Previous mainstream evaluation metrics, i.e., reference-based metrics,
introduce bias into the multi-reference evaluation because they extract edits
without considering the presence of multiple references. To overcome the
problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to
evaluate GEC systems in multi-reference settings. First, CLEME builds chunk
sequences with consistent boundaries for the source, the hypothesis and all the
references, thus eliminating the bias caused by inconsistent edit boundaries.
Then, based on the discovery that there exist boundaries between different
grammatical errors, we automatically determine the grammatical error boundaries
and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach
consistently and substantially outperforms existing reference-based GEC metrics
on multiple reference sets in both corpus-level and sentence-level settings.
Extensive experiments and detailed analyses demonstrate the correctness of our
discovery and the effectiveness of our designed evaluation metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10833">
<div class="article-summary-box-inner">
<span><p>The domain of Botany is rich with metaphorical terms. Those terms play an
important role in the description and identification of flowers and plants.
However, the identification of such terms in discourse is an arduous task. This
leads in some cases to committing errors during translation processes and
lexicographic tasks. The process is even more challenging when it comes to
machine translation, both in the cases of single-word terms and multi-word
terms. One of the recent concerns of Natural Language Processing (NLP)
applications and Machine Translation (MT) technologies is the automatic
identification of metaphor-based words in discourse through Deep Learning (DL).
In this study, we seek to fill this gap through the use of thirteen popular
transformer based models, as well as ChatGPT, and we show that discriminative
models perform better than GPT-3.5 model with our best performer reporting
92.2349% F1 score in metaphoric flower and plant names identification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIwriting: Relations Between Image Generation and Digital Writing. (arXiv:2305.10834v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10834">
<div class="article-summary-box-inner">
<span><p>During 2022, both transformer-based AI text generation sys-tems such as GPT-3
and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion
made exponential leaps forward and are unquestionably altering the fields of
digital art and electronic literature. In this panel a group of electronic
literature authors and theorists consider new oppor-tunities for human
creativity presented by these systems and present new works have produced
during the past year that specifically address these systems as environments
for literary expressions that are translated through iterative interlocutive
processes into visual representations. The premise that binds these
presentations is that these systems and the works gener-ated must be considered
from a literary perspective, as they originate in human writing. In works
ranging from a visual memoir of the personal experience of a health crisis, to
interac-tive web comics, to architectures based on abstract poetic language, to
political satire, four artists explore the capabili-ties of these writing
environments for new genres of literary artist practice, while a digital
culture theorist considers the origins and effects of the particular training
datasets of human language and images on which these new hybrid forms are
based.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10835">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel
parameter-efficient fine-tuning method for pre-trained Language Models (LMs)
that adds input-dependent bias before each Transformer layer. We evaluate AoT
P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa
models, showing that it outperforms BitFit and is comparable or better than
other baseline methods for efficient fine-tuning. Additionally, we assess the
inference overhead of AoT P-Tuning and demonstrate that it introduces
negligible overhead compared to established baseline methods. Our method
enables multi-task inference with a single backbone LM, making it a practical
solution for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10839">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive automatic speech recognition (ASR) has become a mainstream
of ASR modeling because of its fast decoding speed and satisfactory result. To
further boost the performance, relaxing the conditional independence assumption
and cascading large-scaled pre-trained models are two active research
directions. In addition to these strategies, we propose a lexical-aware
non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of
an acoustic encoder, a speech-text shared encoder, and a speech-text shared
decoder. The acoustic encoder is used to process the input speech features as
usual, and the speech-text shared encoder and decoder are designed to train
speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR
model aware of lexical information, so the resulting model is expected to
achieve better results by leveraging the learned linguistic knowledge. A series
of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets.
According to the experiments, the proposed LA-NAT can provide superior results
than other recently proposed non-autoregressive ASR models. In addition, LA-NAT
is a relatively compact model than most non-autoregressive ASR models, and it
is about 58 times faster than the classic autoregressive model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10845">
<div class="article-summary-box-inner">
<span><p>Language is by its very nature incremental in how it is produced and
processed. This property can be exploited by NLP systems to produce fast
responses, which has been shown to be beneficial for real-time interactive
applications. Recent neural network-based approaches for incremental processing
mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct
earlier output, which can be necessary in incremental processing).
Transformers, on the other hand, consume whole sequences, and hence are by
nature non-incremental. A restart-incremental interface that repeatedly passes
longer input prefixes can be used to obtain partial outputs, while providing
the ability to revise. However, this method becomes costly as the sentence
grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision
(TAPIR) and introduce a method to obtain an incremental supervision signal for
learning an adaptive revision policy. Experimental results on sequence
labelling show that our model has better incremental performance and faster
inference speed compared to restart-incremental Transformers, while showing
little degradation on full sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10847">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora. (arXiv:2305.10848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10848">
<div class="article-summary-box-inner">
<span><p>In this paper, we unveil a groundbreaking method to amplify full-text search
lemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm
retrieval algorithm. Our primary aim is to streamline the extraction of a
word's primary form or lemma - a crucial factor in full-text search.
Additionally, we propose a compact dictionary storage strategy, significantly
boosting the speed and precision of lemma retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10865">
<div class="article-summary-box-inner">
<span><p>The difficulty of appropriately assigning credit is particularly heightened
in cooperative MARL with sparse reward, due to the concurrent time and
structural scales involved. Automatic subgoal generation (ASG) has recently
emerged as a viable MARL approach inspired by utilizing subgoals in
intrinsically motivated reinforcement learning. However, end-to-end learning of
complex task planning from sparse rewards without prior knowledge, undoubtedly
requires massive training samples. Moreover, the diversity-promoting nature of
existing ASG methods can lead to the "over-representation" of subgoals,
generating numerous spurious subgoals of limited relevance to the actual task
reward and thus decreasing the sample efficiency of the algorithm. To address
this problem and inspired by the disentangled representation learning, we
propose a novel "disentangled" decision-making method, Semantically Aligned
task decomposition in MARL (SAMA), that prompts pretrained language models with
chain-of-thought that can suggest potential goals, provide suitable goal
decomposition and subgoal allocation as well as self-reflection-based
replanning. Additionally, SAMA incorporates language-grounded RL to train each
agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages
in sample efficiency compared to state-of-the-art ASG methods, as evidenced by
its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition. (arXiv:2305.10866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10866">
<div class="article-summary-box-inner">
<span><p>Implicit Discourse Relation Recognition (IDRR) aims at classifying the
relation sense between two arguments without an explicit connective. Recently,
the ConnPrompt~\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt
learning for IDRR based on the fusion of multi-prompt decisions from three
different yet much similar connective prediction templates. Instead of
multi-prompt ensembling, we propose to design auxiliary tasks with enlightened
prompt learning for the IDRR task. Although an auxiliary task is not used to
directly output final prediction, we argue that during the joint training some
of its learned features can be useful to boost the main task. In light of such
motivations, we propose a task enlightenment prompt learning model, called
TEPrompt, to fuse learned features from three related tasks for IDRR. In
particular, the TEPrompt contains three tasks, viz., Discourse Relation
Recognition (DRR), Sense Semantics Classification (SSC) and Annotated
Connective Prediction (ACP), each with a unique prompt template and an answer
space. In the training phase, we jointly train three prompt learning tasks with
shared argument representation. In the testing phase, we only take the DRR
output with fused features as the final IDRR decision. Experiments with the
same conditions have shown that the proposed TEPrompt outperforms the
ConnPrompt. This can be attributed to the promoted decision features and
language models benefited from joint-training of auxiliary tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventNet-ITA: Italian Frame Parsing for Events. (arXiv:2305.10892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10892">
<div class="article-summary-box-inner">
<span><p>This paper introduces EventNet-ITA, a large, multi-domain corpus annotated
with event frames for Italian, and presents an efficient approach for
multi-label Frame Parsing. The approach is then evaluated on the dataset.
Covering a wide range of individual, social and historical phenomena, the main
contribution of EventNet-ITA is to provide the research community with a
resource for textual event mining and a novel and extensive tool for Frame
Parsing in Italian.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation. (arXiv:2305.10907v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10907">
<div class="article-summary-box-inner">
<span><p>Goal-oriented Script Generation is a new task of generating a list of steps
that can fulfill the given goal. In this paper, we propose to extend the task
from the perspective of cognitive theory. Instead of a simple flat structure,
the steps are typically organized hierarchically - Human often decompose a
complex task into subgoals, where each subgoal can be further decomposed into
steps. To establish the benchmark, we contribute a new dataset, propose several
baseline methods, and set up evaluation metrics. Both automatic and human
evaluation verify the high-quality of dataset, as well as the effectiveness of
incorporating subgoals into hierarchical script generation. Furthermore, We
also design and evaluate the model to discover subgoal, and find that it is a
bit more difficult to decompose the goals than summarizing from segmented
steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10913">
<div class="article-summary-box-inner">
<span><p>Using only image-sentence pairs, weakly-supervised visual-textual grounding
aims to learn region-phrase correspondences of the respective entity mentions.
Compared to the supervised approach, learning is more difficult since bounding
boxes and textual phrases correspondences are unavailable. In light of this, we
propose the Semantic Prior Refinement Model (SPRM), whose predictions are
obtained by combining the output of two main modules. The first untrained
module aims to return a rough alignment between textual phrases and bounding
boxes. The second trained module is composed of two sub-components that refine
the rough alignment to improve the accuracy of the final phrase-bounding box
alignments. The model is trained to maximize the multimodal similarity between
an image and a sentence, while minimizing the multimodal similarity of the same
sentence and a new unrelated image, carefully selected to help the most during
training. Our approach shows state-of-the-art results on two popular datasets,
Flickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6%
absolute improvement. Moreover, thanks to the untrained component, it reaches
competitive performances just using a small fraction of training examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Communication with Attention. (arXiv:2305.10920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10920">
<div class="article-summary-box-inner">
<span><p>To develop computational agents that better communicate using their own
emergent language, we endow the agents with an ability to focus their attention
on particular concepts in the environment. Humans often understand an object or
scene as a composite of concepts and those concepts are further mapped onto
words. We implement this intuition as cross-modal attention mechanisms in
Speaker and Listener agents in a referential game and show attention leads to
more compositional and interpretable emergent language. We also demonstrate how
attention aids in understanding the learned communication protocol by
investigating the attention weights associated with each message symbol and the
alignment of attention weights between Speaker and Listener agents. Overall,
our results suggest that attention is a promising mechanism for developing more
human-like emergent language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10923">
<div class="article-summary-box-inner">
<span><p>Query performance prediction (QPP) is a core task in information retrieval.
The QPP task is to predict the retrieval quality of a search system for a query
without relevance judgments. Research has shown the effectiveness and
usefulness of QPP for ad-hoc search. Recent years have witnessed considerable
progress in conversational search (CS). Effective QPP could help a CS system to
decide an appropriate action to be taken at the next turn. Despite its
potential, QPP for CS has been little studied. We address this research gap by
reproducing and studying the effectiveness of existing QPP methods in the
context of CS. While the task of passage retrieval remains the same in the two
settings, a user query in CS depends on the conversational history, introducing
novel QPP challenges. In particular, we seek to explore to what extent findings
from QPP methods for ad-hoc search generalize to three CS settings: (i)
estimating the retrieval quality of different query rewriting-based retrieval
methods, (ii) estimating the retrieval quality of a conversational dense
retrieval method, and (iii) estimating the retrieval quality for top ranks vs.
deeper-ranked lists. Our findings can be summarized as follows: (i) supervised
QPP methods distinctly outperform unsupervised counterparts only when a
large-scale training set is available; (ii) point-wise supervised QPP methods
outperform their list-wise counterparts in most cases; and (iii) retrieval
score-based unsupervised QPP methods show high effectiveness in assessing the
conversational dense retrieval method, ConvDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10927">
<div class="article-summary-box-inner">
<span><p>The goal of document-grounded dialogue (DocGD) is to generate a response by
grounding the evidence in a supporting document in accordance with the dialogue
context. This process involves four variables that are causally connected.
Recently, task-specific pre-training has greatly boosted performances on many
downstream tasks. Existing DocGD methods, however, continue to rely on general
pre-trained language models without a specifically tailored pre-training
approach that explicitly captures the causal relationships. To tackle this
issue, we are the first to present a causally-complete dataset construction
strategy for building million-level DocGD pre-training corpora. To better
capture causality, we further propose a causally-perturbed pre-training
strategy, which introduces causal perturbations on the variables and optimizes
the overall causal effect. Experiments on three benchmark datasets demonstrate
that our causal pre-training achieves considerable and consistent improvements
under fully-supervised, low-resource, few-shot, and zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10928">
<div class="article-summary-box-inner">
<span><p>NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10930">
<div class="article-summary-box-inner">
<span><p>While multilingual neural machine translation has achieved great success, it
suffers from the off-target issue, where the translation is in the wrong
language. This problem is more pronounced on zero-shot translation tasks. In
this work, we find that failing in encoding discriminative target language
signal will lead to off-target and a closer lexical distance (i.e.,
KL-divergence) between two languages' vocabularies is related with a higher
off-target rate. We also find that solely isolating the vocab of different
languages in the decoder can alleviate the problem. Motivated by the findings,
we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective
algorithm to construct the multilingual vocabulary, that greatly alleviates the
off-target problem of the translation model by increasing the KL-divergence
between languages. We conduct experiments on a multilingual machine translation
benchmark in 11 languages. Experiments show that the off-target rate for 90
translation tasks is reduced from 29\% to 8\%, while the overall BLEU score is
improved by an average of 1.9 points without extra training cost or sacrificing
the supervised directions' performance. We release the code at
\href{https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT}
for reproduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10951">
<div class="article-summary-box-inner">
<span><p>The performance of automatic speech recognition (ASR) systems has advanced
substantially in recent years, particularly for languages for which a large
amount of transcribed speech is available. Unfortunately, for low-resource
languages, such as minority languages, regional languages or dialects, ASR
performance generally remains much lower. In this study, we investigate whether
data augmentation techniques could help improve low-resource ASR performance,
focusing on four typologically diverse minority languages or language variants
(West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For
all four languages, we examine the use of self-training, where an ASR system
trained with the available human-transcribed data is used to generate
transcriptions, which are then combined with the original data to train a new
ASR system. For Gronings, for which there was a pre-existing text-to-speech
(TTS) system available, we also examined the use of TTS to generate ASR
training data from text-only sources. We find that using a self-training
approach consistently yields improved performance (a relative WER reduction up
to 20.5% compared to using an ASR system trained on 24 minutes of manually
transcribed speech). The performance gain from TTS augmentation for Gronings
was even stronger (up to 25.5% relative reduction in WER compared to a system
based on 24 minutes of manually transcribed speech). In sum, our results show
the benefit of using self-training or (if possible) TTS-generated data as an
efficient solution to overcome the limitations of data availability for
resource-scarce languages in order to improve ASR performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10971">
<div class="article-summary-box-inner">
<span><p>Africa has over 2000 indigenous languages but they are under-represented in
NLP research due to lack of datasets. In recent years, there have been progress
in developing labeled corpora for African languages. However, they are often
available in a single domain and may not generalize to other domains. In this
paper, we focus on the task of sentiment classification for cross domain
adaptation. We create a new dataset, NollySenti - based on the Nollywood movie
reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,
Nigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using
classical machine learning methods and pre-trained language models. Leveraging
transfer learning, we compare the performance of cross-domain adaptation from
Twitter domain, and cross-lingual adaptation from English language. Our
evaluation shows that transfer from English in the same target domain leads to
more than 5% improvement in accuracy compared to transfer from Twitter in the
same language. To further mitigate the domain difference, we leverage machine
translation (MT) from English to other Nigerian languages, which leads to a
further improvement of 7% over cross-lingual evaluation. While MT to
low-resource languages are often of low quality, through human evaluation, we
show that most of the translated sentences preserve the sentiment of the
original English reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction. (arXiv:2305.10985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10985">
<div class="article-summary-box-inner">
<span><p>Most research in Relation Extraction (RE) involves the English language,
mainly due to the lack of multi-lingual resources. We propose Multi-CrossRE,
the broadest multi-lingual dataset for RE, including 26 languages in addition
to English, and covering six text domains. Multi-CrossRE is a machine
translated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion
including more than 200 sentences in seven diverse languages checked by native
speakers. We run a baseline model over the 26 new datasets and--as sanity
check--over the 26 back-translations to English. Results on the back-translated
data are consistent with the ones on the original English CrossRE, indicating
high quality of the translation and the resulting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10991">
<div class="article-summary-box-inner">
<span><p>The softmax attention mechanism has emerged as a noteworthy development in
the field of Artificial Intelligence research, building on the successes of
Transformer-based architectures. However, their ever increasing sizes
necessitate ever increasing computational memory, that limits their usage. We
propose KgV, a sigmoid gating mechanism that, in conjunction with softmax
attention, significantly boosts performance without increasing architecture
size. To amend the size requirements, we leverage Tensor Chains to identify and
prune the excess parameters. We find that such excess resides primarily within
the embedding layer, and not in the output linear layer. To further improve
embedding and significantly reduce parameters, we introduce H-SoftPOS, a
hierarchical embedding layer which simultaneously enhances performance.
Remarkably, on the WMT14 English-German validation set, our approach yields a
threefold reduction in perplexity, surpassing the current state-of-the-art,
while reducing parameter counts also by a factor of 3. When we further reduce
the number of parameters up to sevenfold, we can still achieve a 21\% decrease
in perplexity with respect to the baseline Transformer. To understand
generalization capabilities, we conduct experiments on the 7 language pairs of
the WMT17 dataset. Our method outperforms existing techniques in terms of test
loss while simultaneously halving the number of parameters. Moreover, we
observe a 70 times reduction in variance with respect to the prior
state-of-the-art. In conclusion, our proposed method yields significant
improvements in performance and much lower memory cost. We call the resulting
architecture Anthe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10992">
<div class="article-summary-box-inner">
<span><p>Masked language modeling (MLM) is a widely used self-supervised pretraining
objective, where a model needs to predict an original token that is replaced
with a mask given contexts. Although simpler and computationally efficient
pretraining objectives, e.g., predicting the first character of a masked token,
have recently shown comparable results to MLM, no objectives with a masking
scheme actually outperform it in downstream tasks. Motivated by the assumption
that their lack of complexity plays a vital role in the degradation, we
validate whether more complex masked objectives can achieve better results and
investigate how much complexity they should have to perform comparably to MLM.
Our results using GLUE, SQuAD, and Universal Dependencies benchmarks
demonstrate that more complicated objectives tend to show better downstream
results with at least half of the MLM complexity needed to perform comparably
to MLM. Finally, we discuss how we should pretrain a model using a masked
objective from the task complexity perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10998">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) encode a large amount of world knowledge.
However, as such knowledge is frozen at the time of model training, the models
become static and limited by the training data at that time. In order to
further improve the capacity of LLMs for knowledge-intensive tasks, we consider
augmenting LLMs with the large-scale web using search engine. Unlike previous
augmentation sources (e.g., Wikipedia data dump), the web provides broader,
more comprehensive and constantly updated information. In this paper, we
present a web-augmented LLM UNIWEB, which is trained over 16
knowledge-intensive tasks in a unified text-to-text format. Instead of simply
using the retrieved contents from web, our approach has made two major
improvements. Firstly, we propose an adaptive search engine assisted learning
method that can self-evaluate the confidence level of LLM's predictions, and
adaptively determine when to refer to the web for more data, which can avoid
useless or noisy augmentation from web. Secondly, we design a pretraining task,
i.e., continual knowledge learning, based on salient spans prediction, to
reduce the discrepancy between the encoded and retrieved knowledge. Experiments
on a wide range of knowledge-intensive tasks show that our model significantly
outperforms previous retrieval-augmented methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11000">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models are regarded as a crucial step towards
Artificial General Intelligence (AGI) and have garnered significant interest
with the emergence of ChatGPT. However, current speech-language models
typically adopt the cascade paradigm, preventing inter-modal knowledge
transfer. In this paper, we propose SpeechGPT, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and
generating multi-model content. With discrete speech representations, we first
construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes
modality-adaptation pre-training, cross-modal instruction fine-tuning, and
chain-of-modality instruction fine-tuning. The experimental results demonstrate
that SpeechGPT has an impressive capacity to follow multi-modal human
instructions and highlight the potential of handling multiple modalities with
one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11004">
<div class="article-summary-box-inner">
<span><p>Taxonomy completion, a task aimed at automatically enriching an existing
taxonomy with new concepts, has gained significant interest in recent years.
Previous works have introduced complex modules, external information, and
pseudo-leaves to enrich the representation and unify the matching process of
attachment and insertion. While they have achieved good performance, these
introductions may have brought noise and unfairness during training and
scoring. In this paper, we present TaxBox, a novel framework for taxonomy
completion that maps taxonomy concepts to box embeddings and employs two
probabilistic scorers for concept attachment and insertion, avoiding the need
for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a
graph aggregation module to leverage the structural information of the taxonomy
and two lightweight decoders that map features to box embedding and capture
complex relationships between concepts; (2) two probabilistic scorers that
correspond to attachment and insertion operations and ensure the avoidance of
pseudo-leaves; and (3) three learning objectives that assist the model in
mapping concepts more granularly onto the box embedding space. Experimental
results on four real-world datasets suggest that TaxBox outperforms baseline
methods by a considerable margin and surpasses previous state-of-art methods to
a certain extent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FunASR: A Fundamental End-to-End Speech Recognition Toolkit. (arXiv:2305.11013v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11013">
<div class="article-summary-box-inner">
<span><p>This paper introduces FunASR, an open-source speech recognition toolkit
designed to bridge the gap between academic research and industrial
applications. FunASR offers models trained on large-scale industrial corpora
and the ability to deploy them in applications. The toolkit's flagship model,
Paraformer, is a non-autoregressive end-to-end speech recognition model that
has been trained on a manually annotated Mandarin speech recognition dataset
that contains 60,000 hours of speech. To improve the performance of Paraformer,
we have added timestamp prediction and hotword customization capabilities to
the standard Paraformer backbone. In addition, to facilitate model deployment,
we have open-sourced a voice activity detection model based on the Feedforward
Sequential Memory Network (FSMN-VAD) and a text post-processing punctuation
model based on the controllable time-delay Transformer (CT-Transformer), both
of which were trained on industrial corpora. These functional modules provide a
solid foundation for building high-precision long audio speech recognition
services. Compared to other models trained on open datasets, Paraformer
demonstrates superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Silver Syntax Pre-training for Cross-Domain Relation Extraction. (arXiv:2305.11016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11016">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) remains a challenging task, especially when
considering realistic out-of-domain evaluations. One of the main reasons for
this is the limited training size of current RE datasets: obtaining
high-quality (manually annotated) data is extremely expensive and cannot
realistically be repeated for each new domain. An intermediate training step on
data from related tasks has shown to be beneficial across many NLP
tasks.However, this setup still requires supplementary annotated data, which is
often not available. In this paper, we investigate intermediate pre-training
specifically for RE. We exploit the affinity between syntactic structure and
semantic RE, and identify the syntactic relations which are closely related to
RE by being on the shortest dependency path between two entities. We then take
advantage of the high accuracy of current syntactic parsers in order to
automatically obtain large amounts of low-cost pre-training data. By
pre-training our RE model on the relevant syntactic relations, we are able to
outperform the baseline in five out of six cross-domain setups, without any
additional annotated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11023">
<div class="article-summary-box-inner">
<span><p>Natural language understanding includes the tasks of intent detection
(identifying a user's objectives) and slot filling (extracting the entities
relevant to those objectives). Prior slot filling methods assume that each
intent type cannot occur more than once within a message, however this is often
not a valid assumption for real-world settings. In this work, we generalize
slot filling by removing the constraint of unique intents in a message. We cast
this as a JSON generation task and approach it using a language model. We
create a pre-training dataset by combining DBpedia and existing slot filling
datasets that we convert for JSON generation. We also generate an in-domain
dataset using GPT-3. We train T5 models for this task (with and without
exemplars in the prompt) and find that both training datasets improve
performance, and that the model is able to generalize to intent types not seen
during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11029">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) aims to infer complex semantic
relations among entities in a document. Distant supervision (DS) is able to
generate massive auto-labeled data, which can improve DocRE performance. Recent
works leverage pseudo labels generated by the pre-denoising model to reduce
noise in DS data. However, unreliable pseudo labels bring new noise, e.g.,
adding false pseudo labels and losing correct DS labels. Therefore, how to
select effective pseudo labels to denoise DS data is still a challenge in
document-level distant relation extraction. To tackle this issue, we introduce
uncertainty estimation technology to determine whether pseudo labels can be
trusted. In this work, we propose a Document-level distant Relation Extraction
framework with Uncertainty Guided label denoising, UGDRE. Specifically, we
propose a novel instance-level uncertainty estimation method, which measures
the reliability of the pseudo labels with overlapping relations. By further
considering the long-tail problem, we design dynamic uncertainty thresholds for
different types of relations to filter high-uncertainty pseudo labels. We
conduct experiments on two public datasets. Our framework outperforms strong
baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement. (arXiv:2305.11034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11034">
<div class="article-summary-box-inner">
<span><p>State-of-the-art target-oriented opinion word extraction (TOWE) models
typically use BERT-based text encoders that operate on the word level, along
with graph convolutional networks (GCNs) that incorporate syntactic information
extracted from syntax trees. These methods achieve limited gains with GCNs and
have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to
be effective at representing rare words or words with insufficient context
information. To address this issue, this work trades syntax trees for BERT
wordpieces by entirely removing the GCN component from the methods'
architectures. To enhance TOWE performance, we tackle the issue of aspect
representation loss during encoding. Instead of solely utilizing a sentence as
the input, we use a sentence-aspect pair. Our relatively simple approach
achieves state-of-the-art results on benchmark datasets and should serve as a
strong baseline for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11038">
<div class="article-summary-box-inner">
<span><p>Named entity recognition in real-world applications suffers from the
diversity of entity types, the emergence of new entity types, and the lack of
high-quality annotations. To address the above problems, this paper proposes an
in-context learning-based NER approach, which can effectively inject in-context
NER ability into PLMs and recognize entities of novel types on-the-fly using
only a few demonstrative instances. Specifically, we model PLMs as a
meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}.
M}$, and a new entity extractor can be implicitly constructed by applying new
instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M)
}$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be
a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the
above in-context NER ability into PLMs, we propose a meta-function pre-training
algorithm, which pre-trains PLMs by comparing the (instruction,
demonstration)-initialized extractor with a surrogate golden extractor.
Experimental results on 4 few-shot NER datasets show that our method can
effectively inject in-context NER ability into PLMs and significantly
outperforms the PLMs+fine-tuning counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval. (arXiv:2305.11052v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11052">
<div class="article-summary-box-inner">
<span><p>Dense retrieval has shown promise in the first-stage retrieval process when
trained on in-domain labeled datasets. However, previous studies have found
that dense retrieval is hard to generalize to unseen domains due to its weak
modeling of domain-invariant and interpretable feature (i.e., matching signal
between two texts, which is the essence of information retrieval). In this
paper, we propose a novel method to improve the generalization of dense
retrieval via capturing matching signal called BERM. Fully fine-grained
expression and query-oriented saliency are two properties of the matching
signal. Thus, in BERM, a single passage is segmented into multiple units and
two unit-level requirements are proposed for representation as the constraint
in training to obtain the effective matching signal. One is semantic unit
balance and the other is essential matching unit extractability. Unit-level
view and balanced semantics make representation express the text in a
fine-grained manner. Essential matching unit extractability makes passage
representation sensitive to the given query to extract the pure matching
information from the passage containing complex context. Experiments on BEIR
show that our method can be effectively combined with different dense retrieval
training methods (vanilla, hard negatives mining and knowledge distillation) to
improve its generalization ability without any additional inference overhead
and target domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation. (arXiv:2305.11061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11061">
<div class="article-summary-box-inner">
<span><p>Converting text into the structured query language (Text2SQL) is a research
hotspot in the field of natural language processing (NLP), which has broad
application prospects. In the era of big data, the use of databases has
penetrated all walks of life, in which the collected data is large in scale,
diverse in variety, and wide in scope, making the data query cumbersome and
inefficient, and putting forward higher requirements for the Text2SQL model. In
practical applications, the current mainstream end-to-end Text2SQL model is not
only difficult to build due to its complex structure and high requirements for
training data, but also difficult to adjust due to massive parameters. In
addition, the accuracy of the model is hard to achieve the desired result.
Based on this, this paper proposes a pipelined Text2SQL method: SPSQL. This
method disassembles the Text2SQL task into four subtasks--table selection,
column selection, SQL generation, and value filling, which can be converted
into a text classification problem, a sequence labeling problem, and two text
generation problems, respectively. Then, we construct data formats of different
subtasks based on existing data and improve the accuracy of the overall model
by improving the accuracy of each submodel. We also use the named entity
recognition module and data augmentation to optimize the overall model. We
construct the dataset based on the marketing business data of the State Grid
Corporation of China. Experiments demonstrate our proposed method achieves the
best performance compared with the end-to-end method and other pipeline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bits of Grass: Does GPT already know how to write like Whitman?. (arXiv:2305.11064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11064">
<div class="article-summary-box-inner">
<span><p>This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4
models to generate poems in the style of specific authors using zero-shot and
many-shot prompts (which use the maximum context length of 8192 tokens). We
assess the performance of models that are not fine-tuned for generating poetry
in the style of specific authors, via automated evaluation. Our findings
indicate that without fine-tuning, even when provided with the maximum number
of 17 poem examples (8192 tokens) in the prompt, these models do not generate
poetry in the desired style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11068">
<div class="article-summary-box-inner">
<span><p>The purpose of this work is to describe the Orkg-Leaderboard software
designed to extract leaderboards defined as Task-Dataset-Metric tuples
automatically from large collections of empirical research papers in Artificial
Intelligence (AI). The software can support both the main workflows of
scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the
system is integrated with the Open Research Knowledge Graph (ORKG) platform,
which fosters the machine-actionable publishing of scholarly findings. Thus the
system output, when integrated within the ORKG's supported Semantic Web
infrastructure of representing machine-actionable 'resources' on the Web,
enables: 1) broadly, the integration of empirical results of researchers across
the world, thus enabling transparency in empirical research with the potential
to also being complete contingent on the underlying data source(s) of
publications; and 2) specifically, enables researchers to track the progress in
AI with an overview of the state-of-the-art (SOTA) across the most common AI
tasks and their corresponding datasets via dynamic ORKG frontend views
leveraging tables and visualization charts over the machine-actionable data.
Our best model achieves performances above 90% F1 on the \textit{leaderboard}
extraction task, thus proving Orkg-Leaderboards a practically viable tool for
real-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the
leaderboard extraction task to an automated digitalization task, which has
been, for a long time in the community, a crowdsourced endeavor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11070">
<div class="article-summary-box-inner">
<span><p>A considerable number of texts encountered daily are somehow connected with
each other. For example, Wikipedia articles refer to other articles via
hyperlinks, scientific papers relate to others via citations or (co)authors,
while tweets relate via users that follow each other or reshare content. Hence,
a graph-like structure can represent existing connections and be seen as
capturing the "context" of the texts. The question thus arises if extracting
and integrating such context information into a language model might help
facilitate a better automated understanding of the text. In this study, we
experimentally demonstrate that incorporating graph-based contextualization
into BERT model enhances its performance on an example of a classification
task. Specifically, on Pubmed dataset, we observed a reduction in error from
8.51% to 7.96%, while increasing the number of parameters just by 1.6%.
</p>
<p>Our source code: https://github.com/tryptofanik/gc-bert
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering. (arXiv:2305.11072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11072">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation models have succeeded in various tasks,
but improving them for content-related problems using unlabeled data is
challenging. We propose speaker-invariant clustering (Spin), a novel
self-supervised learning method that clusters speech representations and
performs swapped prediction between the original and speaker-perturbed
utterances. Spin disentangles speaker information and preserves content
representations with just 45 minutes of fine-tuning on a single GPU. Spin
improves pre-trained networks and outperforms prior methods in speech
recognition and acoustic unit discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11073">
<div class="article-summary-box-inner">
<span><p>Conformer, a convolution-augmented Transformer variant, has become the de
facto encoder architecture for speech processing due to its superior
performance in various tasks, including automatic speech recognition (ASR),
speech translation (ST) and spoken language understanding (SLU). Recently, a
new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech
ASR benchmark, making it promising for more general speech applications. This
work compares E-Branchformer and Conformer through extensive experiments using
different types of end-to-end sequence-to-sequence models. Results demonstrate
that E-Branchformer achieves comparable or better performance than Conformer in
almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while
being more stable during training. We will release our training configurations
and pre-trained models for reproducibility, which can benefit the speech
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting the Geographical Representativeness of Images from Text-to-Image Models. (arXiv:2305.11080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11080">
<div class="article-summary-box-inner">
<span><p>Recent progress in generative models has resulted in models that produce both
realistic as well as relevant images for most textual inputs. These models are
being used to generate millions of images everyday, and hold the potential to
drastically impact areas such as generative art, digital marketing and data
augmentation. Given their outsized impact, it is important to ensure that the
generated content reflects the artifacts and surroundings across the globe,
rather than over-representing certain parts of the world. In this paper, we
measure the geographical representativeness of common nouns (e.g., a house)
generated through DALL.E 2 and Stable Diffusion models using a crowdsourced
study comprising 540 participants across 27 countries. For deliberately
underspecified inputs without country names, the generated images most reflect
the surroundings of the United States followed by India, and the top
generations rarely reflect surroundings from all other countries (average score
less than 3 out of 5). Specifying the country names in the input increases the
representativeness by 1.44 points on average for DALL.E 2 and 0.75 for Stable
Diffusion, however, the overall scores for many countries still remain low,
highlighting the need for future models to be more geographically inclusive.
Lastly, we examine the feasibility of quantifying the geographical
representativeness of generated images without conducting user studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11095">
<div class="article-summary-box-inner">
<span><p>We investigate the emergent abilities of the recently proposed web-scale
speech model Whisper, by adapting it to unseen tasks with prompt engineering.
We selected three tasks: audio-visual speech recognition (AVSR), code-switched
speech recognition (CS-ASR), and speech translation (ST) on unseen language
pairs. We design task-specific prompts, by either leveraging another
large-scale model, or simply manipulating the special tokens in the default
prompts. Experiments show that compared to the default prompts, our proposed
prompts improve performance by 10% to 45% on the three zero-shot tasks, and
even outperform SotA supervised models on some datasets. In addition, our
experiments reveal many interesting properties of Whisper, including its
robustness to prompts, bias on accents, and the multilingual understanding in
its latent space. Code is available at
https://github.com/jasonppy/PromptingWhisper
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTriggER: Label-Efficient and Robust Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04726">
<div class="article-summary-box-inner">
<span><p>Deep neural models for named entity recognition (NER) have shown impressive
results in overcoming label scarcity and generalizing to unseen entities by
leveraging distant supervision and auxiliary information such as explanations.
However, the costs of acquiring such additional information are generally
prohibitive. In this paper, we present a novel two-stage framework
(AutoTriggER) to improve NER performance by automatically generating and
leveraging ``entity triggers'' which are human-readable cues in the text that
help guide the model to make better decisions. Our framework leverages post-hoc
explanation to generate rationales and strengthens a model's prior knowledge
using an embedding interpolation technique. This approach allows models to
exploit triggers to infer entity boundaries and types instead of solely
memorizing the entity words themselves. Through experiments on three
well-studied NER datasets, AutoTriggER shows strong label-efficiency, is
capable of generalizing to unseen entities, and outperforms the RoBERTa-CRF
baseline by nearly 0.5 F1 points on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation. (arXiv:2202.13047v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13047">
<div class="article-summary-box-inner">
<span><p>Crowdsourced dialogue corpora are usually limited in scale and topic coverage
due to the expensive cost of data curation. This would hinder the
generalization of downstream dialogue models to open-domain topics. In this
work, we leverage large language models for dialogue augmentation in the task
of emotional support conversation (ESC). By treating dialogue augmentation as a
dialogue completion task, we prompt a fine-tuned language model to complete
full dialogues from available dialogue posts of various topics, which are then
postprocessed based on heuristics. Applying this approach, we construct AugESC,
an augmented dataset for the ESC task, which largely extends the scale and
topic coverage of the crowdsourced ESConv corpus. Through comprehensive human
evaluation, we demonstrate that our approach is superior to strong baselines of
dialogue augmentation and that AugESC has comparable dialogue quality to the
crowdsourced corpus. We also conduct human interactive evaluation and prove
that post-training on AugESC improves downstream dialogue models'
generalization ability to open-domain topics. These results suggest the utility
of AugESC and highlight the potential of large language models in improving
data-scarce dialogue generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16973">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) to learn high-level speech representations has
been a popular approach to building Automatic Speech Recognition (ASR) systems
in low-resource settings. However, the common assumption made in literature is
that a considerable amount of unlabeled data is available for the same domain
or language that can be leveraged for SSL pre-training, which we acknowledge is
not feasible in a real-world setting. In this paper, as part of the Interspeech
Gram Vaani ASR challenge, we try to study the effect of domain, language,
dataset size, and other aspects of our upstream pre-training SSL data on the
final performance low-resource downstream ASR task. We also build on the
continued pre-training paradigm to study the effect of prior knowledge
possessed by models trained using SSL. Extensive experiments and studies reveal
that the performance of ASR systems is susceptible to the data used for SSL
pre-training. Their performance improves with an increase in similarity and
volume of pre-training data. We believe our work will be helpful to the speech
community in building better ASR systems in low-resource settings and steer
research towards improving generalization in SSL-based pre-training for speech
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03276">
<div class="article-summary-box-inner">
<span><p>Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
in wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layer's
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from the $i$-th layer, introduces major variance in exit layer indices,
significantly reducing the resulting model's performance. In this paper, we
propose improving PonderNet with a novel deterministic Q-exit criterion and a
revisited model architecture. We adapted the proposed mechanism to ALBERT and
RoBERTa and compared it with recent methods for performing an early exit. We
observed that the proposed changes can be considered significant improvements
on the original PonderNet architecture and outperform PABEE on a wide range of
GLUE tasks. In addition, we also performed an in-depth ablation study of the
proposed architecture to further understand Lambda layers and their
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06674">
<div class="article-summary-box-inner">
<span><p>Recent improvements in KG-to-text generation are due to additional auxiliary
pre-training tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-training
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show our models are competitive while involving fewer
parameters and no additional pre-training tasks. By formulating the problem as
a framework, we can interchange the various proposed components and begin
interpreting KG-to-text generative models based on the topological and type
information found in a graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10505">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have delivered impressive results on many tasks,
particularly vision and language tasks. In many model training situations,
conventional configurations are typically adopted. For example, we often set
the base model with hidden dimensions (i.e. model width) to be 768 and the
number of transformer layers (i.e. model depth) to be 12. In this paper, we
revisit these conventional configurations. Through theoretical analysis and
experimental evaluation, we show that the masked autoencoder is effective in
alleviating the over-smoothing issue in deep transformer training. Based on
this finding, we propose Bamboo, an idea of using deeper and narrower
transformer configurations, for masked autoencoder training. On ImageNet, with
such a simple change in configuration, re-designed model achieves 87.1% top-1
accuracy and outperforms SoTA models like MAE and BEiT. On language tasks,
re-designed model outperforms BERT with default setting by 1.1 points on
average, on GLUE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking-Enhanced Unsupervised Sentence Representation Learning. (arXiv:2209.04333v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04333">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence representation learning has progressed through
contrastive learning and data augmentation methods such as dropout masking.
Despite this progress, sentence encoders are still limited to using only an
input sentence when predicting its semantic vector. In this work, we show that
the semantic meaning of a sentence is also determined by nearest-neighbor
sentences that are similar to the input sentence. Based on this finding, we
propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder
predicts the semantic vector of an input sentence by leveraging its
relationship with other sentences in an external corpus, as well as the input
sentence itself. We evaluate RankEncoder on semantic textual benchmark
datasets. From the experimental results, we verify that 1) RankEncoder achieves
80.07% Spearman's correlation, a 1.1% absolute improvement compared to the
previous state-of-the-art performance, 2) RankEncoder is universally applicable
to existing unsupervised sentence embedding methods, and 3) RankEncoder is
specifically effective for predicting the similarity scores of similar sentence
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Intersection of Context-Free and Regular Languages. (arXiv:2209.06809v2 [cs.FL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06809">
<div class="article-summary-box-inner">
<span><p>The Bar-Hillel construction is a classic result in formal language theory. It
shows, by a simple construction, that the intersection of a context-free
language and a regular language is itself context-free. In the construction,
the regular language is specified by a finite-state automaton. However, neither
the original construction (Bar-Hillel et al., 1961) nor its weighted extension
(Nederhof and Satta, 2003) can handle finite-state automata with
$\varepsilon$-arcs. While it is possible to remove $\varepsilon$-arcs from a
finite-state automaton efficiently without modifying the language, such an
operation modifies the automaton's set of paths. We give a construction that
generalizes the Bar-Hillel in the case where the desired automaton has
$\varepsilon$-arcs, and further prove that our generalized construction leads
to a grammar that encodes the structure of both the input automaton and grammar
while retaining the asymptotic size of the original construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07662">
<div class="article-summary-box-inner">
<span><p>We propose an approach for systematic reasoning that produces human
interpretable proof trees grounded in a factbase. Our approach evokes classic
Prolog-based inference engines, where we replace handcrafted rules by combining
neural language modeling, guided generation, and semiparametric dense
retrieval. We demonstrate this approach through a novel system, NELLIE, which
dynamically instantiates interpretable inference rules that capture and score
entailment (de)compositions over natural language statements. This leads to
strong performance, as shown in the scientific reasoning domain, while also
producing reasoning traces showing how answers derive logically from the
composition of human-verified facts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04982">
<div class="article-summary-box-inner">
<span><p>Generating free-text rationales is a promising step towards explainable NLP,
yet evaluating such rationales remains a challenge. Existing metrics have
mostly focused on measuring the association between the rationale and a given
label. We argue that an ideal metric should focus on the new information
uniquely provided in the rationale that is otherwise not provided in the input
or the label. We investigate this research problem from an
information-theoretic perspective using conditional V-information (Hewitt et
al., 2021). More concretely, we propose a metric called REV (Rationale
Evaluation with conditional V-information), to quantify the amount of new,
label-relevant information in a rationale beyond the information already
available in the input or the label. Experiments across four benchmarks with
reasoning tasks, including chain-of-thought, demonstrate the effectiveness of
REV in evaluating rationale-label pairs, compared to existing metrics. We
further demonstrate REV is consistent with human judgments on rationale
evaluations and provides more sensitive measurements of new information in
free-text rationales. When used alongside traditional performance metrics, REV
provides deeper insights into models' reasoning and prediction processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks. (arXiv:2210.08855v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08855">
<div class="article-summary-box-inner">
<span><p>Span identification aims at identifying specific text spans from text input
and classifying them into pre-defined categories. Different from previous works
that merely leverage the Subordinate (SUB) relation (i.e. if a span is an
instance of a certain category) to train models, this paper for the first time
explores the Peer (PR) relation, which indicates that two spans are instances
of the same category and share similar features. Specifically, a novel Peer
Data Augmentation (PeerDA) approach is proposed which employs span pairs with
the PR relation as the augmentation data for training. PeerDA has two unique
advantages: (1) There are a large number of PR span pairs for augmenting the
training data. (2) The augmented data can prevent the trained model from
over-fitting the superficial span-category mapping by pushing the model to
leverage the span semantics. Experimental results on ten datasets over four
diverse tasks across seven domains demonstrate the effectiveness of PeerDA.
Notably, PeerDA achieves state-of-the-art results on six of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00923">
<div class="article-summary-box-inner">
<span><p>The lack of labeled second language (L2) speech data is a major challenge in
designing mispronunciation detection models. We introduce SpeechBlender - a
fine-grained data augmentation pipeline for generating mispronunciation errors
to overcome such data scarcity. The SpeechBlender utilizes varieties of masks
to target different regions of phonetic units, and use the mixing factors to
linearly interpolate raw speech signals while augmenting pronunciation. The
masks facilitate smooth blending of the signals, generating more effective
samples than the `Cut/Paste' method. Our proposed technique achieves
state-of-the-art results, with Speechocean762, on ASR dependent
mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson
Correlation Coefficient (PCC) compared to the previous state-of-the-art [1].
Additionally, we demonstrate a 5.0% improvement at the phoneme level compared
to our baseline. We also observed a 4.6% increase in F1-score with Arabic
AraVoiceL2 testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01515">
<div class="article-summary-box-inner">
<span><p>We present Multiscale Audio Spectrogram Transformer (MAST) for audio
classification, which brings the concept of multiscale feature hierarchies to
the Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we
first patchify and project it into an initial temporal resolution and embedding
dimension, post which the multiple stages in MAST progressively expand the
embedding dimension while reducing the temporal resolution of the input. We use
a pyramid structure that allows early layers of MAST operating at a high
temporal resolution but low embedding space to model simple low-level acoustic
information and deeper temporally coarse layers to model high-level acoustic
information with high-dimensional embeddings. We also extend our approach to
present a new Self-Supervised Learning (SSL) method called SS-MAST, which
calculates a symmetric contrastive loss between latent representations from a
student and a teacher encoder, leveraging patch-drop, a novel audio
augmentation approach that we introduce. In practice, MAST significantly
outperforms AST by an average accuracy of 3.4% across 8 speech and non-speech
tasks from the LAPE Benchmark, achieving state-of-the-art results on keyword
spotting in Speech Commands. Additionally, our proposed SS-MAST achieves an
absolute average improvement of 2.6% over the previously proposed SSAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01519">
<div class="article-summary-box-inner">
<span><p>We present a new Self-Supervised Learning (SSL) approach to pre-train
encoders on unlabeled audio data that reduces the need for large amounts of
labeled data for audio and speech classification. Our primary aim is to learn
audio representations that can generalize across a large variety of speech and
non-speech tasks in a low-resource un-labeled audio pre-training setting.
Inspired by the recent success of clustering and contrasting learning paradigms
for SSL-based speech representation learning, we propose SLICER (Symmetrical
Learning of Instance and Cluster-level Efficient Representations), which brings
together the best of both clustering and contrasting learning paradigms. We use
a symmetric loss between latent representations from student and teacher
encoders and simultaneously solve instance and cluster-level contrastive
learning tasks. We obtain cluster representations online by just projecting the
input spectrogram into an output subspace with dimensions equal to the number
of clusters. In addition, we propose a novel mel-spectrogram augmentation
procedure, k-mix, based on mixup, which does not require labels and aids
unsupervised representation learning for audio. Overall, SLICER achieves
state-of-the-art results on the LAPE Benchmark \cite{9868132}, significantly
outperforming DeLoRes-M and other prior approaches, which are pre-trained on
$10\times$ larger of unsupervised data. We will make all our codes available on
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11483">
<div class="article-summary-box-inner">
<span><p>This work is intended as a voice in the discussion over the recent claims
that LaMDA, a pretrained language model based on the Transformer model
architecture, is sentient. This claim, if confirmed, would have serious
ramifications in the Natural Language Processing (NLP) community due to
wide-spread use of similar models. However, here we take the position that such
a language model cannot be sentient, or conscious, and that LaMDA in particular
exhibits no advances over other similar models that would qualify it. We
justify this by analysing the Transformer architecture through Integrated
Information Theory. We see the claims of consciousness as part of a wider
tendency to use anthropomorphic language in NLP reporting. Regardless of the
veracity of the claims, we consider this an opportune moment to take stock of
progress in language modelling and consider the ethical implications of the
task. In order to make this work helpful for readers outside the NLP community,
we also present the necessary background in language modelling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15661">
<div class="article-summary-box-inner">
<span><p>Neural sequence models, especially transformers, exhibit a remarkable
capacity for in-context learning. They can construct new predictors from
sequences of labeled examples $(x, f(x))$ presented in the input without
further parameter updates. We investigate the hypothesis that transformer-based
in-context learners implement standard learning algorithms implicitly, by
encoding smaller models in their activations, and updating these implicit
models as new examples appear in the context. Using linear regression as a
prototypical problem, we offer three sources of evidence for this hypothesis.
First, we prove by construction that transformers can implement learning
algorithms for linear models based on gradient descent and closed-form ridge
regression. Second, we show that trained in-context learners closely match the
predictors computed by gradient descent, ridge regression, and exact
least-squares regression, transitioning between different predictors as
transformer depth and dataset noise vary, and converging to Bayesian estimators
for large widths and depths. Third, we present preliminary evidence that
in-context learners share algorithmic features with these predictors: learners'
late layers non-linearly encode weight vectors and moment matrices. These
results suggest that in-context learning is understandable in algorithmic
terms, and that (at least in the linear case) learners may rediscover standard
estimation algorithms. Code and reference implementations are released at
https://github.com/ekinakyurek/google-research/blob/master/incontext.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00193">
<div class="article-summary-box-inner">
<span><p>Step-by-step reasoning approaches like chain of thought (CoT) have proved to
be very effective in inducing reasoning capabilities in large language models.
However, the success of the CoT approach is fundamentally tied to the model
size, and billion parameter-scale models are often needed to get CoT to work.
In this paper, we propose a knowledge distillation approach that leverages the
step-by-step CoT reasoning capabilities of larger models and distills these
abilities into smaller models.
</p>
<p>In this work, we propose an alternative reasoning scheme, Socratic CoT, that
learns a decomposition of the original problem into a sequence of subproblems
and uses it to guide the intermediate reasoning steps. We use Socratic CoT to
train a combination of two small distilled models: a problem decomposer and a
subproblem solver. In practice, given a new problem, the two distilled models
work in sync to decompose and solve complex problems. On multiple reasoning
datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies
boosts the performance of smaller models over 70% compared to the baselines.
Finally, we investigate when Socratic CoT is an effective alternative to CoT,
demonstrating cases where a much smaller model (GPT-2 large) can outperform a
10X larger model (GPT-3 6B). Our code is available here:
https://github.com/kumar-shridhar/Distiiling-LM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04205">
<div class="article-summary-box-inner">
<span><p>Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding
algorithm in Neural Machine Translation. However, MBR performs poorly with
label smoothing, which is surprising as label smoothing provides decent
improvement with beam search and improves generality in various tasks. In this
work, we show that the issue arises from the un-consistency of label smoothing
on the token-level and sequence-level distributions. We demonstrate that even
though label smoothing only causes a slight change in the token-level, the
sequence-level distribution is highly skewed. We coin the issue
\emph{autoregressive over-smoothness}. To address this issue, we propose a
simple and effective method, Distributional Cooling MBR (DC-MBR), which
manipulates the entropy of output distributions by tuning down the Softmax
temperature. We theoretically prove the equivalence between pre-tuning label
smoothing factor and distributional cooling. Extensive experiments on NMT
benchmarks validate that distributional cooling improves MBR in various
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04755">
<div class="article-summary-box-inner">
<span><p>We present Pre-trained Machine Reader (PMR), a novel method for retrofitting
pre-trained masked language models (MLMs) to pre-trained machine reading
comprehension (MRC) models without acquiring labeled data. PMR can resolve the
discrepancy between model pre-training and downstream fine-tuning of existing
MLMs. To build the proposed PMR, we constructed a large volume of
general-purpose and high-quality MRC-style training data by using Wikipedia
hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style
pre-training. Apart from its simplicity, PMR effectively solves extraction
tasks, such as Extractive Question Answering and Named Entity Recognition. PMR
shows tremendous improvements over existing approaches, especially in
low-resource scenarios. When applied to the sequence classification task in the
MRC formulation, PMR enables the extraction of high-quality rationales to
explain the classification process, thereby providing greater prediction
explainability. PMR also has the potential to serve as a unified model for
tackling various extraction and classification tasks in the MRC formulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07634">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models achieve superior performance but are
computationally expensive. Techniques such as pruning and knowledge
distillation have been developed to reduce their sizes and latencies. In this
work, we propose a structured pruning method GRAIN (Gradient-based
Intra-attention pruning), which performs task-specific pruning with knowledge
distillation and yields highly effective models. Different from common
approaches that prune each attention head as a whole, GRAIN inspects and prunes
intra-attention structures, which greatly expands the structure search space
and enables more flexible models. We also propose a gradient separation
strategy that reduces the interference of distillation on pruning for a better
combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003
show that GRAIN notably outperforms other methods, especially in the high
sparsity regime, and achieves $6\sim7\times$ speedups while maintaining
$93\%\sim99\%$ performance. Under extreme compression where only $3\%$
transformer weights remain, the pruned model is still competitive compared to
larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10409">
<div class="article-summary-box-inner">
<span><p>Context is everything, even in commonsense moral reasoning. Changing contexts
can flip the moral judgment of an action; "Lying to a friend" is wrong in
general, but may be morally acceptable if it is intended to protect their life.
</p>
<p>We present ClarifyDelphi, an interactive system that learns to ask
clarification questions (e.g., why did you lie to your friend?) in order to
elicit additional salient contexts of a social or moral situation. We posit
that questions whose potential answers lead to diverging moral judgments are
the most informative. Thus, we propose a reinforcement learning framework with
a defeasibility reward that aims to maximize the divergence between moral
judgments of hypothetical answers to a question. Human evaluation demonstrates
that our system generates more relevant, informative and defeasible questions
compared to competitive baselines. Our work is ultimately inspired by studies
in cognitive science that have investigated the flexibility in moral cognition
(i.e., the diverse contexts in which moral rules can be bent), and we hope that
research in this direction can assist both cognitive and computational
investigations of moral judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11259">
<div class="article-summary-box-inner">
<span><p>The generation of molecules with desired properties has gained tremendous
popularity, revolutionizing the way scientists design molecular structures and
providing valuable support for chemical and drug design. However, despite the
potential of language models in molecule generation, they face numerous
challenges such as the generation of syntactically or chemically flawed
molecules, narrow domain focus, and limitations in creating diverse and
directionally feasible molecules due to a dearth of annotated data or external
molecular databases. To this end, we introduce MolGen, a pre-trained molecular
language model tailored specifically for molecule generation. MolGen acquires
intrinsic structural and grammatical insights by reconstructing over 100
million molecular SELFIES, while facilitating knowledge transfer between
different domains through domain-agnostic molecular prefix tuning. Moreover, we
present a self-feedback paradigm that inspires the pre-trained model to align
with the ultimate goal of producing molecules with desirable properties.
Extensive experiments demonstrate that MolGen achieves superior performance on
well-known molecule generation benchmarks. Further analysis shows that MolGen
can accurately capture molecule distributions, implicitly learn their
structural characteristics, and efficiently explore chemical space. The
pre-trained model, codes, and datasets are publicly available for future
research at https://github.com/zjunlp/MolGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11879">
<div class="article-summary-box-inner">
<span><p>The ease and speed of spreading misinformation and propaganda on the Web
motivate the need to develop trustworthy technology for detecting fallacies in
natural language arguments. However, state-of-the-art language modeling methods
exhibit a lack of robustness on tasks like logical fallacy classification that
require complex reasoning. In this paper, we propose a Case-Based Reasoning
method that classifies new cases of logical fallacy by language-modeling-driven
retrieval and adaptation of historical cases. We design four complementary
strategies to enrich input representation for our model, based on external
information about goals, explanations, counterarguments, and argument
structure. Our experiments in in-domain and out-of-domain settings indicate
that Case-Based Reasoning improves the accuracy and generalizability of
language models. Our ablation studies suggest that representations of similar
cases have a strong impact on the model performance, that models perform well
with fewer retrieved cases, and that the size of the case database has a
negligible effect on the performance. Finally, we dive deeper into the
relationship between the properties of the retrieved cases and the model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12586">
<div class="article-summary-box-inner">
<span><p>The recent advances in neural language models have also been successfully
applied to the field of chemistry, offering generative solutions for classical
problems in molecular design and synthesis planning. These new methods have the
potential to fuel a new era of data-driven automation in scientific discovery.
However, specialized models are still typically required for each task, leading
to the need for problem-specific fine-tuning and neglecting task
interrelations. The main obstacle in this field is the lack of a unified
representation between natural language and chemical representations,
complicating and limiting human-machine interaction. Here, we propose the first
multi-domain, multi-task language model that can solve a wide range of tasks in
both the chemical and natural language domains. Our model can handle chemical
and natural language concurrently, without requiring expensive pre-training on
single domains or task-specific models. Interestingly, sharing weights across
domains remarkably improves our model when benchmarked against state-of-the-art
baselines on single-domain and cross-domain tasks. In particular, sharing
information across domains and tasks gives rise to large improvements in
cross-domain tasks, the magnitude of which increase with scale, as measured by
more than a dozen of relevant metrics. Our work suggests that such models can
robustly and efficiently accelerate discovery in physical sciences by
superseding problem-specific fine-tuning and enhancing human-model
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06829">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate whether symbolic semantic representations,
extracted from deep semantic parsers, can help reasoning over the states of
involved entities in a procedural text. We consider a deep semantic
parser~(TRIPS) and semantic role labeling as two sources of semantic parsing
knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural
reasoning framework. Second, we integrate semantic parsing information into
state-of-the-art neural models to conduct procedural reasoning. Our experiments
indicate that explicitly incorporating such semantic knowledge improves
procedural understanding. This paper presents new metrics for evaluating
procedural reasoning tasks that clarify the challenges and identify differences
among neural, symbolic, and integrated models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks. (arXiv:2304.01665v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01665">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) proficiency in handling deterministic symbolic
reasoning and rule-based tasks remains limited due to their dependency implicit
learning on textual data. To enable fully rule comprehension ability, we
explore how to incorporate compiled neural networks (CoNNs) which weight is
specially designed into the architecture of LMs, to achieve high accuracy and
robust performance. CoNNs are transformer-based neural networks that execute
rules through artificially generated attention weights. Our method, which call
"Neural Comprehension", by incorporating CoNN modules into the LM, the
framework effectively tackles rule-intensive challenges. Our experiments on
symbolic reasoning tasks and real-world arithmetic reasoning tasks demonstrate
the superior performance of our method compared to existing techniques.
Furthermore, our LM achieves flawless execution on symbolic operations tasks,
highlighting the potential of our method in enabling LMs to possess true
symbolic comprehension capabilities. Our code is publicly available at:
https://github.com/WENGSYX/Neural-Comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01890">
<div class="article-summary-box-inner">
<span><p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for
the countries of Brazil, Germany, India and Kenya, to aid training and
interpretability of models. We demonstrate how our lexicon can be used to
interpret model predictions, showing that models developed to classify extreme
speech rely heavily on target words when making predictions. Further, we
propose a method to aid shot selection for training in low-resource settings
via HATELEXICON. In few-shot learning, the selection of shots is of paramount
importance to model performance. In our work, we simulate a few-shot setting
for German and Hindi, using HASOC data for training and the Multilingual
HateCheck (MHC) as a benchmark. We show that selecting shots based on our
lexicon leads to models performing better on MHC than models trained on shots
sampled randomly. Thus, when given only a few training examples, using our
lexicon to select shots containing more sociocultural information leads to
better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05197">
<div class="article-summary-box-inner">
<span><p>With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given appropriate prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause new privacy
threats. To this end, we conduct extensive experiments to support our claims
and discuss LLMs' privacy implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10784">
<div class="article-summary-box-inner">
<span><p>Eye movements during reading offer insights into both the reader's cognitive
processes and the characteristics of the text that is being read. Hence, the
analysis of scanpaths in reading have attracted increasing attention across
fields, ranging from cognitive science over linguistics to computer science. In
particular, eye-tracking-while-reading data has been argued to bear the
potential to make machine-learning-based language models exhibit a more
human-like linguistic behavior. However, one of the main challenges in modeling
human scanpaths in reading is their dual-sequence nature: the words are ordered
following the grammatical rules of the language, whereas the fixations are
chronologically ordered. As humans do not strictly read from left-to-right, but
rather skip or refixate words and regress to previous words, the alignment of
the linguistic and the temporal sequence is non-trivial. In this paper, we
develop Eyettention, the first dual-sequence model that simultaneously
processes the sequence of words and the chronological sequence of fixations.
The alignment of the two sequences is achieved by a cross-sequence attention
mechanism. We show that Eyettention outperforms state-of-the-art models in
predicting scanpaths. We provide an extensive within- and across-data set
evaluation on different languages. An ablation study and qualitative analysis
support an in-depth understanding of the model's behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12955">
<div class="article-summary-box-inner">
<span><p>Human language is full of compositional syntactic structures, and although
neural networks have contributed to groundbreaking improvements in computer
systems that process language, widely-used neural network architectures still
exhibit limitations in their ability to process syntax. To address this issue,
prior work has proposed adding stack data structures to neural networks,
drawing inspiration from theoretical connections between syntax and stacks.
However, these methods employ deterministic stacks that are designed to track
one parse at a time, whereas syntactic ambiguity, which requires a
nondeterministic stack to parse, is extremely common in language. In this
dissertation, we remedy this discrepancy by proposing a method of incorporating
nondeterministic stacks into neural networks. We develop a differentiable data
structure that efficiently simulates a nondeterministic pushdown automaton,
representing an exponential number of computations with a dynamic programming
algorithm. We incorporate this module into two predominant architectures:
recurrent neural networks (RNNs) and transformers. We show that this raises
their formal recognition power to arbitrary context-free languages, and also
aids training, even on deterministic context-free languages. Empirically,
neural networks with nondeterministic stacks learn context-free languages much
more effectively than prior stack-augmented models, including a language with
theoretically maximal parsing difficulty. We also show that an RNN augmented
with a nondeterministic stack is capable of surprisingly powerful behavior,
such as learning cross-serial dependencies, a well-known non-context-free
pattern. We demonstrate improvements on natural language modeling and provide
analysis on a syntactic generalization benchmark. This work represents an
important step toward building systems that learn to use syntax in more
human-like fashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01625">
<div class="article-summary-box-inner">
<span><p>Since the proposal of transformers, these models have been limited to bounded
input lengths, because of their need to attend to every token in the input. In
this work, we propose Unlimiformer: a general approach that wraps any existing
pretrained encoder-decoder transformer, and offloads the cross-attention
computation to a single k-nearest-neighbor (kNN) index, while the returned kNN
distances are the attention dot-product scores. This kNN index can be kept on
either the GPU or CPU memory and queried in sub-linear time; this way, we can
index practically unlimited input sequences, while every attention head in
every decoder layer retrieves its top-k keys, instead of attending to every
key. We evaluate Unlimiformer on several long-document and book-summarization
benchmarks, showing that it can process even 500k token-long inputs from the
BookSum dataset, without any input truncation at test time. We demonstrate that
Unlimiformer improves pretrained models such as BART and Longformer by
extending them to unlimited inputs without additional learned weights and
without modifying their code. We make our code and models publicly available at
https://github.com/abertsch72/unlimiformer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02777">
<div class="article-summary-box-inner">
<span><p>Existing neural machine translation (NMT) studies mainly focus on developing
dataset-specific models based on data from different tasks (e.g., document
translation and chat translation). Although the dataset-specific models have
achieved impressive performance, it is cumbersome as each dataset demands a
model to be designed, trained, and stored. In this work, we aim to unify these
translation tasks into a more general setting. Specifically, we propose a
``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that
works with data from different tasks, and can translate well in multiple
settings simultaneously, and theoretically it can be as many as possible.
Through unified learning, UMLNMT is able to jointly train across multiple
tasks, implementing intelligent on-demand translation. On seven widely-used
translation tasks, including sentence translation, document translation, and
chat translation, our UMLNMT results in substantial improvements over
dataset-specific models with significantly reduced model deployment costs.
Furthermore, UMLNMT can achieve competitive or better performance than
state-of-the-art dataset-specific methods. Human evaluation and in-depth
analysis also demonstrate the superiority of our approach on generating diverse
and high-quality translations. Additionally, we provide a new genre translation
dataset about famous aphorisms with 186k Chinese-&gt;English sentence pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04757">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have significantly advanced natural language
processing (NLP) with their impressive language understanding and generation
capabilities. However, their performance may be suboptimal for domain-specific
tasks that require specialized knowledge due to limited exposure to the related
data. Additionally, the lack of transparency of most state-of-the-art (SOTA)
LLMs, which can only be accessed via APIs, impedes further fine-tuning with
domain custom data. Moreover, providing private data to the LLMs' owner leads
to data privacy problems. To address these challenges, we propose the novel
Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a
knowledge-guiding module to access relevant knowledge without altering the
LLMs' parameters. Our PKG is based on open-source "white-box" language models,
allowing offline memory of any knowledge that LLMs require. We demonstrate that
our PKG framework can enhance the performance of "black-box" LLMs on a range of
domain knowledge-intensive tasks that require factual (+7.9%), tabular
(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05252">
<div class="article-summary-box-inner">
<span><p>In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., "make a cake"), but leaves more specific goals with multi-facet
constraints understudied (e.g., "make a cake for diabetics"). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05352">
<div class="article-summary-box-inner">
<span><p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted significant attention on foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is little understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories:
foundation model pretraining and fine-tuning, architecture design of foundation
model based systems, and responsible-AI-by-design. This taxonomy provides
concrete guidance for making major design decisions when designing foundation
model based systems and highlights trade-offs arising from design decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07375">
<div class="article-summary-box-inner">
<span><p>Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT's upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07424">
<div class="article-summary-box-inner">
<span><p>Contrastive learning-based methods, such as unsup-SimCSE, have achieved
state-of-the-art (SOTA) performances in learning unsupervised sentence
embeddings. However, in previous studies, each embedding used for contrastive
learning only derived from one sentence instance, and we call these embeddings
instance-level embeddings. In other words, each embedding is regarded as a
unique class of its own, whichmay hurt the generalization performance. In this
study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to
smooth the boundaries of embeddings in the feature space. Specifically, we
retrieve embeddings from a dynamic memory buffer according to the semantic
similarity to get a positive embedding group. Then embeddings in the group are
aggregated by a self-attention operation to produce a smoothed instance
embedding for further analysis. We evaluate our method on standard semantic
text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%,
and 79.42% Spearman's correlation on the base of BERT-base, BERT-large,
RoBERTa-base, and RoBERTa-large respectively, a 2.05%, 1.06%, 1.16% and 0.52%
improvement compared to unsup-SimCSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation. (arXiv:2305.07804v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07804">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made remarkable strides in natural language
processing, but their expanding size poses challenges in terms of computational
expense and inefficiency. Conversely, Small Language Models (SLMs) are known
for their efficiency but often encounter difficulties in tasks with limited
capacity and training data, particularly in domain-specific scenarios. In this
paper, we introduce Dr. LLaMA, a method that improves SLMs in the medical
domain through generative data augmentation utilizing LLMs. The objective is to
develop more efficient and capable models tailored for specialized
applications. Our preliminary results on the PubMedQA dataset demonstrate that
LLMs effectively refine and diversify existing question-answer pairs, leading
to improved performance of a significantly smaller model after fine-tuning. The
best SLM surpasses few-shot GPT-4 with under 1.6 billion parameters on the
PubMedQA. Our code and generated data are publicly available to facilitate
further explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07969">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach for detecting ChatGPT-generated vs.
human-written text using language models. To this end, we first collected and
released a pre-processed dataset named OpenGPTText, which consists of rephrased
content generated using ChatGPT. We then designed, implemented, and trained two
different models for text classification, using Robustly Optimized BERT
Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),
respectively. Our models achieved remarkable results, with an accuracy of over
97% on the test dataset, as evaluated through various metrics. Furthermore, we
conducted an interpretability study to showcase our model's ability to extract
and differentiate key features between human-written and ChatGPT-generated
text. Our findings provide important insights into the effective use of
language models to detect generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08099">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have
demonstrated state-of-the-art performance on automatic speech recognition (ASR)
and proved to be extremely useful in low label-resource settings. However, the
success of SSL models has yet to transfer to utterance-level tasks such as
speaker, emotion, and language recognition, which still require supervised
fine-tuning of the SSL models to obtain good performance. We argue that the
problem is caused by the lack of disentangled representations and an
utterance-level learning objective for these tasks. Inspired by how HuBERT uses
clustering to discover hidden acoustic units, we formulate a factor analysis
(FA) model that uses the discovered hidden acoustic units to align the SSL
features. The underlying utterance-level representations are disentangled from
the content of speech using probabilistic inference on the aligned features.
Furthermore, the variational lower bound derived from the FA model provides an
utterance-level objective, allowing error gradients to be backpropagated to the
Transformer layers to learn highly discriminative acoustic units. When used in
conjunction with HuBERT's masked prediction training, our models outperform the
current best model, WavLM, on all utterance-level non-semantic tasks on the
SUPERB benchmark with only 20% of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08208">
<div class="article-summary-box-inner">
<span><p>There have been growing concerns regarding the out-of-domain generalization
ability of natural language processing (NLP) models, particularly in
question-answering (QA) tasks. Current synthesized data augmentation methods
for QA are hampered by increased training costs. To address this issue, we
propose a novel approach that combines prompting methods and linear probing
then fine-tuning strategy, which does not entail additional cost. Our method
has been theoretically and empirically shown to be effective in enhancing the
generalization ability of both generative and discriminative models. Our
approach outperforms state-of-the-art baselines, with an average increase in F1
score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any
pre-trained models and offers a promising solution to the under-explored
cross-domain QA task. We release our source code at GitHub*.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08285">
<div class="article-summary-box-inner">
<span><p>The increasing size of language models raises great research interests in
parameter-efficient fine-tuning such as LoRA that freezes the pre-trained
model, and injects small-scale trainable parameters for multiple downstream
tasks (e.g., summarization, question answering and translation). To further
enhance the efficiency of fine-tuning, we propose a framework that integrates
LoRA and structured layer pruning. The integrated framework is validated on two
created deidentified medical report summarization datasets based on
MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6%
parameters of the original model and pruning over 30% Transformer-layers, our
framework can reduce 50% of GPU memory usage and speed up 100% of the training
phase, while preserving over 92% generation qualities on free-text
sequence-to-sequence tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08503">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have accomplished impressive achievements
in abstractive single-document summarization (SDS). However, such benefits may
not be readily extended to muti-document summarization (MDS), where the
interactions among documents are more complex. Previous works either design new
architectures or new pre-training objectives for MDS, or apply PLMs to MDS
without considering the complex document interactions. While the former does
not make full use of previous pre-training efforts and may not generalize well
across multiple domains, the latter cannot fully attend to the intricate
relationships unique to MDS tasks. In this paper, we enforce hierarchy on both
the encoder and decoder and seek to make better use of a PLM to facilitate
multi-document interactions for the MDS task. We test our design on 10 MDS
datasets across a wide range of domains. Extensive experiments show that our
proposed method can achieve consistent improvements on all these datasets,
outperforming the previous best models, and even achieving better or
competitive results as compared to some models with additional MDS pre-training
or larger model parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08566">
<div class="article-summary-box-inner">
<span><p>In this study, we analyze NLG automatic metrics based on whether human
evaluation aspect is used as context or objective to compute the metrics: (i)
Task-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as
Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse
NLG tasks, yet they have a weak correlation with human. Human-aligned metrics
(CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable
human-like qualities as training objective. However, their effectiveness at
discerning system-level performance and quality of system outputs remain
unclear.
</p>
<p>We present metric preference checklist as a framework to assess the
discriminative power of automatic metrics in three NLG tasks: Text
Summarization, Dialogue Response Generation, and Controlled Generation. We show
that multi-aspect human-aligned metric (UniEval) is not necessarily dominant
over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic
metrics (BLEU, BERTScore), particularly when a disagreement between human
evaluation aspects is present. We also show particular use cases in which
automatic metrics provide a better guidance than human on discriminating
system-level performance. Our proposed framework provides access: (i) for
verifying whether automatic metrics are faithful to human preference,
regardless their correlation level to human; and (ii) for scrutinizing the
strengths and limitations of NLG systems, which are often obscured by a
standard averaging method of evaluation scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08596">
<div class="article-summary-box-inner">
<span><p>Recent research has suggested that there are clear differences in the
language used in the Dark Web compared to that of the Surface Web. As studies
on the Dark Web commonly require textual analysis of the domain, language
models specific to the Dark Web may provide valuable insights to researchers.
In this work, we introduce DarkBERT, a language model pretrained on Dark Web
data. We describe the steps taken to filter and compile the text data used to
train DarkBERT to combat the extreme lexical and structural diversity of the
Dark Web that may be detrimental to building a proper representation of the
domain. We evaluate DarkBERT and its vanilla counterpart along with other
widely used language models to validate the benefits that a Dark Web domain
specific model offers in various use cases. Our evaluations show that DarkBERT
outperforms current language models and may serve as a valuable resource for
future research on the Dark Web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09860">
<div class="article-summary-box-inner">
<span><p>Recent advances in machine translation (MT) have shown that Minimum Bayes
Risk (MBR) decoding can be a powerful alternative to beam search decoding,
especially when combined with neural-based utility functions. However, the
performance of MBR decoding depends heavily on how and how many candidates are
sampled from the model. In this paper, we explore how different sampling
approaches for generating candidate lists for MBR decoding affect performance.
We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k
sampling. Based on our insights into their limitations, we experiment with the
recently proposed epsilon-sampling approach, which prunes away all tokens with
a probability smaller than epsilon, ensuring that each token in a sample
receives a fair probability mass. Through extensive human evaluations, we
demonstrate that MBR decoding based on epsilon-sampling significantly
outperforms not only beam search decoding, but also MBR decoding with all other
tested sampling methods across four language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09941">
<div class="article-summary-box-inner">
<span><p>Transgender and non-binary (TGNB) individuals disproportionately experience
discrimination and exclusion from daily life. Given the recent popularity and
adoption of language generation technologies, the potential to further
marginalize this population only grows. Although a multitude of NLP fairness
literature focuses on illuminating and addressing gender biases, assessing
gender harms for TGNB identities requires understanding how such identities
uniquely interact with societal gender norms and how they differ from gender
binary-centric perspectives. Such measurement frameworks inherently require
centering TGNB voices to help guide the alignment between gender-inclusive NLP
and whom they are intended to serve. Towards this goal, we ground our work in
the TGNB community and existing interdisciplinary literature to assess how the
social reality surrounding experienced marginalization by TGNB persons
contributes to and persists within Open Language Generation (OLG). By first
understanding their marginalization stressors, we evaluate (1) misgendering and
(2) harmful responses to gender disclosure. To do this, we introduce the TANGO
dataset, comprising of template-based text curated from real-world text within
a TGNB-oriented community. We discover a dominance of binary gender norms
within the models; LLMs least misgendered subjects in generated text when
triggered by prompts whose subjects used binary pronouns. Meanwhile,
misgendering was most prevalent when triggering generation with singular they
and neopronouns. When prompted with gender disclosures, LLM text contained
stigmatizing language and scored most toxic when triggered by TGNB gender
disclosure. Our findings warrant further research on how TGNB harms manifest in
LLMs and serve as a broader case study toward concretely grounding the design
of gender-inclusive AI in community voices and interdisciplinary literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10006">
<div class="article-summary-box-inner">
<span><p>Video snapshot compressive imaging (SCI) uses a two-dimensional detector to
capture consecutive video frames during a single exposure time. Following this,
an efficient reconstruction algorithm needs to be designed to reconstruct the
desired video frames. Although recent deep learning-based state-of-the-art
(SOTA) reconstruction algorithms have achieved good results in most tasks, they
still face the following challenges due to excessive model complexity and GPU
memory limitations: 1) these models need high computational cost, and 2) they
are usually unable to reconstruct large-scale video frames at high compression
ratios. To address these issues, we develop an efficient network for video SCI
by using dense connections and space-time factorization mechanism within a
single residual block, dubbed EfficientSCI. The EfficientSCI network can well
establish spatial-temporal correlation by using convolution in the spatial
domain and Transformer in the temporal domain, respectively. We are the first
time to show that an UHD color video with high compression ratio can be
reconstructed from a snapshot 2D measurement using a single end-to-end deep
learning model with PSNR above 32 dB. Extensive results on both simulation and
real data show that our method significantly outperforms all previous SOTA
algorithms with better real-time performance. The code is at
https://github.com/ucaswangls/EfficientSCI.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10250">
<div class="article-summary-box-inner">
<span><p>Revolutionary advancements in Large Language Models have drastically reshaped
our interactions with artificial intelligence systems. Despite this, a notable
hindrance remains-the deficiency of a long-term memory mechanism within these
models. This shortfall becomes increasingly evident in situations demanding
sustained interaction, such as personal companion systems and psychological
counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored
for LLMs. MemoryBank enables the models to summon relevant memories,
continually evolve through continuous memory updates, comprehend, and adapt to
a user personality by synthesizing information from past interactions. To mimic
anthropomorphic behaviors and selectively preserve memory, MemoryBank
incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting
Curve theory, which permits the AI to forget and reinforce memory based on time
elapsed and the relative significance of the memory, thereby offering a
human-like memory mechanism. MemoryBank is versatile in accommodating both
closed-source models like ChatGPT and open-source models like ChatGLM. We
exemplify application of MemoryBank through the creation of an LLM-based
chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned
with psychological dialogs, SiliconFriend displays heightened empathy in its
interactions. Experiment involves both qualitative analysis with real-world
user dialogs and quantitative analysis with simulated dialogs. In the latter,
ChatGPT acts as users with diverse characteristics and generates long-term
dialog contexts covering a wide array of topics. The results of our analysis
reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong
capability for long-term companionship as it can provide emphatic response,
recall relevant memories and understand user personality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10307">
<div class="article-summary-box-inner">
<span><p>Measuring the distance between machine-produced and human language is a
critical open problem. Inspired by empirical findings from psycholinguistics on
the periodicity of entropy in language, we propose FACE, a set of metrics based
on Fourier Analysis of the estimated Cross-Entropy of language, for measuring
the similarity between model-generated and human-written languages. Based on an
open-ended generation task and the experimental data from previous studies, we
find that FACE can effectively identify the human-model gap, scales with model
size, reflects the outcomes of different sampling methods for decoding,
correlates well with other evaluation metrics and with human judgment scores.
FACE is computationally efficient and provides intuitive interpretations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10383">
<div class="article-summary-box-inner">
<span><p>Labeling data is essential for training text classifiers but is often
difficult to accomplish accurately, especially for complex and abstract
concepts. Seeking an improved method, this paper employs a novel approach using
a generative language model (GPT-4) to produce labels and rationales for
large-scale text analysis. We apply this approach to the task of discovering
public value expressions in US AI patents. We collect a database comprising
154,934 patent documents using an advanced Boolean query submitted to
InnovationQ+. The results are merged with full patent text from the USPTO,
resulting in 5.4 million sentences. We design a framework for identifying and
labeling public value expressions in these AI patent sentences. A prompt for
GPT-4 is developed which includes definitions, guidelines, examples, and
rationales for text classification. We evaluate the quality of the labels and
rationales produced by GPT-4 using BLEU scores and topic modeling and find that
they are accurate, diverse, and faithful. These rationales also serve as a
chain-of-thought for the model, a transparent mechanism for human verification,
and support for human annotators to overcome cognitive limitations. We conclude
that GPT-4 achieved a high-level of recognition of public value theory from our
framework, which it also uses to discover unseen public value expressions. We
use the labels produced by GPT-4 to train BERT-based classifiers and predict
sentences on the entire database, achieving high F1 scores for the 3-class
(0.85) and 2-class classification (0.91) tasks. We discuss the implications of
our approach for conducting large-scale text analyses with complex and abstract
concepts and suggest that, with careful framework design and interactive human
oversight, generative language models can offer significant advantages in
quality and in reduced time and costs for producing labels and rationales.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-19 23:11:25.773363866 UTC">2023-05-19 23:11:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>