<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-07-13T01:30:00Z">07-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05492">
<div class="article-summary-box-inner">
<span><p>In this pilot study, we investigate the use of GPT4 to assist in the
peer-review process. Our key hypothesis was that GPT-generated reviews could
achieve comparable helpfulness to human reviewers. By comparing reviews
generated by both human reviewers and GPT models for academic papers submitted
to a major machine learning conference, we provide initial evidence that
artificial intelligence can contribute effectively to the peer-review process.
We also perform robustness experiments with inserted errors to understand which
parts of the paper the model tends to focus on. Our findings open new avenues
for leveraging machine learning tools to address resource constraints in peer
review. The results also shed light on potential enhancements to the review
process and lay the groundwork for further research on scaling oversight in a
domain where human-feedback is increasingly a scarce resource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05493">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to
support English as a foreign language (EFL) students' writing, to effectively
collaborate with it, a student must learn to engineer prompts, that is, the
skill of crafting appropriate instructions so that ChatGPT produces desired
outputs. However, writing an appropriate prompt for ChatGPT is not
straightforward for non-technical users who suffer a trial-and-error process.
This paper examines the content of EFL students' ChatGPT prompts when
completing a writing task and explores patterns in the quality and quantity of
the prompts. The data come from iPad screen recordings of secondary school EFL
students who used ChatGPT and other SOTA chatbots for the first time to
complete the same writing task. The paper presents a case study of four
distinct pathways that illustrate the trial-and-error process and show
different combinations of prompt content and quantity. The cases contribute
evidence for the need to provide prompt engineering education in the context of
the EFL writing classroom, if students are to move beyond an individual
trial-and-error process, learning a greater variety of prompt content and more
sophisticated prompts to support their writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators. (arXiv:2307.05532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05532">
<div class="article-summary-box-inner">
<span><p>Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancements in Scientific Controllable Text Generation Methods. (arXiv:2307.05538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05538">
<div class="article-summary-box-inner">
<span><p>The previous work on controllable text generation is organized using a new
schema we provide in this study. Seven components make up the schema, and each
one is crucial to the creation process. To accomplish controlled generation for
scientific literature, we describe the various modulation strategies utilised
to modulate each of the seven components. We also offer a theoretical study and
qualitative examination of these methods. This insight makes possible new
architectures based on combinations of these components. Future research will
compare these methods empirically to learn more about their strengths and
utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05553">
<div class="article-summary-box-inner">
<span><p>The first automated essay scoring system was developed 50 years ago.
Automated essay scoring systems are developing into systems with richer
functions than the previous simple scoring systems. Its purpose is not only to
score essays but also as a learning tool to improve the writing skill of users.
Feedback is the most important aspect of making an automated essay scoring
system useful in real life. The importance of feedback was already emphasized
in the first AES system. This paper reviews research on feedback including
different feedback types and essay traits on automated essay scoring. We also
reviewed the latest case studies of the automated essay scoring system that
provides feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System. (arXiv:2307.05560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05560">
<div class="article-summary-box-inner">
<span><p>The disease coding task involves assigning a unique identifier from a
controlled vocabulary to each disease mentioned in a clinical document. This
task is relevant since it allows information extraction from unstructured data
to perform, for example, epidemiological studies about the incidence and
prevalence of diseases in a determined context. However, the manual coding
process is subject to errors as it requires medical personnel to be competent
in coding rules and terminology. In addition, this process consumes a lot of
time and energy, which could be allocated to more clinically relevant tasks.
These difficulties can be addressed by developing computational systems that
automatically assign codes to diseases. In this way, we propose a two-step
system for automatically coding diseases in referrals from the Chilean public
healthcare system. Specifically, our model uses a state-of-the-art NER model
for recognizing disease mentions and a search engine system based on
Elasticsearch for assigning the most relevant codes associated with these
disease mentions. The system's performance was evaluated on referrals manually
coded by clinical experts. Our system obtained a MAP score of 0.63 for the
subcategory level and 0.83 for the category level, close to the best-performing
models in the literature. This system could be a support tool for health
professionals, optimizing the coding and management process. Finally, to
guarantee reproducibility, we publicly release the code of our models and
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion. (arXiv:2307.05564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05564">
<div class="article-summary-box-inner">
<span><p>This paper describes our zero-shot approaches for the Visual Word Sense
Disambiguation (VWSD) Task in English. Our preliminary study shows that the
simple approach of matching candidate images with the phrase using CLIP suffers
from the many-to-many nature of image-text pairs. We find that the CLIP text
encoder may have limited abilities in capturing the compositionality in natural
language. Conversely, the descriptive focus of the phrase varies from instance
to instance. We address these issues in our two systems, Augment-CLIP and
Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt
by generating sentences that contain the context phrase with the help of large
language models (LLMs). We further explore CLIP models in other languages, as
the an ambiguous word may be translated into an unambiguous one in the other
language. SD Sampling uses text-to-image Stable Diffusion to generate multiple
images from the given phrase, increasing the likelihood that a subset of images
match the one that paired with the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Extraction as Question Generation and Answering. (arXiv:2307.05567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05567">
<div class="article-summary-box-inner">
<span><p>Recent work on Event Extraction has reframed the task as Question Answering
(QA), with promising results. The advantage of this approach is that it
addresses the error propagation issue found in traditional token-based
classification approaches by directly predicting event arguments without
extracting candidates first. However, the questions are typically based on
fixed templates and they rarely leverage contextual information such as
relevant arguments. In addition, prior QA-based approaches have difficulty
handling cases where there are multiple arguments for the same role. In this
paper, we propose QGA-EE, which enables a Question Generation (QG) model to
generate questions that incorporate rich contextual information instead of
using fixed templates. We also propose dynamic templates to assist the training
of QG model. Experiments show that QGA-EE outperforms all prior
single-task-based models on the ACE05 English dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05578">
<div class="article-summary-box-inner">
<span><p>The fast spread of hate speech on social media impacts the Internet
environment and our society by increasing prejudice and hurting people.
Detecting hate speech has aroused broad attention in the field of natural
language processing. Although hate speech detection has been addressed in
recent work, this task still faces two inherent unsolved challenges. The first
challenge lies in the complex semantic information conveyed in hate speech,
particularly the interference of insulting words in hate speech detection. The
second challenge is the imbalanced distribution of hate speech and non-hate
speech, which may significantly deteriorate the performance of models. To
tackle these challenges, we propose a novel dual contrastive learning (DCL)
framework for hate speech detection. Our framework jointly optimizes the
self-supervised and the supervised contrastive learning loss for capturing
span-level information beyond the token-level emotional semantics used in
existing models, particularly detecting speech containing abusive and insulting
words. Moreover, we integrate the focal loss into the dual contrastive learning
framework to alleviate the problem of data imbalance. We conduct experiments on
two publicly available English datasets, and experimental results show that the
proposed model outperforms the state-of-the-art models and precisely detects
hate speeches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05591">
<div class="article-summary-box-inner">
<span><p>Textual and semantic comprehension of images is essential for generating
proper captions. The comprehension requires detection of objects, modeling of
relations between them, an assessment of the semantics of the scene and,
finally, representing the extracted knowledge in a language space. To achieve
rich language capabilities while ensuring good image-language mappings,
pretrained language models (LMs) were conditioned on pretrained multi-modal
(image-text) models that allow for image inputs. This requires an alignment of
the image representation of the multi-modal model with the language
representations of a generative LM. However, it is not clear how to best
transfer semantics detected by the vision encoder of the multi-modal model to
the LM. We introduce two novel ways of constructing a linear mapping that
successfully transfers semantics between the embedding spaces of the two
pretrained models. The first aligns the embedding space of the multi-modal
language encoder with the embedding space of the pretrained LM via token
correspondences. The latter leverages additional data that consists of
image-text pairs to construct the mapping directly from vision to language
space. Using our semantic mappings, we unlock image captioning for LMs without
access to gradient information. By using different sources of data we achieve
strong captioning performance on MS-COCO and Flickr30k datasets. Even in the
face of limited data, our method partly exceeds the performance of other
zero-shot and even finetuned competitors. Our ablation studies show that even
LMs at a scale of merely 250M parameters can generate decent captions employing
our semantic mappings. Our approach makes image captioning more accessible for
institutions with restricted computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05627">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion (KGC) is the task of inferencing missing facts
from any given knowledge graphs (KG). Previous KGC methods typically represent
knowledge graph entities and relations as trainable continuous embeddings and
fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden
representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the
missing entities. To achieve this, they either use shallow linear
transformations or deep convolutional modules. However, the linear
transformations suffer from the expressiveness issue while the deep
convolutional modules introduce unnecessary inductive bias, which could
potentially degrade the model performance. Thus, we propose a novel
Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer
first segments the embedding into a sequence of patches and then employs
cross-attention modules to allow bi-directional embedding feature interaction
between the entities and relations, leading to a better understanding of the
underlying KG. We conduct experiments on four popular KGC benchmarks, WN18RR,
FB15k-237, YAGO37 and DB100K. The experimental results show significant
performance improvement from existing KGC methods on standard KGC evaluation
metrics, e.g., MRR and H@n. Our analysis first verifies the effectiveness of
our model design choices in PatReFormer. We then find that PatReFormer can
better capture KG information from a large relation embedding dimension.
Finally, we demonstrate that the strength of PatReFormer is at complex relation
types, compared to other KGC models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models. (arXiv:2307.05646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05646">
<div class="article-summary-box-inner">
<span><p>Customer feedback is invaluable to companies as they refine their products.
Monitoring customer feedback can be automated with Aspect Level Sentiment
Classification (ALSC) which allows us to analyse specific aspects of the
products in reviews. Large Language Models (LLMs) are the heart of many
state-of-the-art ALSC solutions, but they perform poorly in some scenarios
requiring Coreference Resolution (CR). In this work, we propose a framework to
improve an LLM's performance on CR-containing reviews by fine tuning on highly
inferential tasks. We show that the performance improvement is likely
attributed to the improved model CR ability. We also release a new dataset that
focuses on CR in ALSC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05695">
<div class="article-summary-box-inner">
<span><p>Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05696">
<div class="article-summary-box-inner">
<span><p>The exponential growth of textual data has created a crucial need for tools
that assist users in extracting meaningful insights. Traditional document
summarization approaches often fail to meet individual user requirements and
lack structure for efficient information processing. To address these
limitations, we propose Summation, a hierarchical personalized concept-based
summarization approach. It synthesizes documents into a concise hierarchical
concept map and actively engages users by learning and adapting to their
preferences. Using a Reinforcement Learning algorithm, Summation generates
personalized summaries for unseen documents on specific topics. This framework
enhances comprehension, enables effective navigation, and empowers users to
extract meaningful insights from large document collections aligned with their
unique requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05722">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized natural language processing
tasks, demonstrating their exceptional capabilities in various domains.
However, their potential for behavior graph understanding in job
recommendations remains largely unexplored. This paper focuses on unveiling the
capability of large language models in understanding behavior graphs and
leveraging this understanding to enhance recommendations in online recruitment,
including the promotion of out-of-distribution (OOD) application. We present a
novel framework that harnesses the rich contextual information and semantic
representations provided by large language models to analyze behavior graphs
and uncover underlying patterns and relationships. Specifically, we propose a
meta-path prompt constructor that leverages LLM recommender to understand
behavior graphs for the first time and design a corresponding path augmentation
module to alleviate the prompt bias introduced by path-based sequence input. By
leveraging this capability, our framework enables personalized and accurate job
recommendations for individual users. We evaluate the effectiveness of our
approach on a comprehensive dataset and demonstrate its ability to improve the
relevance and quality of recommended quality. This research not only sheds
light on the untapped potential of large language models but also provides
valuable insights for developing advanced recommendation systems in the
recruitment market. The findings contribute to the growing field of natural
language processing and offer practical implications for enhancing job search
experiences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05741">
<div class="article-summary-box-inner">
<span><p>As the application space of language models continues to evolve, a natural
question to ask is how we can quickly adapt models to new tasks. We approach
this classic question from a continual learning perspective, in which we aim to
continue fine-tuning models trained on past tasks on new tasks, with the goal
of "transferring" relevant knowledge. However, this strategy also runs the risk
of doing more harm than good, i.e., negative transfer. In this paper, we
construct a new benchmark of task sequences that target different possible
transfer scenarios one might face, such as a sequence of tasks with high
potential of positive transfer, high potential for negative transfer, no
expected effect, or a mixture of each. An ideal learner should be able to
maximally exploit information from all tasks that have any potential for
positive transfer, while also avoiding the negative effects of any distracting
tasks that may confuse it. We then propose a simple, yet effective, learner
that satisfies many of our desiderata simply by leveraging a selective strategy
for initializing new models from past task checkpoints. Still, limitations
remain, and we hope this benchmark can help the community to further build and
analyze such learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation Data Generation and Augmentation using ChatGPT. (arXiv:2307.05779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05779">
<div class="article-summary-box-inner">
<span><p>Neural models have revolutionized the field of machine translation, but
creating parallel corpora is expensive and time-consuming. We investigate an
alternative to manual parallel corpora - hallucinated parallel corpora created
by generative language models. Although these models are themselves trained on
parallel data, they can leverage a multilingual vector space to create data,
and may be able to supplement small manually-procured corpora. Our experiments
highlight two key findings - despite a lack of diversity in their output, the
hallucinated data improves the translation signal, even when the domain clashes
with the original dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models. (arXiv:2307.05782v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05782">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence is making spectacular progress, and one of the best
examples is the development of large language models (LLMs) such as OpenAI's
GPT series. In these lectures, written for readers with a background in
mathematics or physics, we give a brief history and survey of the state of the
art, and describe the underlying transformer architecture in detail. We then
explore some current ideas on how LLMs work and how models trained to predict
the next word in a text are able to perform other tasks displaying
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved POS tagging for spontaneous, clinical speech using data augmentation. (arXiv:2307.05796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05796">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of improving POS tagging of transcripts of
speech from clinical populations. In contrast to prior work on parsing and POS
tagging of transcribed speech, we do not make use of an in domain treebank for
training. Instead, we train on an out of domain treebank of newswire using data
augmentation techniques to make these structures resemble natural, spontaneous
speech. We trained a parser with and without the augmented data and tested its
performance using manually validated POS tags in clinical speech produced by
patients with various types of neurodegenerative conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05827">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05908">
<div class="article-summary-box-inner">
<span><p>This paper presents "Predictive Pipelined Decoding (PPD)," an approach that
speeds up greedy decoding in Large Language Models (LLMs) while maintaining the
exact same output as the original decoding. Unlike conventional strategies, PPD
employs additional compute resources to parallelize the initiation of
subsequent token decoding during the current token decoding. This innovative
method reduces decoding latency and reshapes the understanding of trade-offs in
LLM decoding strategies. We have developed a theoretical framework that allows
us to analyze the trade-off between computation and latency. Using this
framework, we can analytically estimate the potential reduction in latency
associated with our proposed method, achieved through the assessment of the
match rate, represented as p_correct. The results demonstrate that the use of
extra computational resources has the potential to accelerate LLM greedy
decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Contrastive Transfer Learning for Multimodal Language Understanding. (arXiv:2307.05942v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05942">
<div class="article-summary-box-inner">
<span><p>Although domestic service robots are expected to assist individuals who
require support, they cannot currently interact smoothly with people through
natural language. For example, given the instruction "Bring me a bottle from
the kitchen," it is difficult for such robots to specify the bottle in an
indoor environment. Most conventional models have been trained on real-world
datasets that are labor-intensive to collect, and they have not fully leveraged
simulation data through a transfer learning framework. In this study, we
propose a novel transfer learning approach for multimodal language
understanding called Prototypical Contrastive Transfer Learning (PCTL), which
uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task
of identifying target objects in domestic environments according to free-form
natural language instructions. To validate PCTL, we built new real-world and
simulation datasets. Our experiment demonstrated that PCTL outperformed
existing methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas
simple fine-tuning achieved an accuracy of 73.4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05972">
<div class="article-summary-box-inner">
<span><p>We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05973">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06005">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has shown promising capability in learning
text representation. However, existing text-based NAS neither performs a
learnable fusion of neural operations to optimize the architecture, nor encodes
the latent hierarchical categorization behind text input. This paper presents a
novel NAS method, Discretized Differentiable Neural Architecture Search
(DDNAS), for text representation learning and classification. With the
continuous relaxation of architecture representation, DDNAS can use gradient
descent to optimize the search. We also propose a novel discretization layer
via mutual information maximization, which is imposed on every search node to
model the latent hierarchical categorization in text representation. Extensive
experiments conducted on eight diverse real datasets exhibit that DDNAS can
consistently outperform the state-of-the-art NAS methods. While DDNAS relies on
only three basic operations, i.e., convolution, pooling, and none, to be the
candidates of NAS building blocks, its promising performance is noticeable and
extensible to obtain further improvement by adding more different operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06018">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) demonstrate remarkable ability to comprehend,
reason, and generate following nature language instructions. However, the
development of LLMs has been primarily focused on high-resource languages, such
as English, thereby limiting their applicability and research in other
languages. Consequently, we present PolyLM, a multilingual LLM trained on 640
billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its
multilingual capabilities, we 1) integrate bilingual data into training data;
and 2) adopt a curriculum learning strategy that increases the proportion of
non-English data from 30% in the first stage to 60% in the final stage during
pre-training. Further, we propose a multilingual self-instruct method which
automatically generates 132.7K diverse multilingual instructions for model
fine-tuning. To assess the model's performance, we collect several existing
multilingual tasks, including multilingual understanding, question answering,
generation, and translation. Extensive experiments show that PolyLM surpasses
other open-source models such as LLaMA and BLOOM on multilingual tasks while
maintaining comparable performance in English. Our models, alone with the
instruction data and multilingual benchmark, are available at:
\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06029">
<div class="article-summary-box-inner">
<span><p>Although neural machine translation (NMT) models perform well in the general
domain, it remains rather challenging to control their generation behavior to
satisfy the requirement of different users. Given the expensive training cost
and the data scarcity challenge of learning a new model from scratch for each
user requirement, we propose a memory-augmented adapter to steer pretrained NMT
models in a pluggable manner. Specifically, we construct a multi-granular
memory based on the user-provided text samples and propose a new adapter
architecture to combine the model representations and the retrieved results. We
also propose a training strategy using memory dropout to reduce spurious
dependencies between the NMT model and the memory. We validate our approach on
both style- and domain-specific experiments and the results indicate that our
method can outperform several representative pluggable baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Appropriate size of the Mongolian general corpus. (arXiv:2307.06050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06050">
<div class="article-summary-box-inner">
<span><p>This study aims to determine the appropriate size of the Mongolian general
corpus. This study used the Heaps function and Type Token Ratio to determine
the appropriate size of the Mongolian general corpus. The sample corpus of
906,064 tokens comprised texts from 10 domains of newspaper politics, economy,
society, culture, sports, world articles and laws, middle and high school
literature textbooks, interview articles, and podcast transcripts. First, we
estimated the Heaps function with this sample corpus. Next, we observed changes
in the number of types and TTR values while increasing the number of tokens by
one million using the estimated Heaps function. As a result of observation, we
found that the TTR value hardly changed when the number of tokens exceeded from
39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian
general corpus is from 39 to 42 million tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06060">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach for interpreting deep embeddings in the context
of patient clustering. We evaluate our approach on a dataset of participants
with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful
insights into disease progression patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06082">
<div class="article-summary-box-inner">
<span><p>Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing. (arXiv:2307.06124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06124">
<div class="article-summary-box-inner">
<span><p>Current signing avatars are often described as unnatural as they cannot
accurately reproduce all the subtleties of synchronized body behaviors of a
human signer. In this paper, we propose a new dynamic approach for transitions
between signs, focusing on mouthing animations for Portuguese Sign Language.
Although native signers preferred animations with dynamic transitions, we did
not find significant differences in comprehension and perceived naturalness
scores. On the other hand, we show that including mouthing behaviors improved
comprehension and perceived naturalness for novice sign language learners.
Results have implications in computational linguistics, human-computer
interaction, and synthetic animation of signing avatars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06187">
<div class="article-summary-box-inner">
<span><p>In autonomic computing, self-adaptation has been proposed as a fundamental
paradigm to manage the complexity of multiagent systems (MASs). This achieved
by extending a system with support to monitor and adapt itself to achieve
specific concerns of interest. Communication in these systems is key given that
in scenarios involving agent interaction, it enhances cooperation and reduces
coordination challenges by enabling direct, clear information exchange.
However, improving the expressiveness of the interaction communication with
MASs is not without challenges. In this sense, the interplay between
self-adaptive systems and effective communication is crucial for future MAS
advancements. In this paper, we propose the integration of large language
models (LLMs) such as GPT-based technologies into multiagent systems. We anchor
our methodology on the MAPE-K model, which is renowned for its robust support
in monitoring, analyzing, planning, and executing system adaptations in
response to dynamic environments. We also present a practical illustration of
the proposed approach, in which we implement and assess a basic MAS-based
application. The approach significantly advances the state-of-the-art of
self-adaptive systems by proposing a new paradigm for MAS self-adaptation of
autonomous systems based on LLM capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches. (arXiv:2307.06218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06218">
<div class="article-summary-box-inner">
<span><p>Poetry holds immense significance within the cultural and traditional fabric
of any nation. It serves as a vehicle for poets to articulate their emotions,
preserve customs, and convey the essence of their culture. Arabic poetry is no
exception, having played a cherished role in the heritage of the Arabic
community throughout history and maintaining its relevance in the present era.
Typically, comprehending Arabic poetry necessitates the expertise of a linguist
who can analyze its content and assess its quality. This paper presents the
introduction of a framework called \textit{Ashaar}
https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and
pre-trained models designed specifically for the analysis and generation of
Arabic poetry. The pipeline established within our proposed approach
encompasses various aspects of poetry, such as meter, theme, and era
classification. It also incorporates automatic poetry diacritization, enabling
more intricate analyses like automated extraction of the \textit{Arudi} style.
Additionally, we explore the feasibility of generating conditional poetry
through the pre-training of a character-based GPT model. Furthermore, as part
of this endeavor, we provide four datasets: one for poetry generation, another
for diacritization, and two for Arudi-style prediction. These datasets aim to
facilitate research and development in the field of Arabic poetry by enabling
researchers and enthusiasts to delve into the nuances of this rich literary
tradition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06281">
<div class="article-summary-box-inner">
<span><p>Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model's abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model's predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06290">
<div class="article-summary-box-inner">
<span><p>Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models' ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v7 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good techniques providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, techniques from the
IR and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents a method for complex
questions that can seamlessly operate over a mixture of RDF datasets and text
corpora, or individual sources, in a unified framework. Our method, called
UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant
evidences from the RDF data and/or a text corpus, using fine-tuned BERT models.
The resulting graph typically contains all question-relevant evidences but also
a lot of noise. UNIQORN copes with this input by a graph algorithm for Group
Steiner Trees, that identifies the best answer candidates in the context graph.
Experimental results on several benchmarks of complex questions with multiple
entities and relations, show that UNIQORN significantly outperforms
state-of-the-art methods for heterogeneous QA. The graph-based methodology
provides user-interpretable evidence for the complete answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis. (arXiv:2206.02892v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02892">
<div class="article-summary-box-inner">
<span><p>Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions
towards products and services. In the past, ABSA models were discriminative,
but more recently generative models have been used to generate aspects and
polarities directly from text. In contrast, discriminative models commonly
first select aspects from the text, and then classify the aspect's polarity.
Previous results showed that generative models outperform discriminative models
on several English ABSA datasets. Here, we evaluate and contrast two
state-of-the-art discriminative and generative models in several settings:
cross-lingual, cross-domain, and cross-lingual and domain, to understand
generalizability in settings other than English mono-lingual in-domain. Our
more thorough evaluation shows that, contrary to previous studies,
discriminative models can still outperform generative models in almost all
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks. (arXiv:2208.12081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.12081">
<div class="article-summary-box-inner">
<span><p>Indigenous African languages are categorized as under-served in Natural
Language Processing. They therefore experience poor digital inclusivity and
information access. The processing challenge with such languages has been how
to use machine learning and deep learning models without the requisite data.
The Kencorpus project intends to bridge this gap by collecting and storing text
and speech data that is good enough for data-driven solutions in applications
such as machine translation, question answering and transcription in
multilingual communities. The Kencorpus dataset is a text and speech corpus for
three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data
collection was done by researchers from communities, schools, media, and
publishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442
texts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of
Speech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)
were developed. We developed 7,537 Question-Answer pairs for Swahili and
created a text translation set of 13,400 sentences from Dholuo and Luhya into
Swahili. The datasets are useful for downstream machine learning tasks such as
model training and translation. We also developed two proof of concept systems:
for Kiswahili speech-to-text and machine learning system for Question Answering
task, with results of 18.87% word error rate and 80% Exact Match (EM)
respectively. These initial results give great promise to the usability of
Kencorpus to the machine learning community. Kencorpus is one of few public
domain corpora for these three low resource languages and forms a basis of
learning and sharing experiences for similar works especially for low resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Grammar-Based Coding Revisited. (arXiv:2209.13636v2 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13636">
<div class="article-summary-box-inner">
<span><p>We revisit the problem of minimal local grammar-based coding. In this
setting, the local grammar encoder encodes grammars symbol by symbol, whereas
the minimal grammar transform minimizes the grammar length in a preset class of
grammars as given by the length of local grammar encoding. It has been known
that such minimal codes are strongly universal for a strictly positive entropy
rate, whereas the number of rules in the minimal grammar constitutes an upper
bound for the mutual information of the source. Whereas the fully minimal code
is likely intractable, the constrained minimal block code can be efficiently
computed. In this article, we present a new, simpler, and more general proof of
strong universality of the minimal block code, regardless of the entropy rate.
The proof is based on a simple Zipfian bound for ranked probabilities. By the
way, we also show empirically that the number of rules in the minimal block
code cannot clearly discriminate between long-memory and memoryless sources,
such as a text in English and a random permutation of its characters. This
contradicts our previous expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15097">
<div class="article-summary-box-inner">
<span><p>Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00923">
<div class="article-summary-box-inner">
<span><p>The lack of labeled second language (L2) speech data is a major challenge in
designing mispronunciation detection models. We introduce SpeechBlender - a
fine-grained data augmentation pipeline for generating mispronunciation errors
to overcome such data scarcity. The SpeechBlender utilizes varieties of masks
to target different regions of phonetic units, and use the mixing factors to
linearly interpolate raw speech signals while augmenting pronunciation. The
masks facilitate smooth blending of the signals, generating more effective
samples than the `Cut/Paste' method. Our proposed technique achieves
state-of-the-art results, with Speechocean762, on ASR dependent
mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson
Correlation Coefficient (PCC) compared to the previous state-of-the-art [1].
Additionally, we demonstrate a 5.0% improvement at the phoneme level compared
to our baseline. We also observed a 4.6% increase in F1-score with Arabic
AraVoiceL2 testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09746">
<div class="article-summary-box-inner">
<span><p>Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09747">
<div class="article-summary-box-inner">
<span><p>The CoNLL-2003 English named entity recognition (NER) dataset has been widely
used to train and evaluate NER models for almost 20 years. However, it is
unclear how well models that are trained on this 20-year-old data and developed
over a period of decades using the same test set will perform when applied on
modern data. In this paper, we evaluate the generalization of over 20 different
models trained on CoNLL-2003, and show that NER models have very different
generalization. Surprisingly, we find no evidence of performance degradation in
pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using
decades-old data. We investigate why some models generalize well to new data
while others do not, and attempt to disentangle the effects of temporal drift
and overfitting due to test reuse. Our analysis suggests that most
deterioration is due to temporal mismatch between the pre-training corpora and
the downstream test sets. We found that four factors are important for good
generalization: model architecture, number of parameters, time period of the
pre-training corpus, in addition to the amount of fine-tuning data. We suggest
current evaluation methods have, in some sense, underestimated progress on NER
over the past 20 years, as NER models have not only improved on the original
CoNLL-2003 test set, but improved even more on modern data. Our datasets can be
found at https://github.com/ShuhengL/acl2023_conllpp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10258">
<div class="article-summary-box-inner">
<span><p>Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.06024">
<div class="article-summary-box-inner">
<span><p>The availability of quantitative text analysis methods has provided new ways
of analyzing literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear changes in the style
of writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare's writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09582">
<div class="article-summary-box-inner">
<span><p>Understanding how language supports emotion inference remains a topic of
debate in emotion science. The present study investigated whether
language-derived emotion-concept knowledge would causally support emotion
inference by manipulating the language-specific knowledge representations in
large language models. Using the prompt technique, 14 attributes of emotion
concepts were found to be represented by distinct artificial neuron
populations. By manipulating these attribute-related neurons, the majority of
the emotion inference tasks showed performance deterioration compared to random
manipulations. The attribute-specific performance deterioration was related to
the importance of different attributes in human mental space. Our findings
provide causal evidence in support of a language-based mechanism for emotion
inference and highlight the contributions of emotion-concept knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT detectors are biased against non-native English writers. (arXiv:2304.02819v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02819">
<div class="article-summary-box-inner">
<span><p>The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse. The published
version of this study can be accessed at:
www.cell.com/patterns/fulltext/S2666-3899(23)00130-7
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14108">
<div class="article-summary-box-inner">
<span><p>Multimodal datasets are a critical component in recent breakthroughs such as
Stable Diffusion and GPT-4, yet their design does not receive the same research
attention as model architectures or training algorithms. To address this
shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset
experiments centered around a new candidate pool of 12.8 billion image-text
pairs from Common Crawl. Participants in our benchmark design new filtering
techniques or curate new data sources and then evaluate their new dataset by
running our standardized CLIP training code and testing the resulting model on
38 downstream test sets. Our benchmark consists of multiple compute scales
spanning four orders of magnitude, which enables the study of scaling trends
and makes the benchmark accessible to researchers with varying resources. Our
baseline experiments show that the DataComp workflow leads to better training
sets. In particular, our best baseline, DataComp-1B, enables training a CLIP
ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming
OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training
procedure and compute. We release DataComp and all accompanying code at
www.datacomp.ai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04003">
<div class="article-summary-box-inner">
<span><p>Verification of machine learning models used in Natural Language Processing
(NLP) is known to be a hard problem. In particular, many known neural network
verification methods that work for computer vision and other numeric datasets
do not work for NLP. Here, we study technical reasons that underlie this
problem. Based on this analysis, we propose practical methods and heuristics
for preparing NLP datasets and models in a way that renders them amenable to
known verification methods based on abstract interpretation. We implement these
methods as a Python library called ANTONIO that links to the neural network
verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP
dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP
applications. We hope that, thanks to its general applicability, this work will
open novel possibilities for including NLP verification problems into neural
network verification competitions, and will popularise NLP problems within this
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04492">
<div class="article-summary-box-inner">
<span><p>Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08844">
<div class="article-summary-box-inner">
<span><p>Despite their unprecedented success, even the largest language models make
mistakes. Similar to how humans learn and improve using feedback, previous work
proposed providing language models with natural language feedback to guide them
in repairing their outputs. Because human-generated critiques are expensive to
obtain, researchers have devised learned critique generators in lieu of human
critics while assuming one can train downstream models to utilize generated
feedback. However, this approach does not apply to black-box or limited access
models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of
large general-purpose language agents, fine-tuning is neither computationally
nor spatially efficient as it results in multiple copies of the network. In
this work, we introduce RL4F (Reinforcement Learning for Feedback), a
multi-agent collaborative framework where the critique generator is trained to
maximize end-task performance of GPT-3, a fixed model more than 200 times its
size. RL4F produces critiques that help GPT-3 revise its outputs. We study
three datasets for action planning, summarization and alphabetization and show
relative improvements up to 10% in multiple text similarity metrics over other
learned, retrieval-augmented or prompting-based critique generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v5 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12493">
<div class="article-summary-box-inner">
<span><p>Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey. (arXiv:2305.18703v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18703">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05320">
<div class="article-summary-box-inner">
<span><p>Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which evaluates translation quality
on scientific conference talks. The test condition features accented input
speech and terminology-dense contents. The task requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05685">
<div class="article-summary-box-inner">
<span><p>Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80\% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K
conversations with human preferences from Chatbot Arena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. (arXiv:2306.06331v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06331">
<div class="article-summary-box-inner">
<span><p>This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08702">
<div class="article-summary-box-inner">
<span><p>We test similarity-based word alignment models (SimAlign and awesome-align)
in combination with word embeddings from mBERT and XLM-R on parallel sentences
in German and Romansh. Since Romansh is an unseen language, we are dealing with
a zero-shot setting. Using embeddings from mBERT, both models reach an
alignment error rate of 0.22, which outperforms fast_align, a statistical
model, and is on par with similarity-based word alignment for seen languages.
We interpret these results as evidence that mBERT contains information that can
be meaningful and applicable to Romansh.
</p>
<p>To evaluate performance, we also present a new trilingual corpus, which we
call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton
of Grisons in German, Romansh and Italian in the past 25 years. The corpus
contains 4 547 parallel documents and approximately 100 000 sentence pairs in
each language combination. We additionally present a gold standard for
German-Romansh word alignment. The data is available at
https://github.com/eyldlv/DERMIT-Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11197">
<div class="article-summary-box-inner">
<span><p>Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11270">
<div class="article-summary-box-inner">
<span><p>Instruction fine-tuning has recently emerged as a promising approach for
improving the zero-shot capabilities of Large Language Models (LLMs) on new
tasks. This technique has shown particular strength in improving the
performance of modestly sized LLMs, sometimes inducing performance competitive
with much larger model variants. In this paper we ask two questions: (1) How
sensitive are instruction-tuned models to the particular phrasings of
instructions, and, (2) How can we make them more robust to such natural
language variation? To answer the former, we collect a set of 319 instructions
manually written by NLP practitioners for over 80 unique tasks included in
widely used benchmarks, and we evaluate the variance and average performance of
these instructions as compared to instruction phrasings observed during
instruction fine-tuning. We find that using novel (unobserved) but appropriate
instruction phrasings consistently degrades model performance, sometimes
substantially so. Further, such natural instructions yield a wide variance in
downstream performance, despite their semantic equivalence. Put another way,
instruction-tuned models are not especially robust to instruction re-phrasings.
We propose a simple method to mitigate this issue by introducing ``soft
prompt'' embedding parameters and optimizing these to maximize the similarity
between representations of semantically equivalent instructions. We show that
this method consistently improves the robustness of instruction-tuned models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15745">
<div class="article-summary-box-inner">
<span><p>Online communities of involuntary celibates (incels) are a prominent source
of misogynist hate speech. In this paper, we use quantitative text and network
analysis approaches to examine how identity groups are discussed on
incels-dot-is, the largest black-pilled incels forum. We find that this
community produces a wide range of novel identity terms and, while terms for
women are most common, mentions of other minoritized identities are increasing.
An analysis of the associations made with identity groups suggests an
essentialist ideology where physical appearance, as well as gender and racial
hierarchies, determine human value. We discuss implications for research into
automated misogynist hate speech detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00470">
<div class="article-summary-box-inner">
<span><p>Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02682">
<div class="article-summary-box-inner">
<span><p>Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03042">
<div class="article-summary-box-inner">
<span><p>Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
However, this approach is increasingly proven to be impractical owing to the
substantial computational requirements associated with training such large
language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
techniques offer a viable solution by selectively fine-tuning a small subset of
additional parameters, significantly reducing the computational requirements
for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT
adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is
trained using clinical notes obtained from the MIMIC-IV database, thereby
creating a specialised adapter designed for the clinical domain. Additionally,
we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with
Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.
We evaluate this framework on multiple clinical outcome prediction datasets,
comparing it to clinically trained language models. Our proposed framework
achieves a state-of-the-art AUROC score averaged across all clinical downstream
tasks. We observe substantial improvements of 6-9% AUROC score in the
large-scale multilabel classification tasks, such as diagnoses and procedures
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03109">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion. (arXiv:2307.04518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.04518">
<div class="article-summary-box-inner">
<span><p>This document chronicles this author's attempt to explore how words come to
mean what they do, with a particular focus on child language acquisition and
what that means for models of language understanding.\footnote{I say
\emph{historical} because I synthesize the ideas based on when I discovered
them and how those ideas influenced my later thinking.} I explain the setting
for child language learning, how embodiment -- being able to perceive and enact
in the world, including knowledge of concrete and abstract concepts -- is
crucial, and how emotion and cognition relate to each other and the language
learning process. I end with what I think are some of the requirements for a
language-learning agent that learns language in a setting similar to that of
children. This paper can act as a potential guide for ongoing and future work
in modeling language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.05034">
<div class="article-summary-box-inner">
<span><p>We introduce a synthetic dataset called Sentences Involving Complex
Compositional Knowledge (SICCK) and a novel analysis that investigates the
performance of Natural Language Inference (NLI) models to understand
compositionality in logic. We produce 1,304 sentence pairs by modifying 15
examples from the SICK dataset (Marelli et al., 2014). To this end, we modify
the original texts using a set of phrases - modifiers that correspond to
universal quantifiers, existential quantifiers, negation, and other concept
modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to
modify the subject, verb, and object parts of the premise and hypothesis.
Lastly, we annotate these modified texts with the corresponding entailment
labels following NL rules. We conduct a preliminary verification of how well
the change in the structural and semantic composition is captured by neural NLI
models, in both zero-shot and fine-tuned scenarios. We found that the
performance of NLI models under the zero-shot setting is poor, especially for
modified sentences with negation and existential quantifiers. After fine-tuning
this dataset, we observe that models continue to perform poorly over negation,
existential and universal modifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Customized Text Sanitization Mechanism with Differential Privacy. (arXiv:2207.01193v2 [cs.CR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01193">
<div class="article-summary-box-inner">
<span><p>As privacy issues are receiving increasing attention within the Natural
Language Processing (NLP) community, numerous methods have been proposed to
sanitize texts subject to differential privacy. However, the state-of-the-art
text sanitization mechanisms based on metric local differential privacy (MLDP)
do not apply to non-metric semantic similarity measures and cannot achieve good
trade-offs between privacy and utility. To address the above limitations, we
propose a novel Customized Text (CusText) sanitization mechanism based on the
original $\epsilon$-differential privacy (DP) definition, which is compatible
with any similarity measure. Furthermore, CusText assigns each input token a
customized output set of tokens to provide more advanced privacy protection at
the token level. Extensive experiments on several benchmark datasets show that
CusText achieves a better trade-off between privacy and utility than existing
mechanisms. The code is available at https://github.com/sai4july/CusText.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-07-13 23:13:12.032944125 UTC">2023-07-13 23:13:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>