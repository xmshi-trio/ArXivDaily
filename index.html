<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-12T01:30:00Z">09-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges. (arXiv:2309.04550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04550">
<div class="article-summary-box-inner">
<span><p>Unstructured Electronic Health Record (EHR) data often contains critical
information complementary to imaging data that would inform radiologists'
diagnoses. However, time constraints and the large volume of notes frequently
associated with individual patients renders manual perusal of such data to
identify relevant evidence infeasible in practice. Modern Large Language Models
(LLMs) provide a flexible means of interacting with unstructured EHR data, and
may provide a mechanism to efficiently retrieve and summarize unstructured
evidence relevant to a given query. In this work, we propose and evaluate an
LLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we
task the LLM to infer whether a patient has or is at risk of a particular
condition; if so, we prompt the model to summarize the supporting evidence.
Enlisting radiologists for manual evaluation, we find that this LLM-based
approach provides outputs consistently preferred to a standard information
retrieval baseline, but we also highlight the key outstanding challenge: LLMs
are prone to hallucinating evidence. However, we provide results indicating
that model confidence in outputs might indicate when LLMs are hallucinating,
potentially providing a means to address this.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04561">
<div class="article-summary-box-inner">
<span><p>3D visual grounding is the task of localizing the object in a 3D scene which
is referred by a description in natural language. With a wide range of
applications ranging from autonomous indoor robotics to AR/VR, the task has
recently risen in popularity. A common formulation to tackle 3D visual
grounding is grounding-by-detection, where localization is done via bounding
boxes. However, for real-life applications that require physical interactions,
a bounding box insufficiently describes the geometry of an object. We therefore
tackle the problem of dense 3D visual grounding, i.e. referral-based 3D
instance segmentation. We propose a dense 3D grounding network ConcreteNet,
featuring three novel stand-alone modules which aim to improve grounding
performance for challenging repetitive instances, i.e. instances with
distractors of the same semantic class. First, we introduce a bottom-up
attentive fusion module that aims to disambiguate inter-instance relational
cues, next we construct a contrastive training scheme to induce separation in
the latent space, and finally we resolve view-dependent utterances via a
learned global camera token. ConcreteNet ranks 1st on the challenging ScanRefer
online benchmark by a considerable +9.43% accuracy at 50% IoU and has won the
ICCV 3rd Workshop on Language for 3D Scenes "3D Object Localization" challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04564">
<div class="article-summary-box-inner">
<span><p>Large volumes of text data have contributed significantly to the development
of large language models (LLMs) in recent years. This data is typically
acquired by scraping the internet, leading to pretraining datasets comprised of
noisy web text. To date, efforts to prune these datasets down to a higher
quality subset have relied on hand-crafted heuristics encoded as rule-based
filters. In this work, we take a wider view and explore scalable estimates of
data quality that can be used to systematically measure the quality of
pretraining data. We perform a rigorous comparison at scale of the simple data
quality estimator of perplexity, as well as more sophisticated and
computationally intensive estimates of the Error L2-Norm and memorization.
These metrics are used to rank and prune pretraining corpora, and we
subsequently compare LLMs trained on these pruned datasets. Surprisingly, we
find that the simple technique of perplexity outperforms our more
computationally expensive scoring methods. We improve over our no-pruning
baseline while training on as little as 30% of the original training dataset.
Our work sets the foundation for unexplored strategies in automatically
curating high quality corpora and suggests the majority of pretraining data can
be removed while retaining performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking Symptom Inventories using Semantic Textual Similarity. (arXiv:2309.04607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04607">
<div class="article-summary-box-inner">
<span><p>An extensive library of symptom inventories has been developed over time to
measure clinical symptoms, but this variety has led to several long standing
issues. Most notably, results drawn from different settings and studies are not
comparable, which limits reproducibility. Here, we present an artificial
intelligence (AI) approach using semantic textual similarity (STS) to link
symptoms and scores across previously incongruous symptom inventories. We
tested the ability of four pre-trained STS models to screen thousands of
symptom description pairs for related content - a challenging task typically
requiring expert panels. Models were tasked to predict symptom severity across
four different inventories for 6,607 participants drawn from 16 international
data sources. The STS approach achieved 74.8% accuracy across five tasks,
outperforming other models tested. This work suggests that incorporating
contextual, semantic information can assist expert decision-making processes,
yielding gains for both general and disease-specific clinical assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?. (arXiv:2309.04635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04635">
<div class="article-summary-box-inner">
<span><p>Though state-of-the-art (SOTA) NLP systems have achieved remarkable
performance on a variety of language understanding tasks, they primarily focus
on questions that have a correct and a definitive answer. However, in
real-world applications, users often ask questions that don't have a definitive
answer. Incorrectly answering such questions certainly hampers a system's
reliability and trustworthiness. Can SOTA models accurately identify such
questions and provide a reasonable response?
</p>
<p>To investigate the above question, we introduce QnotA, a dataset consisting
of five different categories of questions that don't have definitive answers.
Furthermore, for each QnotA instance, we also provide a corresponding QA
instance i.e. an alternate question that ''can be'' answered. With this data,
we formulate three evaluation tasks that test a system's ability to 'identify',
'distinguish', and 'justify' QnotA questions. Through comprehensive
experiments, we show that even SOTA models including GPT-3 and Flan T5 do not
fare well on these tasks and lack considerably behind the human performance
baseline. We conduct a thorough analysis which further leads to several
interesting findings. Overall, we believe our work and findings will encourage
and facilitate further research in this important area and help develop more
robust models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04646">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown
to achieve remarkable performance across a variety of natural language tasks.
Recent advancements in instruction tuning bring LLMs with ability in following
user's instructions and producing human-like responses. However, the high costs
associated with training and implementing LLMs pose challenges to academic
research. Furthermore, the availability of pretrained LLMs and instruction-tune
datasets for Vietnamese language is limited. To tackle these concerns, we
leverage large-scale instruction-following datasets from open-source projects,
namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and
specific medical domain. To the best of our knowledge, these are the first
instructional dataset for Vietnamese. Subsequently, we utilize
parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs:
Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models:
Bloomz-Chat, Bloomz-Doctor, GPTJ-Chat, GPTJ-Doctor.Finally, we assess the
effectiveness of our methodology on a per-sample basis, taking into
consideration the helpfulness, relevance, accuracy, level of detail in their
responses. This evaluation process entails the utilization of GPT-4 as an
automated scoring mechanism. Despite utilizing a low-cost setup, our method
demonstrates about 20-30\% improvement over the original models in our
evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. (arXiv:2309.04658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04658">
<div class="article-summary-box-inner">
<span><p>Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. (arXiv:2309.04662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04662">
<div class="article-summary-box-inner">
<span><p>We introduce MADLAD-400, a manually audited, general domain 3T token
monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss
the limitations revealed by self-auditing MADLAD-400, and the role data
auditing had in the dataset creation process. We then train and release a
10.7B-parameter multilingual machine translation model on 250 billion tokens
covering over 450 languages using publicly available data, and find that it is
competitive with models that are significantly larger, and report the results
on different domains. In addition, we train a 8B-parameter language model, and
assess the results on few-shot translation. We make the baseline models
available to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04663">
<div class="article-summary-box-inner">
<span><p>Learning paradigms for large language models (LLMs) currently tend to fall
within either in-context learning (ICL) or full fine-tuning. Each of these
comes with their own trade-offs based on available data, model size, compute
cost, ease-of-use, and final quality with neither solution performing well
across-the-board. In this article, we first describe ICL and fine-tuning
paradigms in a way that highlights their natural connections. Based on these
connections, we propose a new learning paradigm called FIAT that fuses the best
of these paradigms together, enabling prompt-engineered instructions and
chain-of-thought reasoning with the very largest models while also using
similar methods to perform parameter updates on a modestly-sized LLM with
parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of
multilingual tasks and observe that FIAT performs better than both ICL and
fine-tuning at scales ranging from 100-10,000 training examples. We hope that
FIAT provides a practical way of harnessing the full potential of LLMs without
needing to make a hard choice between learning paradigms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04679">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models underpin a large portion of modern
NLP tools outside of English. A strong baseline for specializing these models
for specific languages is Language-Adaptive Pre-Training (LAPT). However,
retaining a large cross-lingual vocabulary and embedding matrix comes at
considerable excess computational cost during adaptation. In this study, we
propose several simple techniques to replace a cross-lingual vocabulary with a
compact, language-specific one. Namely, we address strategies for
re-initializing the token embedding matrix after vocabulary specialization. We
then provide a systematic experimental comparison of our techniques, in
addition to the recently-proposed Focus method. We demonstrate that: 1)
Embedding-replacement techniques in the monolingual transfer literature are
inadequate for adapting multilingual models. 2) Replacing cross-lingual
vocabularies with smaller specialized ones provides an efficient method to
improve performance in low-resource languages. 3) Simple embedding
re-initialization techniques based on script-wise sub-distributions rival
techniques such as Focus, which rely on similarity scores obtained from an
auxiliary model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04695">
<div class="article-summary-box-inner">
<span><p>Current methods for Knowledge-Based Question Answering (KBQA) usually rely on
complex training techniques and model frameworks, leading to many limitations
in practical applications. Recently, the emergence of In-Context Learning (ICL)
capabilities in Large Language Models (LLMs) provides a simple and
training-free semantic parsing paradigm for KBQA: Given a small number of
questions and their labeled logical forms as demo examples, LLMs can understand
the task intent and generate the logic form for a new question. However,
current powerful LLMs have little exposure to logic forms during pre-training,
resulting in a high format error rate. To solve this problem, we propose a
code-style in-context learning method for KBQA, which converts the generation
process of unfamiliar logical form into the more familiar code generation
process for LLMs. Experimental results on three mainstream datasets show that
our method dramatically mitigated the formatting error problem in generating
logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the
few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04704">
<div class="article-summary-box-inner">
<span><p>The paper considers the possibility of fine-tuning Llama 2 large language
model (LLM) for the disinformation analysis and fake news detection. For
fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was
fine-tuned for the following tasks: analysing a text on revealing
disinformation and propaganda narratives, fact checking, fake news detection,
manipulation analytics, extracting named entities with their sentiments. The
obtained results show that the fine-tuned Llama 2 model can perform a deep
analysis of texts and reveal complex styles and narratives. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04716">
<div class="article-summary-box-inner">
<span><p>Reproducing research results in the networking community is important for
both academia and industry. The current best practice typically resorts to
three approaches: (1) looking for publicly available prototypes; (2) contacting
the authors to get a private prototype; and (3) manually implementing a
prototype following the description of the publication. However, most published
network research does not have public prototypes and private prototypes are
hard to get. As such, most reproducing efforts are spent on manual
implementation based on the publications, which is both time and labor
consuming and error-prone. In this paper, we boldly propose reproducing network
research results using the emerging large language models (LLMs). In
particular, we first prove its feasibility with a small-scale experiment, in
which four students with essential networking knowledge each reproduces a
different networking system published in prominent conferences and journals by
prompt engineering ChatGPT. We report the experiment's observations and lessons
and discuss future open research questions of this proposal. This work raises
no ethical issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets. (arXiv:2309.04725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04725">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promising performance on various NLP
tasks via task prompting. And their performance can be further improved by
appending task demonstrations to the head of the prompt. And usually, a better
performance can be achieved with more demonstrations. However, asking the users
to write the demonstrations can be cumbersome. As a simple yet cost-effective
workaround, this paper proposes a novel method called EPA (\textbf{E}asy
\textbf{P}rompt \textbf{A}ugmentation)\footnote{While this paper considers
augmenting prompts via demonstrations, we name it EPA as the name EDA is
already taken by a well-known NLP method \citep{wei-zou-2019-eda}.} that
effectively minimizes user efforts in writing demonstrations while improving
the model performance at the same time. EPA achieves these goals by
automatically augmenting the demonstrations with multiple sources/targets,
where each of them paraphrases each other. This is well motivated as augmenting
data via paraphrasing effectively improves neural language models. EPA thus
employs paraphrasing as an augmentation method for in-context learning.
Extensive experiments indicate that EPA effectively improves both NLU and NLG
tasks, covering from natural language inference to machine translation in
translating tens of languages.\footnote{Code and data will be released upon
publication.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Multi-modal Keyphrase Generation via Visual Entity Enhancement and Multi-granularity Image Noise Filtering. (arXiv:2309.04734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04734">
<div class="article-summary-box-inner">
<span><p>Multi-modal keyphrase generation aims to produce a set of keyphrases that
represent the core points of the input text-image pair. In this regard,
dominant methods mainly focus on multi-modal fusion for keyphrase generation.
Nevertheless, there are still two main drawbacks: 1) only a limited number of
sources, such as image captions, can be utilized to provide auxiliary
information. However, they may not be sufficient for the subsequent keyphrase
generation. 2) the input text and image are often not perfectly matched, and
thus the image may introduce noise into the model. To address these
limitations, in this paper, we propose a novel multi-modal keyphrase generation
model, which not only enriches the model input with external knowledge, but
also effectively filters image noise. First, we introduce external visual
entities of the image as the supplementary input to the model, which benefits
the cross-modal semantic alignment for keyphrase generation. Second, we
simultaneously calculate an image-text matching score and image region-text
correlation scores to perform multi-granularity image noise filtering.
Particularly, we introduce the correlation scores between image regions and
ground-truth keyphrases to refine the calculation of the previously-mentioned
correlation scores. To demonstrate the effectiveness of our model, we conduct
several groups of experiments on the benchmark dataset.
</p>
<p>Experimental results and in-depth analyses show that our model achieves the
state-of-the-art performance. Our code is available on
https://github.com/DeepLearnXMU/MM-MKP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04739">
<div class="article-summary-box-inner">
<span><p>Advancements in conversational systems have revolutionized information
access, surpassing the limitations of single queries. However, developing
dialogue systems requires a large amount of training data, which is a challenge
in low-resource domains and languages. Traditional data collection methods like
crowd-sourcing are labor-intensive and time-consuming, making them ineffective
in this context. Data augmentation (DA) is an affective approach to alleviate
the data scarcity problem in conversational systems. This tutorial provides a
comprehensive and up-to-date overview of DA approaches in the context of
conversational systems. It highlights recent advances in conversation
augmentation, open domain and task-oriented conversation generation, and
different paradigms of evaluating these models. We also discuss current
challenges and future directions in order to help researchers and practitioners
to further advance the field in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04766">
<div class="article-summary-box-inner">
<span><p>We present SeaEval, a benchmark for multilingual foundation models. In
addition to characterizing how these models understand and reason with natural
language, we also investigate how well they comprehend cultural practices,
nuances, and values. Alongside standard accuracy metrics, we investigate the
brittleness of foundation models in the dimensions of semantics and
multilinguality. Our analyses span both open-sourced and closed models, leading
to empirical results across classic NLP tasks, reasoning, and cultural
comprehension. Key findings indicate (1) Most models exhibit varied behavior
when given paraphrased instructions. (2) Many models still suffer from exposure
bias (e.g., positional bias, majority label bias). (3) For questions rooted in
factual, scientific, and commonsense knowledge, consistent responses are
expected across multilingual queries that are semantically equivalent. Yet,
most models surprisingly demonstrate inconsistent performance on these queries.
(4) Multilingually-trained models have not attained "balanced multilingual"
capabilities. Our endeavors underscore the need for more generalizable semantic
representations and enhanced multilingual contextualization. SeaEval can serve
as a launchpad for more thorough investigations and evaluations for
multilingual and multicultural scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images. (arXiv:2309.04790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04790">
<div class="article-summary-box-inner">
<span><p>In the real world, knowledge often exists in a multimodal and heterogeneous
form. Addressing the task of question answering with hybrid data types,
including text, tables, and images, is a challenging task (MMHQA). Recently,
with the rise of large language models (LLM), in-context learning (ICL) has
become the most popular way to solve QA problems. We propose MMHQA-ICL
framework for addressing this problems, which includes stronger heterogeneous
data retriever and an image caption module. Most importantly, we propose a
Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage
their powerful performance in this task. We are the first to use end-to-end LLM
prompting method for this task. Experimental results demonstrate that our
framework outperforms all baselines and methods trained on the full dataset,
achieving state-of-the-art results under the few-shot setting on the
MultimodalQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaNS: a Facet-based Narrative Similarity Metric. (arXiv:2309.04823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04823">
<div class="article-summary-box-inner">
<span><p>Similar Narrative Retrieval is a crucial task since narratives are essential
for explaining and understanding events, and multiple related narratives often
help to create a holistic view of the event of interest. To accurately identify
semantically similar narratives, this paper proposes a novel narrative
similarity metric called Facet-based Narrative Similarity (FaNS), based on the
classic 5W1H facets (Who, What, When, Where, Why, and How), which are extracted
by leveraging the state-of-the-art Large Language Models (LLMs). Unlike
existing similarity metrics that only focus on overall lexical/semantic match,
FaNS provides a more granular matching along six different facets independently
and then combines them. To evaluate FaNS, we created a comprehensive dataset by
collecting narratives from AllSides, a third-party news portal. Experimental
results demonstrate that the FaNS metric exhibits a higher correlation (37\%
higher) than traditional text similarity metrics that directly measure the
lexical/semantic match between narratives, demonstrating its effectiveness in
comparing the finer details between a pair of narratives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neurons in Large Language Models: Dead, N-gram, Positional. (arXiv:2309.04827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04827">
<div class="article-summary-box-inner">
<span><p>We analyze a family of large language models in such a lightweight manner
that can be done on a single GPU. Specifically, we focus on the OPT family of
models ranging from 125m to 66b parameters and rely only on whether an FFN
neuron is activated or not. First, we find that the early part of the network
is sparse and represents many discrete features. Here, many neurons (more than
70% in some layers of the 66b model) are "dead", i.e. they never activate on a
large collection of diverse data. At the same time, many of the alive neurons
are reserved for discrete features and act as token and n-gram detectors.
Interestingly, their corresponding FFN updates not only promote next token
candidates as could be expected, but also explicitly focus on removing the
information about triggering them tokens, i.e., current input. To the best of
our knowledge, this is the first example of mechanisms specialized at removing
(rather than adding) information from the residual stream. With scale, models
become more sparse in a sense that they have more dead neurons and token
detectors. Finally, some neurons are positional: them being activated or not
depends largely (or solely) on position and less so (or not at all) on textual
data. We find that smaller models have sets of neurons acting as position range
indicators while larger models operate in a less explicit manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04842">
<div class="article-summary-box-inner">
<span><p>While large language models excel in a variety of natural language processing
(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they
must either rely on off-the-shelf automatic speech recognition (ASR) systems
for transcription, or be equipped with an in-built speech modality. This work
focuses on the former scenario, where LLM's accuracy on SLU tasks is
constrained by the accuracy of a fixed ASR system on the spoken input.
Specifically, we tackle speech-intent classification task, where a high
word-error-rate can limit the LLM's ability to understand the spoken intent.
Instead of chasing a high accuracy by designing complex or specialized
architectures regardless of deployment costs, we seek to answer how far we can
go without substantially changing the underlying ASR and LLM, which can
potentially be shared by multiple unrelated tasks. To this end, we propose
prompting the LLM with an n-best list of ASR hypotheses instead of only the
error-prone 1-best hypothesis. We explore prompt-engineering to explain the
concept of n-best lists to the LLM; followed by the finetuning of Low-Rank
Adapters on the downstream tasks. Our approach using n-best lists proves to be
effective on a device-directed speech detection task as well as on a keyword
spotting task, where systems using n-best list prompts outperform those using
1-best ASR hypothesis; thus paving the way for an efficient method to exploit
ASR uncertainty via LLMs for speech-based applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04849">
<div class="article-summary-box-inner">
<span><p>We propose EmoDistill, a novel speech emotion recognition (SER) framework
that leverages cross-modal knowledge distillation during training to learn
strong linguistic and prosodic representations of emotion from speech. During
inference, our method only uses a stream of speech signals to perform unimodal
SER thus reducing computation overhead and avoiding run-time transcription and
prosodic feature extraction errors. During training, our method distills
information at both embedding and logit levels from a pair of pre-trained
Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on
the IEMOCAP benchmark demonstrate that our method outperforms other unimodal
and multimodal techniques by a considerable margin, and achieves
state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted
accuracy. Detailed ablation studies demonstrate the impact of each component of
our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System. (arXiv:2309.04858v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04858">
<div class="article-summary-box-inner">
<span><p>Neural language models are increasingly deployed into APIs and websites that
allow a user to pass in a prompt and receive generated text. Many of these
systems do not reveal generation parameters. In this paper, we present methods
to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or
nucleus sampling). Our ability to discover which decoding strategy was used has
implications for detecting generated text. Additionally, the process of
discovering the decoding strategy can reveal biases caused by selecting
decoding settings which severely truncate a model's predicted distributions. We
perform our attack on several families of open-source language models, as well
as on production systems (e.g., ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributional Data Augmentation Methods for Low Resource Language. (arXiv:2309.04862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04862">
<div class="article-summary-box-inner">
<span><p>Text augmentation is a technique for constructing synthetic data from an
under-resourced corpus to improve predictive performance. Synthetic data
generation is common in numerous domains. However, recently text augmentation
has emerged in natural language processing (NLP) to improve downstream tasks.
One of the current state-of-the-art text augmentation techniques is easy data
augmentation (EDA), which augments the training data by injecting and replacing
synonyms and randomly permuting sentences. One major obstacle with EDA is the
need for versatile and complete synonym dictionaries, which cannot be easily
found in low-resource languages. To improve the utility of EDA, we propose two
extensions, easy distributional data augmentation (EDDA) and type specific
similar word replacement (TSSR), which uses semantic word context information
and part-of-speech tags for word replacement and augmentation. In an extensive
empirical evaluation, we show the utility of the proposed methods, measured by
F1 score, on two representative datasets in Swedish as an example of a
low-resource language. With the proposed methods, we show that augmented data
improve classification performances in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Chunking with Hierarchical RNN. (arXiv:2309.04919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04919">
<div class="article-summary-box-inner">
<span><p>In Natural Language Processing (NLP), predicting linguistic structures, such
as parsing and chunking, has mostly relied on manual annotations of syntactic
structures. This paper introduces an unsupervised approach to chunking, a
syntactic task that involves grouping words in a non-hierarchical manner. We
present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to
model word-to-chunk and chunk-to-sentence compositions. Our approach involves a
two-stage training process: pretraining with an unsupervised parser and
finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset
reveal a notable improvement over existing unsupervised methods, enhancing
phrase F1 score by up to 6 percentage points. Further, finetuning with
downstream tasks results in an additional performance improvement.
Interestingly, we observe that the emergence of the chunking structure is
transient during the neural model's downstream-task training. This study
contributes to the advancement of unsupervised syntactic structure discovery
and opens avenues for further research in linguistic theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's Hard in English RST Parsing? Predictive Models for Error Analysis. (arXiv:2309.04940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04940">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in Natural Language Processing (NLP), hierarchical
discourse parsing in the framework of Rhetorical Structure Theory remains
challenging, and our understanding of the reasons for this are as yet limited.
In this paper, we examine and model some of the factors associated with parsing
difficulties in previous work: the existence of implicit discourse relations,
challenges in identifying long-distance relations, out-of-vocabulary items, and
more. In order to assess the relative importance of these variables, we also
release two annotated English test-sets with explicit correct and distracting
discourse markers associated with gold standard RST relations. Our results show
that as in shallow discourse parsing, the explicit/implicit distinction plays a
role, but that long-distance dependencies are the main challenge, while lack of
lexical overlap is less of a problem, at least for in-domain parsing. Our final
model is able to predict where errors will occur with an accuracy of 76.3% for
the bottom-up parser and 76.6% for the top-down parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04951">
<div class="article-summary-box-inner">
<span><p>This paper is aimed at evaluating state-of-the-art models for Multi-document
Summarization (MDS) on different types of datasets in various domains and
investigating the limitations of existing models to determine future research
directions. To address this gap, we conducted an extensive literature review to
identify state-of-the-art models and datasets. We analyzed the performance of
PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed
unique challenges due to their varied domains. Our findings show that the
General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the
MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the
identified models on different datasets. Our study provides valuable insights
into the models' strengths and weaknesses, as well as their applicability in
different domains. This work serves as a reference for future MDS research and
contributes to the development of accurate and robust models which can be
utilized on demanding datasets with academically and/or scientifically complex
data as well as generalized, relatively simple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04965">
<div class="article-summary-box-inner">
<span><p>While impressive performance has been achieved in image captioning, the
limited diversity of the generated captions and the large parameter scale
remain major barriers to the real-word application of these systems. In this
work, we propose a lightweight image captioning network in combination with
continuous diffusion, called Prefix-diffusion. To achieve diversity, we design
an efficient method that injects prefix image embeddings into the denoising
process of the diffusion model. In order to reduce trainable parameters, we
employ a pre-trained model to extract image features and further design an
extra mapping network. Prefix-diffusion is able to generate diverse captions
with relatively less parameters, while maintaining the fluency and relevance of
the captions benefiting from the generative capabilities of the diffusion
model. Our work paves the way for scaling up diffusion models for image
captioning, and achieves promising performance compared with recent approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Learning With Knowledge Memorizing Prototypes For Generalized Few-Shot Intent Detection. (arXiv:2309.04971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04971">
<div class="article-summary-box-inner">
<span><p>Generalized Few-Shot Intent Detection (GFSID) is challenging and realistic
because it needs to categorize both seen and novel intents simultaneously.
Previous GFSID methods rely on the episodic learning paradigm, which makes it
hard to extend to a generalized setup as they do not explicitly learn the
classification of seen categories and the knowledge of seen intents. To address
the dilemma, we propose to convert the GFSID task into the class incremental
learning paradigm. Specifically, we propose a two-stage learning framework,
which sequentially learns the knowledge of different intents in various periods
via prompt learning. And then we exploit prototypes for categorizing both seen
and novel intents. Furthermore, to achieve the transfer knowledge of intents in
different stages, for different scenarios we design two knowledge preservation
methods which close to realistic applications. Extensive experiments and
detailed analyses on two widely used datasets show that our framework based on
the class incremental learning paradigm achieves promising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04977">
<div class="article-summary-box-inner">
<span><p>Although syntactic information is beneficial for many NLP tasks, combining it
with contextual information between words to solve the coreference resolution
problem needs to be further explored. In this paper, we propose an end-to-end
parser that combines pre-trained BERT with a Syntactic Relation Graph Attention
Network (RGAT) to take a deeper look into the role of syntactic dependency
information for the coreference resolution task. In particular, the RGAT model
is first proposed, then used to understand the syntactic dependency graph and
learn better task-specific syntactic embeddings. An integrated architecture
incorporating BERT embeddings and syntactic embeddings is constructed to
generate blending representations for the downstream task. Our experiments on a
public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision
learning of the syntactic dependency graph and without fine-tuning the entire
BERT, we increased the F1-score of the previous best model (RGCN-with-BERT)
from 80.3% to 82.5%, compared to the F1-score by single BERT embeddings from
78.5% to 82.5%. Experimental results on another public dataset - OntoNotes 5.0
demonstrate that the performance of the model is also improved by incorporating
syntactic dependency information learned from RGAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Meta Learning for Low-Resource Text Classification. (arXiv:2309.04979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04979">
<div class="article-summary-box-inner">
<span><p>Meta learning have achieved promising performance in low-resource text
classification which aims to identify target classes with knowledge transferred
from source classes with sets of small tasks named episodes. However, due to
the limited training data in the meta-learning scenario and the inherent
properties of parameterized neural networks, poor generalization performance
has become a pressing problem that needs to be addressed. To deal with this
issue, we propose a meta-learning based method called Retrieval-Augmented Meta
Learning(RAML). It not only uses parameterization for inference but also
retrieves non-parametric knowledge from an external corpus to make inferences,
which greatly alleviates the problem of poor generalization performance caused
by the lack of diverse training data in meta-learning. This method differs from
previous models that solely rely on parameters, as it explicitly emphasizes the
importance of non-parametric knowledge, aiming to strike a balance between
parameterized neural networks and non-parametric knowledge. The model is
required to determine which knowledge to access and utilize during inference.
Additionally, our multi-view passages fusion network module can effectively and
efficiently integrate the retrieved information into low-resource
classification task. The extensive experiments demonstrate that RAML
significantly outperforms current SOTA low-resource text classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Word Bias in Zero-shot Prompt-based Classifiers. (arXiv:2309.04992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04992">
<div class="article-summary-box-inner">
<span><p>Prompt-based classifiers are an attractive approach for zero-shot
classification. However, the precise choice of the prompt template and label
words can largely influence performance, with semantically equivalent settings
often showing notable performance difference. This discrepancy can be partly
attributed to word biases, where the classifier may be biased towards classes.
To address this problem, it is possible to optimise classification thresholds
on a labelled data set, however, this mitigates some of the advantages of
prompt-based classifiers. This paper instead approaches this problem by
examining the expected marginal probabilities of the classes. Here,
probabilities are reweighted to have a uniform prior over classes, in an
unsupervised fashion. Further, we draw a theoretical connection between the
class priors and the language models' word prior, and offer the ability to set
a threshold in a zero-resource fashion. We show that matching class priors
correlates strongly with the oracle upper bound performance and demonstrate
large consistent performance gains for prompt settings over a range of NLP
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05007">
<div class="article-summary-box-inner">
<span><p>Humans ask follow-up questions driven by curiosity, which reflects a creative
human cognitive process. We introduce the task of real-world
information-seeking follow-up question generation (FQG), which aims to generate
follow-up questions seeking a more in-depth understanding of an initial
question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world
(initial question, answer, follow-up question) tuples collected from a Reddit
forum providing layman-friendly explanations for open-ended questions. In
contrast to existing datasets, questions in FOLLOWUPQG use more diverse
pragmatic strategies to seek information, and they also show higher-order
cognitive skills (such as applying and relating). We evaluate current question
generation models on their efficacy for generating follow-up questions,
exploring how to generate specific types of follow-up questions based on
step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging
benchmark, as model-generated questions are adequate but far from human-raised
questions in terms of informativeness and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps. (arXiv:2309.05021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05021">
<div class="article-summary-box-inner">
<span><p>Over decades, neuroscience has accumulated a wealth of research results in
the text modality that can be used to explore cognitive processes.
Meta-analysis is a typical method that successfully establishes a link from
text queries to brain activation maps using these research results, but it
still relies on an ideal query environment. In practical applications, text
queries used for meta-analyses may encounter issues such as semantic redundancy
and ambiguity, resulting in an inaccurate mapping to brain images. On the other
hand, large language models (LLMs) like ChatGPT have shown great potential in
tasks such as context understanding and reasoning, displaying a high degree of
consistency with human natural language. Hence, LLMs could improve the
connection between text modality and neuroscience, resolving existing
challenges of meta-analyses. In this study, we propose a method called
Chat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain,
to map open-ended semantic queries to brain activation maps in data-scarce and
complex query environments. By utilizing the understanding and reasoning
capabilities of LLMs, the performance of the mapping model is optimized by
transferring text queries to semantic queries. We demonstrate that Chat2Brain
can synthesize anatomically plausible neural activation patterns for more
complex tasks of text queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.10294">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel unifying framework of bilinear LSTMs that can
represent and utilize the nonlinear interaction of the input features present
in sequence datasets for achieving superior performance over a linear LSTM and
yet not incur more parameters to be learned. To realize this, our unifying
framework allows the expressivity of the linear vs. bilinear terms to be
balanced by correspondingly trading off between the hidden state vector size
vs. approximation quality of the weight matrix in the bilinear term so as to
optimize the performance of our bilinear LSTM, while not incurring more
parameters to be learned. We empirically evaluate the performance of our
bilinear LSTM in several language-based sequence learning tasks to demonstrate
its general applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Are People Asking About COVID-19? A Question Classification Dataset. (arXiv:2005.12522v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.12522">
<div class="article-summary-box-inner">
<span><p>We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources,
which we annotate into 15 question categories and 207 question clusters. The
most common questions in our dataset asked about transmission, prevention, and
societal effects of COVID, and we found that many questions that appeared in
multiple sources were not answered by any FAQ websites of reputable
organizations such as the CDC and FDA. We post our dataset publicly at
https://github.com/JerryWeiAI/COVID-Q. For classifying questions into 15
categories, a BERT baseline scored 58.1% accuracy when trained on 20 examples
per category, and for a question clustering task, a BERT + triplet loss
baseline achieved 49.5% accuracy. We hope COVID-Q can help either for direct
use in developing applied systems or as a domain-specific resource for model
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewB: 200,000+ Sentences for Political Bias Detection. (arXiv:2006.03051v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03051">
<div class="article-summary-box-inner">
<span><p>We present the Newspaper Bias Dataset (NewB), a text corpus of more than
200,000 sentences from eleven news sources regarding Donald Trump. While
previous datasets have labeled sentences as either liberal or conservative,
NewB covers the political views of eleven popular media sources, capturing more
nuanced political viewpoints than a traditional binary classification system
does. We train two state-of-the-art deep learning models to predict the news
source of a given sentence from eleven newspapers and find that a recurrent
neural network achieved top-1, top-3, and top-5 accuracies of 33.3%, 61.4%, and
77.6%, respectively, significantly outperforming a baseline logistic regression
model's accuracies of 18.3%, 42.6%, and 60.8%. Using the news source label of
sentences, we analyze the top n-grams with our model to gain meaningful insight
into the portrayal of Trump by media sources.We hope that the public release of
our dataset will encourage further research in using natural language
processing to analyze more complex political biases.
</p>
<p>Our dataset is posted at https://github.com/JerryWeiAI/NewB .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04553">
<div class="article-summary-box-inner">
<span><p>Recent publications suggest using natural language analysis on database
schema elements to guide tuning and profiling efforts. The underlying
hypothesis is that state-of-the-art language processing methods, so-called
language models, are able to extract information on data properties from schema
text.
</p>
<p>This paper examines that hypothesis in the context of data correlation
analysis: is it possible to find column pairs with correlated data by analyzing
their names via language models? First, the paper introduces a novel benchmark
for data correlation analysis, created by analyzing thousands of Kaggle data
sets (and available for download). Second, it uses that data to study the
ability of language models to predict correlation, based on column names. The
analysis covers different language models, various correlation metrics, and a
multitude of accuracy metrics. It pinpoints factors that contribute to
successful predictions, such as the length of column names as well as the ratio
of words. Finally, \rev{the study analyzes the impact of column types on
prediction performance.} The results show that schema text can be a useful
source of information and inform future research efforts, targeted at
NLP-enhanced database tuning and data profiling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00269">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models learn informative word representations on a
large-scale text corpus through self-supervised learning, which has achieved
promising performance in fields of natural language processing (NLP) after
fine-tuning. These models, however, suffer from poor robustness and lack of
interpretability. We refer to pre-trained language models with knowledge
injection as knowledge-enhanced pre-trained language models (KEPLMs). These
models demonstrate deep understanding and logical reasoning and introduce
interpretability. In this survey, we provide a comprehensive overview of KEPLMs
in NLP. We first discuss the advancements in pre-trained language models and
knowledge representation learning. Then we systematically categorize existing
KEPLMs from three different perspectives. Finally, we outline some potential
directions of KEPLMs for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09847">
<div class="article-summary-box-inner">
<span><p>For human children as well as machine learning systems, a key challenge in
learning a word is linking the word to the visual phenomena it describes. We
explore this aspect of word learning by using the performance of computer
vision systems as a proxy for the difficulty of learning a word from visual
cues. We show that the age at which children acquire different categories of
words is correlated with the performance of visual classification and
captioning systems, over and above the expected effects of word frequency. The
performance of the computer vision systems is correlated with human judgments
of the concreteness of words, which are in turn a predictor of children's word
learning, suggesting that these models are capturing the relationship between
words and visual phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03886">
<div class="article-summary-box-inner">
<span><p>In this essay I will consider a sequence of questions. The first questions
concern the biological function of intelligence in general, and cognitive
prostheses of human intelligence in particular. These will lead into questions
concerning human language, perhaps the most important cognitive prosthesis
humanity has ever developed. While it is traditional to rhapsodize about the
cognitive power encapsulated in human language, I will emphasize how horribly
limited human language is -- and therefore how limited our cognitive abilities
are, despite their being augmented with language. This will lead to questions
of whether human mathematics, being ultimately formulated in terms of human
language, is also deeply limited. I will then combine these questions to pose a
partial, sort-of, sideways answer to the guiding concern of this essay: what we
can ever discern about that we cannot even conceive?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17546">
<div class="article-summary-box-inner">
<span><p>Studying data memorization in neural language models helps us understand the
risks (e.g., to privacy or copyright) associated with models regurgitating
training data and aids in the development of countermeasures. Many prior works
-- and some recently deployed defenses -- focus on "verbatim memorization",
defined as a model generation that exactly matches a substring from the
training set. We argue that verbatim memorization definitions are too
restrictive and fail to capture more subtle forms of memorization.
Specifically, we design and implement an efficient defense that perfectly
prevents all verbatim memorization. And yet, we demonstrate that this "perfect"
filter does not prevent the leakage of training data. Indeed, it is easily
circumvented by plausible and minimally modified "style-transfer" prompts --
and in some cases even the non-modified original prompts -- to extract
memorized information. We conclude by discussing potential alternative
definitions and why defining memorization is a difficult yet crucial open
question for neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04476">
<div class="article-summary-box-inner">
<span><p>Pretrained natural language processing (NLP) models have achieved high
overall performance, but they still make systematic errors. Instead of manual
error analysis, research on slice detection models (SDM), which automatically
identify underperforming groups of datapoints, has caught escalated attention
in Computer Vision for both understanding model behaviors and providing
insights for future model training and designing. However, little research on
SDM and quantitative evaluation of their effectiveness have been conducted on
NLP tasks. Our paper fills the gap by proposing a benchmark named "Discover,
Explain, Improve (DEIM)" for classification NLP tasks along with a new SDM
Edisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM
then unites them under human-understandable concepts and provides comprehensive
evaluation tasks and corresponding quantitative metrics. The evaluation in DEIM
shows that Edisa can accurately select error-prone datapoints with informative
semantic features that summarize error patterns. Detecting difficult datapoints
directly boosts model performance without tuning any original model parameters,
showing that discovered slices are actionable for users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09172">
<div class="article-summary-box-inner">
<span><p>While Emotion Recognition in Conversations (ERC) has seen a tremendous
advancement in the last few years, new applications and implementation
scenarios present novel challenges and opportunities. These range from
leveraging the conversational context, speaker and emotion dynamics modelling,
to interpreting common sense expressions, informal language and sarcasm,
addressing challenges of real time ERC, recognizing emotion causes, different
taxonomies across datasets, multilingual ERC to interpretability. This survey
starts by introducing ERC, elaborating on the challenges and opportunities
pertaining to this task. It proceeds with a description of the emotion
taxonomies and a variety of ERC benchmark datasets employing such taxonomies.
This is followed by descriptions of the most prominent works in ERC with
explanations of the Deep Learning architectures employed. Then, it provides
advisable ERC practices towards better frameworks, elaborating on methods to
deal with subjectivity in annotations and modelling and methods to deal with
the typically unbalanced ERC datasets. Finally, it presents systematic review
tables comparing several works regarding the methods used and their
performance. The survey highlights the advantage of leveraging techniques to
address unbalanced data, the exploration of mixed emotions and the benefits of
incorporating annotation subjectivity in the learning phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Human-Language Model Interaction. (arXiv:2212.09746v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09746">
<div class="article-summary-box-inner">
<span><p>Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02275">
<div class="article-summary-box-inner">
<span><p>This paper explores deep latent variable models for semi-supervised
paraphrase generation, where the missing target pair for unlabelled data is
modelled as a latent paraphrase sequence. We present a novel unsupervised model
named variational sequence auto-encoding reconstruction (VSAR), which performs
latent sequence inference given an observed text. To leverage information from
text pairs, we additionally introduce a novel supervised model we call dual
directional learning (DDL), which is designed to integrate with our proposed
VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct
semi-supervised learning. Still, the combined model suffers from a cold-start
problem. To further combat this issue, we propose an improved weight
initialisation solution, leading to a novel two-stage training scheme we call
knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the
combined model yields competitive performance against the state-of-the-art
supervised baselines on complete data. Furthermore, in scenarios where only a
fraction of the labelled pairs are available, our combined model consistently
outperforms the strong supervised model baseline (DDL) by a significant margin
(p &lt;.05; Wilcoxon test). Our code is publicly available at
"https://github.com/jialin-yu/latent-sequence-paraphrase".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v12 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04321">
<div class="article-summary-box-inner">
<span><p>Understanding the continuous states of objects is essential for task learning
and planning in the real world. However, most existing task learning benchmarks
assume discrete (e.g., binary) object goal states, which poses challenges for
the learning of complex tasks and transferring learned policy from simulated
environments to the real world. Furthermore, state discretization limits a
robot's ability to follow human instructions based on the grounding of actions
and states. To tackle these challenges, we present ARNOLD, a benchmark that
evaluates language-grounded task learning with continuous states in realistic
3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve
understanding object states and learning policies for continuous goals. To
promote language-instructed learning, we provide expert demonstrations with
template-generated language descriptions. We assess task performance by
utilizing the latest language-conditioned policy learning models. Our results
indicate that current models for language-conditioned manipulations continue to
experience significant challenges in novel goal-state generalizations, scene
generalizations, and object generalizations. These findings highlight the need
to develop new algorithms that address this gap and underscore the potential
for further research in this area. Project website:
https://arnold-benchmark.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v4 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07619">
<div class="article-summary-box-inner">
<span><p>We examine the potential of ChatGPT and other large language models in
predicting stock market returns using news headlines. We use ChatGPT to assess
whether each headline is good, bad, or neutral for firms' stock prices. We
document a significantly positive correlation between ChatGPT scores and
subsequent daily stock returns. We find that ChatGPT outperforms traditional
sentiment analysis methods. More basic models such as GPT-1, GPT-2, and BERT
cannot accurately forecast returns, indicating return predictability is an
emerging capacity of complex language models. Long-short strategies based on
ChatGPT-4 deliver the highest Sharpe ratio. Furthermore, we find predictability
in both small and large stocks, suggesting market underreaction to company
news. Predictability is stronger among smaller stocks and stocks with bad news,
consistent with limits-to-arbitrage also playing an important role. Finally, we
propose a new method to evaluate and understand the models' reasoning
capabilities. Overall, our results suggest that incorporating advanced language
models into the investment decision-making process can yield more accurate
predictions and enhance the performance of quantitative trading strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v5 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04087">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04940">
<div class="article-summary-box-inner">
<span><p>The use of modern Natural Language Processing (NLP) techniques has shown to
be beneficial for software engineering tasks, such as vulnerability detection
and type inference. However, training deep NLP models requires significant
computational resources. This paper explores techniques that aim at achieving
the best usage of resources and available information in these models.
</p>
<p>We propose a generic approach, EarlyBIRD, to build composite representations
of code from the early layers of a pre-trained transformer model. We
empirically investigate the viability of this approach on the CodeBERT model by
comparing the performance of 12 strategies for creating composite
representations with the standard practice of only using the last encoder
layer.
</p>
<p>Our evaluation on four datasets shows that several early layer combinations
yield better performance on defect detection, and some combinations improve
multi-class classification. More specifically, we obtain a +2 average
improvement of detection accuracy on Devign with only 3 out of 12 layers of
CodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early
layers can be used to obtain better results using the same resources, as well
as to reduce resource usage during fine-tuning and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08455">
<div class="article-summary-box-inner">
<span><p>We call on the Document AI (DocAI) community to reevaluate current
methodologies and embrace the challenge of creating more practically-oriented
benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to
remediate the halted research progress in understanding visually-rich documents
(VRDs). We present a new dataset with novelties related to types of questions,
answers, and document layouts based on multi-industry, multi-domain, and
multi-page VRDs of various origins, and dates. Moreover, we are pushing the
boundaries of current methods by creating multi-task and multi-domain
evaluation setups that more accurately simulate real-world situations where
powerful generalization and adaptation under low-resource settings are desired.
DUDE aims to set a new standard as a more practical, long-standing benchmark
for the community, and we hope that it will lead to future extensions and
contributions that address real-world challenges. Finally, our work illustrates
the importance of finding more efficient ways to model language, images, and
layout in DocAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15070">
<div class="article-summary-box-inner">
<span><p>Annotating data via crowdsourcing is time-consuming and expensive. Due to
these costs, dataset creators often have each annotator label only a small
subset of the data. This leads to sparse datasets with examples that are marked
by few annotators. The downside of this process is that if an annotator doesn't
get to label a particular example, their perspective on it is missed. This is
especially concerning for subjective NLP datasets where there is no single
correct label: people may have different valid opinions. Thus, we propose using
imputation methods to generate the opinions of all annotators for all examples,
creating a dataset that does not leave out any annotator's view. We then train
and prompt models, using data from the imputed dataset, to make predictions
about the distribution of responses and individual annotations.
</p>
<p>In our analysis of the results, we found that the choice of imputation method
significantly impacts soft label changes and distribution. While the imputation
introduces noise in the prediction of the original dataset, it has shown
potential in enhancing shots for prompts, particularly for low-response-rate
annotators. We have made all of our code and data publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17256">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently shown great potential for
in-context learning, where LLMs learn a new task simply by conditioning on a
few input-label pairs (prompts). Despite their potential, our understanding of
the factors influencing end-task performance and the robustness of in-context
learning remains limited. This paper aims to bridge this knowledge gap by
investigating the reliance of LLMs on shortcuts or spurious correlations within
prompts. Through comprehensive experiments on classification and extraction
tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts
in prompts for downstream tasks. Additionally, we uncover a surprising finding
that larger models are more likely to utilize shortcuts in prompts during
inference. Our findings provide a new perspective on evaluating robustness in
in-context learning and pose new challenges for detecting and mitigating the
use of shortcuts in prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18365">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been applied in various kinds of areas
such as science, finance and software engineering. However, the capability of
LLMs to advance the field of chemistry remains unclear. In this paper, rather
than pursuing state-of-the-art performance, we aim to evaluate capabilities of
LLMs in a wide range of tasks across the chemistry domain. We identify three
key chemistry-related capabilities including understanding, reasoning and
explaining to explore in LLMs and establish a benchmark containing eight
chemistry tasks. Our analysis draws on widely recognized datasets facilitating
a broad exploration of the capacities of LLMs within the context of practical
chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are
evaluated for each chemistry task in zero-shot and few-shot in-context learning
settings with carefully selected demonstration examples and specially crafted
prompts. Our investigation found that GPT-4 outperformed other models and LLMs
exhibit different competitive levels in eight chemistry tasks. In addition to
the key findings from the comprehensive benchmark analysis, our work provides
insights into the limitation of current LLMs and the impact of in-context
learning settings on LLMs' performance across various chemistry tasks. The code
and datasets used in this study are available at
https://github.com/ChemFoundationModels/ChemLLMBench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization. (arXiv:2306.01102v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01102">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13047">
<div class="article-summary-box-inner">
<span><p>Multiple choice exams are widely used to assess candidates across a diverse
range of domains and tasks. To moderate question quality, newly proposed
questions often pass through pre-test evaluation stages before being deployed
into real-world exams. Currently, this evaluation process is manually
intensive, which can lead to time lags in the question development cycle.
Streamlining this process via automation can significantly enhance efficiency,
however, there's a current lack of datasets with adequate pre-test analysis
information. In this paper we introduce CamChoice; a multiple-choice
comprehension dataset of questions at different target levels, with
corresponding candidate selection distributions. We introduce the task of
candidate distribution matching, propose several evaluation metrics for the
task, and demonstrate that automatic systems trained on RACE++ can be leveraged
as baselines for our task. We further demonstrate that these automatic systems
can be used for practical pre-test evaluation tasks such as detecting
underperforming distractors, where our detection systems can automatically
identify poor distractors that few candidates select. We release the data
publicly for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14115">
<div class="article-summary-box-inner">
<span><p>With recent advances in natural language processing, rationalization becomes
an essential self-explaining diagram to disentangle the black box by selecting
a subset of input texts to account for the major variation in prediction. Yet,
existing association-based approaches on rationalization cannot identify true
rationales when two or more snippets are highly inter-correlated and thus
provide a similar contribution to prediction accuracy, so-called spuriousness.
To address this limitation, we novelly leverage two causal desiderata,
non-spuriousness and efficiency, into rationalization from the causal inference
perspective. We formally define a series of probabilities of causation based on
a newly proposed structural causal model of rationalization, with its
theoretical identification established as the main component of learning
necessary and sufficient rationales. The superior performance of the proposed
causal rationalization is demonstrated on real-world review and medical
datasets with extensive experiments compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\alpha$-$\beta$-Factorization and the Binary Case of Simon's Congruence. (arXiv:2306.14192v3 [math.CO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14192">
<div class="article-summary-box-inner">
<span><p>In 1991 H\'ebrard introduced a factorization of words that turned out to be a
powerful tool for the investigation of a word's scattered factors (also known
as (scattered) subwords or subsequences). Based on this, first Karandikar and
Schnoebelen introduced the notion of $k$-richness and later on Barker et al.
the notion of $k$-universality. In 2022 Fleischmann et al. presented a
generalization of the arch factorization by intersecting the arch factorization
of a word and its reverse. While the authors merely used this factorization for
the investigation of shortest absent scattered factors, in this work we
investigate this new $\alpha$-$\beta$-factorization as such. We characterize
the famous Simon congruence of $k$-universal words in terms of $1$-universal
words. Moreover, we apply these results to binary words. In this special case,
we obtain a full characterization of the classes and calculate the index of the
congruence. Lastly, we start investigating the ternary case, present a full
list of possibilities for $\alpha\beta\alpha$-factors, and characterize their
congruence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03941">
<div class="article-summary-box-inner">
<span><p>The Right to be Forgotten (RTBF) was first established as the result of the
ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and
was later included as the Right to Erasure under the General Data Protection
Regulation (GDPR) of European Union to allow individuals the right to request
personal data be deleted by organizations. Specifically for search engines,
individuals can send requests to organizations to exclude their information
from the query results. It was a significant emergent right as the result of
the evolution of technology. With the recent development of Large Language
Models (LLMs) and their use in chatbots, LLM-enabled software systems have
become popular. But they are not excluded from the RTBF. Compared with the
indexing approach used by search engines, LLMs store, and process information
in a completely different way. This poses new challenges for compliance with
the RTBF. In this paper, we explore these challenges and provide our insights
on how to implement technical solutions for the RTBF, including the use of
differential privacy, machine unlearning, model editing, and prompt
engineering. With the rapid advancement of AI and the increasing need of
regulating this powerful technology, learning from the case of RTBF can provide
valuable lessons for technical practitioners, legal experts, organizations, and
authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07940">
<div class="article-summary-box-inner">
<span><p>Referring to solution programs written by other users is helpful for learners
in programming education. However, current online judge systems just list all
solution programs submitted by users for references, and the programs are
sorted based on the submission date and time, execution time, or user rating,
ignoring to what extent the programs can be helpful to be referenced. In
addition, users struggle to refer to a variety of solution approaches since
there are too many duplicated and near-duplicated programs. To motivate
learners to refer to various solutions to learn better solution approaches, in
this paper, we propose an approach to deduplicate and rank common solution
programs in each programming problem. Inspired by the nature that the
many-duplicated program adopts a more common approach and can be a general
reference, we remove the near-duplicated solution programs and rank the unique
programs based on the duplicate count. The experiments on the solution programs
submitted to a real-world online judge system demonstrate that the number of
programs is reduced by 60.20%, whereas the baseline only reduces by 29.59%
after the deduplication, meaning that users only need to refer to 39.80% of
programs on average. Furthermore, our analysis shows that top-10 ranked
programs cover 29.95% of programs on average, indicating that users can grasp
29.95% of solution approaches by referring to only 10 programs. The proposed
approach shows the potential of reducing the learners' burden of referring to
too many solutions and motivating them to learn a variety of solution
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11788">
<div class="article-summary-box-inner">
<span><p>As an application domain where the slightest qualitative improvements can
yield immense value, finance is a promising candidate for early quantum
advantage. Focusing on the rapidly advancing field of Quantum Natural Language
Processing (QNLP), we explore the practical applicability of the two central
approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the
problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data
generation approach, we conduct a case study with more than 1000 realistic
sentences and find that QLSTMs can be trained substantially faster than
DisCoCat while also achieving close to classical results for their available
software implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15217">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) is a technique for training
AI systems to align with human goals. RLHF has emerged as the central method
used to finetune state-of-the-art large language models (LLMs). Despite this
popularity, there has been relatively little public work systematizing its
flaws. In this paper, we (1) survey open problems and fundamental limitations
of RLHF and related methods; (2) overview techniques to understand, improve,
and complement RLHF in practice; and (3) propose auditing and disclosure
standards to improve societal oversight of RLHF systems. Our work emphasizes
the limitations of RLHF and highlights the importance of a multi-faceted
approach to the development of safer AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15453">
<div class="article-summary-box-inner">
<span><p>The paper presents the main characteristics and a preliminary implementation
of a novel computational framework named CompLog. Inspired by probabilistic
programming systems like ProbLog, CompLog builds upon the inferential
mechanisms proposed by Simplicity Theory, relying on the computation of two
Kolmogorov complexities (here implemented as min-path searches via ASP
programs) rather than probabilistic inference. The proposed system enables
users to compute ex-post and ex-ante measures of unexpectedness of a certain
situation, mapping respectively to posterior and prior subjective
probabilities. The computation is based on the specification of world and
mental models by means of causal and descriptive relations between predicates
weighted by complexity. The paper illustrates a few examples of application:
generating relevant descriptions, and providing alternative approaches to
disjunction and to negation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenging the Machinery of Generative AI with Fact-Checking: Ontology-Driven Biological Graphs for Verifying Human Disease-Gene Links. (arXiv:2308.03929v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03929">
<div class="article-summary-box-inner">
<span><p>Methods: we adopted a biological networks approach that enables the
systematic interrogation of ChatGPT's linked entities. In particular, we
designed an ontology-driven fact-checking algorithm that compares biological
graphs constructed from approximately 200,000 PubMed abstracts with
counterparts constructed from a dataset generated using the ChatGPT-3.5 Turbo
model. The nodes refer to biological entities (genes and diseases) that occur
in the text. The edges represent the co-occurrence relationships of two
entities mentioned in the same document, weighted by the proximity distance
between these two entities. This research assumes a ``closed-world
assumption'', meaning that fact-checking is performed only using the literature
dataset as our ground truth. Results: in ten samples of 250 randomly selected
records from the ChatGPT dataset of 1000 ``simulated'' articles , the
fact-checking link accuracy ranged from 70% to 86%, while the remainder of the
links remained unverified. Given the closed world assumption, the fact-checking
precision is significant. When measuring and comparing the proximity distances
of the edges of literature graphs against ChatGPT graphs we found that the
ChatGPT distances were significantly shorter (ranging from 90 to 153) character
distance. In contrast, the proximity distance of biological entities identified
in the literature ranged from 236 to 765 character distance. This pattern held
true for all the relationships among biological entities in the ten samples.
Conclusion: this study demonstrated a reasonably high percentage accuracy of
aggregate fact-checking of disease-gene relationships found in
ChatGPT-generated texts. The strikingly consistent pattern of short proximity
distances across all samples offers an illuminating feedback to the biological
knowledge we possess in the literature today.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11224">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have garnered considerable interest within both
academic and industrial. Yet, the application of LLMs to graph data remains
under-explored. In this study, we evaluate the capabilities of four LLMs in
addressing several analytical problems with graph data. We employ four distinct
evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.
Our results show that: 1) LLMs effectively comprehend graph data in natural
language and reason with graph topology. 2) GPT models can generate logical and
coherent results, outperforming alternatives in correctness. 3) All examined
LLMs face challenges in structural reasoning, with techniques like zero-shot
chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT
models often produce erroneous answers in multi-answer tasks, raising concerns
in fidelity. 5) GPT models exhibit elevated confidence in their outputs,
potentially hindering their rectification capacities. Notably, GPT-4 has
demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own
previous iterations. The code is available at:
https://github.com/Ayame1006/LLMtoGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11257">
<div class="article-summary-box-inner">
<span><p>The semantic parsing-based method is an important research branch for
knowledge-based question answering. It usually generates executable programs
lean upon the question and then conduct them to reason answers over a knowledge
base. Benefit from this inherent mechanism, it has advantages in the
performance and the interpretability. However, traditional semantic parsing
methods usually generate a complete program before executing it, which
struggles with multi-hop question answering over heterogeneous knowledge. On
one hand, generating a complete multi-hop program relies on multiple
heterogeneous supporting facts, and it is difficult for generators to
understand these facts simultaneously. On the other hand, this way ignores the
semantic information of the intermediate answers at each hop, which is
beneficial for subsequent generation. To alleviate these challenges, we propose
a self-iterative framework for multi-hop program generation (HopPG) over
heterogeneous knowledge, which leverages the previous execution results to
retrieve supporting facts and generate subsequent programs hop by hop. We
evaluate our model on MMQA-T^2, and the experimental results show that HopPG
outperforms existing semantic-parsing-based baselines, especially on the
multi-hop questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13032">
<div class="article-summary-box-inner">
<span><p>The paper considers the possibility to fine-tune Llama 2 GPT large language
model (LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13566">
<div class="article-summary-box-inner">
<span><p>Despite the great advance of Multimodal Large Language Models (MLLMs) in both
instruction dataset building and benchmarking, the independence of training and
evaluation makes current MLLMs hard to further improve their capability under
the guidance of evaluation results with a relatively low human cost. In this
paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data
generation, model training, and evaluation. Within each loop iteration, the
MLLM-DataEngine first analyze the weakness of the model based on the evaluation
results, then generate a proper incremental dataset for the next training
iteration and enhance the model capability iteratively. Compared with previous
data collection methods which are separate from the benchmarking, the data
generated by MLLM-DataEngine shows better targeting, quality, and correctness.
For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts
the ratio of different types of data within each incremental dataset based on
the benchmarking results. For quality, we resort to GPT-4 to generate
high-quality data with each given data type. For correctness, prompt design is
critical for the data generation results. Rather than previous hand-crafted
prompt, we propose an Interactive Prompt Optimization strategy, which optimizes
the prompt with the multi-round interaction between human and GPT, and improve
the correctness of generated data greatly. Through extensive experiments, we
find our MLLM-DataEngine could boost the MLLM capability in a targeted and
automatic manner, with only a few human participation. We hope it could be a
general solution for the following MLLMs building. The MLLM-DataEngine has been
open-sourced and is now available at
https://github.com/opendatalab/MLLM-DataEngine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13916">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15772">
<div class="article-summary-box-inner">
<span><p>Mixture-of-experts (MoE) architecture has been proven a powerful method for
diverse tasks in training deep models in many applications. However, current
MoE implementations are task agnostic, treating all tokens from different tasks
in the same manner. In this work, we instead design a novel method that
incorporates task information into MoE models at different granular levels with
shared dynamic task-based adapters. Our experiments and analysis show the
advantages of our approaches over the dense and canonical MoE models on
multi-task multilingual machine translations. With task-specific adapters, our
models can additionally generalize to new tasks efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01717">
<div class="article-summary-box-inner">
<span><p>The objective of topic inference in research proposals aims to obtain the
most suitable disciplinary division from the discipline system defined by a
funding agency. The agency will subsequently find appropriate peer review
experts from their database based on this division. Automated topic inference
can reduce human errors caused by manual topic filling, bridge the knowledge
gap between funding agencies and project applicants, and improve system
efficiency. Existing methods focus on modeling this as a hierarchical
multi-label classification problem, using generative models to iteratively
infer the most appropriate topic information. However, these methods overlook
the gap in scale between interdisciplinary research proposals and
non-interdisciplinary ones, leading to an unjust phenomenon where the automated
inference system categorizes interdisciplinary proposals as
non-interdisciplinary, causing unfairness during the expert assignment. How can
we address this data imbalance issue under a complex discipline system and
hence resolve this unfairness? In this paper, we implement a topic label
inference system based on a Transformer encoder-decoder architecture.
Furthermore, we utilize interpolation techniques to create a series of
pseudo-interdisciplinary proposals from non-interdisciplinary ones during
training based on non-parametric indicators such as cross-topic probabilities
and topic occurrence probabilities. This approach aims to reduce the bias of
the system during model training. Finally, we conduct extensive experiments on
a real-world dataset to verify the effectiveness of the proposed method. The
experimental results demonstrate that our training strategy can significantly
mitigate the unfairness generated in the topic inference task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01940">
<div class="article-summary-box-inner">
<span><p>With the emergence of Large Language Models (LLMs), there has been a
significant improvement in the programming capabilities of models, attracting
growing attention from researchers. We propose CodeApex, a bilingual benchmark
dataset focusing on the programming comprehension and code generation abilities
of LLMs. CodeApex comprises three types of multiple-choice questions:
conceptual understanding, commonsense reasoning, and multi-hop reasoning,
designed to evaluate LLMs on programming comprehension tasks. Additionally,
CodeApex utilizes algorithmic questions and corresponding test cases to assess
the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs,
including both general-purpose and specialized models. GPT exhibits the best
programming capabilities, achieving approximate accuracies of 50% and 56% on
the two tasks, respectively. There is still significant room for improvement in
programming tasks. We hope that CodeApex can serve as a reference for
evaluating the coding capabilities of LLMs, further promoting their development
and growth. Datasets are released at https://github.com/APEXLAB/CodeApex.git.
CodeApex submission website is https://apex.sjtu.edu.cn/codeapex/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRASS: Unified Generation Model for Speech-to-Semantic Tasks. (arXiv:2309.02780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02780">
<div class="article-summary-box-inner">
<span><p>This paper explores the instruction fine-tuning technique for
speech-to-semantic tasks by introducing a unified end-to-end (E2E) framework
that generates target text conditioned on a task-related prompt for audio data.
We pre-train the model using large and diverse data, where instruction-speech
pairs are constructed via a text-to-speech (TTS) system. Extensive experiments
demonstrate that our proposed model achieves state-of-the-art (SOTA) results on
many benchmarks covering speech named entity recognition, speech sentiment
analysis, speech question answering, and more, after fine-tuning. Furthermore,
the proposed model achieves competitive performance in zero-shot and few-shot
scenarios. To facilitate future work on instruction fine-tuning for
speech-to-semantic tasks, we release our instruction dataset and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market. (arXiv:2309.04389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04389">
<div class="article-summary-box-inner">
<span><p>In recent years, great advances in pre-trained language models (PLMs) have
sparked considerable research focus and achieved promising performance on the
approach of dense passage retrieval, which aims at retrieving relative passages
from massive corpus with given questions. However, most of existing datasets
mainly benchmark the models with factoid queries of general commonsense, while
specialised fields such as finance and economics remain unexplored due to the
deficiency of large-scale and high-quality datasets with expert annotations. In
this work, we propose a new task, policy retrieval, by introducing the Chinese
Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages
labeled by experienced experts with relevant articles from 10k+ entries in our
collected Chinese policy corpus. Experiments on lexical, embedding and
fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet
also suggests ample potential for improvement. Our best performing baseline
achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on
dev set.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-12 23:11:11.428062163 UTC">2023-09-12 23:11:11 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>