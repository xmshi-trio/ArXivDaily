<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-01T01:30:00Z">03-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Phone and speaker spatial organization in self-supervised speech representations. (arXiv:2302.14055v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14055">
<div class="article-summary-box-inner">
<span><p>Self-supervised representations of speech are currently being widely used for
a large number of applications. Recently, some efforts have been made in trying
to analyze the type of information present in each of these representations.
Most such work uses downstream models to test whether the representations can
be successfully used for a specific task. The downstream models, though,
typically perform nonlinear operations on the representation extracting
information that may not have been readily available in the original
representation. In this work, we analyze the spatial organization of phone and
speaker information in several state-of-the-art speech representations using
methods that do not require a downstream model. We measure how different layers
encode basic acoustic parameters such as formants and pitch using
representation similarity analysis. Further, we study the extent to which each
representation clusters the speech samples by phone or speaker classes using
non-parametric statistical testing. Our results indicate that models represent
these speech attributes differently depending on the target task used during
pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14057">
<div class="article-summary-box-inner">
<span><p>Automatic detection of multimodal fake news has gained a widespread attention
recently. Many existing approaches seek to fuse unimodal features to produce
multimodal news representations. However, the potential of powerful cross-modal
contrastive learning methods for fake news detection has not been well
exploited. Besides, how to aggregate features from different modalities to
boost the performance of the decision-making process is still an open question.
To address that, we propose COOLANT, a cross-modal contrastive learning
framework for multimodal fake news detection, aiming to achieve more accurate
image-text alignment. To further improve the alignment precision, we leverage
an auxiliary task to soften the loss term of negative samples during the
contrast process. A cross-modal fusion module is developed to learn the
cross-modality correlations. An attention mechanism with an attention guidance
module is implemented to help effectively and interpretably aggregate the
aligned unimodal representations and the cross-modality correlations. Finally,
we evaluate the COOLANT and conduct a comparative study on two widely used
datasets, Twitter and Weibo. The experimental results demonstrate that our
COOLANT outperforms previous approaches by a large margin and achieves new
state-of-the-art results on the two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanations for Automatic Speech Recognition. (arXiv:2302.14062v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14062">
<div class="article-summary-box-inner">
<span><p>We address quality assessment for neural network based ASR by providing
explanations that help increase our understanding of the system and ultimately
help build trust in the system. Compared to simple classification labels,
explaining transcriptions is more challenging as judging their correctness is
not straightforward and transcriptions as a variable-length sequence is not
handled by existing interpretable machine learning models. We provide an
explanation for an ASR transcription as a subset of audio frames that is both a
minimal and sufficient cause of the transcription. To do this, we adapt
existing explainable AI (XAI) techniques from image classification-Statistical
Fault Localisation(SFL) and Causal. Additionally, we use an adapted version of
Local Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in
our experiments. We evaluate the quality of the explanations generated by the
proposed techniques over three different ASR ,Google API, the baseline model of
Sphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14115">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce Vid2Seq, a multi-modal single-stage dense event
captioning model pretrained on narrated videos which are readily-available at
scale. The Vid2Seq architecture augments a language model with special time
tokens, allowing it to seamlessly predict event boundaries and textual
descriptions in the same output sequence. Such a unified model requires
large-scale training data, which is not available in current annotated
datasets. We show that it is possible to leverage unlabeled narrated videos for
dense video captioning, by reformulating sentence boundaries of transcribed
speech as pseudo event boundaries, and using the transcribed speech sentences
as pseudo event captions. The resulting Vid2Seq model pretrained on the
YT-Temporal-1B dataset improves the state of the art on a variety of dense
video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the video paragraph captioning task and the
standard task of video clip captioning. Our code and models will be publicly
released at https://antoyang.github.io/vid2seq.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding. (arXiv:2302.14132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14132">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation learning (SSL) has shown to be
effective in various downstream tasks, but SSL models are usually large and
slow. Model compression techniques such as pruning aim to reduce the model size
and computation without degradation in accuracy. Prior studies focus on the
pruning of Transformers; however, speech models not only utilize a stack of
Transformer blocks, but also combine a frontend network based on multiple
convolutional layers for low-level feature representation learning. This
frontend has a small size but a heavy computational cost. In this work, we
propose three task-specific structured pruning methods to deal with such
heterogeneous networks. Experiments on LibriSpeech and SLURP show that the
proposed method is more accurate than the original wav2vec2-base with 10% to
30% less computation, and is able to reduce the computation by 40% to 50%
without any degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TabGenie: A Toolkit for Table-to-Text Generation. (arXiv:2302.14169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14169">
<div class="article-summary-box-inner">
<span><p>Heterogenity of data-to-text generation datasets limits the research on
data-to-text generation systems. We present TabGenie - a toolkit which enables
researchers to explore, preprocess, and analyze a variety of data-to-text
generation datasets through the unified framework of table-to-text generation.
In TabGenie, all the inputs are represented as tables with associated metadata.
The tables can be explored through the web interface, which also provides an
interactive mode for debugging table-to-text generation, facilitates
side-by-side comparison of generated system outputs, and allows easy exports
for manual analysis. Furthermore, TabGenie is equipped with command line
processing tools and Python bindings for unified dataset loading and
processing. We release TabGenie as a PyPI package and provide its open-source
code and a live demo at https://github.com/kasnerz/tabgenie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14220">
<div class="article-summary-box-inner">
<span><p>Pretrained large character-level language models have been recently
revitalized and shown to be competitive with subword models across a range of
NLP tasks. However, there has not been any research showing their effectiveness
in neural machine translation (NMT). This work performs an extensive comparison
across multiple languages and experimental conditions of state-of-the-art
character- and subword-level pre-trained models (ByT5 and mT5, respectively) on
NMT, and shows that the former not only are effective in translation, but
frequently outperform subword models, particularly in cases where training data
is limited. The only drawback of character models appears to be their
inefficiency (at least 4 times slower to train and for inference). Further
analysis indicates that character models are capable of implicitly translating
on the word or subword level, thereby nullifying a major potential weakness of
operating on the character level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14225">
<div class="article-summary-box-inner">
<span><p>Masked Language Modeling (MLM) is widely used to pretrain language models.
The standard random masking strategy in MLM causes the pre-trained language
models (PLMs) to be biased toward high-frequency tokens. Representation
learning of rare tokens is poor and PLMs have limited performance on downstream
tasks. To alleviate this frequency bias issue, we propose two simple and
effective Weighted Sampling strategies for masking tokens based on the token
frequency and training loss. We apply these two strategies to BERT and obtain
Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity
benchmark (STS) show that WSBERT significantly improves sentence embeddings
over BERT. Combining WSBERT with calibration methods and prompt learning
further improves sentence embeddings. We also investigate fine-tuning WSBERT on
the GLUE benchmark and show that Weighted Sampling also improves the transfer
learning capability of the backbone PLM. We further analyze and provide
insights into how WSBERT improves token embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Summarization via ChatGPT. (arXiv:2302.14229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14229">
<div class="article-summary-box-inner">
<span><p>Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of ChatGPT has attracted wide attention from the computational linguistics
community. However, it is not yet known the performance of ChatGPT on CLS. In
this report, we empirically use various prompts to guide ChatGPT to perform
zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and
provide a preliminary evaluation on its generated summaries.We find that
ChatGPT originally prefers to produce lengthy summaries with more detailed
information. But with the help of an interactive prompt, ChatGPT can balance
between informativeness and conciseness, and significantly improve its CLS
performance. Experimental results on three widely-used CLS datasets show that
ChatGPT outperforms the advanced GPT 3.5 model (i.e., text-davinci-003). In
addition, we provide qualitative case studies to show the superiority of
ChatGPT on CLS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14233">
<div class="article-summary-box-inner">
<span><p>Mining large corpora can generate useful discoveries but is time-consuming
for humans. We formulate a new task, D5, that automatically discovers
differences between two large corpora in a goal-driven way. The task input is a
problem comprising a research goal "$\textit{comparing the side effects of drug
A and drug B}$" and a corpus pair (two large collections of patients'
self-reported reactions after taking each drug). The output is a language
description (discovery) of how these corpora differ (patients taking drug A
"$\textit{mention feelings of paranoia}$" more often). We build a D5 system,
and to quantitatively measure its performance, we 1) contribute a meta-dataset,
OpenD5, aggregating 675 open-ended problems ranging across business, social
sciences, humanities, machine learning, and health, and 2) propose a set of
unified evaluation metrics: validity, relevance, novelty, and significance.
With the dataset and the unified metrics, we confirm that language models can
use the goals to propose more relevant, novel, and significant candidate
discoveries. Finally, our system produces discoveries previously unknown to the
authors on a wide range of applications in OpenD5, including temporal and
demographic differences in discussion topics, political stances and stereotypes
in speech, insights in commercial reviews, and error patterns in NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmented Transformers with Adaptive n-grams Embedding for Multilingual Scene Text Recognition. (arXiv:2302.14261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14261">
<div class="article-summary-box-inner">
<span><p>While vision transformers have been highly successful in improving the
performance in image-based tasks, not much work has been reported on applying
transformers to multilingual scene text recognition due to the complexities in
the visual appearance of multilingual texts. To fill the gap, this paper
proposes an augmented transformer architecture with n-grams embedding and
cross-language rectification (TANGER). TANGER consists of a primary transformer
with single patch embeddings of visual images, and a supplementary transformer
with adaptive n-grams embeddings that aims to flexibly explore the potential
correlations between neighbouring visual patches, which is essential for
feature extraction from multilingual scene texts. Cross-language rectification
is achieved with a loss function that takes into account both language
identification and contextual coherence scoring. Extensive comparative studies
are conducted on four widely used benchmark datasets as well as a new
multilingual scene text dataset containing Indonesian, English, and Chinese
collected from tourism scenes in Indonesia. Our experimental results
demonstrate that TANGER is considerably better compared to the
state-of-the-art, especially in handling complex multilingual scene texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HugNLP: A Unified and Comprehensive Library for Natural Language Processing. (arXiv:2302.14286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14286">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce HugNLP, a unified and comprehensive library for
natural language processing (NLP) with the prevalent backend of HuggingFace
Transformers, which is designed for NLP researchers to easily utilize
off-the-shelf algorithms and develop novel methods with user-defined models and
tasks in real-world scenarios. HugNLP consists of a hierarchical structure
including models, processors and applications that unifies the learning process
of pre-trained language models (PLMs) on different NLP tasks. Additionally, we
present some featured NLP applications to show the effectiveness of HugNLP,
such as knowledge-enhanced PLMs, universal information extraction, low-resource
mining, and code understanding and generation, etc. The source code will be
released on GitHub (https://github.com/wjn1996/HugNLP).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniFLG: Unified Facial Landmark Generator from Text or Speech. (arXiv:2302.14337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14337">
<div class="article-summary-box-inner">
<span><p>Talking face generation has been extensively investigated owing to its wide
applicability. The two primary frameworks used for talking face generation
comprise a text-driven framework, which generates synchronized speech and
talking faces from text, and a speech-driven framework, which generates talking
faces from speech. To integrate these frameworks, this paper proposes a unified
facial landmark generator (UniFLG). The proposed system exploits end-to-end
text-to-speech not only for synthesizing speech but also for extracting a
series of latent representations that are common to text and speech, and feeds
it to a landmark decoder to generate facial landmarks. We demonstrate that our
system achieves higher naturalness in both speech synthesis and facial landmark
generation compared to the state-of-the-art text-driven method. We further
demonstrate that our system can generate facial landmarks from speech of
speakers without facial video data or even speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Spaces of Meanings: the Compositional Language of VLMs. (arXiv:2302.14383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14383">
<div class="article-summary-box-inner">
<span><p>We investigate compositional structures in vector data embeddings from
pre-trained vision-language models (VLMs). Traditionally, compositionality has
been associated with algebraic operations on embeddings of words from a
pre-existing vocabulary. In contrast, we seek to approximate label
representations from a text encoder as combinations of a smaller set of vectors
in the embedding space. These vectors can be seen as "ideal words" which can be
used to generate new concepts in an efficient way. We present a theoretical
framework for understanding linear compositionality, drawing connections with
mathematical representation theory and previous definitions of disentanglement.
We provide theoretical and empirical evidence that ideal words provide good
compositional approximations of composite concepts and can be more effective
than token-based decompositions of the same concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information-Restricted Neural Language Models Reveal Different Brain Regions' Sensitivity to Semantics, Syntax and Context. (arXiv:2302.14389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14389">
<div class="article-summary-box-inner">
<span><p>A fundamental question in neurolinguistics concerns the brain regions
involved in syntactic and semantic processing during speech comprehension, both
at the lexical (word processing) and supra-lexical levels (sentence and
discourse processing). To what extent are these regions separated or
intertwined? To address this question, we trained a lexical language model,
Glove, and a supra-lexical language model, GPT-2, on a text corpus from which
we selectively removed either syntactic or semantic information. We then
assessed to what extent these information-restricted models were able to
predict the time-courses of fMRI signal of humans listening to naturalistic
text. We also manipulated the size of contextual information provided to GPT-2
in order to determine the windows of integration of brain regions involved in
supra-lexical processing. Our analyses show that, while most brain regions
involved in language are sensitive to both syntactic and semantic variables,
the relative magnitudes of these effects vary a lot across these regions.
Furthermore, we found an asymmetry between the left and right hemispheres, with
semantic and syntactic processing being more dissociated in the left hemisphere
than in the right, and the left and right hemispheres showing respectively
greater sensitivity to short and long contexts. The use of
information-restricted NLP models thus shed new light on the spatial
organization of syntactic processing, semantic processing and compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation. (arXiv:2302.14401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14401">
<div class="article-summary-box-inner">
<span><p>We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters
capable of knowledge-grounded conversation in Chinese using a search engine to
access the Internet knowledge. GLM-Dialog offers a series of applicable
techniques for exploiting various external knowledge including both helpful and
noisy knowledge, enabling the creation of robust knowledge-grounded dialogue
LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we
also propose a novel evaluation method to allow humans to converse with
multiple deployed bots simultaneously and compare their performance implicitly
instead of explicitly rating using multidimensional metrics.Comprehensive
evaluations from automatic to human perspective demonstrate the advantages of
GLM-Dialog comparing with existing open source Chinese dialogue models. We
release both the model checkpoint and source code, and also deploy it as a
WeChat application to interact with users. We offer our evaluation platform
online in an effort to prompt the development of open source models and
reliable dialogue evaluation systems. The additional easy-to-use toolkit that
consists of short text entity linking, query generation, and helpful knowledge
classification is also released to enable diverse applications. All the source
code is available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Clarification Requests in Multimodal Collaborative Dialogue Games: Tasks, and an Analysis of the CoDraw Dataset. (arXiv:2302.14406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14406">
<div class="article-summary-box-inner">
<span><p>In visual instruction-following dialogue games, players can engage in repair
mechanisms in face of an ambiguous or underspecified instruction that cannot be
fully mapped to actions in the world. In this work, we annotate Instruction
Clarification Requests (iCRs) in CoDraw, an existing dataset of interactions in
a multimodal collaborative dialogue game. We show that it contains lexically
and semantically diverse iCRs being produced self-motivatedly by players
deciding to clarify in order to solve the task successfully. With 8.8k iCRs
found in 9.9k dialogues, CoDraw-iCR (v1) is a large spontaneous iCR corpus,
making it a valuable resource for data-driven research on clarification in
dialogue. We then formalise and provide baseline models for two tasks:
Determining when to make an iCR and how to recognise them, in order to
investigate to what extent these tasks are learnable from data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases. (arXiv:2302.14413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14413">
<div class="article-summary-box-inner">
<span><p>Recent studies reveal that various biases exist in different NLP tasks, and
over-reliance on biases results in models' poor generalization ability and low
adversarial robustness. To mitigate datasets biases, previous works propose
lots of debiasing techniques to tackle specific biases, which perform well on
respective adversarial sets but fail to mitigate other biases. In this paper,
we propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can
mitigate multiple dataset biases effectively and efficiently. Experiments on
Natural Language Inference and Paraphrase Identification tasks demonstrate that
SMoA outperforms full-finetuning, adapter tuning baselines, and prior strong
debiasing methods. Further analysis indicates the interpretability of SMoA that
sub-adapter can capture specific pattern from the training data and specialize
to handle specific bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text classification dataset and analysis for Uzbek language. (arXiv:2302.14494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14494">
<div class="article-summary-box-inner">
<span><p>Text classification is an important task in Natural Language Processing
(NLP), where the goal is to categorize text data into predefined classes. In
this study, we analyse the dataset creation steps and evaluation techniques of
multi-label news categorisation task as part of text classification. We first
present a newly obtained dataset for Uzbek text classification, which was
collected from 10 different news and press websites and covers 15 categories of
news, press and law texts. We also present a comprehensive evaluation of
different models, ranging from traditional bag-of-words models to deep learning
architectures, on this newly created dataset. Our experiments show that the
Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based
models outperform the rule-based models. The best performance is achieved by
the BERTbek model, which is a transformer-based BERT model trained on the Uzbek
corpus. Our findings provide a good baseline for further research in Uzbek text
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Long Text Modeling with Transformers. (arXiv:2302.14502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14502">
<div class="article-summary-box-inner">
<span><p>Modeling long texts has been an essential technique in the field of natural
language processing (NLP). With the ever-growing number of long documents, it
is important to develop effective modeling methods that can process and analyze
such texts. However, long texts pose important research challenges for existing
text models, with more complex semantics and special characteristics. In this
paper, we provide an overview of the recent advances on long texts modeling
based on Transformer models. Firstly, we introduce the formal definition of
long text modeling. Then, as the core content, we discuss how to process long
input to satisfy the length limitation and design improved Transformer
architectures to effectively extend the maximum context length. Following this,
we discuss how to adapt Transformer models to capture the special
characteristics of long texts. Finally, we describe four typical applications
involving long text modeling and conclude this paper with a discussion of
future directions. Our survey intends to provide researchers with a synthesis
and pointer to related work on long text modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14520">
<div class="article-summary-box-inner">
<span><p>We describe GEMBA, a GPT-based metric for assessment of translation quality,
which works both with a reference translation and without. In our evaluation,
we focus on zero-shot prompting, comparing four prompt variants in two modes,
based on the availability of the reference. We investigate seven versions of
GPT models, including ChatGPT. We show that our method for translation quality
assessment only works with GPT 3.5 and larger models. Comparing to results from
WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in
both modes when compared to MQM-based human labels. Our results are valid on
the system level for all three WMT22 Metrics shared task language pairs, namely
English into German, English into Russian, and Chinese into English. This
provides a first glimpse into the usefulness of pre-trained, generative large
language models for quality assessment of translations. We publicly release all
our code and prompt templates used for the experiments described in this work,
as well as all corresponding scoring results, to allow for external validation
and reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Heteronym Resolution Pipeline Using RAD-TTS Aligners. (arXiv:2302.14523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14523">
<div class="article-summary-box-inner">
<span><p>Grapheme-to-phoneme (G2P) transduction is part of the standard text-to-speech
(TTS) pipeline. However, G2P conversion is difficult for languages that contain
heteronyms -- words that have one spelling but can be pronounced in multiple
ways. G2P datasets with annotated heteronyms are limited in size and expensive
to create, as human labeling remains the primary method for heteronym
disambiguation. We propose a RAD-TTS Aligner-based pipeline to automatically
disambiguate heteronyms in datasets that contain both audio with text
transcripts. The best pronunciation can be chosen by generating all possible
candidates for each heteronym and scoring them with an Aligner model. The
resulting labels can be used to create training datasets for use in both
multi-stage and end-to-end G2P systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face. (arXiv:2302.14534v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14534">
<div class="article-summary-box-inner">
<span><p>We present Spacerini, a modular framework for seamless building and
deployment of interactive search applications, designed to facilitate the
qualitative analysis of large scale research datasets. Spacerini integrates
features from both the Pyserini toolkit and the Hugging Face ecosystem to ease
the indexing text collections and deploy them as search engines for ad-hoc
exploration and to make the retrieval of relevant data points quick and
efficient. The user-friendly interface enables searching through massive
datasets in a no-code fashion, making Spacerini broadly accessible to anyone
looking to qualitatively audit their text collections. This is useful both to
IR~researchers aiming to demonstrate the capabilities of their indexes in a
simple and interactive way, and to NLP~researchers looking to better understand
and audit the failure modes of large language models. The framework is open
source and available on GitHub: https://github.com/castorini/hf-spacerini, and
includes utilities to load, pre-process, index, and deploy local and web search
applications. A portfolio of applications created with Spacerini for a
multitude of use cases can be found by visiting https://hf.co/spacerini.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2022 NIST Language Recognition Evaluation. (arXiv:2302.14624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14624">
<div class="article-summary-box-inner">
<span><p>In 2022, the U.S. National Institute of Standards and Technology (NIST)
conducted the latest Language Recognition Evaluation (LRE) in an ongoing series
administered by NIST since 1996 to foster research in language recognition and
to measure state-of-the-art technology. Similar to previous LREs, LRE22 focused
on conversational telephone speech (CTS) and broadcast narrowband speech (BNBS)
data. LRE22 also introduced new evaluation features, such as an emphasis on
African languages, including low resource languages, and a test set consisting
of segments containing between 3s and 35s of speech randomly sampled and
extracted from longer recordings. A total of 21 research organizations, forming
16 teams, participated in this 3-month long evaluation and made a total of 65
valid system submissions to be evaluated. This paper presents an overview of
LRE22 and an analysis of system performance over different evaluation
conditions. The evaluation results suggest that Oromo and Tigrinya are easier
to detect while Xhosa and Zulu are more challenging. A greater confusability is
seen for some language pairs. When speech duration increased, system
performance significantly increased up to a certain duration, and then a
diminishing return on system performance is observed afterward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H-AES: Towards Automated Essay Scoring for Hindi. (arXiv:2302.14635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14635">
<div class="article-summary-box-inner">
<span><p>The use of Natural Language Processing (NLP) for Automated Essay Scoring
(AES) has been well explored in the English language, with benchmark models
exhibiting performance comparable to human scorers. However, AES in Hindi and
other low-resource languages remains unexplored. In this study, we reproduce
and compare state-of-the-art methods for AES in the Hindi domain. We employ
classical feature-based Machine Learning (ML) and advanced end-to-end models,
including LSTM Networks and Fine-Tuned Transformer Architecture, in our
approach and derive results comparable to those in the English language domain.
Hindi being a low-resource language, lacks a dedicated essay-scoring corpus. We
train and evaluate our models using translated English essays and empirically
measure their performance on our own small-scale, real-world Hindi corpus. We
follow this up with an in-depth analysis discussing prompt-specific behavior of
different language models implemented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic Speech Processing. (arXiv:2302.14638v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14638">
<div class="article-summary-box-inner">
<span><p>Paralinguistic speech processing is important in addressing many issues, such
as sentiment and neurocognitive disorder analyses. Recently, Transformer has
achieved remarkable success in the natural language processing field and has
demonstrated its adaptation to speech. However, previous works on Transformer
in the speech field have not incorporated the properties of speech, leaving the
full potential of Transformer unexplored. In this paper, we consider the
characteristics of speech and propose a general structure-based framework,
called SpeechFormer++, for paralinguistic speech processing. More concretely,
following the component relationship in the speech signal, we design a unit
encoder to model the intra- and inter-unit information (i.e., frames, phones,
and words) efficiently. According to the hierarchical relationship, we utilize
merging blocks to generate features at different granularities, which is
consistent with the structural pattern in the speech signal. Moreover, a word
encoder is introduced to integrate word-grained features into each unit
encoder, which effectively balances fine-grained and coarse-grained
information. SpeechFormer++ is evaluated on the speech emotion recognition
(IEMOCAP &amp; MELD), depression classification (DAIC-WOZ) and Alzheimer's disease
detection (Pitt) tasks. The results show that SpeechFormer++ outperforms the
standard Transformer while greatly reducing the computational cost.
Furthermore, it delivers superior results compared to the state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14679">
<div class="article-summary-box-inner">
<span><p>Electronic Health Records (EHRs) contain sensitive patient information, which
presents privacy concerns when sharing such data. Synthetic data generation is
a promising solution to mitigate these risks, often relying on deep generative
models such as Generative Adversarial Networks (GANs). However, recent studies
have shown that diffusion models offer several advantages over GANs, such as
generation of more realistic synthetic data and stable training in generating
data modalities, including image, text, and sound. In this work, we investigate
the potential of diffusion models for generating realistic mixed-type tabular
EHRs, comparing TabDDPM model with existing methods on four datasets in terms
of data quality, utility, privacy, and augmentation. Our experiments
demonstrate that TabDDPM outperforms the state-of-the-art models across all
evaluation metrics, except for privacy, which confirms the trade-off between
privacy and utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue. (arXiv:2302.14680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14680">
<div class="article-summary-box-inner">
<span><p>The demand for multimodal dialogue systems has been rising in various
domains, emphasizing the importance of interpreting multimodal inputs from
conversational and situational contexts. We explore three methods to tackle
this problem and evaluate them on the largest situated dialogue dataset, SIMMC
2.1. Our best method, scene-dialogue alignment, improves the performance by
~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and
discussion regarding the limitation of our methods and the potential directions
for future works. Our code is publicly available at
https://github.com/holylovenia/multimodal-object-identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Instruction Learning. (arXiv:2302.14691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14691">
<div class="article-summary-box-inner">
<span><p>Instruction learning of Large Language Models (LLMs) has enabled zero-shot
task generalization. However, instruction learning has been predominantly
approached as a fine-tuning problem, including instruction tuning and
reinforcement learning from human feedback, where LLMs are multi-task
fine-tuned on various tasks with instructions. In this paper, we present a
surprising finding that applying in-context learning to instruction learning,
referred to as In-Context Instruction Learning (ICIL), significantly improves
the zero-shot task generalization performance for both pretrained and
instruction-fine-tuned models. One of the core advantages of ICIL is that it
uses a single fixed prompt to evaluate all tasks, which is a concatenation of
cross-task demonstrations. In particular, we demonstrate that the most powerful
instruction-fine-tuned baseline (text-davinci-003) also benefits from ICIL by
9.3%, indicating that the effect of ICIL is complementary to instruction-based
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Japanese CCGBank empirically correct? A case study of passive and causative constructions. (arXiv:2302.14708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14708">
<div class="article-summary-box-inner">
<span><p>The Japanese CCGBank serves as training and evaluation data for developing
Japanese CCG parsers. However, since it is automatically generated from the
Kyoto Corpus, a dependency treebank, its linguistic validity still needs to be
sufficiently verified. In this paper, we focus on the analysis of
passive/causative constructions in the Japanese CCGBank and show that, together
with the compositional semantics of ccg2lambda, a semantic parsing system, it
yields empirically wrong predictions for the nested construction of passives
and causatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-training through Classifier Disagreement for Cross-Domain Opinion Target Extraction. (arXiv:2302.14719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14719">
<div class="article-summary-box-inner">
<span><p>Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental
task in opinion mining that aims to extract the targets (or aspects) on which
opinions have been expressed. Recent work focus on cross-domain OTE, which is
typically encountered in real-world scenarios, where the testing and training
distributions differ. Most methods use domain adversarial neural networks that
aim to reduce the domain gap between the labelled source and unlabelled target
domains to improve target domain performance. However, this approach only
aligns feature distributions and does not account for class-wise feature
alignment, leading to suboptimal results. Semi-supervised learning (SSL) has
been explored as a solution, but is limited by the quality of pseudo-labels
generated by the model. Inspired by the theoretical foundations in domain
adaptation [2], we propose a new SSL approach that opts for selecting target
samples whose model output from a domain-specific teacher and student network
disagree on the unlabelled target data, in an effort to boost the target domain
performance. Extensive experiments on benchmark cross-domain OTE datasets show
that this approach is effective and performs consistently well in settings with
large domain shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Classifying Emotions based on Text: A Comparative Exploration of Different Datasets. (arXiv:2302.14727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14727">
<div class="article-summary-box-inner">
<span><p>Emotion Classification based on text is a task with many applications which
has received growing interest in recent years. This paper presents a
preliminary study with the goal to help researchers and practitioners gain
insight into relatively new datasets as well as emotion classification in
general. We focus on three datasets that were recently presented in the related
literature, and we explore the performance of traditional as well as
state-of-the-art deep learning models in the presence of different
characteristics in the data. We also explore the use of data augmentation in
order to improve performance. Our experimental work shows that state-of-the-art
models such as RoBERTa perform the best for all cases. We also provide
observations and discussion that highlight the complexity of emotion
classification in these datasets and test out the applicability of the models
to actual social media posts we collected and labeled.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation. (arXiv:2302.14785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14785">
<div class="article-summary-box-inner">
<span><p>A key feature of neural models is that they can produce semantic vector
representations of objects (texts, images, speech, etc.) ensuring that similar
objects are close to each other in the vector space. While much work has
focused on learning representations for other modalities, there are no aligned
cross-modal representations for text and knowledge base (KB) elements. One
challenge for learning such representations is the lack of parallel data, which
we use contrastive training on heuristics-based datasets and data augmentation
to overcome, training embedding models on (KB graph, text) pairs. On WebNLG, a
cleaner manually crafted dataset, we show that they learn aligned
representations suitable for retrieval. We then fine-tune on annotated data to
create EREDAT (Ensembled Representations for Evaluation of DAta-to-Text), a
similarity metric between English text and KB graphs. EREDAT outperforms or
matches state-of-the-art metrics in terms of correlation with human judgments
on WebNLG even though, unlike them, it does not require a reference text to
compare against.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Relation Embeddings: Representing the Relations between Discourse Segments in Social Media. (arXiv:2105.01306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01306">
<div class="article-summary-box-inner">
<span><p>Discourse relations are typically modeled as a discrete class that
characterizes the relation between segments of text (e.g. causal explanations,
expansions). However, such predefined discrete classes limits the universe of
potential relationships and their nuanced differences. Analogous to contextual
word embeddings, we propose representing discourse relations as points in high
dimensional continuous space. However, unlike words, discourse relations often
have no surface form (relations are between two segments, often with no word or
phrase in that gap) which presents a challenge for existing embedding
techniques. We present a novel method for automatically creating discourse
relation embeddings (DiscRE), addressing the embedding challenge through a
weakly supervised, multitask approach to learn diverse and nuanced relations
between discourse segments in social media. Results show DiscRE can: (1) obtain
the best performance on Twitter discourse relation classification task (macro
F1=0.76) (2) improve the state of the art in social media causality prediction
(from F1=.79 to .81), (3) perform beyond modern sentence and contextual word
embeddings at traditional discourse relation classification, and (4) capture
novel nuanced relations (e.g. relations semantically at the intersection of
causal explanations and counterfactuals).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06651">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction (KPE) automatically extracts phrases in a document that
provide a concise summary of the core content, which benefits downstream
information retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods
select candidate keyphrases based on the similarity between learned
representations of the candidates and the document. They suffer performance
degradation on long documents due to discrepancy between sequence lengths which
causes mismatch between representations of keyphrase candidates and the
document. In this work, we propose a novel unsupervised embedding-based KPE
approach, Masked Document Embedding Rank (MDERank), to address this problem by
leveraging a mask strategy and ranking candidates by the similarity between
embeddings of the source document and the masked document. We further develop a
KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised
contrastive learning method, which is more compatible to MDERank than vanilla
BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the
proposed MDERank outperforms state-of-the-art unsupervised KPE approach by
average 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and
overall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our
code is available at \url{https://github.com/LinhanZ/mderank}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Police Text Analysis: Topic Modeling and Spatial Relative Density Estimation. (arXiv:2202.04176v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04176">
<div class="article-summary-box-inner">
<span><p>We analyze a large corpus of police incident narrative documents in
understanding the spatial distribution of the topics. The motivation for doing
this is that police narratives in each incident report contains very
fine-grained information that is richer than the category that is manually
assigned by the police. Our approach is to split the corpus into topics using
two different unsupervised machine learning algorithms - Latent Dirichlet
Allocation and Non-negative Matrix Factorization. We validate the performance
of each learned topic model using model coherence. Then, using a k-nearest
neighbors density ratio estimation (kNN-DRE) approach that we propose, we
estimate the spatial density ratio per topic and use this for data discovery
and analysis of each topic, allowing for insights into the described incidents
at scale. We provide a qualitative assessment of each topic and highlight some
key benefits for using our kNN-DRE model for estimating spatial trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding The Robustness of Self-supervised Learning Through Topic Modeling. (arXiv:2203.03539v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03539">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has significantly improved the performance of many
NLP tasks. However, how can self-supervised learning discover useful
representations, and why is it better than traditional approaches such as
probabilistic models are still largely unknown. In this paper, we focus on the
context of topic modeling and highlight a key advantage of self-supervised
learning - when applied to data generated by topic models, self-supervised
learning can be oblivious to the specific model, and hence is less susceptible
to model misspecification. In particular, we prove that commonly used
self-supervised objectives based on reconstruction or contrastive samples can
both recover useful posterior information for general topic models.
Empirically, we show that the same objectives can perform on par with posterior
inference using the correct model, while outperforming posterior inference
using misspecified models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07893">
<div class="article-summary-box-inner">
<span><p>We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove private or guarded information from neural representations. Our
method uses matrix decomposition to project the input representations into
directions with reduced covariance with the guarded information rather than
maximal covariance as factorization methods normally use. We begin with linear
information removal and proceed to generalize our algorithm to the case of
nonlinear information removal using kernels. Our experiments demonstrate that
our algorithm retains better main task performance after removing the guarded
information compared to previous work. In addition, our experiments demonstrate
that we need a relatively small amount of guarded attribute data to remove
information about these attributes, which lowers the exposure to sensitive data
and is more suitable for low-resource scenarios. Code is available at
https://github.com/jasonshaoshun/SAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. (arXiv:2203.13474v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13474">
<div class="article-summary-box-inner">
<span><p>Program synthesis strives to generate a computer program as a solution to a
given problem specification, expressed with input-output examples or natural
language descriptions. The prevalence of large language models advances the
state-of-the-art for program synthesis, though limited training resources and
data impede open access to such models. To democratize this, we train and
release a family of large language models up to 16.1B parameters, called
CODEGEN, on natural language and programming language data, and open source the
training library JAXFORMER. We show the utility of the trained model by
demonstrating that it is competitive with the previous state-of-the-art on
zero-shot Python code generation on HumanEval. We further investigate the
multi-step paradigm for program synthesis, where a single program is factorized
into multiple prompts specifying subproblems. To this end, we construct an open
benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse
problem sets that are factorized into multi-turn prompts. Our analysis on MTPB
shows that the same intent provided to CODEGEN in multi-turn fashion
significantly improves program synthesis over that provided as a single turn.
We make the training library JAXFORMER and model checkpoints available as open
source contribution: https://github.com/salesforce/CodeGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UL2: Unifying Language Learning Paradigms. (arXiv:2205.05131v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05131">
<div class="article-summary-box-inner">
<span><p>Existing pre-trained models are generally geared towards a particular class
of problems. To date, there seems to be still no consensus on what the right
architecture and pre-training setup should be. This paper presents a unified
framework for pre-training models that are universally effective across
datasets and setups. We begin by disentangling architectural archetypes with
pre-training objectives -- two concepts that are commonly conflated. Next, we
present a generalized &amp; unified perspective for self-supervision in NLP and
show how different pre-training objectives can be cast as one another and how
interpolating between different objectives can be effective. We then propose
Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse
pre-training paradigms together. We furthermore introduce a notion of mode
switching, wherein downstream fine-tuning is associated with specific
pre-training schemes. We conduct extensive ablative experiments to compare
multiple pre-training objectives and find that our method pushes the
Pareto-frontier by outperforming T5 &amp; GPT-like models across multiple diverse
setups. By scaling our model up to 20B parameters, we achieve SOTA performance
on 50 well-established supervised finetuning based NLP tasks. Our model also
achieve strong results at in-context learning, outperforming 175B GPT-3 on
zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot
summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B
also works well with chain-of-thought prompting and reasoning, making it an
appealing choice for research into reasoning at a small to medium scale of 20B
parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,
achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release
Flax-based T5X checkpoints for the UL2 20B &amp; Flan-UL2 20B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ER-Test: Evaluating Explanation Regularization Methods for Language Models. (arXiv:2205.12542v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12542">
<div class="article-summary-box-inner">
<span><p>By explaining how humans would solve a given task, human rationales can
provide strong learning signal for neural language models (LMs). Explanation
regularization (ER) aims to improve LM generalization by pushing the LM's
machine rationales (Which input tokens did the LM focus on?) to align with
human rationales (Which input tokens would humans focus on?). Though prior
works primarily study ER via in-distribution (ID) evaluation,
out-of-distribution (OOD) generalization is often more critical in real-world
scenarios, yet ER's effect on OOD generalization has been underexplored. In
this paper, we introduce ER-Test, a framework for evaluating ER models' OOD
generalization along three dimensions: unseen dataset tests, contrast set
tests, and functional tests. Using ER-Test, we extensively analyze how ER
models' OOD generalization varies with different ER design choices. Across two
tasks and six datasets, ER-Test shows that ER has little impact on ID
performance but can yield large OOD performance gains. Also, we find that ER
can improve OOD performance even with limited rationale supervision. ER-Test's
results help demonstrate ER's utility and establish best practices for using ER
effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Networks and the Chomsky Hierarchy. (arXiv:2207.02098v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02098">
<div class="article-summary-box-inner">
<span><p>Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (20'910 models, 15 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never lead to
any non-trivial generalization, despite models having sufficient capacity to
fit the training data perfectly. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the effect of domain selection on automatic speech recognition performance: a case study on Bangladeshi Bangla. (arXiv:2210.12921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12921">
<div class="article-summary-box-inner">
<span><p>The performance of data-driven natural language processing systems is
contingent upon the quality of corpora. However, principal corpus design
criteria are often not identified and examined adequately, particularly in the
speech processing discipline. Speech corpora development requires additional
attention with regard to clean/noisy, read/spontaneous, multi-talker speech,
accents/dialects, etc. Domain selection is also a crucial decision point in
speech corpus development. In this study, we demonstrate the significance of
domain selection by assessing a state-of-the-art Bangla automatic speech
recognition (ASR) model on a novel multi-domain Bangladeshi Bangla ASR
evaluation benchmark - BanSpeech, which contains 7.2 hours of speech and 9802
utterances from 19 distinct domains. The ASR model has been trained with deep
convolutional neural network (CNN), layer normalization technique, and
Connectionist Temporal Classification (CTC) loss criterion on SUBAK.KO, a
mostly read speech corpus for the low-resource and morphologically rich
language Bangla. Experimental evaluation reveals the ASR model on SUBAK.KO
faces difficulty recognizing speech from domains with mostly spontaneous speech
and has a high number of out-of-vocabulary (OOV) words. The same ASR model, on
the other hand, performs better in read speech domains and contains fewer OOV
words. In addition, we report the outcomes of our experiments with layer
normalization, input feature extraction, number of convolutional layers, etc.,
and set a baseline on SUBAK.KO. The BanSpeech will be publicly available to
meet the need for a challenging evaluation benchmark for Bangla ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11596">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as GPT-3 and ChatGPT have recently
demonstrated impressive results across a wide range of tasks. LLMs are still
limited, however, in that they frequently fail at complex reasoning, their
reasoning processes are opaque, they are prone to 'hallucinate' facts, and
there are concerns about their underlying biases. Letting models verbalize
reasoning steps as natural language, a technique known as chain-of-thought
prompting, has recently been proposed as a way to address some of these issues.
Here we present the first release of ThoughtSource, a meta-dataset and software
library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to
improve future artificial intelligence systems by facilitating qualitative
understanding of CoTs, enabling empirical evaluations, and providing training
data. This first release of ThoughtSource integrates six scientific/medical,
three general-domain and five math word question answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04023">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn "prompt engineering" fashion. We also
release codebase for evaluation set extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-then-Seam: Towards Efficient Table-to-Text Generation. (arXiv:2302.05138v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05138">
<div class="article-summary-box-inner">
<span><p>Table-to-text generation aims at automatically generating text to help people
conveniently obtain salient information in tables. Recent works explicitly
decompose the generation process into content planning and surface generation
stages, employing two autoregressive networks for them respectively. However,
they are computationally expensive due to the non-parallelizable nature of
autoregressive decoding and the redundant parameters of two networks. In this
paper, we propose the first totally non-autoregressive table-to-text model
(Plan-then-Seam, PTS) that produces its outputs in parallel with one single
network. PTS firstly writes and calibrates one plan of the content to be
generated with a novel rethinking pointer predictor, and then takes the plan as
the context for seaming to decode the description. These two steps share
parameters and perform iteratively to capture token inter-dependency while
keeping parallel decoding. Experiments on two public benchmarks show that PTS
achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,
while maintaining as least comparable performance against strong two-stage
table-to-text competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divergence-Based Domain Transferability for Zero-Shot Classification. (arXiv:2302.05735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05735">
<div class="article-summary-box-inner">
<span><p>Transferring learned patterns from pretrained neural language models has been
shown to significantly improve effectiveness across a variety of language-based
tasks, meanwhile further tuning on intermediate tasks has been demonstrated to
provide additional performance benefits, provided the intermediate task is
sufficiently related to the target task. However, how to identify related tasks
is an open problem, and brute-force searching effective task combinations is
prohibitively expensive. Hence, the question arises, are we able to improve the
effectiveness and efficiency of tasks with no training examples through
selective fine-tuning? In this paper, we explore statistical measures that
approximate the divergence between domain representations as a means to
estimate whether tuning using one task pair will exhibit performance benefits
over tuning another. This estimation can then be used to reduce the number of
task pairs that need to be tested by eliminating pairs that are unlikely to
provide benefits. Through experimentation over 58 tasks and over 6,600 task
pair combinations, we demonstrate that statistical measures can distinguish
effective task pairs, and the resulting estimates can reduce end-to-end runtime
by up to 40%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11752">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a challenging task of natural language
processing (NLP) and computer vision (CV), attracting significant attention
from researchers. English is a resource-rich language that has witnessed
various developments in datasets and models for visual question answering.
Visual question answering in other languages also would be developed for
resources and models. In addition, there is no multilingual dataset targeting
the visual content of a particular country with its own objects and cultural
characteristics. To address the weakness, we provide the research community
with a benchmark dataset named EVJVQA, including 33,000+ pairs of
question-answer over three languages: Vietnamese, English, and Japanese, on
approximately 5,000 images taken from Vietnam for evaluating multilingual VQA
systems or models. EVJVQA is used as a benchmark dataset for the challenge of
multilingual visual question answering at the 9th Workshop on Vietnamese
Language and Speech Processing (VLSP 2022). This task attracted 62 participant
teams from various universities and organizations. In this article, we present
details of the organization of the challenge, an overview of the methods
employed by shared-task participants, and the results. The highest performances
are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The
multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained
vision model and mT5 for the pre-trained language model, a powerful pre-trained
language model based on the transformer architecture. EVJVQA is a challenging
dataset that motivates NLP and CV researchers to further explore the
multilingual models or systems for visual question answering systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness in Language Models Beyond English: Gaps and Challenges. (arXiv:2302.12578v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12578">
<div class="article-summary-box-inner">
<span><p>With language models becoming increasingly ubiquitous, it has become
essential to address their inequitable treatment of diverse demographic groups
and factors. Most research on evaluating and mitigating fairness harms has been
concentrated on English, while multilingual models and non-English languages
have received comparatively little attention. This paper presents a survey of
fairness in multilingual and non-English contexts, highlighting the
shortcomings of current research and the difficulties faced by methods designed
for English. We contend that the multitude of diverse cultures and languages
across the world makes it infeasible to achieve comprehensive coverage in terms
of constructing fairness datasets. Thus, the measurement and mitigation of
biases must evolve beyond the current dataset-driven practices that are
narrowly focused on specific dimensions and types of biases and, therefore,
impossible to scale across languages and cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary. (arXiv:2302.12746v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12746">
<div class="article-summary-box-inner">
<span><p>Dictionaries are one of the oldest and most used linguistic resources.
Building them is a complex task that, to the best of our knowledge, has yet to
be explored with generative Large Language Models (LLMs). We introduce the
"Spanish Built Factual Freectianary" (Spanish-BFF) as the first Spanish
AI-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We
also define future steps we aim to follow to improve this initial commitment to
the field, such as more additional languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Finetuning for Few-Shot Emotional Speech Recognition. (arXiv:2302.12921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12921">
<div class="article-summary-box-inner">
<span><p>Speech models have long been known to overfit individual speakers for many
classification tasks. This leads to poor generalization in settings where the
speakers are out-of-domain or out-of-distribution, as is common in production
environments. We view speaker adaptation as a few-shot learning problem and
propose investigating transfer learning approaches inspired by recent success
with pre-trained models in natural language tasks. We propose pre-finetuning
speech models on difficult tasks to distill knowledge into few-shot downstream
classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of
four multiclass emotional speech recognition corpora and evaluate our
pre-finetuned models through 33,600 few-shot fine-tuning trials on the
Emotional Speech Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatAug: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13007">
<div class="article-summary-box-inner">
<span><p>Text data augmentation is an effective strategy for overcoming the challenge
of limited sample sizes in many natural language processing (NLP) tasks. This
challenge is especially prominent in the few-shot learning scenario, where the
data in the target domain is generally much scarcer and of lowered quality. A
natural and widely-used strategy to mitigate such challenges is to perform data
augmentation on the training data to better capture the data invariance and
increase the sample size. However, current text data augmentation methods
either can not ensure the correct labeling of the generated data (lacking
faithfulness) or can not ensure sufficient diversity in the generated data
(lacking completeness), or both. Inspired by the recent success of large
language models, especially the development of ChatGPT, which demonstrated
improved language comprehension abilities, in this work, we propose a text data
augmentation approach based on ChatGPT (named ChatAug). ChatGPT is trained on
data with unparalleled linguistic richness and employs a reinforcement training
process with large-scale human feedback, which endows the model with affinity
to the naturalness of human language. Our text data augmentation approach
ChatAug rephrases each sentence in the training samples into multiple
conceptually similar but semantically different samples. The augmented samples
can then be used in downstream model training. Experiment results on few-shot
learning text classification tasks show the superior performance of the
proposed ChatAug approach over state-of-the-art text data augmentation methods
in terms of testing accuracy and distribution of the augmented samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13114">
<div class="article-summary-box-inner">
<span><p>Complex Query Answering (CQA) is an important and fundamental task for
knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and
robust solution to CQA. In the encoding process, most existing QE methods first
parse the logical query into an executable computational direct-acyclic graph
(DAG), then use neural networks to parameterize the operators, and finally,
recursively execute these neuralized operators. However, the
parameterization-and-execution paradigm may be potentially over-complicated, as
it can be structurally simplified by a single neural network encoder.
Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective
for encoding semantic graphs in related tasks. Motivated by this, we propose
sequential query encoding (SQE) as an alternative to encode queries for CQA.
Instead of parameterizing and executing the computational graph, SQE first uses
a search-based algorithm to linearize the computational graph to a sequence of
tokens and then uses a sequence encoder to compute its vector representation.
Then this vector representation is used as a query embedding to retrieve
answers from the embedding space according to similarity scores. Despite its
simplicity, SQE demonstrates state-of-the-art neural query encoding performance
on FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine
types of in-distribution queries. Further experiment shows that SQE also
demonstrates comparable knowledge inference capability on out-of-distribution
queries, whose query types are not observed during the training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Supporting Examples for In-Context Learning. (arXiv:2302.13539v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13539">
<div class="article-summary-box-inner">
<span><p>In-context learning is a new learning paradigm where a language model
observes a few examples and then straightly outputs the test input's
prediction. Previous works have shown that in-context learning is sensitive to
the provided examples and randomly sampled examples show significantly unstable
performance. In this paper, we propose to find ``supporting examples'' for
in-context learning: Given the training dataset, we need to select one
permutation of a few examples, which are informative for the task's in-context
learning and lead to superior performance. Although in traditional
gradient-based learning, e.g., fine-tuning, there are numerous methods to find
a ``coreset'' from the entire dataset, they are sub-optimal and not suitable
for this problem since in-context learning occurs in the language model's
inference without gradients or parameter updates. Additionally, the strong
dependence among in-context examples makes this problem an NP-hard
combinatorial optimization problem and enumerating all possible permutations is
infeasible. Hence we propose a two-stage method to tackle this challenge. First
we propose a novel metric to select informative examples based on the language
model's feedback, with a progressive filtering strategy. And then we propose a
diversity-guided beam search method to refine and evaluate the selected
examples, iteratively. The experimental results show our method significantly
outperforms a wide range of baselines, and further analyses show the
effectiveness of our method and shed light on the properties of supporting
examples and in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP). (arXiv:2302.13814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13814">
<div class="article-summary-box-inner">
<span><p>We study the performance of a commercially available large language model
(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.
To our knowledge, this is the first independent evaluation of ChatGPT. We found
that ChatGPT's performance changes dramatically based on the requirement to
show its work, failing 20% of the time when it provides work compared with 84%
when it does not. Further several factors about MWPs relating to the number of
unknowns and number of operations that lead to a higher probability of failure
when compared with the prior, specifically noting (across all experiments) that
the probability of failure increases linearly with the number of addition and
subtraction operations. We also have released the dataset of ChatGPT's
responses to the MWPs to support further work on the characterization of LLM
performance and present baseline machine learning models to predict if ChatGPT
can correctly answer an MWP. We have released a dataset comprised of ChatGPT's
responses to support further research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13939">
<div class="article-summary-box-inner">
<span><p>As the size of large language models continue to scale, so does the
computational resources required to run it. Spiking neural networks (SNNs) have
emerged as an energy-efficient approach to deep learning that leverage sparse
and event-driven activations to reduce the computational overhead associated
with model inference. While they have become competitive with non-spiking
models on many computer vision tasks, SNNs have also proven to be more
challenging to train. As a result, their performance lags behind modern deep
learning, and we are yet to see the effectiveness of SNNs in language
generation. In this paper, inspired by the RWKV language model, we successfully
implement `SpikeGPT', a generative language model with pure binary,
event-driven spiking activation units. We train the proposed model on three
model variants: 45M, 125M and 260M parameters. To the best of our knowledge,
this is 4x larger than any functional backprop-trained SNN to date. We achieve
this by modifying the transformer block to replace multi-head self attention to
reduce quadratic computational complexity to linear with increasing sequence
length. Input tokens are instead streamed in sequentially to our attention
mechanism (as with typical SNNs). Our preliminary experiments show that
SpikeGPT remains competitive with non-spiking models on tested benchmarks,
while maintaining 5x less energy consumption when processed on neuromorphic
hardware that can leverage sparse, event-driven activations. Our code
implementation is available at https://github.com/ridgerchu/SpikeGPT.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-01 23:13:21.092482966 UTC">2023-03-01 23:13:21 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>