<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-12-04T01:30:00Z">12-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections. (arXiv:2312.00027v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00027">
<div class="article-summary-box-inner">
<span><p>Recent developments in Large Language Models (LLMs) have manifested
significant advancements. To facilitate safeguards against malicious
exploitation, a body of research has concentrated on aligning LLMs with human
preferences and inhibiting their generation of inappropriate content.
Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal
amount of harmful data can easily unalign the target LLM. While being
effective, such fine-tuning-based unalignment approaches also have their own
limitations: (1) non-stealthiness, after fine-tuning, safety audits or
red-teaming can easily expose the potential weaknesses of the unaligned models,
thereby precluding their release/use. (2) non-persistence, the unaligned LLMs
can be easily repaired through re-alignment, i.e., fine-tuning again with
aligned data points. In this work, we show that it is possible to conduct
stealthy and persistent unalignment on large language models via backdoor
injections. We also provide a novel understanding on the relationship between
the backdoor persistence and the activation pattern and further provide
guidelines for potential trigger design. Through extensive experiments, we
demonstrate that our proposed stealthy and persistent unalignment can
successfully pass the safety evaluation while maintaining strong persistence
against re-alignment defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework. (arXiv:2312.00029v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00029">
<div class="article-summary-box-inner">
<span><p>Modern Large language models (LLMs) can still generate responses that may not
be aligned with human expectations or values. While many weight-based alignment
methods have been proposed, many of them still leave models vulnerable to
attacks when used on their own. To help mitigate this issue, we introduce
Bergeron, a framework designed to improve the robustness of LLMs against
adversarial attacks. Bergeron employs a two-tiered architecture. Here, a
secondary LLM serves as a simulated conscience that safeguards a primary LLM.
We do this by monitoring for and correcting potentially harmful text within
both the prompt inputs and the generated outputs of the primary LLM. Empirical
evaluation shows that Bergeron can improve the alignment and robustness of
several popular LLMs without costly fine-tuning. It aids both open-source and
black-box LLMs by complementing and reinforcing their existing alignment
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models. (arXiv:2312.00079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00079">
<div class="article-summary-box-inner">
<span><p>This paper explores advancements in high-fidelity personalized image
generation through the utilization of pre-trained text-to-image diffusion
models. While previous approaches have made significant strides in generating
versatile scenes based on text descriptions and a few input images, challenges
persist in maintaining the subject fidelity within the generated images. In
this work, we introduce an innovative algorithm named HiFi Tuner to enhance the
appearance preservation of objects during personalized image generation. Our
proposed method employs a parameter-efficient fine-tuning framework, comprising
a denoising process and a pivotal inversion process. Key enhancements include
the utilization of mask guidance, a novel parameter regularization technique,
and the incorporation of step-wise subject representations to elevate the
sample fidelity. Additionally, we propose a reference-guided generation
approach that leverages the pivotal inversion of a reference image to mitigate
unwanted subject variations and artifacts. We further extend our method to a
novel image editing task: substituting the subject in an image through textual
manipulations. Experimental evaluations conducted on the DreamBooth dataset
using the Stable Diffusion model showcase promising results. Fine-tuning solely
on textual embeddings improves CLIP-T score by 3.6 points and improves DINO
score by 9.6 points over Textual Inversion. When fine-tuning all parameters,
HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2
points over DreamBooth, establishing a new state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines. (arXiv:2312.00100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00100">
<div class="article-summary-box-inner">
<span><p>Rhetoric, both spoken and written, involves not only content but also style.
One common stylistic tool is $\textit{parallelism}$: the juxtaposition of
phrases which have the same sequence of linguistic ($\textit{e.g.}$,
phonological, syntactic, semantic) features. Despite the ubiquity of
parallelism, the field of natural language processing has seldom investigated
it, missing a chance to better understand the nature of the structure, meaning,
and intent that humans convey. To address this, we introduce the task of
$\textit{rhetorical parallelism detection}$. We construct a formal definition
of it; we provide one new Latin dataset and one adapted Chinese dataset for it;
we establish a family of metrics to evaluate performance on it; and, lastly, we
create baseline systems and novel sequence labeling schemes to capture it. On
our strictest metric, we attain $F_{1}$ scores of $0.40$ and $0.43$ on our
Latin and Chinese datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval. (arXiv:2312.00115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00115">
<div class="article-summary-box-inner">
<span><p>Existing long video retrieval systems are trained and tested in the
paragraph-to-video retrieval regime, where every long video is described by a
single long paragraph. This neglects the richness and variety of possible valid
descriptions of a video, which could be described in moment-by-moment detail,
or in a single phrase summary, or anything in between. To provide a more
thorough evaluation of the capabilities of long video retrieval systems, we
propose a pipeline that leverages state-of-the-art large language models to
carefully generate a diverse set of synthetic captions for long videos. We
validate this pipeline's fidelity via rigorous human inspection. We then
benchmark a representative set of video language models on these synthetic
captions using a few long video datasets, showing that they struggle with the
transformed data, especially the shortest captions. We also propose a
lightweight fine-tuning method, where we use a contrastive loss to learn a
hierarchical embedding loss based on the differing levels of information among
the various captions. Our method improves performance both on the downstream
paragraph-to-video retrieval task (+1.1% R@1 on ActivityNet), as well as for
the various long video retrieval metrics we compute using our synthetic data
(+3.6% R@1 for short descriptions on ActivityNet). For data access and other
details, please refer to our project website at
https://mgwillia.github.io/10k-words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigating News Narratives: A Media Bias Analysis Dataset. (arXiv:2312.00168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00168">
<div class="article-summary-box-inner">
<span><p>The proliferation of biased news narratives across various media platforms
has become a prominent challenge, influencing public opinion on critical topics
like politics, health, and climate change. This paper introduces the
"Navigating News Narratives: A Media Bias Analysis Dataset", a comprehensive
dataset to address the urgent need for tools to detect and analyze media bias.
This dataset encompasses a broad spectrum of biases, making it a unique and
valuable asset in the field of media studies and artificial intelligence. The
dataset is available at
https://figshare.com/articles/dataset/news-media-bias_data_<a href="/abs/json/2442212">json/2442212</a>2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression of end-to-end non-autoregressive image-to-speech system for low-resourced devices. (arXiv:2312.00174v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00174">
<div class="article-summary-box-inner">
<span><p>People with visual impairments have difficulty accessing touchscreen-enabled
personal computing devices like mobile phones and laptops. The image-to-speech
(ITS) systems can assist them in mitigating this problem, but their huge model
size makes it extremely hard to be deployed on low-resourced embedded devices.
In this paper, we aim to overcome this challenge by developing an efficient
endto-end neural architecture for generating audio from tiny segments of
display content on low-resource devices. We introduced a vision
transformers-based image encoder and utilized knowledge distillation to
compress the model from 6.1 million to 2.46 million parameters. Human and
automatic evaluation results show that our approach leads to a very minimal
drop in performance and can speed up the inference time by 22%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Concept Erasure via Kernelized Rate-Distortion Maximization. (arXiv:2312.00194v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00194">
<div class="article-summary-box-inner">
<span><p>Distributed representations provide a vector space that captures meaningful
relationships between data instances. The distributed nature of these
representations, however, entangles together multiple attributes or concepts of
data instances (e.g., the topic or sentiment of a text, characteristics of the
author (age, gender, etc), etc). Recent work has proposed the task of concept
erasure, in which rather than making a concept predictable, the goal is to
remove an attribute from distributed representations while retaining other
information from the original representation space as much as possible. In this
paper, we propose a new distance metric learning-based objective, the
Kernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure.
KRaM fits a transformation of representations to match a specified distance
measure (defined by a labeled concept to erase) using a modified
rate-distortion function. Specifically, KRaM's objective function aims to make
instances with similar concept labels dissimilar in the learned representation
space while retaining other information. We find that optimizing KRaM
effectively erases various types of concepts: categorical, continuous, and
vector-valued variables from data representations across diverse domains. We
also provide a theoretical analysis of several properties of KRaM's objective.
To assess the quality of the learned representations, we propose an alignment
score to evaluate their similarity with the original representation space.
Additionally, we conduct experiments to showcase KRaM's efficacy in various
settings, from erasing binary gender variables in word embeddings to
vector-valued variables in GPT-3 representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relevance-guided Neural Machine Translation. (arXiv:2312.00214v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00214">
<div class="article-summary-box-inner">
<span><p>With the advent of the Transformer architecture, Neural Machine Translation
(NMT) results have shown great improvement lately. However, results in
low-resource conditions still lag behind in both bilingual and multilingual
setups, due to the limited amount of available monolingual and/or parallel
data; hence, the need for methods addressing data scarcity in an efficient, and
explainable way, is eminent. We propose an explainability-based training
approach for NMT, applied in Unsupervised and Supervised model training, for
translation of three languages of varying resources, French, Gujarati, Kazakh,
to and from English. Our results show our method can be promising, particularly
when training in low-resource conditions, outperforming simple training
baselines; though the improvement is marginal, it sets the ground for further
exploration of the approach and the parameters, and its extension to other
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation. (arXiv:2312.00220v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00220">
<div class="article-summary-box-inner">
<span><p>Video topic segmentation unveils the coarse-grained semantic structure
underlying videos and is essential for other video understanding tasks. Given
the recent surge in multi-modal, relying solely on a single modality is
arguably insufficient. On the other hand, prior solutions for similar tasks
like video scene/shot segmentation cater to short videos with clear visual
shifts but falter for long videos with subtle changes, such as livestreams. In
this paper, we introduce a multi-modal video topic segmenter that utilizes both
video transcripts and frames, bolstered by a cross-modal attention mechanism.
Furthermore, we propose a dual-contrastive learning framework adhering to the
unsupervised domain adaptation paradigm, enhancing our model's adaptability to
longer, more semantically complex videos. Experiments on short and long video
corpora demonstrate that our proposed solution, significantly surpasses
baseline methods in terms of both accuracy and transferability, in both intra-
and cross-domain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mark My Words: Analyzing and Evaluating Language Model Watermarks. (arXiv:2312.00273v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00273">
<div class="article-summary-box-inner">
<span><p>The capabilities of large language models have grown significantly in recent
years and so too have concerns about their misuse. In this context, the ability
to distinguish machine-generated text from human-authored content becomes
important. Prior works have proposed numerous schemes to watermark text, which
would benefit from a systematic evaluation framework. This work focuses on text
watermarking techniques - as opposed to image watermarks - and proposes a
comprehensive benchmark for them under different tasks as well as practical
attacks. We focus on three main metrics: quality, size (e.g. the number of
tokens needed to detect a watermark), and tamper-resistance. Current
watermarking techniques are good enough to be deployed: Kirchenbauer et al. can
watermark Llama2-7B-chat with no perceivable loss in quality in under 100
tokens, and with good tamper-resistance to simple attacks, regardless of
temperature. We argue that watermark indistinguishability is too strong a
requirement: schemes that slightly modify logit distributions outperform their
indistinguishable counterparts with no noticeable loss in generation quality.
We publicly release our benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Attribute Control via Closed-Loop Disentanglement. (arXiv:2312.00277v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00277">
<div class="article-summary-box-inner">
<span><p>Changing an attribute of a text without changing the content usually requires
to first disentangle the text into irrelevant attributes and content
representations. After that, in the inference phase, the representation of one
attribute is tuned to a different value, expecting that the corresponding
attribute of the text can also be changed accordingly. The usual way of
disentanglement is to add some constraints on the latent space of an
encoder-decoder architecture, including adversarial-based constraints and
mutual-information-based constraints. However, the previous semi-supervised
processes of attribute change are usually not enough to guarantee the success
of attribute change and content preservation. In this paper, we propose a novel
approach to achieve a robust control of attributes while enhancing content
preservation. In this approach, we use a semi-supervised contrastive learning
method to encourage the disentanglement of attributes in latent spaces.
Differently from previous works, we re-disentangle the reconstructed sentence
and compare the re-disentangled latent space with the original latent space,
which makes a closed-loop disentanglement process. This also helps content
preservation. In addition, the contrastive learning method is also able to
replace the role of minimizing mutual information and adversarial training in
the disentanglement process, which alleviates the computation cost. We
conducted experiments on three text datasets, including the Yelp Service review
dataset, the Amazon Product review dataset, and the GoEmotions dataset. The
experimental results show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection. (arXiv:2312.00292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00292">
<div class="article-summary-box-inner">
<span><p>Deception is the intentional practice of twisting information. It is a
nuanced societal practice deeply intertwined with human societal evolution,
characterized by a multitude of facets. This research explores the problem of
deception through the lens of psychology, employing a framework that
categorizes deception into three forms: lies of omission, lies of commission,
and lies of influence. The primary focus of this study is specifically on
investigating only lies of omission. We propose a novel framework for deception
detection leveraging NLP techniques. We curated an annotated dataset of 876,784
samples by amalgamating a popular large-scale fake news dataset and scraped
news headlines from the Twitter handle of Times of India, a well-known Indian
news media house. Each sample has been labeled with four layers, namely: (i)
the type of omission (speculation, bias, distortion, sounds factual, and
opinion), (ii) colors of lies(black, white, etc), and (iii) the intention of
such lies (to influence, etc) (iv) topic of lies (political, educational,
religious, etc). We present a novel multi-task learning pipeline that leverages
the dataless merging of fine-tuned language models to address the deception
detection task mentioned earlier. Our proposed model achieved an F1 score of
0.87, demonstrating strong performance across all layers including the type,
color, intent, and topic aspects of deceptive content. Finally, our research
explores the relationship between lies of omission and propaganda techniques.
To accomplish this, we conducted an in-depth analysis, uncovering compelling
findings. For instance, our analysis revealed a significant correlation between
loaded language and opinion, shedding light on their interconnectedness. To
encourage further research in this field, we will be making the models and
dataset available with the MIT License, making it favorable for open-source
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PsyAttention: Psychological Attention Model for Personality Detection. (arXiv:2312.00293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00293">
<div class="article-summary-box-inner">
<span><p>Work on personality detection has tended to incorporate psychological
features from different personality models, such as BigFive and MBTI. There are
more than 900 psychological features, each of which is helpful for personality
detection. However, when used in combination, the application of different
calculation standards among these features may result in interference between
features calculated using distinct systems, thereby introducing noise and
reducing performance. This paper adapts different psychological models in the
proposed PsyAttention for personality detection, which can effectively encode
psychological features, reducing their number by 85%. In experiments on the
BigFive and MBTI models, PysAttention achieved average accuracy of 65.66% and
86.30%, respectively, outperforming state-of-the-art methods, indicating that
it is effective at encoding psychological features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agent-OM: Leveraging Large Language Models for Ontology Matching. (arXiv:2312.00326v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00326">
<div class="article-summary-box-inner">
<span><p>Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM-based agents
have become revolutionary in data engineering and have been applied creatively
in various domains, their potential for OM remains underexplored. This study
introduces a novel agent-powered LLM-based design paradigm for OM systems. With
thoughtful consideration of several specific challenges to leverage LLMs for
OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese
agents for retrieval and matching, with a set of simple prompt-based OM tools.
Our framework is implemented in a proof-of-concept system. Evaluations of three
Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM
systems show that our system can achieve very close results to the best
long-standing performance on simple OM tasks and significantly improve the
performance on complex and few-shot OM tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RTQ: Rethinking Video-language Understanding Based on Image-text Model. (arXiv:2312.00347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00347">
<div class="article-summary-box-inner">
<span><p>Recent advancements in video-language understanding have been established on
the foundation of image-text models, resulting in promising outcomes due to the
shared knowledge between images and videos. However, video-language
understanding presents unique challenges due to the inclusion of highly complex
semantic details, which result in information redundancy, temporal dependency,
and scene complexity. Current techniques have only partially tackled these
issues, and our quantitative analysis indicates that some of these methods are
complementary. In light of this, we propose a novel framework called RTQ
(Refine, Temporal model, and Query), which addresses these challenges
simultaneously. The approach involves refining redundant information within
frames, modeling temporal relations among frames, and querying task-specific
information from the videos. Remarkably, our model demonstrates outstanding
performance even in the absence of video-language pre-training, and the results
are comparable with or superior to those achieved by state-of-the-art
pre-training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in NLP. (arXiv:2312.00349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00349">
<div class="article-summary-box-inner">
<span><p>I propose a paradigm for scientific progress in NLP centered around
developing scalable, data-driven theories of linguistic structure. The idea is
to collect data in tightly scoped, carefully defined ways which allow for
exhaustive annotation of behavioral phenomena of interest, and then use machine
learning to construct explanatory theories of these phenomena which can form
building blocks for intelligible AI systems. After laying some conceptual
groundwork, I describe several investigations into data-driven theories of
shallow semantic structure using Question-Answer driven Semantic Role Labeling
(QA-SRL), a schema for annotating verbal predicate-argument relations using
highly constrained question-answer pairs. While this only scratches the surface
of the complex language behaviors of interest in AI, I outline principles for
data collection and theoretical modeling which can inform future scientific
progress. This note summarizes and draws heavily on my PhD thesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs. (arXiv:2312.00353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00353">
<div class="article-summary-box-inner">
<span><p>This paper examines the capacity of LLMs to reason with knowledge graphs
using their internal knowledge graph, i.e., the knowledge graph they learned
during pre-training. Two research questions are formulated to investigate the
accuracy of LLMs in recalling information from pre-training knowledge graphs
and their ability to infer knowledge graph relations from context. To address
these questions, we employ LLMs to perform four distinct knowledge graph
reasoning tasks. Furthermore, we identify two types of hallucinations that may
occur during knowledge reasoning with LLMs: content and ontology hallucination.
Our experimental results demonstrate that LLMs can successfully tackle both
simple and complex knowledge graph reasoning tasks from their own memory, as
well as infer from input context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-driven Real-time Retrieval in Web Search. (arXiv:2312.00372v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00372">
<div class="article-summary-box-inner">
<span><p>Information retrieval in real-time search presents unique challenges distinct
from those encountered in classical web search. These challenges are
particularly pronounced due to the rapid change of user search intent, which is
influenced by the occurrence and evolution of breaking news events, such as
earthquakes, elections, and wars. Previous dense retrieval methods, which
primarily focused on static semantic representation, lack the capacity to
capture immediate search intent, leading to inferior performance in retrieving
the most recent event-related documents in time-sensitive scenarios. To address
this issue, this paper expands the query with event information that represents
real-time search intent. The Event information is then integrated with the
query through a cross-attention mechanism, resulting in a time-context query
representation. We further enhance the model's capacity for event
representation through multi-task training. Since publicly available datasets
such as MS-MARCO do not contain any event information on the query side and
have few time-sensitive queries, we design an automatic data collection and
annotation pipeline to address this issue, which includes ModelZoo-based Coarse
Annotation and LLM-driven Fine Annotation processes. In addition, we share the
training tricks such as two-stage training and hard negative sampling. Finally,
we conduct a set of offline experiments on a million-scale production dataset
to evaluate our approach and deploy an A/B testing in a real online system to
verify the performance. Extensive experimental results demonstrate that our
proposed approach significantly outperforms existing state-of-the-art baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLLiE: Collaborative Training of Large Language Models in an Efficient Way. (arXiv:2312.00407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00407">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are increasingly pivotal in a wide range of
natural language processing tasks. Access to pre-trained models, courtesy of
the open-source community, has made it possible to adapt these models to
specific applications for enhanced performance. However, the substantial
resources required for training these models necessitate efficient solutions.
This paper introduces CoLLiE, an efficient library that facilitates
collaborative training of large language models using 3D parallelism,
parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion,
Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive
functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and
customization. CoLLiE has proven superior training efficiency in comparison
with prevalent solutions in pre-training and fine-tuning scenarios.
Furthermore, we provide an empirical evaluation of the correlation between
model size and GPU memory consumption under different optimization methods, as
well as an analysis of the throughput. Lastly, we carry out a comprehensive
comparison of various optimizers and PEFT methods within the instruction-tuning
context. CoLLiE is available at https://github.com/OpenLMLab/collie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?. (arXiv:2312.00413v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00413">
<div class="article-summary-box-inner">
<span><p>Programming language understanding and representation (a.k.a code
representation learning) has always been a hot and challenging task in software
engineering. It aims to apply deep learning techniques to produce numerical
representations of the source code features while preserving its semantics.
These representations can be used for facilitating subsequent code-related
tasks. The abstract syntax tree (AST), a fundamental code feature, illustrates
the syntactic information of the source code and has been widely used in code
representation learning. However, there is still a lack of systematic and
quantitative evaluation of how well AST-based code representation facilitates
subsequent code-related tasks. In this paper, we first conduct a comprehensive
empirical study to explore the effectiveness of the AST-based code
representation in facilitating follow-up code-related tasks. To do so, we
compare the performance of models trained with code token sequence (Token for
short) based code representation and AST-based code representation on three
popular types of code-related tasks. Surprisingly, the overall quantitative
statistical results demonstrate that models trained with AST-based code
representation consistently perform worse across all three tasks compared to
models trained with Token-based code representation. Our further quantitative
analysis reveals that models trained with AST-based code representation
outperform models trained with Token-based code representation in certain
subsets of samples across all three tasks. We also conduct comprehensive
experiments to evaluate and reveal the impact of the choice of AST
parsing/preprocessing/encoding methods on AST-based code representation and
subsequent code-related tasks. Our study provides future researchers with
detailed guidance on how to select solutions at each stage to fully exploit
AST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Japanese Tort-case Dataset for Rationale-supported Legal Judgment Prediction. (arXiv:2312.00480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00480">
<div class="article-summary-box-inner">
<span><p>This paper presents the first dataset for Japanese Legal Judgment Prediction
(LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort
prediction and its rationale extraction. The rationale extraction task
identifies the court's accepting arguments from alleged arguments by plaintiffs
and defendants, which is a novel task in the field. JTD is constructed based on
annotated 3,477 Japanese Civil Code judgments by 41 legal experts, resulting in
7,978 instances with 59,697 of their alleged arguments from the involved
parties. Our baseline experiments show the feasibility of the proposed two
tasks, and our error analysis by legal experts identifies sources of errors and
suggests future directions of the LJP research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization-based Data Augmentation for Document Classification. (arXiv:2312.00513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00513">
<div class="article-summary-box-inner">
<span><p>Despite the prevalence of pretrained language models in natural language
understanding tasks, understanding lengthy text such as document is still
challenging due to the data sparseness problem. Inspired by that humans develop
their ability of understanding lengthy text from reading shorter text, we
propose a simple yet effective summarization-based data augmentation, SUMMaug,
for document classification. We first obtain easy-to-learn examples for the
target document classification task by summarizing the input of the original
training examples, while optionally merging the original labels to conform to
the summarized input. We then use the generated pseudo examples to perform
curriculum learning. Experimental results on two datasets confirmed the
advantage of our method compared to existing baseline methods in terms of
robustness and accuracy. We release our code and data at
https://github.com/etsurin/summaug.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SurreyAI 2023 Submission for the Quality Estimation Shared Task. (arXiv:2312.00525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00525">
<div class="article-summary-box-inner">
<span><p>Quality Estimation (QE) systems are important in situations where it is
necessary to assess the quality of translations, but there is no reference
available. This paper describes the approach adopted by the SurreyAI team for
addressing the Sentence-Level Direct Assessment shared task in WMT23. The
proposed approach builds upon the TransQuest framework, exploring various
autoencoder pre-trained language models within the MonoTransQuest architecture
using single and ensemble settings. The autoencoder pre-trained language models
employed in the proposed systems are XLMV, InfoXLM-large, and XLMR-large. The
evaluation utilizes Spearman and Pearson correlation coefficients, assessing
the relationship between machine-predicted quality scores and human judgments
for 5 language pairs (English-Gujarati, English-Hindi, English-Marathi,
English-Tamil and English-Telugu). The MonoTQ-InfoXLM-large approach emerges as
a robust strategy, surpassing all other individual models proposed in this
study by significantly improving over the baseline for the majority of the
language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trained MT Metrics Learn to Cope with Machine-translated References. (arXiv:2312.00536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00536">
<div class="article-summary-box-inner">
<span><p>Neural metrics trained on human evaluations of MT tend to correlate well with
human judgments, but their behavior is not fully understood. In this paper, we
perform a controlled experiment and compare a baseline metric that has not been
trained on human evaluations (Prism) to a trained version of the same metric
(Prism+FT). Surprisingly, we find that Prism+FT becomes more robust to
machine-translated references, which are a notorious problem in MT evaluation.
This suggests that the effects of metric training go beyond the intended effect
of improving overall correlation with human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs. (arXiv:2312.00552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00552">
<div class="article-summary-box-inner">
<span><p>Unsupervised relation extraction (URE) aims to extract relations between
named entities from raw text without requiring manual annotations or
pre-existing knowledge bases. In recent studies of URE, researchers put a
notable emphasis on contrastive learning strategies for acquiring relation
representations. However, these studies often overlook two important aspects:
the inclusion of diverse positive pairs for contrastive learning and the
exploration of appropriate loss functions. In this paper, we propose AugURE
with both within-sentence pairs augmentation and augmentation through
cross-sentence pairs extraction to increase the diversity of positive pairs and
strengthen the discriminative power of contrastive learning. We also identify
the limitation of noise-contrastive estimation (NCE) loss for relation
representation learning and propose to apply margin loss for sentence pairs.
Experiments on NYT-FB and TACRED datasets demonstrate that the proposed
relation representation learning and a simple K-Means clustering achieves
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?. (arXiv:2312.00554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00554">
<div class="article-summary-box-inner">
<span><p>The evolution of legal datasets and the advent of large language models
(LLMs) have significantly transformed the legal field, particularly in the
generation of case judgment summaries. However, a critical concern arises
regarding the potential biases embedded within these summaries. This study
scrutinizes the biases present in case judgment summaries produced by legal
datasets and large language models. The research aims to analyze the impact of
biases on legal decision making. By interrogating the accuracy, fairness, and
implications of biases in these summaries, this study contributes to a better
understanding of the role of technology in legal contexts and the implications
for justice systems worldwide. In this study, we investigate biases wrt
Gender-related keywords, Race-related keywords, Keywords related to crime
against women, Country names and religious keywords. The study shows
interesting evidences of biases in the outputs generated by the large language
models and pre-trained abstractive summarization models. The reasoning behind
these biases needs further studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanatory Argument Extraction of Correct Answers in Resident Medical Exams. (arXiv:2312.00567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00567">
<div class="article-summary-box-inner">
<span><p>Developing the required technology to assist medical experts in their
everyday activities is currently a hot topic in the Artificial Intelligence
research field. Thus, a number of large language models (LLMs) and automated
benchmarks have recently been proposed with the aim of facilitating information
extraction in Evidence-Based Medicine (EBM) using natural language as a tool
for mediating in human-AI interaction. The most representative benchmarks are
limited to either multiple-choice or long-form answers and are available only
in English. In order to address these shortcomings, in this paper we present a
new dataset which, unlike previous work: (i) includes not only explanatory
arguments for the correct answer, but also arguments to reason why the
incorrect answers are not correct; (ii) the explanations are written originally
by medical doctors to answer questions from the Spanish Residency Medical
Exams. Furthermore, this new benchmark allows us to setup a novel extractive
task which consists of identifying the explanation of the correct answer
written by medical doctors. An additional benefit of our setting is that we can
leverage the extractive QA paradigm to automatically evaluate performance of
LLMs without resorting to costly manual evaluation by medical experts.
Comprehensive experimentation with language models for Spanish shows that
sometimes multilingual models fare better than monolingual ones, even
outperforming models which have been adapted to the medical domain.
Furthermore, results across the monolingual models are mixed, with supposedly
smaller and inferior models performing competitively. In any case, the obtained
results show that our novel dataset and approach can be an effective technique
to help medical practitioners in identifying relevant evidence-based
explanations for medical questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction-tuning Aligns LLMs to the Human Brain. (arXiv:2312.00575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00575">
<div class="article-summary-box-inner">
<span><p>Instruction-tuning is a widely adopted method of finetuning that enables
large language models (LLMs) to generate output that more closely resembles
human responses to natural language queries, in many cases leading to
human-level performance on diverse testbeds. However, it remains unclear
whether instruction-tuning truly makes LLMs more similar to how humans process
language. We investigate the effect of instruction-tuning on LLM-human
similarity in two ways: (1) brain alignment, the similarity of LLM internal
representations to neural activity in the human language system, and (2)
behavioral alignment, the similarity of LLM and human behavior on a reading
task. We assess 25 vanilla and instruction-tuned LLMs across three datasets
involving humans reading naturalistic stories and sentences. We discover that
instruction-tuning generally enhances brain alignment by an average of 6%, but
does not have a similar effect on behavioral alignment. To identify the factors
underlying LLM-brain alignment, we compute correlations between the brain
alignment of LLMs and various model properties, such as model size, various
problem-solving abilities, and performance on tasks requiring world knowledge
spanning various domains. Notably, we find a strong positive correlation
between brain alignment and model size (r = 0.95), as well as performance on
tasks requiring world knowledge (r = 0.81). Our results demonstrate that
instruction-tuning LLMs improves both world knowledge representations and brain
alignment, suggesting that mechanisms that encode world knowledge in LLMs also
improve representational alignment to the human brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Ethics of Automating Legal Actors. (arXiv:2312.00584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00584">
<div class="article-summary-box-inner">
<span><p>The introduction of large public legal datasets has brought about a
renaissance in legal NLP. Many of these datasets are comprised of legal
judgements - the product of judges deciding cases. This fact, together with the
way machine learning works, means that several legal NLP models are models of
judges. While some have argued for the automation of judges, in this position
piece, we argue that automating the role of the judge raises difficult ethical
challenges, in particular for common law legal systems. Our argument follows
from the social role of the judge in actively shaping the law, rather than
merely applying it. Since current NLP models come nowhere close to having the
facilities necessary for this task, they should not be used to automate judges.
Furthermore, even in the case the models could achieve human-level
capabilities, there would still be remaining ethical concerns inherent in the
automation of the legal process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonparametric Variational Regularisation of Pretrained Transformers. (arXiv:2312.00662v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00662">
<div class="article-summary-box-inner">
<span><p>The current paradigm of large-scale pre-training and fine-tuning Transformer
large language models has lead to significant improvements across the board in
natural language processing. However, such large models are susceptible to
overfitting to their training data, and as a result the models perform poorly
when the domain changes. Also, due to the model's scale, the cost of
fine-tuning the model to the new domain is large. Nonparametric Variational
Information Bottleneck (NVIB) has been proposed as a regulariser for training
cross-attention in Transformers, potentially addressing the overfitting
problem. We extend the NVIB framework to replace all types of attention
functions in Transformers, and show that existing pretrained Transformers can
be reinterpreted as Nonparametric Variational (NV) models using a proposed
identity initialisation. We then show that changing the initialisation
introduces a novel, information-theoretic post-training regularisation in the
attention mechanism, which improves out-of-domain generalisation without any
training. This success supports the hypothesis that pretrained Transformers are
implicitly NV Bayesian models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Efficiency Spectrum of Large Language Models: An Algorithmic Survey. (arXiv:2312.00678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00678">
<div class="article-summary-box-inner">
<span><p>The rapid growth of Large Language Models (LLMs) has been a driving force in
transforming various domains, reshaping the artificial general intelligence
landscape. However, the increasing computational and memory demands of these
models present substantial challenges, hindering both academic research and
practical applications. To address these issues, a wide array of methods,
including both algorithmic and hardware solutions, have been developed to
enhance the efficiency of LLMs. This survey delivers a comprehensive review of
algorithmic advancements aimed at improving LLM efficiency. Unlike other
surveys that typically focus on specific areas such as training or model
compression, this paper examines the multi-faceted dimensions of efficiency
essential for the end-to-end algorithmic development of LLMs. Specifically, it
covers various topics related to efficiency, including scaling laws, data
utilization, architectural innovations, training and tuning strategies, and
inference techniques. This paper aims to serve as a valuable resource for
researchers and practitioners, laying the groundwork for future innovations in
this critical research area. Our repository of relevant references is
maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized word senses: from attention to compositionality. (arXiv:2312.00680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00680">
<div class="article-summary-box-inner">
<span><p>The neural architectures of language models are becoming increasingly
complex, especially that of Transformers, based on the attention mechanism.
Although their application to numerous natural language processing tasks has
proven to be very fruitful, they continue to be models with little or no
interpretability and explainability. One of the tasks for which they are best
suited is the encoding of the contextual sense of words using contextualized
embeddings. In this paper we propose a transparent, interpretable, and
linguistically motivated strategy for encoding the contextual sense of words by
modeling semantic compositionality. Particular attention is given to dependency
relations and semantic notions such as selection preferences and paradigmatic
classes. A partial implementation of the proposed model is carried out and
compared with Transformer-based architectures for a given semantic task, namely
the similarity calculation of word senses in context. The results obtained show
that it is possible to be competitive with linguistically motivated models
instead of using the black boxes underlying complex neural architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach. (arXiv:2312.00688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00688">
<div class="article-summary-box-inner">
<span><p>Guided by grammatical structure, words compose to form sentences, and guided
by discourse structure, sentences compose to form dialogues and documents. The
compositional aspect of sentence and discourse units is often overlooked by
machine learning algorithms. A recent initiative called Quantum Natural
Language Processing (QNLP) learns word meanings as points in a Hilbert space
and acts on them via a translation of grammatical structure into Parametrised
Quantum Circuits (PQCs). Previous work extended the QNLP translation to
discourse structure using points in a closure of Hilbert spaces. In this paper,
we evaluate this translation on a Winograd-style pronoun resolution task. We
train a Variational Quantum Classifier (VQC) for binary classification and
implement an end-to-end pronoun resolution system. The simulations executed on
IBMQ software converged with an F1 score of 87.20%. The model outperformed two
out of three classical coreference resolution systems and neared
state-of-the-art SpanBERT. A mixed quantum-classical model yet improved these
results with an F1 score increase of around 6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeaLLMs -- Large Language Models for Southeast Asia. (arXiv:2312.00738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00738">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable achievements of large language models (LLMs) in
various tasks, there remains a linguistic bias that favors high-resource
languages, such as English, often at the expense of low-resource and regional
languages. To address this imbalance, we introduce SeaLLMs, an innovative
series of language models that specifically focuses on Southeast Asian (SEA)
languages. SeaLLMs are built upon the Llama-2 model and further advanced
through continued pre-training with an extended vocabulary, specialized
instruction and alignment tuning to better capture the intricacies of regional
languages. This allows them to respect and reflect local cultural norms,
customs, stylistic preferences, and legal considerations. Our comprehensive
evaluation demonstrates that SeaLLM-13b models exhibit superior performance
across a wide spectrum of linguistic tasks and assistant-style
instruction-following capabilities relative to comparable open-source models.
Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai,
Khmer, Lao, and Burmese, by large margins while remaining lightweight and
cost-effective to operate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals. (arXiv:2312.00751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.00751">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved remarkable success in a wide range of natural
language processing and computer vision applications. However, the
representation capacity of a deep transformer model is degraded due to the
over-smoothing issue in which the token representations become identical when
the model's depth grows. In this work, we show that self-attention layers in
transformers minimize a functional which promotes smoothness, thereby causing
token uniformity. We then propose a novel regularizer that penalizes the norm
of the difference between the smooth output tokens from self-attention and the
input tokens to preserve the fidelity of the tokens. Minimizing the resulting
regularized energy functional, we derive the Neural Transformer with a
Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models
that can mitigate the over-smoothing issue. We empirically demonstrate the
advantages of NeuTRENO over the baseline transformers and state-of-the-art
methods in reducing the over-smoothing of token representations on various
practical tasks, including object classification, image segmentation, and
language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H_eval: A new hybrid evaluation metric for automatic speech recognition tasks. (arXiv:2211.01722v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01722">
<div class="article-summary-box-inner">
<span><p>Many studies have examined the shortcomings of word error rate (WER) as an
evaluation metric for automatic speech recognition (ASR) systems. Since WER
considers only literal word-level correctness, new evaluation metrics based on
semantic similarity such as semantic distance (SD) and BERTScore have been
developed. However, we found that these metrics have their own limitations,
such as a tendency to overly prioritise keywords. We propose H_eval, a new
hybrid evaluation metric for ASR systems that considers both semantic
correctness and error rate and performs significantly well in scenarios where
WER and SD perform poorly. Due to lighter computation compared to BERTScore, it
offers 49 times reduction in metric computation time. Furthermore, we show that
H_eval correlates strongly with downstream NLP tasks. Also, to reduce the
metric calculation time, we built multiple fast and lightweight models using
distillation techniques
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extensible Prompts for Language Models on Zero-shot Language Style Customization. (arXiv:2212.00616v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00616">
<div class="article-summary-box-inner">
<span><p>We propose eXtensible Prompt (X-Prompt) for prompting a large language model
(LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL
but also an extensible vocabulary of imaginary words. Registering new imaginary
words allows us to instruct the LLM to comprehend concepts that are difficult
to describe with NL words, thereby making a prompt more descriptive. Also,
these imaginary words are designed to be out-of-distribution (OOD) robust so
that they can be (re)used like NL words in various prompts, distinguishing
X-Prompt from soft prompt that is for fitting in-distribution data. We propose
context-augmented learning (CAL) to learn imaginary words for general
usability, enabling them to work properly in OOD (unseen) prompts. We
experiment X-Prompt for zero-shot language style customization as a case study.
The promising results of X-Prompt demonstrate its potential to facilitate
advanced interaction beyond the natural language interface, bridging the
communication gap between humans and LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations. (arXiv:2303.01261v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01261">
<div class="article-summary-box-inner">
<span><p>We present ParrotTTS, a modularized text-to-speech synthesis model leveraging
disentangled self-supervised speech representations. It can train a
multi-speaker variant effectively using transcripts from a single speaker.
ParrotTTS adapts to a new language in low resource setup and generalizes to
languages not seen while training the self-supervised backbone. Moreover,
without training on bilingual or parallel examples, ParrotTTS can transfer
voices across languages while preserving the speaker specific characteristics,
e.g., synthesizing fluent Hindi speech using a French speaker's voice and
accent. We present extensive results in monolingual and multi-lingual
scenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models
using only a fraction of paired data as latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10868">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) become popular, there emerged an important
trend of using multimodality to augment the LLMs' generation ability, which
enables LLMs to better interact with the world. However, there lacks a unified
perception of at which stage and how to incorporate different modalities. In
this survey, we review methods that assist and augment generative models by
retrieving multimodal knowledge, whose formats range from images, codes,
tables, graphs, to audio. Such methods offer a promising solution to important
concerns such as factuality, reasoning, interpretability, and robustness. By
providing an in-depth review, this survey is expected to provide scholars with
a deeper understanding of the methods' applications and encourage them to adapt
existing techniques to the fast-growing field of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06767">
<div class="article-summary-box-inner">
<span><p>Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10703">
<div class="article-summary-box-inner">
<span><p>Multi-step reasoning ability is fundamental to many natural language tasks,
yet it is unclear what constitutes a good reasoning chain and how to evaluate
them. Most existing methods focus solely on whether the reasoning chain leads
to the correct conclusion, but this answer-oriented view may confound reasoning
quality with other spurious shortcuts to predict the answer. To bridge this
gap, we evaluate reasoning chains by viewing them as informal proofs that
derive the final answer. Specifically, we propose ReCEval (Reasoning Chain
Evaluation), a framework that evaluates reasoning chains via two key
properties: (1) correctness, i.e., each step makes a valid inference based on
information contained within the step, preceding steps, and input context, and
(2) informativeness, i.e., each step provides new information that is helpful
towards deriving the generated answer. We evaluate these properties by
developing metrics using natural language inference models and V-Information.
On multiple datasets, we show that ReCEval effectively identifies various error
types and yields notable improvements compared to prior methods. We analyze the
impact of step boundaries, and previous steps on evaluating correctness and
demonstrate that our informativeness metric captures the expected flow of
information in high-quality reasoning chains. Finally, we show that scoring
reasoning chains based on ReCEval improves downstream task performance. Our
code is publicly available at: https://github.com/archiki/ReCEval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03977">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive models have been widely studied in the Complete
Information Scenario (CIS), in which the input has complete information of
corresponding output. However, their explorations in the Incomplete Information
Scenario (IIS) are extremely limited. Our analyses reveal that the IIS's
incomplete input information will augment the inherent limitations of existing
non-autoregressive models trained under Maximum Likelihood Estimation. In this
paper, we propose for the IIS an Adversarial Non-autoregressive Transformer
(ANT) which has two features: 1) Position-Aware Self-Modulation to provide more
reasonable hidden representations, and 2) Dependency Feed Forward Network to
strengthen its capacity in dependency modeling. We compare ANT with other
mainstream models in the IIS and demonstrate that ANT can achieve comparable
performance with much fewer decoding iterations. Furthermore, we show its great
potential in various applications like latent interpolation and semi-supervised
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs. (arXiv:2305.12191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12191">
<div class="article-summary-box-inner">
<span><p>A major concern in using deep learning based generative models for
document-grounded dialogs is the potential generation of responses that are not
\textit{faithful} to the underlying document. Existing automated metrics used
for evaluating the faithfulness of response with respect to the grounding
document measure the degree of similarity between the generated response and
the document's content. However, these automated metrics are far from being
well aligned with human judgments. Therefore, to improve the measurement of
faithfulness, we propose a new metric that utilizes (Conditional) Point-wise
Mutual Information (PMI) between the generated response and the source
document, conditioned on the dialogue. PMI quantifies the extent to which the
document influences the generated response -- with a higher PMI indicating a
more faithful response. We build upon this idea to create a new decoding
technique that incorporates PMI into the response generation process to predict
more faithful responses. Our experiments on the BEGIN benchmark demonstrate an
improved correlation of our metric with human evaluation. We also show that our
decoding technique is effective in generating more faithful responses when
compared to standard decoding techniques on a set of publicly available
document-grounded dialog datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents. (arXiv:2305.14772v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14772">
<div class="article-summary-box-inner">
<span><p>Many real-world applications (e.g., note taking, search) require extracting a
sentence or paragraph from a document and showing that snippet to a human
outside of the source document. Yet, users may find snippets difficult to
understand as they lack context from the original document. In this work, we
use language models to rewrite snippets from scientific documents to be read on
their own. First, we define the requirements and challenges for this
user-facing decontextualization task, such as clarifying where edits occur and
handling references to other documents. Second, we propose a framework that
decomposes the task into three stages: question generation, question answering,
and rewriting. Using this framework, we collect gold decontextualizations from
experienced scientific article readers. We then conduct a range of experiments
across state-of-the-art commercial and open-source language models to identify
how to best provide missing-but-relevant information to models for our task.
Finally, we develop QaDecontext, a simple prompting strategy inspired by our
framework that improves over end-to-end prompting. We conclude with analysis
that finds, while rewriting is easy, question generation and answering remain
challenging for today's models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Conceptual Representation Require Embodiment? Insights From Large Language Models. (arXiv:2305.19103v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19103">
<div class="article-summary-box-inner">
<span><p>To what extent can language alone give rise to complex concepts, or is
embodied experience essential? Recent advancements in large language models
(LLMs) offer fresh perspectives on this question. Although LLMs are trained on
restricted modalities, they exhibit human-like performance in diverse
psychological tasks. Our study compared representations of 4,442 lexical
concepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple
dimensions, including five key domains: emotion, salience, mental
visualization, sensory, and motor experience. We identify two main findings: 1)
Both models strongly align with human representations in non-sensorimotor
domains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;
2) GPT-4's gains are associated with its additional visual learning, which also
appears to benefit related dimensions like haptics and imageability. These
results highlight the limitations of language in isolation, and that the
integration of diverse modalities of inputs leads to a more human-like
conceptual representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03438">
<div class="article-summary-box-inner">
<span><p>Large language models of code (Code-LLMs) have recently brought tremendous
advances to code completion, a fundamental feature of programming assistance
and code intelligence. However, most existing works ignore the possible
presence of bugs in the code context for generation, which are inevitable in
software development. Therefore, we introduce and study the buggy-code
completion problem, inspired by the realistic scenario of real-time code
suggestion where the code context contains potential bugs -- anti-patterns that
can become bugs in the completed program. To systematically study the task, we
introduce two datasets: one with synthetic bugs derived from semantics-altering
operator changes (buggy-HumanEval) and one with realistic bugs derived from
user submissions to coding problems (buggy-FixEval). We find that the presence
of potential bugs significantly degrades the generation performance of the
high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO
on test cases of buggy-HumanEval drop more than 50% given a single potential
bug in the context. Finally, we investigate several post-hoc methods for
mitigating the adverse effect of potential bugs and find that there remains a
significant gap in post-mitigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports. (arXiv:2308.09193v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09193">
<div class="article-summary-box-inner">
<span><p>Bug reports are an essential aspect of software development, and it is
crucial to identify and resolve them quickly to ensure the consistent
functioning of software systems. Retrieving similar bug reports from an
existing database can help reduce the time and effort required to resolve bugs.
In this paper, we compared the effectiveness of semantic textual similarity
methods for retrieving similar bug reports based on a similarity score. We
explored several embedding models such as TF-IDF (Baseline), FastText, Gensim,
BERT, and ADA. We used the Software Defects Data containing bug reports for
various software projects to evaluate the performance of these models. Our
experimental results showed that BERT generally outperformed the rest of the
models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our
study provides insights into the effectiveness of different embedding methods
for retrieving similar bug reports and highlights the impact of selecting the
appropriate one for this task. Our code is available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16911">
<div class="article-summary-box-inner">
<span><p>The unprecedented advancements in Large Language Models (LLMs) have shown a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, enabling LLMs to understand point clouds and offering a new
avenue beyond 2D visual data. PointLLM understands colored object point clouds
with human instructions and generates contextually appropriate responses,
illustrating its grasp of point clouds and common sense. Specifically, it
leverages a point cloud encoder with a powerful LLM to effectively fuse
geometric, appearance, and linguistic information. We collect a novel dataset
comprising 660K simple and 70K complex point-text instruction pairs to enable a
two-stage training strategy: aligning latent spaces and subsequently
instruction-tuning the unified model. To rigorously evaluate the perceptual and
generalization capabilities of PointLLM, we establish two benchmarks:
Generative 3D Object Classification and 3D Object Captioning, assessed through
three different methods, including human evaluation, GPT-4/ChatGPT evaluation,
and traditional metrics. Experimental results reveal PointLLM's superior
performance over existing 2D and 3D baselines, with a notable achievement in
human-evaluated object captioning tasks where it surpasses human annotators in
over 50% of the samples. Codes, datasets, and benchmarks are available at
https://github.com/OpenRobotLab/PointLLM .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00267">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences. However,
gathering high-quality human preference labels can be a time-consuming and
expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al.,
offers a promising alternative that leverages a powerful off-the-shelf LLM to
generate preferences in lieu of human annotators. Across the tasks of
summarization, helpful dialogue generation, and harmless dialogue generation,
RLAIF achieves comparable or superior performance to RLHF, as rated by human
evaluators. Furthermore, RLAIF demonstrates the ability to outperform a
supervised fine-tuned baseline even when the LLM preference labeler is the same
size as the policy. In another experiment, directly prompting the LLM for
reward scores achieves superior performance to the canonical RLAIF setup, where
LLM preference labels are first distilled into a reward model. Finally, we
conduct extensive studies on techniques for generating aligned AI preferences.
Our results suggest that RLAIF can achieve human-level performance, offering a
potential solution to the scalability limitations of RLHF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuantEase: Optimization-based Quantization for Language Models. (arXiv:2309.01885v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01885">
<div class="article-summary-box-inner">
<span><p>With the rising popularity of Large Language Models (LLMs), there has been an
increasing interest in compression techniques that enable their efficient
deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs.
Drawing from recent advances, our work introduces QuantEase, a layer-wise
quantization framework where individual layers undergo separate quantization.
The problem is framed as a discrete-structured non-convex optimization,
prompting the development of algorithms rooted in Coordinate Descent (CD)
techniques. These CD-based methods provide high-quality solutions to the
complex non-convex layer-wise quantization problems. Notably, our CD-based
approach features straightforward updates, relying solely on matrix and vector
operations, circumventing the need for matrix inversion or decomposition. We
also explore an outlier-aware variant of our approach, allowing for retaining
significant weights (outliers) with complete precision. Our proposal attains
state-of-the-art performance in terms of perplexity and zero-shot accuracy in
empirical evaluations across various LLMs and datasets, with relative
improvements up to 15% over methods such as GPTQ. Leveraging careful linear
algebra optimizations, QuantEase can quantize models like Falcon-180B on a
single NVIDIA A100 GPU in $\sim$3 hours. Particularly noteworthy is our
outlier-aware algorithm's capability to achieve near or sub-3-bit quantization
of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform
quantization or grouping techniques, improving upon methods such as SpQR by up
to two times in terms of perplexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16770">
<div class="article-summary-box-inner">
<span><p>Recent advances in machine learning and deep learning have led to the
widespread use of Conversational AI in many practical applications. However, it
is still very challenging to leverage auxiliary information that can provide
conversational context or personalized tuning to improve the quality of
conversations. For example, there has only been limited research on using an
individuals persona information to improve conversation quality, and even
state-of-the-art conversational AI techniques are unable to effectively
leverage signals from heterogeneous sources of auxiliary data, such as
multi-modal interaction data, demographics, SDOH data, etc. In this paper, we
present a novel Persona-Coded Poly-Encoder method that leverages persona
information in a multi-stream encoding scheme to improve the quality of
response generation for conversations. To show the efficacy of the proposed
method, we evaluate our method on two different persona-based conversational
datasets, and compared against two state-of-the-art methods. Our experimental
results and analysis demonstrate that our method can improve conversation
quality over the baseline method Poly-Encoder by 3.32% and 2.94% in terms of
BLEU score and HR@1, respectively. More significantly, our method offers a path
to better utilization of multi-modal data in conversational tasks. Lastly, our
study outlines several challenges and future research directions for advancing
personalized conversational AI technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning. (arXiv:2310.00141v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00141">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) models are typically trained on large
datasets of transcribed speech. As language evolves and new terms come into
use, these models can become outdated and stale. In the context of models
trained on the server but deployed on edge devices, errors may result from the
mismatch between server training data and actual on-device usage. In this work,
we seek to continually learn from on-device user corrections through Federated
Learning (FL) to address this issue. We explore techniques to target fresh
terms that the model has not previously encountered, learn long-tail words, and
mitigate catastrophic forgetting. In experimental evaluations, we find that the
proposed techniques improve model recognition of fresh terms, while preserving
quality on the overall language distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning. (arXiv:2310.10083v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10083">
<div class="article-summary-box-inner">
<span><p>In the ongoing wave of impact driven by large language models (LLMs) like
ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial
research frontier. Since mainstream LLMs tend to be designed for
general-purpose applications, constructing a medical LLM through domain
adaptation is a huge challenge. While instruction-tuning is used to fine-tune
some LLMs, its precise roles in domain adaptation remain unknown. Here we show
the contribution of LoRA-based instruction-tuning to performance in Japanese
medical question-answering tasks. In doing so, we employ a multifaceted
evaluation for multiple-choice questions, including scoring based on "Exact
match" and "Gestalt distance" in addition to the conventional accuracy. Our
findings suggest that LoRA-based instruction-tuning can partially incorporate
domain-specific knowledge into LLMs, with larger models demonstrating more
pronounced effects. Furthermore, our results underscore the potential of
adapting English-centric models for Japanese applications in domain adaptation,
while also highlighting the persisting limitations of Japanese-centric models.
This initiative represents a pioneering effort in enabling medical institutions
to fine-tune and operate models without relying on external services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Llemma: An Open Language Model For Mathematics. (arXiv:2310.10631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10631">
<div class="article-summary-box-inner">
<span><p>We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10638">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs'performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models. (arXiv:2311.03687v2 [cs.PF] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.03687">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have seen great advance in both academia and
industry, and their popularity results in numerous open-source frameworks and
techniques in accelerating LLM pre-training, fine-tuning, and inference.
Training and deploying LLMs are expensive as it requires considerable computing
resources and memory, hence many efficient approaches have been developed for
improving system pipelines as well as operators. However, the runtime
performance can vary significantly across hardware and software stacks, which
makes it difficult to choose the best configuration. In this work, we aim to
benchmark the performance from both macro and micro perspectives. First, we
benchmark the end-to-end performance of pre-training, fine-tuning, and serving
LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and
70B) on three 8-GPU platforms with and without individual optimization
techniques, including ZeRO, quantization, recomputation, FlashAttention. Then,
we dive deeper to provide a detailed runtime analysis of the sub-modules,
including computing and communication operators in LLMs. For end users, our
benchmark and findings help better understand different optimization
techniques, training and inference frameworks, together with hardware platforms
in choosing configurations for deploying LLMs. For researchers, our in-depth
module-wise analyses discover potential opportunities for future work to
further optimize the runtime performance of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization. (arXiv:2311.10847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.10847">
<div class="article-summary-box-inner">
<span><p>This paper introduces a method for adapting LoRA adapters in smaller-sized
language models to arbitrary downstream tasks. Unlike standard
mixture-of-expert architectures, our method employs a gradient-free routing
function to choose a weighted combination of experts without increasing the
compute requirements for training or inference. The results show that
token-level adaptation of LoRA adapters outperforms the base Llama-2-7b model
across mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension
(SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that
the average performance of token-level adaptation outperforms individual models
fine-tuned for each of the tasks with the best performance observed in
adaptation of every-other token during inference. The code for this study is
made available through a public repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.14115">
<div class="article-summary-box-inner">
<span><p>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weak Alignment Supervision from Hybrid Model Improves End-to-end ASR. (arXiv:2311.14835v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.14835">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to create weak alignment supervision from an existing
hybrid system to aid the end-to-end modeling of automatic speech recognition.
Towards this end, we use the existing hybrid ASR system to produce triphone
alignments of the training audios. We then create a cross-entropy loss at a
certain layer of the encoder using the derived alignments. In contrast to the
general one-hot cross-entropy losses, here we use a cross-entropy loss with a
label smoothing parameter to regularize the supervision. As a comparison, we
also conduct the experiments with one-hot cross-entropy losses and CTC losses
with loss weighting. The results show that placing the weak alignment
supervision with the label smoothing parameter of 0.5 at the third encoder
layer outperforms the other two approaches and leads to about 5\% relative WER
reduction on the TED-LIUM 2 dataset over the baseline. We see similar
improvements when applying the method out-of-the-box on a Tagalog end-to-end
ASR system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition. (arXiv:2311.16119v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.16119">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are deployed in interactive contexts with direct
user engagement, such as chatbots and writing assistants. These deployments are
vulnerable to prompt injection and jailbreaking (collectively, prompt hacking),
in which models are manipulated to ignore their original instructions and
follow potentially malicious ones. Although widely acknowledged as a
significant security threat, there is a dearth of large-scale resources and
quantitative studies on prompt hacking. To address this lacuna, we launch a
global prompt hacking competition, which allows for free-form human input
attacks. We elicit 600K+ adversarial prompts against three state-of-the-art
LLMs. We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLLMs-Augmented Visual-Language Representation Learning. (arXiv:2311.18765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.18765">
<div class="article-summary-box-inner">
<span><p>Visual-language pre-training (VLP) has achieved remarkable success in
multi-modal tasks, largely attributed to the availability of large-scale
image-text datasets. In this work, we demonstrate that multi-modal large
language models (MLLMs) can enhance visual-language representation learning by
improving data quality. Our approach is simple, utilizing MLLMs to extend
multiple captions for each image. To prevent the bias introduced by MLLMs'
hallucinations and intrinsic caption styles, we propose "text shearing" to
maintain the same length for extended captions as that of the original
captions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%
and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot
settings, respectively. Notably, we obtain zero-shot results that are
comparable to fine-tuning on target datasets, which encourages more exploration
of the versatile use of MLLMs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-12-04 23:12:31.168978733 UTC">2023-12-04 23:12:31 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>