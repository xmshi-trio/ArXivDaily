<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-07-20T01:30:00Z">07-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study. (arXiv:2307.09532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09532">
<div class="article-summary-box-inner">
<span><p>Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots. (arXiv:2307.09579v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09579">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing and machine learning have led
to the development of chatbot models, such as ChatGPT, that can engage in
conversational dialogue with human users. However, the ability of these models
to generate toxic or harmful responses during a non-toxic multi-turn
conversation remains an open research question. Existing research focuses on
single-turn sentence testing, while we find that 82\% of the individual
non-toxic sentences that elicit toxic behaviors in a conversation are
considered safe by existing tools. In this paper, we design a new attack,
\toxicbot, by fine-tuning a chatbot to engage in conversation with a target
open-domain chatbot. The chatbot is fine-tuned with a collection of crafted
conversation sequences. Particularly, each conversation begins with a sentence
from a crafted prompt sentences dataset. Our extensive evaluation shows that
open-domain chatbot models can be triggered to generate toxic responses in a
multi-turn conversation. In the best scenario, \toxicbot achieves a 67\%
activation rate. The conversation sequences in the fine-tuning stage help
trigger the toxicity in a conversation, which allows the attack to bypass two
defense methods. Our findings suggest that further research is needed to
address chatbot toxicity in a dynamic interactive environment. The proposed
\toxicbot can be used by both industry and researchers to develop methods for
detecting and mitigating toxic responses in conversational dialogue and improve
the robustness of chatbots for end users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noor-Ghateh: A Benchmark Dataset for Evaluating Arabic Word Segmenters in Hadith Domain. (arXiv:2307.09630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09630">
<div class="article-summary-box-inner">
<span><p>There are many complex and rich morphological subtleties in the Arabic
language, which are very useful when analyzing traditional Arabic texts,
especially in the historical and religious contexts, and help in understanding
the meaning of the texts. Vocabulary separation means separating the word into
different parts such as root and affix. In the morphological datasets, the
variety of labels and the number of data samples helps to evaluate the
morphological methods. In this paper, we present a benchmark data set for
evaluating the methods of separating Arabic words which include about 223,690
words from the book of Sharia alIslam, which have been labeled by experts. In
terms of the volume and variety of words, this dataset is superior to other
existing data sets, and as far as we know, there are no Arabic Hadith Domain
texts. To evaluate the dataset, we applied different methods such as Farasa,
Camel, Madamira, and ALP to the dataset and we reported the annotation quality
through four evaluation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation. (arXiv:2307.09701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09701">
<div class="article-summary-box-inner">
<span><p>Rising computational demands of modern natural language processing (NLP)
systems have increased the barrier to entry for cutting-edge research while
posing serious environmental concerns. Yet, progress on model efficiency has
been impeded by practical challenges in model evaluation and comparison. For
example, hardware is challenging to control due to disparate levels of
accessibility across different institutions. Moreover, improvements in metrics
such as FLOPs often fail to translate to progress in real-world applications.
In response, we introduce Pentathlon, a benchmark for holistic and realistic
evaluation of model efficiency. Pentathlon focuses on inference, which accounts
for a majority of the compute in a model's lifecycle. It offers a
strictly-controlled hardware platform, and is designed to mirror real-world
applications scenarios. It incorporates a suite of metrics that target
different aspects of efficiency, including latency, throughput, memory
overhead, and energy consumption. Pentathlon also comes with a software library
that can be seamlessly integrated into any codebase and enable evaluation. As a
standardized and centralized evaluation platform, Pentathlon can drastically
reduce the workload to make fair and reproducible efficiency comparisons. While
initially focused on natural language processing (NLP) models, Pentathlon is
designed to allow flexible extension to other fields. We envision Pentathlon
will stimulate algorithmic innovations in building efficient models, and foster
an increased awareness of the social and environmental implications in the
development of future-generation NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09702">
<div class="article-summary-box-inner">
<span><p>In this article we describe an efficient approach to guiding language model
text generation with regular expressions and context-free grammars. Our
approach adds little to no overhead to the token sequence generation process,
and makes guided generation feasible in practice. An implementation is provided
in the open source Python library Outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility. (arXiv:2307.09705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09705">
<div class="article-summary-box-inner">
<span><p>With the rapid evolution of large language models (LLMs), there is a growing
concern that they may pose risks or have negative social impacts. Therefore,
evaluation of human values alignment is becoming increasingly important.
Previous work mainly focuses on assessing the performance of LLMs on certain
knowledge and reasoning abilities, while neglecting the alignment to human
values, especially in a Chinese context. In this paper, we present CValues, the
first Chinese human values evaluation benchmark to measure the alignment
ability of LLMs in terms of both safety and responsibility criteria. As a
result, we have manually collected adversarial safety prompts across 10
scenarios and induced responsibility prompts from 8 domains by professional
experts. To provide a comprehensive values evaluation of Chinese LLMs, we not
only conduct human evaluation for reliable comparison, but also construct
multi-choice prompts for automatic evaluation. Our findings suggest that while
most Chinese LLMs perform well in terms of safety, there is considerable room
for improvement in terms of responsibility. Moreover, both the automatic and
human evaluation are important for assessing the human values alignment in
different aspects. The benchmark and code is available on ModelScope and
Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09706">
<div class="article-summary-box-inner">
<span><p>Taxonomies are an essential knowledge representation, yet most studies on
automatic taxonomy construction (ATC) resort to manual evaluation to score
proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just
as important as taxonomy construction. We propose RaTE, an automatic label-free
taxonomy scoring procedure, which relies on a large pre-trained language model.
We apply our evaluation procedure to three state-of-the-art ATC algorithms with
which we built seven taxonomies from the Yelp domain, and show that 1) RaTE
correlates well with human judgments and 2) artificially degrading a taxonomy
leads to decreasing RaTE score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction. (arXiv:2307.09744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09744">
<div class="article-summary-box-inner">
<span><p>The integration of natural language processing (NLP) technologies into
educational applications has shown promising results, particularly in the
language learning domain. Recently, many spoken open-domain chatbots have been
used as speaking partners, helping language learners improve their language
skills. However, one of the significant challenges is the high word-error-rate
(WER) when recognizing non-native/non-fluent speech, which interrupts
conversation flow and leads to disappointment for learners. This paper explores
the use of GPT4 for ASR error correction in conversational settings. In
addition to WER, we propose to use semantic textual similarity (STS) and next
response sensibility (NRS) metrics to evaluate the impact of error correction
models on the quality of the conversation. We find that transcriptions
corrected by GPT4 lead to higher conversation quality, despite an increase in
WER. GPT4 also outperforms standard error correction methods without the need
for in-domain training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09782">
<div class="article-summary-box-inner">
<span><p>In the complex domain of large language models (LLMs), striking a balance
between computational efficiency and maintaining model quality is a formidable
challenge. Navigating the inherent limitations of uniform quantization,
particularly when dealing with outliers, and motivated by the launch of
NVIDIA's H100 hardware, this study delves into the viability of floating-point
(FP) quantization, particularly focusing on FP8 and FP4, as a potential
solution. Our comprehensive investigation reveals that for LLMs, FP8 activation
consistently outshines its integer (INT8) equivalent, with the performance edge
becoming more noticeable in models possessing parameters beyond one billion.
For weight quantization, our findings indicate that FP4 exhibits comparable, if
not superior, performance to INT4, simplifying deployment on FP-supported
hardware like H100. To mitigate the overhead from precision alignment caused by
the disparity between weights and activations, we propose two scaling
constraints for weight quantization that negligibly impact the performance
compared to the standard W4A8 model. We additionally enhance our quantization
methods by integrating the Low Rank Compensation (LoRC) strategy, yielding
improvements especially in smaller models. The results of our investigation
emphasize the immense potential of FP quantization for LLMs, paving the way for
high-efficiency deployment in resource-limited settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models. (arXiv:2307.09793v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09793">
<div class="article-summary-box-inner">
<span><p>Since late 2022, Large Language Models (LLMs) have become very prominent with
LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs
are announced each week, many of which are deposited to Hugging Face, a
repository of machine learning models and datasets. To date, nearly 16,000 Text
Generation models have been uploaded to the site. Given the huge influx of
LLMs, it is of interest to know which LLM backbones, settings, training
methods, and families are popular or trending. However, there is no
comprehensive index of LLMs available. We take advantage of the relatively
systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering
and identify communities amongst LLMs using n-grams and term frequency-inverse
document frequency. Our methods successfully identify families of LLMs and
accurately cluster LLMs into meaningful subgroups. We present a public web
application to navigate and explore Constellation, our atlas of 15,821 LLMs.
Constellation rapidly generates a variety of visualizations, namely
dendrograms, graphs, word clouds, and scatter plots. Constellation is available
at the following link: https://constellation.sites.stanford.edu/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification. (arXiv:2307.09813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09813">
<div class="article-summary-box-inner">
<span><p>Event Causality Identification (ECI) aims at determining whether there is a
causal relation between two event mentions. Conventional prompt learning
designs a prompt template to first predict an answer word and then maps it to
the final decision. Unlike conventional prompts, we argue that predicting an
answer word may not be a necessary prerequisite for the ECI task. Instead, we
can first make a deterministic assumption on the existence of causal relation
between two events and then evaluate its rationality to either accept or reject
the assumption. The design motivation is to try the most utilization of the
encyclopedia-like knowledge embedded in a pre-trained language model. In light
of such considerations, we propose a deterministic assumption prompt learning
model, called DAPrompt, for the ECI task. In particular, we design a simple
deterministic assumption template concatenating with the input event pair,
which includes two masks as predicted events' tokens. We use the probabilities
of predicted events to evaluate the assumption rationality for the final event
causality decision. Experiments on the EventStoryLine corpus and
Causal-TimeBank corpus validate our design objective in terms of significant
performance improvements over the state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-takers have a say: understanding the implications of the use of AI in language tests. (arXiv:2307.09885v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09885">
<div class="article-summary-box-inner">
<span><p>Language tests measure a person's ability to use a language in terms of
listening, speaking, reading, or writing. Such tests play an integral role in
academic, professional, and immigration domains, with entities such as
educational institutions, professional accreditation bodies, and governments
using them to assess candidate language proficiency. Recent advances in
Artificial Intelligence (AI) and the discipline of Natural Language Processing
have prompted language test providers to explore AI's potential applicability
within language testing, leading to transformative activity patterns
surrounding language instruction and learning. However, with concerns over AI's
trustworthiness, it is imperative to understand the implications of integrating
AI into language testing. This knowledge will enable stakeholders to make
well-informed decisions, thus safeguarding community well-being and testing
integrity. To understand the concerns and effects of AI usage in language
tests, we conducted interviews and surveys with English test-takers. To the
best of our knowledge, this is the first empirical study aimed at identifying
the implications of AI adoption in language tests from a test-taker
perspective. Our study reveals test-taker perceptions and behavioral patterns.
Specifically, we identify that AI integration may enhance perceptions of
fairness, consistency, and availability. Conversely, it might incite mistrust
regarding reliability and interactivity aspects, subsequently influencing the
behaviors and well-being of test-takers. These insights provide a better
understanding of potential societal implications and assist stakeholders in
making informed decisions concerning AI usage in language testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models can accomplish Business Process Management Tasks. (arXiv:2307.09923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09923">
<div class="article-summary-box-inner">
<span><p>Business Process Management (BPM) aims to improve organizational activities
and their outcomes by managing the underlying processes. To achieve this, it is
often necessary to consider information from various sources, including
unstructured textual documents. Therefore, researchers have developed several
BPM-specific solutions that extract information from textual documents using
Natural Language Processing techniques. These solutions are specific to their
respective tasks and cannot accomplish multiple process-related problems as a
general-purpose instrument. However, in light of the recent emergence of Large
Language Models (LLMs) with remarkable reasoning capabilities, such a
general-purpose instrument with multiple applications now appears attainable.
In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by
applying a specific LLM to three exemplary tasks: mining imperative process
models from textual descriptions, mining declarative process models from
textual descriptions, and assessing the suitability of process tasks from
textual descriptions for robotic process automation. We show that, without
extensive configuration or prompt engineering, LLMs perform comparably to or
better than existing solutions and discuss implications for future BPM research
as well as practical usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GUIDO: A Hybrid Approach to Guideline Discovery & Ordering from Natural Language Texts. (arXiv:2307.09959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09959">
<div class="article-summary-box-inner">
<span><p>Extracting workflow nets from textual descriptions can be used to simplify
guidelines or formalize textual descriptions of formal processes like business
processes and algorithms. The task of manually extracting processes, however,
requires domain expertise and effort. While automatic process model extraction
is desirable, annotating texts with formalized process models is expensive.
Therefore, there are only a few machine-learning-based extraction approaches.
Rule-based approaches, in turn, require domain specificity to work well and can
rarely distinguish relevant and irrelevant information in textual descriptions.
In this paper, we present GUIDO, a hybrid approach to the process model
extraction task that first, classifies sentences regarding their relevance to
the process model, using a BERT-based sentence classifier, and second, extracts
a process model from the sentences classified as relevant, using dependency
parsing. The presented approach achieves significantly better results than a
pure rule-based approach. GUIDO achieves an average behavioral similarity score
of $0.93$. Still, in comparison to purely machine-learning-based approaches,
the annotation costs stay low.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09998">
<div class="article-summary-box-inner">
<span><p>The derivation of mathematical results in specialised fields using Large
Language Models (LLMs) is an emerging research direction that can help identify
models' limitations, and potentially support mathematical discovery. In this
paper, we leverage a symbolic engine to generate derivations of equations at
scale, and investigate the capabilities of LLMs when deriving goal equations
from premises. Specifically, we employ in-context learning for GPT and
fine-tune a range of T5 models to compare the robustness and generalisation of
pre-training strategies to specialised models. Empirical results show that
fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and
out-of-distribution test sets in terms of absolute performance. However, an
in-depth analysis reveals that the fine-tuned models are more sensitive to
perturbations involving unseen symbols and (to a lesser extent) changes to
equation structure. In addition, we analyse 1.7K equations and over 200
derivations to highlight common reasoning errors such as the inclusion of
incorrect, irrelevant, and redundant equations, along with the tendency to skip
derivation steps. Finally, we explore the suitability of existing metrics for
evaluating mathematical derivations finding evidence that, while they capture
general properties such as sensitivity to perturbations, they fail to highlight
fine-grained reasoning errors and essential differences between models.
Overall, this work demonstrates that training models on synthetic data can
improve their mathematical capabilities beyond larger architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10025">
<div class="article-summary-box-inner">
<span><p>Fertility issues are closely related to population security, in 60 years
China's population for the first time in a negative growth trend, the change of
fertility policy is of great concern to the community. 2023 ``two sessions"
proposal ``suggests that the country in the form of legislation, the birth of
the registration of the cancellation of the marriage restriction" This topic
was once a hot topic on the Internet, and ``unbundling" the relationship
between birth registration and marriage has become the focus of social debate.
In this paper, we adopt co-occurrence semantic analysis, topic analysis and
sentiment analysis to conduct multi-granularity semantic analysis of microblog
comments. It is found that the discussion on the proposal of ``removing
marriage restrictions from birth registration" involves the individual, society
and the state at three dimensions, and is detailed into social issues such as
personal behaviour, social ethics and law, and national policy, with people's
sentiment inclined to be negative in most of the topics. Based on this, eight
proposals were made to provide a reference for governmental decision making and
to form a reference method for researching public opinion on political issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10088">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in device-control systems that can interpret
human natural language instructions and execute them on a digital device by
directly controlling its user interface. We present a dataset for
device-control research, Android in the Wild (AITW), which is orders of
magnitude larger than current datasets. The dataset contains human
demonstrations of device interactions, including the screens and actions, and
corresponding natural language instructions. It consists of 715k episodes
spanning 30k unique instructions, four versions of Android (v10-13),and eight
device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It
contains multi-step tasks that require semantic understanding of language and
visual context. This dataset poses a new challenge: actions available through
the user interface must be inferred from their visual appearance. And, instead
of simple UI element-based actions, the action space consists of precise
gestures (e.g., horizontal scrolls to operate carousel widgets). We organize
our dataset to encourage robustness analysis of device-control systems, i.e.,
how well a system performs in the presence of new task descriptions, new
applications, or new platform versions. We develop two agents and report
performance across the dataset. The dataset is available at
https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10098">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained self-supervised language models is widely adopted for
transfer learning to downstream tasks. Fine-tuning can be achieved by freezing
gradients of the pretrained network and only updating gradients of a newly
added classification layer, or by performing gradient updates on all
parameters. Gradual unfreezing makes a trade-off between the two by gradually
unfreezing gradients of whole layers during training. This has been an
effective strategy to trade-off between storage and training speed with
generalization performance. However, it is not clear whether gradually
unfreezing layers throughout training is optimal, compared to sparse variants
of gradual unfreezing which may improve fine-tuning performance. In this paper,
we propose to stochastically mask gradients to regularize pretrained language
models for improving overall fine-tuned performance. We introduce GradDrop and
variants thereof, a class of gradient sparsification methods that mask
gradients during the backward pass, acting as gradient noise. GradDrop is
sparse and stochastic unlike gradual freezing. Extensive experiments on the
multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive
against methods that use additional translated data for intermediate
pretraining and outperforms standard fine-tuning and gradual unfreezing. A
post-analysis shows how GradDrop improves performance with languages it was not
trained on, such as under-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Transformer Extrapolation. (arXiv:2307.10156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10156">
<div class="article-summary-box-inner">
<span><p>Length extrapolation has attracted considerable attention recently since it
allows transformers to be tested on longer sequences than those used in
training. Previous research has shown that this property can be attained by
using carefully designed Relative Positional Encodings (RPEs). While these
methods perform well on a variety of corpora, the conditions for length
extrapolation have yet to be investigated. This paper attempts to determine
what types of RPEs allow for length extrapolation through a thorough
mathematical and empirical analysis. We discover that a transformer is certain
to possess this property as long as the series that corresponds to the RPE's
exponential converges. Two practices are derived from the conditions and
examined in language modeling tasks on a variety of corpora. As a bonus from
the conditions, we derive a new Theoretical Receptive Field (TRF) to measure
the receptive field of RPEs without taking any training steps. Extensive
experiments are conducted on the Wikitext-103, Books, Github, and WikiBook
datasets to demonstrate the viability of our discovered conditions. We also
compare TRF to Empirical Receptive Field (ERF) across different models, showing
consistently matched trends on the aforementioned datasets. The code is
available at https://github.com/OpenNLPLab/Rpe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs. (arXiv:2307.10168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10168">
<div class="article-summary-box-inner">
<span><p>LLMs have shown promise in replicating human-like behavior in crowdsourcing
tasks that were previously thought to be exclusive to human abilities. However,
current efforts focus mainly on simple atomic tasks. We explore whether LLMs
can replicate more complex crowdsourcing pipelines. We find that modern LLMs
can simulate some of crowdworkers' abilities in these "human computation
algorithms," but the level of success is variable and influenced by requesters'
understanding of LLM capabilities, the specific skills required for sub-tasks,
and the optimal interaction modality for performing these sub-tasks. We reflect
on human and LLMs' different sensitivities to instructions, stress the
importance of enabling human-facing safeguards for LLMs, and discuss the
potential of training humans and LLMs with complementary skill sets. Crucially,
we show that replicating crowdsourcing pipelines offers a valuable platform to
investigate (1) the relative strengths of LLMs on different tasks (by
cross-comparing their performances on sub-tasks) and (2) LLMs' potential in
complex tasks, where they can complete part of the tasks while leaving others
to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10169">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10172">
<div class="article-summary-box-inner">
<span><p>Despite advancements in conversational AI, language models encounter
challenges to handle diverse conversational tasks, and existing dialogue
dataset collections often lack diversity and comprehensiveness. To tackle these
issues, we introduce DialogStudio: the largest and most diverse collection of
dialogue datasets, unified under a consistent format while preserving their
original information. Our collection encompasses data from open-domain
dialogues, task-oriented dialogues, natural language understanding,
conversational recommendation, dialogue summarization, and knowledge-grounded
dialogues, making it an incredibly rich and diverse resource for dialogue
research and model training. To further enhance the utility of DialogStudio, we
identify the licenses for each dataset and design domain-aware prompts for
selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we
develop conversational AI models using the dataset collection, and our
experiments in both zero-shot and few-shot learning scenarios demonstrate the
superiority of DialogStudio. To improve transparency and support dataset and
task-based research, as well as language model pre-training, all datasets,
licenses, codes, and models associated with DialogStudio are made publicly
accessible at https://github.com/salesforce/DialogStudio
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14037">
<div class="article-summary-box-inner">
<span><p>Uncertainty approximation in text classification is an important area with
applications in domain adaptation and interpretability. One of the most widely
used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is
computationally expensive as it requires multiple forward passes through the
model. A cheaper alternative is to simply use the softmax based on a single
forward pass without dropout to estimate model uncertainty. However, prior work
has indicated that these predictions tend to be overconfident. In this paper,
we perform a thorough empirical analysis of these methods on five datasets with
two base neural architectures in order to identify the trade-offs between the
two. We compare both softmax and an efficient version of MC Dropout on their
uncertainty approximations and downstream text classification performance,
while weighing their runtime (cost) against performance (benefit). We find
that, while MC dropout produces the best uncertainty approximations, using a
simple softmax leads to competitive and in some cases better uncertainty
estimation for text classification at a much lower computational cost,
suggesting that softmax can in fact be a sufficient uncertainty estimate when
computational resources are a concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01692">
<div class="article-summary-box-inner">
<span><p>Language models exhibit an emergent ability to learn a new task from a small
number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of learning new associations from the input.
We argue that the commonly-used few-shot evaluation using a random selection of
in-context demonstrations can not disentangle models' reliance on such biases,
as most of the randomly-selected demonstrations do not present relations
informative for prediction beyond exposing the task's input-output
distribution.
</p>
<p>Therefore, to evaluate models' in-context learning ability independent of
models' memory, we introduce a Concept-sharing few-shot learning method
choosing the demonstrations that share an underlying concept with the predicted
sample. We extract a set of such concepts from available human explanations and
measure how much models can benefit from presenting these concepts in few-shot
demonstrations.
</p>
<p>We find that most of the recent in-context learners can not consistently
benefit from the demonstrated concepts, irrespective of the model size.
However, we note that T0 models are more sensitive to exhibited concepts,
benefiting from concept-sharing demonstrations in 7 out of 8 evaluation
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation. (arXiv:2212.10551v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10551">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation (MNMT) aims to build a unified model
for many language directions. Existing monolithic models for MNMT encounter two
challenges: parameter interference among languages and inefficient inference
for large models. In this paper, we revisit the classic multi-way structures
and develop a detachable model by assigning each language (or group of
languages) to an individual branch that supports plug-and-play training and
inference. To address the needs of learning representations for all languages
in a unified space, we propose a novel efficient training recipe, upon which we
build an effective detachable model, Lego-MT. For a fair comparison, we collect
data from OPUS and build a translation benchmark covering 433 languages and
1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings
an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters.
The proposed training recipe brings a 28.2$\times$ speedup over the
conventional multi-way training method.\footnote{
\url{https://github.com/CONE-MT/Lego-MT}.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11596">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates six scientific/medical, three general-domain and
five math word question answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12135">
<div class="article-summary-box-inner">
<span><p>The growth of pending legal cases in populous countries, such as India, has
become a major issue. Developing effective techniques to process and understand
legal documents is extremely useful in resolving this problem. In this paper,
we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi
et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that
considers the comprehensive context information in both intra- and
inter-sentence levels to predict rhetorical roles (subtask A) and then train a
Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize
legal entities (subtask B). Our evaluations demonstrate that our designed
models are more accurate than baselines, e.g., with an up to 15.0% better F1
score in subtask B. We achieved notable performance in the task leaderboard,
e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. (arXiv:2303.15056v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15056">
<div class="article-summary-box-inner">
<span><p>Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based cross-modality pretraining methods have recently
exhibited impressive success in diverse fields. In this paper, we propose
GEmo-CLAP, a kind of gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) method for speech emotion recognition. Specifically, a novel
emotion CLAP model (Emo-CLAP) is first built, utilizing various self-supervised
pre-trained models. Second, considering the importance of gender attribute in
speech emotion modeling, the soft label based GEmo-CLAP (SL-GEmo-CLAP) and
multi-task learning based GEmo-CLAP (ML-GEmo-CLAP) are further proposed to
integrate the emotion and gender information of speech signals, forming more
reasonable objectives. Extensive experiments on IEMOCAP show that our proposed
two GEmo-CLAP models consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving the best recognition
performance compared with recent state-of-the-art methods. Noticeably, the
proposed WavLM-based ML-GEmo-CLAP obtains the best UAR of 80.16\% and WAR of
82.06\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12317">
<div class="article-summary-box-inner">
<span><p>In this work, we demonstrate the application of a simple first-order Taylor
expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times
m}$ and utilize it in language modeling. To enhance the basic Taylor expansion,
we introduce iteration and piecewise modeling, leading us to name the algorithm
the Iterative Piecewise Affine (IPA) approximation. The final algorithm
exhibits interesting resemblances to the Transformers decoder architecture. By
comparing parameter arrangements in IPA and Transformers, we observe a
strikingly similar performance, with IPA outperforming Transformers by 1.5\% in
the next token prediction task with cross-entropy loss for smaller sequence
lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02486">
<div class="article-summary-box-inner">
<span><p>Scaling sequence length has become a critical demand in the era of large
language models. However, existing methods struggle with either computational
complexity or model expressivity, rendering the maximum sequence length
restricted. To address this issue, we introduce LongNet, a Transformer variant
that can scale sequence length to more than 1 billion tokens, without
sacrificing the performance on shorter sequences. Specifically, we propose
dilated attention, which expands the attentive field exponentially as the
distance grows. LongNet has significant advantages: 1) it has a linear
computation complexity and a logarithm dependency between any two tokens in a
sequence; 2) it can be served as a distributed trainer for extremely long
sequences; 3) its dilated attention is a drop-in replacement for standard
attention, which can be seamlessly integrated with the existing
Transformer-based optimization. Experiments results demonstrate that LongNet
yields strong performance on both long-sequence modeling and general language
tasks. Our work opens up new possibilities for modeling very long sequences,
e.g., treating a whole corpus or even the entire Internet as a sequence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03135">
<div class="article-summary-box-inner">
<span><p>Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Code released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08621">
<div class="article-summary-box-inner">
<span><p>In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09288">
<div class="article-summary-box-inner">
<span><p>In this work, we develop and release Llama 2, a collection of pretrained and
fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
dialogue use cases. Our models outperform open-source chat models on most
benchmarks we tested, and based on our human evaluations for helpfulness and
safety, may be a suitable substitute for closed-source models. We provide a
detailed description of our approach to fine-tuning and safety improvements of
Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09416">
<div class="article-summary-box-inner">
<span><p>Research in Image Generation has recently made significant progress,
particularly boosted by the introduction of Vision-Language models which are
able to produce high-quality visual content based on textual inputs. Despite
ongoing advancements in terms of generation quality and realism, no methodical
frameworks have been defined yet to quantitatively measure the quality of the
generated content and the adherence with the prompted requests: so far, only
human-based evaluations have been adopted for quality satisfaction and for
comparing different generative methods. We introduce a novel automated method
for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a
generated/edited image and the corresponding prompt/instructions, with a
process inspired by the human cognitive behaviour. ViCE combines the strengths
of Large Language Models (LLMs) and Visual Question Answering (VQA) into a
unified pipeline, aiming to replicate the human cognitive process in quality
assessment. This method outlines visual concepts, formulates image-specific
verification questions, utilizes the Q&amp;A system to investigate the image, and
scores the combined outcome. Although this brave new hypothesis of mimicking
humans in the image evaluation process is in its preliminary assessment stage,
results are promising and open the door to a new form of automatic evaluation
which could have significant impact as the image generation or the image target
editing tasks become more and more sophisticated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers. (arXiv:2307.09455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09455">
<div class="article-summary-box-inner">
<span><p>For real-world language applications, detecting an out-of-distribution (OOD)
sample is helpful to alert users or reject such unreliable samples. However,
modern over-parameterized language models often produce overconfident
predictions for both in-distribution (ID) and OOD samples. In particular,
language models suffer from OOD samples with a similar semantic representation
to ID samples since these OOD samples lie near the ID manifold. A rejection
network can be trained with ID and diverse outlier samples to detect test OOD
samples, but explicitly collecting auxiliary OOD datasets brings an additional
burden for data collection. In this paper, we propose a simple but effective
method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD
dataset by sequentially masking tokens related to ID classes. The surrogate OOD
sample introduced by POE shows a similar representation to ID data, which is
most effective in training a rejection network. Our method does not require any
external OOD data and can be easily implemented within off-the-shelf
Transformers. A comprehensive comparison with state-of-the-art algorithms
demonstrates POE's competitiveness on several text classification benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparative analysis of SRGAN models. (arXiv:2307.09456v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09456">
<div class="article-summary-box-inner">
<span><p>In this study, we evaluate the performance of multiple state-of-the-art SRGAN
(Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN
and EDSR, on a benchmark dataset of real-world images which undergo degradation
using a pipeline. Our results show that some models seem to significantly
increase the resolution of the input images while preserving their visual
quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE
model from huggingface outperforms the remaining candidate models in terms of
both quantitative metrics and subjective visual quality assessments with least
compute overhead. Specifically, EDSR generates images with higher peak
signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and
are seen to return high quality OCR results with Tesseract OCR engine. These
findings suggest that EDSR is a robust and effective approach for single-image
super-resolution and may be particularly well-suited for applications where
high-quality visual fidelity is critical and optimized compute.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-07-20 23:11:29.685071874 UTC">2023-07-20 23:11:29 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>