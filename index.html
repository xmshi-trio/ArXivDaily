<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-15T01:30:00Z">08-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss. (arXiv:2308.06327v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06327">
<div class="article-summary-box-inner">
<span><p>We introduce a bilingual solution to support English as secondary locale for
most primary locales in hybrid automatic speech recognition (ASR) settings. Our
key developments constitute: (a) pronunciation lexicon with grapheme units
instead of phone units, (b) a fully bilingual alignment model and subsequently
bilingual streaming transformer model, (c) a parallel encoder structure with
language identification (LID) loss, (d) parallel encoder with an auxiliary loss
for monolingual projections. We conclude that in comparison to LID loss, our
proposed auxiliary loss is superior in specializing the parallel encoders to
respective monolingual locales, and that contributes to stronger bilingual
learning. We evaluate our work on large-scale training and test tasks for
bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual
models demonstrate strong English code-mixing capability. In particular, the
bilingual IT model improves the word error rate (WER) for a code-mix IT task
from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the
monolingual IT model (9.5%) over IT tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06354">
<div class="article-summary-box-inner">
<span><p>Social determinants of health (SDoH) have an important impact on patient
outcomes but are incompletely collected from the electronic health records
(EHR). This study researched the ability of large language models to extract
SDoH from free text in EHRs, where they are most commonly documented, and
explored the role of synthetic clinical text for improving the extraction of
these scarcely documented, yet extremely valuable, clinical data. 800 patient
notes were annotated for SDoH categories, and several transformer-based models
were evaluated. The study also experimented with synthetic data generation and
assessed for algorithmic bias. Our best-performing models were fine-tuned
Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The
benefit of augmenting fine-tuning with synthetic data varied across model
architecture and size, with smaller Flan-T5 models (base and large) showing the
greatest improvements in performance (delta F1 +0.12 to +0.23). Model
performance was similar on the in-hospital system dataset but worse on the
MIMIC-III dataset. Our best-performing fine-tuned models outperformed zero- and
few-shot performance of ChatGPT-family models for both tasks. These fine-tuned
models were less likely than ChatGPT to change their prediction when
race/ethnicity and gender descriptors were added to the text, suggesting less
algorithmic bias (p&lt;0.05). At the patient-level, our models identified 93.8% of
patients with adverse SDoH, while ICD-10 codes captured 2.0%. Our method can
effectively extracted SDoH information from clinic notes, performing better
compare to GPT zero- and few-shot settings. These models could enhance
real-world evidence on SDoH and aid in identifying patients needing social
support.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models and Knowledge Graphs: Opportunities and Challenges. (arXiv:2308.06374v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06374">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have taken Knowledge Representation -- and the
world -- by storm. This inflection point marks a shift from explicit knowledge
representation to a renewed focus on the hybrid representation of both explicit
knowledge and parametric knowledge. In this position paper, we will discuss
some of the common debate points within the community on LLMs (parametric
knowledge) and Knowledge Graphs (explicit knowledge) and speculate on
opportunities and visions that the renewed focus brings, as well as related
research topics and challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZYN: Zero-Shot Reward Models with Yes-No Questions. (arXiv:2308.06385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06385">
<div class="article-summary-box-inner">
<span><p>In this work, we address the problem of directing the text generations of a
LLM towards a desired behavior, aligning the generated text with the
preferences of the human operator. We propose using another language model as a
critic, reward model in a zero-shot way thanks to the prompt of a Yes-No
question that represents the user preferences, without requiring further
labeled data. This zero-shot reward model provides the learning signal to
further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet
our approach is also compatible in other contexts such as quality-diversity
search. Extensive evidence of the capabilities of the proposed ZYN framework is
provided through experiments in different domains related to text generation,
including detoxification; optimizing sentiment of movie reviews, or any other
attribute; steering the opinion about a particular topic the model may have;
and personalizing prompt generators for text-to-image tasks. Code to be
released at \url{https://github.com/vicgalle/zero-shot-reward-models/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Planning with a LLM. (arXiv:2308.06391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06391">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) can solve many NLP tasks in zero-shot
settings, applications involving embodied agents remain problematic. In
particular, complex plans that require multi-step reasoning become difficult
and too costly as the context window grows. Planning requires understanding the
likely effects of one's actions and identifying whether the current environment
satisfies the goal state. While symbolic planners find optimal solutions
quickly, they require a complete and accurate representation of the planning
problem, severely limiting their use in practical scenarios. In contrast,
modern LLMs cope with noisy observations and high levels of uncertainty when
reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a
neuro-symbolic framework where an LLM works hand-in-hand with a traditional
planner to solve an embodied task. Given action-descriptions, LLM-DP solves
Alfworld faster and more efficiently than a naive LLM ReAct baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance Prediction for Multi-hop Questions. (arXiv:2308.06431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06431">
<div class="article-summary-box-inner">
<span><p>We study the problem of Query Performance Prediction (QPP) for open-domain
multi-hop Question Answering (QA), where the task is to estimate the difficulty
of evaluating a multi-hop question over a corpus. Despite the extensive
research on predicting the performance of ad-hoc and QA retrieval models, there
has been a lack of study on the estimation of the difficulty of multi-hop
questions. The problem is challenging due to the multi-step nature of the
retrieval process, potential dependency of the steps and the reasoning
involved. To tackle this challenge, we propose multHP, a novel pre-retrieval
method for predicting the performance of open-domain multi-hop questions. Our
extensive evaluation on the largest multi-hop QA dataset using several modern
QA systems shows that the proposed model is a strong predictor of the
performance, outperforming traditional single-hop QPP models. Additionally, we
demonstrate that our approach can be effectively used to optimize the
parameters of QA systems, such as the number of documents to be retrieved,
resulting in improved overall retrieval performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy. (arXiv:2308.06450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06450">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation (ERC) has emerged as a research hotspot
in domains such as conversational robots and question-answer systems. How to
efficiently and adequately retrieve contextual emotional cues has been one of
the key challenges in the ERC task. Existing efforts do not fully model the
context and employ complex network structures, resulting in excessive
computational resource overhead without substantial performance improvement. In
this paper, we propose a novel Emotion Recognition Network based on Curriculum
Learning strategy (ERNetCL). The proposed ERNetCL primarily consists of
Temporal Encoder (TE), Spatial Encoder (SE), and Curriculum Learning (CL) loss.
We utilize TE and SE to combine the strengths of previous methods in a
simplistic manner to efficiently capture temporal and spatial contextual
information in the conversation. To simulate the way humans learn curriculum
from easy to hard, we apply the idea of CL to the ERC task to progressively
optimize the network parameters of ERNetCL. At the beginning of training, we
assign lower learning weights to difficult samples. As the epoch increases, the
learning weights for these samples are gradually raised. Extensive experiments
on four datasets exhibit that our proposed method is effective and dramatically
beats other baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension. (arXiv:2308.06454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06454">
<div class="article-summary-box-inner">
<span><p>Although deep learning techniques have shown significant achievements, they
frequently depend on extensive amounts of hand-labeled data and tend to perform
inadequately in few-shot scenarios. The objective of this study is to devise a
strategy that can improve the model's capability to recognize biomedical
entities in scenarios of few-shot learning. By redefining biomedical named
entity recognition (BioNER) as a machine reading comprehension (MRC) problem,
we propose a demonstration-based learning method to address few-shot BioNER,
which involves constructing appropriate task demonstrations. In assessing our
proposed method, we compared the proposed method with existing advanced methods
using six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical,
BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models'
efficacy by reporting F1 scores from both the 25-shot and 50-shot learning
experiments. In 25-shot learning, we observed 1.1% improvements in the average
F1 scores compared to the baseline method, reaching 61.7%, 84.1%, 69.1%, 70.1%,
50.6%, and 59.9% on six datasets, respectively. In 50-shot learning, we further
improved the average F1 scores by 1.0% compared to the baseline method,
reaching 73.1%, 86.8%, 76.1%, 75.6%, 61.7%, and 65.4%, respectively. We
reported that in the realm of few-shot learning BioNER, MRC-based language
models are much more proficient in recognizing biomedical entities compared to
the sequence labeling approach. Furthermore, our MRC-language models can
compete successfully with fully-supervised learning methodologies that rely
heavily on the availability of abundant annotated data. These results highlight
possible pathways for future advancements in few-shot BioNER methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation. (arXiv:2308.06457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06457">
<div class="article-summary-box-inner">
<span><p>The advent of ChatGPT has introduced innovative methods for information
gathering and analysis. However, the information provided by ChatGPT is limited
to text, and the visualization of this information remains constrained.
Previous research has explored zero-shot text-to-video (TTV) approaches to
transform text into videos. However, these methods lacked control over the
identity of the generated audio, i.e., not identity-agnostic, hindering their
effectiveness. To address this limitation, we propose a novel two-stage
framework for person-agnostic video cloning, specifically focusing on TTV
generation. In the first stage, we leverage pretrained zero-shot models to
achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven
talking head generation method is employed to produce compelling videos
privided the audio generated in the first stage. This paper presents a
comparative analysis of different TTS and audio-driven talking head generation
methods, identifying the most promising approach for future research and
development. Some audio and videos samples can be found in the following link:
https://github.com/ZhichaoWang970201/Text-to-Video/tree/main.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. (arXiv:2308.06463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06463">
<div class="article-summary-box-inner">
<span><p>Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06488">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph (KG)-to-Text generation aims at generating fluent
natural-language text that accurately represents the information of a given
knowledge graph. While significant progress has been made in this task by
exploiting the power of pre-trained language models (PLMs) with appropriate
graph structure-aware modules, existing models still fall short of generating
faithful text, especially when the ground-truth natural-language text contains
additional information that is not present in the graph. In this paper, we
develop a KG-to-text generation model that can generate faithful
natural-language text from a given graph, in the presence of noisy reference
text. Our framework incorporates two core ideas: Firstly, we utilize
contrastive learning to enhance the model's ability to differentiate between
faithful and hallucinated information in the text, thereby encouraging the
decoder to generate text that aligns with the input graph. Secondly, we empower
the decoder to control the level of hallucination in the generated text by
employing a controllable text generation technique. We evaluate our model's
performance through the standard quantitative metrics as well as a
ChatGPT-based quantitative and qualitative analysis. Our evaluation
demonstrates the superior performance of our model over state-of-the-art
KG-to-text models on faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsDialogues: Towards Proactive News Grounded Conversation. (arXiv:2308.06501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06501">
<div class="article-summary-box-inner">
<span><p>Hot news is one of the most popular topics in daily conversations. However,
news grounded conversation has long been stymied by the lack of well-designed
task definition and scarce data. In this paper, we propose a novel task,
Proactive News Grounded Conversation, in which a dialogue system can
proactively lead the conversation based on some key topics of the news. In
addition, both information-seeking and chit-chat scenarios are included
realistically, where the user may ask a series of questions about the news
details or express their opinions and be eager to chat. To further develop this
novel task, we collect a human-to-human Chinese dialogue dataset
\ts{NewsDialogues}, which includes 1K conversations with a total of 14.6K
utterances and detailed annotations for target topics and knowledge spans.
Furthermore, we propose a method named Predict-Generate-Rank, consisting of a
generator for grounded knowledge prediction and response generation, and a
ranker for the ranking of multiple responses to alleviate the exposure bias. We
conduct comprehensive experiments to demonstrate the effectiveness of the
proposed method and further present several key findings and challenges to
prompt future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06502">
<div class="article-summary-box-inner">
<span><p>This paper describes the systems submitted by team6 for ChatEval, the DSTC 11
Track 4 competition. We present three different approaches to predicting
turn-level qualities of chatbot responses based on large language models
(LLMs). We report improvement over the baseline using dynamic few-shot examples
from a vector store for the prompts for ChatGPT. We also analyze the
performance of the other two approaches and report needed improvements for
future work. We developed the three systems over just two weeks, showing the
potential of LLMs for this task. An ablation study conducted after the
challenge deadline shows that the new Llama 2 models are closing the
performance gap between ChatGPT and open-source LLMs. However, we find that the
Llama 2 models do not benefit from few-shot examples in the same way as
ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models. (arXiv:2308.06507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06507">
<div class="article-summary-box-inner">
<span><p>Information-seeking conversation, which aims to help users gather information
through conversation, has achieved great progress in recent years. However, the
research is still stymied by the scarcity of training data. To alleviate this
problem, we propose AutoConv for synthetic conversation generation, which takes
advantage of the few-shot learning ability and generation capacity of large
language models (LLM). Specifically, we formulate the conversation generation
problem as a language modeling task, then finetune an LLM with a few human
conversations to capture the characteristics of the information-seeking process
and use it for generating synthetic conversations with high quality.
Experimental results on two frequently-used datasets verify that AutoConv has
substantial improvements over strong baselines and alleviates the dependence on
human annotation. In addition, we also provide several analysis studies to
promote future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion. (arXiv:2308.06512v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06512">
<div class="article-summary-box-inner">
<span><p>Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by
associating attribute-value qualifiers to triples, which effectively represent
additional fine-grained information about its associated triple.
Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown
triples while considering its qualifiers. Most existing approaches to HKGC
exploit a global-level graph structure to encode hyper-relational knowledge
into the graph convolution message passing process. However, the addition of
multi-hop information might bring noise into the triple prediction process. To
address this problem, we propose HyperFormer, a model that considers
local-level sequential information, which encodes the content of the entities,
relations and qualifiers of a triple. More precisely, HyperFormer is composed
of three different modules: an entity neighbor aggregator module allowing to
integrate the information of the neighbors of an entity to capture different
perspectives of it; a relation qualifier aggregator module to integrate
hyper-relational knowledge into the corresponding relation to refine the
representation of relational content; a convolution-based bidirectional
interaction module based on a convolutional operation, capturing pairwise
bidirectional interactions of entity-relation, entity-qualifier, and
relation-qualifier. realize the depth perception of the content related to the
current statement. Furthermore, we introduce a Mixture-of-Experts strategy into
the feed-forward layers of HyperFormer to strengthen its representation
capabilities while reducing the amount of model parameters and computation.
Extensive experiments on three well-known datasets with four different
conditions demonstrate HyperFormer's effectiveness. Datasets and code are
available at https://github.com/zhiweihu1103/HKGC-HyperFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector. (arXiv:2308.06527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06527">
<div class="article-summary-box-inner">
<span><p>This work presents our efforts to reproduce the results of the human
evaluation experiment presented in the paper of Vamvas and Sennrich (2022),
which evaluated an automatic system detecting over- and undertranslations
(translations containing more or less information than the original) in machine
translation (MT) outputs. Despite the high quality of the documentation and
code provided by the authors, we discuss some problems we found in reproducing
the exact experimental setup and offer recommendations for improving
reproducibility. Our replicated results generally confirm the conclusions of
the original study, but in some cases, statistically significant differences
were observed, suggesting a high variability of human annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06546">
<div class="article-summary-box-inner">
<span><p>Extracting meaningful drug-related information chunks, such as adverse drug
events (ADE), is crucial for preventing morbidity and saving many lives. Most
ADE are reported via an unstructured conversation with the medical context.
Hence, applying a general entity recognition approach is not sufficient enough.
The key is how to integrate and align multiple crucial aspects to detect drug
event information, including drug event semantics, syntactic structures, and
medical domain terminology. In this paper, we propose a new multi-aspect
cross-integration framework for drug entity/event detection by capturing and
aligning different context/language/knowledge properties from drug-related
documents. We first construct multi-aspect encoders to describe semantic,
syntactic, and medical document contextual information by conducting those slot
tagging tasks, main drug entity/event detection, part-of-speech tagging, and
general medical named entity recognition. Then, each encoder conducts cross
integration and alignment with other contextual information in three ways,
including the key-value cross, attention cross, and feedforward cross, so the
multi-encoders are integrated in depth. Then, we perform extensive experiments
on two widely used drug-related entity recognition downstream tasks, flat
entity detection and discontinuous event extraction. Our model significantly
outperforms all recent twelve state-of-the-art models. The implementation code
will be released at~\url{https://github.com/adlnlp/mc-dre}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition. (arXiv:2308.06547v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06547">
<div class="article-summary-box-inner">
<span><p>When labeled data is insufficient, semi-supervised learning with the
pseudo-labeling technique can significantly improve the performance of
automatic speech recognition. However, pseudo-labels are often noisy,
containing numerous incorrect tokens. Taking noisy labels as ground-truth in
the loss function results in suboptimal performance. Previous works attempted
to mitigate this issue by either filtering out the nosiest pseudo-labels or
improving the overall quality of pseudo-labels. While these methods are
effective to some extent, it is unrealistic to entirely eliminate incorrect
tokens in pseudo-labels. In this work, we propose a novel framework named
alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the
perspective of the training objective. The framework comprises several
components. Firstly, a generalized CTC loss function is introduced to handle
noisy pseudo-labels by accepting alternative tokens in the positions of
incorrect tokens. Applying this loss function in pseudo-labeling requires
detecting incorrect tokens in the predicted pseudo-labels. In this work, we
adopt a confidence-based error detection method that identifies the incorrect
tokens by comparing their confidence scores with a given threshold, thus
necessitating the confidence score to be discriminative. Hence, the second
proposed technique is the contrastive CTC loss function that widens the
confidence gap between the correctly and incorrectly predicted tokens, thereby
improving the error detection ability. Additionally, obtaining satisfactory
performance with confidence-based error detection typically requires extensive
threshold tuning. Instead, we propose an automatic thresholding method that
uses labeled data as a proxy for determining the threshold, thus saving the
pain of manual tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06552">
<div class="article-summary-box-inner">
<span><p>Cross-lingual open information extraction aims to extract structured
information from raw text across multiple languages. Previous work uses a
shared cross-lingual pre-trained model to handle the different languages but
underuses the potential of the language-specific representation. In this paper,
we propose an effective multi-stage tuning framework called MT4CrossIE,
designed for enhancing cross-lingual open information extraction by injecting
language-specific knowledge into the shared model. Specifically, the
cross-lingual pre-trained model is first tuned in a shared semantic space
(e.g., embedding matrix) in the fixed encoder and then other components are
optimized in the second stage. After enough training, we freeze the pre-trained
model and tune the multiple extra low-rank language-specific modules using
mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we
leverage two-stage prompting to encourage the large language model (LLM) to
annotate the multi-lingual raw data for data-based cross-lingual transfer. The
model is trained with multi-lingual objectives on our proposed dataset
OpenIE4++ by combing the model-based and data-based transfer techniques.
Experimental results on various benchmarks emphasize the importance of
aggregating multiple plug-in-and-play language-specific modules and demonstrate
the effectiveness of MT4CrossIE in cross-lingual
OIE\footnote{\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06595">
<div class="article-summary-box-inner">
<span><p>We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 'instruction families' that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model's response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation. (arXiv:2308.06610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06610">
<div class="article-summary-box-inner">
<span><p>Medical systematic reviews can be very costly and resource intensive. We
explore how Large Language Models (LLMs) can support and be trained to perform
literature screening when provided with a detailed set of selection criteria.
Specifically, we instruction tune LLaMA and Guanaco models to perform abstract
screening for medical systematic reviews. Our best model, Bio-SIEVE,
outperforms both ChatGPT and trained traditional approaches, and generalises
better across medical domains. However, there remains the challenge of adapting
the model to safety-first scenarios. We also explore the impact of multi-task
training with Bio-SIEVE-Multi, including tasks such as PICO extraction and
exclusion reasoning, but find that it is unable to match single-task
Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards
specialising LLMs for the biomedical systematic review process and explore its
future developmental opportunities. We release our models, code and a list of
DOIs to reconstruct our dataset for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion. (arXiv:2308.06696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06696">
<div class="article-summary-box-inner">
<span><p>Recent years have seen significant advancements in multi-modal knowledge
graph completion (MMKGC). MMKGC enhances knowledge graph completion (KGC) by
integrating multi-modal entity information, thereby facilitating the discovery
of unobserved triples in the large-scale knowledge graphs (KGs). Nevertheless,
existing methods emphasize the design of elegant KGC models to facilitate
modality interaction, neglecting the real-life problem of missing modalities in
KGs. The missing modality information impedes modal interaction, consequently
undermining the model's performance. In this paper, we propose a modality
adversarial and contrastive framework (MACO) to solve the modality-missing
problem in MMKGC. MACO trains a generator and discriminator adversarially to
generate missing modality features that can be incorporated into the MMKGC
model. Meanwhile, we design a cross-modal contrastive loss to improve the
performance of the generator. Experiments on public benchmarks with further
explorations demonstrate that MACO could achieve state-of-the-art results and
serve as a versatile framework to bolster various MMKGC models. Our code and
benchmark data are available at https://github.com/zjukg/MACO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaCo: Preconditions Attributed to Commonsense Knowledge. (arXiv:2104.08712v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08712">
<div class="article-summary-box-inner">
<span><p>Humans can seamlessly reason with circumstantial preconditions of commonsense
knowledge. We understand that a glass is used for drinking water, unless the
glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language
models' (LMs) impressive performance on inferring commonsense knowledge, it is
unclear whether they understand the circumstantial preconditions. To address
this gap, we propose a novel challenge of reasoning with circumstantial
preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand
preconditions of commonsense statements expressed in natural language. Based on
this dataset, we create three canonical evaluation tasks and use them to
examine the capability of existing LMs to understand situational preconditions.
Our results reveal a 10-30% gap between machine and human performance on our
tasks, which shows that reasoning with preconditions is an open challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07340">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of transfer learning. Due
to the nature of mathematical texts, which often use domain specific vocabulary
along with equations and math symbols, we posit that the development of a new
BERT model for mathematics would be useful for many mathematical downstream
tasks. In this resource paper, we introduce our multi-institutional effort
(i.e., two learning platforms and three academic institutions in the US) toward
this need: MathBERT, a model created by pre-training the BASE BERT model on a
large mathematical corpus ranging from pre-kindergarten (pre-k), to
high-school, to college graduate level mathematical content. In addition, we
select three general NLP tasks that are often used in mathematics education:
prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge
tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our
experiments show that MathBERT outperforms prior best methods by 1.2-22% and
BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific
vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT
pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT
vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the
participated leaning platforms: Stride, Inc, a commercial educational resource
provider, and ASSISTments.org, a free online educational platform. We release
MathBERT for public usage at: https://github.com/tbs17/MathBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PInKS: Preconditioned Commonsense Inference with Minimal Supervision. (arXiv:2206.07920v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07920">
<div class="article-summary-box-inner">
<span><p>Reasoning with preconditions such as "glass can be used for drinking water
unless the glass is shattered" remains an open problem for language models. The
main challenge lies in the scarcity of preconditions data and the model's lack
of support for such reasoning. We present PInKS, Preconditioned Commonsense
Inference with WeaK Supervision, an improved model for reasoning with
preconditions through minimum supervision. We show, both empirically and
theoretically, that PInKS improves the results on benchmarks focused on
reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1
scores). We further investigate PInKS through PAC-Bayesian informativeness
analysis, precision measures, and ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages. (arXiv:2208.00463v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00463">
<div class="article-summary-box-inner">
<span><p>Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, while employing it results in
two issues: firstly, the untranslated tokens leading to unexpectedly high
translation scores, and secondly, the issue of mismatching errors between
source and hypothesis tokens when applying the greedy matching in XLMRScore. To
mitigate these issues, we suggest replacing untranslated words with the unknown
token and the cross-lingual alignment of the pre-trained model to represent
aligned words closer to each other, respectively. We evaluate the proposed
method on four low-resource language pairs of WMT21 QE shared task, as well as
a new English-Farsi test dataset introduced in this paper. Experiments show
that our method could get comparable results with the supervised baseline for
two zero-shot scenarios, i.e., with less than 0.01 difference in Pearson
correlation, while outperforming unsupervised rivals in all the low-resource
language pairs for above 8%, on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01066">
<div class="article-summary-box-inner">
<span><p>In-context learning refers to the ability of a model to condition on a prompt
sequence consisting of in-context examples (input-output pairs corresponding to
some task) along with a new query input, and generate the corresponding output.
Crucially, in-context learning happens only at inference time without any
parameter updates to the model. While large language models such as GPT-3
exhibit some ability to perform in-context learning, it is unclear what the
relationship is between tasks on which this succeeds and what is present in the
training data. To make progress towards understanding in-context learning, we
consider the well-defined problem of training a model to in-context learn a
function class (e.g., linear functions): that is, given data derived from some
functions in the class, can we train a model to in-context learn "most"
functions from this class? We show empirically that standard Transformers can
be trained from scratch to perform in-context learning of linear functions --
that is, the trained model is able to learn unseen linear functions from
in-context examples with performance comparable to the optimal least squares
estimator. In fact, in-context learning is possible even under two forms of
distribution shift: (i) between the training data of the model and
inference-time prompts, and (ii) between the in-context examples and the query
input during inference. We also show that we can train Transformers to
in-context learn more complex function classes -- namely sparse linear
functions, two-layer neural networks, and decision trees -- with performance
that matches or exceeds task-specific learning algorithms. Our code and models
are available at https://github.com/dtsip/in-context-learning .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08063">
<div class="article-summary-box-inner">
<span><p>To understand a narrative, it is essential to comprehend the temporal event
flows, especially those associated with main characters; however, this can be
challenging with lengthy and unstructured narrative texts. To address this, we
introduce NECE, an open-access, document-level toolkit that automatically
extracts and aligns narrative events in the temporal order of their occurrence.
Through extensive evaluations, we show the high quality of the NECE toolkit and
demonstrates its downstream application in analyzing narrative bias regarding
gender. We also openly discuss the shortcomings of the current approach, and
potential of leveraging generative models in future works. Lastly the NECE
toolkit includes both a Python library and a user-friendly web interface, which
offer equal access to professionals and layman audience alike, to visualize
event chain, obtain narrative flows, or study narrative bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08233">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
The source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08068">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-based learning has gained popularity across many natural
language processing (NLP) tasks by reformulating them into a cloze-style format
to better align pre-trained language models (PLMs) with downstream tasks.
However, applying this approach to relation classification poses unique
challenges. Specifically, associating natural language words that fill the
masked token with semantic relation labels (\textit{e.g.}
\textit{``org:founded\_by}'') is difficult. To address this challenge, this
paper presents a novel prompt-based learning method, namely LabelPrompt, for
the relation classification task. Motivated by the intuition to ``GIVE MODEL
CHOICES!'', we first define additional tokens to represent relation labels,
which regard these tokens as the verbaliser with semantic initialisation and
explicitly construct them with a prompt template method. Then, to mitigate
inconsistency between predicted relations and given entities, we implement an
entity-aware module with contrastive learning. Last, we conduct an attention
query strategy within the self-attention layer to differentiates prompt tokens
and sequence tokens. Together, these strategies enhance the adaptability of
prompt-based learning, especially when only small labelled datasets is
available. Comprehensive experiments on benchmark datasets demonstrate the
superiority of our method, particularly in the few-shot scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12324">
<div class="article-summary-box-inner">
<span><p>Good figure captions help paper readers understand complex scientific
figures. Unfortunately, even published papers often have poorly written
captions. Automatic caption generation could aid paper writers by providing
good starting captions that can be refined for better quality. Prior work often
treated figure caption generation as a vision-to-language task. In this paper,
we show that it can be more effectively tackled as a text summarization task in
scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive
summarization model, to specifically summarize figure-referencing paragraphs
(e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale
arXiv figures show that our method outperforms prior vision methods in both
automatic and human evaluations. We further conducted an in-depth investigation
focused on two key challenges: (i) the common presence of low-quality
author-written captions and (ii) the lack of clear standards for good captions.
Our code and data are available at:
https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05221">
<div class="article-summary-box-inner">
<span><p>Models of eye-movement control during reading, developed largely within
psychology, usually focus on visual, attentional, lexical, and motor processes
but neglect post-lexical language processing; by contrast, models of sentence
comprehension processes, developed largely within psycholinguistics, generally
focus only on post-lexical language processes. We present a model that combines
these two research threads, by integrating eye-movement control and sentence
processing. Developing such an integrated model is extremely challenging and
computationally demanding, but such an integration is an important step toward
complete mathematical models of natural language comprehension in reading. We
combine the SWIFT model of eye-movement control (Seelig et al., 2020,
doi:10.1016/j.jmp.<a href="/abs/2019.10231">2019.10231</a>3) with key components of the Lewis and Vasishth
sentence processing model (Lewis &amp; Vasishth, 2005,
doi:10.1207/s15516709cog0000_25). This integration becomes possible, for the
first time, due in part to recent advances in successful parameter
identification in dynamical models, which allows us to investigate profile
log-likelihoods for individual model parameters. We present a fully implemented
proof-of-concept model demonstrating how such an integrated model can be
achieved; our approach includes Bayesian model inference with Markov Chain
Monte Carlo (MCMC) sampling as a key computational tool. The integrated model,
SEAM, can successfully reproduce eye movement patterns that arise due to
similarity-based interference in reading. To our knowledge, this is the
first-ever integration of a complete process model of eye-movement control with
linguistic dependency completion processes in sentence comprehension. In future
work, this proof of concept model will need to be evaluated using a
comprehensive set of benchmark data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07274">
<div class="article-summary-box-inner">
<span><p>Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13099">
<div class="article-summary-box-inner">
<span><p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13475">
<div class="article-summary-box-inner">
<span><p>Over the years, there has been a paradigm shift in how users access financial
services. With the advancement of digitalization more users have been
preferring the online mode of performing financial activities. This has led to
the generation of a huge volume of financial content. Most investors prefer to
go through these contents before making decisions. Every industry has terms
that are specific to the domain it operates in. Banking and Financial Services
are not an exception to this. In order to fully comprehend these contents, one
needs to have a thorough understanding of the financial terms. Getting a basic
idea about a term becomes easy when it is explained with the help of the broad
category to which it belongs. This broad category is referred to as hypernym.
For example, "bond" is a hypernym of the financial term "alternative
debenture". In this paper, we propose a system capable of extracting and
ranking hypernyms for a given financial term. The system has been trained with
financial text corpora obtained from various sources like DBpedia [4],
Investopedia, Financial Industry Business Ontology (FIBO), prospectus and so
on. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]
and fine-tuned using SentenceBERT [54]. A novel approach has been used to
augment the training set with negative samples. It uses the hierarchy present
in FIBO. Finally, we benchmark the system performance with that of the existing
ones. We establish that it performs better than the existing ones and is also
scalable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14104">
<div class="article-summary-box-inner">
<span><p>Interactions between humans are diverse and context-dependent, but previous
works have treated them as categorical, disregarding the heavy tail of possible
interactions. We propose a new paradigm of learning human-human interactions as
free text from a single still image, allowing for flexibility in modeling the
unlimited space of situations and relationships between people. To overcome the
absence of data labelled specifically for this task, we use knowledge
distillation applied to synthetic caption data produced by a large language
model without explicit supervision. We show that the pseudo-labels produced by
this procedure can be used to train a captioning model to effectively
understand human-human interactions in images, as measured by a variety of
metrics that measure textual and semantic faithfulness and factual groundedness
of our predictions. We further show that our approach outperforms SOTA image
captioning and situation recognition models on this task. We will release our
code and pseudo-labels along with Waldo and Wenda, a manually-curated test set
for still image human-human interaction understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Chain-of-Thought Prompting for Code Generation. (arXiv:2305.06599v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06599">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive
performance in code generation. LLMs take prompts as inputs, and
Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.
CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural
language reasoning steps) and then output the code. However, CoT prompting is
designed for natural language generation and has low accuracy in code
generation.
</p>
<p>In this paper, we propose Structured CoTs (SCoTs) and present a novel
prompting technique for code generation, named SCoT prompting. Our motivation
is source code contains rich structural information and any code can be
composed of three program structures (i.e., sequence, branch, and loop
structures). Intuitively, structured intermediate reasoning steps make for
structured source code. Thus, we ask LLMs to use program structures to build
CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.
Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think
about how to solve requirements from the view of source code and further the
performance of LLMs in code generation. We apply SCoT prompting to two LLMs
(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,
MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline
- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human
developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to
examples and achieves substantial improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11095">
<div class="article-summary-box-inner">
<span><p>We investigate the emergent abilities of the recently proposed web-scale
speech model Whisper, by adapting it to unseen tasks with prompt engineering.
We selected three tasks: audio-visual speech recognition (AVSR), code-switched
speech recognition (CS-ASR), and speech translation (ST) on unseen language
pairs. We design task-specific prompts, by either leveraging another
large-scale model, or simply manipulating the special tokens in the default
prompts. Experiments show that compared to the default prompts, our proposed
prompts improve performance by 10% to 45% on the three zero-shot tasks, and
even outperform SotA supervised models on some datasets. In addition, our
experiments reveal many interesting properties of Whisper, including its
robustness to prompts, bias on accents, and the multilingual understanding in
its latent space. Code is available at
https://github.com/jasonppy/PromptingWhisper
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive cross-modality pretraining approaches have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of gender-attribute-enhanced contrastive language-audio pretraining (CLAP)
method for speech emotion recognition (SER).Specifically, an effective emotion
CLAP model (Emo-CLAP) is first built, using various self-supervised pre-trained
models for SER. Second, given the significance of the gender attribute in
speech emotion modeling, two novel soft label based GEmo-CLAP (SL-GEmo-CLAP)
and multi-task learning based GEmo-CLAP (ML-GEmo-CLAP) are further proposed to
incorporate gender information of speech signals, forming more reasonable
objectives. Experiments on IEMOCAP demonstrate that our proposed two GEmo-CLAPs
consistently outperform the baseline Emo-CLAP with various pre-trained models,
while also achieving the best recognition performance compared with
state-of-the-art SER methods. Remarkably, the proposed WavLM-based SL-GEmo-CLAP
model achieves the best UAR of 81.43\% and WAR of 83.16\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03987">
<div class="article-summary-box-inner">
<span><p>Recently developed large language models have achieved remarkable success in
generating fluent and coherent text. However, these models often tend to
'hallucinate' which critically hampers their reliability. In this work, we
address this crucial problem and propose an approach that actively detects and
mitigates hallucinations during the generation process. Specifically, we first
identify the candidates of potential hallucination leveraging the model's logit
output values, check their correctness through a validation procedure, mitigate
the detected hallucinations, and then continue with the generation process.
Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article
generation task', we first demonstrate the individual efficacy of our detection
and mitigation techniques. Specifically, the detection technique achieves a
recall of ~88% and the mitigation technique successfully mitigates 57.6% of the
correctly detected hallucinations. Importantly, our mitigation technique does
not introduce new hallucinations even in the case of incorrectly detected
hallucinations, i.e., false positives. Then, we show that the proposed active
detection and mitigation approach successfully reduces the hallucinations of
the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the
effectiveness and wide applicability of our approach through additional studies
including performance on different types of questions (multi-hop and false
premise questions) and with another LLM from a different model family (Vicuna).
In summary, our work contributes to improving the reliability and
trustworthiness of large language models, a crucial step en route to enabling
their widespread adoption in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06281">
<div class="article-summary-box-inner">
<span><p>Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model's abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model's predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Guided Generation for Large Language Models. (arXiv:2307.09702v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09702">
<div class="article-summary-box-inner">
<span><p>In this article we show how the problem of neural text generation can be
constructively reformulated in terms of transitions between the states of a
finite-state machine. This framework leads to an efficient approach to guiding
text generation with regular expressions and context-free grammars by allowing
the construction of an index over a language model's vocabulary. The approach
is model agnostic, allows one to enforce domain-specific knowledge and
constraints, and enables the construction of reliable interfaces by
guaranteeing the structure of the generated text. It adds little overhead to
the token sequence generation process and significantly outperforms existing
solutions. An implementation is provided in the open source Python library
Outlines
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16680">
<div class="article-summary-box-inner">
<span><p>Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for API Aspect Analysis. (arXiv:2307.16878v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16878">
<div class="article-summary-box-inner">
<span><p>We present a novel approach - CLAA - for API aspect detection in API reviews
that utilizes transformer models trained with a supervised contrastive loss
objective function. We evaluate CLAA using performance and impact analysis. For
performance analysis, we utilized a benchmark dataset on developer discussions
collected from Stack Overflow and compare the results to those obtained using
state-of-the-art transformer models. Our experiments show that contrastive
learning can significantly improve the performance of transformer models in
detecting aspects such as Performance, Security, Usability, and Documentation.
For impact analysis, we performed empirical and developer study. On a randomly
selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy
while the SOTA baseline achieved 81.5%. According to our developer study
involving 10 participants, the use of 'Stack Overflow + CLAA' resulted in
increased accuracy and confidence during API selection. Replication package:
https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00304">
<div class="article-summary-box-inner">
<span><p>We consider the problem of eliciting compositional generalization
capabilities in large language models (LLMs) with a novel type of prompting
strategy. Compositional generalization empowers the LLMs to solve problems that
are harder than the ones they have seen (i.e., easy-to-hard generalization),
which is a critical reasoning capability of human-like intelligence. However,
even the current state-of-the-art LLMs still struggle with this form of
reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,
which instructs LLMs how to compose basic skills to resolve more complex
problems. We find that it is crucial to demonstrate both the skills and the
compositional examples within the same prompting context. With as few as two
examplars, our SKiC prompting initiates strong synergies between skills and
their composition capabilities. Notably, it empowers LLMs to solve unseen
problems that require innovative skill compositions, achieving near-perfect
generalization on a broad range of challenging compositionality tasks.
Intriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling
them to leverage pre-existing internal skills acquired during earlier
pre-training stages, even when these skills are not explicitly presented in the
prompting context. This results in the capability of LLMs to solve unseen
complex problems by activating and composing internal competencies. With such
prominent features, SKiC prompting is able to achieve state-of-the-art
performance on challenging mathematical reasoning benchmarks (e.g., MATH).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Correction Remain A Problem For Large Language Models?. (arXiv:2308.01776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01776">
<div class="article-summary-box-inner">
<span><p>As large language models, such as GPT, continue to advance the capabilities
of natural language processing (NLP), the question arises: does the problem of
correction still persist? This paper investigates the role of correction in the
context of large language models by conducting two experiments. The first
experiment focuses on correction as a standalone task, employing few-shot
learning techniques with GPT-like models for error correction. The second
experiment explores the notion of correction as a preparatory task for other
NLP tasks, examining whether large language models can tolerate and perform
adequately on texts containing certain levels of noise or errors. By addressing
these experiments, we aim to shed light on the significance of correction in
the era of large language models and its implications for various NLP
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01861">
<div class="article-summary-box-inner">
<span><p>In this work, we make the first attempt to evaluate LLMs in a more
challenging code generation scenario, i.e. class-level code generation. We
first manually construct the first class-level code generation benchmark
ClassEval of 100 class-level Python code generation tasks with approximately
500 person-hours. Based on it, we then perform the first study of 11
state-of-the-art LLMs on class-level code generation. Based on our results, we
have the following main findings. First, we find that all existing LLMs show
much worse performance on class-level code generation compared to on standalone
method-level code generation benchmarks like HumanEval; and the method-level
coding ability cannot equivalently reflect the class-level coding ability among
LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior
than other LLMs on class-level code generation, and the second-tier models
includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very
similar performance. Third, we find that generating the entire class all at
once (i.e. holistic generation strategy) is the best generation strategy only
for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and
compositional) is better strategies for the other models with limited ability
of understanding long instructions and utilizing the middle information.
Lastly, we find the limited model ability of generating method-dependent code
and discuss the frequent error types in generated classes. Our benchmark is
available at https://github.com/FudanSELab/ClassEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever. (arXiv:2308.03365v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03365">
<div class="article-summary-box-inner">
<span><p>Few-shot and zero-shot entity linking focus on the tail and emerging
entities, which are more challenging but closer to real-world scenarios. The
mainstream method is the ''retrieve and rerank'' two-stage framework. In this
paper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity
candidates in an effective manner, which operates in two layers. The first
layer retrieves coarse-grained candidates by leveraging entity names, while the
second layer narrows down the search to fine-grained candidates within the
coarse-grained ones. In addition, this second layer utilizes entity
descriptions to effectively disambiguate tail or new entities that share names
with existing popular entities. Experimental results indicate that our approach
can obtain superior performance without requiring extensive finetuning in the
retrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task
6 on Chinese Few-shot and Zero-shot Entity Linking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03549">
<div class="article-summary-box-inner">
<span><p>Recent advances in Large Language Models (LLMs) have achieved remarkable
breakthroughs in understanding and responding to user intents. However, their
performance lag behind general use cases in some expertise domains, such as
Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs
rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue
data. These models lack the ability for doctor-like proactive inquiry and
multi-turn comprehension and cannot always align responses with safety and
professionalism experts. In this work, we introduce Zhongjing, the first
Chinese medical LLaMA-based LLM that implements an entire training pipeline
from pre-training to reinforcement learning with human feedback (RLHF).
Additionally, we introduce a Chinese multi-turn medical dialogue dataset of
70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly
enhances the model's capability for complex dialogue and proactive inquiry
initiation. We define a refined annotation rule and evaluation criteria given
the biomedical domain's unique characteristics. Results show that our model
outperforms baselines in various capacities and matches the performance of
ChatGPT in a few abilities, despite having 50x training data with previous best
model and 100x parameters with ChatGPT. RLHF further improves the model's
instruction-following ability and safety.We also release our code, datasets and
model for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04424">
<div class="article-summary-box-inner">
<span><p>The joint task of Dialog Sentiment Classification (DSC) and Act Recognition
(DAR) aims to predict the sentiment label and act label for each utterance in a
dialog simultaneously. However, current methods encode the dialog context in
only one direction, which limits their ability to thoroughly comprehend the
context. Moreover, these methods overlook the explicit correlations between
sentiment and act labels, which leads to an insufficient ability to capture
rich sentiment and act clues and hinders effective and accurate reasoning. To
address these issues, we propose a Bi-directional Multi-hop Inference Model
(BMIM) that leverages a feature selection network and a bi-directional
multi-hop inference network to iteratively extract and integrate rich sentiment
and act clues in a bi-directional manner. We also employ contrastive learning
and dual learning to explicitly model the correlations of sentiment and act
labels. Our experiments on two widely-used datasets show that BMIM outperforms
state-of-the-art baselines by at least 2.6% on F1 score in DAR and 1.4% on F1
score in DSC. Additionally, Our proposed model not only improves the
performance but also enhances the interpretability of the joint sentiment and
act prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04498">
<div class="article-summary-box-inner">
<span><p>Dialogue relation extraction (DRE) that identifies the relations between
argument pairs in dialogue text, suffers much from the frequent occurrence of
personal pronouns, or entity and speaker coreference. This work introduces a
new benchmark dataset DialogRE^C+, introducing coreference resolution into the
DRE scenario. With the aid of high-quality coreference knowledge, the reasoning
of argument relations is expected to be enhanced. In DialogRE^C+ dataset, we
manually annotate total 5,068 coreference chains over 36,369 argument mentions
based on the existing DialogRE data, where four different coreference chain
types namely speaker chain, person chain, location chain and organization chain
are explicitly marked. We further develop 4 coreference-enhanced graph-based
DRE models, which learn effective coreference representations for improving the
DRE task. We also train a coreference resolution model based on our annotations
and evaluate the effect of automatically extracted coreference chains
demonstrating the practicality of our dataset and its potential to other
domains and tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04502">
<div class="article-summary-box-inner">
<span><p>It has been a hot research topic to enable machines to understand human
emotions in multimodal contexts under dialogue scenarios, which is tasked with
multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received
consistent attention in recent years, where a diverse range of methods has been
proposed for securing better task performance. Most existing works treat MM-ERC
as a standard multimodal classification problem and perform multimodal feature
disentanglement and fusion for maximizing feature utility. Yet after revisiting
the characteristic of MM-ERC, we argue that both the feature multimodality and
conversational contextualization should be properly modeled simultaneously
during the feature disentanglement and fusion steps. In this work, we target
further pushing the task performance by taking full consideration of the above
insights. On the one hand, during feature disentanglement, based on the
contrastive learning technique, we devise a Dual-level Disentanglement
Mechanism (DDM) to decouple the features into both the modality space and
utterance space. On the other hand, during the feature fusion stage, we propose
a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism
(CRM) for multimodal and context integration, respectively. They together
schedule the proper integrations of multimodal and context features.
Specifically, CFM explicitly manages the multimodal feature contributions
dynamically, while CRM flexibly coordinates the introduction of dialogue
contexts. On two public MM-ERC datasets, our system achieves new
state-of-the-art performance consistently. Further analyses demonstrate that
all our proposed mechanisms greatly facilitate the MM-ERC task by making full
use of the multimodal and context features adaptively. Note that our proposed
methods have the great potential to facilitate a broader range of other
conversational multimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04566">
<div class="article-summary-box-inner">
<span><p>Machine Reading Comprehension (MRC) models tend to take advantage of spurious
correlations (also known as dataset bias or annotation artifacts in the
research community). Consequently, these models may perform the MRC task
without fully comprehending the given context and question, which is
undesirable since it may result in low robustness against distribution shift.
This paper delves into the concept of answer-position bias, where a significant
percentage of training questions have answers located solely in the first
sentence of the context. We propose a Single-Sentence Reader as a new approach
for addressing answer position bias in MRC. We implement this approach using
six different models and thoroughly analyze their performance. Remarkably, our
proposed Single-Sentence Readers achieve results that nearly match those of
models trained on conventional training sets, proving their effectiveness. Our
study also discusses several challenges our Single-Sentence Readers encounter
and proposes a potential solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04711">
<div class="article-summary-box-inner">
<span><p>When provided with sufficient explanatory context, smaller Language Models
have been shown to exhibit strong reasoning ability on challenging short-answer
question-answering tasks where the questions are unseen in training. We
evaluate two methods for further improvement in this setting. Both methods
focus on combining rationales generated by a larger Language Model with longer
contexts created from a multi-hop dense retrieval system. The first method
($\textit{RR}$) involves training a Rationale Ranking model to score both
generated rationales and retrieved contexts with respect to relevance and
truthfulness. We then use the scores to derive combined contexts from both
knowledge sources using a number of combinatory strategies. For the second
method ($\textit{RATD}$) we train a smaller Reasoning model using
retrieval-augmented training datasets such that it becomes proficient at
utilising relevant information from longer text sequences that may be only
partially evidential and frequently contain many irrelevant sentences.
Generally we find that both methods are effective but that the $\textit{RATD}$
method is more straightforward to apply and produces the strongest results in
the unseen setting on which we focus. Our single best Reasoning model using
only 440 million parameters materially improves upon strong comparable prior
baselines for unseen evaluation datasets (StrategyQA 58.9 $\rightarrow$ 61.7
acc., CommonsenseQA 63.6 $\rightarrow$ 72.7 acc., ARC-DA 31.6 $\rightarrow$
52.1 F1, IIRC 25.5 $\rightarrow$ 27.3 F1) and a version utilising our prior
knowledge of each type of question in selecting a context combination strategy
does even better. Our proposed models also generally outperform direct prompts
against much larger models (BLOOM 175B and StableVicuna 13B) in both few-shot
chain-of-thought and few-shot answer-only settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.05081">
<div class="article-summary-box-inner">
<span><p>Video Semantic Role Labeling (VidSRL) aims to detect the salient events from
given videos, by recognizing the predict-argument event structures and the
interrelationships between events. While recent endeavors have put forth
methods for VidSRL, they can be mostly subject to two key drawbacks, including
the lack of fine-grained spatial scene perception and the insufficiently
modeling of video temporality. Towards this end, this work explores a novel
holistic spatio-temporal scene graph (namely HostSG) representation based on
the existing dynamic scene graph structures, which well model both the
fine-grained spatial semantics and temporal dynamics of videos for VidSRL.
Built upon the HostSG, we present a nichetargeting VidSRL framework. A
scene-event mapping mechanism is first designed to bridge the gap between the
underlying scene structure and the high-level event semantic structure,
resulting in an overall hierarchical scene-event (termed ICE) graph structure.
We further perform iterative structure refinement to optimize the ICE graph,
such that the overall structure representation can best coincide with end task
demand. Finally, three subtask predictions of VidSRL are jointly decoded, where
the end-to-end paradigm effectively avoids error propagation. On the benchmark
dataset, our framework boosts significantly over the current best-performing
model. Further analyses are shown for a better understanding of the advances of
our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06111">
<div class="article-summary-box-inner">
<span><p>Auditing financial documents is a very tedious and time-consuming process. As
of today, it can already be simplified by employing AI-based solutions to
recommend relevant text passages from a report for each legal requirement of
rigorous accounting standards. However, these methods need to be fine-tuned
regularly, and they require abundant annotated data, which is often lacking in
industrial environments. Hence, we present ZeroShotALI, a novel recommender
system that leverages a state-of-the-art large language model (LLM) in
conjunction with a domain-specifically optimized transformer-based
text-matching solution. We find that a two-step approach of first retrieving a
number of best matching document sections per legal requirement with a custom
BERT-based model and second filtering these selections using an LLM yields
significant performance improvements over existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06259">
<div class="article-summary-box-inner">
<span><p>We present a scalable method to build a high quality instruction following
language model by automatically labelling human-written text with corresponding
instructions. Our approach, named instruction backtranslation, starts with a
language model finetuned on a small amount of seed data, and a given web
corpus. The seed model is used to construct training examples by generating
instruction prompts for web documents (self-augmentation), and then selecting
high quality examples from among these candidates (self-curation). This data is
then used to finetune a stronger model. Finetuning LLaMa on two iterations of
our approach yields a model that outperforms all other LLaMa-based models on
the Alpaca leaderboard not relying on distillation data, demonstrating highly
effective self-alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04889">
<div class="article-summary-box-inner">
<span><p>The rapid growth of information in the field of Generative Artificial
Intelligence (AI), particularly in the subfields of Natural Language Processing
(NLP) and Machine Learning (ML), presents a significant challenge for
researchers and practitioners to keep pace with the latest developments. To
address the problem of information overload, this report by the Natural
Language Learning Group at Bielefeld University focuses on identifying the most
popular papers on arXiv, with a specific emphasis on NLP and ML. The objective
is to offer a quick guide to the most relevant and widely discussed research,
aiding both newcomers and established researchers in staying abreast of current
trends. In particular, we compile a list of the 40 most popular papers based on
normalized citation counts from the first half of 2023. We observe the
dominance of papers related to Large Language Models (LLMs) and specifically
ChatGPT during the first half of 2023, with the latter showing signs of
declining popularity more recently, however. Further, NLP related papers are
the most influential (around 60\% of top papers) even though there are twice as
many ML related papers in our data. Core issues investigated in the most
heavily cited papers are: LLM efficiency, evaluation techniques, ethical
considerations, embodied agents, and problem-solving with LLMs. Additionally,
we examine the characteristics of top papers in comparison to others outside
the top-40 list (noticing the top paper's focus on LLM related issues and
higher number of co-authors) and analyze the citation distributions in our
dataset, among others.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-15 23:10:33.852543901 UTC">2023-08-15 23:10:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>