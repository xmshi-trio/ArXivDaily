<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-11-01T01:30:00Z">11-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19923">
<div class="article-summary-box-inner">
<span><p>Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
</p>
<p>To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19942">
<div class="article-summary-box-inner">
<span><p>In this work, we address the NER problem by splitting it into two logical
sub-tasks: (1) Span Detection which simply extracts entity mention spans
irrespective of entity type; (2) Span Classification which classifies the spans
into their entity types. Further, we formulate both sub-tasks as
question-answering (QA) problems and produce two leaner models which can be
optimized separately for each sub-task. Experiments with four cross-domain
datasets demonstrate that this two-step approach is both effective and time
efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17
and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all
cases, it achieves a significant reduction in training time compared to its QA
baseline counterpart. The effectiveness of our system stems from fine-tuning
the BERT model twice, separately for span detection and classification. The
source code can be found at https://github.com/c3sr/split-ner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19956">
<div class="article-summary-box-inner">
<span><p>To process novel sentences, language models (LMs) must generalize
compositionally -- combine familiar elements in new ways. What aspects of a
model's structure promote compositional generalization? Focusing on
transformers, we test the hypothesis, motivated by recent theoretical and
empirical work, that transformers generalize more compositionally when they are
deeper (have more layers). Because simply adding layers increases the total
number of parameters, confounding depth and size, we construct three classes of
models which trade off depth for width such that the total number of parameters
is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs
and fine-tune them on tasks that test for compositional generalization. We
report three main conclusions: (1) after fine-tuning, deeper models generalize
better out-of-distribution than shallower models do, but the relative benefit
of additional layers diminishes rapidly; (2) within each family, deeper models
show better language modeling performance, but returns are similarly
diminishing; (3) the benefits of depth for compositional generalization cannot
be attributed solely to better performance on language modeling or on
in-distribution data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023. (arXiv:2310.19970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19970">
<div class="article-summary-box-inner">
<span><p>The CLEF eRisk Laboratory explores solutions to different tasks related to
risk detection on the Internet. In the 2023 edition, Task 1 consisted of
searching for symptoms of depression, the objective of which was to extract
user writings according to their relevance to the BDI Questionnaire symptoms.
Task 2 was related to the problem of early detection of pathological gambling
risks, where the participants had to detect users at risk as quickly as
possible. Finally, Task 3 consisted of estimating the severity levels of signs
of eating disorders. Our research group participated in the first two tasks,
proposing solutions based on Transformers. For Task 1, we applied different
approaches that can be interesting in information retrieval tasks. Two
proposals were based on the similarity of contextualized embedding vectors, and
the other one was based on prompting, an attractive current technique of
machine learning. For Task 2, we proposed three fine-tuned models followed by
decision policy according to criteria defined by an early detection framework.
One model presented extended vocabulary with important words to the addressed
domain. In the last task, we obtained good performances considering the
decision-based metrics, ranking-based metrics, and runtime. In this work, we
explore different ways to deploy the predictive potential of Transformers in
eRisk tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19975">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) has achieved a great success in many natural
language processing (NLP) tasks. This is achieved by pretraining of LLMs on
vast amount of data and then instruction tuning to specific domains. However,
only a few instructions in the biomedical domain have been published. To
address this issue, we introduce BioInstruct, a customized task-specific
instruction dataset containing more than 25,000 examples. This dataset was
generated attractively by prompting a GPT-4 language model with a
three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using
the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical
natural language processing (BioNLP). We conducted instruction tuning on the
LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including
information extraction, question answering, and text generation. We also
evaluated how instructions contributed to model performance using multi-tasking
learning principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design. (arXiv:2310.19998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19998">
<div class="article-summary-box-inner">
<span><p>Transformer neural networks show promising capabilities, in particular for
uses in materials analysis, design and manufacturing, including their capacity
to work effectively with both human language, symbols, code, and numerical
data. Here we explore the use of large language models (LLMs) as a tool that
can support engineering analysis of materials, applied to retrieving key
information about subject areas, developing research hypotheses, discovery of
mechanistic relationships across disparate areas of knowledge, and writing and
executing simulation codes for active knowledge generation based on physical
ground truths. When used as sets of AI agents with specific features,
capabilities, and instructions, LLMs can provide powerful problem solution
strategies for applications in analysis and design problems. Our experiments
focus on using a fine-tuned model, MechGPT, developed based on training data in
the mechanics of materials domain. We first affirm how finetuning endows LLMs
with reasonable understanding of domain knowledge. However, when queried
outside the context of learned matter, LLMs can have difficulty to recall
correct information. We show how this can be addressed using
retrieval-augmented Ontological Knowledge Graph strategies that discern how the
model understands what concepts are important and how they are related.
Illustrated for a use case of relating distinct areas of knowledge - here,
music and proteins - such strategies can also provide an interpretable graph
structure with rich information at the node, edge and subgraph level. We
discuss nonlinear sampling strategies and agent-based modeling applied to
complex question answering, code generation and execution in the context of
automated force field development from actively learned Density Functional
Theory (DFT) modeling, and data analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023. (arXiv:2310.20003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20003">
<div class="article-summary-box-inner">
<span><p>MentalRiskES is a novel challenge that proposes to solve problems related to
early risk detection for the Spanish language. The objective is to detect, as
soon as possible, Telegram users who show signs of mental disorders considering
different tasks. Task 1 involved the users' detection of eating disorders, Task
2 focused on depression detection, and Task 3 aimed at detecting an unknown
disorder. These tasks were divided into subtasks, each one defining a
resolution approach. Our research group participated in subtask A for Tasks 1
and 2: a binary classification problem that evaluated whether the users were
positive or negative. To solve these tasks, we proposed models based on
Transformers followed by a decision policy according to criteria defined by an
early detection framework. One of the models presented an extended vocabulary
with important words for each task to be solved. In addition, we applied a
decision policy based on the history of predictions that the model performs
during user evaluation. For Tasks 1 and 2, we obtained the second-best
performance according to rankings based on classification and latency,
demonstrating the effectiveness and consistency of our approaches for solving
early detection problems in the Spanish language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20033">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models'
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection. (arXiv:2310.20046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20046">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20072">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation of natural language generation has long been an elusive
goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate
human judgements for a particular task and evaluation criterion. Inspired by
the generalization ability of instruction-tuned models, we propose a learned
metric based on instruction tuning. To test our approach, we collected HEAP, a
dataset of human judgements across various NLG tasks and evaluation criteria.
Our findings demonstrate that instruction tuning language models on HEAP yields
good performance on many evaluation tasks, though some criteria are less
trivial to learn than others. Further, jointly training on multiple tasks can
yield additional performance improvements, which can be beneficial for future
tasks with little to no human annotated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20077">
<div class="article-summary-box-inner">
<span><p>The transformer architecture has revolutionized Natural Language Processing
(NLP) and other machine-learning tasks, due to its unprecedented accuracy.
However, their extensive memory and parameter requirements often hinder their
practical applications. In this work, we study the effect of tensor-train
decomposition to improve the accuracy and compress transformer vision-language
neural networks, namely BERT and ViT. We focus both on embedding-layer
compression and partial tensorization of neural networks (PTNN) through an
algorithmic approach. Our novel PTNN approach significantly improves the
accuracy of existing models by up to 5%, all without the need for post-training
adjustments, breaking new ground in the field of tensor decomposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20081">
<div class="article-summary-box-inner">
<span><p>Personalization, the ability to tailor a system to individual users, is an
essential factor in user experience with natural language processing (NLP)
systems. With the emergence of Large Language Models (LLMs), a key question is
how to leverage these models to better personalize user experiences. To
personalize a language model's output, a straightforward approach is to
incorporate past user data into the language model prompt, but this approach
can result in lengthy inputs exceeding limitations on input length and
incurring latency and cost issues. Existing approaches tackle such challenges
by selectively extracting relevant user data (i.e. selective retrieval) to
construct a prompt for downstream tasks. However, retrieval-based methods are
limited by potential information loss, lack of more profound user
understanding, and cold-start challenges. To overcome these limitations, we
propose a novel summary-augmented approach by extending retrieval-augmented
personalization with task-aware user summaries generated by LLMs. The summaries
can be generated and stored offline, enabling real-world systems with runtime
constraints like voice assistants to leverage the power of LLMs. Experiments
show our method with 75% less of retrieved user data is on-par or outperforms
retrieval augmentation on most tasks in the LaMP personalization benchmark. We
demonstrate that offline summarization via LLMs and runtime retrieval enables
better performance for personalization on a range of tasks under practical
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning. (arXiv:2310.20089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20089">
<div class="article-summary-box-inner">
<span><p>Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20093">
<div class="article-summary-box-inner">
<span><p>The success of neural language models (LMs) on many technological tasks has
brought about their potential relevance as scientific theories of language
despite some clear differences between LM training and child language
acquisition. In this paper we argue that some of the most prominent benchmarks
for evaluating the syntactic capacities of LMs may not be sufficiently
rigorous. In particular, we show that the template-based benchmarks lack the
structural diversity commonly found in the theoretical and psychological
studies of language. When trained on small-scale data modeling child language
acquisition, the LMs can be readily matched by simple baseline models. We
advocate for the use of the readily available, carefully curated datasets that
have been evaluated for gradient acceptability by large pools of native
speakers and are designed to probe the structural basis of grammar
specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences
in a way inconsistent with human language users. We conclude with suggestions
for better connecting LMs with the empirical study of child language
acquisition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models. (arXiv:2310.20105v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20105">
<div class="article-summary-box-inner">
<span><p>The accurate classification of student help requests with respect to the type
of help being sought can enable the tailoring of effective responses.
Automatically classifying such requests is non-trivial, but large language
models (LLMs) appear to offer an accessible, cost-effective solution. This
study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying
help requests from students in an introductory programming class. In zero-shot
trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories,
while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests
related to debugging. Fine-tuning the GPT-3.5 model improved its performance to
such an extent that it approximated the accuracy and consistency across
categories observed between two human raters. Overall, this study demonstrates
the feasibility of using LLMs to enhance educational systems through the
automated classification of student needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Large Language Models Better Data Creators. (arXiv:2310.20111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20111">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have advanced the state-of-the-art in
NLP significantly, deploying them for downstream applications is still
challenging due to cost, responsiveness, control, or concerns around privacy
and security. As such, trainable models are still the preferred option in some
cases. However, these models still require human-labeled data for optimal
performance, which is expensive and time-consuming to obtain. In order to
address this issue, several techniques to reduce human effort involve labeling
or generating data using LLMs. Although these methods are effective for certain
applications, in practice they encounter difficulties in real-world scenarios.
Labeling data requires careful data selection, while generating data
necessitates task-specific prompt engineering. In this paper, we propose a
unified data creation pipeline that requires only a single formatting example,
and which is applicable to a broad range of tasks, including traditionally
problematic ones with semantically devoid label spaces. In our experiments we
demonstrate that instruction-following LLMs are highly cost-effective data
creators, and that models trained with these data exhibit performance better
than those trained with human-labeled data (by up to 17.5%) on
out-of-distribution evaluation, while maintaining comparable performance on
in-distribution tasks. These results have important implications for the
robustness of NLP systems deployed in the real-world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ling-CL: Understanding NLP Models through Linguistic Curricula. (arXiv:2310.20121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20121">
<div class="article-summary-box-inner">
<span><p>We employ a characterization of linguistic complexity from psycholinguistic
and language acquisition research to develop data-driven curricula to
understand the underlying linguistic knowledge that models learn to address NLP
tasks. The novelty of our approach is in the development of linguistic
curricula derived from data, existing knowledge about linguistic complexity,
and model behavior during training. By analyzing several benchmark NLP
datasets, our curriculum learning approaches identify sets of linguistic
metrics (indices) that inform the challenges and reasoning required to address
each task. Our work will inform future research in all NLP areas, allowing
linguistic complexity to be considered early in the research and development
process. In addition, our work prompts an examination of gold standards and
fair evaluation in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Prompt Tuning with Learned Prompting Layers. (arXiv:2310.20127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20127">
<div class="article-summary-box-inner">
<span><p>Prompt tuning prepends a soft prompt to the input embeddings or hidden states
and only optimizes the prompt to adapt pretrained models (PTMs) to downstream
tasks. The previous work manually selects prompt layers which are far from
optimal and failed to exploit the potential of prompt tuning. In this work, we
propose a novel framework, \underline{S}elective \underline{P}rompt
\underline{T}uning (SPT), that learns to select the proper prompt layers by
inserting a prompt controlled by a learnable probabilistic gate at each
intermediate layer. We further propose a novel bi-level optimization framework,
SPT-DARTS, that can better optimize the learnable gates and improve the final
prompt tuning performances of the learned prompt layer settings. We conduct
extensive experiments with ten benchmark datasets under the full-data and
few-shot scenarios. The results demonstrate that our SPT framework can perform
better than the previous state-of-the-art PETuning baselines with comparable or
fewer tunable parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20138">
<div class="article-summary-box-inner">
<span><p>Large language models pretrained on a huge amount of data capture rich
knowledge and information in the training data. The ability of data
memorization and regurgitation in pretrained language models, revealed in
previous studies, brings the risk of data leakage. In order to effectively
reduce these risks, we propose a framework DEPN to Detect and Edit Privacy
Neurons in pretrained language models, partially inspired by knowledge neurons
and model editing. In DEPN, we introduce a novel method, termed as privacy
neuron detector, to locate neurons associated with private information, and
then edit these detected privacy neurons by setting their activations to zero.
Furthermore, we propose a privacy neuron aggregator dememorize private
information in a batch processing manner. Experimental results show that our
method can significantly and efficiently reduce the exposure of private data
leakage without deteriorating the performance of the model. Additionally, we
empirically demonstrate the relationship between model memorization and privacy
neurons, from multiple perspectives, including model size, training time,
prompts, privacy neuron distribution, illustrating the robustness of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20144">
<div class="article-summary-box-inner">
<span><p>We introduce EELBERT, an approach for compression of transformer-based models
(e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is
achieved by replacing the input embedding layer of the model with dynamic, i.e.
on-the-fly, embedding computations. Since the input embedding layer accounts
for a significant fraction of the model size, especially for the smaller BERT
variants, replacing this layer with an embedding computation function helps us
reduce the model size significantly. Empirical evaluation on the GLUE benchmark
shows that our BERT variants (EELBERT) suffer minimal regression compared to
the traditional BERT models. Through this approach, we are able to develop our
smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully
trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20150">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20151">
<div class="article-summary-box-inner">
<span><p>Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20153">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, their suitability for domain-specific tasks, is limited
due to their immense scale at deployment, susceptibility to misinformation, and
more importantly, high data annotation costs. We propose a novel Interactive
Multi-Fidelity Learning (IMFL) framework for the cost-effective development of
small domain-specific LMs under limited annotation budgets. Our approach
formulates the domain-specific fine-tuning process as a multi-fidelity learning
problem, focusing on identifying the optimal acquisition strategy that balances
between low-fidelity automatic LLM annotations and high-fidelity human
annotations to maximize model performance. We further propose an
exploration-exploitation query strategy that enhances annotation diversity and
informativeness, incorporating two innovative designs: 1) prompt retrieval that
selects in-context examples from human-annotated samples to improve LLM
annotation, and 2) variable batch size that controls the order for choosing
each fidelity to facilitate knowledge distillation, ultimately enhancing
annotation quality. Extensive experiments on financial and medical tasks
demonstrate that IMFL achieves superior performance compared with single
fidelity annotations. Given a limited budget of human annotation, IMFL
significantly outperforms the human annotation baselines in all four tasks and
achieves very close performance as human annotations on two of the tasks. These
promising results suggest that the high human annotation costs in
domain-specific tasks can be significantly reduced by employing IMFL, which
utilizes fewer human annotations, supplemented with cheaper and faster LLM
(e.g., GPT-3.5) annotations to achieve comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20158">
<div class="article-summary-box-inner">
<span><p>Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20170">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited impressive generation
capabilities, but they suffer from hallucinations when solely relying on their
internal knowledge, especially when answering questions that require less
commonly known information. Retrieval-augmented LLMs have emerged as a
potential solution to ground LLMs in external knowledge. Nonetheless, recent
approaches have primarily emphasized retrieval from unstructured text corpora,
owing to its seamless integration into prompts. When using structured data such
as knowledge graphs, most methods simplify it into natural text, neglecting the
underlying structures. Moreover, a significant gap in the current landscape is
the absence of a realistic benchmark for evaluating the effectiveness of
grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and
text). To fill this gap, we have curated a comprehensive dataset that poses two
unique challenges: (1) Two-hop multi-source questions that require retrieving
information from both open-domain structured and unstructured knowledge
sources; retrieving information from structured knowledge sources is a critical
component in correctly answering the questions. (2) The generation of symbolic
queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another
layer of challenge. Our dataset is created using a combination of automatic
generation through predefined reasoning chains and human annotation. We also
introduce a novel approach that leverages multiple retrieval tools, including
text passage retrieval and symbolic language-assisted retrieval. Our model
outperforms previous approaches by a significant margin, demonstrating its
effectiveness in addressing the above-mentioned reasoning challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20195">
<div class="article-summary-box-inner">
<span><p>The ability to process idiomatic or literal multiword expressions is a
crucial aspect of understanding and generating any language. The task of
generating contextually relevant continuations for narratives containing
idiomatic (or literal) expressions can allow us to test the ability of
generative language models (LMs) in understanding nuanced language containing
non-compositional figurative text. We conduct a series of experiments using
datasets in two distinct languages (English and Portuguese) under three
different training settings (zero-shot, few-shot, and fine-tuned). Our results
suggest that the models are only slightly better at generating continuations
for literal contexts than idiomatic contexts, with exceedingly small margins.
Furthermore, the models studied in this work perform equally well across both
languages, indicating the robustness of generative models in performing this
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Helpful Multimodal Machine Translation. (arXiv:2310.20201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20201">
<div class="article-summary-box-inner">
<span><p>Existing multimodal machine translation (MMT) datasets consist of images and
video captions or instructional video subtitles, which rarely contain
linguistic ambiguity, making visual information ineffective in generating
appropriate translations. Recent work has constructed an ambiguous subtitles
dataset to alleviate this problem but is still limited to the problem that
videos do not necessarily contribute to disambiguation. We introduce EVA
(Extensive training set and Video-helpful evaluation set for Ambiguous
subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En)
parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs,
and corresponding video clips collected from movies and TV episodes. In
addition to the extensive training set, EVA contains a video-helpful evaluation
set in which subtitles are ambiguous, and videos are guaranteed helpful for
disambiguation. Furthermore, we propose SAFA, an MMT model based on the
Selective Attention model with two novel methods: Frame attention loss and
Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully.
Experiments on EVA show that visual information and the proposed methods can
boost translation performance, and our model performs significantly better than
existing MMT models. The EVA dataset and the SAFA model are available at:
https://github.com/ku-nlp/video-helpful-MMT.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20204">
<div class="article-summary-box-inner">
<span><p>Developing clinical prediction models (e.g., mortality prediction) based on
electronic health records (EHRs) typically relies on expert opinion for feature
selection and adjusting observation window size. This burdens experts and
creates a bottleneck in the development process. We propose Retrieval-Enhanced
Medical prediction model (REMed) to address such challenges. REMed can
essentially evaluate an unlimited number of clinical events, select the
relevant ones, and make predictions. This approach effectively eliminates the
need for manual feature selection and enables an unrestricted observation
window. We verified these properties through experiments on 27 clinical tasks
and two independent cohorts from publicly available EHR datasets, where REMed
outperformed other contemporary architectures that aim to handle as many events
as possible. Notably, we found that the preferences of REMed align closely with
those of medical experts. We expect our approach to significantly expedite the
development of EHR prediction models by minimizing clinicians' need for manual
involvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20216">
<div class="article-summary-box-inner">
<span><p>We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4
prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and
GPT-3.5 (14%), but falling short of chance and the baseline set by human
participants (63%). Participants' decisions were based mainly on linguistic
style (35%) and socio-emotional traits (27%), supporting the idea that
intelligence is not sufficient to pass the Turing Test. Participants'
demographics, including education and familiarity with LLMs, did not predict
detection rate, suggesting that even those who understand systems deeply and
interact with them frequently may be susceptible to deception. Despite known
limitations as a test of intelligence, we argue that the Turing Test continues
to be relevant as an assessment of naturalistic communication and deception. AI
models with the ability to masquerade as humans could have widespread societal
consequences, and we analyse the effectiveness of different strategies and
criteria for judging humanlikeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning. (arXiv:2310.20236v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20236">
<div class="article-summary-box-inner">
<span><p>Temporal relation classification is a pair-wise task for identifying the
relation of a temporal link (TLINK) between two mentions, i.e. event, time, and
document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs
involving a common mention do not share information. 2) Existing models with
independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from
using the whole data. This paper presents an event centric model that allows to
manage dynamic event representations across multiple TLINKs. Our model deals
with three TLINK categories with multi-task learning to leverage the full size
of data. The experimental results show that our proposal outperforms
state-of-the-art models and two transfer learning baselines on both the English
and Japanese data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20246">
<div class="article-summary-box-inner">
<span><p>Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20256">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs), such as ChatGPT, have
showcased remarkable zero-shot performance across various NLP tasks. However,
the potential of LLMs in personality detection, which involves identifying an
individual's personality from their written texts, remains largely unexplored.
Drawing inspiration from Psychological Questionnaires, which are carefully
designed by psychologists to evaluate individual personality traits through a
series of targeted items, we argue that these items can be regarded as a
collection of well-structured chain-of-thought (CoT) processes. By
incorporating these processes, LLMs can enhance their capabilities to make more
reasonable inferences on personality from textual input. In light of this, we
propose a novel personality detection method, called PsyCoT, which mimics the
way individuals complete psychological questionnaires in a multi-turn dialogue
manner. In particular, we employ a LLM as an AI assistant with a specialization
in text analysis. We prompt the assistant to rate individual items at each turn
and leverage the historical rating results to derive a conclusive personality
preference. Our experiments demonstrate that PsyCoT significantly improves the
performance and robustness of GPT-3.5 in personality detection, achieving an
average F1 score improvement of 4.23/10.63 points on two benchmark datasets
compared to the standard prompting method. Our code is available at
https://github.com/TaoYang225/PsyCoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis. (arXiv:2310.20260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20260">
<div class="article-summary-box-inner">
<span><p>Learning chess strategies has been investigated widely, with most studies
focussing on learning from previous games using search algorithms. Chess
textbooks encapsulate grandmaster knowledge, explain playing strategies and
require a smaller search space compared to traditional chess agents. This paper
examines chess textbooks as a new knowledge source for enabling machines to
learn how to play chess -- a resource that has not been explored previously. We
developed the LEAP corpus, a first and new heterogeneous dataset with
structured (chess move notations and board states) and unstructured data
(textual descriptions) collected from a chess textbook containing 1164
sentences discussing strategic moves from 91 games. We firstly labelled the
sentences based on their relevance, i.e., whether they are discussing a move.
Each relevant sentence was then labelled according to its sentiment towards the
described move. We performed empirical experiments that assess the performance
of various transformer-based baseline models for sentiment analysis. Our
results demonstrate the feasibility of employing transformer-based sentiment
analysis models for evaluating chess moves, with the best performing model
obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP
corpus to create a larger dataset, which can be used as a solution to the
limited textual resource in the chess domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Entities of Interest from Comparative Product Reviews. (arXiv:2310.20274v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20274">
<div class="article-summary-box-inner">
<span><p>This paper presents a deep learning based approach to extract product
comparison information out of user reviews on various e-commerce websites. Any
comparative product review has three major entities of information: the names
of the products being compared, the user opinion (predicate) and the feature or
aspect under comparison. All these informing entities are dependent on each
other and bound by the rules of the language, in the review. We observe that
their inter-dependencies can be captured well using LSTMs. We evaluate our
system on existing manually labeled datasets and observe out-performance over
the existing Semantic Role Labeling (SRL) framework popular for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20320">
<div class="article-summary-box-inner">
<span><p>To what degree should we ascribe cognitive capacities to Large Language
Models (LLMs), such as the ability to reason about intentions and beliefs known
as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11
base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the
dominant false-belief paradigm, including non-literal language usage and
recursive intentionality; (ii) using newly rewritten versions of standardized
tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides
closed questions; and (iv) benchmarking LLM performance against that of
children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from
the GPT family outperform other models, and often also children. Base-LLMs are
mostly unable to solve ToM tasks, even with specialized prompting. We suggest
that the interlinked evolution and development of language and ToM may help
explain what instruction-tuning adds: rewarding cooperative communication that
takes into account interlocutor and context. We conclude by arguing for a
nuanced perspective on ToM in LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FA Team at the NTCIR-17 UFO Task. (arXiv:2310.20322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20322">
<div class="article-summary-box-inner">
<span><p>The FA team participated in the Table Data Extraction (TDE) and Text-to-Table
Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of
Non-Financial Objects in Financial Reports (UFO). This paper reports our
approach to solving the problems and discusses the official results. We
successfully utilized various enhancement techniques based on the ELECTRA
language model to extract valuable data from tables. Our efforts resulted in an
impressive TDE accuracy rate of 93.43 %, positioning us in second place on the
Leaderboard rankings. This outstanding achievement is a testament to our
proposed approach's effectiveness. In the TTRE task, we proposed the rule-based
method to extract meaningful relationships between the text and tables task and
confirmed the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Erato: Automatizing Poetry Evaluation. (arXiv:2310.20326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20326">
<div class="article-summary-box-inner">
<span><p>We present Erato, a framework designed to facilitate the automated evaluation
of poetry, including that generated by poetry generation systems. Our framework
employs a diverse set of features, and we offer a brief overview of Erato's
capabilities and its potential for expansion. Using Erato, we compare and
contrast human-authored poetry with automatically-generated poetry,
demonstrating its effectiveness in identifying key differences. Our
implementation code and software are freely available under the GNU GPLv3
license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science. (arXiv:2310.20328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20328">
<div class="article-summary-box-inner">
<span><p>In this resource paper we release ChiSCor, a new corpus containing 619
fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was
compiled for studying how children render character perspectives, and
unravelling language and cognition in development, with computational tools.
Unlike existing resources, ChiSCor's stories were produced in natural contexts,
in line with recent calls for more ecologically valid datasets. ChiSCor hosts
text, audio, and annotations for character complexity and linguistic
complexity. Additional metadata (e.g. education of caregivers) is available for
one third of the Dutch children. ChiSCor also includes a small set of 62
English stories. This paper details how ChiSCor was compiled and shows its
potential for future work with three brief case studies: i) we show that the
syntactic complexity of stories is strikingly stable across children's ages;
ii) we extend work on Zipfian distributions in free speech and show that
ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show
that even though ChiSCor is relatively small, the corpus is rich enough to
train informative lemma vectors that allow us to analyse children's language
use. We end with a reflection on the value of narrative datasets in
computational linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20329">
<div class="article-summary-box-inner">
<span><p>Code editing encompasses a variety of pragmatic tasks that developers deal
with daily. Despite its relevance and practical usefulness, automatic code
editing remains an underexplored area in the evolution of deep learning models,
partly due to data scarcity. In this work, we explore the use of large language
models (LLMs) to edit code based on user instructions, covering a broad range
of implicit tasks such as comment insertion, code optimization, and code
refactoring. To facilitate this, we introduce InstructCoder, the first dataset
designed to adapt LLMs for general-purpose code editing, containing
highdiversity code-editing tasks. It consists of over 114,000
instruction-input-output triplets and covers multiple distinct code editing
scenarios. The dataset is systematically expanded through an iterative process
that commences with code editing data sourced from GitHub commits as seed
tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for
more task data. Our experiments demonstrate that open-source LLMs fine-tuned on
InstructCoder can edit code correctly based on users' instructions most of the
time, exhibiting unprecedented code-editing performance levels. Such results
suggest that proficient instruction-finetuning can lead to significant
amelioration in code editing abilities. The dataset and the source code are
available at https://github.com/qishenghu/CodeInstruct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM. (arXiv:2310.20347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20347">
<div class="article-summary-box-inner">
<span><p>We explore the utilization of the Apache TVM open source framework to
automatically generate a family of algorithms that follow the approach taken by
popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in
order to obtain high-performance blocked formulations of the general matrix
multiplication (GEMM). % In addition, we fully automatize the generation
process, by also leveraging the Apache TVM framework to derive a complete
variety of the processor-specific micro-kernels for GEMM. This is in contrast
with the convention in high performance libraries, which hand-encode a single
micro-kernel per architecture using Assembly code. % In global, the combination
of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves
portability, maintainability and, globally, streamlines the software life
cycle; 2)~provides high flexibility to easily tailor and optimize the solution
to different data types, processor architectures, and matrix operand shapes,
yielding performance on a par (or even superior for specific matrix shapes)
with that of hand-tuned libraries; and 3)~features a small memory footprint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction. (arXiv:2310.20352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20352">
<div class="article-summary-box-inner">
<span><p>Argument generation is a challenging task in natural language processing,
which requires rigorous reasoning and proper content organization. Inspired by
recent chain-of-thought prompting that breaks down a complex task into
intermediate steps, we propose Americano, a novel framework with agent
interaction for argument generation. Our approach decomposes the generation
process into sequential actions grounded on argumentation theory, which first
executes actions sequentially to generate argumentative discourse components,
and then produces a final argument conditioned on the components. To further
mimic the human writing process and improve the left-to-right generation
paradigm of current autoregressive language models, we introduce an argument
refinement module which automatically evaluates and refines argument drafts
based on feedback received. We evaluate our framework on the task of
counterargument generation using a subset of Reddit/CMV dataset. The results
show that our method outperforms both end-to-end and chain-of-thought prompting
methods and can generate more coherent and persuasive arguments with diverse
and rich contents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.20384">
<div class="article-summary-box-inner">
<span><p>Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT WEAVER: Using WEight AVERaging to enable lifelong learning for transformer-based models in biomedical semantic search engines. (arXiv:2202.10101v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10101">
<div class="article-summary-box-inner">
<span><p>Recent developments in transfer learning have boosted the advancements in
natural language processing tasks. The performance is, however, dependent on
high-quality, manually annotated training data. Especially in the biomedical
domain, it has been shown that one training corpus is not enough to learn
generic models that are able to efficiently predict on new data. Therefore, in
order to be used in real world applications state-of-the-art models need the
ability of lifelong learning to improve performance as soon as new data are
available - without the need of re-training the whole model from scratch. We
present WEAVER, a simple, yet efficient post-processing method that infuses old
knowledge into the new model, thereby reducing catastrophic forgetting. We show
that applying WEAVER in a sequential manner results in similar word embedding
distributions as doing a combined training on all data at once, while being
computationally more efficient. Because there is no need of data sharing, the
presented method is also easily applicable to federated learning settings and
can for example be beneficial for the mining of electronic health records from
different clinics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06457">
<div class="article-summary-box-inner">
<span><p>Large multilingual language models such as mBERT or XLM-R enable zero-shot
cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed
a data- and compute-efficient method for cross-lingual adjustment of mBERT that
uses a small parallel corpus to make embeddings of related words across
languages similar to each other. They showed it to be effective in NLI for five
European languages. In contrast we experiment with a typologically diverse set
of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their
original implementations to new tasks (XSR, NER, and QA) and an additional
training regime (continual learning). Our study reproduced gains in NLI for
four languages, showed improved NER, XSR, and cross-lingual QA results in three
languages (though some cross-lingual QA gains were not statistically
significant), while mono-lingual QA performance never improved and sometimes
degraded. Analysis of distances between contextualized embeddings of related
and unrelated words (across languages) showed that fine-tuning leads to
"forgetting" some of the cross-lingual alignment information. Based on this
observation, we further improved NLI performance using continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Born for Auto-Tagging: Faster and better with new objective functions. (arXiv:2206.07264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07264">
<div class="article-summary-box-inner">
<span><p>Keyword extraction is a task of text mining. It is applied to increase search
volume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass
scale of online articles and photos efficiently and accurately. BAT is invented
for auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP
not only provides service as a customized recommender system but also increases
the converting rate in E-commerce. The strength of BAT converges faster and
better than other SOTA models, as its 4-layer structure achieves the best F
scores at 50 epochs. In other words, it performs better than other models which
require deeper layers at 100 epochs. To generate rich and clean tags, awoo
creates new objective functions to maintain similar ${\rm F_1}$ scores with
cross-entropy while enhancing ${\rm F_2}$ scores simultaneously. To assure the
even better performance of F scores awoo revamps the learning rate strategy
proposed by Transformer \cite{Transformer} to increase ${\rm F_1}$ and ${\rm
F_2}$ scores at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08823">
<div class="article-summary-box-inner">
<span><p>Grounding language in vision is an active field of research seeking to
construct cognitively plausible word and sentence representations by
incorporating perceptual knowledge from vision into text-based representations.
Despite many attempts at language grounding, achieving an optimal equilibrium
between textual representations of the language and our embodied experiences
remains an open field. Some common concerns are the following. Is visual
grounding advantageous for abstract words, or is its effectiveness restricted
to concrete words? What is the optimal way of bridging the gap between text and
vision? To what extent is perceptual knowledge from images advantageous for
acquiring high-quality embeddings? Leveraging the current advances in machine
learning and natural language processing, the present study addresses these
questions by proposing a simple yet very effective computational grounding
model for pre-trained word embeddings. Our model effectively balances the
interplay between language and vision by aligning textual embeddings with
visual information while simultaneously preserving the distributional
statistics that characterize word usage in text corpora. By applying a learned
alignment, we are able to indirectly ground unseen words including abstract
words. A series of evaluations on a range of behavioural datasets shows that
visual grounding is beneficial not only for concrete words but also for
abstract words, lending support to the indirect theory of abstract concepts.
Moreover, our approach offers advantages for contextualized embeddings, such as
those generated by BERT, but only when trained on corpora of modest,
cognitively plausible sizes. Code and grounded embeddings for English are
available at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How direct is the link between words and images?. (arXiv:2206.15381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15381">
<div class="article-summary-box-inner">
<span><p>Current word embedding models despite their success, still suffer from their
lack of grounding in the real world. In this line of research, Gunther et al.
2022 proposed a behavioral experiment to investigate the relationship between
words and images. In their setup, participants were presented with a target
noun and a pair of images, one chosen by their model and another chosen
randomly. Participants were asked to select the image that best matched the
target noun. In most cases, participants preferred the image selected by the
model. Gunther et al., therefore, concluded the possibility of a direct link
between words and embodied experience. We took their experiment as a point of
departure and addressed the following questions. 1. Apart from utilizing
visually embodied simulation of given images, what other strategies might
subjects have used to solve this task? To what extent does this setup rely on
visual information from images? Can it be solved using purely textual
representations? 2. Do current visually grounded embeddings explain subjects'
selection behavior better than textual embeddings? 3. Does visual grounding
improve the semantic representations of both concrete and abstract words? To
address these questions, we designed novel experiments by using pre-trained
textual and visually grounded word embeddings. Our experiments reveal that
subjects' selection behavior is explained to a large extent based on purely
text-based embeddings and word-based similarities, suggesting a minor
involvement of active embodied experiences. Visually grounded embeddings
offered modest advantages over textual embeddings only in certain cases. These
findings indicate that the experiment by Gunther et al. may not be well suited
for tapping into the perceptual experience of participants, and therefore the
extent to which it measures visually grounded knowledge is unclear.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics. (arXiv:2209.09593v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09593">
<div class="article-summary-box-inner">
<span><p>Efficiency is a key property to foster inclusiveness and reduce environmental
costs, especially in an era of LLMs. In this work, we provide a comprehensive
evaluation of efficiency for MT evaluation metrics. Our approach involves
replacing computation-intensive transformers with lighter alternatives and
employing linear and quadratic approximations for alignment algorithms on top
of LLM representations. We evaluate six (reference-free and reference-based)
metrics across three MT datasets and examine 16 lightweight transformers. In
addition, we look into the training efficiency of metrics like COMET by
utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal
balance between quality and efficiency, (b) CPU speed-ups are more substantial
than those on GPU; (c) WMD approximations yield no efficiency gains while
reducing quality and (d) adapters enhance training efficiency (regarding
backward pass speed and memory requirements) as well as, in some cases, metric
quality. These findings can help to strike a balance between evaluation speed
and quality, which is essential for effective NLG systems. Furthermore, our
research contributes to the ongoing efforts to optimize NLG evaluation metrics
with minimal impact on performance. To our knowledge, ours is the most
comprehensive analysis of different aspects of efficiency for MT metrics
conducted so far.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11087">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models (LLMs) reflect the inherent social biases
of their training corpus. Many methods have been proposed to mitigate this
issue, but they often fail to debias or they sacrifice model accuracy. We use
conceptors--a soft projection method--to identify and remove the bias subspace
in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)
bias subspace projection by post-processing by the conceptor NOT operation; and
(2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly
incorporates the conceptor projection into all layers during training. We find
that conceptor post-processing achieves state-of-the-art (SoTA) debiasing
results while maintaining LLMs' performance on the GLUE benchmark. Further, it
is robust in various scenarios and can mitigate intersectional bias efficiently
by its AND operation on the existing bias subspaces. Although CI-BERT's
training takes all layers' bias into account and can beat its post-processing
counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We
also show the importance of carefully constructing the bias subspace. The best
results are obtained by removing outliers from the list of biased words,
combining them (via the OR operation), and computing their embeddings using the
sentences from a cleaner corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10564">
<div class="article-summary-box-inner">
<span><p>Past work has shown that paired vision-language signals substantially improve
grammar induction in multimodal datasets such as MSCOCO. We investigate whether
advancements in large language models (LLMs) that are only trained with text
could provide strong assistance for grammar induction in multimodal settings.
We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms
previous multi-modal methods, and achieves state-of-the-art grammar induction
performance for various multimodal datasets. Compared to image-aided grammar
induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1
points, with an 85% reduction in parameter count and 1.7x faster training
speed. Across three video-assisted grammar induction benchmarks, LC-PCFG
outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster
training. These results shed light on the notion that text-only language models
might include visually grounded cues that aid in grammar induction in
multimodal contexts. Moreover, our results emphasize the importance of
establishing a robust vision-free baseline when evaluating the benefit of
multimodal approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10764">
<div class="article-summary-box-inner">
<span><p>Domain adaptation aims to transfer the knowledge learned on (data-rich)
source domains to (low-resource) target domains, and a popular method is
invariant representation learning, which matches and aligns the data
distributions on the feature space. Although this method is studied extensively
and applied on classification and regression problems, its adoption on ranking
problems is sporadic, and the few existing implementations lack theoretical
justifications. This paper revisits invariant representation learning for
ranking. Upon reviewing prior work, we found that they implement what we call
item-level alignment, which aligns the distributions of the items being ranked
from all lists in aggregate but ignores their list structure. However, the list
structure should be leveraged, because it is intrinsic to ranking problems
where the data and the metrics are defined and computed on lists, not the items
by themselves. To close this discrepancy, we propose list-level alignment --
learning domain-invariant representations at the higher level of lists. The
benefits are twofold: it leads to the first domain adaptation generalization
bound for ranking, in turn providing theoretical support for the proposed
method, and it achieves better empirical transfer performance for unsupervised
domain adaptation on ranking tasks, including passage reranking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09093">
<div class="article-summary-box-inner">
<span><p>The progress of event extraction research has been hindered by the absence of
wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 205K event mentions with 3,465 different types, making it more than 20x
larger in ontology than today's largest event dataset. GLEN is created by
utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and
PropBank rolesets. This enables us to use the abundant existing annotation for
PropBank as distant supervision. In addition, we also propose a new multi-stage
event detection model CEDAR specifically designed to handle the large ontology
size in GLEN. We show that our model exhibits superior performance compared to
a range of baselines including InstructGPT. Finally, we perform error analysis
and show that label noise is still the largest challenge for improving
performance for this new dataset. Our dataset, code, and models are released at
\url{https://github.com/ZQS1943/GLEN}.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01295">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD for cross-lingual alignment
pretraining, a parallel and large-scale multilingual conversation dataset that
we created by translating the English-only Schema-Guided Dialogue (SGD) dataset
(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately
330k utterances per language. To facilitate aligned cross-lingual
representations, we develop an efficient prompt-tuning-based method for
learning alignment prompts. We also investigate two different classifiers:
NLI-based and vanilla classifiers, and test cross-lingual capability enabled by
the aligned prompts. We evaluate our model's cross-lingual generalization
capabilities on two conversation tasks: slot-filling and intent classification.
Our results demonstrate the strong and efficient modeling ability of NLI-based
classifiers and the large cross-lingual transfer improvements achieved by our
aligned prompts, particularly in few-shot settings. In addition, we highlight
the nice results of our approach compared to LLMs such as text-davinci-003 and
ChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive
performance in English, their cross-lingual capabilities in other languages,
particularly low-resource languages, are limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03216">
<div class="article-summary-box-inner">
<span><p>In this work, we study how the performance of a given direction changes with
its sampling ratio in Multilingual Neural Machine Translation (MNMT). By
training over 200 multilingual models with various model sizes, data sizes, and
language directions, we find it interesting that the performance of certain
translation direction does not always improve with the increase of its weight
in the multi-task optimization objective. Accordingly, scalarization method
leads to a multitask trade-off front that deviates from the traditional Pareto
front when there exists data imbalance in the training corpus, which poses a
great challenge to improve the overall performance of all directions. Based on
our observations, we propose the Double Power Law to predict the unique
performance trade-off front in MNMT, which is robust across various languages,
data adequacy, and the number of tasks. Finally, we formulate the sample ratio
selection problem in MNMT as an optimization problem based on the Double Power
Law. In our experiments, it achieves better performance than temperature
searching and gradient manipulation methods with only 1/5 to 1/2 of the total
training budget. We release the code at
https://github.com/pkunlp-icler/ParetoMNMT for reproduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03531">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a critical task aiming at expanding entities of
the target semantic class described by seed entities. Most existing ESE methods
are retrieval-based frameworks that need to extract contextual features of
entities and calculate the similarity between seed entities and candidate
entities. To achieve the two purposes, they iteratively traverse the corpus and
the entity vocabulary, resulting in poor efficiency and scalability.
Experimental results indicate that the time consumed by the retrieval-based ESE
methods increases linearly with entity vocabulary and corpus size. In this
paper, we firstly propose Generative Entity Set Expansion (GenExpan) framework,
which utilizes a generative pre-trained auto-regressive language model to
accomplish ESE task. Specifically, a prefix tree is employed to guarantee the
validity of entity generation, and automatically generated class names are
adopted to guide the model to generate target entities. Moreover, we propose
Knowledge Calibration and Generative Ranking to further bridge the gap between
generic knowledge of the language model and the goal of ESE task. For
efficiency, expansion time consumed by GenExpan is independent of entity
vocabulary and corpus size, and GenExpan achieves an average 600% speedup
compared to strong baselines. For expansion effectiveness, our framework
outperforms previous state-of-the-art ESE methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07327">
<div class="article-summary-box-inner">
<span><p>Aligning large language models (LLMs) with human preferences has proven to
drastically improve usability and has driven rapid adoption as demonstrated by
ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF) greatly reduce the required
skill and domain knowledge to effectively harness the capabilities of LLMs,
increasing their accessibility and utility across various domains. However,
state-of-the-art alignment techniques like RLHF rely on high-quality human
feedback data, which is expensive to create and often remains proprietary. In
an effort to democratize research on large-scale alignment, we release
OpenAssistant Conversations, a human-generated, human-annotated assistant-style
conversation corpus consisting of 161,443 messages in 35 different languages,
annotated with 461,292 quality ratings, resulting in over 10,000 complete and
fully annotated conversation trees. The corpus is a product of a worldwide
crowd-sourcing effort involving over 13,500 volunteers. Models trained on
OpenAssistant Conversations show consistent improvements on standard benchmarks
over respective base models. We release our code and data under a fully
permissive licence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling keywords and their positions in text generation. (arXiv:2304.09516v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09516">
<div class="article-summary-box-inner">
<span><p>One of the challenges in text generation is to control text generation as
intended by the user. Previous studies proposed specifying the keywords that
should be included in the generated text. However, this approach is
insufficient to generate text that reflect the user's intent. For example,
placing an important keyword at the beginning of the text would help attract
the reader's attention; however, existing methods do not enable such flexible
control. In this paper, we tackle a novel task of controlling not only keywords
but also the position of each keyword in the text generation. To this end, we
propose a task-independent method that uses special tokens to control the
relative position of keywords. Experimental results on summarization and story
generation tasks show that the proposed method can control keywords and their
positions. The experimental results also demonstrate that controlling the
keyword positions can generate summary texts that are closer to the user's
intent than baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09842">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved remarkable progress in solving
various natural language processing tasks due to emergent reasoning abilities.
However, LLMs have inherent limitations as they are incapable of accessing
up-to-date information (stored on the Web or in task-specific knowledge bases),
using external tools, and performing precise mathematical and logical
reasoning. In this paper, we present Chameleon, an AI system that mitigates
these limitations by augmenting LLMs with plug-and-play modules for
compositional reasoning. Chameleon synthesizes programs by composing various
tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python
functions, and heuristic-based modules) for accomplishing complex reasoning
tasks. At the heart of Chameleon is an LLM-based planner that assembles a
sequence of tools to execute to generate the final response. We showcase the
effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning
tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%
overall accuracy on ScienceQA, improving the best published few-shot result by
11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,
lifting the state of the art to 98.78%. Our analysis also shows that the
GPT-4-powered planner exhibits more consistent and rational tool selection via
inferring potential constraints from instructions, compared to a
ChatGPT-powered planner. The project is available at
https://chameleon-llm.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00833">
<div class="article-summary-box-inner">
<span><p>Large language models have been shown to struggle with multi-step reasoning,
and do not retain previous reasoning steps for future use. We propose a simple
method for solving both of these problems by allowing the model to take
Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model
can deviate from the input context at any time to explicitly think and write
down its thoughts. This allows the model to perform reasoning on the fly as it
reads the context and even integrate previous reasoning steps, thus enhancing
its memory with useful information and enabling multi-step reasoning.
Experiments across a wide variety of tasks demonstrate that our method can
outperform chain-of-thought and scratchpad methods by taking Self-Notes that
interleave the input text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01210">
<div class="article-summary-box-inner">
<span><p>Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis evaluation framework to rigorously benchmark the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HumanEval
benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
test insufficiency can lead to mis-ranking. For example, both
WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
while none of them could on HumanEval. Our work not only indicates that prior
popular code synthesis evaluation results do not accurately reflect the true
performance of LLMs for code synthesis, but also opens up a new direction to
improve such programming benchmarks through automated testing. We have
open-sourced our tools, enhanced datasets as well as all LLM-generated code at
https://github.com/evalplus/evalplus to facilitate and accelerate future
LLM-for-code research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01625">
<div class="article-summary-box-inner">
<span><p>Since the proposal of transformers, these models have been limited to bounded
input lengths, because of their need to attend to every token in the input. In
this work, we propose Unlimiformer: a general approach that wraps any existing
pretrained encoder-decoder transformer, and offloads the cross-attention
computation to a single k-nearest-neighbor (kNN) index, while the returned kNN
distances are the attention dot-product scores. This kNN index can be kept on
either the GPU or CPU memory and queried in sub-linear time; this way, we can
index practically unlimited input sequences, while every attention head in
every decoder layer retrieves its top-k keys, instead of attending to every
key. We evaluate Unlimiformer on several long-document and book-summarization
benchmarks, showing that it can process even 500k token-long inputs from the
BookSum dataset, without any input truncation at test time. We demonstrate that
Unlimiformer improves pretrained models such as BART and Longformer by
extending them to unlimited inputs without additional learned weights and
without modifying their code. We make our code and models publicly available at
https://github.com/abertsch72/unlimiformer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive loose optimization for robust question answering. (arXiv:2305.03971v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03971">
<div class="article-summary-box-inner">
<span><p>Question answering methods are well-known for leveraging data bias, such as
the language prior in visual question answering and the position bias in
machine reading comprehension (extractive question answering). Current
debiasing methods often come at the cost of significant in-distribution
performance to achieve favorable out-of-distribution generalizability, while
non-debiasing methods sacrifice a considerable amount of out-of-distribution
performance in order to obtain high in-distribution performance. Therefore, it
is challenging for them to deal with the complicated changing real-world
situations. In this paper, we propose a simple yet effective novel loss
function with adaptive loose optimization, which seeks to make the best of both
worlds for question answering. Our main technical contribution is to reduce the
loss adaptively according to the ratio between the previous and current
optimization state on mini-batch training data. This loose optimization can be
used to prevent non-debiasing methods from overlearning data bias while
enabling debiasing methods to maintain slight bias learning. Experiments on the
visual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,
GQA-OOD, and the extractive question answering dataset SQuAD demonstrate that
our approach enables QA methods to obtain state-of-the-art in- and
out-of-distribution performance in most cases. The source code has been
released publicly in \url{https://github.com/reml-group/ALO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11554">
<div class="article-summary-box-inner">
<span><p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13971">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Augmentation for Multimodal Learning Under Presentation Bias. (arXiv:2305.14083v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14083">
<div class="article-summary-box-inner">
<span><p>In real-world machine learning systems, labels are often derived from user
behaviors that the system wishes to encourage. Over time, new models must be
trained as new training examples and features become available. However,
feedback loops between users and models can bias future user behavior, inducing
a presentation bias in the labels that compromises the ability to train new
models. In this paper, we propose counterfactual augmentation, a novel causal
method for correcting presentation bias using generated counterfactual labels.
Our empirical evaluations demonstrate that counterfactual augmentation yields
better downstream performance compared to both uncorrected models and existing
bias-correction methods. Model analyses further indicate that the generated
counterfactuals align closely with true counterfactuals in an oracle setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14457">
<div class="article-summary-box-inner">
<span><p>Comparative reasoning is a process of comparing objects, concepts, or
entities to draw conclusions, which constitutes a fundamental cognitive
ability. In this paper, we propose a novel framework to pre-train language
models for enhancing their abilities of comparative reasoning over texts. While
there have been approaches for NLP tasks that require comparative reasoning,
they suffer from costly manual data labeling and limited generalizability to
different tasks. Our approach introduces a novel method of collecting scalable
data for text-based entity comparison, which leverages both structured and
unstructured data. Moreover, we present a framework of pre-training language
models via three novel objectives on comparative reasoning. Evaluation on
downstream tasks including comparative question answering, question generation,
and summarization shows that our pre-training framework significantly improves
the comparative reasoning abilities of language models, especially under
low-resource conditions. This work also releases the first integrated benchmark
for comparative reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14627">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have emerged as a widely-used tool for
information seeking, but their generated outputs are prone to hallucination. In
this work, our aim is to allow LLMs to generate text with citations, improving
their factual correctness and verifiability. Existing work mainly relies on
commercial search engines and human evaluation, making it challenging to
reproduce and compare different modeling approaches. We propose ALCE, the first
benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set
of questions and retrieval corpora and requires building end-to-end systems to
retrieve supporting evidence and generate answers with citations. We develop
automatic metrics along three dimensions -- fluency, correctness, and citation
quality -- and demonstrate their strong correlation with human judgements. Our
experiments with state-of-the-art LLMs and novel prompting strategies show that
current systems have considerable room for improvement -- For example, on the
ELI5 dataset, even the best models lack complete citation support 50% of the
time. Our analyses further highlight promising future directions, including
developing better retrievers, advancing long-context LLMs, and improving the
ability to synthesize information from multiple sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14928">
<div class="article-summary-box-inner">
<span><p>Misinformation poses a critical societal challenge, and current approaches
have yet to produce an effective solution. We propose focusing on
generalization, uncertainty, and how to leverage recent large language models,
in order to create more practical tools to evaluate information veracity in
contexts where perfect classification is impossible. We first demonstrate that
GPT-4 can outperform prior methods in multiple settings and languages. Next, we
explore generalization, revealing that GPT-4 and RoBERTa-large exhibit
differences in failure modes. Third, we propose techniques to handle
uncertainty that can detect impossible examples and strongly improve outcomes.
We also discuss results on other language models, temperature, prompting,
versioning, explainability, and web retrieval, each one providing practical
insights and directions for future research. Finally, we publish the LIAR-New
dataset with novel paired English and French misinformation data and
Possibility labels that indicate if there is sufficient context for veracity
evaluation. Overall, this research lays the groundwork for future tools that
can drive real-world progress to combat misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14947">
<div class="article-summary-box-inner">
<span><p>We investigate the predictability of large language model (LLM) capabilities:
given records of past experiments using different model families, numbers of
parameters, tasks, and numbers of in-context examples, can we accurately
predict LLM performance on new experiment configurations? Answering this
question has practical implications for LLM users (e.g., deciding which models
to try), developers (e.g., prioritizing evaluation on representative tasks),
and the research community (e.g., identifying hard-to-predict capabilities that
warrant further investigation).
</p>
<p>We study the performance prediction problem on experiment records from
BIG-bench. On a random train-test split, an MLP-based predictor achieves an
$R^2$ score greater than 95%, indicating the presence of learnable patterns
within the experiment records. We then formulate the problem of searching for
"small-bench," an informative subset of BIG-bench tasks from which the
performance on the full set can be maximally recovered. We find a subset as
informative as BIG-bench Hard for evaluating new model families, while being
$3\times$ smaller. Additionally, we find competitive subsets by clustering task
representations learned by our MLP-based predictor and selecting tasks close to
cluster centroids, highlighting the importance of task diversity in
constructing "small-bench."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios. (arXiv:2305.14987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14987">
<div class="article-summary-box-inner">
<span><p>Tabular data is prevalent across various industries, necessitating
significant time and effort for users to understand and manipulate for their
information-seeking purposes. The advancements in large language models (LLMs)
have shown enormous potential to improve user efficiency. However, the adoption
of LLMs in real-world applications for table information seeking remains
underexplored. In this paper, we investigate the table-to-text capabilities of
different LLMs using four datasets within two real-world information seeking
scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets
for data insight generation, along with the FeTaQA and our newly-constructed
F2WTQ datasets for query-based generation. We structure our investigation
around three research questions, evaluating the performance of LLMs in
table-to-text generation, automated evaluation, and feedback generation,
respectively. Experimental results indicate that the current high-performing
LLM, specifically GPT-4, can effectively serve as a table-to-text generator,
evaluator, and feedback generator, facilitating users' information seeking
purposes in real-world scenarios. However, a significant performance gap still
exists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4
models. Our data and code are publicly available at
https://github.com/yale-nlp/LLM-T2T.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16397">
<div class="article-summary-box-inner">
<span><p>Text-conditioned image generation models have recently shown immense
qualitative success using denoising diffusion processes. However, unlike
discriminative vision-and-language models, it is a non-trivial task to subject
these diffusion-based generative models to automatic fine-grained quantitative
evaluation of high-level phenomena such as compositionality. Towards this goal,
we perform two innovations. First, we transform diffusion-based models (in our
case, Stable Diffusion) for any image-text matching (ITM) task using a novel
method called DiffusionITM. Second, we introduce the Generative-Discriminative
Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language
tasks, bias evaluation and detailed analysis. We find that Stable Diffusion +
DiffusionITM is competitive on many tasks and outperforms CLIP on compositional
tasks like like CLEVR and Winoground. We further boost its compositional
performance with a transfer setup by fine-tuning on MS-COCO while retaining
generative capabilities. We also measure the stereotypical bias in diffusion
models, and find that Stable Diffusion 2.1 is, for the most part, less biased
than Stable Diffusion 1.5. Overall, our results point in an exciting direction
bringing discriminative and generative model evaluation closer. We will release
code and benchmark setup soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17333">
<div class="article-summary-box-inner">
<span><p>Fine-tuning language models (LMs) has yielded success on diverse downstream
tasks, but as LMs grow in size, backpropagation requires a prohibitively large
amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients
using only two forward passes but are theorized to be catastrophically slow for
optimizing large models. In this work, we propose a memory-efficient
zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate
in-place, thereby fine-tuning LMs with the same memory footprint as inference.
For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter
model, whereas fine-tuning with backpropagation can train only a 2.7B LM with
the same budget. We conduct comprehensive experiments across model types
(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks
(classification, multiple-choice, and generation). Our results demonstrate that
(1) MeZO significantly outperforms in-context learning and linear probing; (2)
MeZO achieves comparable performance to fine-tuning with backpropagation across
multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction
in our implementation; (3) MeZO is compatible with both full-parameter and
parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO
can effectively optimize non-differentiable objectives (e.g., maximizing
accuracy or F1). We support our empirical findings with theoretical insights,
highlighting how adequate pre-training and task prompts enable MeZO to
fine-tune huge models, despite classical ZO analyses suggesting otherwise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Emotion Valence is a Joint Deep Learning Task. (arXiv:2305.17422v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17422">
<div class="article-summary-box-inner">
<span><p>The valence analysis of speakers' utterances or written posts helps to
understand the activation and variations of the emotional state throughout the
conversation. More recently, the concept of Emotion Carriers (EC) has been
introduced to explain the emotion felt by the speaker and its manifestations.
In this work, we investigate the natural inter-dependency of valence and ECs
via a multi-task learning approach. We experiment with Pre-trained Language
Models (PLM) for single-task, two-step, and joint settings for the valence and
EC prediction tasks. We compare and evaluate the performance of generative
(GPT-2) and discriminative (BERT) architectures in each setting. We observed
that providing the ground truth label of one task improves the prediction
performance of the models in the other task. We further observed that the
discriminative model achieves the best trade-off of valence and EC prediction
tasks in the joint prediction setting. As a result, we attain a single model
that performs both tasks, thus, saving computation resources at training and
inference times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18654">
<div class="article-summary-box-inner">
<span><p>Transformer large language models (LLMs) have sparked admiration for their
exceptional performance on tasks that demand intricate multi-step reasoning.
Yet, these models simultaneously show failures on surprisingly trivial
problems. This begs the question: Are these errors incidental, or do they
signal more substantial limitations? In an attempt to demystify transformer
LLMs, we investigate the limits of these models across three representative
compositional tasks -- multi-digit multiplication, logic grid puzzles, and a
classic dynamic programming problem. These tasks require breaking problems down
into sub-steps and synthesizing these steps into a precise answer. We formulate
compositional tasks as computation graphs to systematically quantify the level
of complexity, and break down reasoning steps into intermediate sub-procedures.
Our empirical findings suggest that transformer LLMs solve compositional tasks
by reducing multi-step compositional reasoning into linearized subgraph
matching, without necessarily developing systematic problem-solving skills. To
round off our empirical study, we provide theoretical arguments on abstract
multi-step reasoning problems that highlight how autoregressive generations'
performance can rapidly decay with\,increased\,task\,complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transformer Programs. (arXiv:2306.01128v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01128">
<div class="article-summary-box-inner">
<span><p>Recent research in mechanistic interpretability has attempted to
reverse-engineer Transformer models by carefully inspecting network weights and
activations. However, these approaches require considerable manual effort and
still fall short of providing complete, faithful descriptions of the underlying
algorithms. In this work, we introduce a procedure for training Transformers
that are mechanistically interpretable by design. We build on RASP [Weiss et
al., 2021], a programming language that can be compiled into Transformer
weights. Instead of compiling human-written programs into Transformers, we
design a modified Transformer that can be trained using gradient-based
optimization and then automatically converted into a discrete, human-readable
program. We refer to these models as Transformer Programs. To validate our
approach, we learn Transformer Programs for a variety of problems, including an
in-context learning task, a suite of algorithmic problems (e.g. sorting,
recognizing Dyck languages), and NLP tasks including named entity recognition
and text classification. The Transformer Programs can automatically find
reasonable solutions, performing on par with standard Transformers of
comparable size; and, more importantly, they are easy to interpret. To
demonstrate these advantages, we convert Transformers into Python programs and
use off-the-shelf code analysis tools to debug model errors and identify the
"circuits" used to solve different sub-problems. We hope that Transformer
Programs open a new path toward the goal of intrinsically interpretable machine
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Similarities Between Language Models and Neural Response Measurements. (arXiv:2306.01930v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01930">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have complicated internal dynamics, but induce
representations of words and phrases whose geometry we can study. Human
language processing is also opaque, but neural response measurements can
provide (noisy) recordings of activation during listening or reading, from
which we can extract similar representations of words and phrases. Here we
study the extent to which the geometries induced by these representations,
share similarities in the context of brain decoding. We find that the larger
neural language models get, the more their representations are structurally
similar to neural response measurements from brain imaging. Code is available
at \url{https://github.com/coastalcph/brainlm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04125">
<div class="article-summary-box-inner">
<span><p>In order to perform multimodal fusion of heterogeneous signals, we need to
understand their interactions: how each modality individually provides
information useful for a task and how this information changes in the presence
of other modalities. In this paper, we perform a comparative study of how
humans annotate two categorizations of multimodal interactions: (1) partial
labels, where different annotators annotate the label given the first, second,
and both modalities, and (2) counterfactual labels, where the same annotator
annotates the label given the first modality before asking them to explicitly
reason about how their answer changes when given the second. We further propose
an alternative taxonomy based on (3) information decomposition, where
annotators annotate the degrees of redundancy: the extent to which modalities
individually and together give the same predictions, uniqueness: the extent to
which one modality enables a prediction that the other does not, and synergy:
the extent to which both modalities enable one to make a prediction that one
would not otherwise make using individual modalities. Through experiments and
annotations, we highlight several opportunities and limitations of each
approach and propose a method to automatically convert annotations of partial
and counterfactual labels to information decomposition, yielding an accurate
and efficient method for quantifying multimodal interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models. (arXiv:2306.04746v2 [stat.ME] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04746">
<div class="article-summary-box-inner">
<span><p>In computational social science (CSS), researchers analyze documents to
explain social and political phenomena. In most scenarios, CSS researchers
first obtain labels for documents and then explain labels using interpretable
regression analyses in the second step. One increasingly common way to annotate
documents cheaply at scale is through large language models (LLMs). However,
like other scalable ways of producing annotations, such surrogate labels are
often imperfect and biased. We present a new algorithm for using imperfect
annotation surrogates for downstream statistical analyses while guaranteeing
statistical properties -- like asymptotic unbiasedness and proper uncertainty
quantification -- which are fundamental to CSS research. We show that direct
use of surrogate labels in downstream statistical analyses leads to substantial
bias and invalid confidence intervals, even with high surrogate accuracy of
80--90\%. To address this, we build on debiased machine learning to propose the
design-based supervised learning (DSL) estimator. DSL employs a doubly-robust
procedure to combine surrogate labels with a smaller number of high-quality,
gold-standard labels. Our approach guarantees valid inference for downstream
statistical analyses, even when surrogates are arbitrarily biased and without
requiring stringent assumptions, by controlling the probability of sampling
documents for gold-standard labeling. Both our theoretical analysis and
experimental results show that DSL provides valid statistical inference while
achieving root mean squared errors comparable to existing alternatives that
focus only on prediction without inferential guarantees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04751">
<div class="article-summary-box-inner">
<span><p>In this work we explore recent advances in instruction-tuning language models
on a range of open instruction-following datasets. Despite recent claims that
open models can be on par with state-of-the-art proprietary models, these
claims are often accompanied by limited evaluation, making it difficult to
compare models across the board and determine the utility of various resources.
We provide a large set of instruction-tuned models from 6.7B to 65B parameters
in size, trained on 12 instruction datasets ranging from manually curated
(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and
systematically evaluate them on their factual knowledge, reasoning,
multilinguality, coding, and open-ended instruction following abilities through
a collection of automatic, model-based, and human-based metrics. We further
introduce T\"ulu, our best performing instruction-tuned model suite finetuned
on a combination of high-quality open resources. Our experiments show that
different instruction-tuning datasets can uncover or enhance specific skills,
while no single dataset (or combination) provides the best performance across
all evaluations. Interestingly, we find that model and human preference-based
evaluations fail to reflect differences in model capabilities exposed by
benchmark-based evaluations, suggesting the need for the type of systemic
evaluation performed in this work. Our evaluations show that the best model in
any given evaluation reaches on average 87% of ChatGPT performance, and 73% of
GPT-4 performance, suggesting that further investment in building better base
models and instruction-tuning data is required to close the gap. We release our
instruction-tuned models, including a fully finetuned 65B T\"ulu, along with
our code, data, and evaluation framework at
https://github.com/allenai/open-instruct to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05523">
<div class="article-summary-box-inner">
<span><p>Combating disinformation is one of the burning societal crises -- about 67%
of the American population believes that disinformation produces a lot of
uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows
that disinformation can manipulate democratic processes and public opinion,
causing disruption in the share market, panic and anxiety in society, and even
death during crises. Therefore, disinformation should be identified promptly
and, if possible, mitigated. With approximately 3.2 billion images and 720,000
hours of video shared online daily on social media platforms, scalable
detection of multimodal disinformation requires efficient fact verification.
Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR),
the research community lacks substantial effort in multimodal fact
verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3
million samples that pushes the boundaries of the domain of fact verification
via a multimodal fake news dataset, in addition to offering explainability
through the concept of 5W question-answering. Salient features of the dataset
include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii)
associated images, (iv) stable diffusion-generated additional images (i.e.,
visual paraphrases), (v) pixel-level image heatmap to foster image-text
explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news
stories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. (arXiv:2306.06331v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06331">
<div class="article-summary-box-inner">
<span><p>This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models. (arXiv:2306.06815v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06815">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are progressively being utilized as machine
learning services and interface tools for various applications. However, the
security implications of LLMs, particularly in relation to adversarial and
Trojan attacks, remain insufficiently examined. In this paper, we propose
TrojLLM, an automatic and black-box framework to effectively generate universal
and stealthy triggers. When these triggers are incorporated into the input
data, the LLMs' outputs can be maliciously manipulated. Moreover, the framework
also supports embedding Trojans within discrete prompts, enhancing the overall
effectiveness and precision of the triggers' attacks. Specifically, we propose
a trigger discovery algorithm for generating universal triggers for various
inputs by querying victim LLM-based APIs using few-shot data samples.
Furthermore, we introduce a novel progressive Trojan poisoning algorithm
designed to generate poisoned prompts that retain efficacy and transferability
across a diverse range of models. Our experiments and results demonstrate
TrojLLM's capacity to effectively insert Trojans into text prompts in
real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining
exceptional performance on clean test sets. Our work sheds light on the
potential security risks in current models and offers a potential defensive
approach. The source code of TrojLLM is available at
https://github.com/UCF-ML-Research/TrojLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Propagating Knowledge Updates to LMs Through Distillation. (arXiv:2306.09306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09306">
<div class="article-summary-box-inner">
<span><p>Modern language models have the capacity to store and use immense amounts of
knowledge about real-world entities, but it remains unclear how to update such
knowledge stored in model parameters. While prior methods for updating
knowledge in LMs successfully inject atomic facts, updated LMs fail to make
inferences based on injected facts. In this work, we demonstrate that a context
distillation-based approach can both impart knowledge about entities and
propagate that knowledge to enable broader inferences. Our approach consists of
two stages: transfer set generation and distillation on the transfer set. We
first generate a transfer set by prompting a language model to generate
continuations from the entity definition. Then, we update the model parameters
so that the distribution of the LM (the student) matches the distribution of
the LM conditioned on the definition (the teacher) on the transfer set. Our
experiments demonstrate that this approach is more effective at propagating
knowledge updates than fine-tuning and other gradient-based knowledge-editing
methods. Moreover, it does not compromise performance in other contexts, even
when injecting the definitions of up to 150 entities at once.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17256">
<div class="article-summary-box-inner">
<span><p>Recommender systems play a crucial role in helping users discover information
that aligns with their interests based on their past behaviors. However,
developing personalized recommendation systems becomes challenging when
historical records of user-item interactions are unavailable, leading to what
is known as the system cold-start recommendation problem. This issue is
particularly prominent in start-up businesses or platforms with insufficient
user engagement history. Previous studies focus on user or item cold-start
scenarios, where systems could make recommendations for new users or items but
are still trained with historical user-item interactions in the same domain,
which cannot solve our problem. To bridge the gap, our research introduces an
innovative and effective approach, capitalizing on the capabilities of
pre-trained language models. We transform the recommendation process into
sentiment analysis of natural languages containing information of user profiles
and item attributes, where the sentiment polarity is predicted with prompt
learning. By harnessing the extensive knowledge housed within language models,
the prediction can be made without historical user-item interaction records. A
benchmark is also introduced to evaluate the proposed method under the
cold-start setting, and the results demonstrate the effectiveness of our
method. To the best of our knowledge, this is the first study to tackle the
system cold-start recommendation problem. The benchmark and implementation of
the method are available at https://github.com/JacksonWuxs/PromptRec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09009">
<div class="article-summary-box-inner">
<span><p>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)
opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating
code, 6) US Medical License tests, and 7) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was reasonable at identifying prime vs.
composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same
questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity
to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in
June than in March in this task. GPT-4 became less willing to answer sensitive
questions and opinion survey questions in June than in March. GPT-4 performed
better at multi-hop questions in June than in March, while GPT-3.5's
performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting
mistakes in code generation in June than in March. We provide evidence that
GPT-4's ability to follow user instructions has decreased over time, which is
one common factor behind the many behavior drifts. Overall, our findings show
that the behavior of the "same" LLM service can change substantially in a
relatively short amount of time, highlighting the need for continuous
monitoring of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12896">
<div class="article-summary-box-inner">
<span><p>This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05961">
<div class="article-summary-box-inner">
<span><p>Community Question Answering (CQA) platforms steadily gain popularity as they
provide users with fast responses to their queries. The swiftness of these
responses is contingent on a mixture of query-specific and user-related
elements. This paper scrutinizes these contributing factors within the context
of six highly popular CQA platforms, identified through their standout
answering speed. Our investigation reveals a correlation between the time taken
to yield the first response to a question and several variables: the metadata,
the formulation of the questions, and the level of interaction among users.
Additionally, by employing conventional machine learning models to analyze
these metadata and patterns of user interaction, we endeavor to predict which
queries will receive their initial responses promptly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-Separation for Causal Self-Explanation. (arXiv:2309.13391v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13391">
<div class="article-summary-box-inner">
<span><p>Rationalization is a self-explaining framework for NLP models. Conventional
work typically uses the maximum mutual information (MMI) criterion to find the
rationale that is most indicative of the target label. However, this criterion
can be influenced by spurious features that correlate with the causal rationale
or the target label. Instead of attempting to rectify the issues of the MMI
criterion, we propose a novel criterion to uncover the causal rationale, termed
the Minimum Conditional Dependence (MCD) criterion, which is grounded on our
finding that the non-causal features and the target label are
\emph{d-separated} by the causal rationale. By minimizing the dependence
between the unselected parts of the input and the target label conditioned on
the selected rationale candidate, all the causes of the label are compelled to
be selected. In this study, we employ a simple and practical measure of
dependence, specifically the KL-divergence, to validate our proposed MCD
criterion. Empirically, we demonstrate that MCD improves the F1 score by up to
$13.7\%$ compared to previous state-of-the-art MMI-based methods. Our code is
available at: \url{https://github.com/jugechengzi/Rationalization-MCD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14356">
<div class="article-summary-box-inner">
<span><p>Counterfactual examples have proven to be valuable in the field of natural
language processing (NLP) for both evaluating and improving the robustness of
language models to spurious correlations in datasets. Despite their
demonstrated utility for NLP, multimodal counterfactual examples have been
relatively unexplored due to the difficulty of creating paired image-text data
with minimal counterfactual changes. To address this challenge, we introduce a
scalable framework for automatic generation of counterfactual examples using
text-to-image diffusion models. We use our framework to create
COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and
text captions based on the MS-COCO dataset. We validate the quality of
COCO-Counterfactuals through human evaluations and show that existing
multimodal models are challenged by our counterfactual image-text pairs.
Additionally, we demonstrate the usefulness of COCO-Counterfactuals for
improving out-of-domain generalization of multimodal vision-language models via
training data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16844">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach for adapting the DebertaV3 XSmall model
pre-trained in English for Brazilian Portuguese natural language processing
(NLP) tasks. A key aspect of the methodology involves a multistep training
process to ensure the model is effectively tuned for the Portuguese language.
Initial datasets from Carolina and BrWac are preprocessed to address issues
like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of
50,000 tokens is created using SentencePiece. Rather than training from
scratch, the weights of the pre-trained English model are used to initialize
most of the network, with random embeddings, recognizing the expensive cost of
training from scratch. The model is fine-tuned using the replaced token
detection task in the same format of DebertaV3 training. The adapted model,
called DeBERTinha, demonstrates effectiveness on downstream tasks like named
entity recognition, sentiment analysis, and determining sentence relatedness,
outperforming BERTimbau-Large in two tasks despite having only 40M parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments. (arXiv:2310.05964v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05964">
<div class="article-summary-box-inner">
<span><p>After the COVID-19 pandemic caused internet usage to grow by 70%, there has
been an increased number of people all across the world using social media.
Applications like Twitter, Meta Threads, YouTube, and Reddit have become
increasingly pervasive, leaving almost no digital space where public opinion is
not expressed. This paper investigates sentiment and semantic relationships
among comments across various social media platforms, as well as discusses the
importance of shared opinions across these different media platforms, using
word embeddings to analyze components in sentences and documents. It allows
researchers, politicians, and business representatives to trace a path of
shared sentiment among users across the world. This research paper presents
multiple approaches that measure the relatedness of text extracted from user
comments on these popular online platforms. By leveraging embeddings, which
capture semantic relationships between words and help analyze sentiments across
the web, we can uncover connections regarding public opinion as a whole. The
study utilizes pre-existing datasets from YouTube, Reddit, Twitter, and more.
We made use of popular natural language processing models like Bidirectional
Encoder Representations from Transformers (BERT) to analyze sentiments and
explore relationships between comment embeddings. Additionally, we aim to
utilize clustering and Kl-divergence to find semantic relationships within
these comment embeddings across various social media platforms. Our analysis
will enable a deeper understanding of the interconnectedness of online comments
and will investigate the notion of the internet functioning as a large
interconnected brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06827">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) frequently hallucinate on abstractive
summarization tasks such as document-based question-answering, meeting
summarization, and clinical report generation, even though all necessary
information is included in context. However, optimizing LLMs to hallucinate
less on these tasks is challenging, as hallucination is hard to efficiently
evaluate at each optimization step. In this work, we show that reducing
hallucination on a synthetic task can also reduce hallucination on real-world
downstream tasks. Our method, SynTra, first designs a synthetic task where
hallucinations are easy to elicit and measure. It next optimizes the LLM's
system message via prefix-tuning on the synthetic task, and finally transfers
the system message to realistic, hard-to-optimize tasks. Across three realistic
abstractive summarization tasks, SynTra reduces hallucination for two
13B-parameter LLMs using only a synthetic retrieval task for supervision. We
also find that optimizing the system message rather than the model weights can
be critical; fine-tuning the entire model on the synthetic task can
counterintuitively increase hallucination. Overall, SynTra demonstrates that
the extra flexibility of working with synthetic data can help mitigate
undesired behaviors in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08166">
<div class="article-summary-box-inner">
<span><p>Recent advancements enlarge the capabilities of large language models (LLMs)
in zero-shot image-to-text generation and understanding by integrating
multi-modal inputs. However, such success is typically limited to English
scenarios due to the lack of large-scale and high-quality non-English
multi-modal resources, making it extremely difficult to establish competitive
counterparts in other languages. In this paper, we introduce the Ziya-Visual
series, a set of bilingual large-scale vision-language models (LVLMs) designed
to incorporate visual semantics into LLM for multi-modal dialogue. Composed of
Ziya-Visual-Base and Ziya-Visual-Chat, our models adopt the Querying
Transformer from BLIP-2, further exploring the assistance of optimization
schemes such as instruction tuning, multi-stage training and low-rank
adaptation module for visual-language alignment. In addition, we stimulate the
understanding ability of GPT-4 in multi-modal scenarios, translating our
gathered English image-text datasets into Chinese and generating
instruction-response through the in-context learning method. The experiment
results demonstrate that compared to the existing LVLMs, Ziya-Visual achieves
competitive performance across a wide range of English-only tasks including
zero-shot image-text retrieval, image captioning, and visual question
answering. The evaluation leaderboard accessed by GPT-4 also indicates that our
models possess satisfactory image-text understanding and generation
capabilities in Chinese multi-modal scenario dialogues. Code, demo and models
are available at
~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance. (arXiv:2310.10385v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10385">
<div class="article-summary-box-inner">
<span><p>Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing
but often suffers from poor zero-shot (ZS) translation qualities. While prior
work has explored the causes of overall low ZS performance, our work introduces
a fresh perspective: the presence of high variations in ZS performance. This
suggests that MNMT does not uniformly exhibit poor ZS capability; instead,
certain translation directions yield reasonable results. Through systematic
experimentation involving 1,560 language directions spanning 40 languages, we
identify three key factors contributing to high variations in ZS NMT
performance: 1) target side translation capability 2) vocabulary overlap 3)
linguistic properties. Our findings highlight that the target side translation
quality is the most influential factor, with vocabulary overlap consistently
impacting ZS performance. Additionally, linguistic properties, such as language
family and writing system, play a role, particularly with smaller models.
Furthermore, we suggest that the off-target issue is a symptom of inadequate ZS
performance, emphasizing that zero-shot translation challenges extend beyond
addressing the off-target problem. We release the data and models serving as a
benchmark to study zero-shot for future research at
https://github.com/Smu-Tan/ZS-NMT-Variations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality-Diversity through AI Feedback. (arXiv:2310.13032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13032">
<div class="article-summary-box-inner">
<span><p>In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society's capacity
for innovation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models. (arXiv:2310.13673v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13673">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been observed to encode and perpetuate
harmful associations present in the training data. We propose a theoretically
grounded framework called StereoMap to gain insights into their perceptions of
how demographic groups have been viewed by society. The framework is grounded
in the Stereotype Content Model (SCM); a well-established theory from
psychology. According to SCM, stereotypes are not all alike. Instead, the
dimensions of Warmth and Competence serve as the factors that delineate the
nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs'
perceptions of social groups (defined by socio-demographic features) using the
dimensions of Warmth and Competence. Furthermore, the framework enables the
investigation of keywords and verbalizations of reasoning of LLMs' judgments to
uncover underlying factors influencing their perceptions. Our results show that
LLMs exhibit a diverse range of perceptions towards these groups, characterized
by mixed evaluations along the dimensions of Warmth and Competence.
Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs
demonstrate an awareness of social disparities, often stating statistical data
and research findings to support their reasoning. This study contributes to the
understanding of how LLMs perceive and represent social groups, shedding light
on their potential biases and the perpetuation of harmful associations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering. (arXiv:2310.14602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.14602">
<div class="article-summary-box-inner">
<span><p>Recent studies have provided empirical evidence of the wide-ranging potential
of Generative Pre-trained Transformer (GPT), a pretrained language model, in
the field of natural language processing. GPT has been effectively employed as
a decoder within state-of-the-art (SOTA) question answering systems, yielding
exceptional performance across various tasks. However, the current research
landscape concerning GPT's application in Vietnamese remains limited. This
paper aims to address this gap by presenting an implementation of GPT-2 for
community-based question answering specifically focused on COVID-19 related
queries in Vietnamese. We introduce a novel approach by conducting a
comparative analysis of different Transformers vs SOTA models in the
community-based COVID-19 question answering dataset. The experimental findings
demonstrate that the GPT-2 models exhibit highly promising outcomes,
outperforming other SOTA models as well as previous community-based COVID-19
question answering models developed for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.15421">
<div class="article-summary-box-inner">
<span><p>Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.16176">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization aims at generating natural language summaries of a
source document that are succinct while preserving the important elements.
Despite recent advances, neural text summarization models are known to be
susceptible to hallucinating (or more correctly confabulating), that is to
produce summaries with details that are not grounded in the source document. In
this paper, we introduce a simple yet efficient technique, CoBa, to reduce
hallucination in abstractive summarization. The approach is based on two steps:
hallucination detection and mitigation. We show that the former can be achieved
through measuring simple statistics about conditional word probabilities and
distance to context words. Further, we demonstrate that straight-forward
backtracking is surprisingly effective at mitigation. We thoroughly evaluate
the proposed method with prior art on three benchmark datasets for text
summarization. The results show that CoBa is effective and efficient in
reducing hallucination, and offers great adaptability and flexibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17306">
<div class="article-summary-box-inner">
<span><p>Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17680">
<div class="article-summary-box-inner">
<span><p>Imagine a developer who can only change their last line of code, how often
would they have to start writing a function from scratch before it is correct?
Auto-regressive models for code generation from natural language have a similar
limitation: they do not easily allow reconsidering earlier tokens generated. We
introduce CodeFusion, a pre-trained diffusion code generation model that
addresses this limitation by iteratively denoising a complete program
conditioned on the encoded natural language. We evaluate CodeFusion on the task
of natural language to code generation for Bash, Python, and Microsoft Excel
conditional formatting (CF) rules. Experiments show that CodeFusion (75M
parameters) performs on par with state-of-the-art auto-regressive systems
(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and
top-5 accuracy due to its better balance in diversity versus quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17811">
<div class="article-summary-box-inner">
<span><p>Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17876">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of large language models (LLMs) has sparked interest in
data synthesis techniques, aiming to generate diverse and high-quality
synthetic datasets. However, these synthetic datasets often suffer from a lack
of diversity and added noise. In this paper, we present TarGEN, a multi-step
prompting strategy for generating high-quality synthetic datasets utilizing a
LLM. An advantage of TarGEN is its seedless nature; it does not require
specific task instances, broadening its applicability beyond task replication.
We augment TarGEN with a method known as self-correction empowering LLMs to
rectify inaccurately labeled instances during dataset creation, ensuring
reliable labels. To assess our technique's effectiveness, we emulate 8 tasks
from the SuperGLUE benchmark and finetune various language models, including
encoder-only, encoder-decoder, and decoder-only models on both synthetic and
original training sets. Evaluation on the original test set reveals that models
trained on datasets generated by TarGEN perform approximately 1-2% points
better than those trained on original datasets (82.84% via syn. vs. 81.12% on
og. using Flan-T5). When incorporating instruction tuning, the performance
increases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A
comprehensive analysis of the synthetic dataset compared to the original
dataset reveals that the synthetic dataset demonstrates similar or higher
levels of dataset complexity and diversity. Furthermore, the synthetic dataset
displays a bias level that aligns closely with the original dataset. Finally,
when pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive
results on the OpenLLM leaderboard, surpassing the model trained on the
Self-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for
quality data generation and reducing the human efforts to create complex
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18331">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as promising agents for web
navigation tasks, interpreting objectives and interacting with web pages.
However, the efficiency of spliced prompts for such tasks remains
underexplored. We introduces AllTogether, a standardized prompt template that
enhances task context representation, thereby improving LLMs' performance in
HTML-based web navigation. We evaluate the efficacy of this approach through
prompt learning and instruction finetuning based on open-source Llama-2 and
API-accessible GPT models. Our results reveal that models like GPT-4 outperform
smaller models in web navigation tasks. Additionally, we find that the length
of HTML snippet and history trajectory significantly influence performance, and
prior step-by-step instructions prove less effective than real-time
environmental feedback. Overall, we believe our work provides valuable insights
for future research in LLM-driven web agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18356">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have transformed the landscape of artificial
intelligence, while their enormous size presents significant challenges in
terms of computational costs. We introduce LoRAShear, a novel efficient
approach to structurally prune LLMs and recover knowledge. Given general LLMs,
LoRAShear at first creates the dependency graphs over LoRA modules to discover
minimally removal structures and analyze the knowledge distribution. It then
proceeds progressive structured pruning on LoRA adaptors and enables inherent
knowledge transfer to better preserve the information in the redundant
structures. To recover the lost knowledge during pruning, LoRAShear
meticulously studies and proposes a dynamic fine-tuning schemes with dynamic
data adaptors to effectively narrow down the performance gap to the full
models. Numerical results demonstrate that by only using one GPU within a
couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with
only 1.0% performance degradation and significantly outperforms
state-of-the-arts. The source code will be available at
https://github.com/microsoft/lorashear.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19019">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) exhibit impressive reasoning and data
augmentation capabilities in various NLP tasks. However, what about small
models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant
fundamentals, chain of thought, and common mistakes for most NLP samples, which
makes annotation more than just an answer, thus allowing other models to learn
"why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot
score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even
more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we
augmented 58 NLP datasets and taught various student models with different
parameters from OPT and BLOOM series in a multi-task setting. The experimental
results indicate that the data augmentation provided by TeacherLM has brought
significant benefits. We will release the TeacherLM series of models and
augmented datasets as open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Lost Patience Won't Be Robust to Adversarial Slowdown. (arXiv:2310.19152v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19152">
<div class="article-summary-box-inner">
<span><p>In this paper, we systematically evaluate the robustness of multi-exit
language models against adversarial slowdown. To audit their robustness, we
design a slowdown attack that generates natural adversarial text bypassing
early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a
comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark
against adversarial slowdown. We then show our attack significantly reduces the
computational savings provided by the three methods in both white-box and
black-box settings. The more complex a mechanism is, the more vulnerable it is
to adversarial slowdown. We also perform a linguistic analysis of the perturbed
text inputs, identifying common perturbation patterns that our attack
generates, and comparing them with standard adversarial text attacks. Moreover,
we show that adversarial training is ineffective in defeating our slowdown
attack, but input sanitization with a conversational model, e.g., ChatGPT, can
remove perturbations effectively. This result suggests that future work is
needed for developing efficient yet robust multi-exit models. Our code is
available at: https://github.com/ztcoalson/WAFFLE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constituency Parsing using LLMs. (arXiv:2310.19462v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19462">
<div class="article-summary-box-inner">
<span><p>Constituency parsing is a fundamental yet unsolved natural language
processing task. In this paper, we explore the potential of recent large
language models (LLMs) that have exhibited remarkable performance across
various domains and tasks to tackle this task. We employ three linearization
strategies to transform output trees into symbol sequences, such that LLMs can
solve constituency parsing by generating linearized trees. We conduct
experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT,
LLaMA, and Alpaca, comparing their performance against the state-of-the-art
constituency parsers. Our experiments encompass zero-shot, few-shot, and
full-training learning settings, and we evaluate the models on one in-domain
and five out-of-domain test datasets. Our findings reveal insights into LLMs'
performance, generalization abilities, and challenges in constituency parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19596">
<div class="article-summary-box-inner">
<span><p>Prevalent supervised learning methods in natural language processing (NLP)
are notoriously data-hungry, which demand large amounts of high-quality
annotated data. In practice, acquiring such data is a costly endeavor.
Recently, the superior few-shot performance of large language models (LLMs) has
propelled the development of dataset generation, where the training data are
solely synthesized from LLMs. However, such an approach usually suffers from
low-quality issues, and requires orders of magnitude more labeled data to
achieve satisfactory performance. To fully exploit the potential of LLMs and
make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as
annotators and puts them into an active learning loop to determine what to
annotate efficiently. To learn robustly with pseudo labels, we optimize both
the annotation and training processes: (1) we draw k-NN examples from a small
demonstration pool as in-context examples, and (2) we adopt the example
reweighting technique to assign training samples with learnable weights.
Compared with previous approaches, LLMaAA features both efficiency and
reliability. We conduct experiments and analysis on two classic NLP tasks,
named entity recognition and relation extraction. With LLMaAA, task-specific
models trained from LLM-generated labels can outperform the teacher within only
hundreds of annotated examples, which is much more cost-effective than other
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding. (arXiv:2310.19671v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19671">
<div class="article-summary-box-inner">
<span><p>Current Large Language Models (LLMs) are unparalleled in their ability to
generate grammatically correct, fluent text. LLMs are appearing rapidly, and
debates on LLM capacities have taken off, but reflection is lagging behind.
Thus, in this position paper, we first zoom in on the debate and critically
assess three points recurring in critiques of LLM capacities: i) that LLMs only
parrot statistical patterns in the training data; ii) that LLMs master formal
but not functional language competence; and iii) that language learning in LLMs
cannot inform human language learning. Drawing on empirical and theoretical
arguments, we show that these points need more nuance. Second, we outline a
pragmatic perspective on the issue of `real' understanding and intentionality
in LLMs. Understanding and intentionality pertain to unobservable mental states
we attribute to other humans because they have pragmatic value: they allow us
to abstract away from complex underlying mechanics and predict behaviour
effectively. We reflect on the circumstances under which it would make sense
for humans to similarly attribute mental states to LLMs, thereby outlining a
pragmatic philosophical context for LLMs as an increasingly prominent
technology in society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks. (arXiv:2310.19677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19677">
<div class="article-summary-box-inner">
<span><p>Human commonsense understanding of the physical and social world is organized
around intuitive theories. These theories support making causal and moral
judgments. When something bad happens, we naturally ask: who did what, and why?
A rich literature in cognitive science has studied people's causal and moral
intuitions. This work has revealed a number of factors that systematically
influence people's judgments, such as the violation of norms and whether the
harm is avoidable or inevitable. We collected a dataset of stories from 24
cognitive science papers and developed a system to annotate each story with the
factors they investigated. Using this dataset, we test whether large language
models (LLMs) make causal and moral judgments about text-based scenarios that
align with those of human participants. On the aggregate level, alignment has
improved with more recent LLMs. However, using statistical analyses, we find
that LLMs weigh the different factors quite differently from human
participants. These results show how curated, challenge datasets combined with
insights from cognitive science can help us go beyond comparisons based merely
on aggregate metrics: we uncover LLMs implicit tendencies and show to what
extent these align with human intuitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19708">
<div class="article-summary-box-inner">
<span><p>General purpose language models (LMs) encounter difficulties when processing
domain-specific jargon and terminology, which are frequently utilized in
specialized fields such as medicine or industrial settings. Moreover, they
often find it challenging to interpret mixed speech that blends general
language with specialized jargon. This poses a challenge for automatic speech
recognition systems operating within these specific domains. In this work, we
introduce a novel approach that integrates domain-specific or secondary LM into
general-purpose LM. This strategy involves labeling, or ``coloring'', each word
to indicate its association with either the general or the domain-specific LM.
We develop an optimized algorithm that enhances the beam search algorithm to
effectively handle inferences involving colored words. Our evaluations indicate
that this approach is highly effective in integrating jargon into language
tasks. Notably, our method substantially lowers the error rate for
domain-specific words without compromising performance in the general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.19736">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable capabilities across
a broad spectrum of tasks. They have attracted significant attention and been
deployed in numerous downstream applications. Nevertheless, akin to a
double-edged sword, LLMs also present potential risks. They could suffer from
private data leaks or yield inappropriate, harmful, or misleading content.
Additionally, the rapid progress of LLMs raises concerns about the potential
emergence of superintelligent systems without adequate safeguards. To
effectively capitalize on LLM capacities as well as ensure their safe and
beneficial development, it is critical to conduct a rigorous and comprehensive
evaluation of LLMs.
</p>
<p>This survey endeavors to offer a panoramic perspective on the evaluation of
LLMs. We categorize the evaluation of LLMs into three major groups: knowledge
and capability evaluation, alignment evaluation and safety evaluation. In
addition to the comprehensive review on the evaluation methodologies and
benchmarks on these three aspects, we collate a compendium of evaluations
pertaining to LLMs' performance in specialized domains, and discuss the
construction of comprehensive evaluation platforms that cover LLM evaluations
on capabilities, alignment, safety, and applicability.
</p>
<p>We hope that this comprehensive overview will stimulate further research
interests in the evaluation of LLMs, with the ultimate goal of making
evaluation serve as a cornerstone in guiding the responsible development of
LLMs. We envision that this will channel their evolution into a direction that
maximizes societal benefit while minimizing potential risks. A curated list of
related papers has been publicly available at
https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-11-01 23:11:22.881617992 UTC">2023-11-01 23:11:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>